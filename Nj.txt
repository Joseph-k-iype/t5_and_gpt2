# Show visualization options
            st.markdown("##### Visualization")
            viz_type = st.selectbox(
                "Chart Type",
                options=['Bar Chart', 'Line Chart', 'Heatmap', 'Scatter Plot']
            )
            
            if viz_type == 'Bar Chart':
                fig = px.bar(
                    pivot_result['pivot'].reset_index(),
                    x=pivot_result['pivot'].index.name,
                    y=pivot_result['pivot'].columns,
                    barmode='group',
                    title='Pivot Data Visualization'
                )
            elif viz_type == 'Line Chart':
                fig = px.line(
                    pivot_result['pivot'].reset_index(),
                    x=pivot_result['pivot'].index.name,
                    y=pivot_result['pivot'].columns,
                    title='Pivot Data Visualization'
                )
            elif viz_type == 'Heatmap':
                fig = px.imshow(
                    pivot_result['pivot'],
                    title='Pivot Data Heatmap',
                    aspect='auto'
                )
            else:  # Scatter Plot
                fig = px.scatter(
                    pivot_result['pivot'].reset_index(),
                    x=pivot_result['pivot'].index.name,
                    y=pivot_result['pivot'].columns[0],
                    size=pivot_result['pivot'].columns[0] if len(pivot_result['pivot'].columns) > 0 else None,
                    title='Pivot Data Scatter Plot'
                )
            
            # Customize layout
            fig.update_layout(
                height=600,
                showlegend=True,
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                )
            )
            st.plotly_chart(fig, use_container_width=True)

            # Drill-down functionality
            if st.checkbox("Enable Drill-Down"):
                selected_index = st.selectbox(
                    "Select value to drill down",
                    options=pivot_result['pivot'].index
                )
                
                if selected_index:
                    drill_down_data = pivot_result.get('drill_down', {}).get(selected_index)
                    if drill_down_data is not None:
                        st.write("Drill-down view:")
                        st.dataframe(drill_down_data)
            
            # Export options
            st.markdown("##### Export Options")
            col1, col2 = st.columns(2)
            with col1:
                export_format = st.selectbox(
                    "Export Format",
                    options=['Excel', 'CSV', 'PDF']
                )
            with col2:
                if st.button("Export"):
                    try:
                        with self.ui_components.show_spinner("Preparing export..."):
                            if export_format == 'Excel':
                                buffer = BytesIO()
                                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                                    pivot_result['pivot'].to_excel(writer, sheet_name='Pivot Table')
                                    if pivot_result.get('subtotals'):
                                        pd.DataFrame(pivot_result['subtotals']).to_excel(
                                            writer, 
                                            sheet_name='Subtotals'
                                        )
                                buffer.seek(0)
                                
                                st.download_button(
                                    label="Download Excel",
                                    data=buffer,
                                    file_name=f"pivot_analysis_{datetime.now():%Y%m%d_%H%M%S}.xlsx",
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                )
                            elif export_format == 'CSV':
                                csv = pivot_result['pivot'].to_csv(index=True)
                                st.download_button(
                                    label="Download CSV",
                                    data=csv,
                                    file_name=f"pivot_analysis_{datetime.now():%Y%m%d_%H%M%S}.csv",
                                    mime="text/csv"
                                )
                            else:  # PDF
                                # Create PDF report
                                buffer = BytesIO()
                                pdf = self.create_pdf_report(pivot_result, buffer)
                                buffer.seek(0)
                                
                                st.download_button(
                                    label="Download PDF",
                                    data=buffer,
                                    file_name=f"pivot_analysis_{datetime.now():%Y%m%d_%H%M%S}.pdf",
                                    mime="application/pdf"
                                )
                    except Exception as e:
                        self.ui_components.show_error(e, "export process")
        
        except Exception as e:
            self.ui_components.show_error(e, "preview panel")
    
    def create_pdf_report(self, pivot_result: dict, buffer: BytesIO) -> None:
        try:
            doc = SimpleDocTemplate(buffer, pagesize=letter)
            elements = []
            styles = getSampleStyleSheet()
            
            # Add title
            elements.append(Paragraph(
                f"Pivot Analysis Report - {datetime.now():%Y-%m-%d %H:%M:%S}",
                styles['Title']
            ))
            
            # Add summary
            elements.append(Paragraph("Summary", styles['Heading1']))
            elements.append(Paragraph(
                f"Total Records: {pivot_result['filtered_records']}",
                styles['Normal']
            ))
            
            # Add pivot table
            elements.append(Paragraph("Pivot Table", styles['Heading1']))
            pivot_data = [[''] + list(pivot_result['pivot'].columns)]
            for idx, row in pivot_result['pivot'].iterrows():
                pivot_data.append([idx] + list(row))
            
            table = Table(pivot_data)
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('TEXTCOLOR', (0, 1), (-1, -1), colors.black),
                ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
                ('FONTSIZE', (0, 1), (-1, -1), 10),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('BOX', (0, 0), (-1, -1), 2, colors.black)
            ]))
            elements.append(table)
            
            # Build PDF
            doc.build(elements)
            
        except Exception as e:
            raise ErrorHandler.ProcessingError(f"Error creating PDF report: {str(e)}")

# Update the main application to use the enhanced UI
def main():
    st.set_page_config(page_title="Advanced Data Analysis Tool", layout="wide")
    
    # Initialize components
    ui = UIComponents()
    error_handler = ErrorHandler()
    
    try:
        with ui.show_spinner("Initializing application..."):
            if 'pivot_ui' not in st.session_state:
                st.session_state.pivot_ui = AdvancedPivotUI()
            
            st.title("Advanced Data Analysis Tool")
            
            # Initialize session state for data storage
            if 'datasets' not in st.session_state:
                st.session_state.datasets = {}
                st.session_state.merged_data = None
                st.session_state.analysis_history = []
            
            # File Upload Section with error handling
            st.header("1. Data Upload")
            uploaded_files = st.file_uploader(
                "Upload Excel Files",
                type=['xlsx', 'xls'],
                accept_multiple_files=True
            )
            
            if uploaded_files:
                for file in uploaded_files:
                    try:
                        if file.name not in st.session_state.datasets:
                            with ui.show_spinner(f"Reading {file.name}..."):
                                df = pd.read_excel(file)
                                error_handler.validate_dataframe(df)
                                
                                st.session_state.datasets[file.name] = {
                                    'df': df,
                                    'columns': df.columns.tolist(),
                                    'numeric_columns': df.select_dtypes(
                                        include=[np.number]).columns.tolist(),
                                    'categorical_columns': df.select_dtypes(
                                        include=['object']).columns.tolist(),
                                    'date_columns': df.select_dtypes(
                                        include=['datetime64']).columns.tolist()
                                }
                                ui.show_success(f"Successfully loaded {file.name}")
                    except Exception as e:
                        ui.show_error(e, f"loading {file.name}")
            
            # Data Processing Sections
            if st.session_state.datasets:
                # Add tabs for different analysis types
                tab1, tab2, tab3, tab4 = st.tabs([
                    "Data Cleaning",
                    "Data Merging",
                    "Analysis",
                    "History"
                ])
                
                with tab1:
                    st.header("2. Data Cleaning")
                    selected_dataset = st.selectbox(
                        "Select Dataset to Clean",
                        options=list(st.session_state.datasets.keys())
                    )
                    
                    if selected_dataset:
                        try:
                            data_info = st.session_state.datasets[selected_dataset]
                            df = data_info['df']
                            
                            st.write("Dataset Summary:")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                ui.show_metric_card(
                                    "Total Records",
                                    len(df),
                                    help_text="Number of rows in dataset"
                                )
                            with col2:
                                ui.show_metric_card(
                                    "Total Columns",
                                    len(df.columns),
                                    help_text="Number of columns in dataset"
                                )
                            with col3:
                                missing_cells = df.isna().sum().sum()
                                ui.show_metric_card(
                                    "Missing Values",
                                    missing_cells,
                                    help_text="Total number of missing cells"
                                )
                            
                            # Cleaning options
                            st.subheader("Cleaning Options")
                            
                            cleaning_options = {
                                'remove_duplicates': st.checkbox("Remove duplicate rows"),
                                'handle_missing': st.checkbox("Handle missing values"),
                                'standardize_dates': st.checkbox("Standardize date formats")
                            }
                            
                            if cleaning_options['handle_missing']:
                                col1, col2 = st.columns(2)
                                with col1:
                                    numeric_fill = st.selectbox(
                                        "Handle missing numeric values",
                                        options=['mean', 'median', 'zero']
                                    )
                                with col2:
                                    categorical_fill = st.selectbox(
                                        "Handle missing categorical values",
                                        options=['mode', 'unknown']
                                    )
                            
                            if cleaning_options['standardize_dates']:
                                date_columns = st.multiselect(
                                    "Select date columns",
                                    options=data_info['columns']
                                )
                            
                            if st.button("Apply Cleaning"):
                                try:
                                    with ui.show_spinner("Cleaning data..."):
                                        cleaned_df = df.copy()
                                        
                                        if cleaning_options['remove_duplicates']:
                                            cleaned_df = DataCleaner.remove_duplicates(cleaned_df)
                                        
                                        if cleaning_options['handle_missing']:
                                            cleaned_df = DataCleaner.handle_missing_values(
                                                cleaned_df,
                                                numeric_fill,
                                                categorical_fill
                                            )
                                        
                                        if cleaning_options['standardize_dates'] and date_columns:
                                            cleaned_df = DataCleaner.standardize_dates(
                                                cleaned_df,
                                                date_columns
                                            )
                                        
                                        # Update dataset
                                        st.session_state.datasets[selected_dataset]['df'] = cleaned_df
                                        ui.show_success("Data cleaning completed successfully!")
                                        
                                        # Show cleaning summary
                                        st.write("Cleaning Summary:")
                                        st.write({
                                            'original_rows': len(df),
                                            'cleaned_rows': len(cleaned_df),
                                            'removed_duplicates': len(df) - len(cleaned_df),
                                            'filled_missing': df.isna().sum().sum() - cleaned_df.isna().sum().sum()
                                        })
                                except Exception as e:
                                    ui.show_error(e, "data cleaning")
                        
                        except Exception as e:
                            ui.show_error(e, "loading dataset for cleaning")
                
                with tab2:
                    st.header("3. Data Merging")
                    if len(st.session_state.datasets) > 1:
                        try:
                            datasets_to_merge = st.multiselect(
                                "Select datasets to merge",
                                options=list(st.session_state.datasets.keys())
                            )
                            
                            if len(datasets_to_merge) >= 2:
                                merge_config = {}
                                for dataset in datasets_to_merge:
                                    merge_config[dataset] = {
                                        'key': st.selectbox(
                                            f"Select merge key for {dataset}",
                                            options=st.session_state.datasets[dataset]['columns']
                                        ),
                                        'columns': st.multiselect(
                                            f"Select columns to include from {dataset}",
                                            options=st.session_state.datasets[dataset]['columns']
                                        )
                                    }
                                
                                merge_type = st.selectbox(
                                    "Select merge type",
                                    options=['inner', 'left', 'right', 'outer']
                                )
                                
                                if st.button("Merge Datasets"):
                                    try:
                                        with ui.show_spinner("Merging datasets..."):
                                            # Perform merge
                                            merged_df = st.session_state.datasets[
                                                datasets_to_merge[0]
                                            ]['df']
                                            
                                            for dataset in datasets_to_merge[1:]:
                                                merged_df = merged_df.merge(
                                                    st.session_state.datasets[dataset]['df'],
                                                    left_on=merge_config[datasets_to_merge[0]]['key'],
                                                    right_on=merge_config[dataset]['key'],
                                                    how=merge_type
                                                # Custom UI Components and Error Handling
class UIComponents:
    @staticmethod
    def show_spinner(text: str):
        return st.spinner(text)
    
    @staticmethod
    def show_success(text: str, duration: int = 3):
        st.success(text)
        time.sleep(duration)
    
    @staticmethod
    def show_error(error: Exception, context: str = ""):
        error_message = f"Error in {context}: {str(error)}" if context else str(error)
        st.error(error_message)
        
        if st.checkbox("Show detailed error information"):
            st.exception(error)
    
    @staticmethod
    def show_info_card(title: str, content: str, key: str = None):
        with st.container():
            st.markdown(f"""
            <div style='padding: 1rem; border-radius: 0.5rem; border: 1px solid #e0e0e0; margin-bottom: 1rem;'>
                <h4>{title}</h4>
                <p>{content}</p>
            </div>
            """, unsafe_allow_html=True)
    
    @staticmethod
    def show_metric_card(label: str, value: Any, delta: Any = None, 
                        help_text: str = None):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.metric(label=label, value=value, delta=delta, help=help_text)
    
    @staticmethod
    def show_progress(current: int, total: int, text: str = "Processing"):
        progress_bar = st.progress(0)
        progress_text = st.empty()
        
        progress = int((current / total) * 100)
        progress_bar.progress(progress)
        progress_text.text(f"{text}: {progress}%")

class ErrorHandler:
    class DataError(Exception):
        pass
    
    class ConfigurationError(Exception):
        pass
    
    class ProcessingError(Exception):
        pass
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        if df.empty:
            raise ErrorHandler.DataError("DataFrame is empty")
        return True
    
    @staticmethod
    def validate_columns(df: pd.DataFrame, required_columns: List[str]) -> bool:
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise ErrorHandler.DataError(
                f"Missing required columns: {', '.join(missing_columns)}"
            )
        return True
    
    @staticmethod
    def validate_numeric_columns(df: pd.DataFrame, 
                               numeric_columns: List[str]) -> bool:
        non_numeric = [col for col in numeric_columns 
                      if not np.issubdtype(df[col].dtype, np.number)]
        if non_numeric:
            raise ErrorHandler.DataError(
                f"Columns must be numeric: {', '.join(non_numeric)}"
            )
        return True
    
    @staticmethod
    def handle_processing_error(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                raise ErrorHandler.ProcessingError(
                    f"Error in {func.__name__}: {str(e)}"
                )
        return wrapper

# Enhanced UI for Advanced Pivot Table
class AdvancedPivotUI:
    def __init__(self):
        self.ui_components = UIComponents()
        self.error_handler = ErrorHandler()
    
    def render_field_selection(self, df: pd.DataFrame) -> dict:
        try:
            st.subheader("Field Selection")
            
            # Available fields panel
            with st.container():
                st.markdown("### Available Fields")
                col1, col2 = st.columns([1, 3])
                
                with col1:
                    st.markdown("##### Field Types")
                    numeric_fields = df.select_dtypes(include=[np.number]).columns
                    categorical_fields = df.select_dtypes(exclude=[np.number]).columns
                    date_fields = df.select_dtypes(include=['datetime64']).columns
                    
                    st.markdown("**Numeric Fields**")
                    for field in numeric_fields:
                        st.checkbox(f"📊 {field}", key=f"num_{field}")
                    
                    st.markdown("**Categorical Fields**")
                    for field in categorical_fields:
                        st.checkbox(f"📝 {field}", key=f"cat_{field}")
                    
                    st.markdown("**Date Fields**")
                    for field in date_fields:
                        st.checkbox(f"📅 {field}", key=f"date_{field}")
                
                with col2:
                    st.markdown("##### Field Configuration")
                    selected_fields = {
                        'rows': st.multiselect("Row Fields", df.columns,
                                           help="Select fields for row headers"),
                        'columns': st.multiselect("Column Fields", df.columns,
                                              help="Select fields for column headers"),
                        'values': st.multiselect("Value Fields", numeric_fields,
                                             help="Select fields for calculations")
                    }
                    
                    # Field settings
                    if selected_fields['values']:
                        st.markdown("##### Value Field Settings")
                        value_settings = {}
                        for field in selected_fields['values']:
                            with st.expander(f"Settings for {field}"):
                                value_settings[field] = {
                                    'aggregation': st.selectbox(
                                        "Aggregation",
                                        ['sum', 'mean', 'count', 'min', 'max', 'median'],
                                        key=f"agg_{field}"
                                    ),
                                    'format': st.selectbox(
                                        "Display Format",
                                        ['number', 'percentage', 'currency'],
                                        key=f"fmt_{field}"
                                    ),
                                    'conditional_formatting': st.checkbox(
                                        "Enable Conditional Formatting",
                                        key=f"cond_{field}"
                                    )
                                }
                                
                                if value_settings[field]['conditional_formatting']:
                                    value_settings[field]['formatting_rules'] = {
                                        'min_color': st.color_picker(
                                            "Minimum Color",
                                            '#ff0000',
                                            key=f"min_color_{field}"
                                        ),
                                        'max_color': st.color_picker(
                                            "Maximum Color",
                                            '#00ff00',
                                            key=f"max_color_{field}"
                                        )
                                    }
            
            return {
                'fields': selected_fields,
                'settings': value_settings
            }
        
        except Exception as e:
            self.ui_components.show_error(e, "field selection")
            return None
    
    def render_filter_panel(self, df: pd.DataFrame) -> dict:
        try:
            st.subheader("Filters")
            filters = {}
            
            with st.expander("Configure Filters"):
                for col in df.columns:
                    if st.checkbox(f"Filter {col}", key=f"filter_{col}"):
                        with st.container():
                            if df[col].dtype in ['int64', 'float64']:
                                min_val, max_val = float(df[col].min()), float(df[col].max())
                                filters[col] = {
                                    'type': 'range',
                                    'range': st.slider(
                                        f"Range for {col}",
                                        min_value=min_val,
                                        max_value=max_val,
                                        value=(min_val, max_val),
                                        key=f"range_{col}"
                                    )
                                }
                            elif df[col].dtype == 'datetime64[ns]':
                                min_date = df[col].min()
                                max_date = df[col].max()
                                filters[col] = {
                                    'type': 'date_range',
                                    'range': st.date_input(
                                        f"Date range for {col}",
                                        value=(min_date, max_date),
                                        key=f"date_{col}"
                                    )
                                }
                            else:
                                unique_values = df[col].unique()
                                filters[col] = {
                                    'type': 'categorical',
                                    'values': st.multiselect(
                                        f"Select values for {col}",
                                        options=unique_values,
                                        default=unique_values,
                                        key=f"cat_{col}"
                                    )
                                }
            
            return filters
        
        except Exception as e:
            self.ui_components.show_error(e, "filter configuration")
            return None
    
    def render_group_panel(self, df: pd.DataFrame) -> dict:
        try:
            st.subheader("Grouping")
            groups = {}
            
            with st.expander("Configure Groups"):
                for col in df.columns:
                    if st.checkbox(f"Group {col}", key=f"group_{col}"):
                        with st.container():
                            group_type = st.selectbox(
                                f"Grouping type for {col}",
                                options=['numeric_bins', 'date', 'custom'],
                                key=f"group_type_{col}"
                            )
                            
                            if group_type == 'numeric_bins':
                                groups[col] = {
                                    'type': 'numeric_bins',
                                    'bins': st.slider(
                                        f"Number of bins for {col}",
                                        min_value=2,
                                        max_value=10,
                                        value=5,
                                        key=f"bins_{col}"
                                    ),
                                    'labels': st.text_input(
                                        "Bin labels (comma-separated)",
                                        key=f"labels_{col}"
                                    ).split(',') if st.checkbox(
                                        "Custom labels",
                                        key=f"custom_labels_{col}"
                                    ) else None
                                }
                            elif group_type == 'date':
                                groups[col] = {
                                    'type': 'date',
                                    'freq': st.selectbox(
                                        f"Date grouping frequency for {col}",
                                        options=['D', 'W', 'M', 'Q', 'Y'],
                                        key=f"freq_{col}"
                                    ),
                                    'format': st.text_input(
                                        "Date format",
                                        value="%Y-%m-%d",
                                        key=f"format_{col}"
                                    )
                                }
                            else:  # custom
                                groups[col] = {
                                    'type': 'custom',
                                    'mapping': self._parse_mapping(
                                        st.text_area(
                                            f"Mapping for {col} (one per line, format: value=group)",
                                            key=f"mapping_{col}"
                                        )
                                    )
                                }
            
            return groups
        
        except Exception as e:
            self.ui_components.show_error(e, "group configuration")
            return None
    
    def _parse_mapping(self, mapping_text: str) -> dict:
        mapping = {}
        for line in mapping_text.split('\n'):
            if '=' in line:
                value, group = line.split('=')
                mapping[value.strip()] = group.strip()
        return mapping
    
    def render_advanced_options(self) -> dict:
        try:
            st.subheader("Advanced Options")
            options = {}
            
            with st.expander("Configure Advanced Options"):
                # Display options
                st.markdown("##### Display Options")
                options['display'] = {
                    'show_totals': st.checkbox("Show totals", value=True),
                    'show_subtotals': st.checkbox("Show subtotals", value=True),
                    'expand_all': st.checkbox("Expand all levels", value=False),
                    'compact_layout': st.checkbox("Compact layout", value=False)
                }
                
                # Calculation options
                st.markdown("##### Calculation Options")
                options['calculations'] = {
                    'running_total': st.checkbox("Show running total"),
                    'percentage_of_total': st.checkbox("Show percentage of total"),
                    'difference_from_previous': st.checkbox("Show difference from previous"),
                    'moving_average': st.checkbox("Show moving average")
                }
                
                if options['calculations']['moving_average']:
                    options['calculations']['moving_average_window'] = st.number_input(
                        "Moving average window size",
                        min_value=2,
                        value=3
                    )
                
                # Export options
                st.markdown("##### Export Options")
                options['export'] = {
                    'include_charts': st.checkbox("Include charts", value=True),
                    'include_filters': st.checkbox("Include filter details", value=True),
                    'file_format': st.selectbox(
                        "Export format",
                        options=['xlsx', 'csv', 'pdf']
                    )
                }
            
            return options
        
        except Exception as e:
            self.ui_components.show_error(e, "advanced options configuration")
            return None
    
    def render_preview_panel(self, pivot_result: dict):
        try:
            st.subheader("Pivot Table Preview")
            
            # Show summary metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                self.ui_components.show_metric_card(
                    "Total Records",
                    pivot_result['filtered_records'],
                    help_text="Number of records after applying filters"
                )
            with col2:
                self.ui_components.show_metric_card(
                    "Groups",
                    len(pivot_result['subtotals']),
                    help_text="Number of grouping levels"
                )
            with col3:
                memory_usage = pivot_result['pivot'].memory_usage().sum() / 1024 / 1024
                self.ui_components.show_metric_card(
                    "Memory Usage",
                    f"{memory_usage:.2f} MB",
                    help_text="Memory used by pivot table"
                )
            
            # Show pivot table with interactive features
            st.data_editor(
                pivot_result['pivot'],
                use_container_width=True,
                hide_index=False,
                column_config={
                    "_index": "Index",
                    **{col: st.column_config.NumberColumn(
                        col,
                        format="%.2f"
                    ) for col in pivot_result['pivot'].columns}
                }
            )
            
            # Show visualization options
            st.markdown("##### Visualization")
            viz_type = st.selectbox(
                "Chart Type",
                options=['Bar Chart', 'Line Chart', 'Heatmap', 'Scatter Plot']
            )
            
            if viz_type == 'Bar Chart':
                fig = px.bar(
                    pivot_result['pivot'].reset_index(),
                    x=pivot_result['pivot'].index.name,
                    y=pivot_        elif analysis_type == "Advanced Pivot Table":
            st.subheader("Advanced Pivot Table Analysis")
            
            # Initialize pivot analyzer if not exists
            if 'pivot_analyzer' not in st.session_state:
                st.session_state.pivot_analyzer = AdvancedPivotAnalyzer()
            
            # Create tabs for different pivot table configurations
            tab1, tab2, tab3, tab4 = st.tabs(["Fields", "Filters", "Groups", "Advanced"])
            
            with tab1:
                st.subheader("Select Fields")
                col1, col2 = st.columns(2)
                
                with col1:
                    # Row fields
                    row_fields = st.multiselect(
                        "Row Fields",
                        options=st.session_state.merged_data.columns,
                        help="Select fields to show as rows"
                    )
                    
                    # Value fields with aggregation
                    value_fields = {}
                    value_field_options = st.session_state.merged_data.select_dtypes(
                        include=[np.number]
                    ).columns
                    selected_values = st.multiselect(
                        "Value Fields",
                        options=value_field_options,
                        help="Select numeric fields to analyze"
                    )
                    
                    for field in selected_values:
                        agg_func = st.selectbox(
                            f"Aggregation for {field}",
                            options=['sum', 'mean', 'count', 'min', 'max', 'median'],
                            key=f"agg_{field}"
                        )
                        value_fields[field] = agg_func
                
                with col2:
                    # Column fields
                    column_fields = st.multiselect(
                        "Column Fields",
                        options=st.session_state.merged_data.columns,
                        help="Select fields to show as columns"
                    )
                    
                    # Sort options
                    sort_by = st.selectbox(
                        "Sort By",
                        options=['None'] + selected_values
                    )
                    if sort_by != 'None':
                        sort_ascending = st.checkbox("Sort Ascending", value=True)
            
            with tab2:
                st.subheader("Filters")
                filters = {}
                
                # Add filters for each column
                for col in st.session_state.merged_data.columns:
                    if st.checkbox(f"Filter {col}"):
                        if st.session_state.merged_data[col].dtype in ['int64', 'float64']:
                            # Numeric filter
                            min_val = st.session_state.merged_data[col].min()
                            max_val = st.session_state.merged_data[col].max()
                            filter_range = st.slider(
                                f"Range for {col}",
                                min_value=float(min_val),
                                max_value=float(max_val),
                                value=(float(min_val), float(max_val))
                            )
                            filters[col] = {'range': filter_range}
                        else:
                            # Categorical filter
                            unique_values = st.session_state.merged_data[col].unique()
                            selected_values = st.multiselect(
                                f"Select values for {col}",
                                options=unique_values,
                                default=unique_values
                            )
                            filters[col] = {'values': selected_values}
            
            with tab3:
                st.subheader("Groups")
                groups = {}
                
                for col in st.session_state.merged_data.columns:
                    if st.checkbox(f"Create group for {col}"):
                        group_type = st.selectbox(
                            f"Grouping type for {col}",
                            options=['numeric_bins', 'date', 'custom'],
                            key=f"group_type_{col}"
                        )
                        
                        if group_type == 'numeric_bins':
                            num_bins = st.slider(
                                f"Number of bins for {col}",
                                min_value=2,
                                max_value=10,
                                value=5
                            )
                            groups[col] = {
                                'type': 'numeric_bins',
                                'bins': num_bins
                            }
                        elif group_type == 'date':
                            freq = st.selectbox(
                                f"Date grouping frequency for {col}",
                                options=['D', 'W', 'M', 'Q', 'Y']
                            )
                            groups[col] = {
                                'type': 'date',
                                'freq': freq
                            }
                        else:  # custom
                            st.text("Enter custom mapping (one per line, format: value=group)")
                            mapping_text = st.text_area(
                                f"Mapping for {col}",
                                height=100,
                                key=f"mapping_{col}"
                            )
                            if mapping_text:
                                mapping = {}
                                for line in mapping_text.split('\n'):
                                    if '=' in line:
                                        value, group = line.split('=')
                                        mapping[value.strip()] = group.strip()
                                groups[col] = {
                                    'type': 'custom',
                                    'mapping': mapping
                                }
            
            with tab4:
                st.subheader("Advanced Options")
                
                # Calculated fields
                st.write("Calculated Fields")
                add_calc_field = st.checkbox("Add calculated field")
                calculated_fields = {}
                
                if add_calc_field:
                    calc_field_name = st.text_input("Field name")
                    calc_type = st.selectbox(
                        "Calculation type",
                        options=['simple_math', 'rolling', 'percentage']
                    )
                    
                    if calc_type == 'simple_math':
                        expression = st.text_input(
                            "Enter expression (e.g., column1 + column2)"
                        )
                        if calc_field_name and expression:
                            calculated_fields[calc_field_name] = {
                                'type': 'simple_math',
                                'expression': expression
                            }
                    elif calc_type == 'rolling':
                        col = st.selectbox("Select column", options=value_field_options)
                        window = st.number_input("Window size", min_value=2, value=3)
                        func = st.selectbox(
                            "Function",
                            options=['mean', 'sum', 'min', 'max']
                        )
                        if calc_field_name:
                            calculated_fields[calc_field_name] = {
                                'type': 'rolling',
                                'column': col,
                                'window': window,
                                'function': func
                            }
                    else:  # percentage
                        col = st.selectbox("Select column", options=value_field_options)
                        if calc_field_name:
                            calculated_fields[calc_field_name] = {
                                'type': 'percentage',
                                'column': col
                            }
                
                # Other advanced options
                show_totals = st.checkbox("Show totals", value=True)
                enable_drill_down = st.checkbox("Enable drill-down", value=True)
                
                if enable_drill_down:
                    drill_down_columns = st.multiselect(
                        "Select columns for drill-down view",
                        options=st.session_state.merged_data.columns
                    )
            
            # Create pivot table
            if st.button("Generate Pivot Table"):
                if row_fields and value_fields:
                    try:
                        pivot_config = {
                            'rows': row_fields,
                            'columns': column_fields,
                            'values': list(value_fields.keys()),
                            'aggregations': value_fields,
                            'filters': filters,
                            'groups': groups,
                            'calculated_fields': calculated_fields,
                            'show_totals': show_totals,
                            'drill_down_columns': drill_down_columns if enable_drill_down else None
                        }
                        
                        if sort_by != 'None':
                            pivot_config['sort'] = {
                                'column': sort_by,
                                'ascending': sort_ascending
                            }
                        
                        result = st.session_state.pivot_analyzer.create_pivot(
                            st.session_state.merged_data,
                            pivot_config
                        )
                        
                        st.write(f"Showing {result['filtered_records']} records after filtering")
                        
                        # Display pivot table
                        st.write("Pivot Table:")
                        st.write(result['pivot'])
                        
                        # Show subtotals if available
                        if result['subtotals']:
                            st.write("Subtotals:")
                            for level, subtotal in result['subtotals'].items():
                                with st.expander(f"Level {level + 1} Subtotals"):
                                    st.write(subtotal)
                        
                        # Enable drill-down if configured
                        if enable_drill_down:
                            st.write("Click on a value in the pivot table to see detailed records")
                            selected_value = st.selectbox(
                                "Select a value to drill down",
                                options=result['pivot'].index
                            )
                            
                            if selected_value:
                                drill_down_data = st.session_state.pivot_analyzer.get_drill_down(
                                    st.session_state.merged_data,
                                    pivot_config,
                                    {row_fields[0]: [selected_value]}
                                )
                                st.write("Drill-down view:")
                                st.write(drill_down_data)
                        
                        # Add download button
                        buffer = BytesIO()
                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            result['pivot'].to_excel(writer, sheet_name='Pivot')
                            if result['subtotals']:
                                pd.concat(
                                    result['subtotals'].values()
                                ).to_excel(writer, sheet_name='Subtotals')
                        
                        buffer.seek(0)
                        st.download_button(
                            label="Download Pivot Analysis",
                            data=buffer,
                            file_name=f        elif analysis_type == "Machine Learning":
            st.subheader("Machine Learning Analysis")
            
            ml_type = st.selectbox(
                "Select Machine Learning Task",
                ["Clustering", "Anomaly Detection", "Predictive Modeling"]
            )
            
            if ml_type == "Clustering":
                # Select features for clustering
                features = st.multiselect(
                    "Select Features for Clustering",
                    options=st.session_state.merged_data.select_dtypes(
                        include=[np.number]).columns
                )
                
                n_clusters = st.slider(
                    "Number of Clusters",
                    min_value=2,
                    max_value=10,
                    value=3
                )
                
                if features and st.button("Perform Clustering"):
                    ml_analyzer = MLAnalyzer()
                    result = ml_analyzer.perform_clustering(
                        st.session_state.merged_data[features],
                        n_clusters=n_clusters
                    )
                    
                    # Add cluster labels to data
                    clustered_data = st.session_state.merged_data.copy()
                    clustered_data['Cluster'] = result['clusters']
                    
                    # Show clustering results
                    st.write("Clustering Results:")
                    st.write(f"Silhouette Score: {result['silhouette_score']:.3f}")
                    
                    # Visualize clusters
                    if len(features) >= 2:
                        fig = px.scatter(
                            clustered_data,
                            x=features[0],
                            y=features[1],
                            color='Cluster',
                            title="Cluster Visualization"
                        )
                        st.plotly_chart(fig)
                    
                    # Show cluster statistics
                    st.write("Cluster Statistics:")
                    cluster_stats = clustered_data.groupby('Cluster')[features].mean()
                    st.write(cluster_stats)
            
            elif ml_type == "Anomaly Detection":
                features = st.multiselect(
                    "Select Features for Anomaly Detection",
                    options=st.session_state.merged_data.select_dtypes(
                        include=[np.number]).columns
                )
                
                contamination = st.slider(
                    "Contamination Factor",
                    min_value=0.01,
                    max_value=0.5,
                    value=0.1,
                    help="Expected proportion of outliers in the data"
                )
                
                if features and st.button("Detect Anomalies"):
                    ml_analyzer = MLAnalyzer()
                    result = ml_analyzer.detect_anomalies(
                        st.session_state.merged_data[features],
                        contamination=contamination
                    )
                    
                    # Add anomaly labels to data
                    anomaly_data = st.session_state.merged_data.copy()
                    anomaly_data['Is_Anomaly'] = result['anomalies'] == -1
                    anomaly_data['Anomaly_Score'] = result['scores']
                    
                    # Show results
                    st.write(f"Found {result['anomaly_samples']} anomalies "
                            f"({result['anomaly_samples']/len(anomaly_data)*100:.1f}% of data)")
                    
                    # Visualize anomalies
                    if len(features) >= 2:
                        fig = px.scatter(
                            anomaly_data,
                            x=features[0],
                            y=features[1],
                            color='Is_Anomaly',
                            title="Anomaly Detection Visualization"
                        )
                        st.plotly_chart(fig)
                    
                    # Show anomaly details
                    st.write("Top Anomalies:")
                    st.write(anomaly_data[anomaly_data['Is_Anomaly']].sort_values(
                        'Anomaly_Score'
                    ))import streamlit as st
import pandas as pd
import duckdb
import plotly.express as px
from typing import List, Optional
from pydantic import BaseModel, Field
from io import BytesIO
import numpy as np
from datetime import datetime
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest, RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (mean_squared_error, r2_score, accuracy_score, 
                           classification_report, confusion_matrix)
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.lib.styles import getSampleStyleSheet
import json
from datetime import datetime, timedelta
import schedule
import time
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

class DatasetConfig(BaseModel):
    name: str
    columns: List[str]
    date_columns: List[str] = Field(default_factory=list)
    numeric_columns: List[str] = Field(default_factory=list)
    categorical_columns: List[str] = Field(default_factory=list)

class DataCleaner:
    @staticmethod
    def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        return df.drop_duplicates()
    
    @staticmethod
    def handle_missing_values(df: pd.DataFrame, numeric_fill: str = 'mean', 
                            categorical_fill: str = 'mode') -> pd.DataFrame:
        df = df.copy()
        
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                if numeric_fill == 'mean':
                    df[col].fillna(df[col].mean(), inplace=True)
                elif numeric_fill == 'median':
                    df[col].fillna(df[col].median(), inplace=True)
                elif numeric_fill == 'zero':
                    df[col].fillna(0, inplace=True)
            else:
                if categorical_fill == 'mode':
                    df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', 
                                 inplace=True)
                elif categorical_fill == 'unknown':
                    df[col].fillna('Unknown', inplace=True)
        
        return df
    
    @staticmethod
    def standardize_dates(df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:
        df = df.copy()
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        return df

class PivotAnalyzer:
    @staticmethod
    def create_pivot_table(df: pd.DataFrame, 
                          index_cols: List[str],
                          value_cols: List[str],
                          agg_func: str = 'sum',
                          pivot_col: Optional[str] = None) -> pd.DataFrame:
        if pivot_col:
            return pd.pivot_table(
                df,
                values=value_cols,
                index=index_cols,
                columns=[pivot_col],
                aggfunc=agg_func,
                fill_value=0
            )
        else:
            return pd.pivot_table(
                df,
                values=value_cols,
                index=index_cols,
                aggfunc=agg_func,
                fill_value=0
            )
    
    @staticmethod
    def get_available_agg_functions() -> List[str]:
        return ['sum', 'mean', 'count', 'min', 'max', 'median', 'std']

class TimeSeriesAnalyzer:
    @staticmethod
    def decompose_series(data: pd.Series) -> dict:
        decomposition = sm.tsa.seasonal_decompose(data, period=12)
        return {
            'trend': decomposition.trend,
            'seasonal': decomposition.seasonal,
            'residual': decomposition.resid
        }
    
    @staticmethod
    def forecast_prophet(df: pd.DataFrame, date_col: str, value_col: str, 
                        periods: int = 12) -> pd.DataFrame:
        model = Prophet(yearly_seasonality=True, weekly_seasonality=True)
        prophet_df = df[[date_col, value_col]].copy()
        prophet_df.columns = ['ds', 'y']
        model.fit(prophet_df)
        future = model.make_future_dataframe(periods=periods)
        forecast = model.predict(future)
        return forecast

class StatisticalAnalyzer:
    @staticmethod
    def run_hypothesis_test(data1: pd.Series, data2: pd.Series, 
                          test_type: str) -> dict:
        if test_type == 't_test':
            stat, pvalue = stats.ttest_ind(data1, data2)
            return {'statistic': stat, 'p_value': pvalue, 'test': 't_test'}
        elif test_type == 'chi_square':
            stat, pvalue = stats.chi2_contingency(pd.crosstab(data1, data2))[:2]
            return {'statistic': stat, 'p_value': pvalue, 'test': 'chi_square'}
        elif test_type == 'anova':
            stat, pvalue = stats.f_oneway(data1, data2)
            return {'statistic': stat, 'p_value': pvalue, 'test': 'anova'}
    
    @staticmethod
    def calculate_confidence_interval(data: pd.Series, 
                                   confidence: float = 0.95) -> dict:
        mean = data.mean()
        ci = stats.t.interval(confidence, len(data)-1, loc=mean, 
                            scale=stats.sem(data))
        return {'mean': mean, 'ci_lower': ci[0], 'ci_upper': ci[1]}

class MLAnalyzer:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
    
    def perform_clustering(self, df: pd.DataFrame, n_clusters: int = 3) -> dict:
        scaled_data = self.scaler.fit_transform(df)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(scaled_data)
        
        # Calculate silhouette score
        from sklearn.metrics import silhouette_score
        silhouette_avg = silhouette_score(scaled_data, clusters)
        
        return {
            'clusters': clusters,
            'centroids': kmeans.cluster_centers_,
            'inertia': kmeans.inertia_,
            'silhouette_score': silhouette_avg
        }
    
    def detect_anomalies(self, df: pd.DataFrame, contamination: float = 0.1) -> dict:
        scaled_data = self.scaler.fit_transform(df)
        iso_forest = IsolationForest(contamination=contamination, random_state=42)
        anomalies = iso_forest.fit_predict(scaled_data)
        
        anomaly_scores = iso_forest.score_samples(scaled_data)
        
        return {
            'anomalies': anomalies,
            'scores': anomaly_scores,
            'normal_samples': (anomalies == 1).sum(),
            'anomaly_samples': (anomalies == -1).sum()
        }
    
    def train_predictor(self, X: pd.DataFrame, y: pd.Series, 
                       problem_type: str = 'regression') -> dict:
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        if problem_type == 'regression':
            self.model = RandomForestRegressor(n_estimators=100, random_state=42)
            self.model.fit(X_train_scaled, y_train)
            
            # Make predictions
            y_pred = self.model.predict(X_test_scaled)
            
            # Calculate metrics
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            return {
                'mse': mse,
                'r2': r2,
                'feature_importance': dict(zip(X.columns, self.model.feature_importances_))
            }
        else:
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)
            y_encoded = self.label_encoder.fit_transform(y)
            y_train_encoded = self.label_encoder.transform(y_train)
            y_test_encoded = self.label_encoder.transform(y_test)
            
            self.model.fit(X_train_scaled, y_train_encoded)
            y_pred = self.model.predict(X_test_scaled)
            
            return {
                'accuracy': accuracy_score(y_test_encoded, y_pred),
                'classification_report': classification_report(y_test_encoded, y_pred),
                'feature_importance': dict(zip(X.columns, self.model.feature_importances_))
            }
    
    def analyze_text(self, text_series: pd.Series) -> dict:
        # Initialize sentiment analyzer
        sia = SentimentIntensityAnalyzer()
        
        # Get sentiment scores
        sentiments = text_series.apply(lambda x: sia.polarity_scores(str(x)))
        
        # Extract key metrics
        sentiment_df = pd.DataFrame(sentiments.tolist())
        
        # Get most common words
        vectorizer = TfidfVectorizer(max_features=10, stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(text_series.astype(str))
        
        return {
            'average_sentiment': sentiment_df.mean().to_dict(),
            'sentiment_distribution': sentiment_df.compound.value_counts().to_dict(),
            'top_terms': dict(zip(vectorizer.get_feature_names_out(), 
                                tfidf_matrix.sum(axis=0).A1))
        }

class ReportGenerator:
    @staticmethod
    def create_pdf_report(data: dict, filename: str):
        doc = SimpleDocTemplate(filename, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []
        
        # Add title
        elements.append(Paragraph(f"Analysis Report - {datetime.now().strftime('%Y-%m-%d')}", 
                                styles['Title']))
        
        # Add sections based on data
        for section, content in data.items():
            elements.append(Paragraph(section, styles['Heading1']))
            
            if isinstance(content, pd.DataFrame):
                # Convert DataFrame to table
                table_data = [content.columns.tolist()] + content.values.tolist()
                t = Table(table_data)
                t.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 14),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('TEXTCOLOR', (0, 1), (-1, -1), colors.black),
                    ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
                    ('FONTSIZE', (0, 1), (-1, -1), 12),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                elements.append(t)
            else:
                elements.append(Paragraph(str(content), styles['Normal']))
        
        doc.build(elements)

class AnalysisTemplate(BaseModel):
    name: str
    description: str
    analysis_steps: List[dict] = Field(default_factory=list)
    visualization_config: dict = Field(default_factory=dict)
    scheduling: Optional[dict] = None

class CollaborationManager:
    def __init__(self):
        self.shared_analyses = {}
        self.comments = {}
        self.user_permissions = {}
    
    def share_analysis(self, analysis_id: str, users: List[str], 
                      permissions: List[str]):
        self.shared_analyses[analysis_id] = {
            'shared_with': users,
            'permissions': permissions,
            'shared_at': datetime.now()
        }
    
    def add_comment(self, analysis_id: str, user: str, comment: str):
        if analysis_id not in self.comments:
            self.comments[analysis_id] = []
        
        self.comments[analysis_id].append({
            'user': user,
            'comment': comment,
            'timestamp': datetime.now()
        })
    
    def get_analysis_history(self, analysis_id: str) -> List[dict]:
        return self.comments.get(analysis_id, [])

class InsightGenerator:
    @staticmethod
    def generate_statistical_insights(df: pd.DataFrame) -> List[str]:
        insights = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            stats_dict = {
                'mean': df[col].mean(),
                'median': df[col].median(),
                'std': df[col].std(),
                'skew': df[col].skew()
            }
            
            insights.append(f"Column {col}:")
            insights.append(f"- Average value: {stats_dict['mean']:.2f}")
            insights.append(f"- Median value: {stats_dict['median']:.2f}")
            
            if abs(stats_dict['skew']) > 1:
                insights.append(
                    f"- Distribution is {'positively' if stats_dict['skew'] > 0 else 'negatively'} skewed"
                )
            
            # Detect outliers using IQR method
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[col][(df[col] < (Q1 - 1.5 * IQR)) | 
                              (df[col] > (Q3 + 1.5 * IQR))]
            if len(outliers) > 0:
                insights.append(
                    f"- Found {len(outliers)} potential outliers ({(len(outliers)/len(df)*100):.1f}% of data)"
                )
        
        return insights
    
    @staticmethod
    def generate_trend_insights(df: pd.DataFrame, 
                              date_col: str, 
                              value_col: str) -> List[str]:
        insights = []
        df = df.sort_values(date_col)
        
        # Overall trend
        start_value = df[value_col].iloc[0]
        end_value = df[value_col].iloc[-1]
        change_pct = ((end_value - start_value) / start_value) * 100
        
        insights.append(f"Overall trend analysis for {value_col}:")
        insights.append(
            f"- {'Increased' if change_pct > 0 else 'Decreased'} by {abs(change_pct):.1f}% "
            f"from {start_value:.2f} to {end_value:.2f}"
        )
        
        # Seasonality check using autocorrelation
        autocorr = pd.Series(df[value_col]).autocorr()
        if abs(autocorr) > 0.7:
            insights.append("- Strong seasonal pattern detected")
        elif abs(autocorr) > 0.3:
            insights.append("- Moderate seasonal pattern detected")
        
        return insights

class AdvancedPivotAnalyzer:
    def __init__(self):
        self.filters = {}
        self.groups = {}
        self.calculated_fields = {}
    
    def apply_filters(self, df: pd.DataFrame, filters: dict) -> pd.DataFrame:
        filtered_df = df.copy()
        for col, filter_values in filters.items():
            if filter_values.get('values'):
                filtered_df = filtered_df[filtered_df[col].isin(filter_values['values'])]
            if filter_values.get('range'):
                min_val, max_val = filter_values['range']
                filtered_df = filtered_df[
                    (filtered_df[col] >= min_val) & (filtered_df[col] <= max_val)
                ]
        return filtered_df
    
    def create_groups(self, df: pd.DataFrame, group_config: dict) -> pd.DataFrame:
        df = df.copy()
        for col, config in group_config.items():
            if config['type'] == 'numeric_bins':
                df[f"{col}_group"] = pd.qcut(
                    df[col], 
                    q=config['bins'], 
                    labels=config.get('labels')
                )
            elif config['type'] == 'date':
                df[f"{col}_group"] = df[col].dt.to_period(config['freq'])
            elif config['type'] == 'custom':
                mapping = config['mapping']
                df[f"{col}_group"] = df[col].map(mapping)
        return df
    
    def calculate_field(self, df: pd.DataFrame, calc_config: dict) -> pd.Series:
        if calc_config['type'] == 'simple_math':
            expression = calc_config['expression']
            for col in df.columns:
                expression = expression.replace(col, f"df['{col}']")
            return eval(expression)
        elif calc_config['type'] == 'rolling':
            return df[calc_config['column']].rolling(
                window=calc_config['window']
            ).agg(calc_config['function'])
        elif calc_config['type'] == 'percentage':
            return (df[calc_config['column']] / 
                   df[calc_config['column']].sum() * 100)
        return pd.Series()
    
    def create_pivot(self, df: pd.DataFrame, config: dict) -> dict:
        # Apply filters
        if config.get('filters'):
            df = self.apply_filters(df, config['filters'])
        
        # Create groups
        if config.get('groups'):
            df = self.create_groups(df, config['groups'])
        
        # Calculate custom fields
        for field_name, calc_config in config.get('calculated_fields', {}).items():
            df[field_name] = self.calculate_field(df, calc_config)
        
        # Create pivot table
        pivot_df = pd.pivot_table(
            df,
            values=config['values'],
            index=config['rows'],
            columns=config.get('columns'),
            aggfunc=config['aggregations'],
            fill_value=0,
            margins=config.get('show_totals', False)
        )
        
        # Sort if specified
        if config.get('sort'):
            sort_col = config['sort']['column']
            ascending = config['sort'].get('ascending', True)
            pivot_df = pivot_df.sort_values(sort_col, ascending=ascending)
        
        # Calculate subtotals if specified
        subtotals = {}
        if config.get('subtotals'):
            for level in range(len(config['rows'])):
                subtotal = df.groupby(
                    config['rows'][:level+1]
                )[config['values']].agg(config['aggregations'])
                subtotals[level] = subtotal
        
        return {
            'pivot': pivot_df,
            'subtotals': subtotals,
            'filtered_records': len(df)
        }

    def get_drill_down(self, df: pd.DataFrame, config: dict, 
                      filters: dict) -> pd.DataFrame:
        # Apply filters from pivot selection
        filtered_df = self.apply_filters(df, filters)
        
        # Select columns for drill-down
        if config.get('drill_down_columns'):
            return filtered_df[config['drill_down_columns']]
        return filtered_df

class DuckDBAnalyzer:
    def __init__(self):
        self.con = duckdb.connect(database=':memory:', read_only=False)
    
    def load_dataframe(self, df: pd.DataFrame, table_name: str):
        self.con.register(table_name, df)
    
    def run_query(self, query: str) -> pd.DataFrame:
        return self.con.execute(query).df()
    
    def get_summary_stats(self, table_name: str, numeric_columns: List[str]) -> pd.DataFrame:
        if not numeric_columns:
            return pd.DataFrame()
        
        stats_query = f"""
        SELECT 
            {', '.join([f'MIN({col}) as {col}_min, 
                         MAX({col}) as {col}_max, 
                         AVG({col}) as {col}_avg, 
                         STDDEV({col}) as {col}_std' 
                       for col in numeric_columns])}
        FROM {table_name}
        """
        return self.run_query(stats_query)
    
    def get_correlation_matrix(self, table_name: str, numeric_columns: List[str]) -> pd.DataFrame:
        if len(numeric_columns) < 2:
            return pd.DataFrame()
        
        correlations = []
        for col1 in numeric_columns:
            for col2 in numeric_columns:
                query = f"""
                SELECT CORR({col1}, {col2}) as correlation
                FROM {table_name}
                """
                corr = self.run_query(query).iloc[0,0]
                correlations.append({'column1': col1, 'column2': col2, 'correlation': corr})
        
        return pd.DataFrame(correlations)

def main():
    st.set_page_config(page_title="Advanced Data Analysis Tool", layout="wide")
    st.title("Advanced Data Analysis Tool")
    
    # Initialize session state
    if 'datasets' not in st.session_state:
        st.session_state.datasets = {}
        st.session_state.merged_data = None
        st.session_state.analyzer = DuckDBAnalyzer()
    
    # File Upload Section
    st.header("1. Data Upload")
    uploaded_files = st.file_uploader("Upload Excel Files", type=['xlsx', 'xls'], 
                                    accept_multiple_files=True)
    
    if uploaded_files:
        for file in uploaded_files:
            if file.name not in st.session_state.datasets:
                df = pd.read_excel(file)
                config = DatasetConfig(
                    name=file.name,
                    columns=df.columns.tolist(),
                    numeric_columns=df.select_dtypes(include=[np.number]).columns.tolist(),
                    categorical_columns=df.select_dtypes(include=['object']).columns.tolist()
                )
                st.session_state.datasets[file.name] = {'df': df, 'config': config}
    
    # Data Cleaning Section
    if st.session_state.datasets:
        st.header("2. Data Cleaning")
        
        selected_dataset = st.selectbox("Select Dataset to Clean", 
                                      options=list(st.session_state.datasets.keys()))
        
        if selected_dataset:
            df = st.session_state.datasets[selected_dataset]['df']
            config = st.session_state.datasets[selected_dataset]['config']
            
            st.subheader("Cleaning Options")
            col1, col2 = st.columns(2)
            
            with col1:
                remove_duplicates = st.checkbox("Remove Duplicate Rows", value=True)
                numeric_fill = st.selectbox("Handle Missing Numeric Values", 
                                          ['mean', 'median', 'zero'])
            
            with col2:
                categorical_fill = st.selectbox("Handle Missing Categorical Values", 
                                              ['mode', 'unknown'])
                date_columns = st.multiselect("Select Date Columns", 
                                            options=config.columns)
            
            if st.button("Apply Cleaning"):
                # Apply cleaning operations
                if remove_duplicates:
                    df = DataCleaner.remove_duplicates(df)
                
                df = DataCleaner.handle_missing_values(df, numeric_fill, categorical_fill)
                df = DataCleaner.standardize_dates(df, date_columns)
                
                # Update the dataset
                config.date_columns = date_columns
                st.session_state.datasets[selected_dataset]['df'] = df
                st.session_state.datasets[selected_dataset]['config'] = config
                
                st.success("Data cleaning completed!")
    
    # Data Merging Section
    if len(st.session_state.datasets) > 1:
        st.header("3. Data Merging")
        
        merge_cols = {}
        datasets_to_merge = st.multiselect("Select Datasets to Merge", 
                                         options=list(st.session_state.datasets.keys()))
        
        if len(datasets_to_merge) >= 2:
            for dataset in datasets_to_merge:
                merge_cols[dataset] = st.selectbox(
                    f"Select Merge Column for {dataset}",
                    options=st.session_state.datasets[dataset]['config'].columns
                )
            
            if st.button("Merge Datasets"):
                merged_df = st.session_state.datasets[datasets_to_merge[0]]['df']
                
                for dataset in datasets_to_merge[1:]:
                    merged_df = merged_df.merge(
                        st.session_state.datasets[dataset]['df'],
                        left_on=merge_cols[datasets_to_merge[0]],
                        right_on=merge_cols[dataset],
                        how='inner'
                    )
                
                st.session_state.merged_data = merged_df
                st.session_state.analyzer.load_dataframe(merged_df, 'merged_data')
                st.success("Datasets merged successfully!")
    
    # Analysis Section
    if st.session_state.merged_data is not None:
        st.header("4. Data Analysis")
        
        analysis_type = st.selectbox(
            "Select Analysis Type",
            ["Summary Statistics", "Correlation Analysis", "Custom Query", "Visualization", 
             "Basic Pivot Table", "Advanced Pivot Table", "Machine Learning",
             "Text Analysis", "Report Generation", "Templates", "Collaboration", "Dashboard"]
        )
        
        if analysis_type == "Summary Statistics":
            numeric_cols = st.session_state.merged_data.select_dtypes(
                include=[np.number]).columns.tolist()
            stats_df = st.session_state.analyzer.get_summary_stats('merged_data', numeric_cols)
            st.write(stats_df)
        
        elif analysis_type == "Correlation Analysis":
            numeric_cols = st.session_state.merged_data.select_dtypes(
                include=[np.number]).columns.tolist()
            corr_df = st.session_state.analyzer.get_correlation_matrix('merged_data', numeric_cols)
            
            # Create correlation heatmap
            if not corr_df.empty:
                corr_matrix = pd.pivot_table(
                    corr_df, 
                    values='correlation', 
                    index='column1', 
                    columns='column2'
                )
                fig = px.imshow(
                    corr_matrix,
                    labels=dict(color="Correlation"),
                    color_continuous_scale="RdBu"
                )
                st.plotly_chart(fig)
            
        elif analysis_type == "Basic Pivot Table":
            st.subheader("Basic Pivot Table Analysis")
            
            # Select columns for pivot table
            col1, col2 = st.columns(2)
            with col1:
                index_cols = st.multiselect(
                    "Select Row Headers (Index)",
                    options=st.session_state.merged_data.columns,
                    help="Select columns to group by rows"
                )
            with col2:
                value_cols = st.multiselect(
                    "Select Values to Aggregate",
                    options=st.session_state.merged_data.select_dtypes(include=[np.number]).columns,
                    help="Select numeric columns to analyze"
                )
            
            agg_func = st.selectbox(
                "Select Aggregation Function",
                options=PivotAnalyzer.get_available_agg_functions(),
                help="Choose how to aggregate the values"
            )
            
            if index_cols and value_cols:
                try:
                    pivot_df = PivotAnalyzer.create_pivot_table(
                        st.session_state.merged_data,
                        index_cols=index_cols,
                        value_cols=value_cols,
                        agg_func=agg_func
                    )
                    st.write(pivot_df)
                    
                    # Download button for pivot table
                    if st.button("Download Pivot Table"):
                        buffer = BytesIO()
                        pivot_df.to_excel(buffer, index=True)
                        buffer.seek(0)
                        st.download_button(
                            label="Download Excel file",
                            data=buffer,
                            file_name=f"pivot_table_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                except Exception as e:
                    st.error(f"Error creating pivot table: {str(e)}")
        
        elif analysis_type == "Advanced Pivot Table":
            st.subheader("Advanced Pivot Table Analysis")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                index_cols = st.multiselect(
                    "Select Row Headers (Index)",
                    options=st.session_state.merged_data.columns,
                    help="Select columns to group by rows"
                )
            with col2:
                value_cols = st.multiselect(
                    "Select Values to Aggregate",
                    options=st.session_state.merged_data.select_dtypes(include=[np.number]).columns,
                    help="Select numeric columns to analyze"
                )
            with col3:
                pivot_col = st.selectbox(
                    "Select Column Header (Optional)",
                    options=['None'] + list(st.session_state.merged_data.columns),
                    help="Select column to pivot (create column headers)"
                )
            
            # Advanced options
            st.subheader("Advanced Options")
            col1, col2 = st.columns(2)
            with col1:
                agg_func = st.selectbox(
                    "Select Aggregation Function",
                    options=PivotAnalyzer.get_available_agg_functions(),
                    help="Choose how to aggregate the values"
                )
                show_subtotals = st.checkbox("Show Subtotals", value=False)
            
            with col2:
                sort_values = st.selectbox(
                    "Sort Values By",
                    options=['None'] + value_cols if value_cols else ['None']
                )
                ascending = st.checkbox("Sort Ascending", value=True)
            
            if index_cols and value_cols:
                try:
                    pivot_df = PivotAnalyzer.create_pivot_table(
                        st.session_state.merged_data,
                        index_cols=index_cols,
                        value_cols=value_cols,
                        agg_func=agg_func,
                        pivot_col=None if pivot_col == 'None' else pivot_col
                    )
                    
                    # Apply sorting if selected
                    if sort_values != 'None':
                        pivot_df = pivot_df.sort_values(by=sort_values, ascending=ascending)
                    
                    # Add subtotals if selected
                    if show_subtotals and len(index_cols) > 1:
                        for i in range(len(index_cols)):
                            subtotal_df = pivot_df.groupby(index_cols[:i+1]).sum()
                            pivot_df = pd.concat([pivot_df, subtotal_df])
                        pivot_df = pivot_df.sort_index()
                    
                    st.write(pivot_df)
                    
                    # Visualization options
                    st.subheader("Pivot Table Visualization")
                    chart_type = st.selectbox(
                        "Select Chart Type",
                        options=["Bar Chart", "Line Chart", "Heatmap"]
                    )
                    
                    if chart_type == "Bar Chart":
                        fig = px.bar(pivot_df.reset_index(), x=index_cols[0], y=value_cols)
                    elif chart_type == "Line Chart":
                        fig = px.line(pivot_df.reset_index(), x=index_cols[0], y=value_cols)
                    else:  # Heatmap
                        fig = px.imshow(pivot_df,
                                      labels=dict(color="Value"),
                                      aspect="auto")
                    st.plotly_chart(fig)
                    
                    # Download options
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("Download Pivot Table"):
                            buffer = BytesIO()
                            pivot_df.to_excel(buffer, index=True)
                            buffer.seek(0)
                            st.download_button(
                                label="Download Excel file",
                                data=buffer,
                                file_name=f"advanced_pivot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                                mime="application/vnd.ms-excel"
                            )
                    with col2:
                        if st.button("Save to Dashboard"):
                            if 'dashboard_items' not in st.session_state:
                                st.session_state.dashboard_items = []
                            
                            dashboard_item = {
                                'type': 'pivot_table',
                                'config': {
                                    'index_cols': index_cols,
                                    'value_cols': value_cols,
                                    'pivot_col': pivot_col,
                                    'agg_func': agg_func,
                                    'chart_type': chart_type
                                },
                                'title': f"Pivot Analysis - {', '.join(value_cols)}"
                            }
                            st.session_state.dashboard_items.append(dashboard_item)
                            st.success("Added to dashboard!")
                            
                except Exception as e:
                    st.error(f"Error creating advanced pivot table: {str(e)}")
        
        elif analysis_type == "Dashboard":
            st.subheader("Dashboard")
            
            if 'dashboard_items' not in st.session_state:
                st.session_state.dashboard_items = []
            
            if not st.session_state.dashboard_items:
                st.info("Add items to your dashboard from the Analysis sections above!")
            else:
                # Dashboard layout
                for idx, item in enumerate(st.session_state.dashboard_items):
                    with st.expander(f"{item['title']}", expanded=True):
                        if item['type'] == 'pivot_table':
                            config = item['config']
                            pivot_df = PivotAnalyzer.create_pivot_table(
                                st.session_state.merged_data,
                                index_cols=config['index_cols'],
                                value_cols=config['value_cols'],
                                agg_func=config['agg_func'],
                                pivot_col=config['pivot_col']
                            )
                            
                            # Show table
                            st.write(pivot_df)
                            
                            # Show visualization
                            if config['chart_type'] == "Bar Chart":
                                fig = px.bar(pivot_df.reset_index(), 
                                           x=config['index_cols'][0], 
                                           y=config['value_cols'])
                            elif config['chart_type'] == "Line Chart":
                                fig = px.line(pivot_df.reset_index(), 
                                            x=config['index_cols'][0], 
                                            y=config['value_cols'])
                            else:  # Heatmap
                                fig = px.imshow(pivot_df,
                                              labels=dict(color="Value"),
                                              aspect="auto")
                            st.plotly_chart(fig)
                            
                        if st.button(f"Remove from Dashboard", key=f"remove_{idx}"):
                            st.session_state.dashboard_items.pop(idx)
                            st.experimental_rerun()
                
                # Dashboard export
                if st.button("Export Dashboard"):
                    buffer = BytesIO()
                    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                        for idx, item in enumerate(st.session_state.dashboard_items):
                            if item['type'] == 'pivot_table':
                                config = item['config']
                                pivot_df = PivotAnalyzer.create_pivot_table(
                                    st.session_state.merged_data,
                                    index_cols=config['index_cols'],
                                    value_cols=config['value_cols'],
                                    agg_func=config['agg_func'],
                                    pivot_col=config['pivot_col']
                                )
                                pivot_df.to_excel(writer, 
                                                sheet_name=f"Pivot_{idx+1}",
                                                index=True)
                    
                    buffer.seek(0)
                    st.download_button(
                        label="Download Dashboard",
                        data=buffer,
                        file_name=f"dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                                                    mime="application/vnd.ms-excel"
                    )
        
        elif analysis_type == "Time Series Analysis":
            st.subheader("Time Series Analysis")
            
            # Select columns for analysis
            date_col = st.selectbox(
                "Select Date Column",
                options=[col for col in st.session_state.merged_data.columns 
                        if st.session_state.merged_data[col].dtype in ['datetime64[ns]', 'object']]
            )
            value_col = st.selectbox(
                "Select Value Column",
                options=st.session_state.merged_data.select_dtypes(include=[np.number]).columns
            )
            
            analysis_type = st.selectbox(
                "Select Analysis Type",
                ["Decomposition", "Forecasting"]
            )
            
            if date_col and value_col:
                # Ensure date column is datetime
                df = st.session_state.merged_data.copy()
                df[date_col] = pd.to_datetime(df[date_col])
                df = df.sort_values(date_col)
                
                if analysis_type == "Decomposition":
                    try:
                        decomposition = TimeSeriesAnalyzer.decompose_series(
                            df.set_index(date_col)[value_col]
                        )
                        
                        # Plot components
                        fig = px.line(title="Time Series Decomposition")
                        fig.add_trace(px.line(y=decomposition['trend'], 
                                            title="Trend").data[0])
                        fig.add_trace(px.line(y=decomposition['seasonal'], 
                                            title="Seasonal").data[0])
                        fig.add_trace(px.line(y=decomposition['residual'], 
                                            title="Residual").data[0])
                        st.plotly_chart(fig)
                        
                    except Exception as e:
                        st.error(f"Error in decomposition: {str(e)}")
                
                else:  # Forecasting
                    forecast_periods = st.slider(
                        "Select number of periods to forecast",
                        min_value=1,
                        max_value=24,
                        value=12
                    )
                    
                    try:
                        forecast = TimeSeriesAnalyzer.forecast_prophet(
                            df,
                            date_col=date_col,
                            value_col=value_col,
                            periods=forecast_periods
                        )
                        
                        # Plot forecast
                        fig = px.line()
                        fig.add_trace(px.line(
                            x=df[date_col], 
                            y=df[value_col],
                            title="Historical Data"
                        ).data[0])
                        fig.add_trace(px.line(
                            x=forecast['ds'],
                            y=forecast['yhat'],
                            title="Forecast"
                        ).data[0])
                        fig.add_scatter(
                            x=forecast['ds'],
                            y=forecast['yhat_upper'],
                            fill=None,
                            mode='lines',
                            line_color='rgba(0,100,80,0.2)',
                            name='Upper Bound'
                        )
                        fig.add_scatter(
                            x=forecast['ds'],
                            y=forecast['yhat_lower'],
                            fill='tonexty',
                            mode='lines',
                            line_color='rgba(0,100,80,0.2)',
                            name='Lower Bound'
                        )
                        st.plotly_chart(fig)
                        
                        # Show forecast statistics
                        st.write("Forecast Statistics:")
                        forecast_stats = forecast[['ds', 'yhat', 'yhat_lower', 
                                                 'yhat_upper']].tail(forecast_periods)
                        forecast_stats.columns = ['Date', 'Forecast', 'Lower Bound', 
                                                'Upper Bound']
                        st.write(forecast_stats)
                        
                    except Exception as e:
                        st.error(f"Error in forecasting: {str(e)}")
        
        elif analysis_type == "Statistical Tests":
            st.subheader("Statistical Tests")
            
            test_type = st.selectbox(
                "Select Test Type",
                ["T-Test", "Chi-Square Test", "ANOVA", "Confidence Interval"]
            )
            
            if test_type in ["T-Test", "ANOVA"]:
                col1, col2 = st.columns(2)
                with col1:
                    group1_col = st.selectbox(
                        "Select First Group",
                        options=st.session_state.merged_data.select_dtypes(
                            include=[np.number]).columns
                    )
                with col2:
                    group2_col = st.selectbox(
                        "Select Second Group",
                        options=st.session_state.merged_data.select_dtypes(
                            include=[np.number]).columns
                    )
                
                if st.button("Run Test"):
                    result = StatisticalAnalyzer.run_hypothesis_test(
                        st.session_state.merged_data[group1_col],
                        st.session_state.merged_data[group2_col],
                        test_type.lower().replace('-', '_')
                    )
                    
                    st.write(f"Test Statistic: {result['statistic']:.4f}")
                    st.write(f"P-value: {result['p_value']:.4f}")
                    
                    # Interpretation
                    if result['p_value'] < 0.05:
                        st.success("Statistically significant difference found (p < 0.05)")
                    else:
                        st.info("No statistically significant difference found (p >= 0.05)")
            
            elif test_type == "Chi-Square Test":
                col1, col2 = st.columns(2)
                with col1:
                    cat1_col = st.selectbox(
                        "Select First Categorical Variable",
                        options=st.session_state.merged_data.select_dtypes(
                            include=['object']).columns
                    )
                with col2:
                    cat2_col = st.selectbox(
                        "Select Second Categorical Variable",
                        options=st.session_state.merged_data.select_dtypes(
                            include=['object']).columns
                    )
                
                if st.button("Run Test"):
                    result = StatisticalAnalyzer.run_hypothesis_test(
                        st.session_state.merged_data[cat1_col],
                        st.session_state.merged_data[cat2_col],
                        'chi_square'
                    
        
        elif analysis_type == "Custom Query":
            query = st.text_area("Enter your SQL query:", 
                               "SELECT * FROM merged_data LIMIT 5")
            if st.button("Run Query"):
                try:
                    result = st.session_state.analyzer.run_query(query)
                    st.write(result)
                except Exception as e:
                    st.error(f"Error executing query: {str(e)}")
        
        elif analysis_type == "Visualization":
            chart_type = st.selectbox("Select Chart Type", 
                                    ["Line Chart", "Bar Chart", "Scatter Plot"])
            
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox("Select X-axis column", 
                                   options=st.session_state.merged_data.columns)
            with col2:
                y_col = st.selectbox("Select Y-axis column", 
                                   options=st.session_state.merged_data.columns)
            
            if chart_type == "Line Chart":
                fig = px.line(st.session_state.merged_data, x=x_col, y=y_col)
            elif chart_type == "Bar Chart":
                fig = px.bar(st.session_state.merged_data, x=x_col, y=y_col)
            else:  # Scatter Plot
                fig = px.scatter(st.session_state.merged_data, x=x_col, y=y_col)
            
            st.plotly_chart(fig)

if __name__ == "__main__":
    main()
