# -*- coding: utf-8 -*-
"""
Python code demonstrating a general multi-agent framework using Azure OpenAI.
Includes Agent, Orchestrator, and Message structures.
"""

import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union, Callable
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI # Keep using Azure OpenAI client
from pydantic import BaseModel, Field, field_validator, PrivateAttr # Use Pydantic for Agent state if needed
from langchain.chat_models import AzureChatOpenAI # Keep Langchain wrapper
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from collections import namedtuple
import re
import time # For potential delays or async simulation

# --- Basic Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

# Environment file paths
ENV_DIR = "env"
CONFIG_PATH = Path(ENV_DIR) / "config.env"
CREDS_PATH = Path(ENV_DIR) / "credentials.env"
CERT_PATH = Path(ENV_DIR) / "cacert.pem"

# --- Message Structure ---
# Simple message structure for agent communication
Message = namedtuple("Message", ["sender_id", "recipient_id", "content_type", "content", "timestamp"])

# --- Utility Functions (Keep relevant ones) ---

def is_file_readable(filepath: Union[str, Path]) -> bool:
    """Check if a file exists and is readable."""
    path = Path(filepath)
    if not path.is_file() or not os.access(path, os.R_OK):
        logger.warning(f"The file '{path}' does not exist or is not readable.")
        return False
    return True

def str_to_bool(s: Optional[str]) -> bool:
    """Convert a string representation of truth to boolean."""
    return s is not None and s.lower() in ('true', '1', 't', 'y', 'yes')

# --- Environment Management (Original OSEnv - slightly adapted logging) ---

class OSEnv:
    """Manages environment variables for Azure configuration."""
    def __init__(self, config_file: Union[str, Path], creds_file: Union[str, Path], certificate_path: Optional[Union[str, Path]] = None):
        self._var_list = [] # Use internal list
        self._sensitive_keys = {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET', 'OPENAI_API_KEY'} # Add OpenAI key if used directly

        logger.info(f"Loading config from: {config_file}")
        self.bulk_set(config_file, True)
        logger.info(f"Loading credentials from: {creds_file}")
        self.bulk_set(creds_file, False)

        if certificate_path:
             logger.info(f"Attempting to set certificate path: {certificate_path}")
             self.set_certificate_path(certificate_path)

        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            logger.info("Proxy enabled, setting proxy environment variables.")
            self.set_proxy()

        # Determine credential type (Managed Identity or Client Secret)
        # This credential will be used by LangChain or direct OpenAI client
        self.credential = self._get_credential()
        if self.credential:
             logger.info(f"Azure Credential object created: {type(self.credential).__name__}")
        else:
             logger.warning("Could not create Azure credential object.")

        # Note: Token fetching is often handled by the client library itself when needed.
        # Pre-fetching might not be necessary unless explicitly required by a non-standard setup.
        # self.token = self.get_azure_token() # Removed pre-fetching

    def _get_credential(self):
        """Gets the appropriate Azure credential object based on config."""
        try:
            if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
                logger.info("Using Azure Managed Identity (DefaultAzureCredential).")
                # Ensure AZURE_TENANT_ID, AZURE_CLIENT_ID might still be needed depending on environment
                return DefaultAzureCredential()
            else:
                logger.info("Using Azure Client Secret Credential.")
                tenant_id = self.get("AZURE_TENANT_ID")
                client_id = self.get("AZURE_CLIENT_ID")
                client_secret = self.get("AZURE_CLIENT_SECRET")
                if not all([tenant_id, client_id, client_secret]):
                     logger.error("Missing AZURE_TENANT_ID, AZURE_CLIENT_ID, or AZURE_CLIENT_SECRET for ClientSecretCredential.")
                     return None
                return ClientSecretCredential(
                    tenant_id=tenant_id,
                    client_id=client_id,
                    client_secret=client_secret
                )
        except Exception as e:
            logger.error(f"Error creating Azure credential: {e}", exc_info=True)
            return None

    def set_certificate_path(self, path: Union[str, Path]):
        """Sets environment variables for custom CA certificates."""
        try:
            abs_path_str = str(Path(path).resolve())
            if not is_file_readable(abs_path_str):
                 raise FileNotFoundError(f"Resolved certificate path '{abs_path_str}' not found or not readable.")

            self.set("REQUESTS_CA_BUNDLE", abs_path_str)
            self.set("SSL_CERT_FILE", abs_path_str)
            self.set("CURL_CA_BUNDLE", abs_path_str) # May not be used by all libs
            logger.info(f"Set certificate path for requests/ssl: {abs_path_str}")
        except Exception as e:
            logger.error(f"Error setting certificate path '{path}': {e}")
            # Decide if this is critical

    def bulk_set(self, dotenvfile: Union[str, Path], print_val: bool = False) -> None:
        """Loads variables from a .env file."""
        path = Path(dotenvfile)
        if not path.is_absolute():
            path = path.resolve()

        if not is_file_readable(path):
             logger.warning(f"Environment file '{path}' not found or not readable. Skipping.")
             return

        try:
            temp_dict = dotenv_values(path)
            for key, value in temp_dict.items():
                self.set(key, str(value) if value is not None else "", print_val)
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {path}: {e}")

    def set(self, key: str, value: str, print_val: bool = False) -> None:
        """Sets a single environment variable."""
        try:
            os.environ[key] = value
            if key not in self._var_list:
                self._var_list.append(key)
            if print_val:
                log_value = "[REDACTED]" if key in self._sensitive_keys else value
                # Ensure logger is configured before calling info
                if logger.hasHandlers():
                     logger.info(f"Set Env: {key}={log_value}")
                else: # Fallback if logger isn't ready (e.g., during early init)
                     print(f"Set Env: {key}={log_value}")

        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")

    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:
        """Gets an environment variable."""
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            return default

    def set_proxy(self) -> None:
        """Sets HTTP/HTTPS proxy environment variables."""
        # Using generic names from original code, adjust if needed
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")

        if not proxy_domain:
             logger.error("HTTPS_PROXY_DOMAIN is required but not set for proxy configuration.")
             return

        if ad_username and ad_password:
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
        else:
             # Assuming proxy doesn't need auth or uses system auth
             proxy_url = f"https://{proxy_domain}" # Adjust scheme if needed (http/https)
             if ad_username or ad_password:
                  logger.warning("Proxy username or password provided but not both; using proxy without auth.")

        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)

        # Original NO_PROXY list - ensure it's relevant for Azure + internal needs
        no_proxy_domains = [
            'cognitiveservices.azure.com',
            'search.windows.net',
            'openai.azure.com',
            'core.windows.net',
            'azurewebsites.net',
            # Add any other internal domains that shouldn't be proxied
            self.get("CUSTOM_NO_PROXY", "") # Allow adding more via env
        ]
        # Filter out empty strings and join
        final_no_proxy = ",".join(filter(None, no_proxy_domains))
        self.set("NO_PROXY", final_no_proxy, print_val=True)
        logger.info(f"Proxy configured: {proxy_url} (NO_PROXY: {final_no_proxy})")


    def list_env_vars(self) -> None:
        """Lists the environment variables managed by this instance."""
        logger.info("--- Environment Variables Managed by OSEnv ---")
        for var in sorted(self._var_list):
            value = os.getenv(var)
            if var in self._sensitive_keys:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {value}")
        logger.info("---------------------------------------------")


# --- LLM Interaction Service (Based on original AzureChatbot) ---

class AzureLLMService:
    """
    Provides access to Azure OpenAI LLM capabilities, usable by agents.
    Manages the LangChain LLM client setup.
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self.llm = self._setup_llm_client()
        if not self.llm:
             # Log the error within _setup_llm_client
             raise RuntimeError("Failed to initialize Azure OpenAI LLM client.")

    def _setup_llm_client(self):
        """Initializes the AzureChatOpenAI client from LangChain."""
        try:
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini") # Or your deployment name
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15") # Check Azure docs for latest
            azure_endpoint = self.env.get("AZURE_ENDPOINT") # Your Azure OpenAI endpoint

            if not azure_endpoint:
                 logger.critical("AZURE_ENDPOINT environment variable is not set.")
                 return None

            # Option 1: Use Azure AD Token Provider (Recommended for Service Principal / Managed Identity)
            if self.env.credential:
                 logger.info("Configuring AzureChatOpenAI with Azure AD Token Provider.")
                 try:
                      # This provider function handles token refresh automatically
                      token_provider = get_bearer_token_provider(
                           self.env.credential,
                           "https://cognitiveservices.azure.com/.default" # Standard scope for Cognitive Services
                      )
                      llm = AzureChatOpenAI(
                           azure_deployment=model_name, # Use deployment name here
                           model_name=model_name, # Can be the base model name too
                           temperature=temperature,
                           max_tokens=max_tokens,
                           openai_api_version=api_version,
                           azure_endpoint=azure_endpoint,
                           azure_ad_token_provider=token_provider,
                           # openai_api_type="azure_ad", # Often inferred
                      )
                      logger.info(f"AzureChatOpenAI client created successfully using Azure AD for deployment '{model_name}'.")
                      return llm
                 except Exception as e:
                      logger.error(f"Failed to setup AzureChatOpenAI with AD Token Provider: {e}", exc_info=True)
                      # Fallback or re-raise? Let's try API key if available as fallback.
                      pass # Continue to try API key method if AD fails

            # Option 2: Use API Key (Simpler, but less secure for applications)
            api_key = self.env.get("OPENAI_API_KEY") # Standard env var name
            if api_key:
                 logger.info("Configuring AzureChatOpenAI with API Key.")
                 llm = AzureChatOpenAI(
                     azure_deployment=model_name,
                     model_name=model_name,
                     temperature=temperature,
                     max_tokens=max_tokens,
                     openai_api_version=api_version,
                     azure_endpoint=azure_endpoint,
                     openai_api_key=api_key,
                     openai_api_type="azure", # Explicitly set type for key auth
                 )
                 logger.info(f"AzureChatOpenAI client created successfully using API Key for deployment '{model_name}'.")
                 return llm
            else:
                 logger.error("No Azure AD credential setup succeeded and OPENAI_API_KEY is not set.")
                 return None

        except ImportError:
             logger.critical("Import error: `langchain` or `openai` libraries not found. "
                           "Install with `pip install langchain langchain-openai openai azure-identity`")
             return None
        except Exception as e:
            logger.critical(f"Critical error setting up Azure LLM client: {e}", exc_info=True)
            return None

    def invoke(self, prompt: str, system_message: Optional[str] = None, history: Optional[List[Message]] = None) -> str:
        """
        Invokes the LLM with a given prompt, optional system message, and history.
        Manages the conversion to LangChain message types.
        """
        if not self.llm:
            logger.error("LLM client not initialized.")
            return "Error: LLM not available."

        messages = []
        if system_message:
            messages.append(SystemMessage(content=system_message))

        # Convert simple Message history to LangChain format (if provided)
        if history:
             for msg in history:
                  # Basic mapping, assumes simple text content for now
                  if msg.sender_id == "USER": # Or a specific user ID
                       messages.append(HumanMessage(content=str(msg.content)))
                  else: # Assume AI/Agent response
                       messages.append(AIMessage(content=str(msg.content)))

        # Add the current prompt as a Human message
        messages.append(HumanMessage(content=prompt))

        try:
            logger.debug(f"Invoking LLM with messages: {messages}")
            response = self.llm.invoke(messages)
            logger.debug(f"LLM response received: {response.content}")
            return response.content # Extract text content from AIMessage response
        except Exception as e:
            logger.error(f"Error invoking LLM: {e}", exc_info=True)
            return f"Error: Could not get response from LLM. {e}"

# --- Base Agent Class ---

class Agent(BaseModel):
    """
    Base class for an AI agent.
    """
    id: str = Field(default_factory=lambda: f"agent_{uuid.uuid4().hex[:8]}")
    role: str = "Generic Agent"
    goal: str = "Process information and respond."
    # Use PrivateAttr for non-serializable fields like the LLM service
    _llm_service: AzureLLMService = PrivateAttr()
    # Agent's internal state/memory (simple dictionary for now)
    state: Dict[str, Any] = {}
    # Simple message history for context within the agent's processing cycle
    message_history: List[Message] = []

    # Pydantic configuration
    class Config:
        arbitrary_types_allowed = True # Allow complex types like AzureLLMService

    def __init__(self, llm_service: AzureLLMService, **data):
        super().__init__(**data)
        self._llm_service = llm_service
        logger.info(f"Agent initialized: ID={self.id}, Role={self.role}")

    def _construct_prompt(self, task: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Constructs a prompt for the LLM based on role, goal, task, and context."""
        prompt = f"You are an AI agent with the role: {self.role}.\n"
        prompt += f"Your current goal is: {self.goal}\n"

        # Include relevant state or context
        if self.state:
             prompt += f"\nCurrent State/Knowledge:\n{json.dumps(self.state, indent=2)}\n"
        if context:
             prompt += f"\nAdditional Context Provided:\n{json.dumps(context, indent=2)}\n"

        # Include recent message history (optional, could make prompt long)
        # if self.message_history:
        #     history_str = "\nRecent Conversation:\n"
        #     for msg in self.message_history[-5:]: # Limit history size
        #         history_str += f"{msg.sender_id}: {msg.content}\n"
        #     prompt += history_str

        prompt += f"\nCurrent Task: {task}\n\nResponse:"
        return prompt

    def process(self, task: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        Processes a given task using the LLM, considering the agent's role, goal, and state.
        Returns the generated response.
        """
        logger.info(f"Agent {self.id} processing task: {task[:100]}...") # Log snippet
        system_prompt = f"You are an AI agent. Role: {self.role}. Goal: {self.goal}." # Simplified system prompt

        # Construct a more detailed prompt for the user message part
        full_prompt = f"Considering your role and goal, please perform the following task:\n\nTask: {task}"
        if self.state:
             full_prompt += f"\n\nUse the following internal state/knowledge if relevant:\n{json.dumps(self.state, indent=2)}"
        if context:
             full_prompt += f"\n\nConsider this additional context provided for the task:\n{json.dumps(context, indent=2)}"

        # Prepare history for LLM invocation (if needed, convert internal Message format)
        # For simplicity here, we are not passing detailed history directly to invoke,
        # but it's incorporated into the prompt context if uncommented above.
        # llm_history = self._prepare_llm_history() # Implement if needed

        response = self._llm_service.invoke(
            prompt=full_prompt,
            system_message=system_prompt,
            # history=llm_history # Pass formatted history if needed
        )

        logger.info(f"Agent {self.id} generated response snippet: {response[:100]}...")
        # Optionally update agent state based on the response or task
        # self.update_state({"last_task": task, "last_response": response})
        return response

    def receive_message(self, message: Message):
        """Handles receiving a message, potentially updating state or history."""
        logger.debug(f"Agent {self.id} received message from {message.sender_id}")
        self.message_history.append(message)
        # Simple state update: record last message received
        self.update_state({"last_message_from": message.sender_id, "last_message_content": message.content})
        # More complex logic could trigger processing based on message content

    def update_state(self, updates: Dict[str, Any]):
        """Updates the agent's internal state."""
        self.state.update(updates)
        logger.debug(f"Agent {self.id} state updated: {updates}")

# --- Orchestrator Class ---

class Orchestrator:
    """
    Manages a collection of agents, routes tasks/messages, and maintains shared state.
    """
    def __init__(self, llm_service: AzureLLMService):
        self.agents: Dict[str, Agent] = {}
        self.shared_state: Dict[str, Any] = {"task_queue": [], "results": {}}
        self.message_log: List[Message] = []
        self._llm_service = llm_service # Can be used by orchestrator itself if needed
        logger.info("Orchestrator initialized.")

    def add_agent(self, agent: Agent):
        """Adds an agent to the orchestrator."""
        if agent.id in self.agents:
            logger.warning(f"Agent with ID {agent.id} already exists. Overwriting.")
        self.agents[agent.id] = agent
        logger.info(f"Agent added: ID={agent.id}, Role={agent.role}")

    def get_agent(self, agent_id: str) -> Optional[Agent]:
        """Retrieves an agent by ID."""
        return self.agents.get(agent_id)

    def send_message(self, sender_id: str, recipient_id: str, content: Any, content_type: str = "text"):
        """Sends a message from one agent to another or from orchestrator."""
        if recipient_id != "BROADCAST" and recipient_id not in self.agents and recipient_id != "ORCHESTRATOR":
             logger.error(f"Recipient agent {recipient_id} not found.")
             return

        message = Message(
            sender_id=sender_id,
            recipient_id=recipient_id,
            content_type=content_type,
            content=content,
            timestamp=time.time()
        )
        self.message_log.append(message)
        logger.debug(f"Message sent: {sender_id} -> {recipient_id} ({content_type})")

        if recipient_id == "BROADCAST":
            for agent_id, agent in self.agents.items():
                 if agent_id != sender_id: # Don't send to self
                      agent.receive_message(message)
        elif recipient_id == "ORCHESTRATOR":
             # Handle messages directed to the orchestrator itself (e.g., status updates)
             logger.info(f"Orchestrator received message from {sender_id}: {content}")
             # Add specific logic here if needed
             pass
        else:
            recipient_agent = self.get_agent(recipient_id)
            if recipient_agent:
                 recipient_agent.receive_message(message)

    def run_workflow(self, initial_task: str, workflow_definition: List[Dict]):
        """
        Runs a predefined workflow involving multiple agents.
        workflow_definition: List of steps, e.g.,
        [
            {'agent_id': 'researcher', 'task': 'Research topic X based on initial task.'},
            {'agent_id': 'writer', 'task': 'Write a summary based on the research.'},
            # ... more steps
        ]
        """
        logger.info(f"Starting workflow with initial task: {initial_task}")
        current_context = {"initial_task": initial_task}
        last_output = None

        for i, step in enumerate(workflow_definition):
            agent_id = step.get("agent_id")
            task_template = step.get("task") # Task can be a template string

            if not agent_id or not task_template:
                 logger.error(f"Workflow step {i+1} is missing 'agent_id' or 'task'. Skipping.")
                 continue

            agent = self.get_agent(agent_id)
            if not agent:
                 logger.error(f"Agent '{agent_id}' defined in workflow step {i+1} not found. Skipping.")
                 continue

            # Simple templating: Replace placeholders like {last_output} or {initial_task}
            try:
                 task = task_template.format(last_output=last_output, initial_task=initial_task, **self.shared_state, **current_context)
            except KeyError as e:
                 logger.warning(f"Missing key {e} in task template for step {i+1}. Using raw template.")
                 task = task_template
            except Exception as e:
                 logger.error(f"Error formatting task for step {i+1}: {e}. Using raw template.")
                 task = task_template


            logger.info(f"--- Workflow Step {i+1}: Agent={agent_id} ---")
            # Provide current shared state and step-specific context to the agent
            step_context = {**self.shared_state, **current_context}
            logger.debug(f"Passing context to agent {agent_id}: {step_context}")

            try:
                 output = agent.process(task=task, context=step_context)
                 last_output = output # Store output for the next step
                 current_context[f'step_{i+1}_output'] = output # Add step output to context
                 # Optionally update shared state based on output
                 # self.update_shared_state(...)
                 logger.info(f"Step {i+1} completed by {agent_id}.")
                 # Log message flow (optional)
                 self.send_message("ORCHESTRATOR", agent_id, f"Task completed: {task[:50]}...", "status")

            except Exception as e:
                 logger.error(f"Error processing step {i+1} by agent {agent_id}: {e}", exc_info=True)
                 last_output = f"Error in step {i+1}"
                 # Decide whether to halt workflow or continue
                 break # Stop workflow on error for now

        logger.info("Workflow finished.")
        return last_output # Return the final output of the workflow

    def update_shared_state(self, updates: Dict[str, Any]):
        """Updates the orchestrator's shared state."""
        self.shared_state.update(updates)
        logger.debug(f"Orchestrator shared state updated: {updates}")


# --- Example Usage ---

if __name__ == "__main__":
    logger.info("--- Starting Multi-Agent Framework Demo ---")

    # 1. Initialize Environment
    try:
        os_env = OSEnv(
            config_file=CONFIG_PATH,
            creds_file=CREDS_PATH,
            certificate_path=CERT_PATH if is_file_readable(CERT_PATH) else None
        )
        os_env.list_env_vars()
    except Exception as e:
        logger.critical(f"Failed to initialize environment: {e}", exc_info=True)
        sys.exit(1)

    # 2. Initialize LLM Service
    try:
        llm_service = AzureLLMService(env=os_env)
    except Exception as e:
        logger.critical(f"Failed to initialize AzureLLMService: {e}", exc_info=True)
        sys.exit(1)

    # 3. Initialize Orchestrator
    orchestrator = Orchestrator(llm_service=llm_service)

    # 4. Create Agents
    try:
        researcher = Agent(
            llm_service=llm_service,
            role="Technology Researcher",
            goal="Find the latest information on a given technology topic.",
            state={"preferred_sources": ["arxiv.org", "techcrunch.com", "ai.googleblog.com"]} # Example initial state
        )
        orchestrator.add_agent(researcher)

        writer = Agent(
            llm_service=llm_service,
            role="Content Writer",
            goal="Summarize research findings into a concise blog post format.",
            state={"tone": "informative", "audience": "general tech enthusiasts"}
        )
        orchestrator.add_agent(writer)

        editor = Agent(
             llm_service=llm_service,
             role="Editor",
             goal="Review written content for clarity, grammar, and adherence to guidelines.",
             state={"guidelines": "Ensure content is under 500 words and easy to understand."}
        )
        orchestrator.add_agent(editor)

    except Exception as e:
         logger.critical(f"Failed to create agents: {e}", exc_info=True)
         sys.exit(1)


    # 5. Define and Run a Workflow
    initial_topic = "the impact of large language models on software development"
    workflow = [
        {
            "agent_id": researcher.id,
            "task": "Research the topic: '{initial_task}'. Focus on recent developments (last 6 months) and key impacts. Use your preferred sources if possible."
        },
        {
            "agent_id": writer.id,
            "task": "Write a blog post summary (around 400 words) based on the following research findings:\n\n{last_output}\n\nKeep the tone {state[tone]} and target audience {state[audience]} in mind."
            # Note: Accessing agent state directly in task template like {state[tone]} is a simplification.
            # A better approach might involve the agent incorporating its state during processing,
            # or the orchestrator explicitly passing relevant state parts in the context.
            # For this example, we assume the agent uses its state when processing the task.
        },
        {
             "agent_id": editor.id,
             "task": "Review the following draft blog post for clarity, grammar, and ensure it meets the guideline: '{state[guidelines]}'. Provide feedback or the final edited version.\n\nDraft:\n{last_output}"
        }
    ]

    final_result = orchestrator.run_workflow(initial_task=initial_topic, workflow_definition=workflow)

    print("\n" + "="*50)
    print("Workflow Final Result:")
    print("="*50)
    print(final_result)
    print("="*50 + "\n")

    # 6. Optional: Inspect final state or message log
    logger.info(f"Final shared state: {orchestrator.shared_state}")
    # logger.info("Message Log:")
    # for msg in orchestrator.message_log:
    #     logger.info(f"  {msg.timestamp:.0f} | {msg.sender_id} -> {msg.recipient_id} | {msg.content_type}: {str(msg.content)[:80]}...")


    logger.info("--- Multi-Agent Framework Demo Finished ---")
