# Add these imports at the top
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import tiktoken

# Add Tiktoken configuration class before OSEnv
class TiktokenProxyConfig:
    _configured = False
    
    @classmethod
    def configure(cls):
        if not cls._configured:
            session = requests.Session()
            retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
            session.mount('https://', HTTPAdapter(max_retries=retries))
            
            if os.environ.get("HTTPS_PROXY"):
                session.proxies = {
                    "http": os.environ.get("HTTPS_PROXY"),
                    "https": os.environ.get("HTTPS_PROXY")
                }
            
            session.verify = os.environ.get("REQUESTS_CA_BUNDLE", True)
            tiktoken.set_http_client(session)
            cls._configured = True

# Update the EmbeddingClient with a wrapper
class EmbeddingWrapper(Embeddings):
    def __init__(self, embedding_client: EmbeddingClient):
        self.client = embedding_client
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        doc = MyDocument(text=text)
        embedded_doc = self.client.generate_embeddings(doc)
        return embedded_doc.embedding

# Modified PDFReaderAgent
class PDFReaderAgent:
    def __init__(self, vector_db: Chroma, embedding_client: EmbeddingClient):
        self.vector_db = vector_db
        self.embedding_client = embedding_client
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=self.tiktoken_len
        )
        TiktokenProxyConfig.configure()
    
    def tiktoken_len(self, text: str) -> int:
        try:
            enc = tiktoken.get_encoding("cl100k_base")
            return len(enc.encode(text))
        except Exception as e:
            logger.error(f"Tiktoken error: {e}")
            return len(text.split())

    def process_pdf(self, file_path: str) -> List[MyDocument]:
        try:
            docs = []
            reader = PdfReader(file_path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text:
                    chunks = self.text_splitter.split_text(text)
                    for chunk in chunks:
                        doc = MyDocument(
                            id=str(uuid.uuid4()),
                            text=chunk,
                            metadata={
                                "source": file_path,
                                "page": page_num + 1
                            }
                        )
                        doc = self.embedding_client.generate_embeddings(doc)
                        docs.append(doc)
            self.vector_db.add(
                ids=[d.id for d in docs],
                documents=[d.text for d in docs],
                metadatas=[d.metadata for d in docs],
                embeddings=[d.embedding for d in docs]
            )
            return docs
        except Exception as e:
            logger.error(f"Error processing PDF {file_path}: {e}")
            return []

# Updated TopicExtractionAgent
class TopicExtractionAgent:
    def _validate_topics(self, topics: List[str]) -> List[str]:
        results = self.vector_db.similarity_search(
            query=", ".join(topics),
            k=5
        )
        context = "\n".join([d.text for d in results])  # Changed from page_content to text
        
        validation_prompt = f"""Validate and consolidate topics based on context:
        Context: {context}
        Proposed Topics: {topics}
        
        Remove duplicates, merge similar terms, and keep only relevant topics.
        Return cleaned comma-separated values:"""
        
        chain = LLMChain(llm=self.llm, prompt=PromptTemplate.from_template(validation_prompt))
        return [t.strip() for t in chain.run().split(",")]

# Updated workflow setup
def setup_workflow(pdf_directory: str) -> StateGraph:
    env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
    embedding_client = EmbeddingClient()
    
    chroma_client = Chroma(
        embedding_function=EmbeddingWrapper(embedding_client),  # Use wrapper
        persist_directory="./chroma_db",
        client_settings=Settings(anonymized_telemetry=False)
    )
    
    # Rest of the setup remains the same but update the process_pdfs node:
    workflow.add_node("process_pdfs", 
        lambda state: {"documents": [
            doc for f in Path(pdf_directory).glob("*.pdf") 
            for doc in pdf_agent.process_pdf(str(f))
        ]}
    )

# Updated main execution
if __name__ == "__main__":
    # Initialize environment first
    env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
    TiktokenProxyConfig.configure()
    
    pdf_dir = "./pdf_documents"
    workflow = setup_workflow(pdf_dir)
    app = workflow.compile()
    
    # Initialize all state keys
    final_state = app.invoke({
        "documents": [],
        "topics": [],
        "triples": [],
        "ontology": None
    })
    
    # Safe access with get()
    ontology_graph = final_state.get("ontology", Graph())
    if isinstance(ontology_graph, Graph) and len(ontology_graph) > 0:
        ontology_graph.serialize(destination="ontology.owl", format="xml")
        logger.info("Ontology generated and saved to ontology.owl")
    else:
        logger.error("Failed to generate ontology")
