"""
Azure OpenAI service using token provider pattern that aligns with your existing code.
"""

import os
import logging
import time
import traceback
from typing import List, Dict, Any, Optional, Union
from dotenv import dotenv_values
from openai import AzureOpenAI
from app.core.auth_helper import get_azure_credential, get_azure_token_provider

logger = logging.getLogger(__name__)

# Load credentials directly from file
credentials_path = os.path.join("env", "credentials.env")
config_path = os.path.join("env", "config.env")
credentials_values = {}
config_values = {}

try:
    if os.path.isfile(credentials_path):
        logger.info(f"AzureOpenAIService loading credentials from {credentials_path}")
        credentials_values = dotenv_values(credentials_path)
        logger.info(f"Loaded {len(credentials_values)} values from {credentials_path}")
    else:
        logger.warning(f"Credentials file not found: {credentials_path}")
        
    if os.path.isfile(config_path):
        logger.info(f"AzureOpenAIService loading config from {config_path}")
        config_values = dotenv_values(config_path)
        logger.info(f"Loaded {len(config_values)} values from {config_path}")
    else:
        logger.warning(f"Config file not found: {config_path}")
except Exception as e:
    logger.error(f"Error loading env files: {e}")

# Combine values, with credentials taking precedence
all_values = {**config_values, **credentials_values}

# Extract values directly
AZURE_TENANT_ID = all_values.get("AZURE_TENANT_ID", "")
AZURE_CLIENT_ID = all_values.get("AZURE_CLIENT_ID", "")
AZURE_CLIENT_SECRET = all_values.get("AZURE_CLIENT_SECRET", "")
AZURE_OPENAI_ENDPOINT = all_values.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_EMBEDDING_MODEL = all_values.get("AZURE_EMBEDDING_MODEL", "text-embedding-3-large")
AZURE_EMBEDDING_DEPLOYMENT = all_values.get("AZURE_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
AZURE_LLM_MODEL = all_values.get("AZURE_LLM_MODEL", "gpt-4o-mini")
AZURE_LLM_DEPLOYMENT = all_values.get("AZURE_LLM_DEPLOYMENT", "gpt-4o-mini")

# Log values (masked for security)
if AZURE_TENANT_ID:
    masked_tenant = f"{AZURE_TENANT_ID[:4]}...{AZURE_TENANT_ID[-4:]}" if len(AZURE_TENANT_ID) > 8 else "***"
    logger.info(f"Using Azure tenant ID: {masked_tenant}")
else:
    logger.warning("AZURE_TENANT_ID is missing")

if AZURE_CLIENT_ID:
    masked_client = f"{AZURE_CLIENT_ID[:4]}...{AZURE_CLIENT_ID[-4:]}" if len(AZURE_CLIENT_ID) > 8 else "***"
    logger.info(f"Using Azure client ID: {masked_client}")
else:
    logger.warning("AZURE_CLIENT_ID is missing")

if AZURE_CLIENT_SECRET:
    logger.info(f"Azure client secret is set (length: {len(AZURE_CLIENT_SECRET)} characters)")
else:
    logger.warning("AZURE_CLIENT_SECRET is missing")

logger.info(f"Using Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}")
logger.info(f"Using embedding model: {AZURE_EMBEDDING_MODEL}, deployment: {AZURE_EMBEDDING_DEPLOYMENT}")
logger.info(f"Using LLM model: {AZURE_LLM_MODEL}, deployment: {AZURE_LLM_DEPLOYMENT}")

class AzureOpenAIService:
    """Service for interacting with Azure OpenAI models."""
    
    def __init__(self):
        """Initialize the Azure OpenAI service."""
        self.endpoint = AZURE_OPENAI_ENDPOINT
        self.embedding_model = AZURE_EMBEDDING_MODEL
        self.embedding_deployment = AZURE_EMBEDDING_DEPLOYMENT
        self.llm_model = AZURE_LLM_MODEL
        self.llm_deployment = AZURE_LLM_DEPLOYMENT
        self.api_version = "2023-05-15"
        
        # Show the actual values being used
        logger.info(f"AzureOpenAIService initialized with:")
        logger.info(f"  - OpenAI endpoint: {self.endpoint}")
        logger.info(f"  - Embedding model: {self.embedding_model}")
        logger.info(f"  - Embedding deployment: {self.embedding_deployment}")
        logger.info(f"  - LLM model: {self.llm_model}")
        logger.info(f"  - LLM deployment: {self.llm_deployment}")
        
        # Initialize the Azure OpenAI client
        self._initialize_client()
        logger.info(f"AzureOpenAIService initialized successfully")
    
    def _initialize_client(self):
        """Initialize the Azure OpenAI client using the token provider pattern."""
        try:
            # Get token provider from auth_helper
            token_provider = get_azure_token_provider()
            
            # Initialize client with token provider (matching your existing code pattern)
            self.client = AzureOpenAI(
                azure_endpoint=self.endpoint,
                api_version=self.api_version,
                azure_ad_token_provider=token_provider
            )
            
            logger.info("Azure OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Azure OpenAI client: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def refresh_tokens(self):
        """Refresh the Azure tokens by reinitializing the client."""
        try:
            logger.info("Refreshing Azure AD token by reinitializing the client...")
            # Get fresh token provider and re-initialize client
            token_provider = get_azure_token_provider(force_refresh=True)
            
            self.client = AzureOpenAI(
                azure_endpoint=self.endpoint,
                api_version=self.api_version,
                azure_ad_token_provider=token_provider
            )
            
            logger.info("Azure OpenAI client reinitialized with fresh token provider")
            return True
        except Exception as e:
            logger.error(f"Failed to refresh Azure OpenAI token: {e}")
            logger.error(traceback.format_exc())
            return False
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts using Azure OpenAI.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        try:
            # Return empty list if no texts provided
            if not texts:
                return []
                
            logger.info(f"Generating embeddings for {len(texts)} texts")
            
            # Process in smaller batches to avoid overloading the API
            batch_size = 5  # Smaller batch size for stability
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(texts) - 1) // batch_size + 1
                
                try:
                    logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch_texts)} texts)")
                    
                    # Generate embeddings for the batch
                    response = self.client.embeddings.create(
                        input=batch_texts,
                        model=self.embedding_deployment,
                        dimensions=3072  # Maximum dimensions for text-embedding-3-large
                    )
                    
                    # Extract embeddings from response
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    
                    logger.info(f"Successfully processed batch {batch_num}/{total_batches}")
                
                except Exception as batch_error:
                    logger.error(f"Error processing batch {batch_num}: {batch_error}")
                    
                    # Try refreshing token and retrying
                    try:
                        logger.info("Attempting to refresh token and retry...")
                        self.refresh_tokens()
                        
                        # Retry the batch
                        response = self.client.embeddings.create(
                            input=batch_texts,
                            model=self.embedding_deployment,
                            dimensions=3072
                        )
                        
                        batch_embeddings = [item.embedding for item in response.data]
                        all_embeddings.extend(batch_embeddings)
                        
                        logger.info(f"Successfully processed batch {batch_num}/{total_batches} after token refresh")
                    
                    except Exception as retry_error:
                        logger.error(f"Retry failed for batch {batch_num}: {retry_error}")
                        # Add empty embeddings as placeholders
                        empty_embeddings = [[0.0] * 3072 for _ in range(len(batch_texts))]
                        all_embeddings.extend(empty_embeddings)
                
                # Add a delay between batches
                if i + batch_size < len(texts):
                    time.sleep(1.0)
            
            logger.info(f"Generated {len(all_embeddings)} embeddings")
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            logger.error(traceback.format_exc())
            raise
    
    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text using Azure OpenAI.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        try:
            # Process as a batch of 1 for consistency
            batched_result = await self.generate_embeddings([text])
            if batched_result:
                return batched_result[0]
            else:
                # Return empty embedding as fallback
                logger.error("Failed to generate single embedding")
                return [0.0] * 3072
                
        except Exception as e:
            logger.error(f"Error generating single embedding: {e}")
            logger.error(traceback.format_exc())
            # Return empty embedding as fallback
            return [0.0] * 3072
    
    async def generate_completion(self, 
                                messages: List[Dict[str, str]], 
                                temperature: float = 0.0,
                                max_tokens: int = 2000) -> str:
        """
        Generate a completion using Azure OpenAI.
        
        Args:
            messages: List of messages (system, user, assistant)
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        try:
            logger.info("Generating completion")
            
            # Generate completion
            response = self.client.chat.completions.create(
                model=self.llm_deployment,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Extract content from response
            content = response.choices[0].message.content
            
            logger.info("Completion generated successfully")
            return content
            
        except Exception as e:
            logger.error(f"Error generating completion: {e}")
            
            # Try refreshing token and retrying
            try:
                logger.info("Attempting to refresh token and retry...")
                self.refresh_tokens()
                
                # Retry the completion
                response = self.client.chat.completions.create(
                    model=self.llm_deployment,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                content = response.choices[0].message.content
                logger.info("Completion generated successfully after token refresh")
                return content
                
            except Exception as retry_error:
                logger.error(f"Retry failed: {retry_error}")
                logger.error(traceback.format_exc())
                return "Error generating completion. Please try again."
