#!/usr/bin/env python3
"""
PII Classification Tool using OpenAI's o3-mini model
Classifies data fields for Personal Data with detailed reasoning and confidence scores
"""

import pandas as pd
import requests
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import argparse
import sys

class PIIClassifier:
    def __init__(self, base_url: str, api_key: str):
        """
        Initialize the PII Classifier
        
        Args:
            base_url (str): OpenAI API base URL
            api_key (str): OpenAI API key
        """
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = "o3-mini-2025-01-31"
        
    def classify_field(self, name: str, definition: str) -> Dict:
        """
        Classify a single data field for PII using OpenAI o3-mini
        
        Args:
            name (str): Field name
            definition (str): Field definition
            
        Returns:
            Dict: Classification result with reasoning
        """
        prompt = f"""You are a data privacy expert. Analyze the following data field and classify it across two dimensions: sensitivity level and personal data risk.

Data Field Name: "{name}"
Data Field Definition: "{definition}"

Please provide your analysis in the following JSON format:
{{
  "sensitivity_classification": "string (Highly Sensitive, Sensitive, or Non-sensitive)",
  "contains_personal_data": boolean,
  "personal_data_risk_level": "string (High Risk, Medium Risk, Low Risk, or Not PII)",
  "supporting_reasons": ["reason1", "reason2", "reason3"],
  "contradicting_reasons": ["reason1", "reason2", "reason3"],
  "confidence_score": number (0-100),
  "explanation": "detailed explanation of both classifications"
}}

Classification Guidelines:

SENSITIVITY LEVELS:
- Highly Sensitive: Data that could cause severe harm if disclosed (SSN, financial data, health records, biometrics)
- Sensitive: Data that could cause moderate harm if disclosed (names, contact info, demographics)
- Non-sensitive: Data that poses minimal risk if disclosed (public info, technical metadata, aggregated data)

PERSONAL DATA RISK LEVELS:
- High Risk: Direct identifiers that uniquely identify individuals (SSN, passport, driver's license, credit cards)
- Medium Risk: Quasi-identifiers that can identify when combined (name, email, phone, address, DOB)
- Low Risk: Demographic or behavioral data that may indirectly identify (age range, preferences, usage patterns)
- Not PII: Anonymous, aggregated, or purely technical data with no personal identification capability

The contains_personal_data flag should be true for High Risk, Medium Risk, and Low Risk classifications, and false for Not PII.

Provide exactly 3 supporting reasons why these classifications are correct and 3 contradicting reasons why they might be wrong. Base confidence score on the strength of evidence for both classifications combined."""

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        payload = {
            'model': self.model,
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ]
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # Extract JSON from the response
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("No valid JSON found in response")
                
            json_content = content[start_idx:end_idx]
            result = json.loads(json_content)
            
            # Validate required fields
            required_fields = ['sensitivity_classification', 'contains_personal_data', 'personal_data_risk_level', 
                             'supporting_reasons', 'contradicting_reasons', 'confidence_score', 'explanation']
            
            for field in required_fields:
                if field not in result:
                    result[field] = None
                    
            return result
            
        except requests.exceptions.RequestException as e:
            return {
                'sensitivity_classification': 'Error',
                'contains_personal_data': None,
                'personal_data_risk_level': 'Error',
                'supporting_reasons': [],
                'contradicting_reasons': [],
                'confidence_score': 0,
                'explanation': f'API Error: {str(e)}'
            }
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            return {
                'sensitivity_classification': 'Error',
                'contains_personal_data': None,
                'personal_data_risk_level': 'Error',
                'supporting_reasons': [],
                'contradicting_reasons': [],
                'confidence_score': 0,
                'explanation': f'Response parsing error: {str(e)}'
            }
    
    def process_csv(self, csv_path: str, output_prefix: str = "pii_classification_results", 
                   delay: float = 0.1) -> Tuple[List[Dict], str, str]:
        """
        Process a CSV file and classify all fields
        
        Args:
            csv_path (str): Path to input CSV file
            output_prefix (str): Prefix for output files
            delay (float): Delay between API calls in seconds
            
        Returns:
            Tuple[List[Dict], str, str]: Results list, JSON path, CSV path
        """
        # Read CSV file
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            raise ValueError(f"Error reading CSV file: {str(e)}")
        
        # Validate required columns
        required_columns = ['Name', 'Definition']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Remove rows with empty Name or Definition
        df = df.dropna(subset=['Name', 'Definition'])
        df = df[df['Name'].str.strip() != '']
        df = df[df['Definition'].str.strip() != '']
        
        if df.empty:
            raise ValueError("No valid data rows found in CSV")
        
        print(f"Processing {len(df)} rows...")
        
        results = []
        for index, row in df.iterrows():
            name = str(row['Name']).strip()
            definition = str(row['Definition']).strip()
            
            print(f"Processing {index + 1}/{len(df)}: {name}")
            
            # Classify the field
            classification = self.classify_field(name, definition)
            
            # Combine with original data
            result = {
                'original_name': name,
                'original_definition': definition,
                **classification,
                'processed_at': datetime.now().isoformat()
            }
            
            results.append(result)
            
            # Add delay to avoid rate limiting
            if delay > 0:
                time.sleep(delay)
        
        # Generate output file paths
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = f"{output_prefix}_{timestamp}.json"
        csv_output_path = f"{output_prefix}_{timestamp}.csv"
        
        # Save as JSON
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # Save as CSV
        results_df = pd.DataFrame(results)
        
        # Convert list columns to string for CSV
        list_columns = ['supporting_reasons', 'contradicting_reasons']
        for col in list_columns:
            if col in results_df.columns:
                results_df[col] = results_df[col].apply(
                    lambda x: '; '.join(x) if isinstance(x, list) else str(x)
                )
        
        results_df.to_csv(csv_output_path, index=False, encoding='utf-8')
        
        print(f"\nProcessing complete!")
        print(f"Results saved to:")
        print(f"  JSON: {json_path}")
        print(f"  CSV:  {csv_output_path}")
        
        return results, json_path, csv_output_path
    
    def print_summary(self, results: List[Dict]):
        """Print a summary of classification results"""
        if not results:
            print("No results to summarize")
            return
        
        # Count classifications
        sensitivity_counts = {}
        personal_data_counts = {}
        confidence_scores = []
        personal_data_true_count = 0
        
        for result in results:
            # Sensitivity classification counts
            sensitivity = result.get('sensitivity_classification', 'Unknown')
            sensitivity_counts[sensitivity] = sensitivity_counts.get(sensitivity, 0) + 1
            
            # Personal data risk level counts
            risk_level = result.get('personal_data_risk_level', 'Unknown')
            personal_data_counts[risk_level] = personal_data_counts.get(risk_level, 0) + 1
            
            # Count personal data flags
            if result.get('contains_personal_data') is True:
                personal_data_true_count += 1
            
            # Collect confidence scores
            if isinstance(result.get('confidence_score'), (int, float)):
                confidence_scores.append(result['confidence_score'])
        
        print("\n" + "="*70)
        print("CLASSIFICATION SUMMARY")
        print("="*70)
        
        print(f"Total fields processed: {len(results)}")
        print(f"Average confidence score: {sum(confidence_scores)/len(confidence_scores):.1f}%" if confidence_scores else "N/A")
        print(f"Fields containing personal data: {personal_data_true_count} ({(personal_data_true_count/len(results)*100):.1f}%)")
        
        print("\nSENSITIVITY CLASSIFICATION:")
        for sensitivity, count in sorted(sensitivity_counts.items()):
            percentage = (count / len(results)) * 100
            print(f"  {sensitivity}: {count} ({percentage:.1f}%)")
        
        print("\nPERSONAL DATA RISK LEVELS:")
        for risk_level, count in sorted(personal_data_counts.items()):
            percentage = (count / len(results)) * 100
            print(f"  {risk_level}: {count} ({percentage:.1f}%)")


def main():
    """Main function for command line usage"""
    parser = argparse.ArgumentParser(description='PII Classification Tool using OpenAI o3-mini')
    parser.add_argument('csv_file', help='Path to input CSV file with Name and Definition columns')
    parser.add_argument('--base-url', required=True, help='OpenAI API base URL')
    parser.add_argument('--api-key', required=True, help='OpenAI API key')
    parser.add_argument('--output-prefix', default='pii_classification_results', 
                       help='Prefix for output files (default: pii_classification_results)')
    parser.add_argument('--delay', type=float, default=0.1, 
                       help='Delay between API calls in seconds (default: 0.1)')
    parser.add_argument('--summary', action='store_true', 
                       help='Print detailed summary of results')
    
    args = parser.parse_args()
    
    # Validate input file
    if not os.path.exists(args.csv_file):
        print(f"Error: Input file '{args.csv_file}' not found")
        sys.exit(1)
    
    try:
        # Initialize classifier
        classifier = PIIClassifier(args.base_url, args.api_key)
        
        # Process the CSV file
        results, json_path, csv_path = classifier.process_csv(
            args.csv_file, 
            args.output_prefix, 
            args.delay
        )
        
        # Print summary if requested
        if args.summary:
            classifier.print_summary(results)
            
    except Exception as e:
        print(f"Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()


# Example usage as a module:
"""
from pii_classifier import PIIClassifier

# Initialize classifier
classifier = PIIClassifier(
    base_url="https://api.openai.com/v1",
    api_key="your-api-key-here"
)

# Process a CSV file
results, json_path, csv_path = classifier.process_csv("your_data.csv")

# Print summary
classifier.print_summary(results)

# Or classify individual fields
result = classifier.classify_field("user_email", "Email address of the user")
print(f"Sensitivity: {result['sensitivity_classification']}")
print(f"Contains Personal Data: {result['contains_personal_data']}")
print(f"Risk Level: {result['personal_data_risk_level']}")
print(json.dumps(result, indent=2))
"""
