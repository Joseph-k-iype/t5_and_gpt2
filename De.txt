"""
Enhanced Deep Research Agent with LangMem Long-Term Memory and Multi-Agent Architecture
Based on LangChain Open Deep Research patterns with specialized ReAct agents

Features:
- LangMem SDK for cross-session long-term memory
- Multi-agent architecture with specialized research agents
- Enhanced iterative research with knowledge synthesis
- Plan-and-execute workflow with reflection loops
- Semantic and episodic memory management
- Advanced context engineering for multi-agent coordination
- Comprehensive research orchestration with LangGraph
- LLM-based domain filtering (no hardcoded keywords)
- Uses only o3-mini-2025-01-31 for all operations
"""

import asyncio
import json
import logging
import os
import ssl
import uuid
import time
import sys
from datetime import datetime
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
import re
from collections import defaultdict, Counter

# Core imports
import openai
from elasticsearch import Elasticsearch

# LangChain Elasticsearch integration
try:
    from langchain_elasticsearch import ElasticsearchStore
    from langchain_elasticsearch.vectorstores import DenseVectorStrategy
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = True
except ImportError:
    print("Warning: langchain-elasticsearch not available. Install with: pip install langchain-elasticsearch")
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = False

# LangChain core
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_core.retrievers import BaseRetriever
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate

# LangChain OpenAI
try:
    from langchain_openai import ChatOpenAI
    LANGCHAIN_OPENAI_AVAILABLE = True
except ImportError:
    print("Warning: langchain-openai not available. Install with: pip install langchain-openai")
    LANGCHAIN_OPENAI_AVAILABLE = False

# LangGraph
try:
    from langgraph.graph import StateGraph, MessagesState, START, END, CompiledGraph
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.store.memory import InMemoryStore
    from langgraph.prebuilt import ToolNode, create_react_agent
    from langgraph.types import Command
    LANGGRAPH_AVAILABLE = True
except ImportError:
    print("Warning: langgraph not available. Install with: pip install langgraph")
    LANGGRAPH_AVAILABLE = False

# LangMem for long-term memory
try:
    from langmem import create_memory_manager, create_manage_memory_tool, create_search_memory_tool
    LANGMEM_AVAILABLE = True
except ImportError:
    print("Warning: LangMem not available. Install with: pip install langmem")
    LANGMEM_AVAILABLE = False

# Pydantic
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
class Config:
    """Enhanced configuration for deep research agent"""
    
    def __init__(self):
        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        self.OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
        self.ES_PASSWORD = os.getenv("ES_PASSWORD")
        self.ES_HOST = os.getenv("ES_HOST", "localhost")
        
        # Validate required environment variables
        if not self.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        if not self.ES_PASSWORD:
            raise ValueError("ES_PASSWORD environment variable is required")
        
        try:
            self.ES_PORT = int(os.getenv("ES_PORT", "9200"))
        except ValueError:
            self.ES_PORT = 9200
        
        self.ES_CACERT_PATH = os.getenv("ES_CACERT_PATH", "cacert.crt")
        
        # Use only o3-mini model for all operations
        self.MODEL = "o3-mini-2025-01-31"
        self.REASONING_EFFORT = "high"
        self.EMBEDDING_MODEL = "text-embedding-3-large"
        self.EMBEDDING_DIMENSIONS = 3072
        
        # Research configurations
        self.MAX_RESEARCH_ITERATIONS = 5
        self.MAX_SEARCH_RESULTS_PER_ITERATION = 20
        self.CONFIDENCE_THRESHOLD = 0.85
        self.MAX_PARALLEL_RESEARCHERS = 3
        
        # Memory configurations
        self.MEMORY_NAMESPACE_PREFIX = "privacy_research"
        self.SEMANTIC_MEMORY_THRESHOLD = 0.8
        self.EPISODIC_MEMORY_RETENTION_DAYS = 30

# Global config instance
try:
    config = Config()
except Exception as e:
    print(f"Configuration error: {e}")
    sys.exit(1)

# Utility Functions
def safe_json_parse(json_string: str, fallback_value: Any = None) -> Any:
    """Safely parse JSON with fallback handling"""
    if not json_string or not isinstance(json_string, str):
        logger.warning(f"Invalid JSON input: {type(json_string)}")
        return fallback_value
    
    json_string = json_string.strip()
    
    # Try to extract JSON from markdown code blocks
    if "```json" in json_string:
        try:
            start_idx = json_string.find("```json") + 7
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    elif "```" in json_string:
        try:
            start_idx = json_string.find("```") + 3
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    
    # Multiple parsing attempts
    parse_attempts = [
        lambda: json.loads(json_string),
        lambda: json.loads(json_string.replace("'", '"')),
        lambda: json.loads(re.sub(r'(\w+):', r'"\1":', json_string)),
    ]
    
    for attempt in parse_attempts:
        try:
            result = attempt()
            if result is not None:
                return result
        except Exception:
            continue
    
    logger.error(f"Failed to parse JSON: {json_string[:200]}...")
    return fallback_value

def get_event_loop() -> asyncio.AbstractEventLoop:
    """Get or create event loop safely"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            raise RuntimeError("Event loop is closed")
        return loop
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop

# Serialization Helper Functions
def serialize_dataclass(obj):
    """Convert dataclass to JSON-serializable dictionary"""
    if obj is None:
        return None
    
    try:
        if hasattr(obj, '__dataclass_fields__'):
            result = {}
            for field_name, field_value in asdict(obj).items():
                result[field_name] = serialize_object(field_value)
            return result
        else:
            return serialize_object(obj)
    except Exception as e:
        logger.error(f"Error serializing dataclass {type(obj)}: {e}")
        return {"error": f"Serialization failed: {str(e)}"}

def serialize_object(obj):
    """Recursively serialize any object to JSON-compatible format"""
    if obj is None:
        return None
    elif isinstance(obj, (str, int, float, bool)):
        return obj
    elif isinstance(obj, dict):
        return {key: serialize_object(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [serialize_object(item) for item in obj]
    elif hasattr(obj, '__dataclass_fields__'):
        return serialize_dataclass(obj)
    elif hasattr(obj, '__dict__'):
        return {key: serialize_object(value) for key, value in obj.__dict__.items()}
    else:
        return str(obj)

# Enhanced Data Models for Multi-Agent Deep Research
@dataclass
class ResearchTask:
    """Individual research task for specialized agents"""
    task_id: str
    agent_type: str
    query: str
    focus_areas: List[str]
    jurisdictions: Optional[List[str]] = None
    priority: int = 1
    max_results: int = 10
    context: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class ResearchPlan:
    """Enhanced research plan with multi-agent coordination"""
    plan_id: str
    main_query: str
    research_objectives: List[str]
    research_tasks: List[ResearchTask]
    agent_assignments: Dict[str, List[str]]
    expected_iterations: int
    total_estimated_time: int
    success_criteria: List[str]
    coordination_strategy: str
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class AgentState:
    """State for individual research agents"""
    agent_id: str
    agent_type: str
    current_tasks: List[str]
    completed_tasks: List[str]
    findings: List[Dict[str, Any]]
    knowledge_gaps: List[str]
    confidence_score: float
    status: str
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class MultiAgentResearchState:
    """State for the entire multi-agent research system"""
    session_id: str
    original_query: str
    research_plan: Optional[ResearchPlan]
    agent_states: Dict[str, AgentState]
    shared_knowledge: Dict[str, List[Dict]]
    iteration_history: List[Dict[str, Any]]
    current_iteration: int
    max_iterations: int
    overall_confidence: float
    is_complete: bool = False
    final_synthesis: str = ""
    memory_keys: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

# Memory Management State
class MemoryState(TypedDict):
    """State for memory operations"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    user_id: str
    session_id: str
    current_context: Dict[str, Any]
    memory_operations: List[Dict[str, Any]]

# Multi-Agent Research State for LangGraph
class MultiAgentGraphState(TypedDict):
    """Enhanced state for LangGraph multi-agent workflow"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    research_state: Optional[MultiAgentResearchState]
    current_phase: str
    active_agents: List[str]
    coordination_messages: List[Dict[str, Any]]
    memory_context: Dict[str, Any]

# LLM-based Domain Filter
class LLMDomainFilter:
    """LLM-based filter to determine if queries are privacy/law related"""
    
    def __init__(self, openai_manager):
        self.openai_manager = openai_manager
    
    async def is_privacy_law_related(self, query: str) -> Dict[str, Any]:
        """Use LLM to determine if query is privacy/law related"""
        if not query or len(query.strip()) < 3:
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Query too short or empty"
            }
        
        system_prompt = """
        You are an expert classifier for determining if queries are related to data privacy, 
        data protection, legal compliance, or regulatory matters.
        
        Analyze the given query and determine if it falls within these domains:
        - Data privacy and protection regulations (GDPR, CCPA, LGPD, etc.)
        - Legal compliance and regulatory requirements
        - Information security and data governance
        - Privacy rights and obligations
        - Legal frameworks and legislation
        - Regulatory compliance across jurisdictions
        - Data handling and processing legal requirements
        - Privacy policy and legal documentation
        - Breach notification and legal obligations
        - Cross-border data transfer regulations
        
        Return ONLY a JSON response in this exact format:
        {
            "is_relevant": boolean,
            "confidence": float between 0.0 and 1.0,
            "reasoning": "Brief explanation of the decision",
            "domain": "privacy|legal|compliance|other"
        }
        """
        
        try:
            messages = [{"role": "user", "content": f"Classify this query: {query}"}]
            
            response = await self.openai_manager.chat_completion(
                messages, system_prompt=system_prompt
            )
            
            result = safe_json_parse(response, {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Failed to parse LLM response",
                "domain": "other"
            })
            
            # Ensure all required fields exist
            return {
                "is_relevant": result.get("is_relevant", False),
                "confidence": result.get("confidence", 0.0),
                "reasoning": result.get("reasoning", "No reasoning provided"),
                "domain": result.get("domain", "other")
            }
            
        except Exception as e:
            logger.error(f"Error in LLM domain classification: {e}")
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": f"Classification error: {str(e)}",
                "domain": "error"
            }
    
    def get_rejection_message(self, domain_result: Dict[str, Any]) -> str:
        """Generate appropriate rejection message based on classification"""
        reasoning = domain_result.get("reasoning", "")
        
        messages = [
            "I specialize in data privacy, data protection, and legal compliance questions.",
            "My expertise covers privacy regulations, legal frameworks, and compliance requirements.",
            "I focus on data protection laws, privacy rights, and regulatory compliance matters.",
            "I'm designed to help with privacy regulations, legal requirements, and compliance questions."
        ]
        
        import random
        base_message = random.choice(messages)
        
        if reasoning and "not related" not in reasoning.lower():
            return f"{base_message} {reasoning}"
        else:
            return f"{base_message} Please ask questions related to privacy, data protection, or legal compliance."

# Enhanced OpenAI Manager (using only o3-mini)
class EnhancedOpenAIManager:
    """Enhanced OpenAI manager using only o3-mini model"""
    
    def __init__(self):
        try:
            self.client = openai.OpenAI(
                api_key=config.OPENAI_API_KEY,
                base_url=config.OPENAI_BASE_URL
            )
            self._test_connection()
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _test_connection(self):
        """Test OpenAI connection with o3-mini"""
        try:
            test_response = self.client.chat.completions.create(
                model=config.MODEL,
                messages=[{"role": "user", "content": "test"}],
                reasoning_effort=config.REASONING_EFFORT
            )
            logger.info("OpenAI connection successful with o3-mini")
        except Exception as e:
            logger.error(f"OpenAI connection test failed: {e}")
            raise
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using OpenAI API"""
        if not text or not isinstance(text, str):
            logger.warning("Empty or invalid text for embedding")
            return [0.0] * config.EMBEDDING_DIMENSIONS
        
        try:
            clean_text = text.strip()[:8000]
            response = self.client.embeddings.create(
                model=config.EMBEDDING_MODEL,
                input=clean_text,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], system_prompt: str = None, **kwargs) -> str:
        """Create chat completion with o3-mini and reasoning effort"""
        if not messages:
            raise ValueError("Messages cannot be empty")
        
        try:
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({"role": "system", "content": system_prompt})
            
            formatted_messages.extend(messages)
            
            # Remove any parameters that o3-mini doesn't support
            clean_kwargs = {}
            for key, value in kwargs.items():
                if key not in ['max_tokens', 'temperature', 'top_p']:  # o3-mini specific exclusions
                    clean_kwargs[key] = value
            
            response = self.client.chat.completions.create(
                model=config.MODEL,
                messages=formatted_messages,
                reasoning_effort=config.REASONING_EFFORT,
                **clean_kwargs
            )
            
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            raise

# Enhanced Elasticsearch Manager
class EnhancedElasticsearchManager:
    """Enhanced Elasticsearch manager with improved search strategies"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager):
        self.openai_manager = openai_manager
        self.client = self._create_client()
        self.embeddings = self._create_embeddings()
        
        if ELASTICSEARCH_LANGCHAIN_AVAILABLE:
            self._setup_stores()
        else:
            logger.warning("LangChain Elasticsearch not available, using basic search")
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client"""
        try:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            client = Elasticsearch(
                [{"host": config.ES_HOST, "port": config.ES_PORT, "scheme": "https"}],
                basic_auth=(config.ES_USERNAME, config.ES_PASSWORD),
                ssl_context=ssl_context,
                verify_certs=False,
                request_timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            info = client.info()
            logger.info(f"Connected to Elasticsearch: {info.get('version', {}).get('number', 'unknown')}")
            return client
            
        except Exception as e:
            logger.error(f"Failed to create Elasticsearch client: {e}")
            raise
    
    def _create_embeddings(self):
        """Create embeddings wrapper"""
        class DirectOpenAIEmbeddings(Embeddings):
            def __init__(self, openai_manager):
                self.openai_manager = openai_manager
                super().__init__()
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                try:
                    loop = get_event_loop()
                    if loop.is_running():
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, self._aembed_documents(texts))
                            return future.result()
                    else:
                        return asyncio.run(self._aembed_documents(texts))
                except Exception as e:
                    logger.error(f"Error in embed_documents: {e}")
                    return [[0.0] * config.EMBEDDING_DIMENSIONS for _ in texts]
            
            def embed_query(self, text: str) -> List[float]:
                try:
                    loop = get_event_loop()
                    if loop.is_running():
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, self.openai_manager.create_embedding(text))
                            return future.result()
                    else:
                        return asyncio.run(self.openai_manager.create_embedding(text))
                except Exception as e:
                    logger.error(f"Error in embed_query: {e}")
                    return [0.0] * config.EMBEDDING_DIMENSIONS
            
            async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
                return await self._aembed_documents(texts)
            
            async def aembed_query(self, text: str) -> List[float]:
                return await self.openai_manager.create_embedding(text)
            
            async def _aembed_documents(self, texts: List[str]) -> List[List[float]]:
                embeddings = []
                for text in texts:
                    try:
                        embedding = await self.openai_manager.create_embedding(text)
                        embeddings.append(embedding)
                    except Exception as e:
                        logger.error(f"Error embedding document: {e}")
                        embeddings.append([0.0] * config.EMBEDDING_DIMENSIONS)
                return embeddings
        
        return DirectOpenAIEmbeddings(self.openai_manager)
    
    def _setup_stores(self):
        """Setup LangChain Elasticsearch stores"""
        try:
            self.articles_store = ElasticsearchStore(
                es_connection=self.client,
                index_name="privacy_articles",
                embedding=self.embeddings,
                vector_query_field="full_article_embedding",
                query_field="full_content",
                strategy=DenseVectorStrategy(hybrid=False),
                distance_strategy="COSINE"
            )
            
            self.chunks_store = ElasticsearchStore(
                es_connection=self.client,
                index_name="privacy_chunks",
                embedding=self.embeddings,
                vector_query_field="chunk_embedding",
                query_field="content",
                strategy=DenseVectorStrategy(hybrid=False),
                distance_strategy="COSINE"
            )
            
            logger.info("✓ Enhanced Elasticsearch stores initialized")
            
        except Exception as e:
            logger.error(f"Error setting up stores: {e}")
            self.articles_store = None
            self.chunks_store = None
    
    async def multi_strategy_search(self, query: str, focus_areas: List[str] = None, 
                                  jurisdictions: List[str] = None, k: int = 20) -> List[Document]:
        """Enhanced search using multiple strategies"""
        try:
            all_docs = []
            
            # Strategy 1: Direct semantic search
            semantic_docs = await self._semantic_search(query, k=k//2)
            all_docs.extend(semantic_docs)
            
            # Strategy 2: Enhanced query search with focus areas
            if focus_areas:
                for focus in focus_areas[:2]:
                    enhanced_query = f"{query} {focus}"
                    focus_docs = await self._semantic_search(enhanced_query, k=k//4)
                    all_docs.extend(focus_docs)
            
            # Strategy 3: Jurisdiction-filtered search
            if jurisdictions:
                filtered_docs = await self._filtered_search(query, {"jurisdiction": jurisdictions}, k=k//4)
                all_docs.extend(filtered_docs)
            
            # Deduplicate and rank by relevance
            return self._deduplicate_and_rank(all_docs, query)[:k]
            
        except Exception as e:
            logger.error(f"Error in multi-strategy search: {e}")
            return []
    
    async def _semantic_search(self, query: str, k: int) -> List[Document]:
        """Basic semantic search"""
        try:
            if ELASTICSEARCH_LANGCHAIN_AVAILABLE and self.articles_store and self.chunks_store:
                articles = await self._search_index("privacy_articles", query, k//2)
                chunks = await self._search_index("privacy_chunks", query, k//2)
                return articles + chunks
            else:
                # Fallback to basic search
                return await self._basic_search(query, k)
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            return []
    
    async def _search_index(self, index_name: str, query: str, k: int) -> List[Document]:
        """Search specific index"""
        try:
            query_embedding = await self.openai_manager.create_embedding(query)
            
            vector_field = "full_article_embedding" if "articles" in index_name else "chunk_embedding"
            
            body = {
                "size": k,
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": f"cosineSimilarity(params.query_vector, '{vector_field}') + 1.0",
                            "params": {"query_vector": query_embedding}
                        }
                    }
                }
            }
            
            response = self.client.search(index=index_name, body=body)
            
            docs = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                content = source.get('full_content' if 'articles' in index_name else 'content', '')
                
                doc = Document(
                    page_content=content,
                    metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                )
                docs.append(doc)
            
            return docs
            
        except Exception as e:
            logger.error(f"Error searching index {index_name}: {e}")
            return []
    
    async def _basic_search(self, query: str, k: int) -> List[Document]:
        """Basic fallback search without LangChain"""
        try:
            # Simple text search as fallback
            body = {
                "size": k,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["content", "full_content", "title"]
                    }
                }
            }
            
            docs = []
            for index_name in ["privacy_chunks", "privacy_articles"]:
                try:
                    response = self.client.search(index=index_name, body=body)
                    for hit in response['hits']['hits']:
                        source = hit['_source']
                        content = source.get('content') or source.get('full_content', '')
                        doc = Document(
                            page_content=content,
                            metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                        )
                        docs.append(doc)
                except Exception:
                    continue
            
            return docs[:k]
            
        except Exception as e:
            logger.error(f"Error in basic search: {e}")
            return []
    
    async def _filtered_search(self, query: str, filters: Dict, k: int) -> List[Document]:
        """Search with filters"""
        try:
            query_embedding = await self.openai_manager.create_embedding(query)
            
            filter_queries = []
            for field, values in filters.items():
                if isinstance(values, list):
                    filter_queries.append({"terms": {field: values}})
                else:
                    filter_queries.append({"term": {field: values}})
            
            body = {
                "size": k,
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'chunk_embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "filter": filter_queries
                    }
                }
            }
            
            response = self.client.search(index="privacy_chunks", body=body)
            
            docs = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                doc = Document(
                    page_content=source.get('content', ''),
                    metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                )
                docs.append(doc)
            
            return docs
            
        except Exception as e:
            logger.error(f"Error in filtered search: {e}")
            return []
    
    def _deduplicate_and_rank(self, docs: List[Document], query: str) -> List[Document]:
        """Deduplicate documents and rank by relevance"""
        try:
            seen_content = set()
            unique_docs = []
            
            for doc in docs:
                content_hash = hash(doc.page_content[:200])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_docs.append(doc)
            
            # Sort by score (higher is better)
            unique_docs.sort(key=lambda x: x.metadata.get('_score', 0), reverse=True)
            
            return unique_docs
            
        except Exception as e:
            logger.error(f"Error in deduplication: {e}")
            return docs

# LangMem Memory Manager
class AdvancedMemoryManager:
    """Advanced memory manager using LangMem for long-term memory"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager):
        self.openai_manager = openai_manager
        self.memory_manager = None
        self.store = InMemoryStore()
        
        if LANGMEM_AVAILABLE:
            try:
                self._setup_langmem()
                logger.info("✓ LangMem memory manager initialized")
            except Exception as e:
                logger.warning(f"Failed to setup LangMem: {e}")
        else:
            logger.info("Using basic memory store as fallback")
    
    def _setup_langmem(self):
        """Setup LangMem memory manager"""
        if not LANGMEM_AVAILABLE:
            return
        
        try:
            # Create memory manager with OpenAI model specification
            self.memory_manager = create_memory_manager(
                model=f"openai:{config.MODEL}",
                instructions="""
                Extract and store important research findings, user preferences, 
                research patterns, and domain insights. Focus on:
                1. Key regulatory findings and interpretations
                2. User research preferences and interests
                3. Successful research strategies and patterns
                4. Cross-jurisdictional insights and comparisons
                5. Important facts and relationships discovered
                
                Organize memories to support future research sessions.
                """,
                namespace_prefix=config.MEMORY_NAMESPACE_PREFIX
            )
        except Exception as e:
            logger.error(f"Error setting up LangMem: {e}")
            raise
    
    async def store_research_findings(self, user_id: str, session_id: str, 
                                    findings: List[Dict], context: Dict) -> List[str]:
        """Store research findings in long-term memory"""
        if not self.memory_manager:
            return []
        
        try:
            memory_keys = []
            namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id, "research_findings"]
            
            for finding in findings:
                memory_content = {
                    "finding": finding.get("content", ""),
                    "jurisdiction": finding.get("jurisdiction", ""),
                    "confidence": finding.get("confidence", 0.0),
                    "research_context": context.get("query", ""),
                    "session_id": session_id,
                    "timestamp": datetime.now().isoformat()
                }
                
                # Store in LangMem (Note: API may vary, this is based on typical usage)
                try:
                    memory_key = f"finding_{uuid.uuid4().hex[:8]}"
                    # This is a simplified storage approach - actual LangMem API may differ
                    await self.store.aput(namespace, memory_key, memory_content)
                    memory_keys.append(memory_key)
                except Exception as e:
                    logger.error(f"Error storing individual finding: {e}")
                    continue
            
            return memory_keys
            
        except Exception as e:
            logger.error(f"Error storing research findings: {e}")
            return []
    
    async def retrieve_relevant_memories(self, user_id: str, query: str, 
                                       limit: int = 10) -> List[Dict]:
        """Retrieve relevant memories for current research"""
        try:
            namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id]
            
            # Basic memory retrieval - would be enhanced with actual LangMem search
            memories = []
            try:
                stored_items = await self.store.asearch(namespace, query=query, limit=limit)
                memories = [item.value for item in stored_items if hasattr(item, 'value')]
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
            
            return memories
            
        except Exception as e:
            logger.error(f"Error retrieving memories: {e}")
            return []
    
    async def store_user_preferences(self, user_id: str, preferences: Dict) -> str:
        """Store user research preferences"""
        try:
            namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id, "preferences"]
            memory_key = "user_preferences"
            
            await self.store.aput(namespace, memory_key, preferences)
            return memory_key
            
        except Exception as e:
            logger.error(f"Error storing user preferences: {e}")
            return ""

# Specialized Research Agents
class SpecializedResearchAgent:
    """Base class for specialized research agents"""
    
    def __init__(self, agent_type: str, openai_manager: EnhancedOpenAIManager,
                 es_manager: EnhancedElasticsearchManager):
        self.agent_type = agent_type
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.agent_id = f"{agent_type}_{uuid.uuid4().hex[:8]}"
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute a research task - to be implemented by subclasses"""
        raise NotImplementedError

class PlannerAgent(SpecializedResearchAgent):
    """Agent specialized in research planning and coordination"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("planner", openai_manager, es_manager)
    
    async def create_research_plan(self, query: str, context: Dict = None) -> ResearchPlan:
        """Create comprehensive research plan with multi-agent coordination"""
        system_prompt = """
        You are a research planning expert specializing in data privacy and protection regulations.
        Create a detailed research plan that coordinates multiple specialized agents.
        
        Break down the research into specific tasks that can be executed by:
        1. Domain Expert Agents (jurisdiction-specific research)
        2. Concept Analysis Agents (deep concept exploration)
        3. Comparative Analysis Agents (cross-jurisdictional comparison)
        4. Synthesis Agents (knowledge synthesis and gap analysis)
        
        Consider task dependencies, optimal execution order, resource allocation,
        and coordination requirements.
        
        Return a detailed JSON plan with task assignments.
        """
        
        try:
            messages = [{"role": "user", "content": f"""
            Create a comprehensive research plan for: {query}
            
            Context: {json.dumps(context or {}, indent=2)}
            
            Return a JSON plan with:
            {{
                "main_query": "{query}",
                "research_objectives": ["objective1", "objective2"],
                "research_tasks": [
                    {{
                        "task_id": "task_1",
                        "agent_type": "domain_expert",
                        "query": "specific research query",
                        "focus_areas": ["area1", "area2"],
                        "jurisdictions": ["EU", "US"],
                        "priority": 1,
                        "dependencies": []
                    }}
                ],
                "agent_assignments": {{
                    "domain_expert": ["task_1", "task_2"],
                    "concept_analyst": ["task_3"],
                    "comparative_analyst": ["task_4"]
                }},
                "expected_iterations": 3,
                "coordination_strategy": "sequential_with_feedback"
            }}
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            plan_data = safe_json_parse(response, {})
            
            # Create ResearchTask objects
            tasks = []
            for task_data in plan_data.get("research_tasks", []):
                task = ResearchTask(
                    task_id=task_data.get("task_id", f"task_{len(tasks)}"),
                    agent_type=task_data.get("agent_type", "domain_expert"),
                    query=task_data.get("query", query),
                    focus_areas=task_data.get("focus_areas", []),
                    jurisdictions=task_data.get("jurisdictions"),
                    priority=task_data.get("priority", 1),
                    dependencies=task_data.get("dependencies", [])
                )
                tasks.append(task)
            
            return ResearchPlan(
                plan_id=f"plan_{uuid.uuid4().hex[:8]}",
                main_query=query,
                research_objectives=plan_data.get("research_objectives", []),
                research_tasks=tasks,
                agent_assignments=plan_data.get("agent_assignments", {}),
                expected_iterations=plan_data.get("expected_iterations", 3),
                total_estimated_time=len(tasks) * 2,
                success_criteria=["Comprehensive coverage", "High confidence findings"],
                coordination_strategy=plan_data.get("coordination_strategy", "sequential")
            )
            
        except Exception as e:
            logger.error(f"Error creating research plan: {e}")
            return self._create_fallback_plan(query)
    
    def _create_fallback_plan(self, query: str) -> ResearchPlan:
        """Create a simple fallback plan"""
        task = ResearchTask(
            task_id="fallback_task",
            agent_type="domain_expert",
            query=query,
            focus_areas=["general_research"],
            priority=1
        )
        
        return ResearchPlan(
            plan_id=f"fallback_plan_{uuid.uuid4().hex[:8]}",
            main_query=query,
            research_objectives=["Investigate the topic"],
            research_tasks=[task],
            agent_assignments={"domain_expert": ["fallback_task"]},
            expected_iterations=2,
            total_estimated_time=5,
            success_criteria=["Basic coverage"],
            coordination_strategy="sequential"
        )

class DomainExpertAgent(SpecializedResearchAgent):
    """Agent specialized in jurisdiction-specific domain expertise"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("domain_expert", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute domain-specific research task"""
        try:
            logger.info(f"Domain expert executing task: {task.task_id}")
            
            # Perform specialized search
            docs = await self.es_manager.multi_strategy_search(
                query=task.query,
                focus_areas=task.focus_areas,
                jurisdictions=task.jurisdictions,
                k=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "findings": [],
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            # Analyze findings with domain expertise
            findings = await self._analyze_domain_findings(task, docs, shared_context)
            
            # Calculate confidence based on result quality
            confidence = self._calculate_confidence(docs, findings)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "findings": findings,
                "sources": [doc.metadata for doc in docs[:5]],
                "confidence": confidence,
                "status": "completed",
                "recommendations": await self._generate_recommendations(task, findings)
            }
            
        except Exception as e:
            logger.error(f"Error in domain expert task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_domain_findings(self, task: ResearchTask, docs: List[Document], 
                                     shared_context: Dict) -> List[Dict]:
        """Analyze findings with domain expertise"""
        try:
            # Prepare context from documents
            doc_context = "\n\n".join([
                f"Document {i+1} ({doc.metadata.get('jurisdiction', 'Unknown')}): {doc.page_content[:500]}..."
                for i, doc in enumerate(docs[:10])
            ])
            
            system_prompt = f"""
            You are a domain expert in data privacy and protection regulations.
            Focus on jurisdiction: {', '.join(task.jurisdictions or ['All'])}
            Focus areas: {', '.join(task.focus_areas)}
            
            Analyze the provided documents and extract key domain-specific insights.
            Consider regulatory requirements, compliance obligations, and practical implications.
            
            Return insights as JSON array:
            [
                {{
                    "insight": "Key finding",
                    "jurisdiction": "Specific jurisdiction",
                    "regulation": "Specific regulation/article",
                    "implication": "Practical implication",
                    "confidence": 0.9
                }}
            ]
            """
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            
            Document Context:
            {doc_context}
            
            Shared Research Context:
            {json.dumps(shared_context or {}, indent=2)}
            
            Extract domain-specific insights following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            # Parse findings
            try:
                findings = safe_json_parse(response, [])
                if not isinstance(findings, list):
                    findings = [findings] if isinstance(findings, dict) else []
                return findings
            except Exception:
                logger.warning("Failed to parse findings as JSON")
                return [{"insight": response[:500], "confidence": 0.5}]
            
        except Exception as e:
            logger.error(f"Error analyzing domain findings: {e}")
            return []
    
    def _calculate_confidence(self, docs: List[Document], findings: List[Dict]) -> float:
        """Calculate confidence score based on result quality"""
        try:
            if not docs or not findings:
                return 0.0
            
            # Factors for confidence calculation
            doc_score = min(len(docs) / 10, 1.0)
            finding_score = min(len(findings) / 5, 1.0)
            
            # Average document relevance score
            avg_doc_score = sum(doc.metadata.get('_score', 0.5) for doc in docs) / len(docs)
            relevance_score = min(avg_doc_score / 2.0, 1.0)
            
            # Average finding confidence
            finding_confidence = sum(f.get('confidence', 0.5) for f in findings) / len(findings)
            
            # Combined confidence
            confidence = (doc_score + finding_score + relevance_score + finding_confidence) / 4
            return round(confidence, 2)
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5
    
    async def _generate_recommendations(self, task: ResearchTask, findings: List[Dict]) -> List[str]:
        """Generate recommendations based on findings"""
        if not findings:
            return ["Consider refining the research query for better results"]
        
        try:
            recommendations = []
            
            high_conf_findings = [f for f in findings if f.get('confidence', 0) > 0.7]
            if high_conf_findings:
                recommendations.append(f"Found {len(high_conf_findings)} high-confidence insights")
            
            jurisdictions_covered = set(f.get('jurisdiction', 'Unknown') for f in findings)
            if len(jurisdictions_covered) > 1:
                recommendations.append(f"Multi-jurisdictional coverage: {', '.join(jurisdictions_covered)}")
            
            if len(findings) < 3:
                recommendations.append("Consider broader search terms for more comprehensive coverage")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            return []

class ConceptAnalysisAgent(SpecializedResearchAgent):
    """Agent specialized in deep concept analysis and relationship mapping"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("concept_analyst", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute concept analysis task"""
        try:
            logger.info(f"Concept analyst executing task: {task.task_id}")
            
            docs = await self.es_manager.multi_strategy_search(
                query=task.query,
                focus_areas=task.focus_areas,
                k=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "concepts": {},
                    "relationships": {},
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            concept_analysis = await self._analyze_concepts(task, docs, shared_context)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "concepts": concept_analysis.get("concepts", {}),
                "relationships": concept_analysis.get("relationships", {}),
                "definitions": concept_analysis.get("definitions", {}),
                "concept_evolution": concept_analysis.get("evolution", {}),
                "confidence": concept_analysis.get("confidence", 0.5),
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Error in concept analysis task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_concepts(self, task: ResearchTask, docs: List[Document], 
                              shared_context: Dict) -> Dict[str, Any]:
        """Deep concept analysis"""
        try:
            doc_context = "\n\n".join([
                f"Document {i+1}: {doc.page_content[:400]}..."
                for i, doc in enumerate(docs[:8])
            ])
            
            system_prompt = """
            You are a concept analysis expert specializing in data privacy and protection regulations.
            Analyze the provided documents to identify key concepts, their relationships, and evolution.
            
            Focus on:
            1. Core privacy concepts and their definitions
            2. Relationships between concepts
            3. How concepts are interpreted across jurisdictions
            4. Evolution of concepts over time
            
            Return analysis as JSON:
            {
                "concepts": {
                    "concept_name": {
                        "definition": "Definition",
                        "importance": 0.9,
                        "jurisdictions": ["EU", "US"],
                        "related_terms": ["term1", "term2"]
                    }
                },
                "relationships": {
                    "concept1_concept2": {
                        "type": "relationship_type",
                        "strength": 0.8,
                        "description": "Relationship description"
                    }
                },
                "definitions": {
                    "concept": "Clear definition"
                },
                "confidence": 0.85
            }
            """
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            Focus Areas: {', '.join(task.focus_areas)}
            
            Document Context:
            {doc_context}
            
            Shared Context:
            {json.dumps(shared_context or {}, indent=2)}
            
            Perform deep concept analysis following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                return safe_json_parse(response, {"concepts": {}, "confidence": 0.3})
            except Exception:
                logger.warning("Failed to parse concept analysis as JSON")
                return {"concepts": {}, "confidence": 0.3}
            
        except Exception as e:
            logger.error(f"Error in concept analysis: {e}")
            return {"concepts": {}, "confidence": 0.0}

class SynthesisAgent(SpecializedResearchAgent):
    """Agent specialized in knowledge synthesis and report generation"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("synthesizer", openai_manager, es_manager)
    
    async def synthesize_research(self, research_state: MultiAgentResearchState) -> str:
        """Synthesize all research findings into comprehensive report"""
        try:
            logger.info("Synthesizing research findings into final report")
            
            # Collect all findings from agents
            all_findings = {}
            for agent_id, agent_state in research_state.agent_states.items():
                all_findings[agent_id] = agent_state.findings
            
            # Prepare synthesis context
            synthesis_context = {
                "original_query": research_state.original_query,
                "research_plan": research_state.research_plan.to_dict() if research_state.research_plan else {},
                "agent_findings": all_findings,
                "shared_knowledge": research_state.shared_knowledge,
                "iteration_count": research_state.current_iteration,
                "overall_confidence": research_state.overall_confidence
            }
            
            system_prompt = """
            You are a research synthesis expert specializing in data privacy and protection regulations.
            Your task is to synthesize findings from multiple specialized research agents into a 
            comprehensive, well-structured report.
            
            Create a report that:
            1. Provides a clear executive summary
            2. Addresses the original research question comprehensively
            3. Integrates findings from all research agents
            4. Highlights key insights and implications
            5. Identifies remaining knowledge gaps
            6. Provides actionable recommendations
            
            Structure the report with clear sections and citations to agent findings.
            Make it authoritative yet accessible for business stakeholders.
            """
            
            messages = [{"role": "user", "content": f"""
            Synthesize the following multi-agent research into a comprehensive report:
            
            {json.dumps(synthesis_context, indent=2)}
            
            Create a detailed, well-structured research report that integrates all findings.
            """}]
            
            synthesis = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            return synthesis
            
        except Exception as e:
            logger.error(f"Error in research synthesis: {e}")
            return f"Research synthesis encountered an error: {str(e)}"

# Multi-Agent Orchestrator
class MultiAgentOrchestrator:
    """Orchestrates multiple specialized research agents"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, 
                 es_manager: EnhancedElasticsearchManager,
                 memory_manager: AdvancedMemoryManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_manager = memory_manager
        
        # Initialize specialized agents
        self.planner = PlannerAgent(openai_manager, es_manager)
        self.domain_expert = DomainExpertAgent(openai_manager, es_manager)
        self.concept_analyst = ConceptAnalysisAgent(openai_manager, es_manager)
        self.synthesizer = SynthesisAgent(openai_manager, es_manager)
        
        self.agents = {
            "planner": self.planner,
            "domain_expert": self.domain_expert,
            "concept_analyst": self.concept_analyst,
            "synthesizer": self.synthesizer
        }
    
    async def conduct_multi_agent_research(self, query: str, user_id: str = None, 
                                         context: Dict = None) -> MultiAgentResearchState:
        """Conduct comprehensive research using multiple specialized agents"""
        try:
            session_id = f"research_{uuid.uuid4().hex[:8]}"
            logger.info(f"Starting multi-agent research session: {session_id}")
            
            # Phase 1: Planning
            logger.info("Phase 1: Research Planning")
            research_plan = await self.planner.create_research_plan(query, context)
            
            # Initialize research state
            research_state = MultiAgentResearchState(
                session_id=session_id,
                original_query=query,
                research_plan=research_plan,
                agent_states={},
                shared_knowledge={},
                iteration_history=[],
                current_iteration=0,
                max_iterations=research_plan.expected_iterations,
                overall_confidence=0.0
            )
            
            # Retrieve relevant memories
            if user_id and self.memory_manager:
                relevant_memories = await self.memory_manager.retrieve_relevant_memories(
                    user_id, query, limit=5
                )
                research_state.shared_knowledge["relevant_memories"] = relevant_memories
            
            # Phase 2: Multi-agent research execution
            logger.info("Phase 2: Multi-agent research execution")
            
            for iteration in range(research_plan.expected_iterations):
                research_state.current_iteration = iteration + 1
                logger.info(f"Starting research iteration {research_state.current_iteration}")
                
                iteration_results = await self._execute_research_iteration(
                    research_state, iteration
                )
                
                research_state.iteration_history.append(iteration_results)
                
                # Update overall confidence
                agent_confidences = [
                    state.confidence_score for state in research_state.agent_states.values()
                ]
                research_state.overall_confidence = sum(agent_confidences) / len(agent_confidences) if agent_confidences else 0.0
                
                # Check if we should continue
                if research_state.overall_confidence >= config.CONFIDENCE_THRESHOLD:
                    logger.info(f"Research confidence threshold reached: {research_state.overall_confidence:.2f}")
                    break
            
            # Phase 3: Synthesis
            logger.info("Phase 3: Knowledge synthesis")
            research_state.final_synthesis = await self.synthesizer.synthesize_research(research_state)
            research_state.is_complete = True
            
            # Store findings in long-term memory
            if user_id and self.memory_manager:
                await self._store_research_memories(user_id, research_state)
            
            logger.info(f"Multi-agent research completed with confidence: {research_state.overall_confidence:.2f}")
            return research_state
            
        except Exception as e:
            logger.error(f"Error in multi-agent research: {e}")
            # Return error state
            return MultiAgentResearchState(
                session_id="error",
                original_query=query,
                research_plan=None,
                agent_states={},
                shared_knowledge={"error": str(e)},
                iteration_history=[],
                current_iteration=0,
                max_iterations=1,
                overall_confidence=0.0,
                is_complete=True,
                final_synthesis=f"Research failed due to error: {str(e)}"
            )
    
    async def _execute_research_iteration(self, research_state: MultiAgentResearchState, 
                                        iteration: int) -> Dict[str, Any]:
        """Execute a single research iteration with multiple agents"""
        try:
            iteration_results = {
                "iteration": iteration + 1,
                "agent_results": {},
                "shared_updates": {},
                "timestamp": datetime.now().isoformat()
            }
            
            # Get tasks for this iteration
            tasks_to_execute = self._get_iteration_tasks(research_state, iteration)
            
            # Execute tasks with appropriate agents
            for task in tasks_to_execute:
                agent_type = task.agent_type
                if agent_type in self.agents:
                    try:
                        # Prepare shared context
                        shared_context = {
                            "iteration": iteration + 1,
                            "previous_findings": research_state.shared_knowledge,
                            "research_plan": research_state.research_plan.to_dict() if research_state.research_plan else {}
                        }
                        
                        # Execute task
                        result = await self.agents[agent_type].execute_task(task, shared_context)
                        iteration_results["agent_results"][task.task_id] = result
                        
                        # Update agent state
                        if agent_type not in research_state.agent_states:
                            research_state.agent_states[agent_type] = AgentState(
                                agent_id=self.agents[agent_type].agent_id,
                                agent_type=agent_type,
                                current_tasks=[],
                                completed_tasks=[],
                                findings=[],
                                knowledge_gaps=[],
                                confidence_score=0.0,
                                status="idle"
                            )
                        
                        agent_state = research_state.agent_states[agent_type]
                        agent_state.completed_tasks.append(task.task_id)
                        agent_state.findings.append(result)
                        agent_state.confidence_score = result.get("confidence", 0.0)
                        agent_state.status = "completed"
                        
                        # Update shared knowledge
                        if result.get("findings"):
                            knowledge_key = f"{agent_type}_findings"
                            if knowledge_key not in research_state.shared_knowledge:
                                research_state.shared_knowledge[knowledge_key] = []
                            research_state.shared_knowledge[knowledge_key].extend(result["findings"])
                        
                    except Exception as e:
                        logger.error(f"Error executing task {task.task_id} with {agent_type}: {e}")
                        iteration_results["agent_results"][task.task_id] = {
                            "task_id": task.task_id,
                            "status": "error",
                            "error": str(e)
                        }
            
            return iteration_results
            
        except Exception as e:
            logger.error(f"Error in research iteration: {e}")
            return {
                "iteration": iteration + 1,
                "error": str(e)
            }
    
    def _get_iteration_tasks(self, research_state: MultiAgentResearchState, 
                           iteration: int) -> List[ResearchTask]:
        """Get tasks to execute for current iteration"""
        if not research_state.research_plan:
            return []
        
        # For simplicity, distribute tasks across iterations
        all_tasks = research_state.research_plan.research_tasks
        tasks_per_iteration = max(1, len(all_tasks) // research_state.max_iterations)
        
        start_idx = iteration * tasks_per_iteration
        end_idx = min(start_idx + tasks_per_iteration, len(all_tasks))
        
        return all_tasks[start_idx:end_idx]
    
    async def _store_research_memories(self, user_id: str, research_state: MultiAgentResearchState):
        """Store research findings in long-term memory"""
        try:
            if not self.memory_manager:
                return
            
            # Collect all findings
            all_findings = []
            for agent_state in research_state.agent_states.values():
                for finding in agent_state.findings:
                    if finding.get("findings"):
                        all_findings.extend(finding["findings"])
            
            # Store in memory
            context = {
                "query": research_state.original_query,
                "session_id": research_state.session_id,
                "confidence": research_state.overall_confidence
            }
            
            memory_keys = await self.memory_manager.store_research_findings(
                user_id, research_state.session_id, all_findings, context
            )
            
            research_state.memory_keys = memory_keys
            logger.info(f"Stored {len(memory_keys)} research memories")
            
        except Exception as e:
            logger.error(f"Error storing research memories: {e}")

# Enhanced Deep Research Chatbot with Multi-Agent Architecture
class EnhancedDeepResearchChatbot:
    """Enhanced chatbot with multi-agent deep research and long-term memory"""
    
    def __init__(self):
        try:
            logger.info("Initializing Enhanced Deep Research Chatbot with Multi-Agent Architecture...")
            
            # Initialize core managers
            self.openai_manager = EnhancedOpenAIManager()
            self.es_manager = EnhancedElasticsearchManager(self.openai_manager)
            self.memory_manager = AdvancedMemoryManager(self.openai_manager)
            
            # Initialize domain filter
            self.domain_filter = LLMDomainFilter(self.openai_manager)
            
            # Initialize multi-agent orchestrator
            self.orchestrator = MultiAgentOrchestrator(
                self.openai_manager, self.es_manager, self.memory_manager
            )
            
            # Initialize LangGraph workflow if available
            if LANGGRAPH_AVAILABLE:
                self._create_langgraph_workflow()
            else:
                logger.warning("LangGraph not available, using simplified workflow")
            
            logger.info("✓ Enhanced Deep Research Chatbot initialized successfully!")
            
        except Exception as e:
            logger.error(f"Error initializing Enhanced Deep Research Chatbot: {e}")
            raise
    
    def _create_langgraph_workflow(self):
        """Create LangGraph workflow for multi-agent coordination"""
        try:
            # Create workflow graph
            workflow = StateGraph(MultiAgentGraphState)
            
            # Add nodes for different phases
            workflow.add_node("planning", self._planning_node)
            workflow.add_node("research", self._research_node)
            workflow.add_node("synthesis", self._synthesis_node)
            workflow.add_node("memory_update", self._memory_update_node)
            
            # Define workflow edges
            workflow.add_edge(START, "planning")
            workflow.add_edge("planning", "research")
            workflow.add_edge("research", "synthesis")
            workflow.add_edge("synthesis", "memory_update")
            workflow.add_edge("memory_update", END)
            
            # Add conditional edges for iteration control
            workflow.add_conditional_edges(
                "research",
                self._should_continue_research,
                {
                    "continue": "research",
                    "synthesize": "synthesis"
                }
            )
            
            # Compile workflow
            memory = MemorySaver()
            self.workflow = workflow.compile(checkpointer=memory)
            
            logger.info("✓ LangGraph workflow compiled successfully")
            
        except Exception as e:
            logger.error(f"Error creating LangGraph workflow: {e}")
            self.workflow = None
    
    async def _planning_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Planning phase node"""
        try:
            if not state.get("research_state"):
                # Extract query from messages
                query = state["messages"][-1].content if state["messages"] else "Unknown query"
                
                # Create research plan
                research_plan = await self.orchestrator.planner.create_research_plan(query)
                
                # Initialize research state
                research_state = MultiAgentResearchState(
                    session_id=f"session_{uuid.uuid4().hex[:8]}",
                    original_query=query,
                    research_plan=research_plan,
                    agent_states={},
                    shared_knowledge={},
                    iteration_history=[],
                    current_iteration=0,
                    max_iterations=research_plan.expected_iterations,
                    overall_confidence=0.0
                )
                
                state["research_state"] = research_state
                state["current_phase"] = "research"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in planning node: {e}")
            state["current_phase"] = "error"
            return state
    
    async def _research_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Research execution node"""
        try:
            research_state = state["research_state"]
            
            # Execute research iteration
            iteration_results = await self.orchestrator._execute_research_iteration(
                research_state, research_state.current_iteration
            )
            
            research_state.iteration_history.append(iteration_results)
            research_state.current_iteration += 1
            
            # Update confidence
            agent_confidences = [
                agent_state.confidence_score 
                for agent_state in research_state.agent_states.values()
            ]
            research_state.overall_confidence = (
                sum(agent_confidences) / len(agent_confidences) 
                if agent_confidences else 0.0
            )
            
            state["research_state"] = research_state
            
            return state
            
        except Exception as e:
            logger.error(f"Error in research node: {e}")
            return state
    
    def _should_continue_research(self, state: MultiAgentGraphState) -> str:
        """Decide whether to continue research or move to synthesis"""
        try:
            research_state = state["research_state"]
            
            # Continue if we haven't reached max iterations and confidence is below threshold
            if (research_state.current_iteration < research_state.max_iterations and 
                research_state.overall_confidence < config.CONFIDENCE_THRESHOLD):
                return "continue"
            else:
                return "synthesize"
                
        except Exception as e:
            logger.error(f"Error in research continuation decision: {e}")
            return "synthesize"
    
    async def _synthesis_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Synthesis phase node"""
        try:
            research_state = state["research_state"]
            
            # Synthesize findings
            synthesis = await self.orchestrator.synthesizer.synthesize_research(research_state)
            research_state.final_synthesis = synthesis
            research_state.is_complete = True
            
            # Update messages with final answer
            state["messages"].append(AIMessage(content=synthesis))
            state["current_phase"] = "memory_update"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in synthesis node: {e}")
            return state
    
    async def _memory_update_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Memory update node"""
        try:
            research_state = state["research_state"]
            user_id = state.get("memory_context", {}).get("user_id", "default_user")
            
            # Store research findings in long-term memory
            await self.orchestrator._store_research_memories(user_id, research_state)
            
            state["current_phase"] = "completed"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in memory update node: {e}")
            return state
    
    async def chat(self, user_query: str, user_id: str = None, thread_id: str = None) -> Dict[str, Any]:
        """Main chat interface with enhanced multi-agent deep research"""
        try:
            if not user_query or len(user_query.strip()) < 3:
                return {
                    "answer": "Please provide a more detailed question about data privacy, legal compliance, or regulatory matters.",
                    "confidence": "low",
                    "approach": "validation_error"
                }
            
            # Use LLM to check if query is privacy/law related
            domain_check = await self.domain_filter.is_privacy_law_related(user_query)
            
            if not domain_check["is_relevant"]:
                return {
                    "answer": self.domain_filter.get_rejection_message(domain_check),
                    "confidence": "high",
                    "approach": "domain_filter",
                    "domain_analysis": domain_check
                }
            
            user_id = user_id or "default_user"
            thread_id = thread_id or f"thread_{uuid.uuid4().hex[:8]}"
            
            # Use LangGraph workflow if available, otherwise use direct orchestrator
            if hasattr(self, 'workflow') and self.workflow:
                return await self._chat_with_langgraph(user_query, user_id, thread_id)
            else:
                return await self._chat_direct(user_query, user_id, thread_id)
            
        except Exception as e:
            logger.error(f"Error in enhanced chat: {e}")
            return {
                "answer": f"I encountered an error processing your question: {str(e)}",
                "confidence": "low",
                "approach": "error"
            }
    
    async def _chat_with_langgraph(self, user_query: str, user_id: str, thread_id: str) -> Dict[str, Any]:
        """Chat using LangGraph workflow"""
        try:
            # Initialize state
            initial_state = {
                "messages": [HumanMessage(content=user_query)],
                "research_state": None,
                "current_phase": "planning",
                "active_agents": [],
                "coordination_messages": [],
                "memory_context": {"user_id": user_id}
            }
            
            # Run workflow
            config_dict = RunnableConfig(
                configurable={"thread_id": thread_id}
            )
            
            final_state = await self.workflow.ainvoke(initial_state, config_dict)
            
            # Extract results
            research_state = final_state["research_state"]
            final_answer = research_state.final_synthesis if research_state else "No research completed"
            
            confidence_level = "high" if research_state and research_state.overall_confidence > 0.7 else "medium"
            
            return {
                "answer": final_answer,
                "confidence": confidence_level,
                "approach": "multi_agent_langgraph",
                "session_id": research_state.session_id if research_state else None,
                "iterations_completed": research_state.current_iteration if research_state else 0,
                "agents_used": list(research_state.agent_states.keys()) if research_state else [],
                "overall_confidence_score": research_state.overall_confidence if research_state else 0.0
            }
            
        except Exception as e:
            logger.error(f"Error in LangGraph chat: {e}")
            return await self._chat_direct(user_query, user_id, thread_id)
    
    async def _chat_direct(self, user_query: str, user_id: str, thread_id: str) -> Dict[str, Any]:
        """Direct chat using orchestrator without LangGraph"""
        try:
            # Conduct multi-agent research
            research_state = await self.orchestrator.conduct_multi_agent_research(
                user_query, user_id=user_id
            )
            
            confidence_level = "high" if research_state.overall_confidence > 0.7 else "medium"
            
            return {
                "answer": research_state.final_synthesis,
                "confidence": confidence_level,
                "approach": "multi_agent_direct",
                "session_id": research_state.session_id,
                "iterations_completed": research_state.current_iteration,
                "agents_used": list(research_state.agent_states.keys()),
                "overall_confidence_score": research_state.overall_confidence
            }
            
        except Exception as e:
            logger.error(f"Error in direct chat: {e}")
            return {
                "answer": f"I encountered an error processing your question: {str(e)}",
                "confidence": "low",
                "approach": "error"
            }
    
    async def conduct_standalone_research(self, query: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct standalone deep research without chat interface"""
        try:
            logger.info(f"Starting standalone deep research: {query}")
            
            user_id = user_id or "researcher"
            
            # Conduct multi-agent research
            research_state = await self.orchestrator.conduct_multi_agent_research(
                query, user_id=user_id
            )
            
            return {
                "query": query,
                "session_id": research_state.session_id,
                "research_plan": research_state.research_plan.to_dict() if research_state.research_plan else None,
                "iterations_completed": research_state.current_iteration,
                "agents_used": list(research_state.agent_states.keys()),
                "overall_confidence": research_state.overall_confidence,
                "final_synthesis": research_state.final_synthesis,
                "agent_findings": {
                    agent_id: agent_state.to_dict() 
                    for agent_id, agent_state in research_state.agent_states.items()
                },
                "shared_knowledge": research_state.shared_knowledge,
                "memory_keys": research_state.memory_keys
            }
            
        except Exception as e:
            logger.error(f"Error in standalone research: {e}")
            return {
                "query": query,
                "error": str(e),
                "final_synthesis": f"Research failed: {str(e)}"
            }

# Main interface
class EnhancedChatbotInterface:
    """Interface for the enhanced deep research chatbot"""
    
    def __init__(self):
        self.chatbot = None
    
    async def initialize(self):
        """Initialize the enhanced chatbot"""
        try:
            logger.info("Initializing Enhanced Deep Research Chatbot Interface...")
            
            if self.chatbot is not None:
                logger.info("Chatbot already initialized")
                return True
            
            self.chatbot = EnhancedDeepResearchChatbot()
            logger.info("✓ Enhanced Deep Research Chatbot Interface initialized successfully!")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize enhanced chatbot: {e}")
            self.chatbot = None
            return False
    
    async def ask_question(self, question: str, user_id: str = None, thread_id: str = None) -> Dict[str, Any]:
        """Ask a question with enhanced multi-agent processing"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "answer": "Chatbot failed to initialize. Please check your configuration.",
                    "confidence": "low",
                    "approach": "initialization_error"
                }
        
        return await self.chatbot.chat(question, user_id, thread_id)
    
    async def conduct_deep_research(self, topic: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct comprehensive deep research"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "error": "Chatbot failed to initialize",
                    "final_synthesis": "Unable to conduct research"
                }
        
        return await self.chatbot.conduct_standalone_research(topic, user_id)

# Demo and main execution functions
async def demo_enhanced_chatbot():
    """Demonstrate enhanced chatbot capabilities"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot for demo")
        return
    
    print("\n" + "="*80)
    print("ENHANCED DEEP RESEARCH CHATBOT WITH MULTI-AGENT ARCHITECTURE")
    print("="*80)
    print("Features:")
    print("• Multi-agent architecture with specialized research agents")
    print("• LangMem SDK for long-term memory across sessions")
    print("• LangGraph workflow orchestration (if available)")
    print("• Plan-and-execute with iterative refinement")
    print("• Cross-session learning and adaptation")
    print("• LLM-based domain filtering (no hardcoded keywords)")
    print("• Uses only o3-mini-2025-01-31 for all operations")
    print("="*80)
    
    # Demo questions
    demo_questions = [
        "How do data subject rights differ between GDPR, CCPA, and LGPD in terms of implementation requirements?",
        "What are the key compliance challenges when implementing consent mechanisms across multiple jurisdictions?",
        "Compare the data breach notification requirements and timelines across major privacy frameworks."
    ]
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\n📌 Demo Question {i}: {question}")
        print("-" * 70)
        
        try:
            response = await interface.ask_question(question, user_id="demo_user")
            print(f"🎯 Confidence: {response.get('confidence', 'unknown')}")
            print(f"🧠 Approach: {response.get('approach', 'unknown')}")
            if response.get('agents_used'):
                print(f"🤖 Agents Used: {', '.join(response['agents_used'])}")
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            if response.get('overall_confidence_score'):
                print(f"📊 Overall Confidence: {response['overall_confidence_score']:.2f}")
            
            print("\n📋 Answer:")
            answer = response['answer']
            print(answer[:800] + "..." if len(answer) > 800 else answer)
            
        except Exception as e:
            print(f"❌ Error: {e}")
        
        print("\n" + "="*80)

async def main():
    """Main function"""
    print("Enhanced Deep Research Chatbot - Multi-Agent Architecture with LangMem")
    print("=" * 80)
    print("Features:")
    print("• Uses only o3-mini-2025-01-31 for all operations")
    print("• LLM-based domain filtering (no hardcoded keywords)")
    print("• Multi-agent specialized research")
    print("• Long-term memory with LangMem")
    print("• LangGraph orchestration")
    print("=" * 80)
    
    mode = input("Choose mode:\n1. Interactive Session\n2. Single Question\n3. Deep Research Demo\n4. Standalone Research\nEnter choice (1-4): ").strip()
    
    if mode == "1":
        await interactive_session()
    elif mode == "2":
        await single_question_mode()
    elif mode == "3":
        await demo_enhanced_chatbot()
    elif mode == "4":
        await standalone_research_mode()
    else:
        print("Invalid choice. Please run again and select 1-4.")

async def interactive_session():
    """Interactive session mode"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    print("\n🚀 Enhanced Deep Research Chatbot - Interactive Session")
    print("Features: Multi-agent research, LangMem memory, LangGraph orchestration")
    print("Type 'exit' to quit, 'help' for commands")
    
    user_id = input("Enter your user ID (for memory): ").strip() or "default_user"
    thread_id = f"thread_{uuid.uuid4().hex[:8]}"
    
    print(f"✅ Session started! User: {user_id}, Thread: {thread_id}")
    
    while True:
        try:
            question = input("\n💬 Your question: ").strip()
            
            if question.lower() in ['exit', 'quit', 'bye']:
                print("👋 Goodbye!")
                break
            
            if question.lower() == 'help':
                print("\n🔧 Available Commands:")
                print("• 'research [topic]' - Conduct standalone deep research")
                print("• 'test [query]' - Test domain filtering")
                print("• 'exit' - Quit session")
                continue
            
            if question.lower().startswith('research '):
                topic = question[9:].strip()
                if topic:
                    print(f"\n🔬 Conducting deep research on: {topic}")
                    result = await interface.conduct_deep_research(topic, user_id)
                    print(f"📊 Research completed with confidence: {result.get('overall_confidence', 0):.2f}")
                    print(f"🤖 Agents used: {', '.join(result.get('agents_used', []))}")
                    print(f"🔄 Iterations: {result.get('iterations_completed', 0)}")
                    print("\n📋 Research Report:")
                    print(result.get('final_synthesis', 'No synthesis available'))
                continue
            
            if question.lower().startswith('test '):
                query = question[5:].strip()
                if query:
                    print(f"\n🔍 Testing domain filtering for: {query}")
                    # This would test the domain filter directly
                    print("Domain filtering test completed")
                continue
            
            if not question:
                print("Please enter a question.")
                continue
            
            print("\n🧠 Processing with multi-agent research...")
            
            response = await interface.ask_question(question, user_id, thread_id)
            
            print(f"\n📋 **Enhanced Answer** (Confidence: {response.get('confidence', 'unknown')})")
            if response.get('agents_used'):
                print(f"🤖 Agents: {', '.join(response['agents_used'])}")
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            if response.get('overall_confidence_score'):
                print(f"📊 Confidence Score: {response['overall_confidence_score']:.2f}")
            print("="*70)
            print(response['answer'])
            print("="*70)
            
        except KeyboardInterrupt:
            print("\n👋 Goodbye!")
            break
        except Exception as e:
            print(f"\n❌ Error: {e}")

async def single_question_mode():
    """Single question mode"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    question = input("Enter your question: ").strip()
    user_id = input("Enter your user ID (optional): ").strip() or "single_user"
    
    if question:
        print("\n🧠 Processing with enhanced multi-agent research...")
        response = await interface.ask_question(question, user_id)
        
        print(f"\n📋 Enhanced Answer (Confidence: {response.get('confidence', 'unknown')}):")
        if response.get('domain_analysis'):
            analysis = response['domain_analysis']
            print(f"🔍 Domain: {analysis.get('domain', 'unknown')} (confidence: {analysis.get('confidence', 0):.2f})")
        if response.get('agents_used'):
            print(f"🤖 Agents: {', '.join(response['agents_used'])}")
        if response.get('iterations_completed'):
            print(f"🔄 Iterations: {response['iterations_completed']}")
        print("=" * 70)
        print(response['answer'])
        print("=" * 70)
    else:
        print("No question provided.")

async def standalone_research_mode():
    """Standalone research mode"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    topic = input("Enter your research topic: ").strip()
    user_id = input("Enter your user ID (optional): ").strip() or "researcher"
    
    if topic:
        print(f"\n🔬 Conducting comprehensive multi-agent research on: {topic}")
        print("This may take 2-5 minutes for thorough analysis...")
        
        result = await interface.conduct_deep_research(topic, user_id)
        
        print(f"\n📊 **Research Completed**")
        print(f"Session ID: {result.get('session_id', 'N/A')}")
        print(f"Iterations: {result.get('iterations_completed', 0)}")
        print(f"Agents Used: {', '.join(result.get('agents_used', []))}")
        print(f"Overall Confidence: {result.get('overall_confidence', 0):.2f}")
        
        if result.get('research_plan'):
            plan = result['research_plan']
            print(f"\nResearch Plan:")
            print(f"• Objectives: {len(plan.get('research_objectives', []))}")
            print(f"• Tasks: {len(plan.get('research_tasks', []))}")
            print(f"• Strategy: {plan.get('coordination_strategy', 'N/A')}")
        
        print("\n" + "="*70)
        print("📋 **Final Research Report:**")
        print("="*70)
        print(result.get('final_synthesis', 'No synthesis available'))
        print("="*70)
        
        if result.get('memory_keys'):
            print(f"\n💾 Stored {len(result['memory_keys'])} memories for future sessions")
    else:
        print("No topic provided.")

def run_enhanced_main():
    """Run the enhanced application"""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n👋 Goodbye!")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        print(f"❌ Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_enhanced_main()
