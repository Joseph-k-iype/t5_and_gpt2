"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Error-free implementation with o3-mini and proper tool calling
Enhanced with Chain of Thought, Mixture of Thought, and Mixture of Reasoning
Focused on Data Governance Rules (Usage, Transfer, Storage, Access)
"""

import json
import re
import time
import os
from typing import List, Dict, Any, Optional, Annotated, Sequence, TypedDict, Literal
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# ========================= Global Configuration =========================

# Global model configuration for o3-mini
OPENAI_MODEL = "o3-mini"
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
REASONING_EFFORT = "high"  # high, medium, or low

# Global model instance
def get_model():
    """Get configured o3-mini model instance"""
    return ChatOpenAI(
        model=OPENAI_MODEL,
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
        model_kwargs={
            "reasoning_effort": REASONING_EFFORT,
            "max_completion_tokens": 4000
        }
    )


# ========================= State Management =========================

class AgentState(TypedDict):
    """State for the legislation processing agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str
    current_phase: str
    analysis_count: int
    extraction_count: int
    extracted_rules: Dict[str, List[Dict[str, Any]]]
    json_rules: List[Dict[str, Any]]
    validation_results: Dict[str, Any]
    reasoning_pathways: List[Dict[str, Any]]
    reasoning_steps: List[str]


# ========================= Pydantic Models for Tools =========================

class AnalyzeWithReasoningInput(BaseModel):
    legislation_text: str = Field(..., description="Legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Reasoning pathway: structural/semantic/logical/contextual/compliance")
    reasoning_mode: str = Field(..., description="Reasoning mode: deductive/inductive/abductive/analogical/causal")
    focus_domain: str = Field(..., description="Focus domain: data_usage/data_transfer/data_storage/data_access")


class ExtractDataRulesInput(BaseModel):
    legislation_text: str = Field(..., description="Legislation text to extract rules from")
    data_domain: str = Field(..., description="Domain: data_usage/data_transfer/data_storage/data_access")
    reasoning_context: str = Field(default="", description="Context from previous analysis")


class SynthesizeRulesInput(BaseModel):
    all_extracted_rules: Dict[str, List[Dict[str, Any]]] = Field(..., description="All extracted rules by domain")
    reasoning_pathways: List[Dict[str, Any]] = Field(default=[], description="Applied reasoning pathways")


class ConvertToJsonRulesInput(BaseModel):
    synthesized_rules: List[Dict[str, Any]] = Field(..., description="Synthesized rules to convert")


class ValidateJsonRulesInput(BaseModel):
    json_rules: List[Dict[str, Any]] = Field(..., description="JSON rules to validate")


# ========================= Tool Definitions =========================

@tool(args_schema=AnalyzeWithReasoningInput)
def analyze_with_reasoning(
    legislation_text: str,
    reasoning_pathway: str,
    reasoning_mode: str,
    focus_domain: str
) -> str:
    """
    Analyze legislation using Chain of Thought, Mixture of Thought, and Mixture of Reasoning.
    
    This implements:
    - Mixture of Thought: Different analytical pathways (structural, semantic, logical, contextual, compliance)
    - Mixture of Reasoning: Different reasoning modes (deductive, inductive, abductive, analogical, causal)
    - Chain of Thought: Step-by-step analysis process
    """
    
    # Mixture of Thought - Different analytical pathways
    pathway_prompts = {
        "structural": f"""
        STRUCTURAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify main sections containing {focus_domain} provisions
        2. Map hierarchical organization of {focus_domain} rules
        3. Find cross-references between {focus_domain} sections
        4. Analyze document structure patterns for {focus_domain}
        5. Extract structural dependencies in {focus_domain} rules
        
        Focus: Document architecture, section relationships, hierarchical patterns affecting {focus_domain}
        """,
        
        "semantic": f"""
        SEMANTIC ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify key definitions related to {focus_domain}
        2. Extract explicit meanings in {focus_domain} provisions
        3. Infer implicit meanings and legislative intent for {focus_domain}
        4. Analyze stakeholder roles and obligations in {focus_domain}
        5. Map semantic relationships between {focus_domain} concepts
        
        Focus: Meaning extraction, intent analysis, conceptual relationships for {focus_domain}
        """,
        
        "logical": f"""
        LOGICAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify IF-THEN conditional statements for {focus_domain}
        2. Extract MUST/SHALL requirements for {focus_domain}
        3. Find MUST NOT/SHALL NOT prohibitions for {focus_domain}
        4. Map logical operators (AND, OR, NOT) in {focus_domain} rules
        5. Analyze cause-effect relationships in {focus_domain}
        
        Focus: Logical structures, conditions, requirements, prohibitions for {focus_domain}
        """,
        
        "contextual": f"""
        CONTEXTUAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Understand regulatory environment affecting {focus_domain}
        2. Identify industry-specific requirements for {focus_domain}
        3. Extract temporal conditions and deadlines for {focus_domain}
        4. Find exceptions and special cases in {focus_domain}
        5. Analyze contextual triggers and circumstances for {focus_domain}
        
        Focus: Environmental context, exceptions, timing, circumstances for {focus_domain}
        """,
        
        "compliance": f"""
        COMPLIANCE ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Extract mandatory obligations for {focus_domain}
        2. Identify penalties for non-compliance with {focus_domain}
        3. Find audit and reporting requirements for {focus_domain}
        4. Analyze enforcement mechanisms for {focus_domain}
        5. Map compliance verification processes for {focus_domain}
        
        Focus: Obligations, penalties, enforcement, verification for {focus_domain}
        """
    }
    
    # Mixture of Reasoning - Different reasoning modes
    mode_prompts = {
        "deductive": "Apply DEDUCTIVE reasoning: Start from general data governance principles and derive specific rules for this domain",
        "inductive": "Apply INDUCTIVE reasoning: Examine specific examples and patterns to infer general rules for this domain",
        "abductive": "Apply ABDUCTIVE reasoning: Find the best explanation for why these provisions exist and what rules they establish",
        "analogical": "Apply ANALOGICAL reasoning: Compare with known data governance frameworks (GDPR, CCPA) to understand these rules",
        "causal": "Apply CAUSAL reasoning: Identify cause-effect chains - what triggers lead to what consequences in this domain"
    }
    
    analysis_prompt = f"""
    You are an expert legal analyst using advanced reasoning techniques to analyze legislation.
    
    REASONING FRAMEWORK:
    
    PATHWAY: {reasoning_pathway.upper()}
    {pathway_prompts.get(reasoning_pathway, pathway_prompts["structural"])}
    
    REASONING MODE: {reasoning_mode.upper()}  
    {mode_prompts.get(reasoning_mode, mode_prompts["deductive"])}
    
    LEGISLATION TEXT TO ANALYZE:
    {legislation_text}
    
    ANALYSIS REQUIREMENTS:
    - Apply the specified reasoning pathway and mode systematically
    - Focus specifically on {focus_domain} aspects
    - Provide step-by-step reasoning following Chain of Thought methodology
    - Identify key patterns, relationships, and rule structures
    - Extract actionable insights for rule generation
    
    Provide comprehensive analysis with clear findings for {focus_domain}.
    """
    
    model = get_model()
    response = model.invoke(analysis_prompt)
    
    analysis_result = f"""
    REASONING ANALYSIS COMPLETE
    
    Pathway: {reasoning_pathway}
    Mode: {reasoning_mode} 
    Domain: {focus_domain}
    Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
    
    ANALYSIS:
    {response.content}
    
    METADATA:
    - Input length: {len(legislation_text)} characters
    - Reasoning framework: Mixture of Thought + Mixture of Reasoning + Chain of Thought
    - o3-mini reasoning effort: {REASONING_EFFORT}
    """
    
    return analysis_result


@tool(args_schema=ExtractDataRulesInput)
def extract_data_rules(
    legislation_text: str, 
    data_domain: str,
    reasoning_context: str = ""
) -> str:
    """
    Extract specific data governance rules using Chain of Thought methodology.
    Implements systematic step-by-step rule extraction.
    """
    
    # Domain-specific Chain of Thought extraction strategies
    domain_instructions = {
        "data_usage": """
        DATA USAGE RULES EXTRACTION
        
        Chain of Thought Process:
        1. IDENTIFY: Scan for all mentions of data usage, processing, utilization
        2. CATEGORIZE: Group usage rules by type (consent-based, legitimate interest, etc.)
        3. EXTRACT CONDITIONS: What circumstances permit/prohibit data usage?
        4. EXTRACT REQUIREMENTS: What must be done before/during data usage?
        5. EXTRACT PROHIBITIONS: What uses are explicitly forbidden?
        6. EXTRACT CONSEQUENCES: What happens when usage rules are violated?
        7. SYNTHESIZE: Combine findings into structured rules
        
        Focus Areas:
        - Consent requirements and mechanisms
        - Purpose limitation and compatibility
        - Data minimization principles
        - Lawful basis requirements
        - Usage restrictions and prohibitions
        """,
        
        "data_transfer": """
        DATA TRANSFER RULES EXTRACTION
        
        Chain of Thought Process:
        1. IDENTIFY: Find all transfer, sharing, transmission provisions
        2. CATEGORIZE: Internal transfers vs external vs cross-border
        3. EXTRACT CONDITIONS: When are transfers permitted/required?
        4. EXTRACT REQUIREMENTS: What safeguards, agreements, notifications needed?
        5. EXTRACT PROHIBITIONS: What transfers are forbidden?
        6. EXTRACT CONSEQUENCES: Penalties for unauthorized transfers
        7. SYNTHESIZE: Structure transfer governance rules
        
        Focus Areas:
        - Cross-border transfer restrictions
        - Third-party sharing requirements
        - Transfer security and encryption
        - Data processing agreements
        - Notification and consent for transfers
        """,
        
        "data_storage": """
        DATA STORAGE RULES EXTRACTION
        
        Chain of Thought Process:
        1. IDENTIFY: Locate storage, retention, deletion provisions
        2. CATEGORIZE: By data type, storage duration, security level
        3. EXTRACT CONDITIONS: When must data be stored/deleted?
        4. EXTRACT REQUIREMENTS: Security, encryption, backup requirements
        5. EXTRACT PROHIBITIONS: Storage restrictions and limitations
        6. EXTRACT CONSEQUENCES: Penalties for storage violations
        7. SYNTHESIZE: Comprehensive storage governance rules
        
        Focus Areas:
        - Retention periods and deletion schedules
        - Storage security and encryption requirements
        - Data localization requirements
        - Backup and recovery obligations
        - Access controls for stored data
        """,
        
        "data_access": """
        DATA ACCESS RULES EXTRACTION
        
        Chain of Thought Process:
        1. IDENTIFY: Find access rights, permissions, authentication provisions
        2. CATEGORIZE: By user type, data type, access level
        3. EXTRACT CONDITIONS: Who can access what data when?
        4. EXTRACT REQUIREMENTS: Authentication, authorization, audit requirements
        5. EXTRACT PROHIBITIONS: Access restrictions and denials
        6. EXTRACT CONSEQUENCES: Penalties for unauthorized access
        7. SYNTHESIZE: Complete access governance framework
        
        Focus Areas:
        - Role-based access controls
        - Authentication and authorization requirements
        - Data subject access rights
        - Audit trails and monitoring
        - Breach notification requirements
        """
    }
    
    extraction_prompt = f"""
    You are extracting {data_domain} rules using systematic Chain of Thought methodology.
    
    {domain_instructions.get(data_domain, "Extract relevant data governance rules")}
    
    REASONING CONTEXT FROM ANALYSIS:
    {reasoning_context}
    
    LEGISLATION TO PROCESS:
    {legislation_text}
    
    EXTRACTION REQUIREMENTS:
    Follow the Chain of Thought process above step-by-step.
    For each rule identified, extract:
    - Clear, actionable description
    - Specific conditions when rule applies
    - Mandatory requirements (what MUST be done)
    - Explicit prohibitions (what MUST NOT be done)  
    - Consequences for violations
    - Confidence level (0.0-1.0)
    
    OUTPUT FORMAT:
    Return ONLY a JSON array with this exact structure:
    [
        {{
            "rule_id": "unique_identifier",
            "domain": "{data_domain}",
            "description": "Clear rule description",
            "conditions": ["condition1", "condition2"],
            "requirements": ["requirement1", "requirement2"],
            "prohibitions": ["prohibition1", "prohibition2"],
            "consequences": ["consequence1", "consequence2"],
            "confidence": 0.9,
            "source_section": "Section reference",
            "reasoning_applied": "Chain of Thought extraction"
        }}
    ]
    
    Return ONLY the JSON array, no other text or formatting.
    """
    
    model = get_model()
    response = model.invoke(extraction_prompt)
    
    return response.content


@tool(args_schema=SynthesizeRulesInput)
def synthesize_rules(
    all_extracted_rules: Dict[str, List[Dict[str, Any]]],
    reasoning_pathways: List[Dict[str, Any]] = []
) -> str:
    """
    Synthesize rules using convergent reasoning from divergent analysis.
    Implements Mixture of Reasoning for rule synthesis.
    """
    
    synthesis_prompt = f"""
    RULE SYNTHESIS using Convergent Reasoning
    
    You are synthesizing rules extracted through multiple reasoning pathways:
    
    EXTRACTED RULES BY DOMAIN:
    {json.dumps(all_extracted_rules, indent=2)}
    
    APPLIED REASONING PATHWAYS:
    {json.dumps(reasoning_pathways, indent=2) if reasoning_pathways else "Multiple pathways applied"}
    
    SYNTHESIS METHODOLOGY (Chain of Thought):
    
    1. CONVERGENCE ANALYSIS:
       - Identify rules that address the same governance area
       - Find overlapping requirements across domains
       - Detect conflicting or contradictory rules
    
    2. DEDUPLICATION:
       - Remove exact duplicates
       - Merge rules with similar intent but different wording
       - Consolidate related requirements
    
    3. CONFLICT RESOLUTION:
       - Resolve contradictions using legal precedence
       - Apply stricter requirements when conflicts exist
       - Maintain regulatory compliance priorities
    
    4. PRIORITY ASSIGNMENT:
       - Assign priority levels (1-100) based on:
         * Legal mandate strength (MUST vs SHOULD)
         * Penalty severity for violations
         * Regulatory importance
         * Implementation complexity
    
    5. QUALITY ENHANCEMENT:
       - Ensure logical consistency across all rules
       - Verify completeness of rule coverage
       - Optimize for clarity and enforceability
    
    SYNTHESIS REQUIREMENTS:
    - Maintain all essential governance requirements
    - Ensure no critical rules are lost in synthesis
    - Create coherent, non-contradictory rule set
    - Preserve domain-specific nuances
    - Assign meaningful priority levels
    
    OUTPUT FORMAT:
    Return ONLY a JSON array of synthesized rules:
    [
        {{
            "rule_id": "synthesized_rule_id",
            "domain": "primary_domain",
            "description": "Comprehensive rule description",
            "conditions": ["condition1", "condition2"],
            "requirements": ["requirement1", "requirement2"],
            "prohibitions": ["prohibition1", "prohibition2"],
            "consequences": ["consequence1", "consequence2"],
            "priority": 85,
            "confidence": 0.9,
            "synthesis_notes": "How this rule was synthesized",
            "cross_domain_impact": ["affected_domains"]
        }}
    ]
    
    Return ONLY the JSON array.
    """
    
    model = get_model()
    response = model.invoke(synthesis_prompt)
    
    return response.content


@tool(args_schema=ConvertToJsonRulesInput)
def convert_to_json_rules(synthesized_rules: List[Dict[str, Any]]) -> str:
    """
    Convert synthesized rules to json-rules-engine format.
    Creates complex, multi-condition rules with proper logical structure.
    """
    
    conversion_prompt = f"""
    CONVERSION TO JSON-RULES-ENGINE FORMAT
    
    Convert these synthesized data governance rules to json-rules-engine compatible format:
    
    INPUT RULES:
    {json.dumps(synthesized_rules, indent=2)}
    
    JSON-RULES-ENGINE REQUIREMENTS:
    
    STRUCTURE:
    {{
        "name": "unique_rule_name",
        "conditions": {{
            "all": [  // Use "all" for AND logic, "any" for OR logic
                {{
                    "fact": "factName",
                    "operator": "equal|notEqual|lessThan|greaterThan|in|contains",
                    "value": "expectedValue"
                }}
            ]
        }},
        "event": {{
            "type": "governance_rule_triggered",
            "params": {{
                "ruleId": "rule_identifier",
                "domain": "data_domain",
                "action": "required_action",
                "message": "human_readable_message",
                "severity": "high|medium|low",
                "compliance_requirement": true
            }}
        }},
        "priority": 1-100,
        "onSuccess": {{
            "type": "rule_compliance",
            "message": "Compliance check passed"
        }},
        "onFailure": {{
            "type": "rule_violation", 
            "message": "Compliance violation detected"
        }}
    }}
    
    AVAILABLE FACTS for data governance:
    - dataOperation: "usage"|"transfer"|"storage"|"access"
    - dataType: "personal"|"sensitive"|"public"|"anonymous"
    - userConsent: true|false
    - dataEncrypted: true|false  
    - userAuthorized: true|false
    - crossBorderTransfer: true|false
    - retentionPeriod: number (days)
    - userRole: "admin"|"user"|"processor"|"controller"
    - processingPurpose: "marketing"|"analytics"|"operations"|"legal"
    - dataSubjectRequest: true|false
    - auditLogEnabled: true|false
    - mfaEnabled: true|false
    
    CONVERSION LOGIC:
    - Map rule conditions to appropriate facts and operators
    - Create complex multi-condition rules where appropriate
    - Ensure logical consistency in condition combinations
    - Set meaningful priority levels based on rule importance
    - Include comprehensive event parameters for rule handling
    
    OUTPUT FORMAT:
    Return ONLY a JSON array of json-rules-engine formatted rules:
    [
        {{
            "name": "data_usage_consent_required",
            "conditions": {{
                "all": [
                    {{"fact": "dataOperation", "operator": "equal", "value": "usage"}},
                    {{"fact": "dataType", "operator": "equal", "value": "personal"}},
                    {{"fact": "userConsent", "operator": "equal", "value": false}}
                ]
            }},
            "event": {{
                "type": "governance_violation",
                "params": {{
                    "ruleId": "usage_consent_001",
                    "domain": "data_usage",
                    "action": "require_consent",
                    "message": "User consent required for personal data usage",
                    "severity": "high"
                }}
            }},
            "priority": 90
        }}
    ]
    
    Return ONLY the JSON array of json-rules-engine rules.
    """
    
    model = get_model()
    response = model.invoke(conversion_prompt)
    
    return response.content


@tool(args_schema=ValidateJsonRulesInput)
def validate_json_rules(json_rules: List[Dict[str, Any]]) -> str:
    """
    Validate JSON rules for json-rules-engine compatibility and logical consistency.
    Performs comprehensive validation and quality analysis.
    """
    
    validation_prompt = f"""
    COMPREHENSIVE JSON-RULES-ENGINE VALIDATION
    
    Validate these JSON rules for compatibility and quality:
    
    RULES TO VALIDATE:
    {json.dumps(json_rules, indent=2)}
    
    VALIDATION CHECKLIST:
    
    1. STRUCTURAL VALIDATION:
       - Required fields: name, conditions, event
       - Proper JSON structure and syntax
       - Valid condition operators and logic
       - Event structure compliance
    
    2. SEMANTIC VALIDATION:
       - Fact names are meaningful and consistent
       - Operator usage is logical for fact types
       - Value types match expected formats
       - Priority levels are reasonable (1-100)
    
    3. LOGICAL VALIDATION:
       - Condition combinations make logical sense
       - No contradictory conditions within same rule
       - Event actions align with rule conditions
       - Cross-rule consistency and compatibility
    
    4. COMPLIANCE VALIDATION:
       - Rules address key data governance domains
       - Comprehensive coverage of regulatory requirements
       - Appropriate severity levels and priorities
       - Actionable rule outcomes
    
    5. QUALITY METRICS:
       - Rule clarity and enforceability
       - Implementation feasibility  
       - Maintenance complexity
       - Performance considerations
    
    VALIDATION OPERATORS (check usage):
    - equal, notEqual: exact matching
    - lessThan, greaterThan, lessThanInclusive, greaterThanInclusive: numeric comparison
    - in, notIn: array membership
    - contains, doesNotContain: substring/element presence
    
    VALIDATION REPORT FORMAT:
    Return ONLY this JSON validation report:
    {{
        "valid": true|false,
        "total_rules": number,
        "valid_rules": number,
        "invalid_rules": number,
        "errors": ["error descriptions"],
        "warnings": ["warning descriptions"],
        "domain_coverage": {{
            "data_usage": count,
            "data_transfer": count,
            "data_storage": count,
            "data_access": count,
            "general": count
        }},
        "complexity_analysis": {{
            "simple_rules": count,
            "complex_rules": count,
            "average_conditions_per_rule": number,
            "max_conditions_in_rule": number
        }},
        "priority_distribution": {{
            "high_priority": count,
            "medium_priority": count, 
            "low_priority": count
        }},
        "quality_score": 0-100,
        "recommendations": ["improvement suggestions"],
        "json_rules_engine_compatible": true|false
    }}
    
    Return ONLY the JSON validation report.
    """
    
    model = get_model()
    response = model.invoke(validation_prompt)
    
    return response.content


# ========================= Agent Nodes =========================

def agent_node(state: AgentState) -> Dict[str, Any]:
    """
    Main reasoning agent that orchestrates the multi-phase processing.
    Implements systematic progression through reasoning phases.
    """
    
    messages = state["messages"]
    current_phase = state.get("current_phase", "start")
    legislation_text = state.get("legislation_text", "")
    analysis_count = state.get("analysis_count", 0)
    extraction_count = state.get("extraction_count", 0)
    
    model = get_model()
    tools = [analyze_with_reasoning, extract_data_rules, synthesize_rules, convert_to_json_rules, validate_json_rules]
    model_with_tools = model.bind_tools(tools)
    
    # Phase-based system prompts with reasoning strategies
    if current_phase == "start":
        system_msg = f"""You are an expert legal analyst implementing advanced reasoning methodologies to extract data governance rules.

PROCESSING FRAMEWORK:
- Mixture of Thought: Multiple analytical pathways
- Mixture of Reasoning: Different reasoning modes  
- Chain of Thought: Step-by-step systematic analysis

PHASE 1: DIVERGENT ANALYSIS
You must call analyze_with_reasoning exactly 4 times with these specific combinations:

1. analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="structural", reasoning_mode="deductive", focus_domain="data_usage")
2. analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="semantic", reasoning_mode="inductive", focus_domain="data_transfer") 
3. analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="logical", reasoning_mode="abductive", focus_domain="data_storage")
4. analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="contextual", reasoning_mode="analogical", focus_domain="data_access")

Start with the first analysis call now.

LEGISLATION: {legislation_text[:500]}..."""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "analysis"
        
    elif current_phase == "analysis" and analysis_count < 4:
        system_msg = f"""Continue with PHASE 1: DIVERGENT ANALYSIS.

You have completed {analysis_count}/4 analyses. Continue with the next analyze_with_reasoning call.

Remaining analyses needed:
{4 - analysis_count} more analysis calls with different pathway/mode/domain combinations."""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "analysis"
        
    elif current_phase == "analysis" and analysis_count >= 4:
        system_msg = """PHASE 2: RULE EXTRACTION

Now extract rules for each domain. Call extract_data_rules 4 times:

1. extract_data_rules(legislation_text=<full_text>, data_domain="data_usage")
2. extract_data_rules(legislation_text=<full_text>, data_domain="data_transfer")
3. extract_data_rules(legislation_text=<full_text>, data_domain="data_storage")  
4. extract_data_rules(legislation_text=<full_text>, data_domain="data_access")

Start with the first extraction call."""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "extraction"
        
    elif current_phase == "extraction" and extraction_count < 4:
        system_msg = f"""Continue with PHASE 2: RULE EXTRACTION.

You have completed {extraction_count}/4 extractions. Continue with the next extract_data_rules call."""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "extraction"
        
    elif current_phase == "extraction" and extraction_count >= 4:
        system_msg = """PHASE 3: RULE SYNTHESIS

Call synthesize_rules with all extracted rules:
synthesize_rules(all_extracted_rules=<collected_rules>, reasoning_pathways=<pathways_used>)"""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "synthesis"
        
    elif current_phase == "synthesis":
        system_msg = """PHASE 4: JSON CONVERSION

Call convert_to_json_rules with the synthesized rules:
convert_to_json_rules(synthesized_rules=<synthesized_rules>)"""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "conversion"
        
    elif current_phase == "conversion":
        system_msg = """PHASE 5: VALIDATION

Call validate_json_rules with the converted rules:
validate_json_rules(json_rules=<json_rules>)"""
        
        new_messages = [SystemMessage(content=system_msg)] + list(messages)
        new_phase = "validation"
        
    else:
        new_messages = list(messages)
        new_phase = "complete"
    
    response = model_with_tools.invoke(new_messages)
    
    return {
        "messages": [response],
        "current_phase": new_phase
    }


def tool_node(state: AgentState) -> Dict[str, Any]:
    """Execute tools and update state with proper error handling"""
    
    tools = [analyze_with_reasoning, extract_data_rules, synthesize_rules, convert_to_json_rules, validate_json_rules]
    tool_node_instance = ToolNode(tools)
    result = tool_node_instance.invoke(state)
    
    # Initialize updates with current state values
    updates = {"messages": result.get("messages", [])}
    extracted_rules = state.get("extracted_rules", {}).copy()
    analysis_count = state.get("analysis_count", 0)
    extraction_count = state.get("extraction_count", 0)
    reasoning_pathways = state.get("reasoning_pathways", []).copy()
    reasoning_steps = state.get("reasoning_steps", []).copy()
    
    # Process tool results safely
    messages = result.get("messages", [])
    
    for message in messages:
        if isinstance(message, ToolMessage):
            # Safely get content - fix for tuple error
            content = ""
            if hasattr(message, 'content'):
                if isinstance(message.content, str):
                    content = message.content.strip()
                elif isinstance(message.content, (list, tuple)):
                    # Handle case where content might be a tuple/list
                    content = str(message.content).strip()
                else:
                    content = str(message.content).strip()
            
            # Track tool usage
            tool_name = getattr(message, 'name', '')
            reasoning_steps.append(f"Executed {tool_name} at {time.strftime('%H:%M:%S')}")
            
            # Parse JSON responses safely
            try:
                if content and (content.startswith('[') or content.startswith('{')):
                    # Clean content of markdown formatting
                    clean_content = content
                    if "```json" in clean_content:
                        clean_content = clean_content.split("```json")[1].split("```")[0].strip()
                    elif "```" in clean_content:
                        clean_content = clean_content.split("```")[1].split("```")[0].strip()
                    
                    parsed_data = json.loads(clean_content)
                    
                    # Update state based on tool
                    if tool_name == "analyze_with_reasoning":
                        analysis_count += 1
                        reasoning_pathways.append({
                            "tool": tool_name,
                            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                            "result_length": len(content)
                        })
                        
                    elif tool_name == "extract_data_rules" and isinstance(parsed_data, list):
                        extraction_count += 1
                        # Determine domain from rules
                        for rule in parsed_data:
                            if isinstance(rule, dict):
                                domain = rule.get("domain", "unknown")
                                if domain not in extracted_rules:
                                    extracted_rules[domain] = []
                                extracted_rules[domain].append(rule)
                    
                    elif tool_name == "convert_to_json_rules" and isinstance(parsed_data, list):
                        updates["json_rules"] = parsed_data
                        
                    elif tool_name == "validate_json_rules" and isinstance(parsed_data, dict):
                        updates["validation_results"] = parsed_data
                        
            except json.JSONDecodeError as e:
                reasoning_steps.append(f"JSON parse error in {tool_name}: {str(e)}")
            except Exception as e:
                reasoning_steps.append(f"Error processing {tool_name}: {str(e)}")
    
    # Update state
    updates.update({
        "extracted_rules": extracted_rules,
        "analysis_count": analysis_count,
        "extraction_count": extraction_count,
        "reasoning_pathways": reasoning_pathways,
        "reasoning_steps": reasoning_steps
    })
    
    return updates


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """Determine whether to continue processing or end"""
    messages = state["messages"]
    
    if not messages:
        return "end"
        
    last_message = messages[-1]
    
    # Continue if there are tool calls to execute
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # Check if processing is complete
    current_phase = state.get("current_phase", "start")
    if current_phase == "complete":
        return "end"
    
    # Continue processing if not complete
    return "end"


# ========================= Create Graph =========================

def create_legislation_processing_graph():
    """Create the LangGraph workflow"""
    
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    
    # Set entry point
    workflow.set_entry_point("agent")
    
    # Add edges
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END
        }
    )
    
    workflow.add_edge("tools", "agent")
    
    # Compile with memory
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)


# ========================= Main Processing Function =========================

def process_legislation(legislation_text: str) -> Dict[str, Any]:
    """Process legislation using the LangGraph agent with full reasoning framework"""
    
    print("🚀 Processing legislation with o3-mini + advanced reasoning...")
    print("📊 Framework: Chain of Thought + Mixture of Thought + Mixture of Reasoning")
    
    # Create the processing graph
    graph = create_legislation_processing_graph()
    
    # Initial state
    initial_state = {
        "messages": [HumanMessage(content=f"Process this legislation to extract data governance rules:\n\n{legislation_text}")],
        "legislation_text": legislation_text,
        "current_phase": "start",
        "analysis_count": 0,
        "extraction_count": 0,
        "extracted_rules": {},
        "json_rules": [],
        "validation_results": {},
        "reasoning_pathways": [],
        "reasoning_steps": []
    }
    
    # Configuration for memory
    config = {"configurable": {"thread_id": "legislation_processing"}}
    
    try:
        # Run the graph
        final_state = graph.invoke(initial_state, config)
        
        return {
            "status": "completed",
            "extracted_rules": final_state.get("extracted_rules", {}),
            "json_rules": final_state.get("json_rules", []),
            "validation_results": final_state.get("validation_results", {}),
            "reasoning_pathways": final_state.get("reasoning_pathways", []),
            "reasoning_steps": final_state.get("reasoning_steps", []),
            "analysis_count": final_state.get("analysis_count", 0),
            "extraction_count": final_state.get("extraction_count", 0),
            "messages": final_state.get("messages", [])
        }
        
    except Exception as e:
        print(f"❌ Error during processing: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "error": str(e)}


# ========================= Helper Functions =========================

def save_rules_to_file(rules: List[Dict[str, Any]], filename: str = "data_governance_rules.json"):
    """Save JSON rules to file with metadata"""
    output = {
        "rules": rules,
        "metadata": {
            "created_by": "legislation_converter_o3mini",
            "model": OPENAI_MODEL,
            "reasoning_effort": REASONING_EFFORT,
            "reasoning_framework": "Chain of Thought + Mixture of Thought + Mixture of Reasoning",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "rule_count": len(rules),
            "domains_covered": list(set(rule.get("event", {}).get("params", {}).get("domain", "unknown") for rule in rules)),
            "json_rules_engine_compatible": True
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"✅ Saved {len(rules)} rules to {filename}")


def print_results_summary(results: Dict[str, Any]):
    """Print comprehensive processing results summary"""
    print("\n" + "="*80)
    print("📊 PROCESSING RESULTS SUMMARY")
    print("="*80)
    
    # Reasoning framework usage
    print(f"🧠 REASONING FRAMEWORK APPLIED:")
    print(f"   • Analysis phases completed: {results.get('analysis_count', 0)}/4")
    print(f"   • Extraction phases completed: {results.get('extraction_count', 0)}/4")
    print(f"   • Reasoning pathways used: {len(results.get('reasoning_pathways', []))}")
    print(f"   • Total reasoning steps: {len(results.get('reasoning_steps', []))}")
    
    # Extraction results
    extracted = results.get("extracted_rules", {})
    total_extracted = sum(len(rules) for rules in extracted.values())
    print(f"\n📋 RULE EXTRACTION RESULTS:")
    print(f"   • Total extracted rules: {total_extracted}")
    for domain, rules in extracted.items():
        print(f"   • {domain}: {len(rules)} rules")
    
    # Final JSON rules
    json_rules = results.get("json_rules", [])
    print(f"\n⚙️ JSON RULES ENGINE OUTPUT:")
    print(f"   • Final JSON rules: {len(json_rules)}")
    
    # Validation results
    validation = results.get("validation_results", {})
    if validation:
        print(f"\n✅ VALIDATION RESULTS:")
        print(f"   • Validation status: {'PASS' if validation.get('valid', False) else 'FAIL'}")
        print(f"   • Quality score: {validation.get('quality_score', 'N/A')}/100")
        print(f"   • Valid rules: {validation.get('valid_rules', 0)}/{validation.get('total_rules', 0)}")
        print(f"   • JSON-rules-engine compatible: {validation.get('json_rules_engine_compatible', 'Unknown')}")
        
        domain_coverage = validation.get('domain_coverage', {})
        if domain_coverage:
            print(f"   • Domain coverage:")
            for domain, count in domain_coverage.items():
                if count > 0:
                    print(f"     - {domain}: {count} rules")
    
    # Sample rule
    if json_rules:
        print(f"\n📄 SAMPLE JSON RULE:")
        sample = json_rules[0]
        print(f"   • Name: {sample.get('name', 'N/A')}")
        print(f"   • Priority: {sample.get('priority', 'N/A')}")
        conditions = sample.get('conditions', {}).get('all', [])
        print(f"   • Conditions: {len(conditions)}")
        print(f"   • Event type: {sample.get('event', {}).get('type', 'N/A')}")


# ========================= Main Execution =========================

if __name__ == "__main__":
    # Check API key
    if not os.getenv("OPENAI_API_KEY"):
        print("⚠️ Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-openai-api-key'")
        exit(1)
    
    # Sample legislation for testing
    SAMPLE_LEGISLATION = """
    DATA PROTECTION AND PRIVACY ACT
    
    PART II - DATA USAGE REQUIREMENTS
    
    Section 2. Lawful Basis for Data Usage
    2.1 Personal data shall only be used when there is a lawful basis, including:
        (a) The data subject has given explicit consent
        (b) Processing is necessary for contract performance
        (c) Processing is required for legal compliance
        (d) Processing is necessary to protect vital interests
    
    2.2 Data controllers must not use personal data for purposes incompatible with those for which it was originally collected.
    
    2.3 Sensitive data shall not be processed unless:
        (a) The data subject has given explicit written consent
        (b) Processing is necessary for employment law obligations
        (c) Processing is necessary for healthcare purposes
    
    PART III - DATA TRANSFER REGULATIONS
    
    Section 5. Third-Party Data Transfers
    5.1 Data controllers shall not transfer personal data to third parties unless:
        (a) A data processing agreement is in place
        (b) The third party provides appropriate security guarantees
        (c) The data subject has been informed of the transfer
    
    5.2 Cross-border transfers of personal data are prohibited unless:
        (a) The recipient country ensures adequate protection
        (b) Appropriate safeguards are implemented, including:
            - Standard contractual clauses
            - Binding corporate rules
            - Approved certification mechanisms
        (c) The data subject has explicitly consented to the transfer
    
    Section 6. Data Transfer Security
    6.1 All data transfers must be encrypted using industry-standard encryption.
    6.2 Data controllers must maintain logs of all data transfers for at least 3 years.
    
    PART IV - DATA STORAGE REQUIREMENTS
    
    Section 7. Storage Duration and Retention
    7.1 Personal data shall not be stored longer than necessary for the purposes for which it was collected.
    7.2 Data retention periods must be defined and documented for each category of personal data.
    7.3 Upon expiration of the retention period, personal data must be securely deleted or anonymized.
    
    Section 8. Storage Security Requirements
    8.1 Data controllers must implement appropriate technical measures including:
        (a) Encryption of personal data at rest
        (b) Regular backups with tested recovery procedures
        (c) Physical security controls for data storage facilities
        (d) Logical access controls and authentication mechanisms
    
    PART V - DATA ACCESS CONTROLS
    
    Section 10. Access Rights and Permissions
    10.1 Data controllers must implement role-based access controls ensuring:
        (a) Access is granted on a need-to-know basis
        (b) Privileged access is monitored and reviewed quarterly
        (c) Access permissions are revoked upon role change or termination
    
    10.2 Authentication requirements:
        (a) Multi-factor authentication for accessing sensitive data
        (b) Strong password policies enforced
        (c) Session timeouts after 15 minutes of inactivity
    
    Section 11. Data Subject Access Rights
    11.1 Data subjects have the right to:
        (a) Access their personal data within 30 days of request
        (b) Rectify inaccurate personal data
        (c) Request deletion of their personal data
        (d) Object to processing of their personal data
        (e) Request data portability in machine-readable format
    
    PART VII - PENALTIES
    
    Section 14. Administrative Penalties
    14.1 Violation of data usage requirements: Fine up to $1 million or 2% of annual revenue
    14.2 Unauthorized data transfer: Fine up to $2 million or 4% of annual revenue
    14.3 Inadequate storage security: Fine up to $500,000 per incident
    14.4 Access control violations: Fine up to $750,000 per violation
    """
    
    print("🚀 Advanced Data Governance Rules Extractor")
    print(f"🤖 Model: {OPENAI_MODEL} (reasoning effort: {REASONING_EFFORT})")
    print("📊 Reasoning Framework: Chain of Thought + Mixture of Thought + Mixture of Reasoning")
    print("=" * 80)
    
    # Process the legislation
    results = process_legislation(SAMPLE_LEGISLATION)
    
    if results["status"] == "completed":
        print_results_summary(results)
        
        json_rules = results.get("json_rules", [])
        if json_rules:
            save_rules_to_file(json_rules)
            print(f"\n✨ Successfully generated {len(json_rules)} data governance rules!")
            
            # Show sample rule
            print(f"\n📄 Sample JSON Rule Structure:")
            sample_json = json.dumps(json_rules[0], indent=2)
            if len(sample_json) > 600:
                print(sample_json[:600] + "...")
            else:
                print(sample_json)
        else:
            print("\n⚠️ No JSON rules were generated. Check the processing phases.")
            
        # Show reasoning process
        reasoning_steps = results.get("reasoning_steps", [])
        if reasoning_steps:
            print(f"\n🧠 REASONING PROCESS TRACE:")
            for i, step in enumerate(reasoning_steps[-5:], 1):  # Show last 5 steps
                print(f"   {i}. {step}")
    else:
        print(f"\n❌ Processing failed: {results.get('error', 'Unknown error')}")
