"""
Large-Scale TTL to Property Graph Converter
==========================================

Optimized version for handling very large TTL files (multi-GB) using streaming 
and chunked processing to avoid memory limitations.

Key optimizations:
- Streaming parser to avoid loading entire file in memory
- Chunked processing for large datasets
- Progress tracking and resume capability
- Memory usage monitoring
- Parallel processing options
"""

import os
import sys
import time
import logging
import tempfile
import subprocess
import re
import hashlib
import shutil
import platform
from typing import Dict, List, Iterator, Optional
from collections import defaultdict
import pickle
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed

import falkordb
from neo4j import GraphDatabase
from rdflib import Graph, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, XSD, FOAF, SKOS
from urllib.parse import urlparse


class TTLToPropertyGraphConverter:
    """
    Basic TTL to Property Graph converter (embedded for chunk processing).
    This is a simplified version of the original converter for use in chunks.
    """
    
    def __init__(self):
        self.rdf_graph = Graph()
        self.nodes = {}
        self.relationships = []
        self.node_labels = defaultdict(set)
        
        # Common namespace prefixes for cleaner node IDs
        self.namespace_prefixes = {
            "http://www.w3.org/1999/02/22-rdf-syntax-ns#": "rdf:",
            "http://www.w3.org/2000/01/rdf-schema#": "rdfs:",
            "http://www.w3.org/2001/XMLSchema#": "xsd:",
            "http://xmlns.com/foaf/0.1/": "foaf:",
            "http://www.w3.org/2004/02/skos/core#": "skos:",
            "http://purl.org/dc/elements/1.1/": "dc:",
            "http://purl.org/dc/terms/": "dcterms:",
            "http://schema.org/": "schema:",
            "http://dbpedia.org/resource/": "dbr:",
            "http://dbpedia.org/ontology/": "dbo:",
            "https://www.wikidata.org/wiki/": "wd:"
        }

    def load_ttl_file(self, file_path: str) -> None:
        """Load and parse a TTL file using RDFLib."""
        self.rdf_graph.parse(file_path, format="turtle")

    def _get_node_id(self, term) -> str:
        """Generate a clean, unique identifier for a graph node."""
        if isinstance(term, URIRef):
            uri_str = str(term)
            # Try to use namespace prefixes for cleaner IDs
            for namespace, prefix in self.namespace_prefixes.items():
                if uri_str.startswith(namespace):
                    return uri_str.replace(namespace, prefix)
            
            # Extract the last part of the URI for cleaner IDs
            parsed = urlparse(uri_str)
            if parsed.fragment:
                return f"{parsed.netloc}#{parsed.fragment}"
            elif "/" in parsed.path:
                return f"{parsed.netloc}{parsed.path.split('/')[-1]}"
            else:
                return uri_str
                
        elif isinstance(term, Literal):
            # For literals, create a hash-based ID to avoid duplication
            content = str(term)
            if len(content) > 50:
                hash_obj = hashlib.md5(content.encode())
                return f"literal_{hash_obj.hexdigest()[:8]}"
            else:
                # Use sanitized content for short literals
                sanitized = re.sub(r'[^\w\s-]', '_', content)
                return f"literal_{sanitized}"
                
        elif isinstance(term, BNode):
            return f"bnode_{str(term)}"
        else:
            return f"unknown_{str(term)}"

    def _get_clean_property_name(self, predicate: URIRef) -> str:
        """Generate a clean property name from a predicate URI."""
        uri_str = str(predicate)
        
        # Use namespace prefixes
        for namespace, prefix in self.namespace_prefixes.items():
            if uri_str.startswith(namespace):
                return uri_str.replace(namespace, prefix).replace(":", "_")
        
        # Extract the last part of the URI
        parsed = urlparse(uri_str)
        if parsed.fragment:
            return parsed.fragment
        elif "/" in parsed.path:
            return parsed.path.split("/")[-1]
        else:
            return "property"

    def _infer_node_labels(self, subject_id: str, subject: URIRef) -> set:
        """Infer appropriate labels for a node based on RDF types and patterns."""
        labels = set()
        
        # Check for explicit rdf:type declarations
        for _, _, obj in self.rdf_graph.triples((subject, RDF.type, None)):
            if isinstance(obj, URIRef):
                type_name = self._get_clean_property_name(obj)
                labels.add(type_name)
        
        # Infer labels from URI patterns
        uri_str = str(subject)
        if "Person" in uri_str or "people" in uri_str.lower():
            labels.add("Person")
        elif "Organization" in uri_str or "company" in uri_str.lower():
            labels.add("Organization")  
        elif "Place" in uri_str or "location" in uri_str.lower():
            labels.add("Place")
        elif "Event" in uri_str:
            labels.add("Event")
        elif "Concept" in uri_str:
            labels.add("Concept")
        
        # Default label if none found
        if not labels:
            labels.add("Resource")
            
        return labels

    def convert_to_property_graph(self) -> None:
        """Convert RDF triples to property graph format."""
        # Process all triples
        for subject, predicate, obj in self.rdf_graph:
            
            # Create subject node
            subject_id = self._get_node_id(subject)
            if subject_id not in self.nodes:
                self.nodes[subject_id] = {
                    "id": subject_id,
                    "uri": str(subject) if isinstance(subject, URIRef) else None,
                    "type": "URI" if isinstance(subject, URIRef) else "BNode"
                }
                
                # Infer labels for the subject
                if isinstance(subject, URIRef):
                    self.node_labels[subject_id] = self._infer_node_labels(subject_id, subject)
            
            # Handle object based on its type
            if isinstance(obj, URIRef):
                # Object is another resource - create relationship
                obj_id = self._get_node_id(obj)
                
                # Create object node if it doesn't exist
                if obj_id not in self.nodes:
                    self.nodes[obj_id] = {
                        "id": obj_id,
                        "uri": str(obj),
                        "type": "URI"
                    }
                    self.node_labels[obj_id] = self._infer_node_labels(obj_id, obj)
                
                # Create relationship
                relationship = {
                    "source": subject_id,
                    "target": obj_id,
                    "type": self._get_clean_property_name(predicate),
                    "predicate_uri": str(predicate)
                }
                self.relationships.append(relationship)
                
            elif isinstance(obj, Literal):
                # Object is a literal - add as node property
                property_name = self._get_clean_property_name(predicate)
                
                # Handle literal value and datatype
                value = str(obj)
                if obj.datatype:
                    datatype = str(obj.datatype)
                    if datatype == str(XSD.integer):
                        try:
                            value = int(value)
                        except ValueError:
                            pass
                    elif datatype == str(XSD.float) or datatype == str(XSD.double):
                        try:
                            value = float(value)
                        except ValueError:
                            pass
                    elif datatype == str(XSD.boolean):
                        value = value.lower() in ('true', '1')
                    
                    # Store datatype information
                    self.nodes[subject_id][f"{property_name}_datatype"] = datatype
                
                # Handle language tags
                if obj.language:
                    self.nodes[subject_id][f"{property_name}_lang"] = obj.language
                
                self.nodes[subject_id][property_name] = value
                
            elif isinstance(obj, BNode):
                # Object is a blank node - create relationship
                obj_id = self._get_node_id(obj)
                
                if obj_id not in self.nodes:
                    self.nodes[obj_id] = {
                        "id": obj_id,
                        "type": "BNode"
                    }
                    self.node_labels[obj_id] = {"BlankNode"}
                
                relationship = {
                    "source": subject_id,
                    "target": obj_id,
                    "type": self._get_clean_property_name(predicate),
                    "predicate_uri": str(predicate)
                }
                self.relationships.append(relationship)


class LargeTTLConverter:
    """
    Optimized converter for large TTL files using streaming and chunking.
    """
    
    def __init__(self, 
                 chunk_size_mb: int = 100,
                 batch_size: int = 1000,
                 max_memory_gb: int = 4,
                 use_parallel: bool = True,
                 temp_dir: str = None,
                 # FalkorDB connection parameters
                 falkor_host: str = "localhost",
                 falkor_port: int = 6379,
                 falkor_password: str = None,
                 falkor_username: str = None,
                 # Neo4j connection parameters  
                 neo4j_uri: str = "bolt://localhost:7687",
                 neo4j_username: str = "neo4j",
                 neo4j_password: str = "password"):
        """
        Initialize the large-scale converter.
        
        Args:
            chunk_size_mb: Size of each chunk in MB
            batch_size: Number of triples per database batch
            max_memory_gb: Maximum memory usage in GB
            use_parallel: Enable parallel processing
            temp_dir: Temporary directory for intermediate files
            falkor_host: FalkorDB host
            falkor_port: FalkorDB port
            falkor_password: FalkorDB password
            falkor_username: FalkorDB username (if required)
            neo4j_uri: Neo4j connection URI
            neo4j_username: Neo4j username
            neo4j_password: Neo4j password
        """
        self.chunk_size_mb = chunk_size_mb
        self.batch_size = batch_size
        self.max_memory_gb = max_memory_gb
        self.use_parallel = use_parallel
        self.temp_dir = temp_dir or tempfile.gettempdir()
        
        # Database connection parameters
        self.falkor_host = falkor_host
        self.falkor_port = falkor_port
        self.falkor_password = falkor_password
        self.falkor_username = falkor_username
        self.neo4j_uri = neo4j_uri
        self.neo4j_username = neo4j_username
        self.neo4j_password = neo4j_password
        
        # Statistics
        self.total_triples = 0
        self.processed_chunks = 0
        self.start_time = None
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # Log system information
        self._log_system_info()
        
    def _log_system_info(self):
        """Log system information for debugging."""
        try:
            system_info = {
                "Platform": platform.system(),
                "Architecture": platform.architecture()[0],
                "Python Version": platform.python_version(),
                "Temp Directory": self.temp_dir
            }
            
            # Try to get memory info
            try:
                import psutil
                memory = psutil.virtual_memory()
                system_info["Total Memory"] = f"{memory.total / (1024**3):.1f} GB"
                system_info["Available Memory"] = f"{memory.available / (1024**3):.1f} GB"
            except ImportError:
                system_info["Memory Info"] = "psutil not available"
            
            self.logger.info("System Information:")
            for key, value in system_info.items():
                self.logger.info(f"  {key}: {value}")
                
        except Exception as e:
            self.logger.warning(f"Could not gather system info: {e}")
        
    def estimate_processing_time(self, file_path: str) -> Dict[str, float]:
        """
        Estimate processing time based on file size and system specs.
        
        Args:
            file_path: Path to TTL file
            
        Returns:
            Dictionary with time estimates
        """
        file_size_gb = os.path.getsize(file_path) / (1024**3)
        
        # Empirical estimates based on benchmarks
        # These are conservative estimates for safety
        estimates = {
            "file_size_gb": file_size_gb,
            "estimated_triples": int(file_size_gb * 15_000_000),  # ~15M triples per GB
            "parsing_time_hours": file_size_gb * 0.5,  # 30 min per GB with streaming
            "conversion_time_hours": file_size_gb * 0.1,  # 6 min per GB
            "upload_time_hours": file_size_gb * 0.3,  # 18 min per GB
            "total_time_hours": file_size_gb * 0.9,  # ~54 min per GB total
            "memory_required_gb": min(self.max_memory_gb, file_size_gb * 0.1)  # Much less with streaming
        }
        
        return estimates
    
    def check_system_requirements(self, file_path: str) -> bool:
        """
        Check if system can handle the file processing (cross-platform).
        
        Args:
            file_path: Path to TTL file
            
        Returns:
            True if system can handle it
        """
        estimates = self.estimate_processing_time(file_path)
        
        # Check available memory
        try:
            import psutil
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            
            if estimates["memory_required_gb"] > available_memory_gb:
                self.logger.warning(
                    f"Insufficient memory. Required: {estimates['memory_required_gb']:.1f}GB, "
                    f"Available: {available_memory_gb:.1f}GB"
                )
                return False
        except ImportError:
            self.logger.info("psutil not available, skipping memory check")
        
        # Check disk space for temporary files (cross-platform)
        try:
            if platform.system() == "Windows":
                # Use shutil.disk_usage for Windows
                total, used, free = shutil.disk_usage(self.temp_dir)
                free_space_gb = free / (1024**3)
            else:
                # Use os.statvfs for Unix/Linux
                statvfs = os.statvfs(self.temp_dir)
                free_space_gb = statvfs.f_frsize * statvfs.f_avail / (1024**3)
            
            required_space_gb = estimates["file_size_gb"] * 1.5  # 50% overhead for temp files
            
            if required_space_gb > free_space_gb:
                self.logger.warning(
                    f"Insufficient disk space. Required: {required_space_gb:.1f}GB, "
                    f"Available: {free_space_gb:.1f}GB"
                )
                return False
                
        except Exception as e:
            self.logger.warning(f"Could not check disk space: {e}")
            # Continue anyway - not critical
            
        return True
    
    def split_large_ttl(self, file_path: str) -> List[str]:
        """
        Split large TTL file into manageable chunks.
        
        Args:
            file_path: Path to large TTL file
            
        Returns:
            List of chunk file paths
        """
        self.logger.info(f"Splitting large TTL file: {file_path}")
        
        chunk_files = []
        chunk_size_bytes = self.chunk_size_mb * 1024 * 1024
        
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        chunk_num = 0
        current_chunk_size = 0
        current_chunk_lines = []
        prefix_lines = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            # Extract prefix declarations
            for line in f:
                line = line.strip()
                if line.startswith('@prefix') or line.startswith('@base'):
                    prefix_lines.append(line)
                elif line and not line.startswith('#'):
                    # First non-prefix line, reset file pointer
                    f.seek(0)
                    break
            
            # Process the file in chunks
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                    
                # Skip prefix declarations in main content
                if line.startswith('@prefix') or line.startswith('@base'):
                    if line not in prefix_lines:
                        prefix_lines.append(line)
                    continue
                
                line_size = len(line.encode('utf-8'))
                current_chunk_size += line_size
                current_chunk_lines.append(line)
                
                # Check if chunk is large enough
                if current_chunk_size >= chunk_size_bytes:
                    # Write chunk file
                    chunk_file = os.path.join(
                        self.temp_dir, 
                        f"{base_name}_chunk_{chunk_num:04d}.ttl"
                    )
                    
                    with open(chunk_file, 'w', encoding='utf-8') as chunk_f:
                        # Write prefixes
                        for prefix_line in prefix_lines:
                            chunk_f.write(f"{prefix_line}\n")
                        chunk_f.write("\n")
                        
                        # Write chunk content
                        for chunk_line in current_chunk_lines:
                            chunk_f.write(f"{chunk_line}\n")
                    
                    chunk_files.append(chunk_file)
                    chunk_num += 1
                    current_chunk_size = 0
                    current_chunk_lines = []
                    
                    self.logger.info(f"Created chunk {chunk_num}: {chunk_file}")
            
            # Write final chunk if there's remaining data
            if current_chunk_lines:
                chunk_file = os.path.join(
                    self.temp_dir, 
                    f"{base_name}_chunk_{chunk_num:04d}.ttl"
                )
                
                with open(chunk_file, 'w', encoding='utf-8') as chunk_f:
                    for prefix_line in prefix_lines:
                        chunk_f.write(f"{prefix_line}\n")
                    chunk_f.write("\n")
                    
                    for chunk_line in current_chunk_lines:
                        chunk_f.write(f"{chunk_line}\n")
                
                chunk_files.append(chunk_file)
        
        self.logger.info(f"Split into {len(chunk_files)} chunks")
        return chunk_files
    
    def process_chunk_with_original_converter(self, chunk_file: str) -> Dict:
        """
        Process a single chunk using the embedded converter.
        
        Args:
            chunk_file: Path to chunk file
            
        Returns:
            Dictionary with chunk processing results
        """
        try:
            # Create converter for this chunk
            converter = TTLToPropertyGraphConverter()
            
            # Load and convert chunk
            converter.load_ttl_file(chunk_file)
            converter.convert_to_property_graph()
            
            return {
                "success": True,
                "file": chunk_file,
                "nodes": len(converter.nodes),
                "relationships": len(converter.relationships),
                "triples": len(converter.rdf_graph),
                "node_data": converter.nodes,
                "relationship_data": converter.relationships,
                "node_labels": converter.node_labels
            }
            
        except Exception as e:
            self.logger.error(f"Error processing chunk {chunk_file}: {e}")
            return {
                "success": False,
                "file": chunk_file,
                "error": str(e)
            }
    
    def merge_chunk_results(self, chunk_results: List[Dict]) -> Dict:
        """
        Merge results from multiple chunks.
        
        Args:
            chunk_results: List of chunk processing results
            
        Returns:
            Merged results dictionary
        """
        merged_nodes = {}
        merged_relationships = []
        merged_node_labels = defaultdict(set)
        total_triples = 0
        
        for result in chunk_results:
            if not result["success"]:
                continue
                
            total_triples += result["triples"]
            
            # Merge nodes (avoid duplicates)
            for node_id, node_data in result["node_data"].items():
                if node_id in merged_nodes:
                    # Merge properties for existing nodes
                    merged_nodes[node_id].update(node_data)
                else:
                    merged_nodes[node_id] = node_data
            
            # Merge relationships
            merged_relationships.extend(result["relationship_data"])
            
            # Merge node labels
            for node_id, labels in result["node_labels"].items():
                merged_node_labels[node_id].update(labels)
        
        return {
            "nodes": merged_nodes,
            "relationships": merged_relationships,
            "node_labels": merged_node_labels,
            "total_triples": total_triples
        }
    
    def _escape_cypher_string(self, value: str) -> str:
        """Escape string values for Cypher queries."""
        if isinstance(value, str):
            return value.replace("\\", "\\\\").replace("'", "\\'").replace('"', '\\"')
        return str(value)

    def _format_cypher_value(self, value) -> str:
        """Format a value for Cypher query."""
        if isinstance(value, str):
            return f"'{self._escape_cypher_string(value)}'"
        elif isinstance(value, bool):
            return "true" if value else "false"
        elif isinstance(value, (int, float)):
            return str(value)
        else:
            return f"'{self._escape_cypher_string(str(value))}'"

    def test_falkordb_connection(self) -> bool:
        """Test FalkorDB connection with provided credentials."""
        try:
            self.logger.info(f"Testing FalkorDB connection to {self.falkor_host}:{self.falkor_port}")
            
            # Create connection with authentication
            connection_params = {
                "host": self.falkor_host,
                "port": self.falkor_port
            }
            
            if self.falkor_password:
                connection_params["password"] = self.falkor_password
            if self.falkor_username:
                connection_params["username"] = self.falkor_username
                
            db = falkordb.FalkorDB(**connection_params)
            test_graph = db.select_graph("connection_test")
            
            # Simple test query
            test_graph.query("CREATE (test:TestNode {name: 'connection_test'})")
            result = test_graph.query("MATCH (test:TestNode) RETURN count(test)")
            test_graph.query("MATCH (test:TestNode) DELETE test")
            
            self.logger.info("‚úÖ FalkorDB connection successful")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå FalkorDB connection failed: {e}")
            return False

    def test_neo4j_connection(self) -> bool:
        """Test Neo4j connection with provided credentials."""
        try:
            self.logger.info(f"Testing Neo4j connection to {self.neo4j_uri}")
            
            driver = GraphDatabase.driver(
                self.neo4j_uri, 
                auth=(self.neo4j_username, self.neo4j_password)
            )
            
            with driver.session() as session:
                # Test query
                result = session.run("RETURN 1 as test")
                record = result.single()
                if record["test"] == 1:
                    self.logger.info("‚úÖ Neo4j connection successful")
                    driver.close()
                    return True
                    
        except Exception as e:
            self.logger.error(f"‚ùå Neo4j connection failed: {e}")
            return False

    def upload_large_data_to_falkordb(self, merged_data: Dict, graph_name: str = "large_graph"):
        """
        Upload large merged data to FalkorDB with optimized batching and authentication.
        """
        self.logger.info("Uploading large dataset to FalkorDB...")
        
        try:
            # Connect to FalkorDB with authentication
            connection_params = {
                "host": self.falkor_host,
                "port": self.falkor_port
            }
            
            if self.falkor_password:
                connection_params["password"] = self.falkor_password
            if self.falkor_username:
                connection_params["username"] = self.falkor_username
                
            db = falkordb.FalkorDB(**connection_params)
            graph = db.select_graph(graph_name)
            
            # Clear existing data
            self.logger.info("Clearing existing data...")
            graph.query("MATCH (n) DETACH DELETE n")
            
            # Upload nodes in large batches
            nodes = list(merged_data["nodes"].values())
            large_batch_size = min(1000, self.batch_size * 2)  # Adjust batch size
            
            self.logger.info(f"Uploading {len(nodes):,} nodes in batches of {large_batch_size}")
            
            for i in range(0, len(nodes), large_batch_size):
                batch = nodes[i:i + large_batch_size]
                batch_num = i // large_batch_size + 1
                total_batches = (len(nodes) // large_batch_size) + 1
                
                self.logger.info(f"Uploading node batch {batch_num}/{total_batches}")
                
                # Process nodes individually for better error handling
                for node in batch:
                    try:
                        node_id = node["id"]
                        labels = ":".join(merged_data["node_labels"].get(node_id, ["Resource"]))
                        
                        # Build properties string
                        properties = []
                        for key, value in node.items():
                            if key != "id":
                                properties.append(f"{key}: {self._format_cypher_value(value)}")
                        
                        props_str = "{" + ", ".join(properties) + "}" if properties else ""
                        query = f"CREATE (n:{labels} {props_str})"
                        graph.query(query)
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to create node {node.get('id', 'unknown')}: {e}")
            
            # Upload relationships in large batches
            relationships = merged_data["relationships"]
            self.logger.info(f"Uploading {len(relationships):,} relationships in batches of {large_batch_size}")
            
            for i in range(0, len(relationships), large_batch_size):
                batch = relationships[i:i + large_batch_size]
                batch_num = i // large_batch_size + 1
                total_batches = (len(relationships) // large_batch_size) + 1
                
                self.logger.info(f"Uploading relationship batch {batch_num}/{total_batches}")
                
                for rel in batch:
                    try:
                        source_labels = ":".join(merged_data["node_labels"].get(rel["source"], ["Resource"]))
                        target_labels = ":".join(merged_data["node_labels"].get(rel["target"], ["Resource"]))
                        
                        # Build relationship properties
                        rel_props = []
                        for key, value in rel.items():
                            if key not in ["source", "target", "type"]:
                                rel_props.append(f"{key}: {self._format_cypher_value(value)}")
                        
                        rel_props_str = "{" + ", ".join(rel_props) + "}" if rel_props else ""
                        
                        query = f"""
                        MATCH (a:{source_labels} {{id: {self._format_cypher_value(rel["source"])}}})
                        MATCH (b:{target_labels} {{id: {self._format_cypher_value(rel["target"])}}})
                        CREATE (a)-[r:{rel["type"]} {rel_props_str}]->(b)
                        """
                        graph.query(query)
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to create relationship {rel.get('type', 'unknown')}: {e}")
            
            self.logger.info(f"‚úÖ Successfully uploaded {len(nodes):,} nodes and {len(relationships):,} relationships to FalkorDB")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to upload to FalkorDB: {e}")
            raise

    def upload_large_data_to_neo4j(self, merged_data: Dict):
        """
        Upload large merged data to Neo4j with optimized batching and authentication.
        """
        self.logger.info("Uploading large dataset to Neo4j...")
        
        try:
            # Connect to Neo4j
            driver = GraphDatabase.driver(
                self.neo4j_uri, 
                auth=(self.neo4j_username, self.neo4j_password)
            )
            
            with driver.session() as session:
                # Clear existing data
                self.logger.info("Clearing existing data...")
                session.run("MATCH (n) DETACH DELETE n")
                
                # Upload nodes in large batches
                nodes = list(merged_data["nodes"].values())
                large_batch_size = min(1000, self.batch_size * 2)
                
                self.logger.info(f"Uploading {len(nodes):,} nodes in batches of {large_batch_size}")
                
                for i in range(0, len(nodes), large_batch_size):
                    batch = nodes[i:i + large_batch_size]
                    batch_num = i // large_batch_size + 1
                    total_batches = (len(nodes) // large_batch_size) + 1
                    
                    self.logger.info(f"Uploading node batch {batch_num}/{total_batches}")
                    
                    # Process nodes with parameterized queries for better performance
                    for node in batch:
                        try:
                            node_id = node["id"]
                            labels = ":".join(merged_data["node_labels"].get(node_id, ["Resource"]))
                            
                            # Prepare properties for parameterized query
                            properties = {k: v for k, v in node.items() if k != "id"}
                            
                            # Create node with parameters
                            query = f"CREATE (n:{labels} $props)"
                            session.run(query, props=properties)
                            
                        except Exception as e:
                            self.logger.warning(f"Failed to create node {node.get('id', 'unknown')}: {e}")
                
                # Upload relationships in large batches
                relationships = merged_data["relationships"]
                self.logger.info(f"Uploading {len(relationships):,} relationships in batches of {large_batch_size}")
                
                for i in range(0, len(relationships), large_batch_size):
                    batch = relationships[i:i + large_batch_size]
                    batch_num = i // large_batch_size + 1
                    total_batches = (len(relationships) // large_batch_size) + 1
                    
                    self.logger.info(f"Uploading relationship batch {batch_num}/{total_batches}")
                    
                    for rel in batch:
                        try:
                            source_labels = ":".join(merged_data["node_labels"].get(rel["source"], ["Resource"]))
                            target_labels = ":".join(merged_data["node_labels"].get(rel["target"], ["Resource"]))
                            
                            # Prepare relationship properties
                            rel_props = {k: v for k, v in rel.items() if k not in ["source", "target", "type"]}
                            
                            query = f"""
                            MATCH (a:{source_labels} {{id: $source_id}})
                            MATCH (b:{target_labels} {{id: $target_id}})
                            CREATE (a)-[r:{rel["type"]} $props]->(b)
                            """
                            session.run(query, 
                                       source_id=rel["source"], 
                                       target_id=rel["target"], 
                                       props=rel_props)
                            
                        except Exception as e:
                            self.logger.warning(f"Failed to create relationship {rel.get('type', 'unknown')}: {e}")
            
            driver.close()
            self.logger.info(f"‚úÖ Successfully uploaded {len(nodes):,} nodes and {len(relationships):,} relationships to Neo4j")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to upload to Neo4j: {e}")
            raise
    
    def convert_large_ttl(self, 
                         file_path: str,
                         upload_to_falkor: bool = False,
                         upload_to_neo4j: bool = False,
                         falkor_graph_name: str = "large_graph",
                         cleanup_chunks: bool = True) -> Dict:
        """
        Main method to convert large TTL files with optional database uploads.
        
        Args:
            file_path: Path to large TTL file
            upload_to_falkor: Upload to FalkorDB if True
            upload_to_neo4j: Upload to Neo4j if True
            falkor_graph_name: Name for the FalkorDB graph
            cleanup_chunks: Remove temporary chunk files
            
        Returns:
            Processing statistics
        """
        self.start_time = time.time()
        
        # Print estimates
        estimates = self.estimate_processing_time(file_path)
        self.logger.info("=== PROCESSING ESTIMATES ===")
        self.logger.info(f"File size: {estimates['file_size_gb']:.2f} GB")
        self.logger.info(f"Estimated triples: {estimates['estimated_triples']:,}")
        self.logger.info(f"Estimated total time: {estimates['total_time_hours']:.1f} hours")
        self.logger.info(f"Memory required: {estimates['memory_required_gb']:.1f} GB")
        
        # Test database connections if upload is requested
        if upload_to_falkor:
            self.logger.info("Testing FalkorDB connection...")
            if not self.test_falkordb_connection():
                raise RuntimeError("FalkorDB connection failed. Please check credentials and connection settings.")
        
        if upload_to_neo4j:
            self.logger.info("Testing Neo4j connection...")
            if not self.test_neo4j_connection():
                raise RuntimeError("Neo4j connection failed. Please check credentials and connection settings.")
        
        # Check system requirements
        if not self.check_system_requirements(file_path):
            raise RuntimeError("System requirements not met for processing this file")
        
        try:
            # Step 1: Split large file into chunks
            self.logger.info("Step 1: Splitting large TTL file...")
            chunk_files = self.split_large_ttl(file_path)
            
            # Step 2: Process chunks
            self.logger.info("Step 2: Processing chunks...")
            chunk_results = []
            
            if self.use_parallel and len(chunk_files) > 1:
                # Parallel processing
                with ThreadPoolExecutor(max_workers=min(4, len(chunk_files))) as executor:
                    future_to_chunk = {
                        executor.submit(self.process_chunk_with_original_converter, chunk): chunk 
                        for chunk in chunk_files
                    }
                    
                    for future in as_completed(future_to_chunk):
                        chunk = future_to_chunk[future]
                        try:
                            result = future.result()
                            chunk_results.append(result)
                            self.processed_chunks += 1
                            self.logger.info(f"Processed chunk {self.processed_chunks}/{len(chunk_files)}")
                        except Exception as e:
                            self.logger.error(f"Chunk {chunk} failed: {e}")
            else:
                # Sequential processing
                for i, chunk_file in enumerate(chunk_files):
                    self.logger.info(f"Processing chunk {i+1}/{len(chunk_files)}: {chunk_file}")
                    result = self.process_chunk_with_original_converter(chunk_file)
                    chunk_results.append(result)
                    self.processed_chunks += 1
            
            # Step 3: Merge results
            self.logger.info("Step 3: Merging chunk results...")
            merged_data = self.merge_chunk_results(chunk_results)
            
            # Step 4: Upload to databases (optional)
            upload_stats = {"falkor_success": False, "neo4j_success": False}
            
            if upload_to_falkor:
                try:
                    self.logger.info("Step 4a: Uploading to FalkorDB...")
                    self.upload_large_data_to_falkordb(merged_data, falkor_graph_name)
                    upload_stats["falkor_success"] = True
                except Exception as e:
                    self.logger.error(f"FalkorDB upload failed: {e}")
                    upload_stats["falkor_error"] = str(e)
            
            if upload_to_neo4j:
                try:
                    self.logger.info("Step 4b: Uploading to Neo4j...")
                    self.upload_large_data_to_neo4j(merged_data)
                    upload_stats["neo4j_success"] = True
                except Exception as e:
                    self.logger.error(f"Neo4j upload failed: {e}")
                    upload_stats["neo4j_error"] = str(e)
            
            # Step 5: Cleanup
            if cleanup_chunks:
                self.logger.info("Step 5: Cleaning up temporary files...")
                for chunk_file in chunk_files:
                    try:
                        os.remove(chunk_file)
                    except Exception as e:
                        self.logger.warning(f"Failed to remove {chunk_file}: {e}")
            
            # Calculate final statistics
            total_time = time.time() - self.start_time
            stats = {
                "total_nodes": len(merged_data["nodes"]),
                "total_relationships": len(merged_data["relationships"]),
                "total_triples": merged_data["total_triples"],
                "processing_time_seconds": total_time,
                "processing_time_hours": total_time / 3600,
                "chunks_processed": self.processed_chunks,
                "triples_per_second": merged_data["total_triples"] / total_time if total_time > 0 else 0,
                "upload_stats": upload_stats
            }
            
            self.logger.info("=== FINAL STATISTICS ===")
            self.logger.info(f"Total nodes: {stats['total_nodes']:,}")
            self.logger.info(f"Total relationships: {stats['total_relationships']:,}")
            self.logger.info(f"Total triples: {stats['total_triples']:,}")
            self.logger.info(f"Processing time: {stats['processing_time_hours']:.2f} hours")
            self.logger.info(f"Throughput: {stats['triples_per_second']:.0f} triples/second")
            
            if upload_to_falkor:
                status = "‚úÖ Success" if upload_stats["falkor_success"] else "‚ùå Failed"
                self.logger.info(f"FalkorDB upload: {status}")
            
            if upload_to_neo4j:
                status = "‚úÖ Success" if upload_stats["neo4j_success"] else "‚ùå Failed"
                self.logger.info(f"Neo4j upload: {status}")
            
            return stats
            
        except Exception as e:
            self.logger.error(f"Error during conversion: {e}")
            raise


def main():
    """Example usage for large TTL conversion with configurable authentication."""
    import argparse
    
    # Create argument parser
    parser = argparse.ArgumentParser(
        description="Convert large TTL files to property graphs with optional database uploads",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Convert only (no database upload)
    python large_ttl_converter.py data.ttl
    
    # Upload to FalkorDB only
    python large_ttl_converter.py data.ttl --falkor --falkor-password mypass
    
    # Upload to Neo4j only  
    python large_ttl_converter.py data.ttl --neo4j --neo4j-password secret
    
    # Upload to both databases
    python large_ttl_converter.py data.ttl --falkor --neo4j --neo4j-password secret
    
    # Custom connection settings
    python large_ttl_converter.py data.ttl --falkor --falkor-host myserver --falkor-port 6380
        """
    )
    
    # Required arguments
    parser.add_argument("ttl_file", help="Path to the TTL file to convert")
    
    # Database selection
    parser.add_argument("--falkor", action="store_true", help="Upload to FalkorDB")
    parser.add_argument("--neo4j", action="store_true", help="Upload to Neo4j")
    
    # FalkorDB connection settings
    parser.add_argument("--falkor-host", default="localhost", help="FalkorDB host (default: localhost)")
    parser.add_argument("--falkor-port", type=int, default=6379, help="FalkorDB port (default: 6379)")
    parser.add_argument("--falkor-username", help="FalkorDB username")
    parser.add_argument("--falkor-password", help="FalkorDB password")
    parser.add_argument("--falkor-graph", default="converted_graph", help="FalkorDB graph name (default: converted_graph)")
    
    # Neo4j connection settings
    parser.add_argument("--neo4j-uri", default="bolt://localhost:7687", help="Neo4j URI (default: bolt://localhost:7687)")
    parser.add_argument("--neo4j-username", default="neo4j", help="Neo4j username (default: neo4j)")
    parser.add_argument("--neo4j-password", default="password", help="Neo4j password (default: password)")
    
    # Processing options
    parser.add_argument("--chunk-size", type=int, default=200, help="Chunk size in MB (default: 200)")
    parser.add_argument("--batch-size", type=int, default=1000, help="Database batch size (default: 1000)")
    parser.add_argument("--max-memory", type=int, default=8, help="Maximum memory usage in GB (default: 8)")
    parser.add_argument("--no-parallel", action="store_true", help="Disable parallel processing")
    parser.add_argument("--keep-chunks", action="store_true", help="Keep temporary chunk files")
    parser.add_argument("--temp-dir", help="Temporary directory for chunks")
    
    args = parser.parse_args()
    
    # Validate arguments
    if not os.path.exists(args.ttl_file):
        print(f"‚ùå Error: TTL file '{args.ttl_file}' not found")
        return 1
    
    if not args.falkor and not args.neo4j:
        print("‚ö†Ô∏è  Warning: No database upload specified. Only converting to property graph format.")
        print("   Use --falkor and/or --neo4j to upload to databases.")
    
    # Configure converter with user-provided settings
    converter = LargeTTLConverter(
        chunk_size_mb=args.chunk_size,
        batch_size=args.batch_size,
        max_memory_gb=args.max_memory,
        use_parallel=not args.no_parallel,
        temp_dir=args.temp_dir,
        # FalkorDB settings
        falkor_host=args.falkor_host,
        falkor_port=args.falkor_port,
        falkor_username=args.falkor_username,
        falkor_password=args.falkor_password,
        # Neo4j settings
        neo4j_uri=args.neo4j_uri,
        neo4j_username=args.neo4j_username,
        neo4j_password=args.neo4j_password
    )
    
    # Display configuration
    print("üîß Configuration:")
    print(f"   File: {args.ttl_file}")
    print(f"   Chunk size: {args.chunk_size} MB")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Max memory: {args.max_memory} GB")
    print(f"   Parallel processing: {not args.no_parallel}")
    
    if args.falkor:
        print(f"   FalkorDB: {args.falkor_host}:{args.falkor_port} (graph: {args.falkor_graph})")
        if args.falkor_username:
            print(f"   FalkorDB auth: {args.falkor_username}/{'*' * len(args.falkor_password or '')}")
    
    if args.neo4j:
        print(f"   Neo4j: {args.neo4j_uri}")
        print(f"   Neo4j auth: {args.neo4j_username}/{'*' * len(args.neo4j_password)}")
    
    print()
    
    # Process the file
    try:
        stats = converter.convert_large_ttl(
            file_path=args.ttl_file,
            upload_to_falkor=args.falkor,
            upload_to_neo4j=args.neo4j,
            falkor_graph_name=args.falkor_graph,
            cleanup_chunks=not args.keep_chunks
        )
        
        print("\nüéâ Conversion completed successfully!")
        print(f"üìä Processed {stats['total_triples']:,} triples in {stats['processing_time_hours']:.2f} hours")
        print(f"‚ö° Throughput: {stats['triples_per_second']:.0f} triples/second")
        
        # Show upload results
        if args.falkor:
            if stats['upload_stats']['falkor_success']:
                print(f"‚úÖ FalkorDB upload successful")
                print(f"üîç Query your data: redis-cli -p {args.falkor_port} GRAPH.QUERY {args.falkor_graph} \"MATCH (n) RETURN count(n)\"")
            else:
                print(f"‚ùå FalkorDB upload failed: {stats['upload_stats'].get('falkor_error', 'Unknown error')}")
        
        if args.neo4j:
            if stats['upload_stats']['neo4j_success']:
                print(f"‚úÖ Neo4j upload successful")
                print(f"üåê Access Neo4j Browser at: http://localhost:7474")
            else:
                print(f"‚ùå Neo4j upload failed: {stats['upload_stats'].get('neo4j_error', 'Unknown error')}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Conversion failed: {e}")
        return 1


# Alternative programmatic usage examples
def example_falkor_only():
    """Example: Convert and upload to FalkorDB only."""
    converter = LargeTTLConverter(
        chunk_size_mb=150,
        falkor_host="localhost",
        falkor_port=6379,
        falkor_password="your_falkor_password"  # Set if needed
    )
    
    stats = converter.convert_large_ttl(
        file_path="large_dataset.ttl",
        upload_to_falkor=True,
        upload_to_neo4j=False,
        falkor_graph_name="my_knowledge_graph"
    )
    
    return stats


def example_neo4j_only():
    """Example: Convert and upload to Neo4j only."""
    converter = LargeTTLConverter(
        chunk_size_mb=150,
        neo4j_uri="bolt://localhost:7687",
        neo4j_username="neo4j",
        neo4j_password="your_neo4j_password"
    )
    
    stats = converter.convert_large_ttl(
        file_path="large_dataset.ttl",
        upload_to_falkor=False,
        upload_to_neo4j=True
    )
    
    return stats


def example_both_databases():
    """Example: Convert and upload to both databases."""
    converter = LargeTTLConverter(
        chunk_size_mb=200,
        # FalkorDB settings
        falkor_host="falkor.example.com",
        falkor_port=6379,
        falkor_username="admin",
        falkor_password="falkor_secret",
        # Neo4j settings
        neo4j_uri="bolt://neo4j.example.com:7687",
        neo4j_username="neo4j",
        neo4j_password="neo4j_secret"
    )
    
    stats = converter.convert_large_ttl(
        file_path="large_dataset.ttl",
        upload_to_falkor=True,
        upload_to_neo4j=True,
        falkor_graph_name="enterprise_kg"
    )
    
    return stats


def example_custom_authentication():
    """Example: Custom authentication for enterprise setups."""
    converter = LargeTTLConverter(
        # Performance settings for large files
        chunk_size_mb=300,
        batch_size=2000,
        max_memory_gb=16,
        use_parallel=True,
        
        # FalkorDB with custom auth
        falkor_host="falkor.company.com",
        falkor_port=6380,
        falkor_username="data_engineer",
        falkor_password="complex_falkor_password_123",
        
        # Neo4j with custom auth
        neo4j_uri="bolt://neo4j.company.com:7687",
        neo4j_username="graph_admin",
        neo4j_password="complex_neo4j_password_456"
    )
    
    # Test connections before processing
    print("Testing database connections...")
    falkor_ok = converter.test_falkordb_connection()
    neo4j_ok = converter.test_neo4j_connection()
    
    if not falkor_ok or not neo4j_ok:
        print("‚ùå Database connection failed!")
        return None
    
    print("‚úÖ All database connections successful!")
    
    # Process the large file
    stats = converter.convert_large_ttl(
        file_path="enterprise_data.ttl",
        upload_to_falkor=True,
        upload_to_neo4j=True,
        falkor_graph_name="enterprise_knowledge_graph"
    )
    
    return stats


if __name__ == "__main__":
    main()
