"""
Vector retrieval node for the Agentic RAG system.

This module implements the vector retrieval node for the Agentic RAG system,
which retrieves candidate business terms using vector similarity search.
"""

import logging
from typing import Dict, Any, List, Optional
import time

from app.core.embedding import MyDocument

logger = logging.getLogger(__name__)

async def vector_retrieval(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Retrieve candidates using vector similarity search.
    Uses a lower threshold than normal to ensure sufficient recall.
    
    Args:
        state: Current graph state
        
    Returns:
        Updated state with vector retrieval results
    """
    element_name = state.get('element_name', 'Unknown')
    logger.info(f"Performing vector retrieval for: {element_name}")
    
    try:
        # Access the business term manager from state
        bt_manager = state.get("_bt_manager")
        if not bt_manager:
            raise ValueError("Business term manager not available in state")
        
        # Get query information
        query = state.get('query', {})
        original_query = query.get('original_text')
        rephrased_query = query.get('rephrased_text')
        
        # Use rephrased query if available, otherwise use original
        query_text = rephrased_query or original_query
        if not query_text:
            query_text = f"{state.get('element_name', '')}. {state.get('element_description', '')}"
        
        # Generate query for vector search
        query_doc = MyDocument(id=f"query_{state.get('element_id', 'unknown')}", text=query_text)
        
        # Time the embedding generation
        start_time = time.time()
        
        # Generate embedding
        embedding_client = bt_manager.embedding_client
        embedded_query = embedding_client.generate_embeddings(query_doc)
        
        embedding_time = time.time() - start_time
        logger.debug(f"Embedding generation took {embedding_time:.2f}s")
        
        if not embedded_query or not embedded_query.embedding:
            raise ValueError("Failed to generate embedding for the query")
        
        # Get more candidates than needed for better coverage
        num_candidates = max(state.get('top_k', 3) * 5, 30)
        
        # Use a lower threshold for better recall
        initial_threshold = max(0.2, state.get('threshold', 0.5) - 0.2)
        
        # Time the vector search
        search_start_time = time.time()
        
        # Perform vector search
        vector_results = bt_manager.vector_store.find_similar_vectors(
            query_vector=embedded_query.embedding,
            top_k=num_candidates,
            threshold=initial_threshold
        )
        
        search_time = time.time() - search_start_time
        logger.debug(f"Vector search took {search_time:.2f}s")
        
        logger.info(f"Vector retrieval found {len(vector_results)} candidates")
        
        # Add scores to candidates
        candidates = []
        for result in vector_results:
            candidates.append({
                "id": result.get("id", ""),
                "name": result.get("name", ""),
                "description": result.get("description", ""),
                "metadata": result.get("metadata", {}),
                "vector_score": result.get("similarity", 0.0),
                "keyword_score": 0.0,
                "semantic_score": None,
                "final_score": None,
                "reasoning": None
            })
        
        # Update state
        state["vector_results"] = vector_results
        state["candidates"] = candidates
        
        return state
    
    except Exception as e:
        logger.error(f"Error in vector retrieval: {e}", exc_info=True)
        state["error"] = f"Vector retrieval failed: {str(e)}"
        state["vector_results"] = []
        state["candidates"] = []
        return state


def get_default_search_config(top_k: int = 5, threshold: float = 0.3) -> Dict[str, Any]:
    """
    Get default search configuration for vector retrieval.
    
    Args:
        top_k: Number of results to retrieve
        threshold: Minimum similarity threshold
        
    Returns:
        Search configuration dictionary
    """
    return {
        "num_candidates": max(top_k * 5, 30),  # Get more candidates than needed
        "threshold": max(0.2, threshold - 0.2),  # Lower threshold for better recall
        "use_semantic_reranking": True,  # Enable semantic reranking
        "rerank_top_k": min(top_k * 2, 20)  # Number of candidates to rerank
    }
