#!/usr/bin/env python
"""
PBT Import Script - Import Preferred Business Terms (PBTs) from a CSV file.

This script imports PBTs from a CSV file into the vector database (ChromaDB or PostgreSQL).
It uses the BusinessTermManager to handle the data import and vector embedding generation.
Only new terms will have embeddings generated, existing terms will have their metadata and
synonyms preserved and updated.
"""

import os
import sys
import logging
import argparse
import time
import uuid
import json
import chardet
import pandas as pd
from typing import Optional, Dict, List, Any

# Add parent directory to path for module imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.core.business_terms import BusinessTermManager
from app.config.environment import get_os_env
from app.config.settings import get_vector_store
from app.core.embedding import EmbeddingClient, MyDocument

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [%(name)s:%(lineno)d] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

def detect_encoding(file_path: str) -> str:
    """
    Detect the encoding of a file.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Detected encoding (defaults to utf-8)
    """
    try:
        # Read a sample of the file
        with open(file_path, 'rb') as f:
            sample = f.read(min(1024 * 1024, os.path.getsize(file_path)))
        
        # Detect encoding
        result = chardet.detect(sample)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"Detected encoding: {encoding} with confidence {confidence:.2f}")
        
        if encoding and confidence > 0.7:
            return encoding
        else:
            logger.warning(f"Encoding detection uncertain. Using UTF-8.")
            return 'utf-8'
    
    except Exception as e:
        logger.error(f"Error detecting encoding: {e}")
        return 'utf-8'

def main():
    """Main function to import PBTs from a CSV file."""
    parser = argparse.ArgumentParser(description="Import Preferred Business Terms (PBTs) from a CSV file")
    parser.add_argument("csv_file", help="Path to the CSV file containing PBTs")
    parser.add_argument("--encoding", help="Encoding of the CSV file", default=None)
    parser.add_argument("--batch-size", type=int, default=100, help="Batch size for import")
    parser.add_argument("--vector-db-type", help="Type of vector database to use: chroma or postgresql", default="chroma")
    parser.add_argument("--chroma-dir", help="Directory for ChromaDB persistent storage", default="./data/chroma_db")
    parser.add_argument("--chroma-collection", help="Collection name for ChromaDB", default="business_terms")
    parser.add_argument("--embedding-model", help="OpenAI embedding model to use", default="text-embedding-3-large")
    parser.add_argument("--force-regenerate", action="store_true", help="Force regeneration of embeddings and synonyms for all terms")
    
    args = parser.parse_args()
    
    # Ensure CSV file exists
    if not os.path.exists(args.csv_file):
        logger.error(f"CSV file not found: {args.csv_file}")
        sys.exit(1)
    
    # Set environment variables
    if args.vector_db_type:
        os.environ["VECTOR_DB_TYPE"] = args.vector_db_type
    if args.chroma_dir:
        os.environ["CHROMA_PERSIST_DIR"] = args.chroma_dir
    if args.chroma_collection:
        os.environ["CHROMA_COLLECTION"] = args.chroma_collection
    if args.embedding_model:
        os.environ["EMBEDDING_MODEL"] = args.embedding_model
    
    # Auto-detect encoding if not provided
    encoding = args.encoding
    if not encoding:
        encoding = detect_encoding(args.csv_file)
    
    logger.info(f"Starting PBT import from {args.csv_file} with encoding {encoding}")
    logger.info(f"Using vector database type: {os.environ.get('VECTOR_DB_TYPE', 'chroma')}")
    logger.info(f"Using embedding model: {os.environ.get('EMBEDDING_MODEL', 'text-embedding-3-large')}")
    
    # Initialize the environment settings
    env = get_os_env()
    
    try:
        # Initialize the BusinessTermManager
        logger.info("Initializing BusinessTermManager...")
        term_manager = BusinessTermManager()
        
        # Get initial term count
        initial_count = term_manager.get_term_count()
        logger.info(f"Initial term count: {initial_count}")
        
        # Get existing terms for checking duplication
        existing_terms_dict = {}
        if not args.force_regenerate:
            logger.info("Loading existing terms to preserve embeddings and synonyms...")
            existing_terms = term_manager.get_all_terms()
            for term in existing_terms:
                # Create lookup by both ID and name
                existing_terms_dict[term.id] = term
                existing_terms_dict[term.name.lower()] = term
            logger.info(f"Loaded {len(existing_terms)} existing terms")
        
        # Read the CSV file
        logger.info(f"Reading CSV file: {args.csv_file}")
        df = pd.read_csv(args.csv_file, encoding=encoding)
        
        # Check required columns
        required_columns = ["PBT_NAME", "PBT_DEFINITION"]
        if not all(col in df.columns for col in required_columns):
            missing = [col for col in required_columns if col not in df.columns]
            logger.error(f"Missing required columns: {missing}")
            sys.exit(1)
        
        # Add ID column if not present
        if "id" not in df.columns:
            logger.info("Adding ID column")
            df["id"] = [f"pbt_{uuid.uuid4()}" for _ in range(len(df))]
        
        # Initialize embedding client directly for synonym generation
        embedding_client = EmbeddingClient(env)
        
        # Process terms in batches
        total_count = len(df)
        batch_size = args.batch_size
        added_count = 0
        updated_count = 0
        skipped_count = 0
        items_to_add = []
        
        for i in range(0, total_count, batch_size):
            batch_df = df.iloc[i:i+batch_size]
            batch_items = []
            
            logger.info(f"Processing batch {i//batch_size + 1} of {(total_count + batch_size - 1)//batch_size}")
            
            for _, row in batch_df.iterrows():
                # Get term data
                term_id = str(row["id"])
                name = row["PBT_NAME"]
                definition = row["PBT_DEFINITION"]
                
                # Create metadata from all other columns
                metadata = {}
                for col in df.columns:
                    if col not in ["id", "PBT_NAME", "PBT_DEFINITION"] and pd.notna(row[col]):
                        # Clean up key name for ChromaDB compatibility
                        key = col.lower().replace(" ", "_").replace("-", "_")
                        metadata[key] = str(row[col])
                
                # Check if term already exists
                existing_term = None
                if term_id in existing_terms_dict:
                    existing_term = existing_terms_dict[term_id]
                elif name.lower() in existing_terms_dict:
                    existing_term = existing_terms_dict[name.lower()]
                
                if existing_term and not args.force_regenerate:
                    # Update existing term - preserve embeddings but update metadata
                    logger.debug(f"Updating existing term: {name}")
                    
                    # Preserve existing synonyms in metadata
                    if hasattr(existing_term, 'metadata') and existing_term.metadata:
                        # Extract synonyms from metadata if they exist
                        existing_synonyms = existing_term.metadata.get('synonyms_str', '')
                        if existing_synonyms:
                            metadata['synonyms_str'] = existing_synonyms
                    
                    # Add to existing term's metadata rather than replacing
                    if hasattr(existing_term, 'metadata') and existing_term.metadata:
                        for key, value in existing_term.metadata.items():
                            if key not in metadata:
                                metadata[key] = value
                    
                    # Update the term with new metadata but keep the existing embedding
                    term_manager.vector_store.update_term_metadata(
                        term_id=existing_term.id,
                        name=name,
                        description=definition, 
                        metadata=metadata
                    )
                    
                    updated_count += 1
                else:
                    # New term or force regenerate - generate embeddings and synonyms
                    logger.debug(f"Adding new term: {name}")
                    
                    # Generate synonyms for this term
                    synonyms = embedding_client.generate_synonyms(
                        term_name=name,
                        term_definition=definition,
                        max_synonyms=10
                    )
                    
                    # Add synonyms to metadata
                    synonyms_str = ", ".join(synonyms) if synonyms else ""
                    metadata['synonyms_str'] = synonyms_str
                    metadata['synonym_count'] = len(synonyms)
                    
                    # Create embedding document
                    embedding_text = f"PBT Name: {name}. Description: {definition}"
                    if 'cdm' in metadata:
                        embedding_text += f" CDM: {metadata['cdm']}"
                    
                    # Create document for embedding
                    doc = MyDocument(
                        id=term_id,
                        text=embedding_text,
                        metadata=metadata
                    )
                    
                    # Generate embedding
                    doc_with_embedding = embedding_client.generate_embeddings(doc)
                    
                    if not doc_with_embedding.embedding:
                        logger.warning(f"Could not generate embedding for '{name}', skipping")
                        skipped_count += 1
                        continue
                    
                    # Add to batch
                    batch_items.append({
                        "id": term_id,
                        "name": name,
                        "description": definition,
                        "embedding": doc_with_embedding.embedding,
                        "metadata": metadata
                    })
                    
                    added_count += 1
            
            # Add batch to vector store
            if batch_items:
                items_to_add.extend(batch_items)
                
                # Process in smaller sub-batches to avoid overwhelming the vector store
                sub_batch_size = 20
                for j in range(0, len(items_to_add), sub_batch_size):
                    sub_batch = items_to_add[j:j+sub_batch_size]
                    logger.info(f"Adding sub-batch of {len(sub_batch)} terms to vector store")
                    term_manager.vector_store.batch_store_vectors(sub_batch)
                
                # Clear the items to add
                items_to_add = []
        
        # Get final term count
        final_count = term_manager.get_term_count()
        
        logger.info(f"Import complete. Summary:")
        logger.info(f"Initial term count: {initial_count}")
        logger.info(f"Final term count: {final_count}")
        logger.info(f"Terms added: {added_count}")
        logger.info(f"Terms updated: {updated_count}")
        logger.info(f"Terms skipped: {skipped_count}")
        
    except Exception as e:
        logger.error(f"Error importing terms: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
