"""
GDC Record Class Mapping System using OpenAI o3-mini with LangGraph ReAct Agents
Supports ONE-TO-MANY mappings with RAG using text-embedding-3-large
Uses OpenAI API directly for embeddings (no tiktoken)
Implements dynamic chain of thought followed by mixture of experts pattern
Uses Pydantic v2 for validation and latest LangChain/LangGraph
Uses InMemoryVectorStore for semantic search
Excel output with grouping by Record Class
"""

import json
import os
from typing import List, Dict, Optional, Any
import pandas as pd
from pydantic import BaseModel, Field, field_validator
from langchain_openai import ChatOpenAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
import re
from openai import OpenAI
import time
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils.dataframe import dataframe_to_rows

# ==================== GLOBAL CONFIGURATION ====================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_MODEL = "o3-mini"
REASONING_EFFORT = "high"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Validate API key before initializing clients
if not OPENAI_API_KEY:
    print("WARNING: OPENAI_API_KEY is not set. Please set it before running.")

# Initialize OpenAI client for embeddings with proper configuration
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    timeout=60.0,  # 60 second timeout
    max_retries=3   # Retry up to 3 times
)

# Initialize global LLM instance with reasoning_effort as direct parameter
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    reasoning_effort=REASONING_EFFORT
)

# ==================== CUSTOM EMBEDDINGS CLASS ====================

class OpenAIDirectEmbeddings(Embeddings):
    """Custom embeddings class using OpenAI API directly without tiktoken"""
    
    def __init__(self, model: str = EMBEDDING_MODEL, dimensions: int = EMBEDDING_DIMENSIONS):
        self.model = model
        self.dimensions = dimensions
        self.client = openai_client
        self.batch_size = 50  # Reduced batch size for stability
        self.max_chars = 30000  # Approximate character limit (8192 tokens ≈ 30k chars)
        self.chunk_overlap = 200  # Character overlap between chunks
    
    def chunk_text(self, text: str) -> List[str]:
        """Dynamically chunk text if it exceeds character limit"""
        if len(text) <= self.max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            # Calculate end position
            end = start + self.max_chars
            
            # If not at the end, try to break at sentence boundary
            if end < len(text):
                # Look for sentence endings near the chunk boundary
                search_start = max(start, end - 500)
                sentence_ends = ['.', '!', '?', '\n\n']
                
                best_break = None
                for delimiter in sentence_ends:
                    pos = text.rfind(delimiter, search_start, end)
                    if pos != -1:
                        best_break = pos + 1
                        break
                
                # If no sentence boundary found, break at word boundary
                if best_break is None:
                    pos = text.rfind(' ', search_start, end)
                    if pos != -1:
                        best_break = pos
                    else:
                        best_break = end
                
                end = best_break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # Move start position with overlap
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def embed_single_text(self, text: str) -> List[float]:
        """Embed a single text with automatic chunking if needed"""
        chunks = self.chunk_text(text)
        
        if len(chunks) == 1:
            # Single chunk - embed directly
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[text],
                    dimensions=self.dimensions
                )
                return response.data[0].embedding
            except Exception as e:
                error_msg = str(e).lower()
                if "maximum" in error_msg and "token" in error_msg:
                    # Even our estimate was wrong, force chunk more aggressively
                    print(f"  ⚠ Text still too long, forcing more aggressive chunking...")
                    smaller_chunks = self.chunk_text_aggressive(text)
                    return self.average_embeddings(smaller_chunks)
                raise
        else:
            # Multiple chunks - embed each and average
            print(f"  📄 Text chunked into {len(chunks)} parts for embedding")
            return self.average_embeddings(chunks)
    
    def chunk_text_aggressive(self, text: str, max_chars: int = 15000) -> List[str]:
        """More aggressive chunking for very long texts"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + max_chars, len(text))
            
            # Try to break at word boundary
            if end < len(text):
                space_pos = text.rfind(' ', start, end)
                if space_pos > start:
                    end = space_pos
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def average_embeddings(self, chunks: List[str]) -> List[float]:
        """Create embeddings for chunks and return averaged embedding"""
        chunk_embeddings = []
        
        for i, chunk in enumerate(chunks):
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[chunk],
                    dimensions=self.dimensions
                )
                chunk_embeddings.append(response.data[0].embedding)
                time.sleep(0.2)  # Small delay between chunks
            except Exception as e:
                print(f"  ✗ Failed to embed chunk {i+1}/{len(chunks)}: {e}")
                # Skip failed chunks
                continue
        
        if not chunk_embeddings:
            # All chunks failed, return zero vector
            return [0.0] * self.dimensions
        
        # Average the embeddings
        avg_embedding = [
            sum(emb[i] for emb in chunk_embeddings) / len(chunk_embeddings)
            for i in range(self.dimensions)
        ]
        
        return avg_embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using OpenAI API directly with batch processing and chunking"""
        if not texts:
            return []
        
        print(f"Embedding {len(texts)} documents with dynamic chunking...")
        all_embeddings = []
        
        try:
            # Process documents individually to handle chunking properly
            for i, text in enumerate(texts):
                if (i + 1) % 10 == 0:
                    print(f"  Progress: {i+1}/{len(texts)} documents...")
                
                try:
                    # Check if text needs chunking
                    if len(text) > self.max_chars:
                        print(f"  📏 Document {i+1} is long ({len(text)} chars), chunking...")
                    
                    embedding = self.embed_single_text(text)
                    all_embeddings.append(embedding)
                    
                    # Rate limiting
                    if (i + 1) % self.batch_size == 0:
                        time.sleep(1.0)  # Longer delay after batch
                    else:
                        time.sleep(0.1)  # Short delay between documents
                    
                except Exception as doc_error:
                    print(f"  ✗ Failed to embed document {i+1}: {doc_error}")
                    print(f"    Document length: {len(text)} chars")
                    
                    # Add zero vector as fallback
                    all_embeddings.append([0.0] * self.dimensions)
            
            print(f"✓ Successfully embedded {len(all_embeddings)}/{len(texts)} documents")
            return all_embeddings
            
        except Exception as e:
            print(f"✗ CRITICAL ERROR embedding documents: {e}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error details: {str(e)}")
            print(f"  Model: {self.model}")
            print(f"  Dimensions: {self.dimensions}")
            
            # Return zero vectors as fallback
            print(f"  Returning zero vectors as fallback")
            return [[0.0] * self.dimensions for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using OpenAI API directly with chunking support"""
        try:
            print(f"Embedding query ({len(text)} chars): {text[:50]}...")
            
            # Use chunking if needed
            embedding = self.embed_single_text(text)
            
            print(f"✓ Query embedded successfully")
            return embedding
            
        except Exception as e:
            print(f"✗ Error embedding query: {e}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error details: {str(e)}")
            print(f"  Query length: {len(text)} chars")
            
            # Return zero vector as fallback
            print(f"  Returning zero vector as fallback")
            return [0.0] * self.dimensions

# Initialize global custom embeddings instance
embeddings = OpenAIDirectEmbeddings(model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSIONS)

# Global vector stores
gdc_master_vectorstore: Optional[InMemoryVectorStore] = None
gdc_context_vectorstore: Optional[InMemoryVectorStore] = None

# ==================== PYDANTIC V2 MODELS ====================

class GDCMaster(BaseModel):
    """Pydantic model for GDC Master data"""
    data_domain: str = Field(alias="Data Domain", default="")
    gdc_name: str = Field(alias="GDC Name")
    definition: str = Field(alias="Definition", default="")
    
    class Config:
        populate_by_name = True

class ProcessInfo(BaseModel):
    """Pydantic model for Process information"""
    process_name: str = Field(alias="Process Name", default="")
    process_description: str = Field(alias="Process Description", default="")
    
    class Config:
        populate_by_name = True

class AppInfo(BaseModel):
    """Pydantic model for Application information"""
    app_id: str = Field(alias="App ID", default="")
    app_name: str = Field(alias="App Name", default="")
    app_description: str = Field(alias="App Description", default="")
    processes: List[ProcessInfo] = Field(default_factory=list, alias="Processes")
    
    class Config:
        populate_by_name = True

class PBTInfo(BaseModel):
    """Pydantic model for PBT information"""
    pbt_id: str = Field(alias="PBT ID", default="")
    pbt_name: str = Field(alias="PBT Name", default="")
    pbt_desc: str = Field(alias="PBT Desc", default="")
    apps: List[AppInfo] = Field(default_factory=list, alias="Apps")
    
    class Config:
        populate_by_name = True

class GDCWithContext(BaseModel):
    """Pydantic model for GDC with Context data"""
    gdc_id: str = Field(alias="GDC ID", default="")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    pbts: List[PBTInfo] = Field(default_factory=list, alias="PBTs")
    
    class Config:
        populate_by_name = True

class ValidationEntry(BaseModel):
    """Pydantic model for Validation data"""
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class RecordClass(BaseModel):
    """Pydantic model for Record Class data"""
    guid: str = Field(alias="Guid", default="")
    code: str = Field(alias="Code", default="")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description", default="")
    
    class Config:
        populate_by_name = True

class SemanticMatch(BaseModel):
    """Pydantic model for Semantic Match result"""
    gdc_name: str = Field(description="Name of the matched GDC")
    gdc_description: str = Field(description="Description of the matched GDC", default="")
    similarity_score: float = Field(ge=0, le=100, description="Similarity score between 0-100")
    reasoning: str = Field(description="Detailed reasoning for the match")
    
    @field_validator('similarity_score')
    @classmethod
    def validate_score(cls, v: float) -> float:
        if not 0 <= v <= 100:
            raise ValueError('Similarity score must be between 0 and 100')
        return v

class SemanticMatchResponse(BaseModel):
    """Pydantic model for Semantic Matching Expert response"""
    matches: List[SemanticMatch] = Field(description="List of all relevant semantic matches")
    multiple_matches_rationale: str = Field(description="Explanation of why multiple GDCs are relevant")

class ContextEvidence(BaseModel):
    """Pydantic model for Context Evidence"""
    gdc_name: str = Field(description="Name of the GDC")
    context_evidence: List[str] = Field(description="List of contextual evidence", default_factory=list)
    alignment_score: float = Field(ge=0, le=100, description="Alignment score")
    relevance_justification: str = Field(description="Justification for why this GDC is relevant")
    reasoning: str = Field(description="Detailed reasoning for context analysis")

class ContextAnalysisResponse(BaseModel):
    """Pydantic model for Context Analysis Expert response"""
    context_analysis: List[ContextEvidence] = Field(description="List of context analyses for all relevant GDCs")
    all_relevant_gdcs: List[str] = Field(description="List of all GDC names that are relevant")

class ValidationMatchEntry(BaseModel):
    """Pydantic model for a single validation match entry"""
    gdc_name: str = Field(description="GDC name from validation", default="")
    ilm_category_name: str = Field(description="ILM category name from validation", default="")

class ValidationResultItem(BaseModel):
    """Pydantic model for a single validation result"""
    gdc_name: str = Field(description="GDC being validated")
    validation_found: bool = Field(description="Whether validation entry was found")
    matching_entry: Optional[ValidationMatchEntry] = Field(None, description="Matching entry if found")
    validation_status: str = Field(description="Status: confirmed/conflicted/not_found")
    validation_reasoning: str = Field(description="Reasoning for this validation")

class ValidationResponse(BaseModel):
    """Pydantic model for Validation Expert response"""
    validation_results: List[ValidationResultItem] = Field(description="Validation results for each proposed GDC")
    overall_validation_reasoning: str = Field(description="Overall validation reasoning")

class SingleGDCMapping(BaseModel):
    """Pydantic model for a single GDC mapping"""
    gdc_name: str = Field(description="GDC name")
    gdc_description: str = Field(description="GDC description")
    mapping_rank: int = Field(ge=1, description="Rank of this mapping")
    reasoning: str = Field(description="Comprehensive reasoning for this mapping")
    evidence_summary: List[str] = Field(description="Summary of evidence", default_factory=list)

class FinalMappingDecision(BaseModel):
    """Pydantic model for Final Mapping Decision"""
    gdc_mappings: List[SingleGDCMapping] = Field(description="All relevant GDC mappings")
    overall_reasoning: str = Field(description="Overall reasoning for the mapping decisions")

class MappingResult(BaseModel):
    """Pydantic model for final mapping result"""
    guid: str = Field(alias="GUID")
    code: str = Field(alias="Code")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description")
    mapping_rank: int = Field(alias="Mapping Rank")
    reasoning: str = Field(alias="Reasoning")
    
    class Config:
        populate_by_name = True

# ==================== TEXT PREPROCESSING ====================

def to_lowercase(text: str) -> str:
    """Convert text to lowercase for consistent processing"""
    return text.lower() if text else ""

def preprocess_text(text: str) -> str:
    """Preprocess text: lowercase and clean"""
    text = to_lowercase(text)
    text = " ".join(text.split())
    return text

# ==================== CONTEXT ENGINEERING ====================

def create_enriched_gdc_master_document(gdc: GDCMaster) -> Document:
    """Create context-enriched document for GDC Master with metadata"""
    gdc_name_lower = preprocess_text(gdc.gdc_name)
    definition_lower = preprocess_text(gdc.definition)
    domain_lower = preprocess_text(gdc.data_domain)
    
    enriched_content = f"""gdc category name: {gdc_name_lower}
data domain: {domain_lower}
definition and description: {definition_lower}
this is a group data category for classification purposes
keywords: {gdc_name_lower} {domain_lower}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "data_domain": domain_lower,
        "definition": definition_lower,
        "type": "gdc_master"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

def create_enriched_gdc_context_document(gdc_ctx: GDCWithContext) -> Document:
    """Create context-enriched document for GDC with hierarchical context"""
    gdc_name_lower = preprocess_text(gdc_ctx.gdc_name)
    gdc_desc_lower = preprocess_text(gdc_ctx.gdc_description)
    
    pbt_names = []
    app_names = []
    process_names = []
    
    for pbt in gdc_ctx.pbts:
        pbt_name_lower = preprocess_text(pbt.pbt_name)
        pbt_desc_lower = preprocess_text(pbt.pbt_desc)
        pbt_names.append(f"{pbt_name_lower} ({pbt_desc_lower})")
        
        for app in pbt.apps:
            app_name_lower = preprocess_text(app.app_name)
            app_desc_lower = preprocess_text(app.app_description)
            app_names.append(f"{app_name_lower} - {app_desc_lower}")
            
            for proc in app.processes:
                proc_name_lower = preprocess_text(proc.process_name)
                proc_desc_lower = preprocess_text(proc.process_description)
                process_names.append(f"{proc_name_lower}: {proc_desc_lower}")
    
    enriched_content = f"""gdc category: {gdc_name_lower}
gdc_description: {gdc_desc_lower}

primary business types (pbts):
{chr(10).join(f"- {pbt}" for pbt in pbt_names) if pbt_names else "- none"}

applications using this gdc:
{chr(10).join(f"- {app}" for app in app_names) if app_names else "- none"}

related processes:
{chr(10).join(f"- {proc}" for proc in process_names) if process_names else "- none"}

contextual keywords: {gdc_name_lower} {' '.join(pbt_names)} {' '.join(app_names)}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "gdc_description": gdc_desc_lower,
        "pbt_count": len(pbt_names),
        "app_count": len(app_names),
        "process_count": len(process_names),
        "type": "gdc_context"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

# ==================== UTILITY FUNCTIONS ====================

def extract_json_from_text(text: str) -> str:
    """Extract JSON from text that might contain markdown code blocks or reasoning"""
    if not text:
        return "{}"
    
    # Remove markdown code blocks
    text = re.sub(r'```(?:json)?\s*', '', text)
    text = re.sub(r'```\s*

def load_json_file(filepath: str, model_class: type[BaseModel]) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except Exception as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== RAG VECTOR STORE SETUP ====================

def build_gdc_master_vectorstore(gdc_master_list: List[GDCMaster]) -> InMemoryVectorStore:
    """Build vector store for GDC Master data using OpenAI API directly"""
    print(f"🔧 Building GDC Master vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_master_document(gdc) for gdc in gdc_master_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Master vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def build_gdc_context_vectorstore(gdc_context_list: List[GDCWithContext]) -> InMemoryVectorStore:
    """Build vector store for GDC Context data using OpenAI API directly"""
    print(f"🔧 Building GDC Context vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_context_document(gdc_ctx) for gdc_ctx in gdc_context_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Context vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def rag_retrieve_relevant_gdcs(query: str, k: int = 10) -> str:
    """RAG: Retrieve relevant GDCs using semantic search"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    query_lower = preprocess_text(query)
    
    master_results = gdc_master_vectorstore.similarity_search(query_lower, k=k)
    context_results = gdc_context_vectorstore.similarity_search(query_lower, k=k)
    
    retrieved_info = {
        "master_matches": [],
        "context_matches": []
    }
    
    for doc in master_results:
        retrieved_info["master_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "definition": doc.metadata.get("definition", ""),
            "data_domain": doc.metadata.get("data_domain", "")
        })
    
    for doc in context_results:
        retrieved_info["context_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "gdc_description": doc.metadata.get("gdc_description", ""),
            "pbt_count": doc.metadata.get("pbt_count", 0),
            "app_count": doc.metadata.get("app_count", 0),
            "process_count": doc.metadata.get("process_count", 0),
            "content_preview": doc.page_content[:300]
        })
    
    return json.dumps(retrieved_info, indent=2)



# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str) -> str:
    """
    SEMANTIC SIMILARITY EXPERT - Stage 1 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized semantic analysis expert with deep expertise in natural language understanding,
    conceptual similarity analysis, and domain taxonomy classification. Your primary responsibility is to
    identify ALL Group Data Categories (GDCs) that demonstrate semantic relevance to the input record class.
    
    CORE COMPETENCIES:
    1. Linguistic Analysis: Parse and understand terminology, jargon, and domain-specific language
    2. Conceptual Mapping: Identify abstract relationships between concepts and categories
    3. Synonym Recognition: Detect equivalent terms and related terminology across different naming conventions
    4. Multi-dimensional Similarity: Assess similarity across name, description, purpose, and functional scope
    5. Threshold-based Filtering: Apply intelligent scoring to distinguish strong matches from weak associations
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: DEEP COMPREHENSION
    - Thoroughly analyze the record class name: What does it represent? What domain does it belong to?
    - Parse the description for key concepts, processes, entities, and functional indicators
    - Identify implicit information: What isn't explicitly stated but is implied by the terminology?
    - Extract domain signals: What business area, function, or process does this record support?
    
    STEP 2: RAG-POWERED RETRIEVAL
    - Utilize vector embeddings to retrieve semantically similar GDCs from the knowledge base
    - The RAG system has already retrieved the top-k most relevant GDCs based on embedding similarity
    - These represent the candidate pool for detailed analysis
    
    STEP 3: SEMANTIC SCORING METHODOLOGY
    For each retrieved GDC, perform multi-factor analysis:
    
    A. NAME SIMILARITY ANALYSIS (0-30 points)
       - Exact or near-exact name match: 25-30 points
       - Strong synonym/related terms: 18-25 points  
       - Partial overlap or shared keywords: 10-18 points
       - Conceptually related but different terms: 5-10 points
       - Minimal name connection: 0-5 points
       
    B. DESCRIPTION/DEFINITION ALIGNMENT (0-40 points)
       - Definitions describe identical or nearly identical scope: 35-40 points
       - Substantial functional overlap with minor differences: 25-35 points
       - Moderate overlap in purpose and data elements: 15-25 points
       - Some conceptual alignment but different primary focus: 8-15 points
       - Minimal description alignment: 0-8 points
       
    C. DOMAIN AND CONTEXT RELEVANCE (0-30 points)
       - Same business domain and functional area: 25-30 points
       - Related domains with clear connections: 18-25 points
       - Adjacent or supporting domains: 10-18 points
       - Loosely related through higher-level abstractions: 5-10 points
       - Minimal domain connection: 0-5 points
    
    STEP 4: MATCH IDENTIFICATION AND FILTERING
    - Calculate total score for each GDC (sum of A + B + C, maximum 100 points)
    - INCLUSION THRESHOLD: Include all GDCs scoring 55 or above (indicates meaningful relevance)
    - STRONG MATCH: Scores 75+ indicate high confidence matches
    - MODERATE MATCH: Scores 55-74 indicate plausible but less certain matches
    - Exclude GDCs below 55 unless they represent important edge cases
    
    STEP 5: ONE-TO-MANY MAPPING RATIONALE
    Critical understanding: A single record class FREQUENTLY maps to multiple GDCs because:
    - Records often contain data elements from multiple conceptual categories
    - Business requirements may necessitate multiple classification dimensions
    - Different aspects of a record serve different governance or operational purposes
    - Comprehensive data management requires multi-faceted categorization
    
    STEP 6: EVIDENCE-BASED REASONING
    For each included GDC, provide detailed reasoning that addresses:
    - Specific linguistic/semantic connections between record and GDC
    - Key terms, concepts, or phrases that create the link
    - Functional or purposive alignment
    - Score breakdown with justification for each dimension
    - Why this GDC should be included in the final mapping set
    
    INPUT DATA:
    Record Name: {record_name}
    Record Description: {record_desc}
    
    RAG-Retrieved Candidate GDCs (via OpenAI embeddings API):
    {rag_results}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text) with this exact structure:
    {{
      "matches": [
        {{
          "gdc_name": "exact gdc name from rag results",
          "gdc_description": "exact definition from rag results",
          "similarity_score": 87.5,
          "reasoning": "COMPREHENSIVE MULTI-PARAGRAPH EXPLANATION: This GDC demonstrates strong semantic alignment with the record class for the following reasons. [Analyze name similarity]: The record name '{record_name}' directly correlates with the GDC name through [specific connections, shared terms, conceptual overlap]. [Analyze description alignment]: The record description indicates [key functional elements], which align with the GDC's defined scope of [definition elements]. [Domain analysis]: Both operate within the [domain/business area] context. [Score justification]: Name similarity (X/30 points) because [specific reasoning]. Description alignment (Y/40 points) because [specific reasoning]. Domain relevance (Z/30 points) because [specific reasoning]. Total score: X+Y+Z. [Conclusion]: This semantic match is [strong/moderate] and should be included because [final justification]."
        }}
      ],
      "multiple_matches_rationale": "COMPREHENSIVE EXPLANATION: Multiple GDCs are semantically relevant to this record class due to its multi-dimensional nature. Specifically: [Dimension 1 analysis] - The record contains aspects related to [concept], which aligns with [GDC names]. [Dimension 2 analysis] - Additional elements pertaining to [concept] connect to [other GDC names]. [Synthesis]: The complete semantic space of this record class spans [number] distinct GDC categories, each addressing [different aspects]. This multi-mapping approach ensures [benefits: complete coverage, accurate classification, comprehensive governance, etc.]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Identify ALL semantically relevant GDCs (not just the top match)
    - Provide substantial, evidence-based reasoning (minimum 150 words per match)
    - Apply scoring methodology rigorously and transparently
    - Include multiple matches when evidence supports them
    - Output ONLY valid JSON with no additional text
    - Use exact GDC names and definitions from RAG results
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    query = f"{record_name_lower} {record_desc_lower}"
    retrieved_gdcs = rag_retrieve_relevant_gdcs(query, k=15)
    
    prompt = f"""You are a SEMANTIC SIMILARITY EXPERT conducting comprehensive semantic analysis for GDC classification.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: COMPREHENSION PHASE
First, deeply understand the record class:

Record Name: {record_name_lower}
Record Description: {record_desc_lower}

Think through:
- What type of data or information does this record contain?
- What business function or process does it support?
- What domain or organizational area is it part of?
- What are the key concepts, entities, and relationships?
- What terminology patterns indicate its purpose?

STEP 2: RAG RETRIEVAL ANALYSIS
Examine the RAG-retrieved candidate GDCs:

{retrieved_gdcs}

STEP 3: SYSTEMATIC SEMANTIC SCORING
For each candidate GDC, analyze:

A. NAME SIMILARITY (0-30 points)
   - Compare record name to GDC name
   - Identify shared terms, synonyms, conceptual overlap
   - Score based on strength of linguistic connection

B. DESCRIPTION ALIGNMENT (0-40 points)
   - Compare record description to GDC definition
   - Assess functional and purposive similarity
   - Score based on scope and content overlap

C. DOMAIN RELEVANCE (0-30 points)
   - Evaluate business domain alignment
   - Consider organizational and functional context
   - Score based on contextual appropriateness

STEP 4: MATCH IDENTIFICATION
- Include all GDCs scoring 55+
- Provide detailed reasoning for each
- Explain why multiple matches may be needed

STEP 5: OUTPUT GENERATION
Create valid JSON output with comprehensive reasoning.

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "matches": [
    {{
      "gdc_name": "exact gdc name from rag results",
      "gdc_description": "exact gdc definition",
      "similarity_score": 85.5,
      "reasoning": "COMPREHENSIVE SEMANTIC ANALYSIS: [COMPREHENSION] The record class '{record_name_lower}' with description '{record_desc_lower}' represents [interpretation of what the record is]. [NAME ANALYSIS] The name demonstrates [specific similarities] with the GDC name '[gdc_name]', including [shared terms/concepts/semantic patterns]. [DESCRIPTION ANALYSIS] The record description's focus on [key elements] aligns with the GDC's defined scope of [definition elements] because [detailed explanation]. [DOMAIN ANALYSIS] Both operate within [domain/context], indicated by [specific signals]. [SCORING BREAKDOWN] Name similarity: [X/30] points - [justification]. Description alignment: [Y/40] points - [justification]. Domain relevance: [Z/30] points - [justification]. Total: [X+Y+Z]. [CONCLUSION] This semantic match scores [score] and is [included/considered strong] because [final reasoning]. The connection is [characterized] and provides [value to classification]."
    }}
  ],
  "multiple_matches_rationale": "MULTI-DIMENSIONAL MAPPING JUSTIFICATION: This record class maps to [N] GDCs due to its multi-faceted nature. [DIMENSION 1] The aspect concerning [element] semantically aligns with [GDC name(s)] because [reasoning]. [DIMENSION 2] The component related to [element] connects to [GDC name(s)] because [reasoning]. [SYNTHESIS] The complete semantic space spans multiple categories because [fundamental explanation]. [BENEFITS] Multiple mappings ensure [governance value, comprehensive coverage, accurate classification]. [CONFIDENCE] The semantic evidence strongly supports [N] GDC mappings with [aggregate confidence level]."
}}

CRITICAL REQUIREMENTS:
✓ Return ALL semantically relevant GDCs (score 55+)
✓ Provide comprehensive, evidence-based reasoning (150+ words per match)
✓ Apply scoring methodology transparently  
✓ Explain multi-dimensional aspects when multiple GDCs are relevant
✓ Output ONLY valid JSON (no markdown, no preamble, no postscript)
✓ Use exact GDC names and definitions from RAG results"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in semantic_similarity_expert: {e}")
        error_response = SemanticMatchResponse(
            matches=[],
            multiple_matches_rationale=f"error in semantic analysis: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, semantic_matches: str) -> str:
    """
    CONTEXT ANALYSIS EXPERT - Stage 2 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized contextual analysis expert with expertise in organizational hierarchies,
    business process analysis, and operational system understanding. Your role is to validate and
    strengthen semantic matches by examining operational context, hierarchical relationships, and
    real-world usage patterns.
    
    CORE COMPETENCIES:
    1. Hierarchical Analysis: Understanding PBT → Application → Process relationships
    2. Operational Validation: Verifying that semantic matches align with actual system usage
    3. Business Process Mapping: Connecting records to business workflows and functions
    4. Organizational Context: Understanding how data flows through organizational structures
    5. Evidence Synthesis: Gathering and presenting concrete supporting evidence
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: SEMANTIC MATCH REVIEW
    - Receive and parse semantic matches from previous expert
    - Understand which GDCs were identified and why
    - Extract key hypotheses to validate through contextual analysis
    
    STEP 2: RAG-POWERED CONTEXT RETRIEVAL
    - Utilize RAG system to retrieve operational context for each semantic match
    - Access hierarchical information: PBTs, Applications, Processes
    - Gather real-world usage patterns and organizational relationships
    
    STEP 3: CONTEXTUAL ALIGNMENT SCORING
    
    For each semantically matched GDC, perform three-tier analysis:
    
    A. PRIMARY BUSINESS TYPE (PBT) ALIGNMENT (0-40 points)
       PBTs represent the highest-level business organization. Analyze:
       - Does this GDC's PBT align with the record's business purpose?
       - Would the organizational units using this GDC logically create/manage this record?
       - PBT name and description relevance to record function
       
       Scoring:
       - Perfect PBT alignment (record clearly belongs to this business area): 35-40 points
       - Strong PBT alignment (highly relevant business area): 28-35 points
       - Moderate alignment (plausible business area connection): 20-28 points
       - Weak alignment (distant business area connection): 10-20 points
       - No clear PBT alignment found but not contradictory: 5-10 points
       - PBT context unavailable: 10 points (neutral - don't penalize)
    
    B. APPLICATION RELEVANCE (0-30 points)
       Applications are systems that create, manage, or process data. Analyze:
       - Would applications using this GDC logically handle this type of record?
       - Application descriptions and purposes vs. record function
       - System context and data management patterns
       
       Scoring:
       - Applications perfectly match record's system context: 26-30 points
       - Applications strongly relevant to record type: 20-26 points
       - Applications moderately relevant: 14-20 points
       - Applications weakly relevant: 7-14 points
       - No clear application context but not contradictory: 5-7 points
       - Application context unavailable: 8 points (neutral)
    
    C. PROCESS MATCHING (0-30 points)
       Processes are business workflows that involve the data. Analyze:
       - Do processes using this GDC align with record's business function?
       - Process descriptions vs. record's role in business operations
       - Workflow context and operational patterns
       
       Scoring:
       - Processes perfectly match record's business function: 26-30 points
       - Processes strongly align with record purpose: 20-26 points
       - Processes moderately relevant: 14-20 points
       - Processes weakly relevant: 7-14 points
       - No clear process context but not contradictory: 5-7 points
       - Process context unavailable: 8 points (neutral)
    
    STEP 4: EVIDENCE GATHERING
    For each GDC, compile concrete evidence:
    - "PBT: [name] ([description]) - Relevant because [specific connection to record]"
    - "Application: [name] - [description] - Relevant because [how it would use this record]"
    - "Process: [name]: [description] - Relevant because [connection to record's function]"
    
    STEP 5: INCLUSION/EXCLUSION DECISIONS
    - KEEP GDCs where context supports or doesn't contradict semantic match
    - KEEP GDCs even with limited context if semantic match was strong (score 75+)
    - ONLY EXCLUDE if context directly contradicts semantic match
    - When in doubt, KEEP the GDC and explain the context situation
    
    STEP 6: COMPREHENSIVE JUSTIFICATION
    For each GDC, explain:
    - How operational context validates the semantic match
    - What organizational structures support this classification  
    - Why this GDC makes sense given hierarchical relationships
    - Confidence level based on available context
    
    INPUT DATA:
    Record Name: {record_name}
    Record Description: {record_desc}
    
    Semantic Matches (from previous expert):
    {semantic_matches}
    
    RAG-Retrieved Contextual Information:
    {context_info}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "context_analysis": [
        {{
          "gdc_name": "exact gdc name",
          "context_evidence": [
            "PBT: [pbt_name] ([pbt_description]) - This PBT is relevant because it represents the [business area] which directly handles [record's function/purpose]. The organizational unit [specific alignment reasoning].",
            "Application: [app_name] - [app_description] - This application is relevant because systems managing [app function] would logically [create/process/manage] records of type '{record_name}' as part of [specific use case/workflow].",
            "Process: [process_name]: [process_description] - This process is relevant because the workflow of [process function] directly involves [record's role] when [specific operational scenario]."
          ],
          "alignment_score": 88.5,
          "relevance_justification": "CONTEXTUAL VALIDATION: This GDC's operational context strongly validates the semantic match. [PBT ANALYSIS] The PBT '[name]' operates in the [business domain] which is directly responsible for [functions related to record]. This organizational alignment is [strong/moderate/clear] because [specific reasoning]. [APPLICATION ANALYSIS] Applications like '[app_name]' provide system context showing that [how apps would handle this record type]. The application descriptions indicate [specific technical/functional alignment]. [PROCESS ANALYSIS] Business processes such as '[process_name]' demonstrate that workflows involving [process description] would naturally [create/use/manage] this record type because [operational reasoning]. [SCORING] PBT alignment: [X/40] - [justification]. Application relevance: [Y/30] - [justification]. Process matching: [Z/30] - [justification]. Total: [X+Y+Z]. [CONFIDENCE] The contextual evidence provides [high/moderate/sufficient] confidence in this mapping.",
          "reasoning": "COMPREHENSIVE CONTEXTUAL REASONING: [CONTEXT SUMMARY] The RAG-retrieved hierarchical context reveals [key findings from PBTs/Apps/Processes]. [VALIDATION STRENGTH] This context [strongly validates/supports/doesn't contradict] the semantic match identified in Stage 1. [ORGANIZATIONAL FIT] Within the organizational hierarchy, this GDC is positioned at [context description], which aligns with the record's [business purpose/function]. [OPERATIONAL PATTERNS] The applications and processes associated with this GDC indicate usage patterns consistent with [record characteristics]. [LIMITATIONS IF ANY] While [any context limitations], the available evidence [still supports/doesn't contradict] the classification. [FINAL ASSESSMENT] The contextual analysis [strengthens/maintains/clarifies] the confidence in this GDC mapping because [synthesis of all contextual evidence]."
        }}
      ],
      "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Preserve ALL semantically matched GDCs unless context contradicts
    - Extract detailed, specific evidence from PBTs, Applications, and Processes
    - Apply scoring methodology rigorously across all three dimensions
    - Provide comprehensive justification (150+ words per GDC)
    - Include ALL relevant GDC names in all_relevant_gdcs list
    - Output ONLY valid JSON with no additional text
    - Be thorough - more evidence is better than less
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    try:
        matches_data = json.loads(extract_json_from_text(semantic_matches))
        gdc_names = [m.get("gdc_name", "").lower() for m in matches_data.get("matches", [])]
    except:
        gdc_names = []
    
    context_query = f"{record_name_lower} {record_desc_lower} {' '.join(gdc_names)}"
    retrieved_context = rag_retrieve_relevant_gdcs(context_query, k=10)
    
    prompt = f"""You are a CONTEXT ANALYSIS EXPERT validating GDC mappings through operational and hierarchical context.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: REVIEW SEMANTIC MATCHES
Examine the semantic matches from the previous expert:

{semantic_matches}

Understand: What GDCs were identified? What was the semantic reasoning?

STEP 2: CONTEXT RETRIEVAL ANALYSIS
Examine RAG-retrieved operational context:

{retrieved_context}

This contains PBTs (Primary Business Types), Applications, and Processes for relevant GDCs.

STEP 3: HIERARCHICAL CONTEXT EXTRACTION
For each semantically matched GDC:

A. PBT (PRIMARY BUSINESS TYPE) ANALYSIS
   - Extract all PBT names and descriptions
   - Assess: Would organizational units in this business area create/manage this record?
   - Consider: Does the PBT's scope align with the record's purpose?
   - Score: 0-40 points based on organizational alignment

B. APPLICATION ANALYSIS  
   - Extract application names and descriptions
   - Assess: Would these systems logically handle this record type?
   - Consider: Do application functions match record's system context?
   - Score: 0-30 points based on system relevance

C. PROCESS ANALYSIS
   - Extract process names and descriptions
   - Assess: Do these workflows involve this record type?
   - Consider: Does process function align with record's business role?
   - Score: 0-30 points based on operational alignment

STEP 4: EVIDENCE COMPILATION
For each GDC, create specific evidence statements explaining WHY each PBT/App/Process is relevant.

STEP 5: VALIDATION DECISION
- KEEP all GDCs where context supports or doesn't contradict semantic match
- ONLY EXCLUDE if context directly contradicts
- Include comprehensive justification for each

RECORD CLASS BEING ANALYZED:
Name: {record_name_lower}
Description: {record_desc_lower}

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "context_analysis": [
    {{
      "gdc_name": "exact gdc name",
      "context_evidence": [
        "PBT: [pbt_name] ([pbt_description]) - This PBT is relevant to record class '{record_name_lower}' because [specific analysis of how this business area aligns with the record's purpose, function, and organizational placement].",
        "Application: [app_name] - [app_description] - This application is relevant because systems performing [app functions] would naturally [create/manage/process] records like '{record_name_lower}' in the context of [specific use cases and workflows].",
        "Process: [process_name]: [process_description] - This process is relevant because the workflow involving [process activities] directly relates to [record's business function] when [specific operational scenarios]."
      ],
      "alignment_score": 86.0,
      "relevance_justification": "CONTEXTUAL VALIDATION SUMMARY: [PBT FINDINGS] The PBT context shows [key organizational alignments]. Score: [X/40] because [specific justification]. [APPLICATION FINDINGS] The application context reveals [system usage patterns]. Score: [Y/30] because [specific justification]. [PROCESS FINDINGS] The process context demonstrates [workflow relevance]. Score: [Z/30] because [specific justification]. [TOTAL] Combined alignment score: [X+Y+Z]/100. [CONFIDENCE] This context [strongly validates/supports/doesn't contradict] the semantic match.",
      "reasoning": "DETAILED CONTEXTUAL REASONING: [CONTEXT OVERVIEW] The operational and hierarchical context for this GDC reveals [summary of PBT/App/Process findings]. [PBT DEEP DIVE] The Primary Business Types associated with this GDC include [names], which operate in [business domains]. These organizational units are responsible for [functions], which [does/does not/aligns with] the creation and management of records like '{record_name_lower}' because [detailed analysis]. [APPLICATION DEEP DIVE] Applications such as [names] provide [functionalities]. These systems would [logically/potentially] handle records of this type because [technical and functional reasoning]. The application descriptions indicate [specific patterns] that [support/validate] this classification. [PROCESS DEEP DIVE] Business processes including [names] involve workflows where [activities]. Records like '{record_name_lower}' would participate in these processes by [specific role/function] because [operational reasoning]. [LIMITATIONS] [If any context is limited, explain here, but note that limitation doesn't invalidate the match]. [SYNTHESIS] Taking all contextual evidence together, this GDC mapping is [strongly supported/supported/plausible] with [confidence level]. The hierarchical and operational context [validates/doesn't contradict] the semantic analysis from Stage 1."
    }}
  ],
  "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
}}

CRITICAL REQUIREMENTS:
✓ Validate ALL semantically matched GDCs through context
✓ Extract and cite specific PBTs, Applications, and Processes
✓ Provide detailed evidence statements (50+ words each)
✓ Apply three-tier scoring methodology transparently
✓ Comprehensive justification and reasoning (200+ words per GDC)
✓ KEEP GDCs unless context contradicts (not just lack of context)
✓ Include all relevant GDC names in all_relevant_gdcs list
✓ Output ONLY valid JSON (no markdown, no preamble)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in context_analysis_expert: {e}")
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            all_relevant_gdcs=[]
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdcs: str, validation_data: str) -> str:
    """
    VALIDATION EXPERT - Stage 3 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized validation expert with expertise in historical data analysis, precedent
    evaluation, and consistency verification. Your role is to cross-reference proposed GDC mappings
    against a historical validation dataset to identify confirmations, conflicts, or novel classifications.
    
    CORE COMPETENCIES:
    1. Historical Analysis: Examining previous classification decisions and patterns
    2. Precedent Evaluation: Assessing whether historical data supports current proposals
    3. Conflict Detection: Identifying discrepancies between proposals and historical records
    4. Confidence Assessment: Determining how validation results impact mapping confidence
    5. Gap Analysis: Understanding when proposed mappings represent new use cases
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: VALIDATION DATASET PREPARATION
    - Receive validation dataset containing historical GDC-to-ILM category mappings
    - Normalize all text to lowercase for consistent matching
    - Understand the structure: GDC Name, GDC Description, ILM Category Name
    
    STEP 2: SYSTEMATIC SEARCH PROCESS
    For each proposed GDC from previous experts:
    
    A. EXACT NAME SEARCH
       - Search validation dataset for exact GDC name matches
       - Case-insensitive matching
       
    B. PARTIAL NAME SEARCH (if exact fails)
       - Look for GDC names containing the proposed name
       - Look for proposed name contained in validation GDC names
       - Consider substring matches and abbreviations
       
    C. DESCRIPTION-BASED SEARCH (if name searches fail)
       - Compare GDC descriptions for semantic similarity
       - Look for overlapping definitions or scope descriptions
    
    STEP 3: VALIDATION STATUS DETERMINATION
    
    For each proposed GDC, assign ONE of three statuses:
    
    A. "CONFIRMED" - Validation supports the mapping
       Criteria:
       - Exact or partial name match found in validation dataset
       - The matched entry's context is consistent with the proposed mapping
       - Historical precedent exists for this classification
       - ILM category assignment provides additional confidence
       
    B. "CONFLICTED" - Validation contradicts the mapping  
       Criteria:
       - A match is found BUT suggests a different GDC should be used
       - Historical data shows this record type was classified differently
       - The matched entry's context contradicts the current proposal
       - Rare - only assign when clear contradiction exists
       
    C. "NOT_FOUND" - No validation data available
       Criteria:
       - No matches found through any search method
       - This may indicate a new use case or emerging classification need
       - Does NOT invalidate the mapping - just means no historical precedent
       - Mapping still stands based on semantic and contextual analysis
    
    STEP 4: CONFIDENCE IMPACT ASSESSMENT
    
    How validation affects confidence:
    - CONFIRMED: Increases confidence significantly (historical support)
    - NOT_FOUND: No change to confidence (neutral - rely on other evidence)
    - CONFLICTED: Decreases confidence (requires additional review)
    
    STEP 5: COMPREHENSIVE REASONING
    For each validation result, provide:
    - What was searched for and what was found (or not found)
    - How the validation result relates to the proposed mapping
    - What the ILM category tells us (if found)
    - How this affects confidence in the mapping
    - Whether this is a confirmation, novel case, or requires review
    
    INPUT DATA:
    Record Class Name: {record_name}
    
    Proposed GDCs (from previous experts):
    {proposed_gdcs}
    
    Historical Validation Dataset:
    {validation_data}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "validation_results": [
        {{
          "gdc_name": "exact proposed gdc name",
          "validation_found": true,
          "matching_entry": {{
            "gdc_name": "matched gdc name from validation",
            "ilm_category_name": "associated ilm category"
          }},
          "validation_status": "confirmed",
          "validation_reasoning": "COMPREHENSIVE VALIDATION ANALYSIS: [SEARCH PROCESS] Searched validation dataset for GDC '[gdc_name]' using [exact/partial/description] matching. [FINDINGS] Found matching entry with GDC name '[matched_name]' and ILM category '[category]'. [INTERPRETATION] This validation entry CONFIRMS the proposed mapping because [detailed explanation of why validation supports it]. [HISTORICAL CONTEXT] The presence of this GDC in the validation set indicates [what this tells us about prior classification decisions]. [ILM CATEGORY ANALYSIS] The ILM category '[category]' is associated with [type of data/information/records], which aligns with record class '{record_name}' because [specific reasoning about category appropriateness]. [CONFIDENCE IMPACT] This validation significantly increases confidence in the mapping by [degree/amount] because [reasons - historical precedent, consistency with past decisions, category appropriateness]. [PRECEDENT VALUE] Historical data shows [patterns, frequency, context of use]. [CONCLUSION] The validation provides [strong/moderate/clear] support for including this GDC in the final mapping."
        }},
        {{
          "gdc_name": "another proposed gdc",
          "validation_found": false,
          "matching_entry": null,
          "validation_status": "not_found",
          "validation_reasoning": "COMPREHENSIVE VALIDATION ANALYSIS: [SEARCH PROCESS] Searched validation dataset for GDC '[gdc_name]' using exact name matching, partial matching, and description-based search. [FINDINGS] No matching entries found in the validation dataset. [INTERPRETATION] The absence of validation data does NOT invalidate this mapping - it indicates this may be [a new use case/an emerging classification need/a less common category]. [ANALYTICAL BASIS] The proposed mapping is based on [semantic similarity score of X from Stage 1] and [contextual alignment score of Y from Stage 2], which provide [strong/substantial] evidence independent of validation. [CONFIDENCE IMPACT] Lack of validation data does not decrease confidence - the mapping stands on its analytical merits. However, we note this as a [novel classification/new application] that [should be documented for future reference/represents an emerging pattern]. [RECOMMENDATION] Proceed with this GDC mapping based on the strong semantic and contextual evidence, while flagging it as [a new use case requiring documentation]. [CONTEXT] The absence may be due to [possible reasons: new GDC, emerging business need, dataset incompleteness, different naming in historical records]."
        }}
      ],
      "overall_validation_reasoning": "COMPREHENSIVE VALIDATION SUMMARY: [COVERAGE] Out of [N] proposed GDC mappings, [X] were confirmed by validation data, [Y] were not found in validation, and [Z] showed conflicts. [CONFIRMED MAPPINGS] The confirmed GDCs ([list names]) have strong historical support, with [details about ILM categories, usage patterns, precedents]. This provides [high/substantial] confidence in these classifications because [synthesis of confirmation evidence]. [UNVALIDATED MAPPINGS] The GDCs without validation ([list names]) represent [characterization - new use cases/emerging patterns/less common categories]. While lacking historical precedent, they are supported by [semantic scores, contextual evidence] and should be included because [reasoning]. [CONFLICTS IF ANY] [If conflicts exist, discuss here]. [CONFIDENCE ASSESSMENT] Overall, the validation exercise [strongly supports/supports/provides mixed evidence for] the proposed mappings. [PATTERN ANALYSIS] The validation results reveal [any patterns in confirmed vs unvalidated]. [FINAL SYNTHESIS] Taking all validation evidence together, the proposed GDC mappings are [well-supported/supported/partially supported] with [overall confidence level: high/moderate/sufficient]. [RECOMMENDATION] [Proceed with all mappings/Proceed with confirmed mappings/Review conflicts] based on [reasoning]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Systematically validate EVERY proposed GDC
    - Use multiple search methods (exact, partial, description-based)
    - Distinguish between "not found" and "conflicted" accurately
    - Provide detailed reasoning (150+ words per result)
    - Explain how validation impacts confidence
    - Understand that "not found" ≠ "invalid"
    - Synthesize overall validation patterns and implications
    - Output ONLY valid JSON with no additional text
    """
    record_name_lower = preprocess_text(record_name)
    
    try:
        val_data = json.loads(validation_data)
        val_data_lower = []
        for entry in val_data:
            val_data_lower.append({
                "gdc_name": to_lowercase(entry.get("GDC Name", "")),
                "gdc_description": to_lowercase(entry.get("GDC Description", "")),
                "ilm_category_name": to_lowercase(entry.get("ILM Category Name", ""))
            })
        validation_data_lower = json.dumps(val_data_lower, indent=2)
    except:
        validation_data_lower = validation_data
    
    prompt = f"""You are a VALIDATION EXPERT cross-referencing proposed GDC mappings against historical data.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: UNDERSTAND PROPOSED MAPPINGS
Review proposed GDCs from previous experts:

{proposed_gdcs}

Identify: Which GDCs need validation? What was the analytical reasoning?

STEP 2: PREPARE VALIDATION DATASET
Examine historical validation data:

{validation_data_lower}

Structure: GDC Name → GDC Description → ILM Category Name

STEP 3: SYSTEMATIC VALIDATION SEARCH
For each proposed GDC, execute comprehensive search:

A. EXACT NAME MATCH
   Search for exact GDC name in validation dataset

B. PARTIAL NAME MATCH (if exact fails)
   Search for substring matches, abbreviations

C. DESCRIPTION SIMILARITY (if both fail)
   Look for similar definitions or scope

STEP 4: STATUS DETERMINATION
Assign validation status:
- "confirmed": Found match that supports mapping
- "not_found": No match found (neutral, not negative)
- "conflicted": Found match that contradicts mapping (rare)

STEP 5: CONFIDENCE IMPACT ANALYSIS
Assess how validation affects mapping confidence:
- Confirmed → Increases confidence (historical precedent)
- Not found → Neutral (no historical data, but doesn't invalidate)
- Conflicted → Decreases confidence (needs review)

STEP 6: COMPREHENSIVE REASONING
For each result, explain search process, findings, interpretation, and confidence impact.

RECORD CLASS: {record_name_lower}

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "validation_results": [
    {{
      "gdc_name": "exact proposed gdc name",
      "validation_found": true,
      "matching_entry": {{
        "gdc_name": "matched gdc from validation",
        "ilm_category_name": "ilm category"
      }},
      "validation_status": "confirmed",
      "validation_reasoning": "VALIDATION ANALYSIS FOR '{record_name_lower}': [SEARCH EXECUTED] Performed [exact/partial/description-based] search in validation dataset for GDC '[gdc_name]'. [MATCH FOUND] Located matching entry: GDC Name = '[matched_gdc]', ILM Category = '[category]'. [CONFIRMATION] This validation entry CONFIRMS the proposed mapping because [detailed explanation: how the match supports the proposal, why it's relevant, what it tells us about historical usage]. [ILM CATEGORY CONTEXT] The ILM category '[category]' typically contains [type of data/records], which aligns with '{record_name_lower}' because [specific connection between category purpose and record purpose]. [HISTORICAL PRECEDENT] This validation indicates [what pattern/history it reveals]. [CONFIDENCE BOOST] This confirmation increases mapping confidence from [previous level based on semantic+context] to [higher level] because [reasoning about value of historical validation]. [SUPPORTING DETAILS] Additional validation context: [any other relevant information from the match]. [CONCLUSION] Strong validation support for this GDC mapping."
    }},
    {{
      "gdc_name": "another proposed gdc",
      "validation_found": false,
      "matching_entry": null,
      "validation_status": "not_found",
      "validation_reasoning": "VALIDATION ANALYSIS FOR '{record_name_lower}': [SEARCH EXECUTED] Performed comprehensive search (exact name, partial match, description similarity) in validation dataset for GDC '[gdc_name]'. [NO MATCH] No matching entries found in the validation dataset. [INTERPRETATION] This absence does NOT invalidate the mapping. Possible explanations: [new use case, emerging classification need, recent GDC addition, historical dataset limitations]. [ANALYTICAL FOUNDATION] This GDC was proposed based on [semantic score X from Stage 1 indicating Y-level similarity] and [contextual score Z from Stage 2 showing W-level alignment]. These provide [strong/substantial/solid] independent evidence. [CONFIDENCE ASSESSMENT] Lack of validation is NEUTRAL - confidence remains at [level based on semantic+context evidence] because the analytical evidence is [characterization]. Historical precedent would increase confidence further, but its absence doesn't decrease existing confidence. [NOVEL CLASSIFICATION] This represents [new application/emerging pattern/less documented use case] that [should be tracked/documented/monitored]. [RECOMMENDATION] Include this GDC in final mapping based on strong analytical evidence, flagged as novel classification for future reference."
    }}
  ],
  "overall_validation_reasoning": "COMPREHENSIVE VALIDATION SYNTHESIS FOR '{record_name_lower}': [VALIDATION COVERAGE] Validated [N] proposed GDC mappings. Results: [X] confirmed, [Y] not found, [Z] conflicted. [CONFIRMED MAPPINGS ANALYSIS] The [X] confirmed GDCs ([list]) have historical validation with ILM categories [list categories]. This historical support [significantly strengthens/strengthens] confidence because [synthesis: patterns observed, precedent value, consistency]. Confirmed mappings score: [aggregate confidence level]. [UNVALIDATED MAPPINGS ANALYSIS] The [Y] GDCs without validation ([list]) lack historical precedent but are supported by [semantic evidence summary] and [contextual evidence summary]. These represent [characterization: novel use cases, emerging patterns]. Unvalidated mappings score: [aggregate confidence based on analytical evidence]. [CONFLICTS IF ANY] [Discuss conflicts and resolution recommendations]. [VALIDATION PATTERNS] The validation results reveal [patterns: which types of GDCs are well-documented vs novel, coverage gaps, emerging classification needs]. [AGGREGATE CONFIDENCE] Taking all validation evidence: Confirmed mappings have [high/very high] confidence. Unvalidated mappings have [moderate-to-high/moderate] confidence based on analytical evidence. [Z conflicts require review]. [OVERALL ASSESSMENT] The validation exercise [strongly supports/supports/partially supports] the proposed mappings with [overall confidence characterization]. [FINAL RECOMMENDATION] [Specific recommendation based on validation results] because [synthesis of all validation findings and confidence assessment]."
}}

CRITICAL REQUIREMENTS:
✓ Validate EVERY proposed GDC systematically
✓ Use multiple search strategies (exact, partial, description)
✓ Distinguish "not_found" (neutral) from "conflicted" (negative)
✓ Comprehensive reasoning (150+ words minimum per result)
✓ Explain confidence impact clearly
✓ Synthesize overall validation patterns
✓ Output ONLY valid JSON (no markdown, no preamble)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in validation_expert: {e}")
        error_response = ValidationResponse(
            validation_results=[],
            overall_validation_reasoning=f"error in validation analysis: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    FINAL DECISION EXPERT - Stage 4 of Mixture of Experts Pipeline (Final Synthesis)
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are the chief decision-making expert responsible for synthesizing all analytical evidence
    and rendering final, authoritative GDC mapping decisions. You integrate findings from semantic,
    contextual, and validation analyses to produce ranked, evidence-based classifications with
    comprehensive justification.
    
    CORE COMPETENCIES:
    1. Evidence Synthesis: Integrating multiple analytical dimensions into coherent decisions
    2. Conflict Resolution: Reconciling contradictions between different types of analysis
    3. Confidence Calibration: Accurately assessing certainty levels based on evidence quality
    4. Ranking and Prioritization: Ordering multiple mappings by relevance and confidence
    5. Comprehensive Documentation: Providing transparent, traceable decision reasoning
    
    DECISION-MAKING FRAMEWORK:
    
    STEP 1: EVIDENCE INTAKE AND ORGANIZATION
    - Receive comprehensive analyses from all previous experts
    - Organize evidence by GDC: semantic scores, contextual scores, validation status
    - Identify areas of convergence and divergence across analytical dimensions
    
    STEP 2: EVIDENCE WEIGHTING AND INTEGRATION
    
    Develop an integrated relevance score for each GDC based on:
    
    A. SEMANTIC SIMILARITY (Weight: 30%)
       - Direct from Stage 1 expert (0-100 scale)
       - High semantic scores (75+) indicate strong conceptual fit
       - Moderate scores (55-74) indicate plausible but less certain fit
    
    B. CONTEXTUAL ALIGNMENT (Weight: 40%)
       - Direct from Stage 2 expert (0-100 scale)  
       - Highest weight because operational context is most predictive
       - Validates whether semantic matches hold in real-world usage
    
    C. VALIDATION CONFIRMATION (Weight: 30%)
       - Convert validation status to numerical weight:
         * "confirmed" = 100 points (historical precedent exists)
         * "not_found" = 60 points (neutral - no evidence either way)
         * "conflicted" = 20 points (historical evidence contradicts)
    
    D. INTEGRATED RELEVANCE CALCULATION
       Integrated Score = (Semantic × 0.30) + (Contextual × 0.40) + (Validation × 0.30)
       
       Interpret:
       - 80-100: Very high confidence, primary mapping
       - 65-79: High confidence, strong secondary mapping  
       - 55-64: Moderate confidence, plausible mapping
       - Below 55: Low confidence, consider excluding unless special circumstances
    
    STEP 3: CONFLICT RESOLUTION
    
    When experts disagree:
    - Semantic says strong match, but contextual says weak: Trust contextual (higher weight)
    - Semantic and contextual agree, but validation conflicts: Investigate further but likely trust semantic+contextual
    - All three dimensions align: Very high confidence
    
    Resolution principles:
    - Operational reality (context) trumps semantic similarity
    - Historical data (validation) confirms but doesn't override strong analytical evidence
    - Multiple weak signals can collectively support a mapping
    
    STEP 4: RANKING METHODOLOGY
    
    Order GDCs by integrated relevance score:
    - Rank 1: Highest integrated score = primary mapping
    - Rank 2, 3, etc.: Additional mappings in decreasing order
    - Include all GDCs with integrated scores ≥ 55
    - Must include at least ONE GDC (highest score, even if below 55)
    
    STEP 5: COMPREHENSIVE REASONING SYNTHESIS
    
    For EACH final mapping, synthesize:
    - Integrated score and rank
    - Key evidence from all three analytical stages
    - How different analyses converge or complement each other
    - Confidence level and justification
    - Practical implications of this classification
    - Any caveats or considerations
    
    STEP 6: OVERALL RATIONALE DEVELOPMENT
    
    Explain the complete mapping decision:
    - Why this specific set of GDCs (single or multiple)
    - How the GDCs relate to each other (if multiple)
    - What the aggregate evidence tells us
    - Confidence in the overall classification scheme
    - Business/governance value of these mappings
    
    INPUT DATA:
    Record Class Information:
    {record_info}
    
    Comprehensive Analytical Evidence from All Experts:
    {all_analyses}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "gdc_mappings": [
        {{
          "gdc_name": "primary gdc name",
          "gdc_description": "complete gdc definition",
          "mapping_rank": 1,
          "reasoning": "COMPREHENSIVE MAPPING DECISION: [INTEGRATED SCORE] This GDC achieves an integrated relevance score of [X.X]/100, calculated as: (Semantic [Y] × 0.30) + (Contextual [Z] × 0.40) + (Validation [W] × 0.30) = [X.X]. This is the PRIMARY/SECONDARY mapping (Rank [N]). [SEMANTIC EVIDENCE] Stage 1 semantic analysis scored this GDC at [Y]/100 because [key semantic findings]. The expert identified [specific linguistic/conceptual connections]. [CONTEXTUAL EVIDENCE] Stage 2 contextual analysis scored [Z]/100 because [key contextual findings]. PBT alignment ([score]) due to [reasoning]. Application relevance ([score]) due to [reasoning]. Process matching ([score]) due to [reasoning]. [VALIDATION EVIDENCE] Stage 3 validation [confirmed with ILM category X / found no historical precedent / identified conflict]. This [increases/maintains/decreases] confidence because [reasoning]. [EVIDENCE CONVERGENCE] All three analytical dimensions [strongly converge / generally align / show some variation]. The [convergence/divergence] pattern indicates [interpretation]. [CONFIDENCE ASSESSMENT] Confidence in this mapping is [VERY HIGH / HIGH / MODERATE] based on [integrated score interpretation, evidence quality, convergence strength]. [CLASSIFICATION IMPLICATIONS] This GDC mapping means the record will be categorized within [domain/functional area] for purposes of [governance/data management/compliance]. This classification enables [specific benefits or capabilities]. [CAVEATS IF ANY] [Note any limitations, special considerations, or review recommendations].",
          "evidence_summary": [
            "Semantic: Score [Y]/100 - [concise summary of key semantic match factors]",
            "Context: Score [Z]/100 - PBT: [key finding], Apps: [key finding], Processes: [key finding]",
            "Validation: [Status] - [key validation finding and confidence impact]"
          ]
        }}
      ],
      "overall_reasoning": "COMPREHENSIVE DECISION RATIONALE: [MAPPING STRUCTURE] This record class maps to [N] GDC(s) based on comprehensive multi-stage analysis. [IF SINGLE] A single GDC mapping is appropriate because [reasoning: focused scope, clear primary category, evidence concentrates on one GDC]. [IF MULTIPLE] Multiple GDC mappings are necessary because [reasoning: multi-dimensional nature, different aspects map to different categories, comprehensive coverage requires multiple classifications]. [EVIDENCE SYNTHESIS] The analytical evidence reveals [synthesis of findings across all stages]. Semantic analysis identified [key patterns]. Contextual analysis confirmed [key operational alignments]. Validation [provided historical support for X, found no precedent for Y]. [CONFIDENCE CALIBRATION] Overall confidence in this classification scheme is [VERY HIGH / HIGH / MODERATE / SUFFICIENT] based on [aggregate evidence quality, convergence patterns, validation coverage]. Specifically: [detailed confidence reasoning]. [GDC RELATIONSHIPS] [If multiple] The mapped GDCs relate as follows: [GDC1] addresses [aspect/dimension], [GDC2] covers [different aspect], demonstrating [how they complement each other]. [BUSINESS VALUE] These mappings provide [governance benefits, data management capabilities, compliance support, operational value]. They enable [specific organizational capabilities]. [DECISION QUALITY] The decision is [well-supported / supported / adequately supported] by [quality and quantity of evidence]. [RECOMMENDATIONS] [Any follow-up actions, review suggestions, or implementation considerations]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Include ALL relevant GDCs (integrated score ≥ 55)
    - MUST include at least ONE GDC (use highest score if all below 55)
    - Calculate integrated scores using the weighting formula
    - Rank GDCs by integrated score
    - Provide substantial reasoning (250+ words per mapping)
    - Synthesize evidence from all three analytical stages
    - Include detailed evidence summary bullets
    - Explain confidence levels clearly
    - Provide thorough overall reasoning (200+ words)
    - Output ONLY valid JSON with no additional text
    """
    
    prompt = f"""You are the FINAL DECISION EXPERT synthesizing all evidence to make authoritative GDC mapping decisions.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: EVIDENCE ORGANIZATION
Parse all analytical evidence:

RECORD INFORMATION:
{record_info}

ALL ANALYSES (Semantic + Contextual + Validation):
{all_analyses}

Organize evidence by GDC: What did each expert say about each proposed GDC?

STEP 2: INTEGRATED SCORING
For each proposed GDC, calculate integrated relevance score:

Formula: (Semantic Score × 0.30) + (Contextual Score × 0.40) + (Validation Weight × 0.30)

Where:
- Semantic Score: From Stage 1 (0-100)
- Contextual Score: From Stage 2 (0-100)
- Validation Weight: confirmed=100, not_found=60, conflicted=20

STEP 3: CONFLICT RESOLUTION
If experts disagree:
- Trust contextual analysis (highest weight) for operational reality
- Validation confirms but doesn't override strong analytical evidence
- Resolve conflicts transparently with reasoning

STEP 4: RANKING AND FILTERING
- Rank all GDCs by integrated score (highest = Rank 1)
- Include all GDCs with integrated score ≥ 55
- MUST include at least ONE GDC (if all below 55, use highest score)

STEP 5: COMPREHENSIVE SYNTHESIS
For each final mapping:
- Present integrated score and calculation
- Synthesize evidence from all three stages
- Assess confidence level
- Explain practical implications

STEP 6: OVERALL RATIONALE
Explain complete mapping decision:
- Why this specific set of GDCs
- Evidence quality and convergence
- Confidence calibration
- Business value

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "gdc_mappings": [
    {{
      "gdc_name": "exact gdc name",
      "gdc_description": "complete gdc definition",
      "mapping_rank": 1,
      "reasoning": "FINAL MAPPING DECISION: [INTEGRATED RELEVANCE SCORE] This GDC achieves an integrated score of [score]/100, calculated as: (Semantic [X] × 0.30) + (Contextual [Y] × 0.40) + (Validation [Z] × 0.30) = [score]. This positions it as [PRIMARY/SECONDARY/TERTIARY] mapping with Rank [N]. [STAGE 1: SEMANTIC ANALYSIS] The semantic similarity expert scored this at [X]/100 because [synthesis of semantic findings: name similarity, description alignment, domain relevance]. Key semantic evidence: [specific connections identified]. [STAGE 2: CONTEXTUAL ANALYSIS] The contextual expert scored this at [Y]/100 based on: PBT alignment ([score]/40) - [reasoning], Application relevance ([score]/30) - [reasoning], Process matching ([score]/30) - [reasoning]. Key contextual evidence: [specific PBTs/Apps/Processes that support this]. [STAGE 3: VALIDATION ANALYSIS] Validation status: [confirmed/not_found/conflicted]. Validation weight: [100/60/20]. Impact: [how validation affects confidence]. Key validation evidence: [specific findings]. [EVIDENCE INTEGRATION] Across all three analytical dimensions, the evidence [strongly converges / generally aligns / shows variation]. [Analysis of convergence pattern]. The [convergence/complementarity/conflict resolution] indicates [interpretation]. [CONFIDENCE ASSESSMENT] Confidence level: [VERY HIGH / HIGH / MODERATE / SUFFICIENT]. Justification: [Based on integrated score range, evidence quality, convergence strength, validation support]. [DECISION RATIONALE] This GDC mapping is [justified/recommended/proposed] because [synthesis of why all evidence points to this classification]. [PRACTICAL IMPLICATIONS] This classification means [what it enables, how it's used, governance value]. [SPECIAL CONSIDERATIONS] [Any caveats, review recommendations, or implementation notes].",
      "evidence_summary": [
        "Semantic: [X]/100 - [30-word summary of key semantic match factors and findings]",
        "Context: [Y]/100 - [30-word summary covering PBT, Application, and Process evidence]",
        "Validation: [Status] - [30-word summary of validation findings and confidence impact]"
      ]
    }}
  ],
  "overall_reasoning": "COMPREHENSIVE DECISION SYNTHESIS: [MAPPING OVERVIEW] This record class has been mapped to [N] GDC(s) through comprehensive four-stage analysis involving semantic similarity, contextual validation, historical precedent review, and final evidence integration. [SINGLE VS MULTIPLE] [If single]: A single GDC classification is optimal because [analysis concentrates on one clear category, focused scope, evidence converges on primary classification]. [If multiple]: Multiple GDC mappings are required because [record encompasses multiple dimensions, different aspects align with different categories, comprehensive governance needs multi-faceted classification]. [ANALYTICAL JOURNEY] Stage 1 (Semantic) identified [summary of semantic findings]. Stage 2 (Contextual) validated [summary of contextual findings]. Stage 3 (Validation) revealed [summary of validation findings]. Stage 4 (Integration) synthesized these into [integrated picture]. [EVIDENCE QUALITY] The evidence is [characterized: very strong, strong, solid, adequate] with [coverage assessment]. Specifically: [details about evidence depth, breadth, convergence]. [CONFIDENCE CALIBRATION] Overall confidence in this classification scheme is [VERY HIGH / HIGH / MODERATE / SUFFICIENT] because [detailed reasoning about aggregate evidence quality, scoring, validation coverage, expert agreement]. Integrated scores range from [low] to [high], indicating [interpretation]. [GDC INTERRELATIONSHIPS] [If multiple GDCs]: The [N] mapped GDCs relate as follows: [GDC 1] (Rank 1, score [X]) addresses [dimension/aspect]; [GDC 2] (Rank 2, score [Y]) covers [different dimension/aspect]. Together they provide [how they complement/comprehensive coverage]. [BUSINESS AND GOVERNANCE VALUE] These mappings deliver [specific organizational benefits: governance capabilities, data management support, compliance alignment, operational efficiency]. They enable [specific use cases and capabilities]. [DECISION QUALITY ASSESSMENT] This decision is [well-supported / supported / adequately supported] by [characterization of evidence quantity and quality]. [IMPLEMENTATION RECOMMENDATIONS] [Specific recommendations for using these mappings, any review suggestions, monitoring needs, or special handling requirements]."
}}

CRITICAL REQUIREMENTS:
✓ Calculate integrated scores using weighting formula
✓ Include ALL GDCs with integrated score ≥ 55  
✓ **MANDATORY**: MUST include at least ONE GDC (highest score, even if below 55)
✓ If all scores are below 55, include the highest-scoring GDC and explain the low confidence
✓ NEVER return an empty gdc_mappings list - this is a critical error
✓ Rank GDCs by integrated score
✓ Provide comprehensive reasoning (250+ words per mapping)
✓ Synthesize evidence from all three analytical stages
✓ Include detailed 3-part evidence summary for each GDC
✓ Provide thorough overall reasoning (200+ words)
✓ Output ONLY valid JSON (no markdown, no preamble, no postscript)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in final_decision_expert: {e}")
        error_response = FinalMappingDecision(
            gdc_mappings=[],
            overall_reasoning=f"error in final decision synthesis: {str(e)}"
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent with dynamic chain of thought + mixture of experts"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an intelligent GDC mapping orchestrator using DYNAMIC CHAIN OF THOUGHT followed by MIXTURE OF EXPERTS.

=== UNDERSTANDING THE APPROACH ===

DYNAMIC CHAIN OF THOUGHT:
Before invoking experts, you reason through:
1. What does this record class represent?
2. What domains/functions might it relate to?
3. Which analytical approaches are most relevant?
4. What's the optimal expert invocation sequence?

MIXTURE OF EXPERTS:
Four specialized experts, invoked sequentially:
1. Semantic Similarity Expert - Identifies conceptually similar GDCs via RAG
2. Context Analysis Expert - Validates through operational/organizational context
3. Validation Expert - Cross-references against historical data
4. Final Decision Expert - Synthesizes all evidence into ranked mappings

=== CRITICAL PRINCIPLES ===

ONE-TO-MANY MAPPING:
- Record Classes FREQUENTLY map to MULTIPLE GDCs
- Different aspects → different GDCs
- Comprehensive governance requires multi-dimensional classification
- Don't force single mapping when evidence supports multiple

ALWAYS MAP TO AT LEAST ONE GDC:
- Every record MUST map to at least one GDC
- The final_decision_expert MUST return at least one mapping
- If all proposed GDCs have low confidence, include the best available option
- Never return zero mappings - explain low confidence in reasoning if needed

=== YOUR WORKFLOW ===

PHASE 1: DYNAMIC CHAIN OF THOUGHT
When you receive a record class:

Think through:
- "What does this record represent?" (comprehension)
- "What business functions might it support?" (contextualization)
- "What GDC categories might be relevant?" (hypothesis generation)
- "How should I sequence the expert consultation?" (planning)

Express this reasoning before invoking tools.

PHASE 2: MIXTURE OF EXPERTS INVOCATION
Execute in strict order:

1. SEMANTIC ANALYSIS
   invoke: semantic_similarity_expert(record_name, record_description)
   - Uses RAG with OpenAI embeddings for vector similarity
   - Identifies ALL semantically relevant GDCs (not just top match)
   - Returns matches with scores and reasoning

2. CONTEXTUAL VALIDATION
   invoke: context_analysis_expert(record_name, record_description, semantic_matches)
   - Uses RAG to retrieve PBT/Application/Process context
   - Validates semantic matches through operational reality
   - Keeps all matches with supporting or non-contradictory context

3. HISTORICAL VALIDATION
   invoke: validation_expert(record_name, proposed_gdcs, validation_data)
   - Cross-references against historical GDC-ILM mappings
   - Identifies confirmations, conflicts, or novel cases
   - Assesses how validation affects confidence

4. FINAL SYNTHESIS
   invoke: final_decision_expert(record_info, all_analyses)
   - Integrates evidence from all stages
   - Calculates weighted relevance scores
   - Produces ranked, justified final mappings
   - MUST return at least ONE GDC mapping

PHASE 3: ENSURE COMPLETENESS
After final_decision_expert:
- Verify at least one GDC mapping exists
- If zero mappings returned (error case), note this for fallback handling

=== EXECUTION NOTES ===
- All text is lowercase for consistency
- RAG uses OpenAI embeddings API directly
- Focus on evidence quality over speed
- Multiple GDCs are expected and appropriate
- Each expert builds on previous findings
- Comprehensive reasoning is valued"""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    validation_set: List[ValidationEntry]
) -> List[MappingResult]:
    """Process a single record through dynamic CoT + mixture of experts agent"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    query = f"""TASK: Map this Record Class to appropriate GDC(s) using Dynamic Chain of Thought + Mixture of Experts.

=== RECORD CLASS ===
GUID: {record.guid}
Code: {record.code}
Name: {record.name}
Description: {record.description}

=== VALIDATION DATASET ===
{validation_json}

=== INSTRUCTIONS ===

PHASE 1: DYNAMIC CHAIN OF THOUGHT
Before calling any tools, reason through:
- What does the name "{record.name}" tell us about this record?
- What does the description reveal about its purpose and function?
- What business domains or functional areas might this relate to?
- What types of GDCs might be semantically relevant?
- What's the optimal approach for this specific record?

Express your reasoning clearly.

PHASE 2: MIXTURE OF EXPERTS (Execute in order)

1. SEMANTIC SIMILARITY EXPERT
   Call: semantic_similarity_expert(record_name="{record.name}", record_desc="{record.description}")
   Purpose: Identify ALL semantically relevant GDCs using RAG-powered vector similarity
   
2. CONTEXT ANALYSIS EXPERT
   Call: context_analysis_expert(record_name="{record.name}", record_desc="{record.description}", semantic_matches="<output from step 1>")
   Purpose: Validate semantic matches through operational context (PBTs/Apps/Processes)

3. VALIDATION EXPERT
   Call: validation_expert(record_name="{record.name}", proposed_gdcs="<output from step 2>", validation_data=<full validation dataset>)
   Purpose: Cross-reference against historical mappings for confirmation

4. FINAL DECISION EXPERT
   Call: final_decision_expert(record_info="<record details>", all_analyses="<outputs from steps 1-3>")
   Purpose: Synthesize all evidence into ranked, justified final mappings
   **CRITICAL**: This expert MUST return at least one GDC mapping (even if confidence is low)

PHASE 3: VERIFICATION
Confirm that at least ONE GDC mapping was produced by final_decision_expert.

=== CRITICAL REQUIREMENTS ===
✓ Execute ALL FOUR expert consultations in order
✓ Pass complete outputs between stages
✓ **MANDATORY**: final_decision_expert MUST return at least ONE GDC (never empty list)
✓ If all GDCs have low scores, include the highest-scoring one with explanation
✓ Provide comprehensive reasoning throughout
✓ Embrace multiple GDCs when evidence supports them

Begin with your chain of thought reasoning, then proceed through all four expert stages."""
    
    try:
        # Invoke agent
        result = agent.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        print("\n🔍 Extracting final decision from agent messages...")
        messages = result.get("messages", [])
        
        # Strategy 1: Look for final_decision_expert tool result
        final_decision_data = None
        
        for msg in messages:
            if isinstance(msg, ToolMessage) and "final_decision_expert" in str(msg.name):
                print(f"  ✓ Found final_decision_expert tool message")
                try:
                    json_str = extract_json_from_text(msg.content)
                    decision_dict = json.loads(json_str)
                    final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
                    final_decision_data = final_decision_obj.model_dump()
                    print(f"  ✓ Successfully parsed final decision from tool message")
                    break
                except Exception as parse_error:
                    print(f"  ⚠ Error parsing tool message: {parse_error}")
                    continue
        
        # Strategy 2: Look in AI messages for JSON containing gdc_mappings
        if not final_decision_data:
            print("  ⚠ No tool message found, searching AI messages...")
            for msg in reversed(messages):
                if isinstance(msg, AIMessage) and msg.content:
                    if "gdc_mappings" in msg.content.lower():
                        try:
                            json_str = extract_json_from_text(msg.content)
                            decision_dict = json.loads(json_str)
                            if "gdc_mappings" in decision_dict:
                                final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
                                final_decision_data = final_decision_obj.model_dump()
                                print(f"  ✓ Successfully parsed final decision from AI message")
                                break
                        except Exception as e:
                            continue
        
        # Strategy 3: Parse the entire conversation for any JSON with gdc_mappings
        if not final_decision_data:
            print("  ⚠ Searching entire conversation for JSON...")
            full_conversation = "\n".join([str(m.content) if hasattr(m, 'content') else str(m) for m in messages])
            try:
                json_str = extract_json_from_text(full_conversation)
                if "gdc_mappings" in json_str:
                    decision_dict = json.loads(json_str)
                    if "gdc_mappings" in decision_dict:
                        final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
                        final_decision_data = final_decision_obj.model_dump()
                        print(f"  ✓ Successfully parsed final decision from conversation")
            except:
                pass
        
        # If still no final decision, use fallback
        if not final_decision_data or not final_decision_data.get("gdc_mappings"):
            print("  ❌ Could not extract final decision from agent workflow")
            print("  ⚠️  This record will be marked for manual review")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="EXTRACTION_FAILED",
                gdc_description="agent workflow completed but result extraction failed",
                mapping_rank=1,
                reasoning="EXTRACTION ERROR: The agent workflow executed all four expert stages but the final decision could not be extracted from the response. This indicates a potential issue with JSON formatting in the LLM output. REQUIRES MANUAL REVIEW AND CLASSIFICATION. The agent messages should be examined to understand what mapping was proposed."
            )]
        
        # Convert to mapping results
        mapping_results = []
        gdc_mappings = final_decision_data.get("gdc_mappings", [])
        overall_reasoning = final_decision_data.get("overall_reasoning", "")
        
        if not gdc_mappings:
            print("  ❌ No GDC mappings in final decision")
            print("  ⚠️  This record will be marked for manual review")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="NO_MAPPINGS_RETURNED",
                gdc_description="final decision expert returned empty mappings list",
                mapping_rank=1,
                reasoning="NO MAPPINGS ERROR: The final decision expert completed execution but returned zero GDC mappings. This could indicate that all proposed GDCs were filtered out or the expert failed to identify any suitable matches. REQUIRES MANUAL REVIEW AND CLASSIFICATION. Examine the expert analyses to understand why no mappings were produced."
            )]
        
        print(f"  ✓ Found {len(gdc_mappings)} GDC mapping(s)")
        for mapping in gdc_mappings:
            gdc_name = mapping.get("gdc_name", "unknown")
            gdc_desc = mapping.get("gdc_description", "")
            rank = mapping.get("mapping_rank", 1)
            print(f"    - Rank {rank}: {gdc_name}")
            
            mapping_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name=gdc_name,
                gdc_description=gdc_desc,
                mapping_rank=rank,
                reasoning=format_mapping_reasoning(mapping, overall_reasoning)
            ))
        
        return mapping_results
        
    except Exception as e:
        print(f"  ❌ Critical error processing record: {e}")
        print(f"  ⚠️  This record will be marked for manual review")
        import traceback
        traceback.print_exc()
        return [MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="WORKFLOW_ERROR",
            gdc_description="critical error during agent workflow execution",
            mapping_rank=1,
            reasoning=f"WORKFLOW ERROR: A critical exception occurred during the agent workflow execution. Error details: {str(e)}. This prevented the completion of the mapping analysis. REQUIRES MANUAL REVIEW AND CLASSIFICATION. The error should be investigated and resolved before attempting to remap this record."
        )]

def extract_tool_results_from_messages(messages: List) -> Dict[str, Any]:
    """Extract results from each expert tool call"""
    tool_results = {
        'semantic_similarity_expert': None,
        'context_analysis_expert': None,
        'validation_expert': None,
        'final_decision_expert': None
    }
    
    print("\n🔍 Analyzing tool calls in agent messages...")
    
    for i, msg in enumerate(messages):
        if isinstance(msg, ToolMessage):
            tool_name = str(msg.name) if hasattr(msg, 'name') else "unknown"
            print(f"  [{i}] ToolMessage: {tool_name}")
            
            if tool_name in tool_results and msg.content:
                tool_results[tool_name] = msg.content
                print(f"      ✓ Captured output (length: {len(msg.content)} chars)")
        elif isinstance(msg, AIMessage):
            # Check if AI message contains tool calls
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tc in msg.tool_calls:
                    tool_name = tc.get('name', 'unknown')
                    print(f"  [{i}] AIMessage requesting tool: {tool_name}")
    
    # Summary
    called_tools = [k for k, v in tool_results.items() if v is not None]
    print(f"\n  📊 Tools successfully called: {len(called_tools)}/4")
    for tool in called_tools:
        print(f"      ✓ {tool}")
    
    missing_tools = [k for k, v in tool_results.items() if v is None]
    if missing_tools:
        print(f"  ⚠️  Tools NOT called: {len(missing_tools)}")
        for tool in missing_tools:
            print(f"      ✗ {tool}")
    
    return tool_results
    """Format reasoning for output"""
    parts = []
    
    reasoning = mapping.get("reasoning", "")
    if reasoning:
        parts.append(f"MAPPING REASONING:\n{reasoning}")
    
    evidence = mapping.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nEVIDENCE SUMMARY:\n" + "\n".join(f"• {e}" for e in evidence))
    
    if overall_reasoning and mapping.get("mapping_rank", 1) == 1:
        parts.append(f"\n\nOVERALL CONTEXT:\n{overall_reasoning}")
    
    return "\n".join(parts) if parts else "comprehensive analysis completed"

# ==================== EXCEL OUTPUT WITH GROUPING ====================

def save_results_to_grouped_excel(results: List[MappingResult], filename: str = "GDC_Mapping_Results.xlsx"):
    """Save results to Excel with grouping by Record Class"""
    print(f"\n💾 Saving results to Excel with grouping: {filename}")
    
    # Convert to DataFrame
    results_dict = [r.model_dump(by_alias=True) for r in results]
    df = pd.DataFrame(results_dict)
    
    # Sort by Name (Record Class) and then by Mapping Rank
    df = df.sort_values(by=['Name', 'Mapping Rank'])
    
    # Create Excel workbook
    wb = Workbook()
    ws = wb.active
    ws.title = "GDC Mappings"
    
    # Define styles
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)
    
    group_header_fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
    group_header_font = Font(bold=True, size=10)
    
    error_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    error_font = Font(bold=True, color="9C0006", size=10)
    
    normal_font = Font(size=10)
    wrap_alignment = Alignment(wrap_text=True, vertical="top")
    center_alignment = Alignment(horizontal="center", vertical="center")
    
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    # Write headers
    headers = list(df.columns)
    for col_idx, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col_idx, value=header)
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = center_alignment
        cell.border = thin_border
    
    # Set column widths
    column_widths = {
        'GUID': 35,
        'Code': 15,
        'Name': 30,
        'Description': 40,
        'GDC Name': 25,
        'GDC Description': 35,
        'Mapping Rank': 12,
        'Reasoning': 60
    }
    
    for col_idx, header in enumerate(headers, 1):
        ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = column_widths.get(header, 20)
    
    # Write data with grouping
    current_row = 2
    current_record_name = None
    group_start_row = 2
    
    for idx, row in df.iterrows():
        record_name = row['Name']
        
        # Check if we're starting a new group
        if record_name != current_record_name:
            # If not the first group, we can add visual separation or grouping
            if current_record_name is not None:
                # Add a subtle separator (optional)
                pass
            
            current_record_name = record_name
            group_start_row = current_row
        
        # Write row data
        for col_idx, header in enumerate(headers, 1):
            value = row[header]
            cell = ws.cell(row=current_row, column=col_idx, value=value)
            cell.font = normal_font
            cell.border = thin_border
            
            # Check if this is an error row
            is_error_row = row['GDC Name'] in [
                'FINAL_DECISION_NOT_CALLED', 'EMPTY_FINAL_DECISION', 'JSON_PARSE_ERROR',
                'VALIDATION_ERROR', 'NO_MAPPINGS_IN_DECISION', 'CRITICAL_ERROR',
                'EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR'
            ]
            
            # Special formatting for error rows
            if is_error_row:
                cell.fill = error_fill
                cell.font = error_font
            # Special formatting for certain columns
            elif header in ['Description', 'GDC Description', 'Reasoning']:
                cell.alignment = wrap_alignment
            elif header == 'Mapping Rank':
                cell.alignment = center_alignment
                
                # Highlight primary mappings (Rank 1)
                if value == 1:
                    cell.fill = PatternFill(start_color="E2EFDA", end_color="E2EFDA", fill_type="solid")
                    cell.font = Font(bold=True, size=10)
            else:
                cell.alignment = Alignment(vertical="top")
        
        current_row += 1
    
    # Freeze the header row
    ws.freeze_panes = "A2"
    
    # Auto-filter
    ws.auto_filter.ref = ws.dimensions
    
    # Add a note about error codes at the bottom
    note_row = ws.max_row + 2
    ws.cell(row=note_row, column=1, value="ERROR CODE EXPLANATIONS:").font = Font(bold=True, size=10)
    note_row += 1
    ws.cell(row=note_row, column=1, value="FINAL_DECISION_NOT_CALLED: Agent stopped before calling final_decision_expert")
    note_row += 1
    ws.cell(row=note_row, column=1, value="EMPTY_FINAL_DECISION: final_decision_expert returned empty content")
    note_row += 1
    ws.cell(row=note_row, column=1, value="JSON_PARSE_ERROR: Output was not valid JSON")
    note_row += 1
    ws.cell(row=note_row, column=1, value="VALIDATION_ERROR: JSON failed Pydantic validation")
    note_row += 1
    ws.cell(row=note_row, column=1, value="NO_MAPPINGS_IN_DECISION: Expert returned zero mappings (violates requirement)")
    note_row += 1
    ws.cell(row=note_row, column=1, value="CRITICAL_ERROR: Unhandled exception during processing")
    note_row += 1
    ws.cell(row=note_row, column=1, value="")
    note_row += 1
    ws.cell(row=note_row, column=1, value="All error cases require manual review and classification. Check logs for details.").font = Font(bold=True)
    
    # Add a summary sheet
    summary_ws = wb.create_sheet("Summary")
    summary_ws.append(["Metric", "Value"])
    summary_ws.append(["Total Record Classes", df['Name'].nunique()])
    summary_ws.append(["Total GDC Mappings", len(df)])
    summary_ws.append(["Average Mappings per Record", f"{len(df) / df['Name'].nunique():.2f}"])
    summary_ws.append(["Unique GDCs Used", df['GDC Name'].nunique()])
    
    # Count error cases
    error_codes = ['FINAL_DECISION_NOT_CALLED', 'EMPTY_FINAL_DECISION', 'JSON_PARSE_ERROR',
                   'VALIDATION_ERROR', 'NO_MAPPINGS_IN_DECISION', 'CRITICAL_ERROR',
                   'EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR']
    error_count = len(df[df['GDC Name'].isin(error_codes)])
    summary_ws.append([""])
    summary_ws.append(["Records Requiring Manual Review", error_count])
    if error_count > 0:
        summary_ws.append(["⚠️ Note", "Error rows are highlighted in red in main sheet"])
    
    # Format summary sheet
    for row in summary_ws.iter_rows(min_row=1, max_row=1, min_col=1, max_col=2):
        for cell in row:
            cell.fill = header_fill
            cell.font = header_font
            cell.border = thin_border
    
    for row in summary_ws.iter_rows(min_row=2, max_row=summary_ws.max_row, min_col=1, max_col=2):
        for cell in row:
            cell.border = thin_border
            cell.font = normal_font
    
    summary_ws.column_dimensions['A'].width = 30
    summary_ws.column_dimensions['B'].width = 20
    
    # Add a GDC frequency sheet
    gdc_freq_ws = wb.create_sheet("GDC Frequency")
    gdc_freq = df['GDC Name'].value_counts().reset_index()
    gdc_freq.columns = ['GDC Name', 'Frequency']
    
    for r_idx, row in enumerate(dataframe_to_rows(gdc_freq, index=False, header=True), 1):
        for c_idx, value in enumerate(row, 1):
            cell = gdc_freq_ws.cell(row=r_idx, column=c_idx, value=value)
            if r_idx == 1:
                cell.fill = header_fill
                cell.font = header_font
            else:
                cell.font = normal_font
            cell.border = thin_border
    
    gdc_freq_ws.column_dimensions['A'].width = 30
    gdc_freq_ws.column_dimensions['B'].width = 15
    
    # Save workbook
    wb.save(filename)
    print(f"✓ Excel file saved: {filename}")
    print(f"  📊 Sheets: GDC Mappings (main), Summary, GDC Frequency")
    print(f"  🎨 Features: Grouping by Record Class, Color coding, Auto-filter, Frozen headers")

# ==================== MAIN EXECUTION ====================

def test_openai_connection():
    """Test OpenAI API connection and embedding capability"""
    print("\n🔍 Testing OpenAI API connection...")
    
    if not OPENAI_API_KEY:
        print("❌ OPENAI_API_KEY is not set!")
        return False
    
    print(f"✓ API Key is set (length: {len(OPENAI_API_KEY)})")
    print(f"✓ Base URL: {OPENAI_BASE_URL}")
    print(f"✓ Embedding Model: {EMBEDDING_MODEL}")
    print(f"✓ Embedding Dimensions: {EMBEDDING_DIMENSIONS}")
    
    try:
        print("\n🧪 Testing embedding with a sample text...")
        test_response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=["test connection"],
            dimensions=EMBEDDING_DIMENSIONS
        )
        
        if test_response.data and len(test_response.data[0].embedding) == EMBEDDING_DIMENSIONS:
            print(f"✅ Successfully created test embedding!")
            print(f"✓ Embedding dimension: {len(test_response.data[0].embedding)}")
            return True
        else:
            print(f"❌ Test embedding failed - unexpected response format")
            return False
            
    except Exception as e:
        print(f"❌ Connection test failed!")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")
        
        # Check for common errors
        if "api_key" in str(e).lower() or "authentication" in str(e).lower():
            print("\n💡 Troubleshooting: API Key issue")
            print("   - Verify your API key is correct")
            print("   - Check if the key has necessary permissions")
            print("   - Ensure the key is not expired")
        elif "rate" in str(e).lower() or "limit" in str(e).lower():
            print("\n💡 Troubleshooting: Rate limit issue")
            print("   - Wait a few moments and try again")
            print("   - Check your OpenAI usage limits")
        elif "network" in str(e).lower() or "connection" in str(e).lower():
            print("\n💡 Troubleshooting: Network issue")
            print("   - Check your internet connection")
            print("   - Verify you can access api.openai.com")
            print("   - Check if you're behind a proxy/firewall")
        
        return False

def main():
    """Main execution function"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM")
    print("Dynamic Chain of Thought + Mixture of Experts")
    print("RAG with OpenAI Embeddings API | One-to-Many Mappings")
    print("Excel Output with Grouping")
    print("=" * 80)
    
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY environment variable not set")
        print("\nPlease set it using:")
        print("  export OPENAI_API_KEY='your-api-key'")
        print("\nOr in Python:")
        print("  os.environ['OPENAI_API_KEY'] = 'your-api-key'")
        return
    
    # Test OpenAI connection before proceeding
    if not test_openai_connection():
        print("\n❌ OpenAI connection test failed. Please fix the issues above before continuing.")
        return
    
    print("\n📁 Loading data...")
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded {len(gdc_master)} GDC master entries")
    print(f"✓ Loaded {len(gdc_context)} GDC context entries")
    print(f"✓ Loaded {len(validation_set)} validation entries")
    print(f"✓ Loaded {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load required data files")
        return
    
    print("\n🔍 Building RAG vector stores (using OpenAI API directly)...")
    gdc_master_vectorstore = build_gdc_master_vectorstore(gdc_master)
    gdc_context_vectorstore = build_gdc_context_vectorstore(gdc_context)
    
    print("\n🤖 Creating Dynamic CoT + Mixture of Experts Agent...")
    agent = create_react_agent_workflow()
    print(f"✓ Model: {OPENAI_MODEL}")
    print(f"✓ Reasoning: {REASONING_EFFORT}")
    print(f"✓ Embeddings: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)")
    print(f"✓ Approach: Dynamic Chain of Thought + Mixture of Experts")
    print(f"✓ Mode: One-to-many with RAG + Fallback guarantees")
    
    print("\n🚀 Starting mapping process...\n")
    all_results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\n{'='*80}")
        print(f"RECORD {i}/{len(record_classes)}: {record.name}")
        print(f"{'='*80}")
        
        try:
            record_mappings = process_single_record(
                agent=agent,
                record=record,
                validation_set=validation_set
            )
            
            all_results.extend(record_mappings)
            
            # Display results
            gdc_names = [m.gdc_name for m in record_mappings]
            print(f"\n✅ MAPPED TO {len(record_mappings)} GDC(s):")
            for mapping in record_mappings:
                print(f"   Rank {mapping.mapping_rank}: {mapping.gdc_name}")
            
        except Exception as e:
            print(f"\n❌ ERROR: {e}")
            print(f"   Marking record for manual review...")
            import traceback
            traceback.print_exc()
            
            # Create error result for manual review
            error_result = MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="PROCESSING_ERROR",
                gdc_description="error in main processing loop",
                mapping_rank=1,
                reasoning=f"PROCESSING ERROR: An exception occurred in the main processing loop. Error: {str(e)}. REQUIRES MANUAL REVIEW AND CLASSIFICATION."
            )
            all_results.append(error_result)
    
    print("\n" + "="*80)
    print("💾 SAVING RESULTS")
    print("="*80)
    
    # Save to Excel with grouping
    save_results_to_grouped_excel(all_results, "GDC_Mapping_Results.xlsx")
    
    # Also save to CSV as backup
    results_dict = [r.model_dump(by_alias=True) for r in all_results]
    df = pd.DataFrame(results_dict)
    df.to_csv("GDC_Mapping_Results.csv", index=False, encoding='utf-8-sig')
    print(f"✓ CSV backup saved: GDC_Mapping_Results.csv")
    
    print("\n📊 FINAL SUMMARY")
    print("="*80)
    print(f"Total Record Classes Processed: {len(record_classes)}")
    print(f"Total GDC Mappings Generated: {len(all_results)}")
    if len(record_classes) > 0:
        print(f"Average Mappings per Record: {len(all_results)/len(record_classes):.2f}")
    print(f"Unique GDCs Used: {len(set(r.gdc_name for r in all_results))}")
    
    # Check for error cases
    error_gdcs = [r for r in all_results if r.gdc_name in [
        'FINAL_DECISION_NOT_CALLED', 'EMPTY_FINAL_DECISION', 'JSON_PARSE_ERROR', 
        'VALIDATION_ERROR', 'NO_MAPPINGS_IN_DECISION', 'CRITICAL_ERROR',
        'EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR'
    ]]
    if error_gdcs:
        print(f"\n⚠️  Records Requiring Manual Review: {len(error_gdcs)}")
        print("   These records encountered errors during automated mapping:")
        for err in error_gdcs:
            print(f"   - {err.name}: {err.gdc_name}")
    
    # Show mapping distribution
    from collections import Counter
    mapping_counts = Counter(r.gdc_name for r in all_results)
    
    error_codes = ['FINAL_DECISION_NOT_CALLED', 'EMPTY_FINAL_DECISION', 'JSON_PARSE_ERROR',
                   'VALIDATION_ERROR', 'NO_MAPPINGS_IN_DECISION', 'CRITICAL_ERROR',
                   'EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR']
    
    print(f"\nTop 5 Most Common GDCs:")
    for gdc, count in mapping_counts.most_common(5):
        if gdc not in error_codes:
            print(f"  - {gdc}: {count} mappings")
    
    print("="*80)
    print("✅ PROCESSING COMPLETE")
    print("="*80)

if __name__ == "__main__":
    main(), '', text)
    
    # Try to find JSON object or array - be more aggressive
    # Look for the last complete JSON object (in case there's reasoning before it)
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = list(re.finditer(json_pattern, text, re.DOTALL))
    
    if matches:
        # Try each match from last to first (most likely to be the final output)
        for match in reversed(matches):
            json_str = match.group(0)
            try:
                # Validate it's actual JSON
                json.loads(json_str)
                return json_str
            except:
                continue
    
    # Fallback: try to find anything that looks like JSON
    json_match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
    if json_match:
        return json_match.group(1)
    
    return text

def load_json_file(filepath: str, model_class: type[BaseModel]) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except Exception as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== RAG VECTOR STORE SETUP ====================

def build_gdc_master_vectorstore(gdc_master_list: List[GDCMaster]) -> InMemoryVectorStore:
    """Build vector store for GDC Master data using OpenAI API directly"""
    print(f"🔧 Building GDC Master vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_master_document(gdc) for gdc in gdc_master_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Master vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def build_gdc_context_vectorstore(gdc_context_list: List[GDCWithContext]) -> InMemoryVectorStore:
    """Build vector store for GDC Context data using OpenAI API directly"""
    print(f"🔧 Building GDC Context vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_context_document(gdc_ctx) for gdc_ctx in gdc_context_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Context vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def rag_retrieve_relevant_gdcs(query: str, k: int = 10) -> str:
    """RAG: Retrieve relevant GDCs using semantic search"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    query_lower = preprocess_text(query)
    
    master_results = gdc_master_vectorstore.similarity_search(query_lower, k=k)
    context_results = gdc_context_vectorstore.similarity_search(query_lower, k=k)
    
    retrieved_info = {
        "master_matches": [],
        "context_matches": []
    }
    
    for doc in master_results:
        retrieved_info["master_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "definition": doc.metadata.get("definition", ""),
            "data_domain": doc.metadata.get("data_domain", "")
        })
    
    for doc in context_results:
        retrieved_info["context_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "gdc_description": doc.metadata.get("gdc_description", ""),
            "pbt_count": doc.metadata.get("pbt_count", 0),
            "app_count": doc.metadata.get("app_count", 0),
            "process_count": doc.metadata.get("process_count", 0),
            "content_preview": doc.page_content[:300]
        })
    
    return json.dumps(retrieved_info, indent=2)



# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str) -> str:
    """
    SEMANTIC SIMILARITY EXPERT - Stage 1 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized semantic analysis expert with deep expertise in natural language understanding,
    conceptual similarity analysis, and domain taxonomy classification. Your primary responsibility is to
    identify ALL Group Data Categories (GDCs) that demonstrate semantic relevance to the input record class.
    
    CORE COMPETENCIES:
    1. Linguistic Analysis: Parse and understand terminology, jargon, and domain-specific language
    2. Conceptual Mapping: Identify abstract relationships between concepts and categories
    3. Synonym Recognition: Detect equivalent terms and related terminology across different naming conventions
    4. Multi-dimensional Similarity: Assess similarity across name, description, purpose, and functional scope
    5. Threshold-based Filtering: Apply intelligent scoring to distinguish strong matches from weak associations
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: DEEP COMPREHENSION
    - Thoroughly analyze the record class name: What does it represent? What domain does it belong to?
    - Parse the description for key concepts, processes, entities, and functional indicators
    - Identify implicit information: What isn't explicitly stated but is implied by the terminology?
    - Extract domain signals: What business area, function, or process does this record support?
    
    STEP 2: RAG-POWERED RETRIEVAL
    - Utilize vector embeddings to retrieve semantically similar GDCs from the knowledge base
    - The RAG system has already retrieved the top-k most relevant GDCs based on embedding similarity
    - These represent the candidate pool for detailed analysis
    
    STEP 3: SEMANTIC SCORING METHODOLOGY
    For each retrieved GDC, perform multi-factor analysis:
    
    A. NAME SIMILARITY ANALYSIS (0-30 points)
       - Exact or near-exact name match: 25-30 points
       - Strong synonym/related terms: 18-25 points  
       - Partial overlap or shared keywords: 10-18 points
       - Conceptually related but different terms: 5-10 points
       - Minimal name connection: 0-5 points
       
    B. DESCRIPTION/DEFINITION ALIGNMENT (0-40 points)
       - Definitions describe identical or nearly identical scope: 35-40 points
       - Substantial functional overlap with minor differences: 25-35 points
       - Moderate overlap in purpose and data elements: 15-25 points
       - Some conceptual alignment but different primary focus: 8-15 points
       - Minimal description alignment: 0-8 points
       
    C. DOMAIN AND CONTEXT RELEVANCE (0-30 points)
       - Same business domain and functional area: 25-30 points
       - Related domains with clear connections: 18-25 points
       - Adjacent or supporting domains: 10-18 points
       - Loosely related through higher-level abstractions: 5-10 points
       - Minimal domain connection: 0-5 points
    
    STEP 4: MATCH IDENTIFICATION AND FILTERING
    - Calculate total score for each GDC (sum of A + B + C, maximum 100 points)
    - INCLUSION THRESHOLD: Include all GDCs scoring 55 or above (indicates meaningful relevance)
    - STRONG MATCH: Scores 75+ indicate high confidence matches
    - MODERATE MATCH: Scores 55-74 indicate plausible but less certain matches
    - Exclude GDCs below 55 unless they represent important edge cases
    
    STEP 5: ONE-TO-MANY MAPPING RATIONALE
    Critical understanding: A single record class FREQUENTLY maps to multiple GDCs because:
    - Records often contain data elements from multiple conceptual categories
    - Business requirements may necessitate multiple classification dimensions
    - Different aspects of a record serve different governance or operational purposes
    - Comprehensive data management requires multi-faceted categorization
    
    STEP 6: EVIDENCE-BASED REASONING
    For each included GDC, provide detailed reasoning that addresses:
    - Specific linguistic/semantic connections between record and GDC
    - Key terms, concepts, or phrases that create the link
    - Functional or purposive alignment
    - Score breakdown with justification for each dimension
    - Why this GDC should be included in the final mapping set
    
    INPUT DATA:
    Record Name: {record_name}
    Record Description: {record_desc}
    
    RAG-Retrieved Candidate GDCs (via OpenAI embeddings API):
    {rag_results}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text) with this exact structure:
    {{
      "matches": [
        {{
          "gdc_name": "exact gdc name from rag results",
          "gdc_description": "exact definition from rag results",
          "similarity_score": 87.5,
          "reasoning": "COMPREHENSIVE MULTI-PARAGRAPH EXPLANATION: This GDC demonstrates strong semantic alignment with the record class for the following reasons. [Analyze name similarity]: The record name '{record_name}' directly correlates with the GDC name through [specific connections, shared terms, conceptual overlap]. [Analyze description alignment]: The record description indicates [key functional elements], which align with the GDC's defined scope of [definition elements]. [Domain analysis]: Both operate within the [domain/business area] context. [Score justification]: Name similarity (X/30 points) because [specific reasoning]. Description alignment (Y/40 points) because [specific reasoning]. Domain relevance (Z/30 points) because [specific reasoning]. Total score: X+Y+Z. [Conclusion]: This semantic match is [strong/moderate] and should be included because [final justification]."
        }}
      ],
      "multiple_matches_rationale": "COMPREHENSIVE EXPLANATION: Multiple GDCs are semantically relevant to this record class due to its multi-dimensional nature. Specifically: [Dimension 1 analysis] - The record contains aspects related to [concept], which aligns with [GDC names]. [Dimension 2 analysis] - Additional elements pertaining to [concept] connect to [other GDC names]. [Synthesis]: The complete semantic space of this record class spans [number] distinct GDC categories, each addressing [different aspects]. This multi-mapping approach ensures [benefits: complete coverage, accurate classification, comprehensive governance, etc.]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Identify ALL semantically relevant GDCs (not just the top match)
    - Provide substantial, evidence-based reasoning (minimum 150 words per match)
    - Apply scoring methodology rigorously and transparently
    - Include multiple matches when evidence supports them
    - Output ONLY valid JSON with no additional text
    - Use exact GDC names and definitions from RAG results
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    query = f"{record_name_lower} {record_desc_lower}"
    retrieved_gdcs = rag_retrieve_relevant_gdcs(query, k=15)
    
    prompt = f"""You are a SEMANTIC SIMILARITY EXPERT conducting comprehensive semantic analysis for GDC classification.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: COMPREHENSION PHASE
First, deeply understand the record class:

Record Name: {record_name_lower}
Record Description: {record_desc_lower}

Think through:
- What type of data or information does this record contain?
- What business function or process does it support?
- What domain or organizational area is it part of?
- What are the key concepts, entities, and relationships?
- What terminology patterns indicate its purpose?

STEP 2: RAG RETRIEVAL ANALYSIS
Examine the RAG-retrieved candidate GDCs:

{retrieved_gdcs}

STEP 3: SYSTEMATIC SEMANTIC SCORING
For each candidate GDC, analyze:

A. NAME SIMILARITY (0-30 points)
   - Compare record name to GDC name
   - Identify shared terms, synonyms, conceptual overlap
   - Score based on strength of linguistic connection

B. DESCRIPTION ALIGNMENT (0-40 points)
   - Compare record description to GDC definition
   - Assess functional and purposive similarity
   - Score based on scope and content overlap

C. DOMAIN RELEVANCE (0-30 points)
   - Evaluate business domain alignment
   - Consider organizational and functional context
   - Score based on contextual appropriateness

STEP 4: MATCH IDENTIFICATION
- Include all GDCs scoring 55+
- Provide detailed reasoning for each
- Explain why multiple matches may be needed

STEP 5: OUTPUT GENERATION
Create valid JSON output with comprehensive reasoning.

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "matches": [
    {{
      "gdc_name": "exact gdc name from rag results",
      "gdc_description": "exact gdc definition",
      "similarity_score": 85.5,
      "reasoning": "COMPREHENSIVE SEMANTIC ANALYSIS: [COMPREHENSION] The record class '{record_name_lower}' with description '{record_desc_lower}' represents [interpretation of what the record is]. [NAME ANALYSIS] The name demonstrates [specific similarities] with the GDC name '[gdc_name]', including [shared terms/concepts/semantic patterns]. [DESCRIPTION ANALYSIS] The record description's focus on [key elements] aligns with the GDC's defined scope of [definition elements] because [detailed explanation]. [DOMAIN ANALYSIS] Both operate within [domain/context], indicated by [specific signals]. [SCORING BREAKDOWN] Name similarity: [X/30] points - [justification]. Description alignment: [Y/40] points - [justification]. Domain relevance: [Z/30] points - [justification]. Total: [X+Y+Z]. [CONCLUSION] This semantic match scores [score] and is [included/considered strong] because [final reasoning]. The connection is [characterized] and provides [value to classification]."
    }}
  ],
  "multiple_matches_rationale": "MULTI-DIMENSIONAL MAPPING JUSTIFICATION: This record class maps to [N] GDCs due to its multi-faceted nature. [DIMENSION 1] The aspect concerning [element] semantically aligns with [GDC name(s)] because [reasoning]. [DIMENSION 2] The component related to [element] connects to [GDC name(s)] because [reasoning]. [SYNTHESIS] The complete semantic space spans multiple categories because [fundamental explanation]. [BENEFITS] Multiple mappings ensure [governance value, comprehensive coverage, accurate classification]. [CONFIDENCE] The semantic evidence strongly supports [N] GDC mappings with [aggregate confidence level]."
}}

CRITICAL REQUIREMENTS:
✓ Return ALL semantically relevant GDCs (score 55+)
✓ Provide comprehensive, evidence-based reasoning (150+ words per match)
✓ Apply scoring methodology transparently  
✓ Explain multi-dimensional aspects when multiple GDCs are relevant
✓ Output ONLY valid JSON (no markdown, no preamble, no postscript)
✓ Use exact GDC names and definitions from RAG results"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in semantic_similarity_expert: {e}")
        error_response = SemanticMatchResponse(
            matches=[],
            multiple_matches_rationale=f"error in semantic analysis: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, semantic_matches: str) -> str:
    """
    CONTEXT ANALYSIS EXPERT - Stage 2 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized contextual analysis expert with expertise in organizational hierarchies,
    business process analysis, and operational system understanding. Your role is to validate and
    strengthen semantic matches by examining operational context, hierarchical relationships, and
    real-world usage patterns.
    
    CORE COMPETENCIES:
    1. Hierarchical Analysis: Understanding PBT → Application → Process relationships
    2. Operational Validation: Verifying that semantic matches align with actual system usage
    3. Business Process Mapping: Connecting records to business workflows and functions
    4. Organizational Context: Understanding how data flows through organizational structures
    5. Evidence Synthesis: Gathering and presenting concrete supporting evidence
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: SEMANTIC MATCH REVIEW
    - Receive and parse semantic matches from previous expert
    - Understand which GDCs were identified and why
    - Extract key hypotheses to validate through contextual analysis
    
    STEP 2: RAG-POWERED CONTEXT RETRIEVAL
    - Utilize RAG system to retrieve operational context for each semantic match
    - Access hierarchical information: PBTs, Applications, Processes
    - Gather real-world usage patterns and organizational relationships
    
    STEP 3: CONTEXTUAL ALIGNMENT SCORING
    
    For each semantically matched GDC, perform three-tier analysis:
    
    A. PRIMARY BUSINESS TYPE (PBT) ALIGNMENT (0-40 points)
       PBTs represent the highest-level business organization. Analyze:
       - Does this GDC's PBT align with the record's business purpose?
       - Would the organizational units using this GDC logically create/manage this record?
       - PBT name and description relevance to record function
       
       Scoring:
       - Perfect PBT alignment (record clearly belongs to this business area): 35-40 points
       - Strong PBT alignment (highly relevant business area): 28-35 points
       - Moderate alignment (plausible business area connection): 20-28 points
       - Weak alignment (distant business area connection): 10-20 points
       - No clear PBT alignment found but not contradictory: 5-10 points
       - PBT context unavailable: 10 points (neutral - don't penalize)
    
    B. APPLICATION RELEVANCE (0-30 points)
       Applications are systems that create, manage, or process data. Analyze:
       - Would applications using this GDC logically handle this type of record?
       - Application descriptions and purposes vs. record function
       - System context and data management patterns
       
       Scoring:
       - Applications perfectly match record's system context: 26-30 points
       - Applications strongly relevant to record type: 20-26 points
       - Applications moderately relevant: 14-20 points
       - Applications weakly relevant: 7-14 points
       - No clear application context but not contradictory: 5-7 points
       - Application context unavailable: 8 points (neutral)
    
    C. PROCESS MATCHING (0-30 points)
       Processes are business workflows that involve the data. Analyze:
       - Do processes using this GDC align with record's business function?
       - Process descriptions vs. record's role in business operations
       - Workflow context and operational patterns
       
       Scoring:
       - Processes perfectly match record's business function: 26-30 points
       - Processes strongly align with record purpose: 20-26 points
       - Processes moderately relevant: 14-20 points
       - Processes weakly relevant: 7-14 points
       - No clear process context but not contradictory: 5-7 points
       - Process context unavailable: 8 points (neutral)
    
    STEP 4: EVIDENCE GATHERING
    For each GDC, compile concrete evidence:
    - "PBT: [name] ([description]) - Relevant because [specific connection to record]"
    - "Application: [name] - [description] - Relevant because [how it would use this record]"
    - "Process: [name]: [description] - Relevant because [connection to record's function]"
    
    STEP 5: INCLUSION/EXCLUSION DECISIONS
    - KEEP GDCs where context supports or doesn't contradict semantic match
    - KEEP GDCs even with limited context if semantic match was strong (score 75+)
    - ONLY EXCLUDE if context directly contradicts semantic match
    - When in doubt, KEEP the GDC and explain the context situation
    
    STEP 6: COMPREHENSIVE JUSTIFICATION
    For each GDC, explain:
    - How operational context validates the semantic match
    - What organizational structures support this classification  
    - Why this GDC makes sense given hierarchical relationships
    - Confidence level based on available context
    
    INPUT DATA:
    Record Name: {record_name}
    Record Description: {record_desc}
    
    Semantic Matches (from previous expert):
    {semantic_matches}
    
    RAG-Retrieved Contextual Information:
    {context_info}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "context_analysis": [
        {{
          "gdc_name": "exact gdc name",
          "context_evidence": [
            "PBT: [pbt_name] ([pbt_description]) - This PBT is relevant because it represents the [business area] which directly handles [record's function/purpose]. The organizational unit [specific alignment reasoning].",
            "Application: [app_name] - [app_description] - This application is relevant because systems managing [app function] would logically [create/process/manage] records of type '{record_name}' as part of [specific use case/workflow].",
            "Process: [process_name]: [process_description] - This process is relevant because the workflow of [process function] directly involves [record's role] when [specific operational scenario]."
          ],
          "alignment_score": 88.5,
          "relevance_justification": "CONTEXTUAL VALIDATION: This GDC's operational context strongly validates the semantic match. [PBT ANALYSIS] The PBT '[name]' operates in the [business domain] which is directly responsible for [functions related to record]. This organizational alignment is [strong/moderate/clear] because [specific reasoning]. [APPLICATION ANALYSIS] Applications like '[app_name]' provide system context showing that [how apps would handle this record type]. The application descriptions indicate [specific technical/functional alignment]. [PROCESS ANALYSIS] Business processes such as '[process_name]' demonstrate that workflows involving [process description] would naturally [create/use/manage] this record type because [operational reasoning]. [SCORING] PBT alignment: [X/40] - [justification]. Application relevance: [Y/30] - [justification]. Process matching: [Z/30] - [justification]. Total: [X+Y+Z]. [CONFIDENCE] The contextual evidence provides [high/moderate/sufficient] confidence in this mapping.",
          "reasoning": "COMPREHENSIVE CONTEXTUAL REASONING: [CONTEXT SUMMARY] The RAG-retrieved hierarchical context reveals [key findings from PBTs/Apps/Processes]. [VALIDATION STRENGTH] This context [strongly validates/supports/doesn't contradict] the semantic match identified in Stage 1. [ORGANIZATIONAL FIT] Within the organizational hierarchy, this GDC is positioned at [context description], which aligns with the record's [business purpose/function]. [OPERATIONAL PATTERNS] The applications and processes associated with this GDC indicate usage patterns consistent with [record characteristics]. [LIMITATIONS IF ANY] While [any context limitations], the available evidence [still supports/doesn't contradict] the classification. [FINAL ASSESSMENT] The contextual analysis [strengthens/maintains/clarifies] the confidence in this GDC mapping because [synthesis of all contextual evidence]."
        }}
      ],
      "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Preserve ALL semantically matched GDCs unless context contradicts
    - Extract detailed, specific evidence from PBTs, Applications, and Processes
    - Apply scoring methodology rigorously across all three dimensions
    - Provide comprehensive justification (150+ words per GDC)
    - Include ALL relevant GDC names in all_relevant_gdcs list
    - Output ONLY valid JSON with no additional text
    - Be thorough - more evidence is better than less
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    try:
        matches_data = json.loads(extract_json_from_text(semantic_matches))
        gdc_names = [m.get("gdc_name", "").lower() for m in matches_data.get("matches", [])]
    except:
        gdc_names = []
    
    context_query = f"{record_name_lower} {record_desc_lower} {' '.join(gdc_names)}"
    retrieved_context = rag_retrieve_relevant_gdcs(context_query, k=10)
    
    prompt = f"""You are a CONTEXT ANALYSIS EXPERT validating GDC mappings through operational and hierarchical context.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: REVIEW SEMANTIC MATCHES
Examine the semantic matches from the previous expert:

{semantic_matches}

Understand: What GDCs were identified? What was the semantic reasoning?

STEP 2: CONTEXT RETRIEVAL ANALYSIS
Examine RAG-retrieved operational context:

{retrieved_context}

This contains PBTs (Primary Business Types), Applications, and Processes for relevant GDCs.

STEP 3: HIERARCHICAL CONTEXT EXTRACTION
For each semantically matched GDC:

A. PBT (PRIMARY BUSINESS TYPE) ANALYSIS
   - Extract all PBT names and descriptions
   - Assess: Would organizational units in this business area create/manage this record?
   - Consider: Does the PBT's scope align with the record's purpose?
   - Score: 0-40 points based on organizational alignment

B. APPLICATION ANALYSIS  
   - Extract application names and descriptions
   - Assess: Would these systems logically handle this record type?
   - Consider: Do application functions match record's system context?
   - Score: 0-30 points based on system relevance

C. PROCESS ANALYSIS
   - Extract process names and descriptions
   - Assess: Do these workflows involve this record type?
   - Consider: Does process function align with record's business role?
   - Score: 0-30 points based on operational alignment

STEP 4: EVIDENCE COMPILATION
For each GDC, create specific evidence statements explaining WHY each PBT/App/Process is relevant.

STEP 5: VALIDATION DECISION
- KEEP all GDCs where context supports or doesn't contradict semantic match
- ONLY EXCLUDE if context directly contradicts
- Include comprehensive justification for each

RECORD CLASS BEING ANALYZED:
Name: {record_name_lower}
Description: {record_desc_lower}

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "context_analysis": [
    {{
      "gdc_name": "exact gdc name",
      "context_evidence": [
        "PBT: [pbt_name] ([pbt_description]) - This PBT is relevant to record class '{record_name_lower}' because [specific analysis of how this business area aligns with the record's purpose, function, and organizational placement].",
        "Application: [app_name] - [app_description] - This application is relevant because systems performing [app functions] would naturally [create/manage/process] records like '{record_name_lower}' in the context of [specific use cases and workflows].",
        "Process: [process_name]: [process_description] - This process is relevant because the workflow involving [process activities] directly relates to [record's business function] when [specific operational scenarios]."
      ],
      "alignment_score": 86.0,
      "relevance_justification": "CONTEXTUAL VALIDATION SUMMARY: [PBT FINDINGS] The PBT context shows [key organizational alignments]. Score: [X/40] because [specific justification]. [APPLICATION FINDINGS] The application context reveals [system usage patterns]. Score: [Y/30] because [specific justification]. [PROCESS FINDINGS] The process context demonstrates [workflow relevance]. Score: [Z/30] because [specific justification]. [TOTAL] Combined alignment score: [X+Y+Z]/100. [CONFIDENCE] This context [strongly validates/supports/doesn't contradict] the semantic match.",
      "reasoning": "DETAILED CONTEXTUAL REASONING: [CONTEXT OVERVIEW] The operational and hierarchical context for this GDC reveals [summary of PBT/App/Process findings]. [PBT DEEP DIVE] The Primary Business Types associated with this GDC include [names], which operate in [business domains]. These organizational units are responsible for [functions], which [does/does not/aligns with] the creation and management of records like '{record_name_lower}' because [detailed analysis]. [APPLICATION DEEP DIVE] Applications such as [names] provide [functionalities]. These systems would [logically/potentially] handle records of this type because [technical and functional reasoning]. The application descriptions indicate [specific patterns] that [support/validate] this classification. [PROCESS DEEP DIVE] Business processes including [names] involve workflows where [activities]. Records like '{record_name_lower}' would participate in these processes by [specific role/function] because [operational reasoning]. [LIMITATIONS] [If any context is limited, explain here, but note that limitation doesn't invalidate the match]. [SYNTHESIS] Taking all contextual evidence together, this GDC mapping is [strongly supported/supported/plausible] with [confidence level]. The hierarchical and operational context [validates/doesn't contradict] the semantic analysis from Stage 1."
    }}
  ],
  "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
}}

CRITICAL REQUIREMENTS:
✓ Validate ALL semantically matched GDCs through context
✓ Extract and cite specific PBTs, Applications, and Processes
✓ Provide detailed evidence statements (50+ words each)
✓ Apply three-tier scoring methodology transparently
✓ Comprehensive justification and reasoning (200+ words per GDC)
✓ KEEP GDCs unless context contradicts (not just lack of context)
✓ Include all relevant GDC names in all_relevant_gdcs list
✓ Output ONLY valid JSON (no markdown, no preamble)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in context_analysis_expert: {e}")
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            all_relevant_gdcs=[]
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdcs: str, validation_data: str) -> str:
    """
    VALIDATION EXPERT - Stage 3 of Mixture of Experts Pipeline
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are a specialized validation expert with expertise in historical data analysis, precedent
    evaluation, and consistency verification. Your role is to cross-reference proposed GDC mappings
    against a historical validation dataset to identify confirmations, conflicts, or novel classifications.
    
    CORE COMPETENCIES:
    1. Historical Analysis: Examining previous classification decisions and patterns
    2. Precedent Evaluation: Assessing whether historical data supports current proposals
    3. Conflict Detection: Identifying discrepancies between proposals and historical records
    4. Confidence Assessment: Determining how validation results impact mapping confidence
    5. Gap Analysis: Understanding when proposed mappings represent new use cases
    
    ANALYTICAL FRAMEWORK:
    
    STEP 1: VALIDATION DATASET PREPARATION
    - Receive validation dataset containing historical GDC-to-ILM category mappings
    - Normalize all text to lowercase for consistent matching
    - Understand the structure: GDC Name, GDC Description, ILM Category Name
    
    STEP 2: SYSTEMATIC SEARCH PROCESS
    For each proposed GDC from previous experts:
    
    A. EXACT NAME SEARCH
       - Search validation dataset for exact GDC name matches
       - Case-insensitive matching
       
    B. PARTIAL NAME SEARCH (if exact fails)
       - Look for GDC names containing the proposed name
       - Look for proposed name contained in validation GDC names
       - Consider substring matches and abbreviations
       
    C. DESCRIPTION-BASED SEARCH (if name searches fail)
       - Compare GDC descriptions for semantic similarity
       - Look for overlapping definitions or scope descriptions
    
    STEP 3: VALIDATION STATUS DETERMINATION
    
    For each proposed GDC, assign ONE of three statuses:
    
    A. "CONFIRMED" - Validation supports the mapping
       Criteria:
       - Exact or partial name match found in validation dataset
       - The matched entry's context is consistent with the proposed mapping
       - Historical precedent exists for this classification
       - ILM category assignment provides additional confidence
       
    B. "CONFLICTED" - Validation contradicts the mapping  
       Criteria:
       - A match is found BUT suggests a different GDC should be used
       - Historical data shows this record type was classified differently
       - The matched entry's context contradicts the current proposal
       - Rare - only assign when clear contradiction exists
       
    C. "NOT_FOUND" - No validation data available
       Criteria:
       - No matches found through any search method
       - This may indicate a new use case or emerging classification need
       - Does NOT invalidate the mapping - just means no historical precedent
       - Mapping still stands based on semantic and contextual analysis
    
    STEP 4: CONFIDENCE IMPACT ASSESSMENT
    
    How validation affects confidence:
    - CONFIRMED: Increases confidence significantly (historical support)
    - NOT_FOUND: No change to confidence (neutral - rely on other evidence)
    - CONFLICTED: Decreases confidence (requires additional review)
    
    STEP 5: COMPREHENSIVE REASONING
    For each validation result, provide:
    - What was searched for and what was found (or not found)
    - How the validation result relates to the proposed mapping
    - What the ILM category tells us (if found)
    - How this affects confidence in the mapping
    - Whether this is a confirmation, novel case, or requires review
    
    INPUT DATA:
    Record Class Name: {record_name}
    
    Proposed GDCs (from previous experts):
    {proposed_gdcs}
    
    Historical Validation Dataset:
    {validation_data}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "validation_results": [
        {{
          "gdc_name": "exact proposed gdc name",
          "validation_found": true,
          "matching_entry": {{
            "gdc_name": "matched gdc name from validation",
            "ilm_category_name": "associated ilm category"
          }},
          "validation_status": "confirmed",
          "validation_reasoning": "COMPREHENSIVE VALIDATION ANALYSIS: [SEARCH PROCESS] Searched validation dataset for GDC '[gdc_name]' using [exact/partial/description] matching. [FINDINGS] Found matching entry with GDC name '[matched_name]' and ILM category '[category]'. [INTERPRETATION] This validation entry CONFIRMS the proposed mapping because [detailed explanation of why validation supports it]. [HISTORICAL CONTEXT] The presence of this GDC in the validation set indicates [what this tells us about prior classification decisions]. [ILM CATEGORY ANALYSIS] The ILM category '[category]' is associated with [type of data/information/records], which aligns with record class '{record_name}' because [specific reasoning about category appropriateness]. [CONFIDENCE IMPACT] This validation significantly increases confidence in the mapping by [degree/amount] because [reasons - historical precedent, consistency with past decisions, category appropriateness]. [PRECEDENT VALUE] Historical data shows [patterns, frequency, context of use]. [CONCLUSION] The validation provides [strong/moderate/clear] support for including this GDC in the final mapping."
        }},
        {{
          "gdc_name": "another proposed gdc",
          "validation_found": false,
          "matching_entry": null,
          "validation_status": "not_found",
          "validation_reasoning": "COMPREHENSIVE VALIDATION ANALYSIS: [SEARCH PROCESS] Searched validation dataset for GDC '[gdc_name]' using exact name matching, partial matching, and description-based search. [FINDINGS] No matching entries found in the validation dataset. [INTERPRETATION] The absence of validation data does NOT invalidate this mapping - it indicates this may be [a new use case/an emerging classification need/a less common category]. [ANALYTICAL BASIS] The proposed mapping is based on [semantic similarity score of X from Stage 1] and [contextual alignment score of Y from Stage 2], which provide [strong/substantial] evidence independent of validation. [CONFIDENCE IMPACT] Lack of validation data does not decrease confidence - the mapping stands on its analytical merits. However, we note this as a [novel classification/new application] that [should be documented for future reference/represents an emerging pattern]. [RECOMMENDATION] Proceed with this GDC mapping based on the strong semantic and contextual evidence, while flagging it as [a new use case requiring documentation]. [CONTEXT] The absence may be due to [possible reasons: new GDC, emerging business need, dataset incompleteness, different naming in historical records]."
        }}
      ],
      "overall_validation_reasoning": "COMPREHENSIVE VALIDATION SUMMARY: [COVERAGE] Out of [N] proposed GDC mappings, [X] were confirmed by validation data, [Y] were not found in validation, and [Z] showed conflicts. [CONFIRMED MAPPINGS] The confirmed GDCs ([list names]) have strong historical support, with [details about ILM categories, usage patterns, precedents]. This provides [high/substantial] confidence in these classifications because [synthesis of confirmation evidence]. [UNVALIDATED MAPPINGS] The GDCs without validation ([list names]) represent [characterization - new use cases/emerging patterns/less common categories]. While lacking historical precedent, they are supported by [semantic scores, contextual evidence] and should be included because [reasoning]. [CONFLICTS IF ANY] [If conflicts exist, discuss here]. [CONFIDENCE ASSESSMENT] Overall, the validation exercise [strongly supports/supports/provides mixed evidence for] the proposed mappings. [PATTERN ANALYSIS] The validation results reveal [any patterns in confirmed vs unvalidated]. [FINAL SYNTHESIS] Taking all validation evidence together, the proposed GDC mappings are [well-supported/supported/partially supported] with [overall confidence level: high/moderate/sufficient]. [RECOMMENDATION] [Proceed with all mappings/Proceed with confirmed mappings/Review conflicts] based on [reasoning]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Systematically validate EVERY proposed GDC
    - Use multiple search methods (exact, partial, description-based)
    - Distinguish between "not found" and "conflicted" accurately
    - Provide detailed reasoning (150+ words per result)
    - Explain how validation impacts confidence
    - Understand that "not found" ≠ "invalid"
    - Synthesize overall validation patterns and implications
    - Output ONLY valid JSON with no additional text
    """
    record_name_lower = preprocess_text(record_name)
    
    try:
        val_data = json.loads(validation_data)
        val_data_lower = []
        for entry in val_data:
            val_data_lower.append({
                "gdc_name": to_lowercase(entry.get("GDC Name", "")),
                "gdc_description": to_lowercase(entry.get("GDC Description", "")),
                "ilm_category_name": to_lowercase(entry.get("ILM Category Name", ""))
            })
        validation_data_lower = json.dumps(val_data_lower, indent=2)
    except:
        validation_data_lower = validation_data
    
    prompt = f"""You are a VALIDATION EXPERT cross-referencing proposed GDC mappings against historical data.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: UNDERSTAND PROPOSED MAPPINGS
Review proposed GDCs from previous experts:

{proposed_gdcs}

Identify: Which GDCs need validation? What was the analytical reasoning?

STEP 2: PREPARE VALIDATION DATASET
Examine historical validation data:

{validation_data_lower}

Structure: GDC Name → GDC Description → ILM Category Name

STEP 3: SYSTEMATIC VALIDATION SEARCH
For each proposed GDC, execute comprehensive search:

A. EXACT NAME MATCH
   Search for exact GDC name in validation dataset

B. PARTIAL NAME MATCH (if exact fails)
   Search for substring matches, abbreviations

C. DESCRIPTION SIMILARITY (if both fail)
   Look for similar definitions or scope

STEP 4: STATUS DETERMINATION
Assign validation status:
- "confirmed": Found match that supports mapping
- "not_found": No match found (neutral, not negative)
- "conflicted": Found match that contradicts mapping (rare)

STEP 5: CONFIDENCE IMPACT ANALYSIS
Assess how validation affects mapping confidence:
- Confirmed → Increases confidence (historical precedent)
- Not found → Neutral (no historical data, but doesn't invalidate)
- Conflicted → Decreases confidence (needs review)

STEP 6: COMPREHENSIVE REASONING
For each result, explain search process, findings, interpretation, and confidence impact.

RECORD CLASS: {record_name_lower}

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "validation_results": [
    {{
      "gdc_name": "exact proposed gdc name",
      "validation_found": true,
      "matching_entry": {{
        "gdc_name": "matched gdc from validation",
        "ilm_category_name": "ilm category"
      }},
      "validation_status": "confirmed",
      "validation_reasoning": "VALIDATION ANALYSIS FOR '{record_name_lower}': [SEARCH EXECUTED] Performed [exact/partial/description-based] search in validation dataset for GDC '[gdc_name]'. [MATCH FOUND] Located matching entry: GDC Name = '[matched_gdc]', ILM Category = '[category]'. [CONFIRMATION] This validation entry CONFIRMS the proposed mapping because [detailed explanation: how the match supports the proposal, why it's relevant, what it tells us about historical usage]. [ILM CATEGORY CONTEXT] The ILM category '[category]' typically contains [type of data/records], which aligns with '{record_name_lower}' because [specific connection between category purpose and record purpose]. [HISTORICAL PRECEDENT] This validation indicates [what pattern/history it reveals]. [CONFIDENCE BOOST] This confirmation increases mapping confidence from [previous level based on semantic+context] to [higher level] because [reasoning about value of historical validation]. [SUPPORTING DETAILS] Additional validation context: [any other relevant information from the match]. [CONCLUSION] Strong validation support for this GDC mapping."
    }},
    {{
      "gdc_name": "another proposed gdc",
      "validation_found": false,
      "matching_entry": null,
      "validation_status": "not_found",
      "validation_reasoning": "VALIDATION ANALYSIS FOR '{record_name_lower}': [SEARCH EXECUTED] Performed comprehensive search (exact name, partial match, description similarity) in validation dataset for GDC '[gdc_name]'. [NO MATCH] No matching entries found in the validation dataset. [INTERPRETATION] This absence does NOT invalidate the mapping. Possible explanations: [new use case, emerging classification need, recent GDC addition, historical dataset limitations]. [ANALYTICAL FOUNDATION] This GDC was proposed based on [semantic score X from Stage 1 indicating Y-level similarity] and [contextual score Z from Stage 2 showing W-level alignment]. These provide [strong/substantial/solid] independent evidence. [CONFIDENCE ASSESSMENT] Lack of validation is NEUTRAL - confidence remains at [level based on semantic+context evidence] because the analytical evidence is [characterization]. Historical precedent would increase confidence further, but its absence doesn't decrease existing confidence. [NOVEL CLASSIFICATION] This represents [new application/emerging pattern/less documented use case] that [should be tracked/documented/monitored]. [RECOMMENDATION] Include this GDC in final mapping based on strong analytical evidence, flagged as novel classification for future reference."
    }}
  ],
  "overall_validation_reasoning": "COMPREHENSIVE VALIDATION SYNTHESIS FOR '{record_name_lower}': [VALIDATION COVERAGE] Validated [N] proposed GDC mappings. Results: [X] confirmed, [Y] not found, [Z] conflicted. [CONFIRMED MAPPINGS ANALYSIS] The [X] confirmed GDCs ([list]) have historical validation with ILM categories [list categories]. This historical support [significantly strengthens/strengthens] confidence because [synthesis: patterns observed, precedent value, consistency]. Confirmed mappings score: [aggregate confidence level]. [UNVALIDATED MAPPINGS ANALYSIS] The [Y] GDCs without validation ([list]) lack historical precedent but are supported by [semantic evidence summary] and [contextual evidence summary]. These represent [characterization: novel use cases, emerging patterns]. Unvalidated mappings score: [aggregate confidence based on analytical evidence]. [CONFLICTS IF ANY] [Discuss conflicts and resolution recommendations]. [VALIDATION PATTERNS] The validation results reveal [patterns: which types of GDCs are well-documented vs novel, coverage gaps, emerging classification needs]. [AGGREGATE CONFIDENCE] Taking all validation evidence: Confirmed mappings have [high/very high] confidence. Unvalidated mappings have [moderate-to-high/moderate] confidence based on analytical evidence. [Z conflicts require review]. [OVERALL ASSESSMENT] The validation exercise [strongly supports/supports/partially supports] the proposed mappings with [overall confidence characterization]. [FINAL RECOMMENDATION] [Specific recommendation based on validation results] because [synthesis of all validation findings and confidence assessment]."
}}

CRITICAL REQUIREMENTS:
✓ Validate EVERY proposed GDC systematically
✓ Use multiple search strategies (exact, partial, description)
✓ Distinguish "not_found" (neutral) from "conflicted" (negative)
✓ Comprehensive reasoning (150+ words minimum per result)
✓ Explain confidence impact clearly
✓ Synthesize overall validation patterns
✓ Output ONLY valid JSON (no markdown, no preamble)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in validation_expert: {e}")
        error_response = ValidationResponse(
            validation_results=[],
            overall_validation_reasoning=f"error in validation analysis: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    FINAL DECISION EXPERT - Stage 4 of Mixture of Experts Pipeline (Final Synthesis)
    
    EXPERT ROLE AND RESPONSIBILITY:
    You are the chief decision-making expert responsible for synthesizing all analytical evidence
    and rendering final, authoritative GDC mapping decisions. You integrate findings from semantic,
    contextual, and validation analyses to produce ranked, evidence-based classifications with
    comprehensive justification.
    
    CORE COMPETENCIES:
    1. Evidence Synthesis: Integrating multiple analytical dimensions into coherent decisions
    2. Conflict Resolution: Reconciling contradictions between different types of analysis
    3. Confidence Calibration: Accurately assessing certainty levels based on evidence quality
    4. Ranking and Prioritization: Ordering multiple mappings by relevance and confidence
    5. Comprehensive Documentation: Providing transparent, traceable decision reasoning
    
    DECISION-MAKING FRAMEWORK:
    
    STEP 1: EVIDENCE INTAKE AND ORGANIZATION
    - Receive comprehensive analyses from all previous experts
    - Organize evidence by GDC: semantic scores, contextual scores, validation status
    - Identify areas of convergence and divergence across analytical dimensions
    
    STEP 2: EVIDENCE WEIGHTING AND INTEGRATION
    
    Develop an integrated relevance score for each GDC based on:
    
    A. SEMANTIC SIMILARITY (Weight: 30%)
       - Direct from Stage 1 expert (0-100 scale)
       - High semantic scores (75+) indicate strong conceptual fit
       - Moderate scores (55-74) indicate plausible but less certain fit
    
    B. CONTEXTUAL ALIGNMENT (Weight: 40%)
       - Direct from Stage 2 expert (0-100 scale)  
       - Highest weight because operational context is most predictive
       - Validates whether semantic matches hold in real-world usage
    
    C. VALIDATION CONFIRMATION (Weight: 30%)
       - Convert validation status to numerical weight:
         * "confirmed" = 100 points (historical precedent exists)
         * "not_found" = 60 points (neutral - no evidence either way)
         * "conflicted" = 20 points (historical evidence contradicts)
    
    D. INTEGRATED RELEVANCE CALCULATION
       Integrated Score = (Semantic × 0.30) + (Contextual × 0.40) + (Validation × 0.30)
       
       Interpret:
       - 80-100: Very high confidence, primary mapping
       - 65-79: High confidence, strong secondary mapping  
       - 55-64: Moderate confidence, plausible mapping
       - Below 55: Low confidence, consider excluding unless special circumstances
    
    STEP 3: CONFLICT RESOLUTION
    
    When experts disagree:
    - Semantic says strong match, but contextual says weak: Trust contextual (higher weight)
    - Semantic and contextual agree, but validation conflicts: Investigate further but likely trust semantic+contextual
    - All three dimensions align: Very high confidence
    
    Resolution principles:
    - Operational reality (context) trumps semantic similarity
    - Historical data (validation) confirms but doesn't override strong analytical evidence
    - Multiple weak signals can collectively support a mapping
    
    STEP 4: RANKING METHODOLOGY
    
    Order GDCs by integrated relevance score:
    - Rank 1: Highest integrated score = primary mapping
    - Rank 2, 3, etc.: Additional mappings in decreasing order
    - Include all GDCs with integrated scores ≥ 55
    - Must include at least ONE GDC (highest score, even if below 55)
    
    STEP 5: COMPREHENSIVE REASONING SYNTHESIS
    
    For EACH final mapping, synthesize:
    - Integrated score and rank
    - Key evidence from all three analytical stages
    - How different analyses converge or complement each other
    - Confidence level and justification
    - Practical implications of this classification
    - Any caveats or considerations
    
    STEP 6: OVERALL RATIONALE DEVELOPMENT
    
    Explain the complete mapping decision:
    - Why this specific set of GDCs (single or multiple)
    - How the GDCs relate to each other (if multiple)
    - What the aggregate evidence tells us
    - Confidence in the overall classification scheme
    - Business/governance value of these mappings
    
    INPUT DATA:
    Record Class Information:
    {record_info}
    
    Comprehensive Analytical Evidence from All Experts:
    {all_analyses}
    
    OUTPUT REQUIREMENTS:
    Return ONLY a valid JSON object (no markdown, no explanatory text):
    {{
      "gdc_mappings": [
        {{
          "gdc_name": "primary gdc name",
          "gdc_description": "complete gdc definition",
          "mapping_rank": 1,
          "reasoning": "COMPREHENSIVE MAPPING DECISION: [INTEGRATED SCORE] This GDC achieves an integrated relevance score of [X.X]/100, calculated as: (Semantic [Y] × 0.30) + (Contextual [Z] × 0.40) + (Validation [W] × 0.30) = [X.X]. This is the PRIMARY/SECONDARY mapping (Rank [N]). [SEMANTIC EVIDENCE] Stage 1 semantic analysis scored this GDC at [Y]/100 because [key semantic findings]. The expert identified [specific linguistic/conceptual connections]. [CONTEXTUAL EVIDENCE] Stage 2 contextual analysis scored [Z]/100 because [key contextual findings]. PBT alignment ([score]) due to [reasoning]. Application relevance ([score]) due to [reasoning]. Process matching ([score]) due to [reasoning]. [VALIDATION EVIDENCE] Stage 3 validation [confirmed with ILM category X / found no historical precedent / identified conflict]. This [increases/maintains/decreases] confidence because [reasoning]. [EVIDENCE CONVERGENCE] All three analytical dimensions [strongly converge / generally align / show some variation]. The [convergence/divergence] pattern indicates [interpretation]. [CONFIDENCE ASSESSMENT] Confidence in this mapping is [VERY HIGH / HIGH / MODERATE] based on [integrated score interpretation, evidence quality, convergence strength]. [CLASSIFICATION IMPLICATIONS] This GDC mapping means the record will be categorized within [domain/functional area] for purposes of [governance/data management/compliance]. This classification enables [specific benefits or capabilities]. [CAVEATS IF ANY] [Note any limitations, special considerations, or review recommendations].",
          "evidence_summary": [
            "Semantic: Score [Y]/100 - [concise summary of key semantic match factors]",
            "Context: Score [Z]/100 - PBT: [key finding], Apps: [key finding], Processes: [key finding]",
            "Validation: [Status] - [key validation finding and confidence impact]"
          ]
        }}
      ],
      "overall_reasoning": "COMPREHENSIVE DECISION RATIONALE: [MAPPING STRUCTURE] This record class maps to [N] GDC(s) based on comprehensive multi-stage analysis. [IF SINGLE] A single GDC mapping is appropriate because [reasoning: focused scope, clear primary category, evidence concentrates on one GDC]. [IF MULTIPLE] Multiple GDC mappings are necessary because [reasoning: multi-dimensional nature, different aspects map to different categories, comprehensive coverage requires multiple classifications]. [EVIDENCE SYNTHESIS] The analytical evidence reveals [synthesis of findings across all stages]. Semantic analysis identified [key patterns]. Contextual analysis confirmed [key operational alignments]. Validation [provided historical support for X, found no precedent for Y]. [CONFIDENCE CALIBRATION] Overall confidence in this classification scheme is [VERY HIGH / HIGH / MODERATE / SUFFICIENT] based on [aggregate evidence quality, convergence patterns, validation coverage]. Specifically: [detailed confidence reasoning]. [GDC RELATIONSHIPS] [If multiple] The mapped GDCs relate as follows: [GDC1] addresses [aspect/dimension], [GDC2] covers [different aspect], demonstrating [how they complement each other]. [BUSINESS VALUE] These mappings provide [governance benefits, data management capabilities, compliance support, operational value]. They enable [specific organizational capabilities]. [DECISION QUALITY] The decision is [well-supported / supported / adequately supported] by [quality and quantity of evidence]. [RECOMMENDATIONS] [Any follow-up actions, review suggestions, or implementation considerations]."
    }}
    
    CRITICAL SUCCESS FACTORS:
    - Include ALL relevant GDCs (integrated score ≥ 55)
    - MUST include at least ONE GDC (use highest score if all below 55)
    - Calculate integrated scores using the weighting formula
    - Rank GDCs by integrated score
    - Provide substantial reasoning (250+ words per mapping)
    - Synthesize evidence from all three analytical stages
    - Include detailed evidence summary bullets
    - Explain confidence levels clearly
    - Provide thorough overall reasoning (200+ words)
    - Output ONLY valid JSON with no additional text
    """
    
    prompt = f"""You are the FINAL DECISION EXPERT synthesizing all evidence to make authoritative GDC mapping decisions.

CHAIN OF THOUGHT REASONING - FOLLOW THESE STEPS:

STEP 1: EVIDENCE ORGANIZATION
Parse all analytical evidence:

RECORD INFORMATION:
{record_info}

ALL ANALYSES (Semantic + Contextual + Validation):
{all_analyses}

Organize evidence by GDC: What did each expert say about each proposed GDC?

STEP 2: INTEGRATED SCORING
For each proposed GDC, calculate integrated relevance score:

Formula: (Semantic Score × 0.30) + (Contextual Score × 0.40) + (Validation Weight × 0.30)

Where:
- Semantic Score: From Stage 1 (0-100)
- Contextual Score: From Stage 2 (0-100)
- Validation Weight: confirmed=100, not_found=60, conflicted=20

STEP 3: CONFLICT RESOLUTION
If experts disagree:
- Trust contextual analysis (highest weight) for operational reality
- Validation confirms but doesn't override strong analytical evidence
- Resolve conflicts transparently with reasoning

STEP 4: RANKING AND FILTERING
- Rank all GDCs by integrated score (highest = Rank 1)
- Include all GDCs with integrated score ≥ 55
- MUST include at least ONE GDC (if all below 55, use highest score)

STEP 5: COMPREHENSIVE SYNTHESIS
For each final mapping:
- Present integrated score and calculation
- Synthesize evidence from all three stages
- Assess confidence level
- Explain practical implications

STEP 6: OVERALL RATIONALE
Explain complete mapping decision:
- Why this specific set of GDCs
- Evidence quality and convergence
- Confidence calibration
- Business value

OUTPUT FORMAT (VALID JSON ONLY - NO MARKDOWN, NO EXTRA TEXT):
{{
  "gdc_mappings": [
    {{
      "gdc_name": "exact gdc name",
      "gdc_description": "complete gdc definition",
      "mapping_rank": 1,
      "reasoning": "FINAL MAPPING DECISION: [INTEGRATED RELEVANCE SCORE] This GDC achieves an integrated score of [score]/100, calculated as: (Semantic [X] × 0.30) + (Contextual [Y] × 0.40) + (Validation [Z] × 0.30) = [score]. This positions it as [PRIMARY/SECONDARY/TERTIARY] mapping with Rank [N]. [STAGE 1: SEMANTIC ANALYSIS] The semantic similarity expert scored this at [X]/100 because [synthesis of semantic findings: name similarity, description alignment, domain relevance]. Key semantic evidence: [specific connections identified]. [STAGE 2: CONTEXTUAL ANALYSIS] The contextual expert scored this at [Y]/100 based on: PBT alignment ([score]/40) - [reasoning], Application relevance ([score]/30) - [reasoning], Process matching ([score]/30) - [reasoning]. Key contextual evidence: [specific PBTs/Apps/Processes that support this]. [STAGE 3: VALIDATION ANALYSIS] Validation status: [confirmed/not_found/conflicted]. Validation weight: [100/60/20]. Impact: [how validation affects confidence]. Key validation evidence: [specific findings]. [EVIDENCE INTEGRATION] Across all three analytical dimensions, the evidence [strongly converges / generally aligns / shows variation]. [Analysis of convergence pattern]. The [convergence/complementarity/conflict resolution] indicates [interpretation]. [CONFIDENCE ASSESSMENT] Confidence level: [VERY HIGH / HIGH / MODERATE / SUFFICIENT]. Justification: [Based on integrated score range, evidence quality, convergence strength, validation support]. [DECISION RATIONALE] This GDC mapping is [justified/recommended/proposed] because [synthesis of why all evidence points to this classification]. [PRACTICAL IMPLICATIONS] This classification means [what it enables, how it's used, governance value]. [SPECIAL CONSIDERATIONS] [Any caveats, review recommendations, or implementation notes].",
      "evidence_summary": [
        "Semantic: [X]/100 - [30-word summary of key semantic match factors and findings]",
        "Context: [Y]/100 - [30-word summary covering PBT, Application, and Process evidence]",
        "Validation: [Status] - [30-word summary of validation findings and confidence impact]"
      ]
    }}
  ],
  "overall_reasoning": "COMPREHENSIVE DECISION SYNTHESIS: [MAPPING OVERVIEW] This record class has been mapped to [N] GDC(s) through comprehensive four-stage analysis involving semantic similarity, contextual validation, historical precedent review, and final evidence integration. [SINGLE VS MULTIPLE] [If single]: A single GDC classification is optimal because [analysis concentrates on one clear category, focused scope, evidence converges on primary classification]. [If multiple]: Multiple GDC mappings are required because [record encompasses multiple dimensions, different aspects align with different categories, comprehensive governance needs multi-faceted classification]. [ANALYTICAL JOURNEY] Stage 1 (Semantic) identified [summary of semantic findings]. Stage 2 (Contextual) validated [summary of contextual findings]. Stage 3 (Validation) revealed [summary of validation findings]. Stage 4 (Integration) synthesized these into [integrated picture]. [EVIDENCE QUALITY] The evidence is [characterized: very strong, strong, solid, adequate] with [coverage assessment]. Specifically: [details about evidence depth, breadth, convergence]. [CONFIDENCE CALIBRATION] Overall confidence in this classification scheme is [VERY HIGH / HIGH / MODERATE / SUFFICIENT] because [detailed reasoning about aggregate evidence quality, scoring, validation coverage, expert agreement]. Integrated scores range from [low] to [high], indicating [interpretation]. [GDC INTERRELATIONSHIPS] [If multiple GDCs]: The [N] mapped GDCs relate as follows: [GDC 1] (Rank 1, score [X]) addresses [dimension/aspect]; [GDC 2] (Rank 2, score [Y]) covers [different dimension/aspect]. Together they provide [how they complement/comprehensive coverage]. [BUSINESS AND GOVERNANCE VALUE] These mappings deliver [specific organizational benefits: governance capabilities, data management support, compliance alignment, operational efficiency]. They enable [specific use cases and capabilities]. [DECISION QUALITY ASSESSMENT] This decision is [well-supported / supported / adequately supported] by [characterization of evidence quantity and quality]. [IMPLEMENTATION RECOMMENDATIONS] [Specific recommendations for using these mappings, any review suggestions, monitoring needs, or special handling requirements]."
}}

CRITICAL REQUIREMENTS:
✓ Calculate integrated scores using weighting formula
✓ Include ALL GDCs with integrated score ≥ 55  
✓ **MANDATORY**: MUST include at least ONE GDC (highest score, even if below 55)
✓ If all scores are below 55, include the highest-scoring GDC and explain the low confidence
✓ NEVER return an empty gdc_mappings list - this is a critical error
✓ Rank GDCs by integrated score
✓ Provide comprehensive reasoning (250+ words per mapping)
✓ Synthesize evidence from all three analytical stages
✓ Include detailed 3-part evidence summary for each GDC
✓ Provide thorough overall reasoning (200+ words)
✓ Output ONLY valid JSON (no markdown, no preamble, no postscript)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"  ⚠ Error in final_decision_expert: {e}")
        error_response = FinalMappingDecision(
            gdc_mappings=[],
            overall_reasoning=f"error in final decision synthesis: {str(e)}"
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent with dynamic chain of thought + mixture of experts"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an intelligent GDC mapping orchestrator using DYNAMIC CHAIN OF THOUGHT followed by MIXTURE OF EXPERTS.

=== UNDERSTANDING THE APPROACH ===

DYNAMIC CHAIN OF THOUGHT:
Before invoking experts, you reason through:
1. What does this record class represent?
2. What domains/functions might it relate to?
3. Which analytical approaches are most relevant?
4. What's the optimal expert invocation sequence?

MIXTURE OF EXPERTS:
Four specialized experts, invoked sequentially:
1. Semantic Similarity Expert - Identifies conceptually similar GDCs via RAG
2. Context Analysis Expert - Validates through operational/organizational context
3. Validation Expert - Cross-references against historical data
4. Final Decision Expert - Synthesizes all evidence into ranked mappings

=== CRITICAL PRINCIPLES ===

ONE-TO-MANY MAPPING:
- Record Classes FREQUENTLY map to MULTIPLE GDCs
- Different aspects → different GDCs
- Comprehensive governance requires multi-dimensional classification
- Don't force single mapping when evidence supports multiple

ALWAYS MAP TO AT LEAST ONE GDC:
- Every record MUST map to at least one GDC
- The final_decision_expert MUST return at least one mapping
- If all proposed GDCs have low confidence, include the best available option
- Never return zero mappings - explain low confidence in reasoning if needed

=== YOUR WORKFLOW ===

PHASE 1: DYNAMIC CHAIN OF THOUGHT
When you receive a record class:

Think through:
- "What does this record represent?" (comprehension)
- "What business functions might it support?" (contextualization)
- "What GDC categories might be relevant?" (hypothesis generation)
- "How should I sequence the expert consultation?" (planning)

Express this reasoning before invoking tools.

PHASE 2: MIXTURE OF EXPERTS INVOCATION
Execute in strict order:

1. SEMANTIC ANALYSIS
   invoke: semantic_similarity_expert(record_name, record_description)
   - Uses RAG with OpenAI embeddings for vector similarity
   - Identifies ALL semantically relevant GDCs (not just top match)
   - Returns matches with scores and reasoning

2. CONTEXTUAL VALIDATION
   invoke: context_analysis_expert(record_name, record_description, semantic_matches)
   - Uses RAG to retrieve PBT/Application/Process context
   - Validates semantic matches through operational reality
   - Keeps all matches with supporting or non-contradictory context

3. HISTORICAL VALIDATION
   invoke: validation_expert(record_name, proposed_gdcs, validation_data)
   - Cross-references against historical GDC-ILM mappings
   - Identifies confirmations, conflicts, or novel cases
   - Assesses how validation affects confidence

4. FINAL SYNTHESIS
   invoke: final_decision_expert(record_info, all_analyses)
   - Integrates evidence from all stages
   - Calculates weighted relevance scores
   - Produces ranked, justified final mappings
   - MUST return at least ONE GDC mapping

PHASE 3: ENSURE COMPLETENESS
After final_decision_expert:
- Verify at least one GDC mapping exists
- If zero mappings returned (error case), note this for fallback handling

=== EXECUTION NOTES ===
- All text is lowercase for consistency
- RAG uses OpenAI embeddings API directly
- Focus on evidence quality over speed
- Multiple GDCs are expected and appropriate
- Each expert builds on previous findings
- Comprehensive reasoning is valued"""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    validation_set: List[ValidationEntry]
) -> List[MappingResult]:
    """Process a single record through dynamic CoT + mixture of experts agent"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    query = f"""TASK: Map this Record Class to appropriate GDC(s) using Dynamic Chain of Thought + Mixture of Experts.

=== RECORD CLASS ===
GUID: {record.guid}
Code: {record.code}
Name: {record.name}
Description: {record.description}

=== VALIDATION DATASET ===
{validation_json}

=== MANDATORY WORKFLOW ===

You MUST complete ALL FOUR stages in order. Do not stop until all four tools have been called.

STAGE 1: SEMANTIC SIMILARITY ANALYSIS
Call: semantic_similarity_expert(record_name="{record.name}", record_desc="{record.description}")
Wait for results, then proceed to Stage 2.

STAGE 2: CONTEXT ANALYSIS
Call: context_analysis_expert(record_name="{record.name}", record_desc="{record.description}", semantic_matches="<results from stage 1>")
Wait for results, then proceed to Stage 3.

STAGE 3: VALIDATION ANALYSIS
Call: validation_expert(record_name="{record.name}", proposed_gdcs="<results from stage 2>", validation_data=<validation dataset>)
Wait for results, then proceed to Stage 4.

STAGE 4: FINAL DECISION (MANDATORY)
Call: final_decision_expert(record_info="GUID: {record.guid}, Code: {record.code}, Name: {record.name}, Description: {record.description}", all_analyses="<results from stages 1-3>")

After calling final_decision_expert, your task is COMPLETE. The final_decision_expert MUST return at least one GDC mapping.

=== CRITICAL ===
- You MUST call all 4 tools
- Do NOT stop after 1, 2, or 3 tools
- final_decision_expert is REQUIRED - this is where the actual mapping decision is made
- Do NOT try to make the final decision yourself - let the final_decision_expert do it"""
    
    try:
        # Invoke agent
        result = agent.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        messages = result.get("messages", [])
        print(f"\n📨 Agent returned {len(messages)} messages")
        
        # Extract tool results
        tool_results = extract_tool_results_from_messages(messages)
        
        # Check if final_decision_expert was called
        if tool_results['final_decision_expert'] is None:
            print("\n⚠️  CRITICAL: final_decision_expert was NOT called by agent")
            print("    Agent may have stopped prematurely. Calling final_decision_expert manually...")
            
            # Manually call final_decision_expert with accumulated results
            record_info = f"GUID: {record.guid}, Code: {record.code}, Name: {record.name}, Description: {record.description}"
            
            # Gather all analyses
            all_analyses = {
                "semantic_analysis": tool_results['semantic_similarity_expert'] or "Not available",
                "context_analysis": tool_results['context_analysis_expert'] or "Not available",
                "validation_analysis": tool_results['validation_expert'] or "Not available"
            }
            all_analyses_str = json.dumps(all_analyses, indent=2)
            
            # Call the tool directly
            from langchain_core.tools import ToolException
            try:
                final_result = final_decision_expert.invoke({
                    "record_info": record_info,
                    "all_analyses": all_analyses_str
                })
                tool_results['final_decision_expert'] = final_result
                print("    ✓ Manual call to final_decision_expert completed")
            except Exception as e:
                print(f"    ✗ Manual call failed: {e}")
                return [MappingResult(
                    guid=record.guid,
                    code=record.code,
                    name=record.name,
                    description=record.description,
                    gdc_name="FINAL_DECISION_NOT_CALLED",
                    gdc_description="agent did not invoke final decision expert",
                    mapping_rank=1,
                    reasoning=f"WORKFLOW INCOMPLETE: The agent stopped after calling {len([k for k,v in tool_results.items() if v is not None])} tools but did not call final_decision_expert. Manual invocation also failed: {str(e)}. REQUIRES MANUAL REVIEW."
                )]
        
        # Extract final decision from tool result
        final_decision_content = tool_results['final_decision_expert']
        
        if not final_decision_content:
            print("\n❌ final_decision_expert returned empty content")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="EMPTY_FINAL_DECISION",
                gdc_description="final decision expert returned no content",
                mapping_rank=1,
                reasoning="EMPTY RESPONSE: The final_decision_expert was called but returned empty content. REQUIRES MANUAL REVIEW."
            )]
        
        print(f"\n📄 Final decision content length: {len(final_decision_content)} chars")
        print(f"    Preview: {final_decision_content[:200]}...")
        
        # Try to parse the final decision
        try:
            json_str = extract_json_from_text(final_decision_content)
            print(f"\n🔧 Extracted JSON length: {len(json_str)} chars")
            
            decision_dict = json.loads(json_str)
            print(f"    ✓ Successfully parsed JSON")
            
            # Validate with Pydantic
            final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
            final_decision_data = final_decision_obj.model_dump()
            print(f"    ✓ Validated with Pydantic")
            
        except json.JSONDecodeError as e:
            print(f"\n❌ JSON parsing error: {e}")
            print(f"    Attempted to parse: {json_str[:500]}...")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="JSON_PARSE_ERROR",
                gdc_description="final decision output was not valid json",
                mapping_rank=1,
                reasoning=f"JSON PARSE ERROR: The final_decision_expert returned content but it could not be parsed as valid JSON. Error: {str(e)}. REQUIRES MANUAL REVIEW."
            )]
        except Exception as e:
            print(f"\n❌ Validation error: {e}")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="VALIDATION_ERROR",
                gdc_description="final decision output failed pydantic validation",
                mapping_rank=1,
                reasoning=f"VALIDATION ERROR: The JSON was parsed but failed Pydantic validation. Error: {str(e)}. REQUIRES MANUAL REVIEW."
            )]
        
        # Extract GDC mappings
        gdc_mappings = final_decision_data.get("gdc_mappings", [])
        overall_reasoning = final_decision_data.get("overall_reasoning", "")
        
        if not gdc_mappings:
            print("\n❌ No GDC mappings in final decision")
            return [MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="NO_MAPPINGS_IN_DECISION",
                gdc_description="final decision contained zero gdc mappings",
                mapping_rank=1,
                reasoning="NO MAPPINGS: The final_decision_expert completed but returned zero GDC mappings in the gdc_mappings list. This violates the requirement that at least one GDC must be mapped. REQUIRES MANUAL REVIEW."
            )]
        
        # Convert to MappingResult objects
        print(f"\n✅ Found {len(gdc_mappings)} GDC mapping(s)")
        mapping_results = []
        
        for mapping in gdc_mappings:
            gdc_name = mapping.get("gdc_name", "unknown")
            gdc_desc = mapping.get("gdc_description", "")
            rank = mapping.get("mapping_rank", 1)
            print(f"    Rank {rank}: {gdc_name}")
            
            mapping_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name=gdc_name,
                gdc_description=gdc_desc,
                mapping_rank=rank,
                reasoning=format_mapping_reasoning(mapping, overall_reasoning)
            ))
        
        return mapping_results
        
    except Exception as e:
        print(f"\n❌ Critical error processing record: {e}")
        import traceback
        traceback.print_exc()
        return [MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="CRITICAL_ERROR",
            gdc_description="unhandled exception during processing",
            mapping_rank=1,
            reasoning=f"CRITICAL ERROR: An unhandled exception occurred. Error: {str(e)}. See logs for traceback. REQUIRES MANUAL REVIEW."
        )]

def format_mapping_reasoning(mapping: Dict[str, Any], overall_reasoning: str) -> str:
    """Format reasoning for output"""
    parts = []
    
    reasoning = mapping.get("reasoning", "")
    if reasoning:
        parts.append(f"MAPPING REASONING:\n{reasoning}")
    
    evidence = mapping.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nEVIDENCE SUMMARY:\n" + "\n".join(f"• {e}" for e in evidence))
    
    if overall_reasoning and mapping.get("mapping_rank", 1) == 1:
        parts.append(f"\n\nOVERALL CONTEXT:\n{overall_reasoning}")
    
    return "\n".join(parts) if parts else "comprehensive analysis completed"

# ==================== EXCEL OUTPUT WITH GROUPING ====================

def save_results_to_grouped_excel(results: List[MappingResult], filename: str = "GDC_Mapping_Results.xlsx"):
    """Save results to Excel with grouping by Record Class"""
    print(f"\n💾 Saving results to Excel with grouping: {filename}")
    
    # Convert to DataFrame
    results_dict = [r.model_dump(by_alias=True) for r in results]
    df = pd.DataFrame(results_dict)
    
    # Sort by Name (Record Class) and then by Mapping Rank
    df = df.sort_values(by=['Name', 'Mapping Rank'])
    
    # Create Excel workbook
    wb = Workbook()
    ws = wb.active
    ws.title = "GDC Mappings"
    
    # Define styles
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)
    
    group_header_fill = PatternFill(start_color="D9E1F2", end_color="D9E1F2", fill_type="solid")
    group_header_font = Font(bold=True, size=10)
    
    error_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    error_font = Font(bold=True, color="9C0006", size=10)
    
    normal_font = Font(size=10)
    wrap_alignment = Alignment(wrap_text=True, vertical="top")
    center_alignment = Alignment(horizontal="center", vertical="center")
    
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    # Write headers
    headers = list(df.columns)
    for col_idx, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col_idx, value=header)
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = center_alignment
        cell.border = thin_border
    
    # Set column widths
    column_widths = {
        'GUID': 35,
        'Code': 15,
        'Name': 30,
        'Description': 40,
        'GDC Name': 25,
        'GDC Description': 35,
        'Mapping Rank': 12,
        'Reasoning': 60
    }
    
    for col_idx, header in enumerate(headers, 1):
        ws.column_dimensions[ws.cell(row=1, column=col_idx).column_letter].width = column_widths.get(header, 20)
    
    # Write data with grouping
    current_row = 2
    current_record_name = None
    group_start_row = 2
    
    for idx, row in df.iterrows():
        record_name = row['Name']
        
        # Check if we're starting a new group
        if record_name != current_record_name:
            # If not the first group, we can add visual separation or grouping
            if current_record_name is not None:
                # Add a subtle separator (optional)
                pass
            
            current_record_name = record_name
            group_start_row = current_row
        
        # Write row data
        for col_idx, header in enumerate(headers, 1):
            value = row[header]
            cell = ws.cell(row=current_row, column=col_idx, value=value)
            cell.font = normal_font
            cell.border = thin_border
            
            # Check if this is an error row
            is_error_row = row['GDC Name'] in ['EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR']
            
            # Special formatting for error rows
            if is_error_row:
                cell.fill = error_fill
                cell.font = error_font
            # Special formatting for certain columns
            elif header in ['Description', 'GDC Description', 'Reasoning']:
                cell.alignment = wrap_alignment
            elif header == 'Mapping Rank':
                cell.alignment = center_alignment
                
                # Highlight primary mappings (Rank 1)
                if value == 1:
                    cell.fill = PatternFill(start_color="E2EFDA", end_color="E2EFDA", fill_type="solid")
                    cell.font = Font(bold=True, size=10)
            else:
                cell.alignment = Alignment(vertical="top")
        
        current_row += 1
    
    # Freeze the header row
    ws.freeze_panes = "A2"
    
    # Auto-filter
    ws.auto_filter.ref = ws.dimensions
    
    # Add a note about error codes at the bottom
    note_row = ws.max_row + 2
    ws.cell(row=note_row, column=1, value="ERROR CODE EXPLANATIONS:").font = Font(bold=True, size=10)
    note_row += 1
    ws.cell(row=note_row, column=1, value="EXTRACTION_FAILED: Agent workflow completed but result extraction failed - check LLM output formatting")
    note_row += 1
    ws.cell(row=note_row, column=1, value="NO_MAPPINGS_RETURNED: Final decision expert returned zero mappings - review expert analyses")
    note_row += 1
    ws.cell(row=note_row, column=1, value="WORKFLOW_ERROR: Critical exception during agent workflow - check error logs")
    note_row += 1
    ws.cell(row=note_row, column=1, value="PROCESSING_ERROR: Error in main processing loop - check error logs")
    note_row += 1
    ws.cell(row=note_row, column=1, value="All error cases require manual review and classification.")
    
    # Add a summary sheet
    summary_ws = wb.create_sheet("Summary")
    summary_ws.append(["Metric", "Value"])
    summary_ws.append(["Total Record Classes", df['Name'].nunique()])
    summary_ws.append(["Total GDC Mappings", len(df)])
    summary_ws.append(["Average Mappings per Record", f"{len(df) / df['Name'].nunique():.2f}"])
    summary_ws.append(["Unique GDCs Used", df['GDC Name'].nunique()])
    
    # Count error cases
    error_count = len(df[df['GDC Name'].isin(['EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR'])])
    summary_ws.append([""])
    summary_ws.append(["Records Requiring Manual Review", error_count])
    if error_count > 0:
        summary_ws.append(["⚠️ Note", "Error rows are highlighted in red in main sheet"])
    
    # Format summary sheet
    for row in summary_ws.iter_rows(min_row=1, max_row=1, min_col=1, max_col=2):
        for cell in row:
            cell.fill = header_fill
            cell.font = header_font
            cell.border = thin_border
    
    for row in summary_ws.iter_rows(min_row=2, max_row=summary_ws.max_row, min_col=1, max_col=2):
        for cell in row:
            cell.border = thin_border
            cell.font = normal_font
    
    summary_ws.column_dimensions['A'].width = 30
    summary_ws.column_dimensions['B'].width = 20
    
    # Add a GDC frequency sheet
    gdc_freq_ws = wb.create_sheet("GDC Frequency")
    gdc_freq = df['GDC Name'].value_counts().reset_index()
    gdc_freq.columns = ['GDC Name', 'Frequency']
    
    for r_idx, row in enumerate(dataframe_to_rows(gdc_freq, index=False, header=True), 1):
        for c_idx, value in enumerate(row, 1):
            cell = gdc_freq_ws.cell(row=r_idx, column=c_idx, value=value)
            if r_idx == 1:
                cell.fill = header_fill
                cell.font = header_font
            else:
                cell.font = normal_font
            cell.border = thin_border
    
    gdc_freq_ws.column_dimensions['A'].width = 30
    gdc_freq_ws.column_dimensions['B'].width = 15
    
    # Save workbook
    wb.save(filename)
    print(f"✓ Excel file saved: {filename}")
    print(f"  📊 Sheets: GDC Mappings (main), Summary, GDC Frequency")
    print(f"  🎨 Features: Grouping by Record Class, Color coding, Auto-filter, Frozen headers")

# ==================== MAIN EXECUTION ====================

def test_openai_connection():
    """Test OpenAI API connection and embedding capability"""
    print("\n🔍 Testing OpenAI API connection...")
    
    if not OPENAI_API_KEY:
        print("❌ OPENAI_API_KEY is not set!")
        return False
    
    print(f"✓ API Key is set (length: {len(OPENAI_API_KEY)})")
    print(f"✓ Base URL: {OPENAI_BASE_URL}")
    print(f"✓ Embedding Model: {EMBEDDING_MODEL}")
    print(f"✓ Embedding Dimensions: {EMBEDDING_DIMENSIONS}")
    
    try:
        print("\n🧪 Testing embedding with a sample text...")
        test_response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=["test connection"],
            dimensions=EMBEDDING_DIMENSIONS
        )
        
        if test_response.data and len(test_response.data[0].embedding) == EMBEDDING_DIMENSIONS:
            print(f"✅ Successfully created test embedding!")
            print(f"✓ Embedding dimension: {len(test_response.data[0].embedding)}")
            return True
        else:
            print(f"❌ Test embedding failed - unexpected response format")
            return False
            
    except Exception as e:
        print(f"❌ Connection test failed!")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")
        
        # Check for common errors
        if "api_key" in str(e).lower() or "authentication" in str(e).lower():
            print("\n💡 Troubleshooting: API Key issue")
            print("   - Verify your API key is correct")
            print("   - Check if the key has necessary permissions")
            print("   - Ensure the key is not expired")
        elif "rate" in str(e).lower() or "limit" in str(e).lower():
            print("\n💡 Troubleshooting: Rate limit issue")
            print("   - Wait a few moments and try again")
            print("   - Check your OpenAI usage limits")
        elif "network" in str(e).lower() or "connection" in str(e).lower():
            print("\n💡 Troubleshooting: Network issue")
            print("   - Check your internet connection")
            print("   - Verify you can access api.openai.com")
            print("   - Check if you're behind a proxy/firewall")
        
        return False

def main():
    """Main execution function"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM")
    print("Dynamic Chain of Thought + Mixture of Experts")
    print("RAG with OpenAI Embeddings API | One-to-Many Mappings")
    print("Excel Output with Grouping")
    print("=" * 80)
    
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY environment variable not set")
        print("\nPlease set it using:")
        print("  export OPENAI_API_KEY='your-api-key'")
        print("\nOr in Python:")
        print("  os.environ['OPENAI_API_KEY'] = 'your-api-key'")
        return
    
    # Test OpenAI connection before proceeding
    if not test_openai_connection():
        print("\n❌ OpenAI connection test failed. Please fix the issues above before continuing.")
        return
    
    print("\n📁 Loading data...")
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded {len(gdc_master)} GDC master entries")
    print(f"✓ Loaded {len(gdc_context)} GDC context entries")
    print(f"✓ Loaded {len(validation_set)} validation entries")
    print(f"✓ Loaded {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load required data files")
        return
    
    print("\n🔍 Building RAG vector stores (using OpenAI API directly)...")
    gdc_master_vectorstore = build_gdc_master_vectorstore(gdc_master)
    gdc_context_vectorstore = build_gdc_context_vectorstore(gdc_context)
    
    print("\n🤖 Creating Dynamic CoT + Mixture of Experts Agent...")
    agent = create_react_agent_workflow()
    print(f"✓ Model: {OPENAI_MODEL}")
    print(f"✓ Reasoning: {REASONING_EFFORT}")
    print(f"✓ Embeddings: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)")
    print(f"✓ Approach: Dynamic Chain of Thought + Mixture of Experts")
    print(f"✓ Mode: One-to-many with RAG + Fallback guarantees")
    
    print("\n🚀 Starting mapping process...\n")
    all_results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\n{'='*80}")
        print(f"RECORD {i}/{len(record_classes)}: {record.name}")
        print(f"{'='*80}")
        
        try:
            record_mappings = process_single_record(
                agent=agent,
                record=record,
                validation_set=validation_set
            )
            
            all_results.extend(record_mappings)
            
            # Display results
            gdc_names = [m.gdc_name for m in record_mappings]
            print(f"\n✅ MAPPED TO {len(record_mappings)} GDC(s):")
            for mapping in record_mappings:
                print(f"   Rank {mapping.mapping_rank}: {mapping.gdc_name}")
            
        except Exception as e:
            print(f"\n❌ ERROR: {e}")
            print(f"   Marking record for manual review...")
            import traceback
            traceback.print_exc()
            
            # Create error result for manual review
            error_result = MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="PROCESSING_ERROR",
                gdc_description="error in main processing loop",
                mapping_rank=1,
                reasoning=f"PROCESSING ERROR: An exception occurred in the main processing loop. Error: {str(e)}. REQUIRES MANUAL REVIEW AND CLASSIFICATION."
            )
            all_results.append(error_result)
    
    print("\n" + "="*80)
    print("💾 SAVING RESULTS")
    print("="*80)
    
    # Save to Excel with grouping
    save_results_to_grouped_excel(all_results, "GDC_Mapping_Results.xlsx")
    
    # Also save to CSV as backup
    results_dict = [r.model_dump(by_alias=True) for r in all_results]
    df = pd.DataFrame(results_dict)
    df.to_csv("GDC_Mapping_Results.csv", index=False, encoding='utf-8-sig')
    print(f"✓ CSV backup saved: GDC_Mapping_Results.csv")
    
    print("\n📊 FINAL SUMMARY")
    print("="*80)
    print(f"Total Record Classes Processed: {len(record_classes)}")
    print(f"Total GDC Mappings Generated: {len(all_results)}")
    if len(record_classes) > 0:
        print(f"Average Mappings per Record: {len(all_results)/len(record_classes):.2f}")
    print(f"Unique GDCs Used: {len(set(r.gdc_name for r in all_results))}")
    
    # Check for error cases
    error_gdcs = [r for r in all_results if r.gdc_name in ['EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR']]
    if error_gdcs:
        print(f"\n⚠️  Records Requiring Manual Review: {len(error_gdcs)}")
        print("   These records encountered errors during automated mapping:")
        for err in error_gdcs:
            print(f"   - {err.name}: {err.gdc_name}")
    
    # Show mapping distribution
    from collections import Counter
    mapping_counts = Counter(r.gdc_name for r in all_results)
    print(f"\nTop 5 Most Common GDCs:")
    for gdc, count in mapping_counts.most_common(5):
        if gdc not in ['EXTRACTION_FAILED', 'NO_MAPPINGS_RETURNED', 'WORKFLOW_ERROR', 'PROCESSING_ERROR']:
            print(f"  - {gdc}: {count} mappings")
    
    print("="*80)
    print("✅ PROCESSING COMPLETE")
    print("="*80)

if __name__ == "__main__":
    main()
