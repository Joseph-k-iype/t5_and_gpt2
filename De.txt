"""
High-Performance Data Enhancement API based on ISO/IEC 11179-1:2023 Standards
Built with FastAPI, LangChain, LangGraph, and configurable AI models (o3-mini/phi-4)
"""

import asyncio
import csv
import logging
import uuid
import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from enum import Enum
import re

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import openai
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langgraph.prebuilt import create_react_agent
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict


# Configuration from environment variables
class Config:
    # API Configuration
    API_TITLE = os.getenv("API_TITLE", "ISO 11179 Data Enhancement API")
    API_VERSION = os.getenv("API_VERSION", "1.0.0")
    API_PORT = int(os.getenv("API_PORT", "8000"))
    API_HOST = os.getenv("API_HOST", "0.0.0.0")
    
    # Model Configuration
    MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "openai")  # openai or phi4
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "o3-mini-2025-01-31")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    
    # Phi-4 Configuration
    PHI4_API_KEY = os.getenv("PHI4_API_KEY", "")  # May be empty if not required
    PHI4_BASE_URL = os.getenv("PHI4_BASE_URL", "http://localhost:8001")  # Your custom endpoint
    PHI4_MODEL = os.getenv("PHI4_MODEL", "phi4")
    
    # Model Parameters
    MODEL_TEMPERATURE = float(os.getenv("MODEL_TEMPERATURE", "0.3"))
    MODEL_MAX_TOKENS = int(os.getenv("MODEL_MAX_TOKENS", "4000"))
    
    # Enhancement Configuration
    DEFAULT_MAX_ITERATIONS = int(os.getenv("DEFAULT_MAX_ITERATIONS", "3"))
    QUALITY_THRESHOLD = float(os.getenv("QUALITY_THRESHOLD", "0.6"))  # Threshold for good vs poor
    
    # File Paths
    ABBREVIATIONS_FILE = os.getenv("ABBREVIATIONS_FILE", "data/abbreviations.csv")
    
    # Feature Flags
    ENABLE_FEEDBACK = os.getenv("ENABLE_FEEDBACK", "true").lower() == "true"
    ENABLE_CACHING = os.getenv("ENABLE_CACHING", "true").lower() == "true"
    
    # Logging
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# Configure logging
logging.basicConfig(level=getattr(logging, Config.LOG_LEVEL))
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title=Config.API_TITLE,
    description="High-performance API for validating and enhancing data elements according to ISO/IEC 11179-1:2023 standards",
    version=Config.API_VERSION
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize AI model based on configuration
def get_llm():
    """Get the configured language model"""
    if Config.MODEL_PROVIDER.lower() == "openai":
        if not Config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required when using OpenAI provider")
        
        return ChatOpenAI(
            model=Config.OPENAI_MODEL,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL,
            temperature=Config.MODEL_TEMPERATURE,
            max_tokens=Config.MODEL_MAX_TOKENS,
            streaming=True
        )
    elif Config.MODEL_PROVIDER.lower() == "phi4":
        # Use ChatOpenAI with custom endpoint for Phi-4 (OpenAI-compatible API)
        return ChatOpenAI(
            model=Config.PHI4_MODEL,
            api_key=Config.PHI4_API_KEY or "dummy-key",  # Some endpoints don't require auth
            base_url=Config.PHI4_BASE_URL,
            temperature=Config.MODEL_TEMPERATURE,
            max_tokens=Config.MODEL_MAX_TOKENS,
            streaming=True
        )
    else:
        raise ValueError(f"Unsupported model provider: {Config.MODEL_PROVIDER}")

llm = get_llm()

# Global abbreviations dictionary
ABBREVIATIONS = {}

class QualityStatus(str, Enum):
    GOOD = "good"
    POOR = "poor"

class ProcessInfo(BaseModel):
    process_id: str
    process_name: str
    process_description: str

class DataElementInput(BaseModel):
    id: str
    existing_name: str
    existing_description: str
    example: Optional[str] = None
    processes: List[ProcessInfo]

class EnhancementRequest(BaseModel):
    data_element: DataElementInput
    max_iterations: int = Field(default=Config.DEFAULT_MAX_ITERATIONS, ge=1, le=10)
    cdm: Optional[str] = None
    include_feedback: bool = Field(default=Config.ENABLE_FEEDBACK)

class ValidationResponse(BaseModel):
    id: str
    name_valid: bool
    name_feedback: Optional[str] = None
    description_valid: bool
    description_feedback: Optional[str] = None
    quality_status: QualityStatus
    suggested_improvements: List[str]

class EnhancedData(BaseModel):
    id: str
    existing_name: str
    existing_description: str
    example: Optional[str]
    process: List[ProcessInfo]
    cdm: Optional[str]
    enhanced_name: str
    enhanced_description: str
    quality_status: QualityStatus
    enhancement_iterations: int
    validation_feedback: Optional[List[str]] = None
    enhancement_feedback: Optional[List[str]] = None
    confidence_score: float

class EnhancementResponse(BaseModel):
    request_id: str
    status: str
    enhanced_data: Optional[EnhancedData] = None
    error_message: Optional[str] = None

# State for LangGraph
class EnhancementState(TypedDict):
    messages: Annotated[List, add_messages]
    data_element: DataElementInput
    max_iterations: int
    current_iteration: int
    enhanced_name: str
    enhanced_description: str
    validation_feedback: List[str]
    enhancement_feedback: List[str]
    quality_status: QualityStatus
    confidence_score: float
    cdm: Optional[str]
    abbreviations: Dict[str, str]
    include_feedback: bool

async def load_abbreviations():
    """Load abbreviations from CSV file"""
    global ABBREVIATIONS
    try:
        if os.path.exists(Config.ABBREVIATIONS_FILE):
            with open(Config.ABBREVIATIONS_FILE, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                ABBREVIATIONS = {row['abbreviation']: row['full_form'] for row in reader}
                logger.info(f"Loaded {len(ABBREVIATIONS)} abbreviations from {Config.ABBREVIATIONS_FILE}")
        else:
            # Default abbreviations if file doesn't exist
            ABBREVIATIONS = {
                "ID": "Identifier", "DOC": "Document", "AUTH": "Authority",
                "MGMT": "Management", "CTRL": "Control", "SYS": "System",
                "INFO": "Information", "REQ": "Requirement", "PROC": "Process",
                "ORG": "Organization", "LOC": "Location", "DEPT": "Department",
                "ADDR": "Address", "NUM": "Number", "DATE": "Date",
                "TIME": "Time", "AMT": "Amount", "QTY": "Quantity",
                "PCT": "Percentage", "DESC": "Description", "NAME": "Name"
            }
            logger.info(f"Using default abbreviations ({len(ABBREVIATIONS)} entries)")
    except Exception as e:
        logger.error(f"Failed to load abbreviations: {e}")
        ABBREVIATIONS = {}

def expand_abbreviations(text: str, abbreviations: Dict[str, str]) -> str:
    """Expand abbreviations in text"""
    expanded_text = text
    for abbrev, full_form in abbreviations.items():
        # Use word boundaries to avoid partial matches
        pattern = r'\b' + re.escape(abbrev) + r'\b'
        expanded_text = re.sub(pattern, full_form, expanded_text, flags=re.IGNORECASE)
    return expanded_text

@tool
def iso_11179_validator(name: str, description: str) -> Dict[str, Any]:
    """
    Validates data element name and description against ISO/IEC 11179-1:2023 standards
    
    Args:
        name: Data element name to validate
        description: Data element description to validate
    
    Returns:
        Dictionary with validation results
    """
    validation_result = {
        "name_valid": True,
        "name_issues": [],
        "description_valid": True,
        "description_issues": [],
        "iso_compliance_score": 0.0
    }
    
    # Name validation according to ISO 11179 standards
    name_lower = name.lower()
    
    # Check for object and property pattern
    if not any(pattern in name_lower for pattern in ["of", "for", "by", "in", "from"]):
        validation_result["name_issues"].append("Name should follow 'property of object' pattern")
        validation_result["name_valid"] = False
    
    # Check for clarity and understandability
    if len(name.split()) < 2:
        validation_result["name_issues"].append("Name should be more descriptive with multiple meaningful words")
        validation_result["name_valid"] = False
    
    # Check for abbreviations
    abbrev_pattern = r'\b[A-Z]{2,}\b'
    if re.search(abbrev_pattern, name):
        validation_result["name_issues"].append("Consider expanding abbreviations for clarity")
    
    # Description validation
    if len(description.split()) < 5:
        validation_result["description_issues"].append("Description should be more comprehensive (minimum 5 words)")
        validation_result["description_valid"] = False
    
    # Check for ISO 11179 required elements in description
    description_lower = description.lower()
    
    if not any(element in description_lower for element in ["defines", "represents", "identifies", "indicates"]):
        validation_result["description_issues"].append("Description should clearly state what the data element represents")
        validation_result["description_valid"] = False
    
    # Calculate compliance score
    total_checks = 6
    passed_checks = 0
    
    if validation_result["name_valid"]:
        passed_checks += 2
    if validation_result["description_valid"]:
        passed_checks += 2
    if not validation_result["name_issues"]:
        passed_checks += 1
    if not validation_result["description_issues"]:
        passed_checks += 1
    
    validation_result["iso_compliance_score"] = passed_checks / total_checks
    
    return validation_result

@tool
def quality_assessor(name: str, description: str, context: str = "") -> Dict[str, Any]:
    """
    Assesses the overall quality of data element name and description
    
    Args:
        name: Data element name
        description: Data element description
        context: Additional context from processes or CDM
    
    Returns:
        Quality assessment results
    """
    quality_score = 0.0
    
    # Clarity assessment
    clarity_score = min(1.0, len(name.split()) / 5.0)  # Optimal around 3-5 words
    quality_score += clarity_score * 0.3
    
    # Completeness assessment
    completeness_score = min(1.0, len(description.split()) / 15.0)  # Optimal around 10-20 words
    quality_score += completeness_score * 0.3
    
    # Consistency assessment (basic check for terminology alignment)
    consistency_score = 0.8  # Default
    if context and any(term in description.lower() for term in context.lower().split()):
        consistency_score = 1.0
    quality_score += consistency_score * 0.2
    
    # Precision assessment
    precision_score = 0.7  # Default
    if "specific" in description.lower() or "particular" in description.lower():
        precision_score = 0.9
    quality_score += precision_score * 0.2
    
    # Determine quality status based on threshold
    status = QualityStatus.GOOD if quality_score >= Config.QUALITY_THRESHOLD else QualityStatus.POOR
    
    return {
        "quality_score": quality_score,
        "quality_status": status,
        "factors": {
            "clarity": clarity_score,
            "completeness": completeness_score,
            "consistency": consistency_score,
            "precision": precision_score
        }
    }

def create_validation_agent():
    """Create a LangGraph agent for data validation"""
    
    system_prompt = """
    You are an expert data governance specialist with deep knowledge of ISO/IEC 11179-1:2023 metadata registry standards.
    
    Your role is to validate data element names and descriptions according to these key ISO 11179 principles:
    
    1. **Data Element Naming**: Should follow the pattern of "property of object" or "object property"
       - Object: The entity being described (e.g., person, document, transaction)
       - Property: The characteristic being measured (e.g., name, date, amount, status)
    
    2. **Data Element Definition**: Must include:
       - **Purpose**: Why this data element exists
       - **Scope**: What it covers (optional but recommended)
       - **Source**: Where the data comes from (optional)
       - **Meaning**: What it represents in business terms
    
    3. **Clarity Requirements**:
       - Must be understandable by non-SMEs
       - Avoid technical jargon unless necessary
       - Expand abbreviations when possible
       - Use precise, unambiguous language
    
    4. **Quality Criteria**:
       - **Completeness**: All necessary information present
       - **Consistency**: Aligns with organizational standards
       - **Clarity**: Easy to understand
       - **Precision**: Specific and unambiguous
    
    Use the available tools to validate the data element and provide specific, actionable feedback.
    Always explain your reasoning and provide suggestions for improvement.
    """
    
    tools = [iso_11179_validator, quality_assessor]
    
    return create_react_agent(
        llm,
        tools,
        prompt=system_prompt
    )

def create_enhancement_agent():
    """Create a LangGraph agent for data enhancement"""
    
    system_prompt = """
    You are an expert data architect specializing in ISO/IEC 11179-1:2023 compliant data element enhancement.
    
    Your goal is to transform existing data element names and descriptions into high-quality, standards-compliant versions.
    
    **Enhancement Guidelines:**
    
    1. **Name Enhancement**:
       - Transform to "property of object" pattern
       - Example: "country_id_doc" â†’ "country of identity document issuing authority"
       - Expand abbreviations using the provided dictionary
       - Ensure clarity and business meaning
    
    2. **Description Enhancement**:
       - Structure: "A [property] that [purpose/meaning] [scope if applicable] [source if applicable]"
       - Example: "A country that identifies the jurisdiction where the identity document issuing authority is legally established and operates"
       - Include business context and purpose
       - Make it understandable to non-technical users
    
    3. **Context Integration**:
       - Use process information to understand business context
       - Incorporate CDM references if provided
       - Ensure alignment with organizational data architecture
    
    4. **Quality Assurance**:
       - Validate against ISO 11179 standards
       - Ensure business clarity
       - Check for completeness and precision
       - Maintain consistency with existing terminology
    
    Use the available tools to validate your enhancements and iterate until high quality is achieved.
    Always provide confidence scores and detailed feedback on changes made.
    """
    
    tools = [iso_11179_validator, quality_assessor]
    
    return create_react_agent(
        llm,
        tools,
        prompt=system_prompt
    )

# Create agents
validation_agent = create_validation_agent()
enhancement_agent = create_enhancement_agent()

def create_enhancement_workflow():
    """Create the enhancement workflow using LangGraph"""
    
    def validation_node(state: EnhancementState) -> EnhancementState:
        """Validate the current data element"""
        data_element = state["data_element"]
        
        # Expand abbreviations in name and description
        expanded_name = expand_abbreviations(data_element.existing_name, state["abbreviations"])
        expanded_description = expand_abbreviations(data_element.existing_description, state["abbreviations"])
        
        # Prepare context
        process_context = " ".join([p.process_description for p in data_element.processes])
        cdm_context = state.get("cdm", "")
        full_context = f"Process context: {process_context}\nCDM context: {cdm_context}"
        
        # Run validation
        validation_prompt = f"""
        Please validate this data element according to ISO/IEC 11179-1:2023 standards:
        
        Name: {expanded_name}
        Description: {expanded_description}
        Example: {data_element.example or 'Not provided'}
        Context: {full_context}
        
        Provide detailed feedback on:
        1. Name compliance with ISO 11179 naming conventions
        2. Description completeness and clarity
        3. Overall quality assessment
        4. Specific improvement suggestions
        """
        
        result = validation_agent.invoke({"messages": [HumanMessage(content=validation_prompt)]})
        
        # Extract validation feedback from the result
        if state["include_feedback"]:
            feedback_content = result["messages"][-1].content
            state["validation_feedback"].append(feedback_content)
        
        return state
    
    def enhancement_node(state: EnhancementState) -> EnhancementState:
        """Enhance the data element"""
        data_element = state["data_element"]
        
        # Prepare enhancement context
        process_context = " ".join([p.process_description for p in data_element.processes])
        cdm_context = state.get("cdm", "")
        previous_feedback = "\n".join(state["validation_feedback"]) if state["include_feedback"] else ""
        
        enhancement_prompt = f"""
        Please enhance this data element to be fully compliant with ISO/IEC 11179-1:2023:
        
        Current Name: {data_element.existing_name}
        Current Description: {data_element.existing_description}
        Example: {data_element.example or 'Not provided'}
        
        Business Context:
        {process_context}
        
        CDM Context: {cdm_context}
        
        {f"Previous Validation Feedback: {previous_feedback}" if previous_feedback else ""}
        
        Available abbreviations for expansion: {list(state["abbreviations"].keys())}
        
        Please provide:
        1. Enhanced name following "property of object" pattern
        2. Enhanced description with purpose, scope, and clear meaning
        3. Explanation of changes made
        4. Quality assessment of the enhanced version
        
        Ensure the result is understandable by non-SMEs while maintaining technical accuracy.
        """
        
        result = enhancement_agent.invoke({"messages": [HumanMessage(content=enhancement_prompt)]})
        
        # Extract enhanced content from the result
        enhancement_content = result["messages"][-1].content
        if state["include_feedback"]:
            state["enhancement_feedback"].append(enhancement_content)
        
        # Parse the enhanced name and description from the response
        lines = enhancement_content.split('\n')
        enhanced_name = ""
        enhanced_description = ""
        
        for line in lines:
            if "enhanced name:" in line.lower() or "name:" in line.lower():
                enhanced_name = line.split(':', 1)[1].strip() if ':' in line else enhanced_name
            elif "enhanced description:" in line.lower() or "description:" in line.lower():
                enhanced_description = line.split(':', 1)[1].strip() if ':' in line else enhanced_description
        
        # If parsing failed, provide defaults
        if not enhanced_name:
            enhanced_name = expand_abbreviations(data_element.existing_name, state["abbreviations"])
        if not enhanced_description:
            enhanced_description = expand_abbreviations(data_element.existing_description, state["abbreviations"])
        
        state["enhanced_name"] = enhanced_name
        state["enhanced_description"] = enhanced_description
        state["current_iteration"] += 1
        
        return state
    
    def quality_check_node(state: EnhancementState) -> EnhancementState:
        """Check if quality meets requirements"""
        # Use quality assessor tool
        quality_result = quality_assessor.invoke({
            "name": state["enhanced_name"],
            "description": state["enhanced_description"],
            "context": " ".join([p.process_description for p in state["data_element"].processes])
        })
        
        state["quality_status"] = quality_result["quality_status"]
        state["confidence_score"] = quality_result["quality_score"]
        
        return state
    
    def should_continue(state: EnhancementState) -> str:
        """Determine if we should continue enhancement"""
        if state["current_iteration"] >= state["max_iterations"]:
            return "end"
        if state["quality_status"] == QualityStatus.GOOD:
            return "end"
        return "enhance"
    
    # Build the workflow graph
    workflow = StateGraph(EnhancementState)
    
    # Add nodes
    workflow.add_node("validate", validation_node)
    workflow.add_node("enhance", enhancement_node)
    workflow.add_node("quality_check", quality_check_node)
    
    # Add edges
    workflow.add_edge(START, "validate")
    workflow.add_edge("validate", "enhance")
    workflow.add_edge("enhance", "quality_check")
    workflow.add_conditional_edges(
        "quality_check",
        should_continue,
        {
            "enhance": "enhance",
            "end": END
        }
    )
    
    return workflow.compile()

# Create the enhancement workflow
enhancement_workflow = create_enhancement_workflow()

@app.on_event("startup")
async def startup_event():
    """Initialize the application"""
    await load_abbreviations()
    current_model = Config.OPENAI_MODEL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_MODEL
    current_url = Config.OPENAI_BASE_URL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_BASE_URL
    logger.info(f"Data Enhancement API started successfully using {Config.MODEL_PROVIDER} model: {current_model} at {current_url}")

@app.post("/validate", response_model=ValidationResponse)
async def validate_data_element(request: EnhancementRequest):
    """
    Validate a data element against ISO/IEC 11179-1:2023 standards
    """
    try:
        data_element = request.data_element
        
        # Expand abbreviations
        expanded_name = expand_abbreviations(data_element.existing_name, ABBREVIATIONS)
        expanded_description = expand_abbreviations(data_element.existing_description, ABBREVIATIONS)
        
        # Prepare context
        process_context = " ".join([p.process_description for p in data_element.processes])
        
        # Validate using ISO 11179 validator
        validation_result = iso_11179_validator.invoke({
            "name": expanded_name,
            "description": expanded_description
        })
        
        # Assess quality
        quality_result = quality_assessor.invoke({
            "name": expanded_name,
            "description": expanded_description,
            "context": process_context
        })
        
        # Generate feedback
        name_feedback = None
        description_feedback = None
        
        if request.include_feedback:
            name_feedback = "Name is compliant with ISO 11179 standards" if validation_result["name_valid"] else "; ".join(validation_result["name_issues"])
            description_feedback = "Description meets quality requirements" if validation_result["description_valid"] else "; ".join(validation_result["description_issues"])
        
        # Generate improvement suggestions
        suggestions = []
        if validation_result["name_issues"]:
            suggestions.extend([f"Name: {issue}" for issue in validation_result["name_issues"]])
        if validation_result["description_issues"]:
            suggestions.extend([f"Description: {issue}" for issue in validation_result["description_issues"]])
        
        if quality_result["quality_score"] < Config.QUALITY_THRESHOLD:
            suggestions.append("Consider providing more business context and examples")
        
        return ValidationResponse(
            id=data_element.id,
            name_valid=validation_result["name_valid"],
            name_feedback=name_feedback,
            description_valid=validation_result["description_valid"],
            description_feedback=description_feedback,
            quality_status=quality_result["quality_status"],
            suggested_improvements=suggestions
        )
        
    except Exception as e:
        logger.error(f"Validation error: {e}")
        raise HTTPException(status_code=500, detail=f"Validation failed: {str(e)}")

@app.post("/enhance", response_model=EnhancementResponse)
async def enhance_data_element(request: EnhancementRequest):
    """
    Enhance a data element to comply with ISO/IEC 11179-1:2023 standards
    """
    try:
        request_id = str(uuid.uuid4())
        
        # Initialize state for the workflow
        initial_state = EnhancementState(
            messages=[],
            data_element=request.data_element,
            max_iterations=request.max_iterations,
            current_iteration=0,
            enhanced_name="",
            enhanced_description="",
            validation_feedback=[],
            enhancement_feedback=[],
            quality_status=QualityStatus.POOR,
            confidence_score=0.0,
            cdm=request.cdm,
            abbreviations=ABBREVIATIONS,
            include_feedback=request.include_feedback
        )
        
        # Run the enhancement workflow
        final_state = enhancement_workflow.invoke(initial_state)
        
        # Prepare enhanced data response
        enhanced_data = EnhancedData(
            id=request.data_element.id,
            existing_name=request.data_element.existing_name,
            existing_description=request.data_element.existing_description,
            example=request.data_element.example,
            process=request.data_element.processes,
            cdm=request.cdm,
            enhanced_name=final_state["enhanced_name"],
            enhanced_description=final_state["enhanced_description"],
            quality_status=final_state["quality_status"],
            enhancement_iterations=final_state["current_iteration"],
            validation_feedback=final_state["validation_feedback"] if request.include_feedback else None,
            enhancement_feedback=final_state["enhancement_feedback"] if request.include_feedback else None,
            confidence_score=final_state["confidence_score"]
        )
        
        return EnhancementResponse(
            request_id=request_id,
            status="completed",
            enhanced_data=enhanced_data,
            error_message=None
        )
        
    except Exception as e:
        logger.error(f"Enhancement error: {e}")
        return EnhancementResponse(
            request_id=str(uuid.uuid4()),
            status="failed",
            enhanced_data=None,
            error_message=str(e)
        )

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    current_model = Config.OPENAI_MODEL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_MODEL
    current_url = Config.OPENAI_BASE_URL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_BASE_URL
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": Config.API_VERSION,
        "model_provider": Config.MODEL_PROVIDER,
        "model": current_model,
        "endpoint": current_url,
        "abbreviations_loaded": len(ABBREVIATIONS),
        "quality_threshold": Config.QUALITY_THRESHOLD
    }

@app.get("/config")
async def get_config():
    """Get current configuration"""
    current_model = Config.OPENAI_MODEL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_MODEL
    current_url = Config.OPENAI_BASE_URL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_BASE_URL
    
    return {
        "model_provider": Config.MODEL_PROVIDER,
        "model": current_model,
        "endpoint": current_url,
        "temperature": Config.MODEL_TEMPERATURE,
        "max_tokens": Config.MODEL_MAX_TOKENS,
        "quality_threshold": Config.QUALITY_THRESHOLD,
        "default_max_iterations": Config.DEFAULT_MAX_ITERATIONS,
        "feedback_enabled": Config.ENABLE_FEEDBACK,
        "abbreviations_count": len(ABBREVIATIONS)
    }

@app.get("/abbreviations")
async def get_abbreviations():
    """Get loaded abbreviations"""
    return {"abbreviations": ABBREVIATIONS}

@app.post("/test-connection")
async def test_model_connection():
    """Test connection to the configured model"""
    try:
        # Simple test message
        test_message = "Hello, please respond with 'Connection successful'"
        result = llm.invoke([HumanMessage(content=test_message)])
        
        return {
            "status": "success",
            "model_provider": Config.MODEL_PROVIDER,
            "model": Config.OPENAI_MODEL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_MODEL,
            "endpoint": Config.OPENAI_BASE_URL if Config.MODEL_PROVIDER == "openai" else Config.PHI4_BASE_URL,
            "response_preview": result.content[:100] + "..." if len(result.content) > 100 else result.content
        }
    except Exception as e:
        return {
            "status": "error",
            "model_provider": Config.MODEL_PROVIDER,
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host=Config.API_HOST, port=Config.API_PORT)
