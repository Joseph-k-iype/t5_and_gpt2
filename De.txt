import json
import csv
import os

def create_flat_csvs(data: dict, output_dir: str):
    """
    Converts the unified legal JSON into five separate, fully flattened CSV files.

    Args:
        data (dict): The dictionary loaded from the JSON file.
        output_dir (str): The path to the folder for output files.
    """
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # --- 1. Process 'unified_simplified_rules' ---
    simplified_rules = data.get("unified_simplified_rules", [])
    if simplified_rules:
        output_path = os.path.join(output_dir, 'simplified_rules.csv')
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            fieldnames = list(simplified_rules[0].keys())
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for rule in simplified_rules:
                if isinstance(rule.get('conditions'), list):
                    rule['conditions'] = "; ".join(rule['conditions'])
                if isinstance(rule.get('key_phrases'), list):
                    rule['key_phrases'] = "; ".join(map(str, rule['key_phrases']))
                writer.writerow(rule)
        print(f"‚úÖ Successfully created {output_path}")

    # --- Process 'unified_decision_tables' ---
    decision_tables = data.get("unified_decision_tables", {}).get("decision_tables", [])
    if not decision_tables:
        print("‚ö†Ô∏è No decision tables found to process.")
        return
        
    # Prepare lists to hold rows for the four related CSV files
    tables_metadata_rows, rules_rows, references_rows, condition_defs_rows = [], [], [], []
    
    # Pre-scan to find all possible condition keys for the rules table header
    all_rule_condition_keys = set()
    for table in decision_tables:
        for rule in table.get("rules", []):
            all_rule_condition_keys.update(rule.get("conditions", {}).keys())
    
    # Iterate through the data to populate the lists
    for table in decision_tables:
        table_id = table.get("table_id")
        
        # 2. Prepare data for decision_tables.csv (metadata only)
        tables_metadata_rows.append({
            "table_id": table_id,
            "name": table.get("name"),
            "description": table.get("description"),
        })
        
        # 3. Prepare data for the new decision_table_condition_definitions.csv
        for cond_name, cond_values in table.get("conditions", {}).items():
            condition_defs_rows.append({
                "table_id": table_id,
                "condition_name": cond_name,
                "possible_values": "; ".join(map(str, cond_values))
            })

        # Process each rule within the table
        for rule in table.get("rules", []):
            rule_id = rule.get("rule_id")
            
            # 4. Prepare data for decision_table_rules.csv (with flattened conditions)
            rule_row = {
                "table_id": table_id,
                "rule_id": rule_id,
                "priority": rule.get("priority"),
                "source_rule": rule.get("source_rule"),
                "actions": "; ".join(rule.get("actions", []))
            }
            # Add each condition as its own prefixed column
            for key, value in rule.get("conditions", {}).items():
                rule_row[f"condition_{key}"] = value
            rules_rows.append(rule_row)

            # 5. Prepare data for decision_table_references.csv
            for reference in rule.get("references", []):
                reference['table_id'] = table_id
                reference['rule_id'] = rule_id
                references_rows.append(reference)

    # --- Write all CSV files ---
    
    # 2b. Write decision_tables.csv
    if tables_metadata_rows:
        output_path = os.path.join(output_dir, 'decision_tables.csv')
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=tables_metadata_rows[0].keys())
            writer.writeheader()
            writer.writerows(tables_metadata_rows)
        print(f"‚úÖ Successfully created {output_path}")

    # 3b. Write decision_table_condition_definitions.csv
    if condition_defs_rows:
        output_path = os.path.join(output_dir, 'decision_table_condition_definitions.csv')
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=condition_defs_rows[0].keys())
            writer.writeheader()
            writer.writerows(condition_defs_rows)
        print(f"‚úÖ Successfully created {output_path}")

    # 4b. Write decision_table_rules.csv
    if rules_rows:
        output_path = os.path.join(output_dir, 'decision_table_rules.csv')
        # Define the full, dynamic header for the rules table
        base_headers = ["table_id", "rule_id", "priority", "source_rule", "actions"]
        condition_headers = sorted([f"condition_{key}" for key in all_rule_condition_keys])
        full_headers = base_headers + condition_headers
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            # Use extrasaction='ignore' because each row only has a subset of all possible condition columns
            writer = csv.DictWriter(f, fieldnames=full_headers, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(rules_rows)
        print(f"‚úÖ Successfully created {output_path}")

    # 5b. Write decision_table_references.csv
    if references_rows:
        output_path = os.path.join(output_dir, 'decision_table_references.csv')
        all_headers = set().union(*(d.keys() for d in references_rows))
        fieldnames = ['table_id', 'rule_id'] + sorted([h for h in all_headers if h not in ['table_id', 'rule_id']])
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(references_rows)
        print(f"‚úÖ Successfully created {output_path}")

# --- Main Execution ---
if __name__ == "__main__":
    io_directory = 'output'
    input_filename = os.path.join(io_directory, 'input.json')

    try:
        with open(input_filename, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        create_flat_csvs(json_data, io_directory)

    except FileNotFoundError:
        print(f"‚ùå Error: The file '{input_filename}' was not found.")
        print(f"üëâ Please place your JSON file inside the '{io_directory}' folder and name it 'input.json'.")
    except json.JSONDecodeError:
        print(f"‚ùå Error: The file '{input_filename}' is not a valid JSON file.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
