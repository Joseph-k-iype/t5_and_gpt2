"""
Legal Document Processing System
Converts legislation and laws into machine-readable rules using multi-agent orchestration.

Key Features:
- PDF document ingestion and legal rule extraction
- SKOS ontology generation (JSON-LD, TTL, XML)
- Multi-agent system with ReAct agents using LangGraph
- Chain of experts approach with specialist agents
- LangMem for long-term memory
- Dynamic domain classification using LLM capabilities
- Directed acyclic graph support for querying
"""

import os
import json
import asyncio
import logging
import re
import hashlib
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

# Core dependencies
import PyPDF2
import numpy as np
from rdflib import Graph, Namespace, URIRef, Literal
from rdflib.namespace import RDF, RDFS, SKOS, XSD, OWL
from openai import OpenAI

# LangChain/LangGraph dependencies
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver

# LangMem dependencies
from langmem import create_manage_memory_tool, create_search_memory_tool, create_memory_manager

# Data processing
import spacy
import networkx as nx

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LegalDocumentProcessor:
    """Main orchestrator for legal document processing system."""
    
    def __init__(self, config_path: str, openai_base_url: str = None):
        """
        Initialize the legal document processor.
        
        Args:
            config_path: Path to JSON configuration file
            openai_base_url: Base URL for OpenAI API connection
        """
        self.config = self._load_config(config_path)
        self.openai_base_url = openai_base_url or "https://api.openai.com/v1"
        
        # Initialize OpenAI client
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=self.openai_base_url
        )
        
        # Initialize models
        self.llm = ChatOpenAI(
            model="o3-mini-2025-01-31",
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=self.openai_base_url,
            model_kwargs={"reasoning_effort": "high"}
        )
        
        # Initialize local tiktoken encoding
        self.encoding = self._load_local_tiktoken_encoding()
        
        # Initialize NLP models
        self.nlp = spacy.load("en_core_web_sm")
        
        # Initialize memory store
        self.memory_store = InMemoryStore()
        
        # Initialize memory manager
        self.memory_manager = create_memory_manager(
            "o3-mini-2025-01-31",
            instructions="Extract legal rules, obligations, permissions, prohibitions, and domain classifications for data management compliance."
        )
        
        # Initialize knowledge graph
        self.knowledge_graph = nx.DiGraph()
        
        # Initialize RDF graph for SKOS ontology
        self.rdf_graph = Graph()
        self._setup_namespaces()
        
        # Initialize multi-agent system
        self._setup_agents()
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from JSON file."""
        with open(config_path, 'r') as f:
            return json.load(f)
    
    def _load_local_tiktoken_encoding(self):
        """Load tiktoken encoding from local models folder."""
        try:
            import tiktoken
            
            # Try to load from local folder first
            tiktoken_models_path = Path("./tiktoken_models")
            if tiktoken_models_path.exists():
                # Set the tiktoken cache directory to our local folder
                os.environ["TIKTOKEN_CACHE_DIR"] = str(tiktoken_models_path)
                logger.info(f"Using local tiktoken models from: {tiktoken_models_path}")
            
            # Load encoding for GPT-4 (which is compatible with o3-mini)
            encoding = tiktoken.encoding_for_model("gpt-4")
            logger.info("Successfully loaded tiktoken encoding")
            return encoding
            
        except Exception as e:
            logger.warning(f"Failed to load tiktoken encoding: {e}")
            # Fallback: create a simple token counter
            return self._create_fallback_tokenizer()
    
    def _create_fallback_tokenizer(self):
        """Create a fallback tokenizer if tiktoken fails."""
        class FallbackTokenizer:
            def encode(self, text: str) -> list:
                # Approximate token count (rough estimate: 1 token â‰ˆ 4 characters)
                return list(range(len(text) // 4))
        
        logger.warning("Using fallback tokenizer - token counts will be approximate")
        return FallbackTokenizer()
    
    def _setup_namespaces(self):
        """Setup RDF namespaces for SKOS ontology."""
        self.LEG = Namespace("http://example.org/legal-ontology#")
        self.DATA_MGT = Namespace("http://example.org/data-management#")
        
        # Bind namespaces
        self.rdf_graph.bind("leg", self.LEG)
        self.rdf_graph.bind("data_mgt", self.DATA_MGT)
        self.rdf_graph.bind("skos", SKOS)
        self.rdf_graph.bind("rdfs", RDFS)
        self.rdf_graph.bind("owl", OWL)
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI client directly."""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-large",
                input=texts,
                dimensions=3072
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return []
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF document with improved handling for large files."""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_parts = []
                
                total_pages = len(pdf_reader.pages)
                logger.info(f"Extracting text from {total_pages} pages")
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():  # Only add non-empty pages
                        # Clean up text
                        page_text = self._clean_extracted_text(page_text)
                        text_parts.append(f"[PAGE {page_num + 1}]\n{page_text}")
                    
                    if (page_num + 1) % 10 == 0:
                        logger.info(f"Processed {page_num + 1}/{total_pages} pages")
                
                full_text = "\n\n".join(text_parts)
                logger.info(f"Extracted {len(full_text)} characters from PDF")
                return full_text
                
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    
    def _clean_extracted_text(self, text: str) -> str:
        """Clean extracted PDF text for better processing."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Fix common PDF extraction issues
        text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)  # Fix hyphenated words across lines
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Normalize paragraph breaks
        
        # Remove page headers/footers patterns
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'\d{1,3}\s*$', '', text, flags=re.MULTILINE)  # Remove page numbers at end of line
        
        return text.strip()
    
    def chunk_document_semantically(self, text: str, max_tokens: int = 6000, overlap_tokens: int = 200) -> List[Dict[str, Any]]:
        """
        Chunk document using semantic boundaries (sections, articles, paragraphs).
        
        Args:
            text: Full document text
            max_tokens: Maximum tokens per chunk
            overlap_tokens: Overlap between chunks for context preservation
            
        Returns:
            List of chunk dictionaries with metadata
        """
        # Parse document with spaCy
        doc = self.nlp(text)
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_metadata = {
            "section": None,
            "start_char": 0,
            "end_char": 0,
            "chunk_id": 0
        }
        
        # Legal document section patterns
        section_patterns = [
            r'(?i)^(SECTION|ARTICLE|CHAPTER|PART|CLAUSE|PARAGRAPH)\s+\d+',
            r'(?i)^(\d+\.)+\s',  # Numbered sections like 1.1, 1.2.3
            r'(?i)^[A-Z][A-Z\s]{10,}$',  # ALL CAPS headers
        ]
        
        sentences = list(doc.sents)
        overlap_text = ""
        
        for i, sent in enumerate(sentences):
            sent_text = sent.text.strip()
            sent_tokens = len(self.encoding.encode(sent_text))
            
            # Check if this is a section header
            is_section_header = any(
                re.match(pattern, sent_text) 
                for pattern in section_patterns
            )
            
            # If we hit a section header and have content, finalize current chunk
            if is_section_header and current_chunk:
                self._finalize_chunk(chunks, current_chunk, chunk_metadata, overlap_text)
                
                # Start new chunk with overlap
                current_chunk = overlap_text
                current_tokens = len(self.encoding.encode(current_chunk))
                chunk_metadata = {
                    "section": sent_text,
                    "start_char": sent.start_char,
                    "end_char": sent.end_char,
                    "chunk_id": len(chunks)
                }
            
            # Check if adding this sentence would exceed token limit
            if current_tokens + sent_tokens > max_tokens and current_chunk:
                self._finalize_chunk(chunks, current_chunk, chunk_metadata, overlap_text)
                
                # Start new chunk with overlap
                current_chunk = overlap_text + " " + sent_text
                current_tokens = len(self.encoding.encode(current_chunk))
                chunk_metadata = {
                    "section": chunk_metadata.get("section"),
                    "start_char": sent.start_char,
                    "end_char": sent.end_char,
                    "chunk_id": len(chunks)
                }
            else:
                # Add sentence to current chunk
                if current_chunk:
                    current_chunk += " " + sent_text
                else:
                    current_chunk = sent_text
                    chunk_metadata["start_char"] = sent.start_char
                
                current_tokens += sent_tokens
                chunk_metadata["end_char"] = sent.end_char
            
            # Prepare overlap text (last few sentences)
            if i >= len(sentences) - 3:  # Last 3 sentences for overlap
                overlap_sentences = sentences[max(0, i-2):i+1]
                overlap_text = " ".join([s.text.strip() for s in overlap_sentences])
                overlap_token_count = len(self.encoding.encode(overlap_text))
                if overlap_token_count > overlap_tokens:
                    overlap_text = " ".join([s.text.strip() for s in overlap_sentences[-2:]])
        
        # Finalize last chunk
        if current_chunk:
            self._finalize_chunk(chunks, current_chunk, chunk_metadata, "")
        
        logger.info(f"Document chunked into {len(chunks)} semantic chunks")
        return chunks
    
    def _finalize_chunk(self, chunks: List[Dict[str, Any]], chunk_text: str, 
                       metadata: Dict[str, Any], overlap_text: str):
        """Finalize and add chunk to chunks list."""
        chunk_data = {
            "text": chunk_text.strip(),
            "tokens": len(self.encoding.encode(chunk_text)),
            "chunk_id": metadata["chunk_id"],
            "section": metadata.get("section"),
            "start_char": metadata["start_char"],
            "end_char": metadata["end_char"],
            "overlap": overlap_text.strip() if overlap_text else None
        }
        chunks.append(chunk_data)
    
    def create_chunk_embeddings(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Create embeddings for document chunks.
        
        Args:
            chunks: List of chunk dictionaries
            
        Returns:
            Chunks with embeddings added
        """
        # Extract texts for embedding
        chunk_texts = [chunk["text"] for chunk in chunks]
        
        # Generate embeddings in batches to handle large documents
        batch_size = 100  # OpenAI API limit consideration
        all_embeddings = []
        
        for i in range(0, len(chunk_texts), batch_size):
            batch_texts = chunk_texts[i:i + batch_size]
            batch_embeddings = self.get_embeddings(batch_texts)
            all_embeddings.extend(batch_embeddings)
            
            logger.info(f"Generated embeddings for batch {i//batch_size + 1}/{(len(chunk_texts)-1)//batch_size + 1}")
        
        # Add embeddings to chunks
        for i, chunk in enumerate(chunks):
            if i < len(all_embeddings):
                chunk["embedding"] = all_embeddings[i]
                chunk["embedding_model"] = "text-embedding-3-large"
                chunk["embedding_dimensions"] = 3072
        
        return chunks
    
    def find_similar_chunks(self, query_text: str, chunks: List[Dict[str, Any]], 
                           top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Find chunks most similar to query using embeddings.
        
        Args:
            query_text: Query text
            chunks: List of chunks with embeddings
            top_k: Number of top similar chunks to return
            
        Returns:
            Top similar chunks with similarity scores
        """
        # Get query embedding
        query_embeddings = self.get_embeddings([query_text])
        if not query_embeddings:
            return []
        
        query_embedding = query_embeddings[0]
        
        # Calculate similarities
        similarities = []
        for chunk in chunks:
            if "embedding" in chunk:
                # Cosine similarity
                chunk_embedding = np.array(chunk["embedding"])
                query_vec = np.array(query_embedding)
                
                similarity = np.dot(chunk_embedding, query_vec) / (
                    np.linalg.norm(chunk_embedding) * np.linalg.norm(query_vec)
                )
                
                similarities.append({
                    **chunk,
                    "similarity_score": float(similarity)
                })
        
        # Sort by similarity and return top_k
        similarities.sort(key=lambda x: x["similarity_score"], reverse=True)
        return similarities[:top_k]
    
    def _setup_agents(self):
        """Setup the multi-agent system using LangGraph."""
        
        # Define agent state
        class LegalAgentState(MessagesState):
            document_id: str
            jurisdiction: str
            organization: str
            extracted_rules: List[Dict[str, Any]]
            domain_classifications: Dict[str, List[str]]
            ontology_concepts: List[Dict[str, Any]]
            processing_stage: str
            chunk_id: Optional[int] = None
            chunk_context: Optional[Dict[str, Any]] = None
            
        # Create memory tools
        memory_tools = [
            create_manage_memory_tool(namespace=("legal_memory",)),
            create_search_memory_tool(namespace=("legal_memory",))
        ]
        
        # Document Analysis Agent
        @tool
        def analyze_document_structure(text: str) -> Dict[str, Any]:
            """Analyze document structure and extract sections."""
            doc = self.nlp(text)
            
            sections = []
            current_section = None
            
            for sent in doc.sents:
                # Identify section headers using linguistic patterns
                if any(pattern in sent.text.lower() for pattern in 
                      ['section', 'article', 'chapter', 'part', 'clause']):
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "title": sent.text.strip(),
                        "content": [],
                        "start_pos": sent.start_char
                    }
                elif current_section:
                    current_section["content"].append(sent.text.strip())
            
            if current_section:
                sections.append(current_section)
            
            return {
                "sections": sections,
                "total_sentences": len(list(doc.sents)),
                "entities": [(ent.text, ent.label_) for ent in doc.ents]
            }
        
        # Rule Extraction Agent
        @tool
        def extract_legal_rules(text: str) -> List[Dict[str, Any]]:
            """Extract legal rules using ReAct reasoning."""
            
            prompt = f"""
            Analyze the following legal text and extract structured rules, obligations, permissions, and prohibitions.
            
            For each rule found, provide:
            1. Subject (who the rule applies to)
            2. Predicate (the action or state)
            3. Object (what is affected)
            4. Modality (obligation/permission/prohibition)
            5. Conditions (when it applies)
            6. Exceptions (when it doesn't apply)
            
            Text: {text[:3000]}...
            
            Return a JSON list of rules with the structure above.
            """
            
            try:
                response = self.openai_client.chat.completions.create(
                    model="o3-mini-2025-01-31",
                    messages=[
                        {"role": "system", "content": "You are a legal expert specializing in extracting structured rules from legal documents."},
                        {"role": "user", "content": prompt}
                    ],
                    reasoning_effort="high"
                )
                
                rules_text = response.choices[0].message.content
                if "```json" in rules_text:
                    rules_text = rules_text.split("```json")[1].split("```")[0]
                
                rules = json.loads(rules_text)
                return rules
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from rule extraction")
                return []
            except Exception as e:
                logger.error(f"Error in rule extraction: {e}")
                return []
        
        # Domain Classification Agent
        @tool
        def classify_data_management_domains(rules: List[Dict[str, Any]]) -> Dict[str, List[int]]:
            """Classify rules into data management domains using LLM capabilities."""
            
            prompt = f"""
            Classify the following legal rules into data management domains. Analyze the content and dynamically determine relevant domains.
            
            Consider areas such as:
            - Data Storage and Retention
            - Data Usage and Processing
            - Data Movement and Transfer
            - Privacy and Consent
            - Security and Protection
            - Access and Disclosure
            - Data Quality and Integrity
            - Governance and Compliance
            
            For each rule, determine which domains it relates to and explain why.
            
            Rules: {json.dumps(rules, indent=2)}
            
            Return a JSON object mapping domain names to lists of rule indices (0-based).
            """
            
            try:
                response = self.openai_client.chat.completions.create(
                    model="o3-mini-2025-01-31",
                    messages=[
                        {"role": "system", "content": "You are a data management expert specializing in regulatory compliance classification."},
                        {"role": "user", "content": prompt}
                    ],
                    reasoning_effort="high"
                )
                
                classification_text = response.choices[0].message.content
                if "```json" in classification_text:
                    classification_text = classification_text.split("```json")[1].split("```")[0]
                
                domains = json.loads(classification_text)
                return domains
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from domain classification")
                return {}
            except Exception as e:
                logger.error(f"Error in domain classification: {e}")
                return {}
        
        # Ontology Generation Agent
        @tool
        def generate_skos_concepts(rules: List[Dict[str, Any]], domains: Dict[str, List[int]]) -> List[Dict[str, Any]]:
            """Generate SKOS concepts from extracted rules."""
            
            concepts = []
            
            # Generate concepts for each rule
            for i, rule in enumerate(rules):
                concept_id = f"rule_{i}_{hashlib.md5(str(rule).encode()).hexdigest()[:8]}"
                
                concept = {
                    "id": concept_id,
                    "type": "skos:Concept",
                    "prefLabel": f"{rule.get('subject', '')} {rule.get('predicate', '')}",
                    "definition": self._generate_rule_definition(rule),
                    "broader": [],
                    "narrower": [],
                    "related": [],
                    "inScheme": "legal_compliance_scheme",
                    "modality": rule.get("modality", ""),
                    "conditions": rule.get("conditions", []),
                    "exceptions": rule.get("exceptions", []),
                    "domains": [domain for domain, rule_indices in domains.items() if i in rule_indices]
                }
                
                concepts.append(concept)
            
            # Generate domain concepts
            for domain_name in domains.keys():
                domain_concept = {
                    "id": f"domain_{domain_name.lower().replace(' ', '_')}",
                    "type": "skos:Concept",
                    "prefLabel": domain_name,
                    "definition": f"Legal rules and requirements related to {domain_name.lower()}",
                    "broader": ["data_management"],
                    "narrower": [c["id"] for c in concepts if domain_name in c.get("domains", [])],
                    "topConceptOf": "legal_compliance_scheme"
                }
                concepts.append(domain_concept)
            
            return concepts
        
        # Create specialized agents
        self.document_analyzer = create_react_agent(
            self.llm,
            tools=[analyze_document_structure] + memory_tools,
            store=self.memory_store
        )
        
        self.rule_extractor = create_react_agent(
            self.llm,
            tools=[extract_legal_rules] + memory_tools,
            store=self.memory_store
        )
        
        self.domain_classifier = create_react_agent(
            self.llm,
            tools=[classify_data_management_domains] + memory_tools,
            store=self.memory_store
        )
        
        self.ontology_generator = create_react_agent(
            self.llm,
            tools=[generate_skos_concepts] + memory_tools,
            store=self.memory_store
        )
        
        # Create supervisor agent
        def supervisor_node(state: LegalAgentState):
            """Supervisor agent that orchestrates the workflow."""
            
            stage = state.get("processing_stage", "start")
            
            if stage == "start":
                return {
                    "messages": [AIMessage(content="Starting document analysis")],
                    "processing_stage": "document_analysis"
                }
            elif stage == "document_analysis":
                return {
                    "messages": [AIMessage(content="Moving to rule extraction")],
                    "processing_stage": "rule_extraction"
                }
            elif stage == "rule_extraction":
                return {
                    "messages": [AIMessage(content="Moving to domain classification")],
                    "processing_stage": "domain_classification"
                }
            elif stage == "domain_classification":
                return {
                    "messages": [AIMessage(content="Moving to ontology generation")],
                    "processing_stage": "ontology_generation"
                }
            else:
                return {
                    "messages": [AIMessage(content="Processing complete")],
                    "processing_stage": "complete"
                }
        
        # Build the workflow graph
        workflow = StateGraph(LegalAgentState)
        
        # Add nodes
        workflow.add_node("supervisor", supervisor_node)
        workflow.add_node("document_analyzer", self.document_analyzer)
        workflow.add_node("rule_extractor", self.rule_extractor)
        workflow.add_node("domain_classifier", self.domain_classifier)
        workflow.add_node("ontology_generator", self.ontology_generator)
        
        # Add edges
        workflow.add_edge(START, "supervisor")
        workflow.add_edge("supervisor", "document_analyzer")
        workflow.add_edge("document_analyzer", "rule_extractor")
        workflow.add_edge("rule_extractor", "domain_classifier")
        workflow.add_edge("domain_classifier", "ontology_generator")
        workflow.add_edge("ontology_generator", END)
        
        # Compile the workflow
        self.workflow = workflow.compile(checkpointer=MemorySaver())
    
    def _generate_rule_definition(self, rule: Dict[str, Any]) -> str:
        """Generate a human-readable definition for a rule."""
        modality = rule.get("modality", "").lower()
        subject = rule.get("subject", "")
        predicate = rule.get("predicate", "")
        obj = rule.get("object", "")
        
        if modality == "obligation":
            return f"{subject} must {predicate} {obj}"
        elif modality == "permission":
            return f"{subject} may {predicate} {obj}"
        elif modality == "prohibition":
            return f"{subject} must not {predicate} {obj}"
        else:
            return f"{subject} {predicate} {obj}"
    
    async def process_document(self, pdf_path: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process a legal document through the multi-agent system with chunking."""
        
        # Extract text from PDF
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            raise ValueError("Failed to extract text from PDF")
        
        logger.info(f"Processing document: {document_metadata.get('id', 'unknown')}")
        logger.info(f"Document length: {len(text)} characters")
        
        # Create semantic chunks for large documents
        chunks = self.chunk_document_semantically(text, max_tokens=6000, overlap_tokens=200)
        logger.info(f"Document split into {len(chunks)} chunks")
        
        # Create embeddings for chunks
        chunks_with_embeddings = self.create_chunk_embeddings(chunks)
        
        # Process chunks through multi-agent system
        all_rules = []
        all_domain_classifications = {}
        all_concepts = []
        
        # Process each chunk through the workflow
        for i, chunk in enumerate(chunks_with_embeddings):
            logger.info(f"Processing chunk {i+1}/{len(chunks_with_embeddings)}")
            
            # Create initial state for this chunk
            chunk_state = {
                "messages": [HumanMessage(content=f"Process this legal document chunk: {chunk['text'][:1500]}...")],
                "document_id": document_metadata.get("id", "unknown"),
                "jurisdiction": document_metadata.get("jurisdiction", "unknown"),
                "organization": document_metadata.get("organization", "unknown"),
                "chunk_id": chunk["chunk_id"],
                "chunk_context": {
                    "section": chunk.get("section"),
                    "tokens": chunk["tokens"],
                    "chunk_index": i,
                    "total_chunks": len(chunks_with_embeddings)
                },
                "extracted_rules": [],
                "domain_classifications": {},
                "ontology_concepts": [],
                "processing_stage": "start"
            }
            
            try:
                # Run the workflow for this chunk
                chunk_result = await self.workflow.ainvoke(chunk_state)
                
                # Aggregate results
                chunk_rules = chunk_result.get("extracted_rules", [])
                for rule in chunk_rules:
                    rule["source_chunk"] = i
                    rule["chunk_section"] = chunk.get("section")
                
                all_rules.extend(chunk_rules)
                
                # Merge domain classifications
                chunk_domains = chunk_result.get("domain_classifications", {})
                for domain, rule_indices in chunk_domains.items():
                    if domain not in all_domain_classifications:
                        all_domain_classifications[domain] = []
                    # Adjust rule indices to account for previous chunks
                    adjusted_indices = [idx + len(all_rules) - len(chunk_rules) for idx in rule_indices]
                    all_domain_classifications[domain].extend(adjusted_indices)
                
                all_concepts.extend(chunk_result.get("ontology_concepts", []))
                
            except Exception as e:
                logger.error(f"Error processing chunk {i}: {e}")
                continue
        
        # Create final aggregated result
        aggregated_result = {
            "document_id": document_metadata.get("id", "unknown"),
            "jurisdiction": document_metadata.get("jurisdiction", "unknown"),
            "organization": document_metadata.get("organization", "unknown"),
            "extracted_rules": all_rules,
            "domain_classifications": all_domain_classifications,
            "ontology_concepts": all_concepts,
            "processing_metadata": {
                "total_chunks": len(chunks_with_embeddings),
                "total_characters": len(text),
                "chunks_processed": len([c for c in chunks_with_embeddings if "embedding" in c])
            }
        }
        
        # Generate ontology outputs
        ontology_outputs = self._generate_ontology_outputs(aggregated_result)
        
        # Update knowledge graph
        self._update_knowledge_graph(aggregated_result, chunks_with_embeddings)
        
        # Store in long-term memory
        await self._store_in_memory(aggregated_result, chunks_with_embeddings)
        
        return {
            "document_id": aggregated_result["document_id"],
            "jurisdiction": aggregated_result["jurisdiction"],
            "organization": aggregated_result["organization"],
            "rules": aggregated_result["extracted_rules"],
            "domains": aggregated_result["domain_classifications"],
            "concepts": aggregated_result["ontology_concepts"],
            "ontology": ontology_outputs,
            "chunks": chunks_with_embeddings,
            "processing_metadata": aggregated_result["processing_metadata"],
            "knowledge_graph_nodes": len(self.knowledge_graph.nodes),
            "knowledge_graph_edges": len(self.knowledge_graph.edges)
        }
    
    def process_all_documents(self) -> List[Dict[str, Any]]:
        """Process all documents defined in the configuration."""
        results = []
        
        for doc_config in self.config["documents"]:
            try:
                logger.info(f"Processing document: {doc_config['id']}")
                
                # Check if file exists
                if not Path(doc_config["path"]).exists():
                    logger.warning(f"File not found: {doc_config['path']}")
                    results.append({
                        "document_id": doc_config['id'],
                        "error": f"File not found: {doc_config['path']}",
                        "success": False
                    })
                    continue
                
                # Run async processing
                result = asyncio.run(self.process_document(
                    doc_config["path"],
                    doc_config
                ))
                
                result["success"] = True
                results.append(result)
                logger.info(f"Successfully processed {doc_config['id']}: {len(result['rules'])} rules extracted")
                
            except Exception as e:
                logger.error(f"Failed to process document {doc_config['id']}: {e}")
                results.append({
                    "document_id": doc_config['id'],
                    "error": str(e),
                    "success": False
                })
        
        return results
    
    def _generate_ontology_outputs(self, processing_result: Dict[str, Any]) -> Dict[str, str]:
        """Generate SKOS ontology in multiple formats."""
        
        # Clear existing graph
        self.rdf_graph = Graph()
        self._setup_namespaces()
        
        # Create concept scheme
        scheme_uri = self.LEG["legal_compliance_scheme"]
        self.rdf_graph.add((scheme_uri, RDF.type, SKOS.ConceptScheme))
        self.rdf_graph.add((scheme_uri, SKOS.prefLabel, Literal("Legal Compliance Ontology")))
        self.rdf_graph.add((scheme_uri, RDFS.comment, Literal("SKOS ontology for legal compliance rules and data management requirements")))
        
        # Add concepts
        concepts = processing_result.get("ontology_concepts", [])
        for concept_data in concepts:
            concept_uri = self.LEG[concept_data["id"]]
            
            # Basic concept properties
            self.rdf_graph.add((concept_uri, RDF.type, SKOS.Concept))
            self.rdf_graph.add((concept_uri, SKOS.prefLabel, Literal(concept_data.get("prefLabel", ""))))
            self.rdf_graph.add((concept_uri, SKOS.definition, Literal(concept_data.get("definition", ""))))
            self.rdf_graph.add((concept_uri, SKOS.inScheme, scheme_uri))
            
            # Hierarchical relationships
            for broader_id in concept_data.get("broader", []):
                broader_uri = self.LEG[broader_id]
                self.rdf_graph.add((concept_uri, SKOS.broader, broader_uri))
            
            for narrower_id in concept_data.get("narrower", []):
                narrower_uri = self.LEG[narrower_id]
                self.rdf_graph.add((concept_uri, SKOS.narrower, narrower_uri))
            
            # Related concepts
            for related_id in concept_data.get("related", []):
                related_uri = self.LEG[related_id]
                self.rdf_graph.add((concept_uri, SKOS.related, related_uri))
            
            # Custom properties for legal rules
            if concept_data.get("modality"):
                self.rdf_graph.add((concept_uri, self.LEG.modality, Literal(concept_data["modality"])))
            
            # Add domain classifications
            for domain in concept_data.get("domains", []):
                domain_uri = self.DATA_MGT[domain.lower().replace(" ", "_")]
                self.rdf_graph.add((concept_uri, self.LEG.appliesToDomain, domain_uri))
        
        # Generate output formats
        outputs = {
            "json_ld": self.rdf_graph.serialize(format="json-ld"),
            "turtle": self.rdf_graph.serialize(format="turtle"),
            "xml": self.rdf_graph.serialize(format="xml")
        }
        
        return outputs
    
    def _update_knowledge_graph(self, result: Dict[str, Any], chunks: List[Dict[str, Any]] = None):
        """Update the knowledge graph with extracted information including chunks."""
        
        document_id = result["document_id"]
        jurisdiction = result["jurisdiction"]
        
        # Add document node
        self.knowledge_graph.add_node(
            document_id,
            type="document",
            jurisdiction=jurisdiction,
            organization=result["organization"],
            rules_count=len(result.get("extracted_rules", [])),
            processing_metadata=result.get("processing_metadata", {})
        )
        
        # Add chunk nodes if available
        if chunks:
            for chunk in chunks:
                chunk_id = f"{document_id}_chunk_{chunk['chunk_id']}"
                
                self.knowledge_graph.add_node(
                    chunk_id,
                    type="chunk",
                    document_id=document_id,
                    chunk_id=chunk["chunk_id"],
                    text=chunk["text"][:500] + "..." if len(chunk["text"]) > 500 else chunk["text"],
                    section=chunk.get("section"),
                    tokens=chunk["tokens"],
                    has_embedding="embedding" in chunk,
                    start_char=chunk.get("start_char"),
                    end_char=chunk.get("end_char")
                )
                
                # Connect document to chunk
                self.knowledge_graph.add_edge(document_id, chunk_id, relationship="contains_chunk")
        
        # Add rule nodes and relationships
        for i, rule in enumerate(result.get("extracted_rules", [])):
            rule_id = f"{document_id}_rule_{i}"
            
            self.knowledge_graph.add_node(
                rule_id,
                type="rule",
                modality=rule.get("modality"),
                subject=rule.get("subject"),
                predicate=rule.get("predicate"),
                object=rule.get("object"),
                conditions=rule.get("conditions", []),
                exceptions=rule.get("exceptions", []),
                source_chunk=rule.get("source_chunk"),
                chunk_section=rule.get("chunk_section")
            )
            
            # Connect document to rule
            self.knowledge_graph.add_edge(document_id, rule_id, relationship="contains_rule")
            
            # Connect chunk to rule if chunk information available
            if "source_chunk" in rule and chunks:
                chunk_id = f"{document_id}_chunk_{rule['source_chunk']}"
                if self.knowledge_graph.has_node(chunk_id):
                    self.knowledge_graph.add_edge(chunk_id, rule_id, relationship="contains_rule")
            
            # Connect to domains
            for domain, rule_indices in result.get("domain_classifications", {}).items():
                if i in rule_indices:
                    domain_node = f"domain_{domain.lower().replace(' ', '_')}"
                    if not self.knowledge_graph.has_node(domain_node):
                        self.knowledge_graph.add_node(domain_node, type="domain", name=domain)
                    
                    self.knowledge_graph.add_edge(rule_id, domain_node, relationship="belongs_to_domain")
        
        logger.info(f"Updated knowledge graph: {len(self.knowledge_graph.nodes)} nodes, {len(self.knowledge_graph.edges)} edges")
    
    async def _store_in_memory(self, result: Dict[str, Any], chunks: List[Dict[str, Any]] = None):
        """Store processing results in long-term memory with chunk support."""
        
        # Store document-level information
        document_memory = {
            "type": "processed_document",
            "document_id": result["document_id"],
            "jurisdiction": result["jurisdiction"],
            "organization": result["organization"],
            "rules_count": len(result.get("extracted_rules", [])),
            "domains": list(result.get("domain_classifications", {}).keys()),
            "processing_metadata": result.get("processing_metadata", {}),
            "timestamp": datetime.now().isoformat()
        }
        
        # Store using LangMem
        try:
            await self.memory_manager.add_memory(
                namespace=("legal_memory", result["jurisdiction"], "documents"),
                key=result["document_id"],
                value=document_memory
            )
        except Exception as e:
            logger.warning(f"Failed to store document memory: {e}")
        
        # Store individual rules with enhanced context
        for i, rule in enumerate(result.get("extracted_rules", [])):
            rule_memory = {
                "type": "legal_rule",
                "content": rule,
                "document_id": result["document_id"],
                "jurisdiction": result["jurisdiction"],
                "rule_index": i,
                "source_chunk": rule.get("source_chunk"),
                "chunk_section": rule.get("chunk_section"),
                "timestamp": datetime.now().isoformat()
            }
            
            rule_id = f"rule_{result['document_id']}_{i}"
            try:
                await self.memory_manager.add_memory(
                    namespace=("legal_memory", result["jurisdiction"], "rules"),
                    key=rule_id,
                    value=rule_memory
                )
            except Exception as e:
                logger.warning(f"Failed to store rule memory {rule_id}: {e}")
        
        logger.info(f"Stored memory for document {result['document_id']} with {len(result.get('extracted_rules', []))} rules")
    
    def query_knowledge_graph(self, query: str, use_embeddings: bool = True) -> Dict[str, Any]:
        """
        Query the knowledge graph using natural language with optional embedding-based search.
        
        Args:
            query: Natural language query
            use_embeddings: Whether to use embedding-based semantic search
            
        Returns:
            Query results with relevant nodes and edges
        """
        if use_embeddings:
            # Find all chunks with embeddings
            all_chunks = []
            for node_id, node_data in self.knowledge_graph.nodes(data=True):
                if node_data.get("type") == "chunk" and node_data.get("has_embedding"):
                    # Reconstruct chunk data for similarity search
                    chunk_data = {
                        "chunk_id": node_data.get("chunk_id"),
                        "text": node_data.get("text", ""),
                        "section": node_data.get("section"),
                        "tokens": node_data.get("tokens"),
                        "embedding": []  # Would need to retrieve from storage
                    }
                    all_chunks.append(chunk_data)
            
            if not all_chunks:
                return {"query": query, "results": [], "message": "No chunks with embeddings available for search"}
            
            # Note: In a real implementation, you'd retrieve embeddings from storage
            # For now, return keyword-based search
            return self._keyword_search(query)
        else:
            return self._keyword_search(query)
    
    def _keyword_search(self, query: str) -> Dict[str, Any]:
        """Fallback keyword-based search."""
        nodes = []
        edges = []
        
        # Simple keyword matching
        query_terms = query.lower().split()
        
        for node_id, node_data in self.knowledge_graph.nodes(data=True):
            node_text = str(node_data).lower()
            if any(term in node_text for term in query_terms):
                nodes.append({"id": node_id, "data": node_data})
        
        # Get edges for relevant nodes
        for node in nodes:
            for edge in self.knowledge_graph.edges(node["id"], data=True):
                edges.append({
                    "source": edge[0],
                    "target": edge[1],
                    "data": edge[2]
                })
        
        return {
            "query": query,
            "nodes": nodes,
            "edges": edges,
            "search_method": "keyword_based",
            "graph_stats": {
                "total_nodes": len(self.knowledge_graph.nodes),
                "total_edges": len(self.knowledge_graph.edges)
            }
        }
    
    def export_ontology(self, output_dir: str):
        """Export the complete ontology in multiple formats."""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Export SKOS ontology
        ontology_outputs = self._generate_ontology_outputs({"ontology_concepts": []})
        
        for format_name, content in ontology_outputs.items():
            file_extension = {
                "json_ld": "jsonld",
                "turtle": "ttl", 
                "xml": "xml"
            }[format_name]
            
            with open(output_path / f"legal_ontology.{file_extension}", 'w') as f:
                f.write(content)
        
        # Export knowledge graph
        try:
            nx.write_gexf(self.knowledge_graph, output_path / "knowledge_graph.gexf")
        except Exception as e:
            logger.warning(f"Failed to export knowledge graph: {e}")
        
        logger.info(f"Ontology exported to {output_dir}")

# Example usage and configuration
if __name__ == "__main__":
    # Simplified configuration - just array of documents with mappings
    config = {
        "documents": [
            {
                "id": "gdpr_2016_679",
                "title": "General Data Protection Regulation",
                "path": "/path/to/gdpr_regulation_2016_679.pdf",
                "jurisdiction": "EU",
                "organization": "European_Union",
                "country": "European Union"
            },
            {
                "id": "ccpa_2018",
                "title": "California Consumer Privacy Act",
                "path": "/path/to/ccpa_2018.pdf",
                "jurisdiction": "California",
                "organization": "California_State",
                "country": "United States"
            },
            {
                "id": "uk_dpa_2018",
                "title": "Data Protection Act 2018",
                "path": "/path/to/uk_dpa_2018.pdf",
                "jurisdiction": "UK",
                "organization": "UK_Government",
                "country": "United Kingdom"
            },
            {
                "id": "singapore_pdpa_2012",
                "title": "Personal Data Protection Act",
                "path": "/path/to/singapore_pdpa_2012.pdf",
                "jurisdiction": "Singapore",
                "organization": "Singapore_Government",
                "country": "Singapore"
            },
            {
                "id": "basel_iii_framework",
                "title": "Basel III International Regulatory Framework",
                "path": "/path/to/basel_iii_framework.pdf",
                "jurisdiction": "International",
                "organization": "Basel_Committee",
                "country": "International"
            }
        ],
        "output_directory": "./ontology_output",
        "openai_base_url": "https://api.openai.com/v1"
    }
    
    # Save configuration
    with open("legal_config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    # Initialize and run processor
    async def main():
        # Note: Ensure tiktoken_models folder exists with downloaded models
        tiktoken_models_path = Path("./tiktoken_models")
        if not tiktoken_models_path.exists():
            print("Warning: tiktoken_models folder not found. Creating placeholder...")
            tiktoken_models_path.mkdir(exist_ok=True)
            print("Please download tiktoken models to ./tiktoken_models/ folder")
        
        processor = LegalDocumentProcessor(
            config_path="legal_config.json",
            openai_base_url=config.get("openai_base_url", "https://api.openai.com/v1")
        )
        
        # Process all documents from config
        results = processor.process_all_documents()
        
        # Print summary
        successful_docs = [r for r in results if r.get("success", True)]
        print(f"Successfully processed {len(successful_docs)} out of {len(results)} documents")
        
        for result in successful_docs:
            print(f"- {result['document_id']}: {len(result.get('rules', []))} rules extracted")
        
        # Export ontology
        processor.export_ontology(config["output_directory"])
        
        # Example query
        query_result = processor.query_knowledge_graph("data storage requirements", use_embeddings=True)
        print(f"Query result: {len(query_result.get('nodes', []))} relevant nodes found")
        
        # Example chunk analysis
        if results:
            sample_result = [r for r in results if r.get("success", True)]
            if sample_result and sample_result[0].get("chunks"):
                chunks = sample_result[0]["chunks"]
                print(f"Sample document chunking:")
                print(f"- Total chunks: {len(chunks)}")
                print(f"- Average tokens per chunk: {sum(c['tokens'] for c in chunks) / len(chunks):.0f}")
                print(f"- Chunks with embeddings: {sum(1 for c in chunks if 'embedding' in c)}")
    
    # Run the system
    asyncio.run(main())
