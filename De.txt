"""
Enhanced Legal Document Analyzer with FalkorDB Graph RAG
Uses FalkorDB for semantic knowledge graphs with embeddings
Complete implementation with all features
"""

from typing import Dict, List, Optional, Any, TypedDict, Annotated
import json
from dataclasses import dataclass
import logging
import re
import operator
import os

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langchain_community.graphs import FalkorDBGraph
from langchain_community.vectorstores import FalkorDBVector

from openai import OpenAI

from src.utils.document_chunker import DocumentChunker
from src.config import Config

logger = logging.getLogger(__name__)


# ============================================================================
# EMBEDDING SERVICE (Direct OpenAI API - No Tiktoken)
# ============================================================================

class EmbeddingService:
    """Direct OpenAI embedding service without tiktoken"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-large"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.dimension = 3072  # text-embedding-3-large dimension
    
    def embed_text(self, text: str) -> List[float]:
        """Generate embedding for single text"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return [0.0] * self.dimension
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        try:
            response = self.client.embeddings.create(
                input=texts,
                model=self.model
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Batch embedding error: {e}")
            return [[0.0] * self.dimension for _ in texts]


# ============================================================================
# FALKORDB KNOWLEDGE GRAPH
# ============================================================================

class FalkorDBKnowledgeGraph:
    """FalkorDB-based semantic knowledge graph for legal analysis"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, 
                 graph_name: str = "legal_knowledge_graph",
                 embedding_service: EmbeddingService = None):
        """Initialize FalkorDB connection"""
        try:
            self.graph = FalkorDBGraph(
                database=graph_name,
                host=host,
                port=port
            )
            self.graph_name = graph_name
            self.embedding_service = embedding_service
            
            # Create indexes for efficient querying
            self._create_indexes()
            
            # Statistics
            self.stats = {
                "requirements": 0,
                "actions": 0,
                "evidence": 0,
                "constraints": 0,
                "policies": 0,
                "citations": 0
            }
            
            logger.info(f"Connected to FalkorDB graph: {graph_name}")
            
        except Exception as e:
            logger.error(f"FalkorDB connection error: {e}")
            logger.info("Falling back to in-memory graph")
            self.graph = None
            self.stats = {}
    
    def _create_indexes(self):
        """Create indexes for common query patterns"""
        if not self.graph:
            return
        
        try:
            # Index on requirement IDs
            self.graph.query("CREATE INDEX FOR (r:Requirement) ON (r.id)")
            
            # Index on action types
            self.graph.query("CREATE INDEX FOR (a:Action) ON (a.type)")
            
            # Index on evidence perspective
            self.graph.query("CREATE INDEX FOR (e:Evidence) ON (e.perspective)")
            
            # Index on constraint types
            self.graph.query("CREATE INDEX FOR (c:Constraint) ON (c.type)")
            
            # Index on levels
            self.graph.query("CREATE INDEX FOR (n) ON (n.level)")
            
            logger.info("Created FalkorDB indexes")
        except Exception as e:
            logger.warning(f"Index creation warning: {e}")
    
    def add_requirement(self, req_id: str, description: str, level: int,
                       classification: str, citations: List[str]):
        """Add requirement node with embedding"""
        if not self.graph:
            self.stats["requirements"] = self.stats.get("requirements", 0) + 1
            return
        
        try:
            # Generate embedding for requirement
            embedding = None
            if self.embedding_service and description:
                embedding = self.embedding_service.embed_text(description)
            
            # Escape quotes in description and citations
            desc_escaped = description.replace("'", "\\'").replace('"', '\\"')
            citations_str = "|".join([c.replace("'", "\\'") for c in citations[:5]])  # Limit citations
            
            # Create requirement node
            query = f"""
            CREATE (r:Requirement {{
                id: '{req_id}',
                description: '{desc_escaped[:1000]}',
                level: {level},
                classification: '{classification}',
                citations: '{citations_str[:2000]}',
                timestamp: timestamp()
            }})
            """
            
            if embedding:
                # Store embedding as property
                embedding_str = ",".join([str(x) for x in embedding[:100]])  # Limit for storage
                query = f"""
                CREATE (r:Requirement {{
                    id: '{req_id}',
                    description: '{desc_escaped[:1000]}',
                    level: {level},
                    classification: '{classification}',
                    citations: '{citations_str[:2000]}',
                    embedding_sample: '{embedding_str}',
                    timestamp: timestamp()
                }})
                """
            
            self.graph.query(query)
            self.stats["requirements"] = self.stats.get("requirements", 0) + 1
            
        except Exception as e:
            logger.error(f"Error adding requirement: {e}")
    
    def add_action(self, action_id: str, action_type: str, description: str,
                  actor: str, req_id: str, citations: List[str]):
        """Add action node and link to requirement"""
        if not self.graph:
            self.stats["actions"] = self.stats.get("actions", 0) + 1
            return
        
        try:
            desc_escaped = description.replace("'", "\\'").replace('"', '\\"')
            citations_str = "|".join([c.replace("'", "\\'") for c in citations[:3]])
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (a:Action {{
                id: '{action_id}',
                type: '{action_type}',
                description: '{desc_escaped[:500]}',
                actor: '{actor}',
                citations: '{citations_str[:1000]}',
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_ACTION]->(a)
            """
            
            self.graph.query(query)
            self.stats["actions"] = self.stats.get("actions", 0) + 1
            
        except Exception as e:
            logger.error(f"Error adding action: {e}")
    
    def add_evidence(self, evidence_id: str, evidence_type: str, 
                    description: str, perspective: str, req_id: str,
                    citations: List[str]):
        """Add evidence node"""
        if not self.graph:
            self.stats["evidence"] = self.stats.get("evidence", 0) + 1
            return
        
        try:
            desc_escaped = description.replace("'", "\\'").replace('"', '\\"')
            citations_str = "|".join([c.replace("'", "\\'") for c in citations[:3]])
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (e:Evidence {{
                id: '{evidence_id}',
                type: '{evidence_type}',
                description: '{desc_escaped[:500]}',
                perspective: '{perspective}',
                citations: '{citations_str[:1000]}',
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_EVIDENCE]->(e)
            """
            
            self.graph.query(query)
            self.stats["evidence"] = self.stats.get("evidence", 0) + 1
            
        except Exception as e:
            logger.error(f"Error adding evidence: {e}")
    
    def add_constraint(self, constraint_id: str, constraint_type: str,
                      description: str, operator: str, value: Any, req_id: str):
        """Add constraint node"""
        if not self.graph:
            self.stats["constraints"] = self.stats.get("constraints", 0) + 1
            return
        
        try:
            desc_escaped = description.replace("'", "\\'").replace('"', '\\"')
            value_str = str(value).replace("'", "\\'")[:200]
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (c:Constraint {{
                id: '{constraint_id}',
                type: '{constraint_type}',
                description: '{desc_escaped[:500]}',
                operator: '{operator}',
                value: '{value_str}',
                timestamp: timestamp()
            }})
            CREATE (r)-[:HAS_CONSTRAINT]->(c)
            """
            
            self.graph.query(query)
            self.stats["constraints"] = self.stats.get("constraints", 0) + 1
            
        except Exception as e:
            logger.error(f"Error adding constraint: {e}")
    
    def find_related_requirements(self, req_id: str, max_depth: int = 2) -> List[str]:
        """Find related requirements using graph traversal"""
        if not self.graph:
            return []
        
        try:
            query = f"""
            MATCH path = (r1:Requirement {{id: '{req_id}'}})-[*1..{max_depth}]-(r2:Requirement)
            RETURN DISTINCT r2.id AS related_id
            LIMIT 10
            """
            
            result = self.graph.query(query)
            return [row['related_id'] for row in result]
            
        except Exception as e:
            logger.error(f"Error finding related: {e}")
            return []
    
    def get_requirement_summary(self, req_id: str) -> Dict[str, Any]:
        """Get comprehensive summary of requirement and its relationships"""
        if not self.graph:
            return {}
        
        try:
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            OPTIONAL MATCH (r)-[:REQUIRES_ACTION]->(a:Action)
            OPTIONAL MATCH (r)-[:REQUIRES_EVIDENCE]->(e:Evidence)
            OPTIONAL MATCH (r)-[:HAS_CONSTRAINT]->(c:Constraint)
            RETURN r.description AS description,
                   r.classification AS classification,
                   count(DISTINCT a) AS action_count,
                   count(DISTINCT e) AS evidence_count,
                   count(DISTINCT c) AS constraint_count
            """
            
            result = self.graph.query(query)
            if result:
                return dict(result[0])
            return {}
            
        except Exception as e:
            logger.error(f"Error getting summary: {e}")
            return {}
    
    def semantic_search(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Semantic search using embeddings"""
        if not self.embedding_service:
            return []
        
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.embed_text(query_text)
            
            # For now, return simple text match
            # Full vector search would require FalkorDB vector extension
            query = f"""
            MATCH (r:Requirement)
            WHERE r.description CONTAINS '{query_text[:100]}'
            RETURN r.id AS id, r.description AS description, r.level AS level
            LIMIT {top_k}
            """
            
            result = self.graph.query(query)
            return [dict(row) for row in result]
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get graph statistics"""
        if not self.graph:
            return self.stats
        
        try:
            # Count nodes by type
            counts = {}
            for node_type in ["Requirement", "Action", "Evidence", "Constraint"]:
                query = f"MATCH (n:{node_type}) RETURN count(n) AS count"
                result = self.graph.query(query)
                counts[node_type.lower()] = result[0]['count'] if result else 0
            
            # Count relationships
            query = "MATCH ()-[r]->() RETURN count(r) AS count"
            result = self.graph.query(query)
            counts['relationships'] = result[0]['count'] if result else 0
            
            return counts
            
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return self.stats


# ============================================================================
# STATE DEFINITIONS FOR LANGGRAPH
# ============================================================================

class AgentState(TypedDict):
    """State for LangGraph agent"""
    messages: Annotated[List[BaseMessage], operator.add]
    chunk_text: str
    chunk_id: int
    level: int
    rule_name: str
    jurisdiction: str
    enterprise_context: Optional[Dict[str, Any]]
    
    description: str
    citations: List[Dict[str, Any]]
    data_actions: List[Dict[str, Any]]
    user_evidence: List[Dict[str, Any]]
    system_evidence: List[Dict[str, Any]]
    constraints: List[Dict[str, Any]]
    enterprise_policies: List[Dict[str, Any]]
    classification: str
    classification_reasoning: str
    
    chain_of_thought: List[str]
    expert_opinions: Dict[str, Any]
    thought_tree: Dict[str, Any]
    
    kg_nodes: List[Dict[str, Any]]
    kg_edges: List[Dict[str, Any]]
    
    next_step: str
    iteration: int
    max_iterations: int


# ============================================================================
# LANGGRAPH AGENT NODES
# ============================================================================

class LegalAnalysisAgent:
    """LangGraph agent with advanced reasoning"""
    
    def __init__(self, llm: ChatOpenAI, kg: FalkorDBKnowledgeGraph):
        self.llm = llm
        self.kg = kg
    
    def chain_of_thought_reasoning(self, state: AgentState) -> AgentState:
        """Chain of Thought reasoning node"""
        prompt = f"""Analyze this legal text using step-by-step reasoning.

TEXT:
{state['chunk_text'][:2500]}

Rule: {state['rule_name']}
Jurisdiction: {state['jurisdiction']}
Level: {state['level']}

IMPORTANT NOTES:
- "DataVisa" is an internal data governance tool - treat it as an enterprise system
- Ignore classification markers like "INTERNAL" at document end

Think through systematically:
1. What is the core legal requirement stated in this text?
2. Who is affected - users performing actions, or systems that must be implemented?
3. What specific data operations are mentioned (sharing/accessing data, storing/hosting data, or using/processing data)?
4. What conditions or limitations apply?
5. Is this describing actions that ARE allowed under conditions (CONDITION), or actions that are NOT allowed or restricted (RESTRICTION)?

For each point, provide a clear statement with an exact citation from the text (max 150 characters) and explain why it's relevant.

Return valid JSON:
{{
    "thought_steps": ["clear step 1", "clear step 2", "clear step 3", "clear step 4", "clear step 5"],
    "main_requirement": "Complete sentence describing what the rule requires",
    "affected_parties": ["user", "system"],
    "data_operations": ["specific operation type"],
    "conditions": ["complete sentence for condition 1", "complete sentence for condition 2"],
    "classification": "condition" or "restriction",
    "citations": [
        {{
            "text": "exact quote from text above",
            "reasoning": "why this quote supports the requirement",
            "supports": "which aspect it supports"
        }}
    ]
}}"""
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        try:
            result = self._extract_json(response.content)
            state['chain_of_thought'] = result.get('thought_steps', [])
            
            if result.get('main_requirement'):
                state['description'] += result['main_requirement'] + " "
            
            for cite in result.get('citations', []):
                if cite.get('text') and len(cite['text']) > 20:
                    state['citations'].append({
                        "text": cite['text'],
                        "reasoning": cite.get('reasoning', ''),
                        "chunk_id": state['chunk_id'],
                        "level": state['level'],
                        "method": "chain_of_thought"
                    })
            
            if not state.get('classification') and result.get('classification'):
                state['classification'] = result['classification']
            
        except Exception as e:
            logger.error(f"Chain of thought error: {e}")
            state['chain_of_thought'].append(f"Error in CoT: {str(e)}")
        
        state['next_step'] = 'mixture_of_experts'
        return state
    
    def mixture_of_experts(self, state: AgentState) -> AgentState:
        """Mixture of Experts reasoning"""
        experts = {
            "legal_expert": "Extract legal obligations with precise citations",
            "data_privacy_specialist": "Identify data handling operations and categorize them",
            "technical_architect": "Determine what systems must implement",
            "compliance_officer": "Identify what users must do to comply"
        }
        
        expert_results = {}
        
        for expert_role, expert_task in experts.items():
            prompt = f"""You are a {expert_role.replace('_', ' ').title()}.

Task: {expert_task}

TEXT:
{state['chunk_text'][:2000]}

Rule: {state['rule_name']}

REMEMBER: DataVisa is an internal data governance tool.

Provide JSON:
{{
    "key_findings": ["complete sentence finding 1", "complete sentence finding 2"],
    "data_actions": [
        {{
            "type": "data_sharing_and_access" or "data_storage_and_hosting" or "data_usage",
            "description": "Complete sentence describing what must be done",
            "actor": "user" or "system",
            "citations": [{{"text": "exact quote"}}]
        }}
    ],
    "requirements": [
        {{
            "description": "Complete sentence describing requirement",
            "perspective": "user" or "system",
            "citations": [{{"text": "exact quote"}}]
        }}
    ],
    "constraints": [
        {{
            "type": "temporal" or "technical" or "procedural",
            "description": "Complete sentence describing constraint",
            "citations": [{{"text": "exact quote"}}]
        }}
    ]
}}"""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                expert_results[expert_role] = self._extract_json(response.content)
            except Exception as e:
                logger.error(f"Expert {expert_role} error: {e}")
                expert_results[expert_role] = {"error": str(e)}
        
        state['expert_opinions'] = expert_results
        self._consolidate_expert_findings(state, expert_results)
        
        state['next_step'] = 'tree_of_thought'
        return state
    
    def tree_of_thought(self, state: AgentState) -> AgentState:
        """Tree of Thought exploration"""
        prompt = f"""Explore alternative interpretations of this rule.

TEXT:
{state['chunk_text'][:2000]}

Current understanding:
- Description: {state['description'][:300]}
- Actions found: {len(state['data_actions'])}
- Evidence found: {len(state['user_evidence']) + len(state['system_evidence'])}

Generate 3 interpretation paths:

Path 1 - Conservative: Strictest reading of the text
Path 2 - Balanced: Reasonable middle-ground interpretation  
Path 3 - Expansive: Broadest reasonable interpretation

For each, explain implications for compliance in complete sentences.

Return JSON:
{{
    "paths": [
        {{
            "name": "conservative",
            "interpretation": "Complete explanation of this interpretation",
            "implications": ["complete sentence 1", "complete sentence 2"],
            "required_actions": ["complete sentence 1", "complete sentence 2"],
            "confidence": "high" or "medium" or "low"
        }}
    ],
    "recommended_path": "conservative" or "balanced" or "expansive",
    "reasoning": "Complete explanation of why this path is recommended"
}}"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = self._extract_json(response.content)
            state['thought_tree'] = result
            
            state['chain_of_thought'].append(
                f"Explored {len(result.get('paths', []))} interpretation paths. "
                f"Recommended: {result.get('recommended_path', 'balanced')}"
            )
        except Exception as e:
            logger.error(f"Tree of thought error: {e}")
            state['thought_tree'] = {"error": str(e)}
        
        state['next_step'] = 'knowledge_graph_integration'
        return state
    
    def knowledge_graph_integration(self, state: AgentState) -> AgentState:
        """Integrate into FalkorDB knowledge graph"""
        chunk_id = state['chunk_id']
        level = state['level']
        
        req_id = f"req_{state['rule_name']}_{level}_{chunk_id}".replace(" ", "_")
        
        self.kg.add_requirement(
            req_id,
            state['description'],
            level,
            state['classification'],
            [c['text'] for c in state['citations']]
        )
        
        for i, action in enumerate(state['data_actions']):
            action_id = f"{req_id}_action_{i}"
            self.kg.add_action(
                action_id,
                action['type'],
                action['description'],
                action.get('actor', 'unknown'),
                req_id,
                [c.get('text', '') for c in action.get('citations', [])]
            )
        
        for i, evidence in enumerate(state['user_evidence']):
            evidence_id = f"{req_id}_evidence_user_{i}"
            self.kg.add_evidence(
                evidence_id,
                'user_requirement',
                evidence['description'],
                'user',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])]
            )
        
        for i, evidence in enumerate(state['system_evidence']):
            evidence_id = f"{req_id}_evidence_system_{i}"
            self.kg.add_evidence(
                evidence_id,
                'system_requirement',
                evidence['description'],
                'system',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])]
            )
        
        for i, constraint in enumerate(state['constraints']):
            constraint_id = f"{req_id}_constraint_{i}"
            self.kg.add_constraint(
                constraint_id,
                constraint.get('type', 'general'),
                constraint['description'],
                constraint.get('operator', 'eq'),
                constraint.get('right_operand', None),
                req_id
            )
        
        summary = self.kg.get_requirement_summary(req_id)
        state['chain_of_thought'].append(f"Knowledge graph integration: {summary}")
        
        stats = self.kg.get_statistics()
        state['kg_nodes'] = [{"type": k, "count": v} for k, v in stats.items()]
        
        state['next_step'] = 'validate'
        return state
    
    def validate_and_refine(self, state: AgentState) -> AgentState:
        """Validation and refinement"""
        issues = []
        
        if len(state['description']) < 100:
            issues.append("Description too short")
        
        if len(state['citations']) == 0:
            issues.append("No citations")
        
        if len(state['data_actions']) == 0:
            issues.append("No data actions")
        
        if len(state['user_evidence']) == 0 and len(state['system_evidence']) == 0:
            issues.append("No evidence")
        
        if not state.get('classification'):
            issues.append("No classification")
        
        if issues and state['iteration'] < state['max_iterations']:
            prompt = f"""Refine this analysis to address these issues: {', '.join(issues)}

Current state:
- Description: {state['description'][:400]}
- Citations: {len(state['citations'])}
- Actions: {len(state['data_actions'])}

Original text:
{state['chunk_text'][:2000]}

Extract what was missed. Provide complete sentences. Return JSON with additional description, citations, data_actions, and evidence."""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                refinements = self._extract_json(response.content)
                
                if refinements.get('description'):
                    state['description'] += " " + refinements['description']
                
                for cite in refinements.get('citations', []):
                    if cite.get('text'):
                        state['citations'].append(cite)
                
                for action in refinements.get('data_actions', []):
                    state['data_actions'].append(action)
                
                for evidence in refinements.get('user_evidence', []):
                    state['user_evidence'].append(evidence)
                
                for evidence in refinements.get('system_evidence', []):
                    state['system_evidence'].append(evidence)
                
                if refinements.get('classification') and not state.get('classification'):
                    state['classification'] = refinements['classification']
                
                state['iteration'] += 1
                state['next_step'] = 'validate'
            except Exception as e:
                logger.error(f"Refinement error: {e}")
                state['next_step'] = 'end'
        else:
            state['next_step'] = 'end'
        
        return state
    
    def _extract_json(self, text: str) -> Dict[str, Any]:
        """Extract JSON from response"""
        try:
            return json.loads(text)
        except:
            pass
        
        match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
        
        pattern = r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}'
        matches = re.findall(pattern, text, re.DOTALL)
        for match in sorted(matches, key=len, reverse=True):
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and len(parsed) > 2:
                    return parsed
            except:
                continue
        
        return {}
    
    def _consolidate_expert_findings(self, state: AgentState, expert_results: Dict[str, Any]):
        """Consolidate expert findings"""
        for expert_role, results in expert_results.items():
            if 'error' in results:
                continue
            
            for action in results.get('data_actions', []):
                if action.get('description'):
                    state['data_actions'].append(action)
            
            for req in results.get('requirements', []):
                if req.get('description'):
                    perspective = req.get('perspective', 'user')
                    if perspective == 'user':
                        state['user_evidence'].append(req)
                    else:
                        state['system_evidence'].append(req)
            
            for constraint in results.get('constraints', []):
                if constraint.get('description'):
                    state['constraints'].append(constraint)


# ============================================================================
# ENHANCED ANALYZER
# ============================================================================

class EnhancedLegalDocumentAnalyzer:
    """Enhanced analyzer with FalkorDB and embeddings"""
    
    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        if not self.config.API_KEY:
            raise ValueError("OPENAI_API_KEY required")
        
        # Initialize LLM without temperature/max_tokens
        self.llm = ChatOpenAI(
            model=self.config.CHAT_MODEL,
            openai_api_key=self.config.API_KEY,
            openai_api_base=self.config.BASE_URL
        )
        
        self.chunker = DocumentChunker(
            chunk_size=getattr(self.config, 'CHUNK_SIZE', 4000),
            chunk_overlap=getattr(self.config, 'OVERLAP_SIZE', 300),
            respect_boundaries=True
        )
        
        # Initialize embedding service
        self.embedding_service = EmbeddingService(
            api_key=self.config.API_KEY,
            model=self.config.EMBEDDING_MODEL
        )
        
        # Initialize FalkorDB
        falkordb_host = getattr(self.config, 'FALKORDB_HOST', 'localhost')
        falkordb_port = getattr(self.config, 'FALKORDB_PORT', 6379)
        
        self.kg = FalkorDBKnowledgeGraph(
            host=falkordb_host,
            port=falkordb_port,
            embedding_service=self.embedding_service
        )
        
        self.agent = LegalAnalysisAgent(self.llm, self.kg)
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build LangGraph workflow"""
        workflow = StateGraph(AgentState)
        
        workflow.add_node("chain_of_thought", self.agent.chain_of_thought_reasoning)
        workflow.add_node("mixture_of_experts", self.agent.mixture_of_experts)
        workflow.add_node("tree_of_thought", self.agent.tree_of_thought)
        workflow.add_node("knowledge_graph", self.agent.knowledge_graph_integration)
        workflow.add_node("validate", self.agent.validate_and_refine)
        
        workflow.set_entry_point("chain_of_thought")
        
        workflow.add_conditional_edges(
            "chain_of_thought",
            lambda state: state['next_step']
        )
        workflow.add_conditional_edges(
            "mixture_of_experts",
            lambda state: state['next_step']
        )
        workflow.add_conditional_edges(
            "tree_of_thought",
            lambda state: state['next_step']
        )
        workflow.add_conditional_edges(
            "knowledge_graph",
            lambda state: state['next_step']
        )
        workflow.add_conditional_edges(
            "validate",
            lambda state: state['next_step'],
            {
                "validate": "validate",
                "end": END
            }
        )
        
        return workflow.compile()
    
    def analyze_chunk(self, chunk: Dict[str, Any], rule_name: str,
                     jurisdiction: str, level: int,
                     enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze chunk with LangGraph"""
        
        initial_state = AgentState(
            messages=[],
            chunk_text=chunk['text'],
            chunk_id=chunk['chunk_id'],
            level=level,
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            enterprise_context=enterprise_context,
            description="",
            citations=[],
            data_actions=[],
            user_evidence=[],
            system_evidence=[],
            constraints=[],
            enterprise_policies=[],
            classification="",
            classification_reasoning="",
            chain_of_thought=[],
            expert_opinions={},
            thought_tree={},
            kg_nodes=[],
            kg_edges=[],
            next_step="chain_of_thought",
            iteration=0,
            max_iterations=2
        )
        
        try:
            final_state = self.workflow.invoke(initial_state)
            
            return {
                "description": final_state['description'].strip(),
                "citations": final_state['citations'],
                "data_actions": final_state['data_actions'],
                "user_evidence": final_state['user_evidence'],
                "system_evidence": final_state['system_evidence'],
                "constraints": final_state['constraints'],
                "enterprise_policies": final_state['enterprise_policies'],
                "classification": final_state['classification'] or "condition",
                "classification_reasoning": final_state['classification_reasoning'],
                "metadata": {
                    "chunk_id": chunk['chunk_id'],
                    "level": level,
                    "chain_of_thought": final_state['chain_of_thought'],
                    "expert_opinions": final_state['expert_opinions'],
                    "thought_tree": final_state['thought_tree'],
                    "kg_integration": {
                        "nodes": final_state['kg_nodes']
                    }
                }
            }
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            return self._empty_analysis(chunk['chunk_id'], level)
    
    def analyze_document(self, rule_name: str, jurisdiction: str,
                        document_text: str, level: int,
                        enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze document"""
        print(f"\n{'='*60}")
        print(f"Analyzing: {rule_name} (Level {level})")
        print(f"Document: {len(document_text)} chars")
        print(f"Using: FalkorDB + LangGraph + Embeddings")
        print(f"{'='*60}")
        
        chunks = self.chunker.chunk_document(
            text=document_text,
            metadata={"rule_name": rule_name, "jurisdiction": jurisdiction, "level": level}
        )
        
        print(f"Created {len(chunks)} chunks")
        
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            print(f"\nChunk {i+1}/{len(chunks)}...")
            
            analysis = self.analyze_chunk(
                chunk, rule_name, jurisdiction, level, enterprise_context
            )
            
            if analysis:
                chunk_analyses.append(analysis)
                print(f"  ✓ Description: {len(analysis['description'])} chars")
                print(f"  ✓ Citations: {len(analysis['citations'])}")
                print(f"  ✓ Actions: {len(analysis['data_actions'])}")
                print(f"  ✓ User Evidence: {len(analysis['user_evidence'])}")
                print(f"  ✓ System Evidence: {len(analysis['system_evidence'])}")
        
        print(f"\nMerging {len(chunk_analyses)} analyses...")
        final = self._merge_analyses(chunk_analyses, rule_name, jurisdiction, level)
        
        print(f"\n✓ Complete:")
        print(f"  Description: {len(final['description'])} chars")
        print(f"  Citations: {len(final['citations'])}")
        print(f"  Actions: {len(final['data_actions'])}")
        print(f"  User Evidence: {len(final['user_evidence'])}")
        print(f"  System Evidence: {len(final['system_evidence'])}")
        
        return final
    
    def _merge_analyses(self, analyses: List[Dict[str, Any]], rule_name: str,
                       jurisdiction: str, level: int) -> Dict[str, Any]:
        """Merge analyses"""
        if not analyses:
            return self._empty_analysis(0, level)
        
        # Combine descriptions into grammatically correct paragraph
        descriptions = [a['description'] for a in analyses if a['description']]
        combined_desc = " ".join(descriptions).strip()
        
        # Ensure proper sentence structure
        if combined_desc and not combined_desc.endswith('.'):
            combined_desc += '.'
        
        # Deduplicate all components
        all_citations = []
        seen_cites = set()
        for analysis in analyses:
            for cite in analysis.get('citations', []):
                cite_key = cite.get('text', '')[:50]
                if cite_key and cite_key not in seen_cites:
                    all_citations.append(cite)
                    seen_cites.add(cite_key)
        
        all_actions = []
        seen_actions = set()
        for analysis in analyses:
            for action in analysis.get('data_actions', []):
                action_key = (action.get('type', ''), action.get('description', '')[:50])
                if action_key[1] and action_key not in seen_actions:
                    all_actions.append(action)
                    seen_actions.add(action_key)
        
        all_user_evidence = []
        seen_user = set()
        for analysis in analyses:
            for evidence in analysis.get('user_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_user:
                    all_user_evidence.append(evidence)
                    seen_user.add(evidence_key)
        
        all_system_evidence = []
        seen_system = set()
        for analysis in analyses:
            for evidence in analysis.get('system_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_system:
                    all_system_evidence.append(evidence)
                    seen_system.add(evidence_key)
        
        all_constraints = []
        seen_constraints = set()
        for analysis in analyses:
            for constraint in analysis.get('constraints', []):
                constraint_key = (constraint.get('type', ''), constraint.get('description', '')[:50])
                if constraint_key[1] and constraint_key not in seen_constraints:
                    all_constraints.append(constraint)
                    seen_constraints.add(constraint_key)
        
        classifications = [a.get('classification', '') for a in analyses if a.get('classification')]
        final_classification = "condition"
        if "restriction" in classifications:
            final_classification = "restriction"
        elif classifications:
            final_classification = classifications[0]
        
        reasonings = [a.get('classification_reasoning', '') for a in analyses if a.get('classification_reasoning')]
        combined_reasoning = " ".join(reasonings)
        
        return {
            "description": combined_desc,
            "citations": all_citations,
            "data_actions": all_actions,
            "user_evidence": all_user_evidence,
            "system_evidence": all_system_evidence,
            "constraints": all_constraints,
            "enterprise_policies": [],
            "classification": final_classification,
            "classification_reasoning": combined_reasoning,
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "level": level,
                "chunks_processed": len(analyses),
                "total_citations": len(all_citations),
                "total_actions": len(all_actions),
                "total_user_evidence": len(all_user_evidence),
                "total_system_evidence": len(all_system_evidence),
                "total_constraints": len(all_constraints),
                "kg_stats": self.kg.get_statistics()
            }
        }
    
    def _empty_analysis(self, chunk_id: int, level: int) -> Dict[str, Any]:
        """Empty analysis"""
        return {
            "description": "",
            "citations": [],
            "data_actions": [],
            "user_evidence": [],
            "system_evidence": [],
            "constraints": [],
            "enterprise_policies": [],
            "classification": "condition",
            "classification_reasoning": "",
            "metadata": {"chunk_id": chunk_id, "level": level}
        }
    
    def analyze_multi_level(self, rule_name: str, jurisdiction: str,
                           level_1_text: str, level_2_text: str, level_3_text: str,
                           enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Multi-level analysis"""
        print(f"\n{'#'*60}")
        print(f"# MULTI-LEVEL: {rule_name}")
        print(f"# FalkorDB + LangGraph + Embeddings")
        print(f"{'#'*60}")
        
        print(f"\n>>> LEVEL 1: LEGISLATION ({len(level_1_text)} chars)")
        level_1 = self.analyze_document(rule_name, jurisdiction, level_1_text, 1, enterprise_context)
        
        print(f"\n>>> LEVEL 2: GUIDANCE ({len(level_2_text)} chars)")
        level_2 = self.analyze_document(rule_name, jurisdiction, level_2_text, 2, enterprise_context)
        
        print(f"\n>>> LEVEL 3: ENTERPRISE ({len(level_3_text)} chars)")
        level_3 = self.analyze_document(rule_name, jurisdiction, level_3_text, 3, enterprise_context)
        
        combined = {
            "description": " ".join([
                level_1['description'],
                level_2['description'],
                level_3['description']
            ]).strip(),
            "citations": level_1['citations'] + level_2['citations'] + level_3['citations'],
            "data_actions": level_1['data_actions'] + level_2['data_actions'] + level_3['data_actions'],
            "user_evidence": level_1['user_evidence'] + level_2['user_evidence'] + level_3['user_evidence'],
            "system_evidence": level_1['system_evidence'] + level_2['system_evidence'] + level_3['system_evidence'],
            "constraints": level_1['constraints'] + level_2['constraints'] + level_3['constraints'],
            "enterprise_policies": level_3.get('enterprise_policies', []),
            "classification": level_1['classification'],
            "classification_reasoning": " ".join([
                level_1.get('classification_reasoning', ''),
                level_2.get('classification_reasoning', ''),
                level_3.get('classification_reasoning', '')
            ]).strip(),
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "enterprise_context": enterprise_context,
                "level_1_chunks": level_1['metadata']['chunks_processed'],
                "level_2_chunks": level_2['metadata']['chunks_processed'],
                "level_3_chunks": level_3['metadata']['chunks_processed'],
                "total_citations": len(level_1['citations']) + len(level_2['citations']) + len(level_3['citations']),
                "total_actions": len(level_1['data_actions']) + len(level_2['data_actions']) + len(level_3['data_actions']),
                "kg_stats": self.kg.get_statistics()
            }
        }
        
        print(f"\n{'#'*60}")
        print(f"# COMPLETE")
        print(f"{'#'*60}")
        print(f"  Description: {len(combined['description'])} chars")
        print(f"  Citations: {len(combined['citations'])}")
        print(f"  Actions: {len(combined['data_actions'])}")
        print(f"  User Evidence: {len(combined['user_evidence'])}")
        print(f"  System Evidence: {len(combined['system_evidence'])}")
        print(f"  Constraints: {len(combined['constraints'])}")
        
        kg_stats = combined['metadata']['kg_stats']
        if kg_stats:
            print(f"  FalkorDB: {kg_stats}")
        
        return combined
