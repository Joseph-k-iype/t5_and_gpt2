"""
ISO 11179 Compliant Data Enrichment and Mapping System with Memory
===================================================================
This system enriches field names according to ISO/IEC 11179-1:2023 standards,
with procedural and episodic memory to learn from past processing and improve over time.
"""

import json
import os
import pandas as pd
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Sequence
import numpy as np
from openai import OpenAI
import time
from datetime import datetime
from collections import defaultdict

# LangChain and LangGraph imports
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_REASONING_MODEL = "o3-mini"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"

openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

INPUT_JSON_PATH = "cib_long_json.json"
EXCEL_PATH = "pbt.xlsx"
ACRONYM_CSV_PATH = "acronyms.csv"
OUTPUT_CSV_PATH = "enriched_mapped_output.csv"
CACHE_JSON_PATH = "mapping_cache.json"
PROCEDURAL_MEMORY_PATH = "procedural_memory.json"
EPISODIC_MEMORY_PATH = "episodic_memory.json"

# Global memory stores
MAPPING_CACHE = {}
PROCEDURAL_MEMORY = {
    "prefix_patterns": {},  # Maps prefixes to likely objects
    "suffix_patterns": {},  # Maps suffixes to likely properties
    "enrichment_rules": [],  # Common enrichment transformations
    "pii_patterns": {},  # PII classification patterns
    "modelling_decisions": [],  # When modelling was necessary vs unnecessary
    "high_confidence_strategies": defaultdict(int),  # Which strategies work best
    "acronym_patterns": {}  # Common acronym usage patterns
}
EPISODIC_MEMORY = []  # List of processing episodes


# ============================================================================
# CUSTOM EMBEDDINGS CLASS FOR OPENAI API
# ============================================================================

class OpenAIEmbeddings(Embeddings):
    """Custom embeddings class that uses OpenAI API directly."""
    
    def __init__(self, model: str = OPENAI_EMBEDDING_MODEL):
        self.model = model
        self.client = openai_client
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using OpenAI API."""
        embeddings = []
        total = len(texts)
        for idx, text in enumerate(texts, 1):
            print(f"      üìä Embedding document {idx}/{total}")
            embedding = self.embed_query(text)
            embeddings.append(embedding)
            time.sleep(0.1)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using OpenAI API."""
        truncated_text = text[:8000]
        print(f"      üîπ Generating embedding for: {truncated_text[:80]}...")
        response = self.client.embeddings.create(
            model=self.model,
            input=[truncated_text]
        )
        print(f"      ‚úì Embedding generated (dimension: {len(response.data[0].embedding)})")
        return response.data[0].embedding


# ============================================================================
# MEMORY MANAGEMENT FUNCTIONS
# ============================================================================

def load_procedural_memory():
    """Load procedural memory (learned patterns and rules) from JSON."""
    global PROCEDURAL_MEMORY
    if os.path.exists(PROCEDURAL_MEMORY_PATH):
        try:
            with open(PROCEDURAL_MEMORY_PATH, 'r') as f:
                loaded = json.load(f)
                # Convert defaultdict for strategies
                loaded['high_confidence_strategies'] = defaultdict(int, loaded.get('high_confidence_strategies', {}))
                PROCEDURAL_MEMORY = loaded
            
            total_patterns = (
                len(PROCEDURAL_MEMORY['prefix_patterns']) +
                len(PROCEDURAL_MEMORY['suffix_patterns']) +
                len(PROCEDURAL_MEMORY['enrichment_rules']) +
                len(PROCEDURAL_MEMORY['pii_patterns'])
            )
            print(f"   ‚Üí Loaded procedural memory: {total_patterns} patterns learned")
            print(f"      ‚Ä¢ Prefix patterns: {len(PROCEDURAL_MEMORY['prefix_patterns'])}")
            print(f"      ‚Ä¢ Suffix patterns: {len(PROCEDURAL_MEMORY['suffix_patterns'])}")
            print(f"      ‚Ä¢ Enrichment rules: {len(PROCEDURAL_MEMORY['enrichment_rules'])}")
            print(f"      ‚Ä¢ PII patterns: {len(PROCEDURAL_MEMORY['pii_patterns'])}")
            print(f"      ‚Ä¢ Modelling decisions: {len(PROCEDURAL_MEMORY['modelling_decisions'])}")
        except Exception as e:
            print(f"   ‚Üí Could not load procedural memory: {e}")
            PROCEDURAL_MEMORY = initialize_procedural_memory()
    else:
        print(f"   ‚Üí No procedural memory found, starting fresh")
        PROCEDURAL_MEMORY = initialize_procedural_memory()


def initialize_procedural_memory():
    """Initialize empty procedural memory structure."""
    return {
        "prefix_patterns": {},
        "suffix_patterns": {},
        "enrichment_rules": [],
        "pii_patterns": {},
        "modelling_decisions": [],
        "high_confidence_strategies": defaultdict(int),
        "acronym_patterns": {}
    }


def save_procedural_memory():
    """Save procedural memory to JSON file."""
    try:
        # Convert defaultdict to regular dict for JSON serialization
        save_data = dict(PROCEDURAL_MEMORY)
        save_data['high_confidence_strategies'] = dict(save_data['high_confidence_strategies'])
        
        with open(PROCEDURAL_MEMORY_PATH, 'w') as f:
            json.dump(save_data, f, indent=2)
        print(f"   ‚Üí Saved procedural memory: {len(PROCEDURAL_MEMORY['prefix_patterns'])} patterns")
    except Exception as e:
        print(f"   ‚ö† Warning: Could not save procedural memory: {e}")


def load_episodic_memory():
    """Load episodic memory (specific processing episodes) from JSON."""
    global EPISODIC_MEMORY
    if os.path.exists(EPISODIC_MEMORY_PATH):
        try:
            with open(EPISODIC_MEMORY_PATH, 'r') as f:
                EPISODIC_MEMORY = json.load(f)
            print(f"   ‚Üí Loaded episodic memory: {len(EPISODIC_MEMORY)} episodes")
        except Exception as e:
            print(f"   ‚Üí Could not load episodic memory: {e}")
            EPISODIC_MEMORY = []
    else:
        print(f"   ‚Üí No episodic memory found, starting fresh")
        EPISODIC_MEMORY = []


def save_episodic_memory():
    """Save episodic memory to JSON file."""
    try:
        with open(EPISODIC_MEMORY_PATH, 'w') as f:
            json.dump(EPISODIC_MEMORY, f, indent=2)
        print(f"   ‚Üí Saved episodic memory: {len(EPISODIC_MEMORY)} episodes")
    except Exception as e:
        print(f"   ‚ö† Warning: Could not save episodic memory: {e}")


def update_procedural_memory(
    original_field: str,
    enriched_name: str,
    object_name: str,
    property_name: str,
    strategy: str,
    confidence: float,
    pii_category: str,
    object_was_modelled: bool,
    property_was_modelled: bool
):
    """Update procedural memory with learned patterns from successful processing."""
    
    # Extract prefix and suffix from original field
    field_lower = original_field.lower()
    
    # Learn prefix patterns (first 3-5 chars)
    for prefix_len in [3, 4, 5]:
        if len(field_lower) >= prefix_len:
            prefix = field_lower[:prefix_len]
            if confidence >= 70:  # Only learn from high-confidence mappings
                if prefix not in PROCEDURAL_MEMORY['prefix_patterns']:
                    PROCEDURAL_MEMORY['prefix_patterns'][prefix] = {}
                if object_name not in PROCEDURAL_MEMORY['prefix_patterns'][prefix]:
                    PROCEDURAL_MEMORY['prefix_patterns'][prefix][object_name] = 0
                PROCEDURAL_MEMORY['prefix_patterns'][prefix][object_name] += 1
    
    # Learn suffix patterns (last 3-5 chars)
    for suffix_len in [3, 4, 5]:
        if len(field_lower) >= suffix_len:
            suffix = field_lower[-suffix_len:]
            if confidence >= 70:
                if suffix not in PROCEDURAL_MEMORY['suffix_patterns']:
                    PROCEDURAL_MEMORY['suffix_patterns'][suffix] = {}
                if property_name not in PROCEDURAL_MEMORY['suffix_patterns'][suffix]:
                    PROCEDURAL_MEMORY['suffix_patterns'][suffix][property_name] = 0
                PROCEDURAL_MEMORY['suffix_patterns'][suffix][property_name] += 1
    
    # Learn enrichment transformation rules
    if confidence >= 80:
        enrichment_rule = {
            "pattern": original_field.lower(),
            "enriched": enriched_name,
            "confidence": confidence
        }
        # Avoid duplicates
        existing = [r for r in PROCEDURAL_MEMORY['enrichment_rules'] if r['pattern'] == enrichment_rule['pattern']]
        if not existing:
            PROCEDURAL_MEMORY['enrichment_rules'].append(enrichment_rule)
            # Keep only top 100 rules
            if len(PROCEDURAL_MEMORY['enrichment_rules']) > 100:
                PROCEDURAL_MEMORY['enrichment_rules'] = sorted(
                    PROCEDURAL_MEMORY['enrichment_rules'],
                    key=lambda x: x['confidence'],
                    reverse=True
                )[:100]
    
    # Learn PII patterns
    field_keywords = field_lower.split('_')
    for keyword in field_keywords:
        if len(keyword) > 2 and confidence >= 70:
            if keyword not in PROCEDURAL_MEMORY['pii_patterns']:
                PROCEDURAL_MEMORY['pii_patterns'][keyword] = {}
            if pii_category not in PROCEDURAL_MEMORY['pii_patterns'][keyword]:
                PROCEDURAL_MEMORY['pii_patterns'][keyword][pii_category] = 0
            PROCEDURAL_MEMORY['pii_patterns'][keyword][pii_category] += 1
    
    # Learn modelling decisions
    if object_was_modelled or property_was_modelled:
        modelling_decision = {
            "field": original_field,
            "object_modelled": object_was_modelled,
            "property_modelled": property_was_modelled,
            "object": object_name,
            "property": property_name,
            "confidence": confidence,
            "was_necessary": confidence >= 70  # High confidence suggests good modelling decision
        }
        PROCEDURAL_MEMORY['modelling_decisions'].append(modelling_decision)
        # Keep only recent 50 decisions
        if len(PROCEDURAL_MEMORY['modelling_decisions']) > 50:
            PROCEDURAL_MEMORY['modelling_decisions'] = PROCEDURAL_MEMORY['modelling_decisions'][-50:]
    
    # Track successful strategies
    if confidence >= 70:
        PROCEDURAL_MEMORY['high_confidence_strategies'][strategy] += 1


def add_episodic_memory(
    eim_id: str,
    original_field: str,
    enriched_name: str,
    enriched_description: str,
    object_name: str,
    property_name: str,
    strategy: str,
    confidence: float,
    object_was_modelled: bool,
    property_was_modelled: bool,
    pii_category: str,
    enrichment_confidence: float,
    mapping_confidence: float,
    pii_confidence: float,
    processing_status: str
):
    """Add a new episode to episodic memory."""
    episode = {
        "timestamp": datetime.now().isoformat(),
        "eim_id": eim_id,
        "original_field": original_field,
        "enriched_name": enriched_name,
        "enriched_description": enriched_description[:200],  # Truncate for space
        "mapping": {
            "object": object_name,
            "property": property_name,
            "object_was_modelled": object_was_modelled,
            "property_was_modelled": property_was_modelled,
            "strategy": strategy,
            "confidence": confidence
        },
        "pii_category": pii_category,
        "confidence_scores": {
            "enrichment": enrichment_confidence,
            "mapping": mapping_confidence,
            "pii": pii_confidence
        },
        "status": processing_status
    }
    
    EPISODIC_MEMORY.append(episode)
    
    # Keep only recent 1000 episodes to manage memory
    if len(EPISODIC_MEMORY) > 1000:
        EPISODIC_MEMORY[:] = EPISODIC_MEMORY[-1000:]


def query_episodic_memory(field_name: str, top_k: int = 5) -> List[Dict]:
    """Query episodic memory for similar past processing episodes."""
    field_lower = field_name.lower()
    
    similar_episodes = []
    for episode in EPISODIC_MEMORY:
        original = episode['original_field'].lower()
        
        # Simple similarity: check word overlap
        field_words = set(field_lower.split('_'))
        episode_words = set(original.split('_'))
        
        if field_words and episode_words:
            overlap = len(field_words & episode_words) / max(len(field_words), len(episode_words))
            
            if overlap > 0.3:  # At least 30% word overlap
                similar_episodes.append({
                    "episode": episode,
                    "similarity": overlap
                })
    
    # Sort by similarity and return top_k
    similar_episodes.sort(key=lambda x: x['similarity'], reverse=True)
    return similar_episodes[:top_k]


def get_procedural_insights(field_name: str) -> Dict[str, Any]:
    """Get procedural insights (learned patterns) for a field."""
    field_lower = field_name.lower()
    insights = {
        "likely_objects": [],
        "likely_properties": [],
        "likely_pii_category": None,
        "similar_enrichments": [],
        "best_strategies": []
    }
    
    # Check prefix patterns
    for prefix_len in [3, 4, 5]:
        if len(field_lower) >= prefix_len:
            prefix = field_lower[:prefix_len]
            if prefix in PROCEDURAL_MEMORY['prefix_patterns']:
                objects = PROCEDURAL_MEMORY['prefix_patterns'][prefix]
                sorted_objects = sorted(objects.items(), key=lambda x: x[1], reverse=True)
                insights['likely_objects'].extend([obj for obj, count in sorted_objects[:3]])
    
    # Check suffix patterns
    for suffix_len in [3, 4, 5]:
        if len(field_lower) >= suffix_len:
            suffix = field_lower[-suffix_len:]
            if suffix in PROCEDURAL_MEMORY['suffix_patterns']:
                properties = PROCEDURAL_MEMORY['suffix_patterns'][suffix]
                sorted_props = sorted(properties.items(), key=lambda x: x[1], reverse=True)
                insights['likely_properties'].extend([prop for prop, count in sorted_props[:3]])
    
    # Check PII patterns
    field_keywords = field_lower.split('_')
    pii_votes = defaultdict(int)
    for keyword in field_keywords:
        if keyword in PROCEDURAL_MEMORY['pii_patterns']:
            for category, count in PROCEDURAL_MEMORY['pii_patterns'][keyword].items():
                pii_votes[category] += count
    
    if pii_votes:
        insights['likely_pii_category'] = max(pii_votes.items(), key=lambda x: x[1])[0]
    
    # Check enrichment rules
    for rule in PROCEDURAL_MEMORY['enrichment_rules']:
        if rule['pattern'] in field_lower or field_lower in rule['pattern']:
            insights['similar_enrichments'].append(rule)
    
    # Get best strategies
    if PROCEDURAL_MEMORY['high_confidence_strategies']:
        sorted_strategies = sorted(
            PROCEDURAL_MEMORY['high_confidence_strategies'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        insights['best_strategies'] = [strategy for strategy, count in sorted_strategies[:3]]
    
    return insights


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def call_openai_with_retry(messages: List[Dict], max_retries: int = 10) -> str:
    """Call OpenAI API with retry logic until success."""
    print(f"      ü§ñ Calling {OPENAI_REASONING_MODEL} model...")
    for attempt in range(max_retries):
        try:
            response = openai_client.chat.completions.create(
                model=OPENAI_REASONING_MODEL,
                messages=messages
            )
            print(f"      ‚úì LLM response received (tokens: ~{len(response.choices[0].message.content.split())} words)")
            return response.choices[0].message.content
        except Exception as e:
            print(f"      ‚ö† Attempt {attempt + 1}/{max_retries} failed: {e}")
            time.sleep(2 ** attempt)
            if attempt == max_retries - 1:
                raise
    raise Exception(f"Failed after {max_retries} attempts")


def load_cache():
    """Load the mapping cache from JSON file."""
    global MAPPING_CACHE
    if os.path.exists(CACHE_JSON_PATH):
        try:
            with open(CACHE_JSON_PATH, 'r') as f:
                MAPPING_CACHE = json.load(f)
            print(f"   ‚Üí Loaded {len(MAPPING_CACHE)} cached mappings")
        except Exception as e:
            print(f"   ‚Üí Could not load cache: {e}")
            MAPPING_CACHE = {}
    else:
        print(f"   ‚Üí No cache file found")
        MAPPING_CACHE = {}


def save_cache():
    """Save the mapping cache to JSON file."""
    try:
        with open(CACHE_JSON_PATH, 'w') as f:
            json.dump(MAPPING_CACHE, f, indent=2)
    except Exception as e:
        print(f"   ‚ö† Warning: Could not save cache: {e}")


def check_cache(enriched_name: str, enriched_description: str, confidence_threshold: float = 70.0) -> Dict:
    """Check if similar mapping exists in cache with good confidence."""
    cache_key = f"{enriched_name.lower()}|{enriched_description[:100].lower()}"
    
    if cache_key in MAPPING_CACHE:
        cached = MAPPING_CACHE[cache_key]
        if cached.get('confidence', 0) >= confidence_threshold:
            print(f"      üíæ Cache HIT: {cached['object']}.{cached['property']} (conf: {cached['confidence']})")
            return cached
    
    for key, cached in MAPPING_CACHE.items():
        cached_name = key.split('|')[0]
        name_words = set(enriched_name.lower().split())
        cached_words = set(cached_name.split())
        overlap = len(name_words & cached_words) / max(len(name_words), len(cached_words), 1)
        
        if overlap > 0.7 and cached.get('confidence', 0) >= confidence_threshold:
            print(f"      üíæ Cache HIT (similar): {cached['object']}.{cached['property']} (overlap: {overlap:.2f})")
            return cached
    
    print(f"      üíæ Cache MISS")
    return None


def add_to_cache(enriched_name: str, enriched_description: str, obj: str, prop: str, confidence: float, was_modelled_obj: bool, was_modelled_prop: bool):
    """Add a successful mapping to the cache."""
    cache_key = f"{enriched_name.lower()}|{enriched_description[:100].lower()}"
    MAPPING_CACHE[cache_key] = {
        'object': obj,
        'property': prop,
        'confidence': confidence,
        'object_was_modelled': was_modelled_obj,
        'property_was_modelled': was_modelled_prop
    }


# ============================================================================
# ISO 11179 STANDARDS SYSTEM PROMPTS
# ============================================================================

ISO_11179_NAMING_PRINCIPLES = """
ISO/IEC 11179-1:2023 NAMING PRINCIPLES: Use Title Case, proper spacing, semantic clarity.
Structure: [Object Class] [Qualifier(s)] [Property] [Representation]
Derive object class from FIELD NAME, not application context.
OUTPUT: Enriched field name only, no explanation.
"""

ISO_11179_DEFINITION_PRINCIPLES = """
ISO/IEC 11179-4 DEFINITION PRINCIPLES: State what concept IS. Include essential characteristics,
context, constraints. Use complete sentences with proper grammar.
OUTPUT: Definition text only, no preamble.
"""

PII_CLASSIFICATION_FRAMEWORK = """
PII CATEGORIES: NON PERSONAL DATA, PERSONAL DATA, SENSITIVE PERSONAL DATA
OUTPUT: JSON with pii_category, is_pii, regulatory_considerations, detailed_rationale
"""

ONTOLOGY_MODELLING_PRINCIPLES = """
MODELLING PRINCIPLES (ISO 11179, BIAN, FIBO):
- Use clear, unambiguous names following [Object Class] [Property] structure
- Follow BIAN service domain naming conventions
- Use FIBO foundational concepts when applicable
- Check standard ontology patterns (Dublin Core, Schema.org, W3C PROV, SKOS)
OUTPUT: JSON with modelled_object, modelled_property, ontology_alignment, modelling_rationale
"""


# ============================================================================
# AGENT STATE DEFINITION
# ============================================================================

class EnrichmentState(TypedDict):
    """State schema for the enrichment and mapping agent system."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    current_field: Dict[str, Any]
    enriched_name: str
    enriched_description: str
    object_name: str
    property_name: str
    object_was_modelled: bool
    property_was_modelled: bool
    pii_classification: Dict[str, Any]
    enrichment_rationale: str
    mapping_rationale: str
    enrichment_confidence: Dict[str, Any]
    mapping_confidence: Dict[str, Any]
    pii_confidence: Dict[str, Any]
    intersection_vector_store: Any
    object_vector_store: Any
    property_vector_store: Any
    all_objects: List[str]
    all_properties: List[str]
    objects_dict: Dict[str, List[str]]
    acronym_dict: Dict[str, str]
    expert_opinions: List[str]
    reasoning_chain: List[str]
    procedural_insights: Dict[str, Any]
    similar_episodes: List[Dict]
    mapping_strategy: str
    mapping_confidence_score: float


# ============================================================================
# TOOLS FOR REACT AGENTS
# ============================================================================

@tool
def iso_11179_name_enrichment_expert(field_name: str, field_context: str, acronym_list: str, procedural_insights: str) -> str:
    """Expert agent for enriching field names with procedural memory insights."""
    prompt = f"""You are an ISO 11179 metadata naming standards expert.

TASK: Transform technical field name into ISO 11179 compliant name.

ORIGINAL: {field_name}
CONTEXT: {field_context}
ACRONYMS: {acronym_list}

PROCEDURAL MEMORY INSIGHTS (learned patterns from past processing):
{procedural_insights}

{ISO_11179_NAMING_PRINCIPLES}

Consider learned patterns but prioritize semantic correctness.
OUTPUT: Enriched name only."""

    messages = [{"role": "user", "content": prompt}]
    return call_openai_with_retry(messages).strip()


@tool
def iso_11179_definition_expert(enriched_name: str, original_name: str, context: str, acronym_list: str) -> str:
    """Expert agent for creating ISO 11179 compliant definitions."""
    prompt = f"""You are an ISO 11179 definition expert.

ENRICHED NAME: {enriched_name}
ORIGINAL: {original_name}
CONTEXT: {context}
ACRONYMS: {acronym_list}

{ISO_11179_DEFINITION_PRINCIPLES}"""

    messages = [{"role": "user", "content": prompt}]
    return call_openai_with_retry(messages).strip()


@tool
def three_level_semantic_mapper(
    enriched_name: str,
    enriched_description: str,
    intersection_matches: str,
    object_matches: str,
    property_matches: str,
    all_objects_list: str,
    all_properties_list: str,
    objects_dict_str: str,
    procedural_insights: str,
    similar_episodes: str
) -> str:
    """Three-level mapper with procedural and episodic memory."""
    prompt = f"""You are a semantic data modeling expert with ISO 11179, BIAN, and FIBO expertise.

TASK: Map enriched element using three-level strategy or model new terms.

ENRICHED: {enriched_name}
DESCRIPTION: {enriched_description}

LEVEL 1 - INTERSECTION: {intersection_matches}
LEVEL 2 - OBJECTS: {object_matches}
LEVEL 3 - PROPERTIES: {property_matches}

AVAILABLE OBJECTS: {all_objects_list}
AVAILABLE PROPERTIES: {all_properties_list}
OBJECT-PROPERTY MAPPING: {objects_dict_str}

PROCEDURAL MEMORY (learned patterns):
{procedural_insights}

EPISODIC MEMORY (similar past cases):
{similar_episodes}

STRATEGY:
1. Check intersection matches - good semantic fit?
2. If object good but property weak: keep object, search all properties
3. If property good but object weak: keep property, search all objects
4. If all weak: MODEL new terms using ISO 11179/BIAN/FIBO

{ONTOLOGY_MODELLING_PRINCIPLES}

OUTPUT JSON:
{{
    "best_object": "<object>",
    "best_property": "<property>",
    "object_was_modelled": <bool>,
    "property_was_modelled": <bool>,
    "confidence": <0-100>,
    "strategy_used": "<intersection|object_fixed_property_search|property_fixed_object_search|full_modelling>",
    "ontology_alignment": ["<standards>"],
    "reasoning": "<explanation>"
}}"""

    messages = [{"role": "user", "content": prompt}]
    return call_openai_with_retry(messages).strip()


@tool
def pii_classification_expert(enriched_name: str, enriched_description: str, object_name: str, property_name: str, procedural_insights: str) -> str:
    """PII classification expert with procedural memory."""
    prompt = f"""You are a data privacy and PII classification expert.

DATA ELEMENT:
- Name: {enriched_name}
- Description: {enriched_description}
- Object: {object_name}
- Property: {property_name}

PROCEDURAL MEMORY (learned PII patterns):
{procedural_insights}

{PII_CLASSIFICATION_FRAMEWORK}

Consider learned patterns but prioritize regulatory correctness.
OUTPUT: JSON only."""

    messages = [{"role": "user", "content": prompt}]
    return call_openai_with_retry(messages).strip()


# ============================================================================
# SUPERVISOR AGENT
# ============================================================================

def supervisor_validation_agent(state: EnrichmentState) -> EnrichmentState:
    """Supervisor with memory-aware validation."""
    prompt = f"""You are a SUPERVISOR AGENT validating enrichment and mapping.

ORIGINAL: {state['current_field'].get('Field Name', 'N/A')}
ENRICHED: {state['enriched_name']}
MAPPING: {state['object_name']}.{state['property_name']}
MODELLED: Object={state.get('object_was_modelled', False)}, Property={state.get('property_was_modelled', False)}
STRATEGY: {state.get('mapping_strategy', 'unknown')}

PROCEDURAL INSIGHTS: {json.dumps(state.get('procedural_insights', {}), indent=2)}
SIMILAR EPISODES: {json.dumps(state.get('similar_episodes', [])[:2], indent=2)}

VALIDATION:
1. Enrichment quality (ISO 11179 compliance)
2. Mapping correctness (semantic fit)
3. Modelling necessity (was it truly needed?)
4. PII classification appropriateness
5. Consistency with learned patterns

CONFIDENCE SCORING (per aspect):
- 3 supporting reasons with weights (0.0-1.0)
- 3 contradictory reasons with weights (0.0-1.0)
- Score: ((sum_supporting - sum_contradictory) / (sum_supporting + sum_contradictory)) * 100

FOR MODELLING: High confidence (>80) only if truly no suitable PBT term exists.

OUTPUT JSON:
{{
    "validation_passed": <bool>,
    "quality_score": <1-10>,
    "enrichment_rationale": "<explanation>",
    "mapping_rationale": "<explanation with modelling justification>",
    "concerns": ["<concerns>"],
    "recommendations": ["<recommendations>"],
    "final_approval": <bool>,
    "enrichment_confidence": {{
        "supporting_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "contradictory_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "confidence_score": <0-100>,
        "confidence_rationale": "<explanation>"
    }},
    "mapping_confidence": {{
        "supporting_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "contradictory_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "confidence_score": <0-100>,
        "confidence_rationale": "<explanation>"
    }},
    "pii_confidence": {{
        "supporting_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "contradictory_reasons": [{{"reason": "<reason>", "weight": <0.0-1.0>}}, ...],
        "confidence_score": <0-100>,
        "confidence_rationale": "<explanation>"
    }}
}}"""

    messages = [{"role": "user", "content": prompt}]
    supervisor_result = json.loads(call_openai_with_retry(messages).strip())
    
    state['enrichment_rationale'] = supervisor_result.get('enrichment_rationale', '')
    state['mapping_rationale'] = supervisor_result.get('mapping_rationale', '')
    state['enrichment_confidence'] = supervisor_result.get('enrichment_confidence', {})
    state['mapping_confidence'] = supervisor_result.get('mapping_confidence', {})
    state['pii_confidence'] = supervisor_result.get('pii_confidence', {})
    
    state['reasoning_chain'].append(
        f"SUPERVISOR: Quality {supervisor_result.get('quality_score', 0)}/10. "
        f"Conf: E={state['enrichment_confidence'].get('confidence_score', 0)}, "
        f"M={state['mapping_confidence'].get('confidence_score', 0)}, "
        f"P={state['pii_confidence'].get('confidence_score', 0)}"
    )
    
    return state


# ============================================================================
# WORKFLOW NODE
# ============================================================================

def enrichment_coordinator_node(state: EnrichmentState) -> EnrichmentState:
    """Coordinator with memory integration."""
    current_field = state['current_field']
    field_name = current_field.get('Field Name', '')
    context = f"App: {current_field.get('Application Name', '')}, Desc: {current_field.get('Application Description', '')}"
    
    print(f"\n{'='*80}")
    print(f"PROCESSING: {field_name}")
    print(f"{'='*80}")
    
    state['reasoning_chain'] = []
    state['expert_opinions'] = []
    state['object_was_modelled'] = False
    state['property_was_modelled'] = False
    state['mapping_strategy'] = 'unknown'
    state['mapping_confidence_score'] = 0
    
    acronym_dict = state.get('acronym_dict', {})
    acronym_list = "\n".join([f"  - {acr} = {exp}" for acr, exp in acronym_dict.items()])
    
    # STEP 0: Query Memory Systems
    print("\n[STEP 0] Querying Memory Systems...")
    procedural_insights = get_procedural_insights(field_name)
    similar_episodes = query_episodic_memory(field_name, top_k=3)
    
    state['procedural_insights'] = procedural_insights
    state['similar_episodes'] = similar_episodes
    
    if procedural_insights['likely_objects']:
        print(f"   üß† Procedural: Likely objects = {procedural_insights['likely_objects'][:3]}")
    if procedural_insights['likely_properties']:
        print(f"   üß† Procedural: Likely properties = {procedural_insights['likely_properties'][:3]}")
    if similar_episodes:
        print(f"   üìö Episodic: Found {len(similar_episodes)} similar past cases")
        for ep in similar_episodes[:2]:
            print(f"      ‚Ä¢ {ep['episode']['original_field']} ‚Üí {ep['episode']['mapping']['object']}.{ep['episode']['mapping']['property']}")
    
    # STEP 1: Enrichment
    print("\n[STEP 1] ISO 11179 Name Enrichment...")
    try:
        enriched_name = iso_11179_name_enrichment_expert.invoke({
            "field_name": field_name,
            "field_context": context,
            "acronym_list": acronym_list,
            "procedural_insights": json.dumps(procedural_insights, indent=2)
        })
        state['enriched_name'] = enriched_name
        print(f"   ‚úì Enriched: {enriched_name}")
    except Exception as e:
        print(f"   ‚ùå Failed: {e}")
        state['enriched_name'] = field_name
    
    # STEP 2: Definition
    print("\n[STEP 2] Definition Generation...")
    try:
        enriched_description = iso_11179_definition_expert.invoke({
            "enriched_name": state['enriched_name'],
            "original_name": field_name,
            "context": context,
            "acronym_list": acronym_list
        })
        state['enriched_description'] = enriched_description
        print(f"   ‚úì Definition: {enriched_description[:80]}...")
    except Exception as e:
        print(f"   ‚ùå Failed: {e}")
        state['enriched_description'] = f"A data element representing {state['enriched_name'].lower()}."
    
    # STEP 3: Check Cache
    print("\n[STEP 3] Checking Cache...")
    cached_result = check_cache(state['enriched_name'], state['enriched_description'])
    
    if cached_result:
        state['object_name'] = cached_result['object']
        state['property_name'] = cached_result['property']
        state['object_was_modelled'] = cached_result.get('object_was_modelled', False)
        state['property_was_modelled'] = cached_result.get('property_was_modelled', False)
        state['mapping_strategy'] = 'cache_hit'
        state['mapping_confidence_score'] = cached_result.get('confidence', 0)
        print(f"   ‚úì Using cache: {state['object_name']}.{state['property_name']}")
    else:
        # STEP 4: Three-Level Mapping
        print("\n[STEP 4] Three-Level Semantic Mapping...")
        try:
            query_text = f"{state['enriched_name']} {state['enriched_description']}"
            
            # Query all three stores
            intersection_store = state['intersection_vector_store']
            intersection_docs = intersection_store.similarity_search_with_score(query_text, k=5)
            intersection_matches = [{"object": d.metadata['object'], "property": d.metadata['property'], "score": float(s)} for d, s in intersection_docs]
            
            object_store = state['object_vector_store']
            object_docs = object_store.similarity_search_with_score(query_text, k=5)
            object_matches = [{"object": d.metadata['object'], "score": float(s)} for d, s in object_docs]
            
            property_store = state['property_vector_store']
            property_docs = property_store.similarity_search_with_score(query_text, k=5)
            property_matches = [{"property": d.metadata['property'], "score": float(s)} for d, s in property_docs]
            
            print(f"   ‚Üí Top intersection: {intersection_matches[0]['object']}.{intersection_matches[0]['property']} ({intersection_matches[0]['score']:.4f})")
            
            # Call mapper with memory
            mapping_result = three_level_semantic_mapper.invoke({
                "enriched_name": state['enriched_name'],
                "enriched_description": state['enriched_description'],
                "intersection_matches": json.dumps(intersection_matches, indent=2),
                "object_matches": json.dumps(object_matches, indent=2),
                "property_matches": json.dumps(property_matches, indent=2),
                "all_objects_list": ", ".join(state['all_objects'][:50]),
                "all_properties_list": ", ".join(state['all_properties'][:50]),
                "objects_dict_str": json.dumps({k: v[:5] for k, v in list(state['objects_dict'].items())[:20]}, indent=2),
                "procedural_insights": json.dumps(procedural_insights, indent=2),
                "similar_episodes": json.dumps([ep['episode'] for ep in similar_episodes], indent=2)
            })
            
            mapping_data = json.loads(mapping_result)
            state['object_name'] = mapping_data.get('best_object', 'Unknown')
            state['property_name'] = mapping_data.get('best_property', 'Unknown')
            state['object_was_modelled'] = mapping_data.get('object_was_modelled', False)
            state['property_was_modelled'] = mapping_data.get('property_was_modelled', False)
            state['mapping_strategy'] = mapping_data.get('strategy_used', 'unknown')
            state['mapping_confidence_score'] = mapping_data.get('confidence', 0)
            
            print(f"   ‚úì Mapped: {state['object_name']}.{state['property_name']}")
            print(f"      Strategy: {state['mapping_strategy']}, Conf: {state['mapping_confidence_score']}")
            print(f"      Modelled: Obj={state['object_was_modelled']}, Prop={state['property_was_modelled']}")
            
            # Cache if good confidence
            if state['mapping_confidence_score'] >= 60:
                add_to_cache(state['enriched_name'], state['enriched_description'], state['object_name'], state['property_name'], state['mapping_confidence_score'], state['object_was_modelled'], state['property_was_modelled'])
            
        except Exception as e:
            print(f"   ‚ùå Mapping failed: {e}")
            state['object_name'] = 'Unknown'
            state['property_name'] = 'Unknown'
    
    # STEP 5: PII Classification
    print("\n[STEP 5] PII Classification...")
    try:
        pii_insights = {"likely_pii": procedural_insights.get('likely_pii_category')}
        pii_result = pii_classification_expert.invoke({
            "enriched_name": state['enriched_name'],
            "enriched_description": state['enriched_description'],
            "object_name": state['object_name'],
            "property_name": state['property_name'],
            "procedural_insights": json.dumps(pii_insights, indent=2)
        })
        pii_data = json.loads(pii_result)
        state['pii_classification'] = pii_data
        print(f"   ‚úì PII: {pii_data.get('pii_category', 'UNKNOWN')}")
    except Exception as e:
        print(f"   ‚ùå Failed: {e}")
        state['pii_classification'] = {"pii_category": "NON PERSONAL DATA", "is_pii": False, "regulatory_considerations": [], "detailed_rationale": "Failed"}
    
    # STEP 6: Supervisor Validation
    print("\n[STEP 6] Supervisor Validation...")
    try:
        state = supervisor_validation_agent(state)
        print(f"   ‚úì Validation complete")
    except Exception as e:
        print(f"   ‚ö† Validation failed: {e}")
        state['enrichment_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
        state['mapping_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
        state['pii_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
    
    # STEP 7: Update Memory Systems
    print("\n[STEP 7] Updating Memory Systems...")
    try:
        # Update procedural memory
        update_procedural_memory(
            field_name,
            state['enriched_name'],
            state['object_name'],
            state['property_name'],
            state['mapping_strategy'],
            state['mapping_confidence_score'],
            state['pii_classification'].get('pii_category', 'NON PERSONAL DATA'),
            state['object_was_modelled'],
            state['property_was_modelled']
        )
        
        # Add episodic memory
        add_episodic_memory(
            current_field.get('EIM ID', ''),
            field_name,
            state['enriched_name'],
            state['enriched_description'],
            state['object_name'],
            state['property_name'],
            state['mapping_strategy'],
            state['mapping_confidence_score'],
            state['object_was_modelled'],
            state['property_was_modelled'],
            state['pii_classification'].get('pii_category', 'NON PERSONAL DATA'),
            state['enrichment_confidence'].get('confidence_score', 0),
            state['mapping_confidence'].get('confidence_score', 0),
            state['pii_confidence'].get('confidence_score', 0),
            'SUCCESS'
        )
        print(f"   ‚úì Memory updated")
    except Exception as e:
        print(f"   ‚ö† Memory update failed: {e}")
    
    print(f"\n{'='*80}\n")
    return state


def should_continue(state: EnrichmentState) -> Literal["continue", "end"]:
    return "end"


# ============================================================================
# WORKFLOW
# ============================================================================

def create_enrichment_graph():
    workflow = StateGraph(EnrichmentState)
    workflow.add_node("enrichment_coordinator", enrichment_coordinator_node)
    workflow.set_entry_point("enrichment_coordinator")
    workflow.add_conditional_edges("enrichment_coordinator", should_continue, {"continue": "enrichment_coordinator", "end": END})
    return workflow.compile()


# ============================================================================
# MAIN PROCESSING
# ============================================================================

def process_data_enrichment_and_mapping():
    print("="*80)
    print("ISO 11179 DATA ENRICHMENT WITH MEMORY SYSTEMS")
    print("="*80)
    
    # Load all memory systems
    print("\n[1] Loading Memory Systems...")
    load_cache()
    load_procedural_memory()
    load_episodic_memory()
    
    # Load input
    print("\n[2] Loading Input...")
    with open(INPUT_JSON_PATH, 'r') as f:
        input_data = json.load(f)
    print(f"   ‚Üí {len(input_data)} records")
    
    # Load acronyms
    print("\n[3] Loading Acronyms...")
    acronym_dict = {}
    if os.path.exists(ACRONYM_CSV_PATH):
        acronym_df = pd.read_csv(ACRONYM_CSV_PATH)
        for _, row in acronym_df.iterrows():
            acronym_dict[str(row['acronym']).strip().upper()] = str(row['expansion']).strip()
        print(f"   ‚Üí {len(acronym_dict)} acronyms")
    
    # Load PBT and create THREE vector stores
    print("\n[4] Creating Three Vector Stores...")
    excel_df = pd.read_excel(EXCEL_PATH)
    
    objects_dict = {}
    all_objects = []
    all_properties = []
    
    for _, row in excel_df.iterrows():
        obj = str(row['Object name']).strip()
        prop = str(row['Property name']).strip()
        
        if obj not in objects_dict:
            objects_dict[obj] = []
            all_objects.append(obj)
        if prop not in objects_dict[obj]:
            objects_dict[obj].append(prop)
        if prop not in all_properties:
            all_properties.append(prop)
    
    print(f"   ‚Üí {len(all_objects)} objects, {len(all_properties)} properties")
    
    embeddings = OpenAIEmbeddings()
    
    # Intersection store
    intersection_docs = [Document(page_content=f"{obj} {prop}", metadata={"object": obj, "property": prop}) for obj in all_objects for prop in objects_dict[obj]]
    intersection_store = InMemoryVectorStore.from_documents(intersection_docs, embeddings)
    print(f"   ‚Üí Intersection store: {len(intersection_docs)} docs")
    
    # Object store
    object_docs = [Document(page_content=obj, metadata={"object": obj}) for obj in all_objects]
    object_store = InMemoryVectorStore.from_documents(object_docs, embeddings)
    print(f"   ‚Üí Object store: {len(object_docs)} docs")
    
    # Property store
    property_docs = [Document(page_content=prop, metadata={"property": prop}) for prop in all_properties]
    property_store = InMemoryVectorStore.from_documents(property_docs, embeddings)
    print(f"   ‚Üí Property store: {len(property_docs)} docs")
    
    # Create workflow
    print("\n[5] Initializing Workflow...")
    app = create_enrichment_graph()
    
    # Process records
    print(f"\n[6] Processing Records...")
    
    processed_ids = set()
    resume_mode = False
    if os.path.exists(OUTPUT_CSV_PATH):
        existing_df = pd.read_csv(OUTPUT_CSV_PATH)
        processed_ids = set(existing_df['EIM ID'].tolist())
        print(f"   ‚Üí RESUME: {len(processed_ids)} already processed")
        resume_mode = True
    
    successful = 0
    failed = 0
    skipped = 0
    modelled_obj = 0
    modelled_prop = 0
    
    for idx, record in enumerate(input_data):
        eim_id = record.get("EIM ID", "")
        
        if eim_id in processed_ids:
            skipped += 1
            continue
        
        print(f"\nRecord {idx + 1}/{len(input_data)} (EIM ID: {eim_id})")
        
        try:
            initial_state = {
                "messages": [], "current_field": record, "enriched_name": "", "enriched_description": "",
                "object_name": "", "property_name": "", "object_was_modelled": False, "property_was_modelled": False,
                "pii_classification": {}, "enrichment_rationale": "", "mapping_rationale": "",
                "enrichment_confidence": {}, "mapping_confidence": {}, "pii_confidence": {},
                "intersection_vector_store": intersection_store, "object_vector_store": object_store,
                "property_vector_store": property_store, "all_objects": all_objects, "all_properties": all_properties,
                "objects_dict": objects_dict, "acronym_dict": acronym_dict, "expert_opinions": [],
                "reasoning_chain": [], "procedural_insights": {}, "similar_episodes": [],
                "mapping_strategy": "unknown", "mapping_confidence_score": 0
            }
            
            final_state = app.invoke(initial_state)
            pii_class = final_state['pii_classification']
            
            def format_reasons(reasons_list):
                return " | ".join([f"{i+1}. {r.get('reason', 'N/A')} (w:{r.get('weight', 0)})" for i, r in enumerate(reasons_list)]) if reasons_list else ""
            
            enrich_conf = final_state.get('enrichment_confidence', {})
            map_conf = final_state.get('mapping_confidence', {})
            pii_conf = final_state.get('pii_confidence', {})
            
            result = {
                "EIM ID": eim_id,
                "Application Name": record.get("Application Name", ""),
                "Application Description": record.get("Application Description", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": final_state['enriched_name'],
                "Enriched Description": final_state['enriched_description'],
                "Mapped Object": final_state['object_name'],
                "Mapped Property": final_state['property_name'],
                "Mapping Strategy": final_state.get('mapping_strategy', 'unknown'),
                "Object PBT Was Modelled": final_state.get('object_was_modelled', False),
                "Property PBT Was Modelled": final_state.get('property_was_modelled', False),
                "PII Category": pii_class.get('pii_category', ''),
                "Is PII": pii_class.get('is_pii', False),
                "Regulatory Considerations": ", ".join(pii_class.get('regulatory_considerations', [])),
                "PII Rationale": pii_class.get('detailed_rationale', ''),
                "Enrichment Rationale": final_state['enrichment_rationale'],
                "Mapping Rationale": final_state['mapping_rationale'],
                "Enrichment Confidence Score": enrich_conf.get('confidence_score', 0),
                "Enrichment Supporting Reasons": format_reasons(enrich_conf.get('supporting_reasons', [])),
                "Enrichment Contradictory Reasons": format_reasons(enrich_conf.get('contradictory_reasons', [])),
                "Enrichment Confidence Rationale": enrich_conf.get('confidence_rationale', ''),
                "Mapping Confidence Score": map_conf.get('confidence_score', 0),
                "Mapping Supporting Reasons": format_reasons(map_conf.get('supporting_reasons', [])),
                "Mapping Contradictory Reasons": format_reasons(map_conf.get('contradictory_reasons', [])),
                "Mapping Confidence Rationale": map_conf.get('confidence_rationale', ''),
                "PII Confidence Score": pii_conf.get('confidence_score', 0),
                "PII Supporting Reasons": format_reasons(pii_conf.get('supporting_reasons', [])),
                "PII Contradictory Reasons": format_reasons(pii_conf.get('contradictory_reasons', [])),
                "PII Confidence Rationale": pii_conf.get('confidence_rationale', ''),
                "Original ISR Classification": record.get("ISR Classification", ""),
                "Processing Status": "SUCCESS"
            }
            
            successful += 1
            if result.get("Object PBT Was Modelled", False):
                modelled_obj += 1
            if result.get("Property PBT Was Modelled", False):
                modelled_prop += 1
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            result = {
                "EIM ID": eim_id, "Application Name": record.get("Application Name", ""),
                "Application Description": record.get("Application Description", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": record.get("Field Name", ""), "Enriched Description": f"Error: {str(e)[:200]}",
                "Mapped Object": "Unknown", "Mapped Property": "Unknown", "Mapping Strategy": "error",
                "Object PBT Was Modelled": False, "Property PBT Was Modelled": False,
                "PII Category": "NON PERSONAL DATA", "Is PII": False, "Regulatory Considerations": "",
                "PII Rationale": f"Failed: {str(e)[:200]}", "Enrichment Rationale": "Error", "Mapping Rationale": "Error",
                "Enrichment Confidence Score": 0, "Enrichment Supporting Reasons": "", "Enrichment Contradictory Reasons": "",
                "Enrichment Confidence Rationale": "Failed", "Mapping Confidence Score": 0, "Mapping Supporting Reasons": "",
                "Mapping Contradictory Reasons": "", "Mapping Confidence Rationale": "Failed", "PII Confidence Score": 0,
                "PII Supporting Reasons": "", "PII Contradictory Reasons": "", "PII Confidence Rationale": "Failed",
                "Original ISR Classification": record.get("ISR Classification", ""), "Processing Status": "FAILED"
            }
            failed += 1
        
        # Write incrementally
        result_df = pd.DataFrame([result])
        if not resume_mode and successful + failed == 1:
            result_df.to_csv(OUTPUT_CSV_PATH, index=False, mode='w')
        else:
            result_df.to_csv(OUTPUT_CSV_PATH, index=False, mode='a', header=False)
    
    # Save all memory systems
    print("\n[7] Saving Memory Systems...")
    save_cache()
    save_procedural_memory()
    save_episodic_memory()
    
    print(f"\n{'='*80}")
    print("PROCESSING COMPLETE!")
    print(f"{'='*80}")
    print(f"Total: {len(input_data)}")
    if skipped > 0:
        print(f"Skipped: {skipped}")
    print(f"Success: {successful}")
    print(f"Failed: {failed}")
    print(f"\nüìù MODELLING:")
    print(f"   Objects: {modelled_obj}")
    print(f"   Properties: {modelled_prop}")
    print(f"\nüß† MEMORY:")
    print(f"   Cache: {len(MAPPING_CACHE)} entries")
    print(f"   Procedural patterns: {len(PROCEDURAL_MEMORY['prefix_patterns']) + len(PROCEDURAL_MEMORY['suffix_patterns'])}")
    print(f"   Episodic events: {len(EPISODIC_MEMORY)}")
    print(f"\nOutput: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    process_data_enrichment_and_mapping()
