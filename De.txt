"""
Nodes for the Agentic RAG LangGraph implementation.

This package contains the node implementations for the Agentic RAG graph:
- query_understanding: Analyzes the query and extracts key concepts
- vector_retrieval: Performs vector similarity search
- keyword_retrieval: Performs keyword-based retrieval
- synonym_expansion: Expands queries with synonyms
- relevance_evaluation: Evaluates candidate relevance with LLM
- result_preparation: Prepares final results
"""

from .query_understanding import query_understanding
from .vector_retrieval import vector_retrieval
from .keyword_retrieval import keyword_retrieval
from .synonym_expansion import synonym_expansion
from .relevance_evaluation import relevance_evaluation
from .result_preparation import result_preparation

__all__ = [
    "query_understanding",
    "vector_retrieval",
    "keyword_retrieval",
    "synonym_expansion", 
    "relevance_evaluation",
    "result_preparation"
]





"""
Keyword retrieval node for the Agentic RAG system.

This module implements the keyword retrieval node for the Agentic RAG system,
which retrieves candidate business terms using keyword matching as a fallback
or complementary approach to vector retrieval.
"""

import logging
import time
from typing import Dict, Any, List, Set, Optional

from app.utils.text_processing import (
    extract_tokens, 
    calculate_token_overlap,
    calculate_token_overlap_with_weights, 
    clean_text
)

logger = logging.getLogger(__name__)

async def keyword_retrieval(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Perform keyword-based retrieval as a fallback or complementary approach.
    This helps when vector search doesn't yield good results.
    
    Args:
        state: Current graph state
        
    Returns:
        Updated state with keyword retrieval results
    """
    element_name = state.get('element_name', 'Unknown')
    logger.info(f"Performing keyword retrieval for: {element_name}")
    
    try:
        # Get business term manager
        bt_manager = state.get("_bt_manager")
        if not bt_manager:
            raise ValueError("Business term manager not available in state")
        
        # Extract query info
        element_name = state.get('element_name', '')
        element_description = state.get('element_description', '')
        
        # Check if we have query info from previous node
        query_info = state.get('query', {})
        rephrased_query = query_info.get('rephrased_text')
        
        # Combine all query sources
        query_text = f"{element_name} {element_description}"
        if rephrased_query:
            query_text += f" {rephrased_query}"
            
        # Extract keywords from query
        query_keywords = set(extract_tokens(query_text))
        
        if not query_keywords:
            logger.warning(f"No valid keywords extracted from query: {query_text}")
            state["keyword_results"] = []
            return state
            
        logger.debug(f"Extracted keywords: {', '.join(query_keywords)}")
        
        # Time the retrieval process
        start_time = time.time()
        
        # Get all terms from the database
        all_terms = bt_manager.get_all_terms()
        
        # Calculate keyword overlap for each term
        keyword_results = []
        for term in all_terms:
            # Create term text
            term_text = f"{term.get('name', '')} {term.get('description', '')}"
            term_keywords = set(extract_tokens(term_text))
            
            # Calculate similarity score
            similarity = calculate_token_overlap_with_weights(query_keywords, term_keywords)
            
            # Only include terms with some similarity
            if similarity > 0.1:
                term_copy = term.copy()
                term_copy["similarity"] = similarity
                keyword_results.append(term_copy)
        
        # Sort by similarity score
        keyword_results.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
        
        retrieval_time = time.time() - start_time
        logger.debug(f"Keyword retrieval processing took {retrieval_time:.2f}s")
        
        # Limit to top results
        top_limit = max(state.get('top_k', 3) * 3, 20)
        keyword_results = keyword_results[:top_limit]
        
        logger.info(f"Keyword retrieval found {len(keyword_results)} candidates")
        
        # Update candidates with keyword results
        if state.get("candidates") is None:
            state["candidates"] = []
            
        # First, create a map of existing candidates by ID
        candidate_map = {c["id"]: c for c in state["candidates"]}
        
        # Add new candidates or update existing ones
        for result in keyword_results:
            candidate_id = result.get("id", "")
            if candidate_id in candidate_map:
                # Update existing candidate with keyword score
                candidate_map[candidate_id]["keyword_score"] = result.get("similarity", 0.0)
            else:
                # Add new candidate
                state["candidates"].append({
                    "id": result.get("id", ""),
                    "name": result.get("name", ""),
                    "description": result.get("description", ""),
                    "metadata": result.get("metadata", {}),
                    "vector_score": 0.0,
                    "keyword_score": result.get("similarity", 0.0),
                    "semantic_score": None,
                    "final_score": None,
                    "reasoning": None
                })
        
        # Store keyword results
        state["keyword_results"] = keyword_results
        
        return state
    
    except Exception as e:
        logger.error(f"Error in keyword retrieval: {e}", exc_info=True)
        state["error"] = f"Keyword retrieval failed: {str(e)}"
        state["keyword_results"] = []
        return state


def calculate_ngram_overlap(text1: str, text2: str, n: int = 2) -> float:
    """
    Calculate n-gram overlap between two texts.
    
    This is particularly useful for phrases and multi-word concepts.
    
    Args:
        text1: First text
        text2: Second text
        n: Size of n-grams
        
    Returns:
        Overlap score (0-1)
    """
    if not text1 or not text2:
        return 0.0
        
    # Clean texts
    text1_clean = clean_text(text1)
    text2_clean = clean_text(text2)
    
    # Extract tokens
    tokens1 = text1_clean.split()
    tokens2 = text2_clean.split()
    
    # Create n-grams
    ngrams1 = set()
    ngrams2 = set()
    
    for i in range(len(tokens1) - n + 1):
        ngrams1.add(" ".join(tokens1[i:i+n]))
    
    for i in range(len(tokens2) - n + 1):
        ngrams2.add(" ".join(tokens2[i:i+n]))
    
    # Calculate Jaccard similarity
    if not ngrams1 or not ngrams2:
        return 0.0
        
    intersection = ngrams1.intersection(ngrams2)
    union = ngrams1.union(ngrams2)
    
    return len(intersection) / len(union)
