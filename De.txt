"""
Global Data Privacy Multi-Jurisdiction AI Processing System

This enhanced system processes data privacy documents from multiple countries and organizations,
creates vector embeddings, builds comprehensive cross-jurisdictional graph structures, 
and maintains long-term memory for global privacy compliance analysis.

Enhanced Features:
- Multi-jurisdiction document processing (GDPR, UK GDPR, CCPA, PIPEDA, etc.)
- Organization-specific policy processing (ISO 27001, SOC 2, etc.)
- Global cross-referencing across all jurisdictions and frameworks
- Adaptive document type detection for any privacy framework
- Jurisdiction-agnostic prompting system
- Multi-framework ReAct agent with comparative analysis tools
- Comprehensive compliance mapping across global standards

Supported Frameworks:
- GDPR (EU), UK GDPR, CCPA (California), PIPEDA (Canada)
- LGPD (Brazil), PDPA (Singapore), Privacy Act (Australia)
- Organization standards: ISO 27001, SOC 2, NIST, PCI DSS
- Custom organizational policies and frameworks

Installation:
pip install pymupdf openai elasticsearch pydantic langchain langgraph langchain-text-splitters langchain-openai langchain-core
"""

import asyncio
import json
import logging
import os
import ssl
import uuid
import glob
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Tuple, Union

import openai
import pymupdf  # Modern PyMuPDF import (not fitz)
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from pydantic import BaseModel, Field, ConfigDict
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import ToolNode, create_react_agent


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Jurisdiction Mapper Class for Configuration Management
class JurisdictionMapper:
    """Manages jurisdiction mappings from external configuration file"""
    
    def __init__(self, mapper_file_path: str = "mapper.json"):
        self.mapper_file_path = mapper_file_path
        self.mappings = {}
        self.content_patterns = {}
        self.framework_relationships = {}
        self.default_fallback = {}
        self.load_mappings()
    
    def load_mappings(self):
        """Load jurisdiction mappings from JSON file"""
        try:
            if not os.path.exists(self.mapper_file_path):
                logger.warning(f"Mapper file not found: {self.mapper_file_path}")
                self._create_default_mapper()
            
            with open(self.mapper_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Load configuration sections
            self.mappings = data.get("filename_patterns", {})
            self.content_patterns = data.get("content_patterns", {})
            self.framework_relationships = data.get("framework_relationships", {})
            self.default_fallback = data.get("configuration", {}).get("default_fallback", {
                "jurisdiction": "Unknown",
                "framework_type": "custom",
                "region": "Unknown",
                "full_name": "Custom Framework"
            })
            
            # Add custom mappings if they exist
            custom_mappings = data.get("custom_mappings", {})
            self.mappings.update(custom_mappings)
            
            logger.info(f"Successfully loaded {len(self.mappings)} jurisdiction mappings from {self.mapper_file_path}")
            
        except Exception as e:
            logger.error(f"Error loading jurisdiction mappings: {e}")
            self._create_default_mapper()
    
    def _create_default_mapper(self):
        """Create a minimal default mapper if file doesn't exist"""
        default_mappings = {
            "gdpr": {
                "patterns": ["gdpr", "general_data_protection"],
                "jurisdiction": "EU",
                "framework_type": "regulation",
                "region": "European Union",
                "full_name": "General Data Protection Regulation"
            },
            "uk_gdpr": {
                "patterns": ["uk_gdpr", "uk_data_protection"],
                "jurisdiction": "UK", 
                "framework_type": "regulation",
                "region": "United Kingdom",
                "full_name": "UK General Data Protection Regulation"
            }
        }
        
        self.mappings = default_mappings
        self.default_fallback = {
            "jurisdiction": "Unknown",
            "framework_type": "custom", 
            "region": "Unknown",
            "full_name": "Custom Framework"
        }
        
        logger.warning(f"Using default jurisdiction mappings. Create {self.mapper_file_path} for full configuration.")
    
    def detect_document_type(self, file_path: str, content: str) -> Tuple[str, Dict]:
        """Enhanced document type detection using mapper configuration"""
        filename = os.path.basename(file_path).lower()
        content_lower = content.lower()
        
        # First try filename pattern matching
        for doc_type, mapping in self.mappings.items():
            patterns = mapping.get("patterns", [])
            for pattern in patterns:
                if pattern.lower() in filename:
                    logger.info(f"Matched {doc_type} via filename pattern: {pattern}")
                    return doc_type.upper(), mapping
        
        # Then try content pattern matching
        for pattern_group, indicators in self.content_patterns.items():
            for indicator in indicators:
                if indicator.lower() in content_lower:
                    # Try to map pattern group to a document type
                    doc_type = pattern_group.replace("_indicators", "")
                    if doc_type in self.mappings:
                        logger.info(f"Matched {doc_type} via content pattern: {indicator}")
                        return doc_type.upper(), self.mappings[doc_type]
        
        # Enhanced content-based detection for common patterns
        content_mappings = {
            "gdpr": ["general data protection regulation", "regulation (eu) 2016/679", "data protection board"],
            "uk_gdpr": ["uk gdpr", "data protection act 2018", "ico"],
            "ccpa": ["california consumer privacy act", "ccpa", "california attorney general"],
            "lgpd": ["lei geral de proteção de dados", "lgpd", "anpd"],
            "iso_27001": ["iso 27001", "iso/iec 27001", "information security management"],
            "soc_2": ["soc 2", "service organization control", "aicpa"],
        }
        
        for doc_type, patterns in content_mappings.items():
            for pattern in patterns:
                if pattern in content_lower and doc_type in self.mappings:
                    logger.info(f"Matched {doc_type} via enhanced content detection: {pattern}")
                    return doc_type.upper(), self.mappings[doc_type]
        
        # Fallback to custom framework
        logger.info(f"No specific match found for {file_path}, using fallback")
        return "CUSTOM_FRAMEWORK", self.default_fallback
    
    def get_mapping(self, doc_type: str) -> Dict:
        """Get mapping information for a document type"""
        doc_type_lower = doc_type.lower()
        return self.mappings.get(doc_type_lower, self.default_fallback)
    
    def get_jurisdiction_info(self, doc_type: str) -> Dict:
        """Get jurisdiction information for a document type"""
        mapping = self.get_mapping(doc_type)
        return {
            "jurisdiction": mapping.get("jurisdiction", self.default_fallback["jurisdiction"]),
            "framework_type": mapping.get("framework_type", self.default_fallback["framework_type"]),
            "region": mapping.get("region", self.default_fallback["region"]),
            "full_name": mapping.get("full_name", self.default_fallback["full_name"]),
            "authority": mapping.get("authority", "Unknown"),
            "key_concepts": mapping.get("key_concepts", []),
            "scope": mapping.get("scope", "Unknown")
        }
    
    def get_all_jurisdictions(self) -> List[str]:
        """Get list of all available jurisdictions"""
        jurisdictions = set()
        for mapping in self.mappings.values():
            jurisdictions.add(mapping.get("jurisdiction", "Unknown"))
        return sorted(list(jurisdictions))
    
    def get_framework_relationships(self, doc_type: str) -> Dict:
        """Get related frameworks for a document type"""
        relationships = {
            "equivalent": [],
            "complementary": [],
            "hierarchical_parent": None,
            "hierarchical_children": []
        }
        
        doc_type_lower = doc_type.lower()
        
        # Check equivalent frameworks
        for equiv_group in self.framework_relationships.get("equivalent_frameworks", []):
            if doc_type_lower in equiv_group:
                relationships["equivalent"] = [f for f in equiv_group if f != doc_type_lower]
        
        # Check complementary frameworks
        for comp_group in self.framework_relationships.get("complementary_frameworks", []):
            if doc_type_lower in comp_group:
                relationships["complementary"] = [f for f in comp_group if f != doc_type_lower]
        
        # Check hierarchical relationships
        hierarchical = self.framework_relationships.get("hierarchical_relationships", {})
        for parent, children in hierarchical.items():
            if doc_type_lower in children:
                relationships["hierarchical_parent"] = parent
            elif doc_type_lower == parent:
                relationships["hierarchical_children"] = children
        
        return relationships
    
    def add_custom_mapping(self, doc_type: str, mapping: Dict):
        """Add a custom mapping at runtime"""
        self.mappings[doc_type.lower()] = mapping
        logger.info(f"Added custom mapping for {doc_type}")
    
    def reload_mappings(self):
        """Reload mappings from file"""
        logger.info("Reloading jurisdiction mappings...")
        self.load_mappings()


# Global mapper instance
jurisdiction_mapper = JurisdictionMapper()

# Utility Functions
def safe_json_parse(json_string: str, fallback_value: Any = None) -> Any:
    """Safely parse JSON with fallback handling"""
    if not json_string or not isinstance(json_string, str):
        logger.warning(f"Invalid JSON input: {type(json_string)}")
        return fallback_value
    
    # Clean the JSON string
    json_string = json_string.strip()
    
    # Try to extract JSON from markdown code blocks
    if "```json" in json_string:
        try:
            start_idx = json_string.find("```json") + 7
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    elif "```" in json_string:
        try:
            start_idx = json_string.find("```") + 3
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    
    # Multiple parsing attempts with different strategies
    parse_attempts = [
        lambda: json.loads(json_string),
        lambda: json.loads(json_string.replace("'", '"')),  # Fix single quotes
        lambda: json.loads(re.sub(r'(\w+):', r'"\1":', json_string)),  # Fix unquoted keys
        lambda: eval(json_string) if json_string.startswith('{') and json_string.endswith('}') else None,
    ]
    
    for attempt in parse_attempts:
        try:
            result = attempt()
            if result is not None:
                return result
        except Exception as e:
            continue
    
    logger.error(f"Failed to parse JSON: {json_string[:200]}...")
    return fallback_value


def create_fallback_articles(content: str, document_type: str) -> List[Dict]:
    """Create fallback article structure when AI parsing fails"""
    try:
        # Enhanced regex patterns for different document types
        patterns = [
            r'(Article\s+\d+[^:]*:?[^\n]*)\n([^A]*?)(?=Article\s+\d+|Chapter\s+\d+|Section\s+\d+|$)',
            r'(Section\s+\d+[^:]*:?[^\n]*)\n([^S]*?)(?=Section\s+\d+|Chapter\s+\d+|Article\s+\d+|$)',
            r'(Chapter\s+\d+[^:]*:?[^\n]*)\n([^C]*?)(?=Chapter\s+\d+|Section\s+\d+|Article\s+\d+|$)',
            r'(\d+\.\s+[^\n]*)\n([^\d]*?)(?=\d+\.\s+|$)',
        ]
        
        fallback_articles = []
        
        for pattern in patterns:
            articles = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
            if articles:
                for i, (title, article_content) in enumerate(articles[:10]):  # Limit to 10 articles
                    fallback_articles.append({
                        "chapter_number": f"Auto-{i+1}",
                        "article_number": f"Art-{i+1}",
                        "title": title.strip(),
                        "full_content": article_content.strip() if article_content else title,
                        "key_concepts": []
                    })
                break
        
        # If no articles found, create sections based on content length
        if not fallback_articles:
            content_length = len(content)
            section_size = min(2000, content_length // 5)  # Create up to 5 sections
            
            for i in range(0, min(content_length, 10000), section_size):
                section_content = content[i:i+section_size]
                if section_content.strip():
                    fallback_articles.append({
                        "chapter_number": f"Section-{len(fallback_articles)+1}",
                        "article_number": f"S{len(fallback_articles)+1}",
                        "title": f"{document_type} Section {len(fallback_articles)+1}",
                        "full_content": section_content.strip(),
                        "key_concepts": []
                    })
        
        logger.info(f"Created {len(fallback_articles)} fallback articles for {document_type}")
        return fallback_articles
        
    except Exception as e:
        logger.error(f"Error creating fallback articles: {e}")
        return [{
            "chapter_number": "1",
            "article_number": "1", 
            "title": f"{document_type} Document",
            "full_content": content[:2000] if content else "No content available",
            "key_concepts": []
        }]


def create_fallback_chunk_analysis(text_chunks: List[str]) -> List[Dict]:
    """Create fallback chunk analysis when AI analysis fails"""
    try:
        fallback_analysis = []
        for i, chunk in enumerate(text_chunks):
            # Extract potential title from first line
            lines = chunk.strip().split('\n')
            potential_title = lines[0] if lines else f"Section {i+1}"
            
            # Clean title
            if len(potential_title) > 100:
                potential_title = f"Section {i+1}"
            
            fallback_analysis.append({
                "chunk_index": i,
                "parent_article_number": None,
                "chapter_number": f"Ch-{(i//10)+1}",
                "article_number": None,
                "title": potential_title.strip(),
                "key_concepts": []
            })
        
        logger.info(f"Created fallback analysis for {len(text_chunks)} chunks")
        return fallback_analysis
        
    except Exception as e:
        logger.error(f"Error creating fallback chunk analysis: {e}")
        return []


def detect_document_type(file_path: str, content: str) -> str:
    """Enhanced document type detection using jurisdiction mapper"""
    try:
        doc_type, mapping = jurisdiction_mapper.detect_document_type(file_path, content)
        return doc_type
    except Exception as e:
        logger.error(f"Error in document type detection: {e}")
        return "CUSTOM_FRAMEWORK"


# Configuration
class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
    ES_PASSWORD = os.getenv("ES_PASSWORD")
    ES_HOST = os.getenv("ES_HOST", "localhost")
    ES_PORT = int(os.getenv("ES_PORT", "9200"))
    ES_CACERT_PATH = os.getenv("ES_CACERT_PATH", "cacert.crt")
    
    # Model configurations
    O3_MINI_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS = 3072
    REASONING_EFFORT = "high"
    
    # Text splitting configurations
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    SEPARATORS = ["\n\n", "\n", ". ", " ", ""]
    
    # HNSW & Quantization configurations
    VECTOR_INDEX_TYPE = "int8_hnsw"  # Auto scalar quantization
    HNSW_M = 16
    HNSW_EF_CONSTRUCTION = 200
    CONFIDENCE_INTERVAL = 0.95
    
    # Performance optimizations
    ENABLE_PRELOAD = True
    MAX_SEGMENTS = 10


# Enhanced Pydantic Models for Multi-Jurisdiction Support
class ChapterReference(BaseModel):
    """Reference to a chapter in a document"""
    document_type: str = Field(description="Document type (GDPR, CCPA, ISO_27001, etc.)")
    jurisdiction: str = Field(description="Jurisdiction or organization")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None, description="Article identifier")
    title: str = Field(description="Chapter/Article title")
    relevance_score: float = Field(description="Relevance score 0-1")
    relationship_type: str = Field(description="Type of relationship (supports, contradicts, references, etc.)")


class FullArticle(BaseModel):
    """Complete article with full content embedding"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    article_id: str = Field(description="Unique article identifier")
    document_type: str = Field(description="Document type (GDPR, CCPA, ISO_27001, etc.)")
    jurisdiction: str = Field(description="Jurisdiction or organization")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: str = Field(description="Article identifier")
    title: str = Field(description="Full article title")
    full_content: str = Field(description="Complete article text")
    full_article_embedding: List[float] = Field(description="Embedding of entire article")
    chunk_ids: List[str] = Field(description="IDs of chunks belonging to this article")
    key_concepts: List[str] = Field(default_factory=list, description="Key legal concepts")
    framework_type: str = Field(description="Framework type (regulation, standard, policy, etc.)")
    created_at: datetime = Field(default_factory=datetime.now)


class DocumentChunk(BaseModel):
    """Individual document chunk with metadata"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    chunk_id: str = Field(description="Unique chunk identifier")
    parent_article_id: Optional[str] = Field(default=None, description="Parent article ID")
    document_type: str = Field(description="Document type (GDPR, CCPA, ISO_27001, etc.)")
    jurisdiction: str = Field(description="Jurisdiction or organization")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None)
    title: str = Field(description="Section title")
    content: str = Field(description="Text content")
    chunk_embedding: Optional[List[float]] = Field(default=None, description="Chunk-level vector embedding")
    supporting_references: List[ChapterReference] = Field(default_factory=list)
    framework_type: str = Field(description="Framework type (regulation, standard, policy, etc.)")
    page_number: Optional[int] = Field(default=None, description="Source page number")
    chunk_index: int = Field(description="Sequential chunk index")
    created_at: datetime = Field(default_factory=datetime.now)
    processed_by_agent: Optional[str] = Field(default=None)


class CrossDocumentLink(BaseModel):
    """Cross-document relationship"""
    source_chunk_id: str
    target_chunk_id: str
    source_jurisdiction: str
    target_jurisdiction: str
    relationship_type: str
    confidence_score: float
    created_at: datetime = Field(default_factory=datetime.now)


class AgentMemory(BaseModel):
    """Long-term memory structure for agents"""
    memory_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    agent_name: str
    memory_type: str  # semantic, episodic, procedural
    content: Dict[str, Any]
    namespace: List[str]
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)


# State Management
class ProcessingState(TypedDict):
    """State for the processing workflow"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    full_articles: List[FullArticle]
    documents: List[DocumentChunk]
    current_chunk: Optional[DocumentChunk]
    cross_links: List[CrossDocumentLink]
    processing_stage: str
    agent_memories: List[AgentMemory]
    elasticsearch_client: Optional[Any]


# Enhanced Elasticsearch Manager for Multi-Jurisdiction Support
class ElasticsearchManager:
    """Manages Elasticsearch operations with SSL authentication and HNSW optimization for global frameworks"""
    
    def __init__(self):
        self.client = self._create_client()
        self._setup_indices()
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client with SSL configuration"""
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        if os.path.exists(Config.ES_CACERT_PATH):
            ssl_context.load_verify_locations(Config.ES_CACERT_PATH)
        
        return Elasticsearch(
            [{"host": Config.ES_HOST, "port": Config.ES_PORT, "scheme": "https"}],
            basic_auth=(Config.ES_USERNAME, Config.ES_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True,
            request_timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
    
    def _setup_indices(self):
        """Setup Elasticsearch indices with advanced HNSW and quantization configurations for multi-jurisdiction support"""
        
        def create_vector_field_config(field_name: str) -> Dict:
            """Create optimized vector field configuration with HNSW and quantization"""
            return {
                "type": "dense_vector",
                "dims": Config.EMBEDDING_DIMENSIONS,
                "index": True,
                "similarity": "cosine",
                "index_options": {
                    "type": Config.VECTOR_INDEX_TYPE,
                    "m": Config.HNSW_M,
                    "ef_construction": Config.HNSW_EF_CONSTRUCTION,
                    "confidence_interval": Config.CONFIDENCE_INTERVAL
                }
            }
        
        # Enhanced full articles index for global frameworks
        articles_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "merge.policy.max_merged_segment": "5gb",
                    "merge.policy.segments_per_tier": 4,
                    "store.preload": ["vec", "vem", "nvd", "nvm"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "full_content": {"type": "text", "analyzer": "standard"},
                    "full_article_embedding": create_vector_field_config("full_article_embedding"),
                    "chunk_ids": {"type": "keyword"},
                    "key_concepts": {"type": "keyword"},
                    "framework_type": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Enhanced document chunks index for global frameworks
        chunks_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "5s",
                    "merge.policy.max_merged_segment": "2gb",
                    "merge.policy.segments_per_tier": 4,
                    "store.preload": ["veq", "vemq"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "chunk_id": {"type": "keyword"},
                    "parent_article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "chunk_embedding": create_vector_field_config("chunk_embedding"),
                    "supporting_references": {
                        "type": "nested",
                        "properties": {
                            "document_type": {"type": "keyword"},
                            "jurisdiction": {"type": "keyword"},
                            "chapter_number": {"type": "keyword"},
                            "article_number": {"type": "keyword"},
                            "title": {"type": "text"},
                            "relevance_score": {"type": "float"},
                            "relationship_type": {"type": "keyword"}
                        }
                    },
                    "framework_type": {"type": "keyword"},
                    "page_number": {"type": "integer"},
                    "chunk_index": {"type": "integer"},
                    "created_at": {"type": "date"},
                    "processed_by_agent": {"type": "keyword"}
                }
            }
        }
        
        # Enhanced cross-document links index for global cross-referencing
        links_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "1s",
                }
            },
            "mappings": {
                "properties": {
                    "source_chunk_id": {"type": "keyword"},
                    "target_chunk_id": {"type": "keyword"},
                    "source_jurisdiction": {"type": "keyword"},
                    "target_jurisdiction": {"type": "keyword"},
                    "relationship_type": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Agent memories index
        memories_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                }
            },
            "mappings": {
                "properties": {
                    "memory_id": {"type": "keyword"},
                    "agent_name": {"type": "keyword"},
                    "memory_type": {"type": "keyword"},
                    "content": {"type": "object"},
                    "namespace": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
        
        # Create indices with enhanced global support
        indices = {
            "privacy_articles": articles_mapping,
            "privacy_chunks": chunks_mapping,
            "privacy_links": links_mapping,
            "agent_memories": memories_mapping
        }
        
        for index_name, mapping in indices.items():
            if not self.client.indices.exists(index=index_name):
                self.client.indices.create(index=index_name, body=mapping)
                logger.info(f"Created global privacy index with HNSW optimization: {index_name}")
            else:
                logger.info(f"Index already exists: {index_name}")
    
    def index_article(self, article: FullArticle) -> bool:
        """Index a full article"""
        try:
            doc = article.model_dump()
            doc["created_at"] = article.created_at.isoformat()
            
            response = self.client.index(
                index="privacy_articles",
                id=article.article_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing article {article.article_id}: {e}")
            raise
    
    def index_chunk(self, chunk: DocumentChunk) -> bool:
        """Index a document chunk"""
        try:
            doc = chunk.model_dump()
            doc["created_at"] = chunk.created_at.isoformat()
            
            response = self.client.index(
                index="privacy_chunks",
                id=chunk.chunk_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing chunk {chunk.chunk_id}: {e}")
            raise
    
    def index_link(self, link: CrossDocumentLink) -> bool:
        """Index a cross-document link"""
        try:
            doc = link.model_dump()
            doc["created_at"] = link.created_at.isoformat()
            
            response = self.client.index(
                index="privacy_links",
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing link: {e}")
            raise
    
    def index_memory(self, memory: AgentMemory) -> bool:
        """Index agent memory"""
        try:
            doc = memory.model_dump()
            doc["created_at"] = memory.created_at.isoformat()
            doc["updated_at"] = memory.updated_at.isoformat()
            
            response = self.client.index(
                index="agent_memories",
                id=memory.memory_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing memory {memory.memory_id}: {e}")
            raise
    
    def hybrid_search(self, query: str, embedding: List[float], filters: Dict = None, 
                     search_level: str = "both", rescore: bool = True) -> Dict[str, List[Dict]]:
        """Enhanced hybrid search with global multi-jurisdiction support"""
        results = {}
        
        try:
            # Search articles
            if search_level in ["articles", "both"]:
                article_search = {
                    "knn": {
                        "field": "full_article_embedding",
                        "query_vector": embedding,
                        "k": 15,  # Increased for multi-jurisdiction
                        "num_candidates": 75,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^3", "full_content^2", "key_concepts^2", "jurisdiction^1.5"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 30,  # Increased for multi-jurisdiction
                }
                
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    article_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'full_article_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                article_response = self.client.search(index="privacy_articles", body=article_search)
                results["articles"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in article_response["hits"]["hits"]
                ]
            
            # Search chunks
            if search_level in ["chunks", "both"]:
                chunk_search = {
                    "knn": {
                        "field": "chunk_embedding",
                        "query_vector": embedding,
                        "k": 15,  # Increased for multi-jurisdiction
                        "num_candidates": 75,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^2", "content^1.5", "jurisdiction^1.5"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 30,  # Increased for multi-jurisdiction
                }
                
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    chunk_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'chunk_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                chunk_response = self.client.search(index="privacy_chunks", body=chunk_search)
                results["chunks"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in chunk_response["hits"]["hits"]
                ]
            
            return results
            
        except Exception as e:
            logger.error(f"Error in enhanced global hybrid search: {e}")
            raise
    
    def get_related_chunks(self, chunk_id: str) -> List[Dict]:
        """Get chunks related through graph links"""
        try:
            outgoing_query = {
                "query": {"term": {"source_chunk_id": chunk_id}},
                "size": 100  # Increased for multi-jurisdiction
            }
            
            incoming_query = {
                "query": {"term": {"target_chunk_id": chunk_id}},
                "size": 100  # Increased for multi-jurisdiction
            }
            
            outgoing_response = self.client.search(index="privacy_links", body=outgoing_query)
            incoming_response = self.client.search(index="privacy_links", body=incoming_query)
            
            related_chunk_ids = set()
            for hit in outgoing_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["target_chunk_id"])
            for hit in incoming_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["source_chunk_id"])
            
            if not related_chunk_ids:
                return []
            
            chunks_query = {
                "query": {"terms": {"chunk_id": list(related_chunk_ids)}},
                "size": len(related_chunk_ids)
            }
            
            chunks_response = self.client.search(index="privacy_chunks", body=chunks_query)
            return [hit["_source"] for hit in chunks_response["hits"]["hits"]]
            
        except Exception as e:
            logger.error(f"Error getting related chunks for {chunk_id}: {e}")
            raise
    
    def get_jurisdiction_summary(self) -> Dict[str, Any]:
        """Get summary of all jurisdictions and frameworks in the system using jurisdiction mapper"""
        try:
            aggs_query = {
                "size": 0,
                "aggs": {
                    "jurisdictions": {
                        "terms": {"field": "jurisdiction", "size": 50}
                    },
                    "document_types": {
                        "terms": {"field": "document_type", "size": 50}
                    },
                    "framework_types": {
                        "terms": {"field": "framework_type", "size": 20}
                    }
                }
            }
            
            response = self.client.search(index="privacy_chunks", body=aggs_query)
            
            # Enhance with jurisdiction mapper information
            jurisdictions = [bucket["key"] for bucket in response["aggregations"]["jurisdictions"]["buckets"]]
            document_types = [bucket["key"] for bucket in response["aggregations"]["document_types"]["buckets"]]
            framework_types = [bucket["key"] for bucket in response["aggregations"]["framework_types"]["buckets"]]
            
            # Get additional details from jurisdiction mapper
            available_jurisdictions = jurisdiction_mapper.get_all_jurisdictions()
            
            return {
                "total_documents": response["hits"]["total"]["value"],
                "jurisdictions": jurisdictions,
                "document_types": document_types,
                "framework_types": framework_types,
                "available_jurisdictions": available_jurisdictions,
                "supported_frameworks": len(jurisdiction_mapper.mappings),
                "mapper_version": jurisdiction_mapper.mappings.get("configuration", {}).get("version", "1.0")
            }
            
        except Exception as e:
            logger.error(f"Error getting jurisdiction summary: {e}")
            return {}
    
    def get_index_stats(self) -> Dict[str, Dict]:
        """Get index statistics for performance monitoring"""
        try:
            indices = ["privacy_articles", "privacy_chunks", "privacy_links", "agent_memories"]
            stats = {}
            
            for index_name in indices:
                try:
                    index_stats = self.client.indices.stats(index=index_name)
                    index_data = index_stats["indices"].get(index_name, {})
                    
                    stats[index_name] = {
                        "documents": index_data.get("primaries", {}).get("docs", {}).get("count", 0),
                        "size_bytes": index_data.get("primaries", {}).get("store", {}).get("size_in_bytes", 0),
                        "segments": index_data.get("primaries", {}).get("segments", {}).get("count", 0),
                        "vector_size_estimate": {
                            "estimated_memory": "Unknown",
                            "quantization_savings": "Unknown",
                            "hnsw_overhead": "Unknown"
                        }
                    }
                except Exception as e:
                    logger.error(f"Error getting stats for {index_name}: {e}")
                    stats[index_name] = {"error": str(e)}
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting index statistics: {e}")
            return {}
    
    def optimize_indices(self):
        """Optimize indices for production use"""
        try:
            indices = ["privacy_articles", "privacy_chunks", "privacy_links", "agent_memories"]
            
            for index_name in indices:
                try:
                    # Force merge to reduce segments
                    self.client.indices.forcemerge(
                        index=index_name,
                        max_num_segments=Config.MAX_SEGMENTS,
                        wait_for_completion=False
                    )
                    logger.info(f"Started force merge for {index_name}")
                except Exception as e:
                    logger.error(f"Error optimizing {index_name}: {e}")
                    
        except Exception as e:
            logger.error(f"Error in index optimization: {e}")


# OpenAI API Manager (unchanged)
class OpenAIManager:
    """Manages OpenAI API calls with direct API usage"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using OpenAI API directly"""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    async def reasoning_completion(self, messages: List[Dict], system_prompt: str = None) -> str:
        """Create completion using o3-mini with high reasoning effort"""
        try:
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({"role": "developer", "content": system_prompt})
            
            formatted_messages.extend(messages)
            
            response = self.client.chat.completions.create(
                model=Config.O3_MINI_MODEL,
                messages=formatted_messages,
                reasoning_effort=Config.REASONING_EFFORT
            )
            
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in reasoning completion: {e}")
            raise


# Enhanced Document Processing Agent for Multi-Jurisdiction Support
class DocumentProcessingAgent:
    """Agent for processing and chunking documents with multi-jurisdiction support"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.name = "DocumentProcessor"
        
        # Initialize the recursive text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            separators=Config.SEPARATORS,
            keep_separator=True,
            is_separator_regex=False,
        )
    
    def _extract_text_from_pdf(self, file_path: str) -> Tuple[str, Dict[int, str]]:
        """Extract text from PDF file using modern PyMuPDF 1.26+ with page tracking"""
        try:
            doc = pymupdf.open(file_path)
            
            if doc.needs_pass:
                doc.close()
                raise ValueError(f"PDF {file_path} is password protected. Please provide an unencrypted version.")
            
            page_count = doc.page_count
            if page_count == 0:
                doc.close()
                raise ValueError(f"PDF {file_path} contains no pages")
            
            full_text = ""
            page_texts = {}
            extracted_pages = 0
            
            for page in doc:
                try:
                    page_text = page.get_text()
                    if page_text.strip():
                        page_texts[page.number] = page_text
                        full_text += f"\n\n--- PAGE {page.number + 1} ---\n\n"
                        full_text += page_text
                        extracted_pages += 1
                except Exception as page_error:
                    logger.warning(f"Error extracting text from page {page.number + 1} in {file_path}: {page_error}")
                    continue
            
            doc.close()
            
            if extracted_pages == 0:
                raise ValueError(f"No text could be extracted from PDF {file_path}")
            
            logger.info(f"Successfully extracted text from {extracted_pages}/{page_count} pages in {file_path}")
            return full_text.strip(), page_texts
            
        except Exception as e:
            logger.error(f"Error extracting text from PDF {file_path}: {e}")
            raise
    
    def _extract_page_number(self, chunk_text: str) -> Optional[int]:
        """Extract page number from chunk text containing page markers"""
        import re
        page_match = re.search(r'--- PAGE (\d+) ---', chunk_text)
        if page_match:
            return int(page_match.group(1))
        return None
    
    async def process_document(self, file_path: str, document_type: str = None) -> Tuple[List[FullArticle], List[DocumentChunk]]:
        """Process a PDF document with comprehensive multi-jurisdiction support"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                raise FileNotFoundError(f"Document file not found: {file_path}")
            
            logger.info(f"Extracting text from PDF: {file_path}")
            content, page_texts = self._extract_text_from_pdf(file_path)
            
            if not content.strip():
                logger.error(f"No text extracted from PDF: {file_path}")
                raise ValueError(f"No text content found in PDF: {file_path}")
            
            if not document_type:
                document_type = detect_document_type(file_path, content)
                logger.info(f"Determined document type: {document_type} for {file_path}")
            
            # Get jurisdiction and framework info using mapper
            jurisdiction_info = jurisdiction_mapper.get_jurisdiction_info(document_type)
            jurisdiction = jurisdiction_info["jurisdiction"]
            framework_type = jurisdiction_info["framework_type"]
            
            logger.info(f"Processing {document_type} document from {jurisdiction}: {file_path} ({len(content)} characters)")
            logger.info(f"Framework details: {jurisdiction_info['full_name']} ({framework_type})")
            
            # Step 1: Use recursive text splitter to create comprehensive chunks
            logger.info("Creating comprehensive text chunks using RecursiveCharacterTextSplitter...")
            try:
                text_chunks = self.text_splitter.split_text(content)
                logger.info(f"Created {len(text_chunks)} text chunks")
            except Exception as e:
                logger.error(f"Error in text splitting: {e}")
                # Fallback: simple splitting
                chunk_size = Config.CHUNK_SIZE
                text_chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
                logger.info(f"Fallback splitting created {len(text_chunks)} chunks")
            
            # Step 2: Enhanced article identification with jurisdiction-aware prompting
            article_system_prompt = f"""
            You are an expert in global data privacy and protection regulations. Analyze this {framework_type} 
            document from {jurisdiction} and identify complete articles/sections with their proper structure.
            
            Document Type: {document_type}
            Jurisdiction: {jurisdiction}
            Framework Type: {framework_type}
            
            For this type of document, identify:
            - Articles, sections, chapters, or clauses (depending on the document structure)
            - Key privacy and data protection concepts
            - Legal obligations and requirements
            - Rights and responsibilities
            
            Return ONLY a valid JSON object with the following structure:
            {{
                "articles": [
                    {{
                        "chapter_number": "string",
                        "article_number": "string", 
                        "title": "string",
                        "full_content": "complete article text including all subsections",
                        "key_concepts": ["concept1", "concept2"]
                    }}
                ]
            }}
            
            Make sure to return valid JSON only. Do not include explanations or markdown formatting.
            Focus on privacy-related content such as: data processing, consent, data subject rights, 
            security measures, breach notification, data transfers, compliance requirements.
            """
            
            article_messages = [
                {"role": "user", "content": f"Document type: {document_type}\nJurisdiction: {jurisdiction}\n\nDocument content:\n{content[:8000]}"}
            ]
            
            articles_data = []
            try:
                article_response = await self.openai_manager.reasoning_completion(article_messages, article_system_prompt)
                articles_json = safe_json_parse(article_response, {"articles": []})
                articles_data = articles_json.get("articles", []) if isinstance(articles_json, dict) else []
                
                if not articles_data:
                    logger.warning("No articles found in AI response, using fallback")
                    articles_data = create_fallback_articles(content, document_type)
                else:
                    logger.info(f"AI successfully identified {len(articles_data)} articles")
                    
            except Exception as e:
                logger.error(f"Error in article analysis: {e}, using fallback")
                articles_data = create_fallback_articles(content, document_type)
            
            # Step 3: Enhanced chunk analysis with jurisdiction-aware prompting
            chunk_system_prompt = f"""
            You are an expert in {jurisdiction} data privacy regulations. For each provided text chunk 
            from this {framework_type} document, identify:
            
            - Which article/section it belongs to
            - Its specific role within the privacy framework
            - Key privacy and data protection concepts
            - Regulatory obligations or requirements
            - Data subject rights or controller/processor duties
            
            Document Context:
            - Type: {document_type}
            - Jurisdiction: {jurisdiction}
            - Framework: {framework_type}
            
            Return ONLY a valid JSON object:
            {{
                "chunk_analysis": [
                    {{
                        "chunk_index": 0,
                        "parent_article_number": "string or null",
                        "chapter_number": "string",
                        "article_number": "string or null",
                        "title": "descriptive title for this chunk",
                        "key_concepts": ["concept1", "concept2"]
                    }}
                ]
            }}
            
            Return valid JSON only. Analyze ALL provided chunks.
            """
            
            # Send chunks in batches for analysis
            chunk_analyses = []
            batch_size = 5
            
            for i in range(0, len(text_chunks), batch_size):
                try:
                    batch_chunks = text_chunks[i:i+batch_size]
                    batch_content = ""
                    
                    for idx, chunk in enumerate(batch_chunks):
                        batch_content += f"\n\n=== CHUNK {i + idx} ===\n{chunk[:1000]}"
                    
                    chunk_messages = [
                        {"role": "user", "content": f"Document type: {document_type}\nJurisdiction: {jurisdiction}\n\nText chunks to analyze:\n{batch_content}"}
                    ]
                    
                    try:
                        batch_response = await self.openai_manager.reasoning_completion(chunk_messages, chunk_system_prompt)
                        batch_json = safe_json_parse(batch_response, {"chunk_analysis": []})
                        batch_analysis = batch_json.get("chunk_analysis", []) if isinstance(batch_json, dict) else []
                        
                        if batch_analysis:
                            chunk_analyses.extend(batch_analysis)
                            logger.info(f"Successfully analyzed batch {i//batch_size + 1}")
                        else:
                            logger.warning(f"No analysis for batch {i//batch_size + 1}, using fallback")
                            fallback_batch = create_fallback_chunk_analysis(batch_chunks)
                            for fb_chunk in fallback_batch:
                                fb_chunk["chunk_index"] = i + fb_chunk["chunk_index"]
                            chunk_analyses.extend(fallback_batch)
                            
                    except Exception as batch_error:
                        logger.error(f"Error analyzing batch {i//batch_size + 1}: {batch_error}")
                        fallback_batch = create_fallback_chunk_analysis(batch_chunks)
                        for fb_chunk in fallback_batch:
                            fb_chunk["chunk_index"] = i + fb_chunk["chunk_index"]
                        chunk_analyses.extend(fallback_batch)
                    
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//batch_size + 1}: {e}")
                    continue
            
            # Ensure we have analysis for all chunks
            if len(chunk_analyses) < len(text_chunks):
                logger.warning(f"Missing analysis for some chunks, creating fallback for remaining")
                missing_chunks = text_chunks[len(chunk_analyses):]
                fallback_analyses = create_fallback_chunk_analysis(missing_chunks)
                for fb_chunk in fallback_analyses:
                    fb_chunk["chunk_index"] = len(chunk_analyses) + fb_chunk["chunk_index"]
                chunk_analyses.extend(fallback_analyses)
            
            # Step 4: Create Enhanced FullArticle objects with jurisdiction info
            full_articles = []
            article_chunk_map = {}
            
            for article_data in articles_data:
                try:
                    article_id = str(uuid.uuid4())
                    
                    title = str(article_data.get('title', 'Untitled Article'))
                    full_content = str(article_data.get('full_content', ''))
                    
                    try:
                        full_text = f"{title} {full_content}"
                        full_embedding = await self.openai_manager.create_embedding(full_text[:3000])
                    except Exception as e:
                        logger.error(f"Error creating embedding for article {article_id}: {e}")
                        full_embedding = [0.0] * Config.EMBEDDING_DIMENSIONS
                    
                    article = FullArticle(
                        article_id=article_id,
                        document_type=document_type,
                        jurisdiction=jurisdiction,
                        chapter_number=str(article_data.get("chapter_number", "Unknown")),
                        article_number=str(article_data.get("article_number", f"A{len(full_articles)+1}")),
                        title=title,
                        full_content=full_content,
                        full_article_embedding=full_embedding,
                        chunk_ids=[],
                        key_concepts=article_data.get("key_concepts", []) if isinstance(article_data.get("key_concepts"), list) else [],
                        framework_type=framework_type
                    )
                    
                    full_articles.append(article)
                    
                    try:
                        self.es_manager.index_article(article)
                    except Exception as e:
                        logger.error(f"Error indexing article {article_id}: {e}")
                    
                    article_chunk_map[article.article_number] = {
                        "article": article,
                        "chunk_ids": []
                    }
                    
                except Exception as e:
                    logger.error(f"Error creating article: {e}")
                    continue
            
            # Step 5: Create Enhanced DocumentChunk objects with jurisdiction info
            chunks = []
            
            for i, chunk_text in enumerate(text_chunks):
                try:
                    chunk_id = str(uuid.uuid4())
                    
                    chunk_analysis = None
                    for analysis in chunk_analyses:
                        if analysis.get("chunk_index") == i:
                            chunk_analysis = analysis
                            break
                    
                    if not chunk_analysis:
                        chunk_analysis = {
                            "parent_article_number": None,
                            "chapter_number": f"Ch-{(i//10)+1}",
                            "article_number": None,
                            "title": f"Section {i + 1}",
                            "key_concepts": []
                        }
                    
                    # Find parent article
                    parent_article = None
                    parent_article_number = chunk_analysis.get("parent_article_number")
                    
                    if parent_article_number and parent_article_number in article_chunk_map:
                        parent_article = article_chunk_map[parent_article_number]["article"]
                    
                    # Extract page number
                    page_number = self._extract_page_number(chunk_text)
                    
                    # Create chunk embedding
                    try:
                        title = str(chunk_analysis.get('title', f'Section {i + 1}'))
                        embedding_text = f"{title} {chunk_text[:1000]}"
                        chunk_embedding = await self.openai_manager.create_embedding(embedding_text)
                    except Exception as e:
                        logger.error(f"Error creating embedding for chunk {chunk_id}: {e}")
                        chunk_embedding = [0.0] * Config.EMBEDDING_DIMENSIONS
                    
                    chunk = DocumentChunk(
                        chunk_id=chunk_id,
                        parent_article_id=parent_article.article_id if parent_article else None,
                        document_type=document_type,
                        jurisdiction=jurisdiction,
                        chapter_number=str(chunk_analysis.get("chapter_number", "Unknown")),
                        article_number=str(chunk_analysis.get("article_number")) if chunk_analysis.get("article_number") else None,
                        title=str(chunk_analysis.get("title", f"Section {i + 1}")),
                        content=chunk_text,
                        chunk_embedding=chunk_embedding,
                        framework_type=framework_type,
                        page_number=page_number,
                        chunk_index=i,
                        processed_by_agent=self.name
                    )
                    
                    chunks.append(chunk)
                    
                    try:
                        self.es_manager.index_chunk(chunk)
                    except Exception as e:
                        logger.error(f"Error indexing chunk {chunk_id}: {e}")
                    
                    # Update parent article's chunk_ids
                    if parent_article_number and parent_article_number in article_chunk_map:
                        article_chunk_map[parent_article_number]["chunk_ids"].append(chunk_id)
                    
                    if i % 10 == 0:
                        await asyncio.sleep(0.1)
                        
                except Exception as e:
                    logger.error(f"Error creating chunk {i}: {e}")
                    continue
            
            # Step 6: Update articles with their chunk IDs
            for article_number, article_info in article_chunk_map.items():
                try:
                    article = article_info["article"]
                    article.chunk_ids = article_info["chunk_ids"]
                    self.es_manager.index_article(article)
                except Exception as e:
                    logger.error(f"Error updating article {article_number}: {e}")
            
            logger.info(f"Successfully processed {len(full_articles)} articles and {len(chunks)} chunks for {document_type} ({jurisdiction})")
            logger.info(f"Total text chunks created: {len(text_chunks)}")
            logger.info(f"Chunks with page numbers: {sum(1 for chunk in chunks if chunk.page_number is not None)}")
            
            return full_articles, chunks
            
        except Exception as e:
            logger.error(f"Error processing document {file_path}: {e}")
            raise


# Enhanced Cross-Reference Agent for Global Multi-Jurisdiction Analysis
class CrossReferenceAgent:
    """Agent for finding cross-references between documents across all jurisdictions"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager, 
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "CrossReferenceAgent"
    
    async def find_cross_references(self, chunk: DocumentChunk, all_chunks: List[DocumentChunk]) -> List[CrossDocumentLink]:
        """Find cross-references for a given chunk across ALL jurisdictions and frameworks"""
        links = []
        
        chunk_embedding = chunk.chunk_embedding
        if not chunk_embedding:
            chunk_embedding = await self.openai_manager.create_embedding(
                f"{chunk.title} {chunk.content}"
            )
        
        # Enhanced hybrid search across ALL jurisdictions
        search_results = self.es_manager.hybrid_search(
            query=f"{chunk.title} {chunk.content[:500]}",
            embedding=chunk_embedding,
            search_level="chunks"
        )
        
        # Filter candidates from DIFFERENT jurisdictions/frameworks for cross-referencing
        candidate_chunks = []
        for result in search_results.get("chunks", []):
            if (result["chunk_id"] != chunk.chunk_id and 
                (result["jurisdiction"] != chunk.jurisdiction or 
                 result["document_type"] != chunk.document_type) and
                result.get("_score", 0) > 0.4):  # Lower threshold for cross-jurisdiction matching
                candidate_chunks.append(result)
        
        # Analyze relationships with top candidates across jurisdictions
        for candidate in candidate_chunks[:10]:  # Increased for multi-jurisdiction
            relationship = await self._analyze_cross_jurisdiction_relationship(chunk, candidate)
            
            if relationship and relationship["confidence_score"] > 0.5:  # Lower threshold for cross-jurisdiction
                link = CrossDocumentLink(
                    source_chunk_id=chunk.chunk_id,
                    target_chunk_id=candidate["chunk_id"],
                    source_jurisdiction=chunk.jurisdiction,
                    target_jurisdiction=candidate["jurisdiction"],
                    relationship_type=relationship["relationship_type"],
                    confidence_score=relationship["confidence_score"]
                )
                
                links.append(link)
                self.es_manager.index_link(link)
        
        # Store findings in long-term memory
        await self._store_cross_reference_memory(chunk, links)
        
        return links
    
    async def _analyze_cross_jurisdiction_relationship(self, source_chunk: DocumentChunk, target_chunk: Dict) -> Optional[Dict]:
        """Analyze relationship between chunks from different jurisdictions/frameworks"""
        system_prompt = """
        You are an expert legal analyst specializing in global data privacy and protection laws. 
        Analyze the relationship between two legal text chunks from different jurisdictions or frameworks.
        
        Consider these types of cross-jurisdictional relationships:
        1. "equivalent": Similar requirements or obligations across jurisdictions
        2. "stricter": One jurisdiction has more stringent requirements
        3. "complementary": Requirements that work together across frameworks
        4. "contradictory": Conflicting requirements between jurisdictions
        5. "supportive": One supports implementation of the other
        6. "prerequisite": One is required before the other can be implemented
        7. "alternative": Different approaches to the same privacy goal
        
        Confidence scoring for cross-jurisdiction analysis:
        - 0.8-1.0: Clear, well-established relationship
        - 0.6-0.7: Good relationship with some interpretation needed
        - 0.5-0.5: Moderate relationship, requires careful analysis
        - Below 0.5: Weak or unclear relationship
        
        Return ONLY valid JSON format:
        {
            "relationship_type": "string",
            "confidence_score": 0.0,
            "explanation": "string"
        }
        
        Return null if no meaningful relationship exists (confidence < 0.5).
        Focus on privacy concepts like: consent, data processing, subject rights, security, 
        breach notification, data transfers, compliance obligations.
        """
        
        messages = [
            {
                "role": "user", 
                "content": f"""
                Source chunk ({source_chunk.jurisdiction} - {source_chunk.document_type}):
                Framework Type: {source_chunk.framework_type}
                Chapter: {source_chunk.chapter_number}
                Article: {source_chunk.article_number or 'N/A'}
                Title: {source_chunk.title}
                Content: {source_chunk.content[:800]}...
                
                Target chunk ({target_chunk['jurisdiction']} - {target_chunk['document_type']}):
                Framework Type: {target_chunk['framework_type']}
                Chapter: {target_chunk['chapter_number']}
                Article: {target_chunk.get('article_number', 'N/A')}
                Title: {target_chunk['title']}
                Content: {target_chunk['content'][:800]}...
                
                Analyze the cross-jurisdictional relationship between these privacy/data protection provisions.
                Consider how organizations operating in multiple jurisdictions would need to handle both requirements.
                """
            }
        ]
        
        try:
            response = await self.openai_manager.reasoning_completion(messages, system_prompt)
            
            result = safe_json_parse(response, None)
            
            if result and isinstance(result, dict):
                if all(key in result for key in ['relationship_type', 'confidence_score', 'explanation']):
                    confidence = float(result.get("confidence_score", 0))
                    if confidence >= 0.5:  # Lower threshold for cross-jurisdiction
                        return {
                            "relationship_type": str(result["relationship_type"]),
                            "confidence_score": confidence,
                            "explanation": str(result["explanation"])
                        }
            
            return None
            
        except Exception as e:
            logger.error(f"Error analyzing cross-jurisdiction relationship: {e}")
            return None
    
    async def _store_cross_reference_memory(self, chunk: DocumentChunk, links: List[CrossDocumentLink]):
        """Store cross-reference findings in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "jurisdiction": chunk.jurisdiction,
            "framework_type": chunk.framework_type,
            "chapter_number": chunk.chapter_number,
            "chunk_index": chunk.chunk_index,
            "found_links": len(links),
            "link_types": ",".join([link.relationship_type for link in links]) if links else "",
            "cross_jurisdictions": ",".join(list(set([link.target_jurisdiction for link in links]))) if links else "",
            "analysis_timestamp": datetime.now().isoformat(),
            "page_number": chunk.page_number
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="episodic",
            content=memory_content,
            namespace=["cross_reference", chunk.jurisdiction, chunk.framework_type]
        )
        
        self.es_manager.index_memory(memory)
        
        try:
            await self.memory_store.aput(
                namespace=tuple(["cross_reference", self.name]),
                key=f"analysis_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing memory in LangGraph store: {e}")


# Enhanced Linking Agent for Global Framework Support
class LinkingAgent:
    """Agent for maintaining and updating document links across all jurisdictions"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager,
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "LinkingAgent"
    
    async def update_chunk_references(self, chunk: DocumentChunk, related_links: List[CrossDocumentLink]) -> DocumentChunk:
        """Update chunk with supporting references from all jurisdictions"""
        
        related_chunks = self.es_manager.get_related_chunks(chunk.chunk_id)
        
        references = []
        for related_chunk in related_chunks:
            link = next(
                (l for l in related_links 
                 if l.source_chunk_id == chunk.chunk_id and l.target_chunk_id == related_chunk["chunk_id"] or
                    l.target_chunk_id == chunk.chunk_id and l.source_chunk_id == related_chunk["chunk_id"]),
                None
            )
            
            if link:
                reference = ChapterReference(
                    document_type=related_chunk["document_type"],
                    jurisdiction=related_chunk["jurisdiction"],
                    chapter_number=related_chunk["chapter_number"],
                    article_number=related_chunk.get("article_number"),
                    title=related_chunk["title"],
                    relevance_score=link.confidence_score,
                    relationship_type=link.relationship_type
                )
                references.append(reference)
        
        chunk.supporting_references = references
        self.es_manager.index_chunk(chunk)
        
        await self._store_linking_memory(chunk, references)
        
        return chunk
    
    async def _store_linking_memory(self, chunk: DocumentChunk, references: List[ChapterReference]):
        """Store linking activity in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "jurisdiction": chunk.jurisdiction,
            "framework_type": chunk.framework_type,
            "chunk_index": chunk.chunk_index,
            "references_added": len(references),
            "reference_types": ",".join([ref.relationship_type for ref in references]) if references else "",
            "linked_jurisdictions": ",".join(list(set([ref.jurisdiction for ref in references]))) if references else "",
            "timestamp": datetime.now().isoformat(),
            "page_number": chunk.page_number
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="procedural",
            content=memory_content,
            namespace=["linking", chunk.jurisdiction, chunk.framework_type]
        )
        
        self.es_manager.index_memory(memory)
        
        try:
            await self.memory_store.aput(
                namespace=tuple(["linking", self.name]),
                key=f"linking_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing linking memory: {e}")


# Enhanced ReAct Query Agent for Global Multi-Jurisdiction Analysis
class ReActQueryAgent:
    """ReAct-based agent for intelligent global privacy framework querying and analysis"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager, 
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "ReActQueryAgent"
        
        # Create the ReAct agent with o3-mini model
        self.model = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Enhanced system prompt for global privacy frameworks
        self.system_prompt = """
        You are a world-class expert in global data privacy and protection laws with access to comprehensive 
        databases covering multiple jurisdictions and frameworks. You can reason through complex legal queries 
        and use tools to find relevant information across all privacy regulations and standards.
        
        Your expertise covers:
        - Regional Regulations: GDPR (EU), UK GDPR, CCPA/CPRA (California), PIPEDA (Canada), 
          LGPD (Brazil), PDPA (Singapore/Thailand), Privacy Act (Australia), PIPL (China), DPA (South Africa)
        - International Standards: ISO 27001/27002, SOC 2, NIST CSF, PCI DSS
        - Organizational Policies: Company privacy policies, vendor agreements, custom frameworks
        
        Your capabilities include:
        - Searching through all global privacy frameworks simultaneously
        - Finding cross-jurisdictional relationships and requirements
        - Analyzing compliance obligations across multiple regions
        - Providing detailed comparative analysis between frameworks
        - Identifying conflicts and harmonization opportunities
        - Offering implementation guidance for multi-jurisdictional operations
        
        When answering questions:
        1. Think step by step about what jurisdictions and frameworks are relevant
        2. Use appropriate tools to search across all relevant sources
        3. Analyze relationships and differences between jurisdictions
        4. Consider practical implementation challenges for global organizations
        5. Provide comprehensive, well-reasoned answers with specific citations
        6. Highlight regional differences and compliance strategies
        
        Always be thorough in your analysis and use multiple searches across different 
        jurisdictions to provide complete, globally-informed answers.
        """
        
        # Create enhanced tools list for the ReAct agent
        self.tools = [
            self._create_search_articles_tool(),
            self._create_search_chunks_tool(),
            self._create_search_both_tool(),
            self._create_get_relationships_tool(),
            self._create_compare_jurisdictions_tool(),
            self._create_analyze_global_concept_tool(),
            self._create_compliance_gap_analysis_tool(),
            self._create_jurisdiction_summary_tool()
        ]
        
        # Create the ReAct agent
        self.react_agent = None
    
    def _create_compare_jurisdictions_tool(self):
        """Create tool for comparing specific topics across multiple jurisdictions"""
        @tool
        async def compare_jurisdictions_tool(topic: str, jurisdictions: str = "all") -> str:
            """
            Compare how a specific privacy topic is handled across multiple jurisdictions.
            
            Args:
                topic: The privacy topic or concept to compare
                jurisdictions: Comma-separated list of jurisdictions or "all" for comprehensive comparison
            
            Returns:
                String with comparative analysis across jurisdictions
            """
            try:
                embedding = await self.openai_manager.create_embedding(topic)
                
                # Get all results first
                all_results = self.es_manager.hybrid_search(
                    topic, embedding, search_level="both"
                )
                
                # Group by jurisdiction
                jurisdiction_results = {}
                
                for result_type in ["articles", "chunks"]:
                    if result_type in all_results:
                        for item in all_results[result_type]:
                            jurisdiction = item.get('jurisdiction', 'Unknown')
                            if jurisdiction not in jurisdiction_results:
                                jurisdiction_results[jurisdiction] = []
                            jurisdiction_results[jurisdiction].append({
                                'type': result_type[:-1],  # Remove 's'
                                'data': item
                            })
                
                # Filter jurisdictions if specified
                if jurisdictions != "all":
                    requested_jurisdictions = [j.strip().upper() for j in jurisdictions.split(",")]
                    jurisdiction_results = {
                        k: v for k, v in jurisdiction_results.items() 
                        if k.upper() in requested_jurisdictions
                    }
                
                comparison = []
                comparison.append(f"=== GLOBAL COMPARISON: {topic.upper()} ===\n")
                
                for jurisdiction, results in jurisdiction_results.items():
                    comparison.append(f"--- {jurisdiction} ---")
                    
                    # Get best result for this jurisdiction
                    best_result = max(results, key=lambda x: x['data'].get('_score', 0)) if results else None
                    
                    if best_result:
                        data = best_result['data']
                        comparison.append(
                            f"Primary {best_result['type']}: {data.get('title', 'N/A')}\n"
                            f"Document: {data.get('document_type', 'N/A')}\n"
                            f"Framework: {data.get('framework_type', 'N/A')}\n"
                            f"Content: {data.get('content' if 'content' in data else 'full_content', '')[:400]}...\n"
                        )
                    else:
                        comparison.append("No relevant content found.\n")
                
                if not jurisdiction_results:
                    return f"No results found for topic: {topic}"
                
                return "\n\n".join(comparison)
                
            except Exception as e:
                return f"Error comparing jurisdictions: {str(e)}"
        
        return compare_jurisdictions_tool
    
    def _create_analyze_global_concept_tool(self):
        """Create tool for deep analysis of privacy concepts across all frameworks"""
        @tool
        async def analyze_global_concept_tool(concept: str) -> str:
            """
            Perform comprehensive global analysis of a privacy concept across all frameworks.
            
            Args:
                concept: The privacy concept or term to analyze globally
            
            Returns:
                String with comprehensive global analysis
            """
            try:
                embedding = await self.openai_manager.create_embedding(concept)
                
                results = self.es_manager.hybrid_search(
                    concept, embedding, search_level="both"
                )
                
                analysis = []
                analysis.append(f"=== GLOBAL PRIVACY ANALYSIS: {concept.upper()} ===\n")
                
                # Group by framework type
                framework_groups = {}
                all_items = []
                
                for result_type in ["articles", "chunks"]:
                    if result_type in results:
                        for item in results[result_type]:
                            all_items.append(item)
                            framework_type = item.get('framework_type', 'unknown')
                            if framework_type not in framework_groups:
                                framework_groups[framework_type] = []
                            framework_groups[framework_type].append(item)
                
                # Analyze by framework type
                for framework_type, items in framework_groups.items():
                    analysis.append(f"--- {framework_type.upper()} FRAMEWORKS ---")
                    
                    jurisdiction_coverage = {}
                    for item in items[:5]:  # Top 5 per framework type
                        jurisdiction = item.get('jurisdiction', 'Unknown')
                        if jurisdiction not in jurisdiction_coverage:
                            jurisdiction_coverage[jurisdiction] = []
                        jurisdiction_coverage[jurisdiction].append(item)
                    
                    for jurisdiction, jurisdiction_items in jurisdiction_coverage.items():
                        best_item = max(jurisdiction_items, key=lambda x: x.get('_score', 0))
                        analysis.append(
                            f"{jurisdiction}: {best_item.get('document_type', 'N/A')} - "
                            f"{best_item.get('title', 'N/A')}\n"
                            f"   Content: {best_item.get('content' if 'content' in best_item else 'full_content', '')[:200]}...\n"
                        )
                
                # Cross-jurisdictional insights
                if len(framework_groups) > 1:
                    analysis.append("--- CROSS-FRAMEWORK INSIGHTS ---")
                    analysis.append(f"Concept appears across {len(framework_groups)} framework types")
                    analysis.append(f"Total jurisdictions covered: {len(set(item.get('jurisdiction') for item in all_items))}")
                    
                    # Count references
                    total_refs = sum(len(item.get('supporting_references', [])) for item in all_items)
                    if total_refs > 0:
                        analysis.append(f"Cross-references found: {total_refs}")
                
                if not all_items:
                    return f"No global content found for concept: {concept}"
                
                return "\n\n".join(analysis)
                
            except Exception as e:
                return f"Error analyzing global concept: {str(e)}"
        
        return analyze_global_concept_tool
    
    def _create_compliance_gap_analysis_tool(self):
        """Create tool for identifying compliance gaps across jurisdictions"""
        @tool
        async def compliance_gap_analysis_tool(business_context: str, target_jurisdictions: str) -> str:
            """
            Analyze compliance gaps for a business operating across multiple jurisdictions.
            
            Args:
                business_context: Description of business operations and data processing
                target_jurisdictions: Comma-separated list of jurisdictions to analyze
            
            Returns:
                String with compliance gap analysis and recommendations
            """
            try:
                jurisdictions_list = [j.strip().upper() for j in target_jurisdictions.split(",")]
                
                # Search for relevant requirements in each jurisdiction
                embedding = await self.openai_manager.create_embedding(business_context)
                
                gap_analysis = []
                gap_analysis.append(f"=== COMPLIANCE GAP ANALYSIS ===")
                gap_analysis.append(f"Business Context: {business_context}")
                gap_analysis.append(f"Target Jurisdictions: {', '.join(jurisdictions_list)}\n")
                
                jurisdiction_requirements = {}
                
                for jurisdiction in jurisdictions_list:
                    # Search for requirements in this jurisdiction
                    results = self.es_manager.hybrid_search(
                        business_context, embedding, 
                        filters={"jurisdiction": [jurisdiction]},
                        search_level="both"
                    )
                    
                    jurisdiction_requirements[jurisdiction] = {
                        "articles": results.get("articles", [])[:3],
                        "chunks": results.get("chunks", [])[:5]
                    }
                
                # Analyze each jurisdiction
                for jurisdiction, requirements in jurisdiction_requirements.items():
                    gap_analysis.append(f"--- {jurisdiction} REQUIREMENTS ---")
                    
                    if requirements["articles"]:
                        gap_analysis.append("Primary Regulations:")
                        for article in requirements["articles"]:
                            gap_analysis.append(
                                f"• {article.get('document_type')}: {article.get('title', 'N/A')}\n"
                                f"  Key requirement: {article.get('full_content', '')[:200]}...\n"
                            )
                    
                    if requirements["chunks"]:
                        gap_analysis.append("Specific Obligations:")
                        for chunk in requirements["chunks"]:
                            gap_analysis.append(
                                f"• {chunk.get('title', 'N/A')}\n"
                                f"  Requirement: {chunk.get('content', '')[:150]}...\n"
                            )
                    
                    if not requirements["articles"] and not requirements["chunks"]:
                        gap_analysis.append("No specific requirements found for this business context.\n")
                
                # Gap identification
                gap_analysis.append("--- POTENTIAL COMPLIANCE GAPS ---")
                
                all_requirements = []
                for jurisdiction, requirements in jurisdiction_requirements.items():
                    all_requirements.extend(requirements["articles"] + requirements["chunks"])
                
                if len(all_requirements) > 1:
                    gap_analysis.append("Cross-jurisdictional considerations needed:")
                    gap_analysis.append(f"• {len(set(req.get('document_type') for req in all_requirements))} different frameworks to comply with")
                    gap_analysis.append(f"• {len(jurisdictions_list)} jurisdictions with potentially different requirements")
                    gap_analysis.append("• Review conflicting requirements and implement strictest standards")
                else:
                    gap_analysis.append("Limited regulatory coverage found - may need additional research")
                
                return "\n\n".join(gap_analysis)
                
            except Exception as e:
                return f"Error in compliance gap analysis: {str(e)}"
        
        return compliance_gap_analysis_tool
    
    def _create_jurisdiction_summary_tool(self):
        """Create tool for getting jurisdiction coverage summary"""
        @tool
        async def jurisdiction_summary_tool() -> str:
            """
            Get a summary of all jurisdictions and frameworks available in the system.
            
            Returns:
                String with comprehensive system coverage summary
            """
            try:
                summary_data = self.es_manager.get_jurisdiction_summary()
                
                if not summary_data:
                    return "No jurisdiction data available"
                
                summary = []
                summary.append("=== GLOBAL PRIVACY FRAMEWORK COVERAGE ===\n")
                
                summary.append(f"Total Documents Indexed: {summary_data.get('total_documents', 0):,}")
                
                if summary_data.get('jurisdictions'):
                    summary.append(f"\nJurisdictions Covered ({len(summary_data['jurisdictions'])}):")
                    for jurisdiction in sorted(summary_data['jurisdictions']):
                        # Get readable name from mappings
                        readable_name = None
                        for key, info in jurisdiction_mapper.mappings.items():
                            if info.get('region') == jurisdiction:
                                readable_name = f"{jurisdiction} ({info.get('full_name', '')})"
                                break
                        summary.append(f"• {readable_name or jurisdiction}")
                
                if summary_data.get('document_types'):
                    summary.append(f"\nDocument Types ({len(summary_data['document_types'])}):")
                    for doc_type in sorted(summary_data['document_types']):
                        # Get readable name from mappings
                        readable_name = jurisdiction_mapper.mappings.get(doc_type.lower(), {}).get('full_name', doc_type)
                        summary.append(f"• {doc_type} - {readable_name}")
                
                if summary_data.get('framework_types'):
                    summary.append(f"\nFramework Types ({len(summary_data['framework_types'])}):")
                    for framework_type in sorted(summary_data['framework_types']):
                        summary.append(f"• {framework_type.title()}")
                
                summary.append("\n=== CAPABILITIES ===")
                summary.append("• Cross-jurisdictional compliance analysis")
                summary.append("• Multi-framework comparison and harmonization")
                summary.append("• Gap analysis for global operations")
                summary.append("• Conflict identification and resolution")
                summary.append("• Implementation guidance for complex compliance scenarios")
                
                return "\n".join(summary)
                
            except Exception as e:
                return f"Error getting jurisdiction summary: {str(e)}"
        
        return jurisdiction_summary_tool
    
    # Include enhanced versions of the other tools with multi-jurisdiction support
    def _create_search_articles_tool(self):
        """Create tool for searching full articles with enhanced multi-jurisdiction support"""
        @tool
        async def search_articles_tool(query: str, jurisdiction: str = None, document_type: str = None) -> str:
            """
            Search for full privacy/data protection articles based on a query.
            
            Args:
                query: The search query describing what you're looking for
                jurisdiction: Optional filter for specific jurisdiction (EU, California, Canada, etc.)
                document_type: Optional filter for specific document type (GDPR, CCPA, ISO_27001, etc.)
            
            Returns:
                String containing relevant articles with titles, content, and scores
            """
            try:
                if not query or not isinstance(query, str):
                    return "Error: Invalid query provided"
                
                embedding = await self.openai_manager.create_embedding(query.strip())
                filters = {}
                if jurisdiction:
                    filters["jurisdiction"] = [jurisdiction.upper()]
                if document_type:
                    filters["document_type"] = [document_type.upper()]
                
                results = self.es_manager.hybrid_search(query.strip(), embedding, filters, search_level="articles")
                
                if not results or not results.get("articles"):
                    return f"No articles found for query: {query}"
                
                formatted_results = []
                articles = results["articles"][:8]  # Increased for multi-jurisdiction
                
                for i, article in enumerate(articles):
                    try:
                        score = article.get('_score', 0)
                        doc_type = article.get('document_type', 'N/A')
                        jurisdiction = article.get('jurisdiction', 'N/A')
                        framework_type = article.get('framework_type', 'N/A')
                        chapter = article.get('chapter_number', 'N/A')
                        article_num = article.get('article_number', 'N/A')
                        title = article.get('title', 'N/A')
                        content = article.get('full_content', '')
                        concepts = article.get('key_concepts', [])
                        
                        if not isinstance(concepts, list):
                            concepts = []
                        
                        formatted_results.append(
                            f"Article {i+1} (Score: {score:.3f}):\n"
                            f"Jurisdiction: {jurisdiction}\n"
                            f"Document: {doc_type}\n"
                            f"Framework: {framework_type}\n"
                            f"Chapter: {chapter}, Article: {article_num}\n"
                            f"Title: {title}\n"
                            f"Content: {content[:500]}...\n"
                            f"Key Concepts: {', '.join(concepts[:5])}\n"
                        )
                    except Exception as e:
                        logger.error(f"Error formatting article result {i}: {e}")
                        continue
                
                if not formatted_results:
                    return f"Found {len(articles)} articles but could not format results for query: {query}"
                
                return "\n---\n".join(formatted_results)
                
            except Exception as e:
                logger.error(f"Error in search_articles_tool: {e}")
                return f"Error searching articles: {str(e)}"
        
        return search_articles_tool
    
    def _create_search_chunks_tool(self):
        """Create tool for searching document chunks with enhanced multi-jurisdiction support"""
        @tool
        async def search_chunks_tool(query: str, jurisdiction: str = None, document_type: str = None) -> str:
            """
            Search for specific privacy/data protection document chunks/sections based on a query.
            
            Args:
                query: The search query describing what you're looking for
                jurisdiction: Optional filter for specific jurisdiction (EU, California, Canada, etc.)
                document_type: Optional filter for specific document type (GDPR, CCPA, ISO_27001, etc.)
            
            Returns:
                String containing relevant chunks with context and scores
            """
            try:
                if not query or not isinstance(query, str):
                    return "Error: Invalid query provided"
                
                embedding = await self.openai_manager.create_embedding(query.strip())
                filters = {}
                if jurisdiction:
                    filters["jurisdiction"] = [jurisdiction.upper()]
                if document_type:
                    filters["document_type"] = [document_type.upper()]
                
                results = self.es_manager.hybrid_search(query.strip(), embedding, filters, search_level="chunks")
                
                if not results or not results.get("chunks"):
                    return f"No chunks found for query: {query}"
                
                formatted_results = []
                chunks = results["chunks"][:10]  # Increased for multi-jurisdiction
                
                for i, chunk in enumerate(chunks):
                    try:
                        score = chunk.get('_score', 0)
                        doc_type = chunk.get('document_type', 'N/A')
                        jurisdiction = chunk.get('jurisdiction', 'N/A')
                        framework_type = chunk.get('framework_type', 'N/A')
                        chapter = chunk.get('chapter_number', 'N/A')
                        article_num = chunk.get('article_number', 'N/A')
                        title = chunk.get('title', 'N/A')
                        content = chunk.get('content', '')
                        page_num = chunk.get('page_number', 'N/A')
                        references = chunk.get('supporting_references', [])
                        
                        formatted_results.append(
                            f"Chunk {i+1} (Score: {score:.3f}):\n"
                            f"Jurisdiction: {jurisdiction}\n"
                            f"Document: {doc_type}\n"
                            f"Framework: {framework_type}\n"
                            f"Chapter: {chapter}, Article: {article_num}\n"
                            f"Title: {title}\n"
                            f"Page: {page_num}\n"
                            f"Content: {content[:400]}...\n"
                            f"Cross-references: {len(references)}\n"
                        )
                    except Exception as e:
                        logger.error(f"Error formatting chunk result {i}: {e}")
                        continue
                
                if not formatted_results:
                    return f"Found {len(chunks)} chunks but could not format results for query: {query}"
                
                return "\n---\n".join(formatted_results)
                
            except Exception as e:
                logger.error(f"Error in search_chunks_tool: {e}")
                return f"Error searching chunks: {str(e)}"
        
        return search_chunks_tool
    
    def _create_search_both_tool(self):
        """Create tool for comprehensive search across articles and chunks with multi-jurisdiction support"""
        @tool
        async def search_both_tool(query: str, jurisdiction: str = None, document_type: str = None) -> str:
            """
            Comprehensive search across both full articles and chunks for maximum coverage.
            
            Args:
                query: The search query describing what you're looking for
                jurisdiction: Optional filter for specific jurisdiction (EU, California, Canada, etc.)
                document_type: Optional filter for specific document type (GDPR, CCPA, ISO_27001, etc.)
            
            Returns:
                String containing both article and chunk results with analysis
            """
            try:
                if not query or not isinstance(query, str):
                    return "Error: Invalid query provided"
                
                embedding = await self.openai_manager.create_embedding(query.strip())
                filters = {}
                if jurisdiction:
                    filters["jurisdiction"] = [jurisdiction.upper()]
                if document_type:
                    filters["document_type"] = [document_type.upper()]
                
                results = self.es_manager.hybrid_search(query.strip(), embedding, filters, search_level="both")
                
                formatted_output = []
                
                # Articles section
                if results.get("articles"):
                    formatted_output.append("=== FULL ARTICLES ===")
                    for i, article in enumerate(results["articles"][:4]):  # Increased for multi-jurisdiction
                        formatted_output.append(
                            f"Article {i+1}: {article.get('jurisdiction')} - {article.get('document_type')} - "
                            f"{article.get('title', 'N/A')} (Score: {article.get('_score', 0):.3f})\n"
                            f"Framework: {article.get('framework_type', 'N/A')}\n"
                            f"Chapter {article.get('chapter_number')}, Article {article.get('article_number')}\n"
                            f"Content: {article.get('full_content', '')[:300]}...\n"
                        )
                
                # Chunks section
                if results.get("chunks"):
                    formatted_output.append("=== SPECIFIC SECTIONS ===")
                    for i, chunk in enumerate(results["chunks"][:6]):  # Increased for multi-jurisdiction
                        formatted_output.append(
                            f"Section {i+1}: {chunk.get('jurisdiction')} - {chunk.get('document_type')} - "
                            f"{chunk.get('title', 'N/A')} (Score: {chunk.get('_score', 0):.3f})\n"
                            f"Framework: {chunk.get('framework_type', 'N/A')}\n"
                            f"Chapter {chunk.get('chapter_number')}, Page {chunk.get('page_number', 'N/A')}\n"
                            f"Content: {chunk.get('content', '')[:250]}...\n"
                        )
                
                if not formatted_output:
                    return f"No results found for query: {query}"
                
                return "\n\n".join(formatted_output)
                
            except Exception as e:
                return f"Error in comprehensive search: {str(e)}"
        
        return search_both_tool
    
    def _create_get_relationships_tool(self):
        """Create tool for finding relationships between chunks across all jurisdictions"""
        @tool
        async def get_relationships_tool(chunk_id: str) -> str:
            """
            Find all relationships and cross-references for a specific chunk across all jurisdictions.
            
            Args:
                chunk_id: The ID of the chunk to find relationships for
            
            Returns:
                String describing all related chunks and their relationships across jurisdictions
            """
            try:
                related_chunks = self.es_manager.get_related_chunks(chunk_id)
                
                if not related_chunks:
                    return f"No relationships found for chunk ID: {chunk_id}"
                
                # Group by jurisdiction
                jurisdiction_groups = {}
                for related in related_chunks:
                    jurisdiction = related.get('jurisdiction', 'Unknown')
                    if jurisdiction not in jurisdiction_groups:
                        jurisdiction_groups[jurisdiction] = []
                    jurisdiction_groups[jurisdiction].append(related)
                
                formatted_results = []
                formatted_results.append(f"Found {len(related_chunks)} related chunks across {len(jurisdiction_groups)} jurisdictions:\n")
                
                for jurisdiction, chunks in jurisdiction_groups.items():
                    formatted_results.append(f"--- {jurisdiction} ---")
                    for i, related in enumerate(chunks):
                        formatted_results.append(
                            f"{i+1}. {related.get('document_type', 'N/A')} - {related.get('title', 'N/A')}\n"
                            f"   Framework: {related.get('framework_type', 'N/A')}\n"
                            f"   Chapter: {related.get('chapter_number', 'N/A')}\n"
                            f"   Content: {related.get('content', '')[:200]}...\n"
                        )
                
                return "\n".join(formatted_results)
                
            except Exception as e:
                return f"Error finding relationships: {str(e)}"
        
        return get_relationships_tool
    
    def create_react_agent(self) -> Any:
        """Create and configure the ReAct agent with proper error handling"""
        try:
            if not Config.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY is required but not set")
            
            try:
                chat_model = ChatOpenAI(
                    model=Config.O3_MINI_MODEL,
                    api_key=Config.OPENAI_API_KEY,
                    base_url=Config.OPENAI_BASE_URL,
                    temperature=0.1,
                    streaming=False,
                    timeout=60,
                    max_retries=3
                )
                
                test_response = chat_model.invoke("Test")
                logger.info("Chat model successfully initialized and tested")
                
            except Exception as e:
                logger.error(f"Error initializing chat model: {e}")
                raise ValueError(f"Failed to initialize chat model: {e}")
            
            if not self.tools or len(self.tools) == 0:
                raise ValueError("No tools available for ReAct agent")
            
            logger.info(f"Creating global privacy ReAct agent with {len(self.tools)} tools")
            
            try:
                self.react_agent = create_react_agent(
                    model=chat_model,
                    tools=self.tools,
                    prompt=self.system_prompt
                )
                
                logger.info("Global privacy ReAct agent created successfully")
                return self.react_agent
                
            except Exception as e:
                logger.error(f"Error creating ReAct agent: {e}")
                raise ValueError(f"Failed to create ReAct agent: {e}")
            
        except Exception as e:
            logger.error(f"Error in create_react_agent: {e}")
            raise
    
    async def query(self, user_query: str, thread_id: str = None) -> Dict[str, Any]:
        """Process a user query using the ReAct agent with comprehensive error handling"""
        try:
            if not user_query or not isinstance(user_query, str):
                return {
                    "query": user_query,
                    "answer": "Error: Invalid query provided",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            user_query = user_query.strip()
            if len(user_query) < 3:
                return {
                    "query": user_query,
                    "answer": "Error: Query too short, please provide a more detailed question",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            if not self.react_agent:
                try:
                    self.create_react_agent()
                except Exception as e:
                    return {
                        "query": user_query,
                        "answer": f"Error: Could not initialize ReAct agent - {str(e)}",
                        "thread_id": thread_id,
                        "messages": []
                    }
            
            if not thread_id:
                thread_id = f"global_privacy_query_{uuid.uuid4().hex[:8]}"
            
            config = {
                "configurable": {
                    "thread_id": thread_id,
                }
            }
            
            try:
                logger.info(f"Processing global privacy ReAct query: {user_query[:100]}...")
                
                response = self.react_agent.invoke(
                    {"messages": [{"role": "user", "content": user_query}]},
                    config=config
                )
                
                if not response:
                    return {
                        "query": user_query,
                        "answer": "Error: No response from ReAct agent",
                        "thread_id": thread_id,
                        "messages": []
                    }
                
            except Exception as e:
                logger.error(f"Error invoking ReAct agent: {e}")
                return {
                    "query": user_query,
                    "answer": f"Error during ReAct processing: {str(e)}",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            messages = response.get("messages", [])
            final_answer = "No answer generated"
            
            try:
                for message in reversed(messages):
                    if hasattr(message, 'content') and message.content:
                        content = str(message.content).strip()
                        if (content and 
                            not content.startswith('{"') and 
                            not content.startswith('Error:') and
                            len(content) > 10):
                            final_answer = content
                            break
                
                if final_answer == "No answer generated" and messages:
                    for message in messages:
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content).strip()
                            if content and len(content) > 20:
                                final_answer = content
                                break
                
            except Exception as e:
                logger.error(f"Error extracting answer from messages: {e}")
                final_answer = f"Error extracting answer: {str(e)}"
            
            try:
                await self._store_query_memory(user_query, final_answer, thread_id)
            except Exception as e:
                logger.error(f"Error storing query memory: {e}")
            
            return {
                "query": user_query,
                "answer": final_answer,
                "thread_id": thread_id,
                "messages": messages
            }
            
        except Exception as e:
            logger.error(f"Unexpected error processing ReAct query: {e}")
            return {
                "query": user_query or "Unknown query",
                "answer": f"Unexpected error processing query: {str(e)}",
                "thread_id": thread_id,
                "messages": []
            }
    
    async def _store_query_memory(self, query: str, answer: str, thread_id: str):
        """Store query and response in long-term memory"""
        try:
            memory_content = {
                "query": query,
                "answer_length": len(answer),
                "thread_id": thread_id or "unknown",
                "timestamp": datetime.now().isoformat(),
                "agent_type": "GlobalPrivacyReAct"
            }
            
            memory = AgentMemory(
                agent_name=self.name,
                memory_type="episodic",
                content=memory_content,
                namespace=["global_privacy_queries", "multi_jurisdiction_analysis"]
            )
            
            self.es_manager.index_memory(memory)
            
            await self.memory_store.aput(
                namespace=tuple(["global_privacy_queries", self.name]),
                key=f"query_{uuid.uuid4()}",
                value=memory_content
            )
            
        except Exception as e:
            logger.error(f"Error storing query memory: {e}")


# Enhanced Tools for Global Multi-Jurisdiction Support
@tool
async def search_similar_chunks(query: str, jurisdiction: str = None, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for similar chunks using hybrid search with multi-jurisdiction support"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        openai_manager = config["configurable"]["openai_manager"]
        
        embedding = await openai_manager.create_embedding(query)
        
        filters = {}
        if jurisdiction:
            filters["jurisdiction"] = [jurisdiction.upper()]
        if document_type:
            filters["document_type"] = [document_type.upper()]
        
        results = es_manager.hybrid_search(query, embedding, filters, search_level="chunks")
        return results.get("chunks", [])
        
    except Exception as e:
        logger.error(f"Error in search_similar_chunks: {e}")
        return []


@tool
async def search_full_articles(query: str, jurisdiction: str = None, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for full articles using hybrid search with multi-jurisdiction support"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        openai_manager = config["configurable"]["openai_manager"]
        
        embedding = await openai_manager.create_embedding(query)
        
        filters = {}
        if jurisdiction:
            filters["jurisdiction"] = [jurisdiction.upper()]
        if document_type:
            filters["document_type"] = [document_type.upper()]
        
        results = es_manager.hybrid_search(query, embedding, filters, search_level="articles")
        return results.get("articles", [])
        
    except Exception as e:
        logger.error(f"Error in search_full_articles: {e}")
        return []


@tool
async def search_both_levels(query: str, jurisdiction: str = None, document_type: str = None, config: RunnableConfig = None) -> Dict[str, List[Dict]]:
    """Search both full articles and chunks using hybrid search with multi-jurisdiction support"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        openai_manager = config["configurable"]["openai_manager"]
        
        embedding = await openai_manager.create_embedding(query)
        
        filters = {}
        if jurisdiction:
            filters["jurisdiction"] = [jurisdiction.upper()]
        if document_type:
            filters["document_type"] = [document_type.upper()]
        
        results = es_manager.hybrid_search(query, embedding, filters, search_level="both")
        return results
        
    except Exception as e:
        logger.error(f"Error in search_both_levels: {e}")
        return {"articles": [], "chunks": []}


@tool
async def get_chunk_relationships(chunk_id: str, config: RunnableConfig = None) -> List[Dict]:
    """Get all relationships for a specific chunk across all jurisdictions"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        related_chunks = es_manager.get_related_chunks(chunk_id)
        return related_chunks
        
    except Exception as e:
        logger.error(f"Error in get_chunk_relationships: {e}")
        return []


@tool
async def store_agent_memory(agent_name: str, memory_type: str, content: Dict, 
                           namespace: List[str], config: RunnableConfig = None) -> bool:
    """Store information in agent's long-term memory"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        memory_store = config["configurable"]["memory_store"]
        
        serializable_content = {}
        for key, value in content.items():
            if isinstance(value, list):
                if all(isinstance(item, str) for item in value):
                    serializable_content[key] = ",".join(value) if value else ""
                else:
                    serializable_content[key] = str(value)
            else:
                serializable_content[key] = value
        
        memory = AgentMemory(
            agent_name=agent_name,
            memory_type=memory_type,
            content=serializable_content,
            namespace=namespace
        )
        
        es_success = es_manager.index_memory(memory)
        
        try:
            await memory_store.aput(
                namespace=tuple(namespace + [agent_name]),
                key=memory.memory_id,
                value=serializable_content
            )
            return es_success
        except Exception as e:
            logger.error(f"Error storing in LangGraph memory: {e}")
            return es_success
            
    except Exception as e:
        logger.error(f"Error in store_agent_memory: {e}")
        return False


@tool
async def react_query_global_privacy(query: str, config: RunnableConfig = None) -> str:
    """Use ReAct agent for intelligent global privacy analysis and querying"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        openai_manager = config["configurable"]["openai_manager"]
        memory_store = config["configurable"]["memory_store"]
        
        react_agent = ReActQueryAgent(openai_manager, es_manager, memory_store)
        
        result = await react_agent.query(query)
        
        return result["answer"]
        
    except Exception as e:
        logger.error(f"Error in ReAct global privacy query: {e}")
        return f"Error processing query with ReAct agent: {str(e)}"


# Enhanced Main workflow nodes for multi-jurisdiction support
async def document_processing_node(state: ProcessingState) -> ProcessingState:
    """Process all PDF documents from the data directory with multi-jurisdiction support"""
    logger.info("Starting enhanced multi-jurisdiction document processing...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        
        processor = DocumentProcessingAgent(openai_manager, es_manager)
        
        all_articles = []
        all_chunks = []
        
        data_dir = "data"
        if not os.path.exists(data_dir):
            error_msg = f"Data directory not found: {data_dir}. Please create the directory and add PDF files."
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        pdf_pattern = os.path.join(data_dir, "*.pdf")
        pdf_files = glob.glob(pdf_pattern)
        
        if not pdf_files:
            error_msg = f"No PDF files found in {data_dir} directory. Please add PDF files to process."
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        logger.info(f"Found {len(pdf_files)} PDF files for multi-jurisdiction processing:")
        for pdf_file in pdf_files:
            logger.info(f"  - {os.path.basename(pdf_file)}")
        
        successful_files = 0
        jurisdiction_counts = {}
        
        for i, pdf_file in enumerate(pdf_files):
            try:
                logger.info(f"Processing PDF {i+1}/{len(pdf_files)} with multi-jurisdiction support: {pdf_file}")
                
                articles, chunks = await processor.process_document(pdf_file)
                
                if articles or chunks:
                    all_articles.extend(articles)
                    all_chunks.extend(chunks)
                    successful_files += 1
                    
                    # Track jurisdictions
                    for article in articles:
                        jurisdiction = article.jurisdiction
                        jurisdiction_counts[jurisdiction] = jurisdiction_counts.get(jurisdiction, 0) + 1
                    
                    logger.info(f"Successfully processed {pdf_file}: {len(articles)} articles, {len(chunks)} chunks")
                else:
                    logger.warning(f"No content extracted from {pdf_file}")
                
                if i < len(pdf_files) - 1:
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"Failed to process {pdf_file}: {e}")
                continue
        
        if successful_files == 0:
            error_msg = "No documents were successfully processed. Please check your PDF files and try again."
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        if successful_files < len(pdf_files):
            logger.warning(f"Only {successful_files}/{len(pdf_files)} files processed successfully")
        
        state["full_articles"] = all_articles
        state["documents"] = all_chunks
        state["processing_stage"] = "cross_referencing"
        
        # Enhanced logging for multi-jurisdiction
        logger.info(f"Multi-jurisdiction document processing completed:")
        logger.info(f"  Total articles: {len(all_articles)}")
        logger.info(f"  Total chunks: {len(all_chunks)}")
        logger.info(f"  Jurisdictions covered: {list(jurisdiction_counts.keys())}")
        
        success_message = AIMessage(
            content=f"Multi-jurisdiction document processing completed successfully. "
                   f"Processed {successful_files} PDF files across {len(jurisdiction_counts)} jurisdictions, "
                   f"created {len(all_articles)} articles and {len(all_chunks)} chunks."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in multi-jurisdiction document processing node: {e}")
        error_message = AIMessage(content=f"Multi-jurisdiction document processing failed: {str(e)}")
        state["messages"].append(error_message)
        state["processing_stage"] = "error"
        raise


async def cross_reference_node(state: ProcessingState) -> ProcessingState:
    """Find cross-references between documents across ALL jurisdictions"""
    logger.info("Finding cross-references across ALL jurisdictions and frameworks...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        memory_store = InMemoryStore()
        
        cross_ref_agent = CrossReferenceAgent(openai_manager, es_manager, memory_store)
        
        all_links = []
        documents = state.get("documents", [])
        
        if not documents:
            logger.warning("No documents found for cross-reference analysis")
            state["cross_links"] = []
            state["processing_stage"] = "linking"
            return state
        
        # Group documents by jurisdiction for analysis
        jurisdiction_groups = {}
        for doc in documents:
            jurisdiction = doc.jurisdiction
            if jurisdiction not in jurisdiction_groups:
                jurisdiction_groups[jurisdiction] = []
            jurisdiction_groups[jurisdiction].append(doc)
        
        logger.info(f"Analyzing cross-references across {len(jurisdiction_groups)} jurisdictions for {len(documents)} chunks...")
        
        successful_analyses = 0
        cross_jurisdiction_links = 0
        
        for i, chunk in enumerate(documents):
            try:
                logger.info(f"Analyzing global cross-references for chunk {i+1}/{len(documents)}: {chunk.title[:50]}...")
                
                links = await cross_ref_agent.find_cross_references(chunk, documents)
                
                if links:
                    all_links.extend(links)
                    # Count cross-jurisdiction links
                    cross_jurisdiction_links += sum(1 for link in links if link.source_jurisdiction != link.target_jurisdiction)
                    logger.info(f"Found {len(links)} cross-document links for chunk {chunk.chunk_index}")
                
                successful_analyses += 1
                
                if (i + 1) % 20 == 0:
                    logger.info(f"Progress: {i+1}/{len(documents)} chunks analyzed, {len(all_links)} total links found")
                
                if i < len(documents) - 1:
                    await asyncio.sleep(0.3)  # Slightly longer delay for cross-jurisdiction analysis
                    
            except Exception as e:
                logger.error(f"Error analyzing chunk {i+1} ({chunk.chunk_id}): {e}")
                continue
        
        state["cross_links"] = all_links
        state["processing_stage"] = "linking"
        
        logger.info(f"Global cross-reference analysis completed:")
        logger.info(f"  Total links found: {len(all_links)}")
        logger.info(f"  Cross-jurisdiction links: {cross_jurisdiction_links}")
        logger.info(f"  Successful analyses: {successful_analyses}/{len(documents)}")
        
        success_message = AIMessage(
            content=f"Global cross-reference analysis completed. Found {len(all_links)} cross-document links "
                   f"({cross_jurisdiction_links} cross-jurisdiction) from {successful_analyses} chunk analyses."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in global cross-reference node: {e}")
        error_message = AIMessage(content=f"Global cross-reference analysis failed: {str(e)}")
        state["messages"].append(error_message)
        state["cross_links"] = []
        state["processing_stage"] = "linking"
        return state


async def linking_node(state: ProcessingState) -> ProcessingState:
    """Update chunks with supporting references across all jurisdictions"""
    logger.info("Updating chunk references with global cross-jurisdictional linking...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        memory_store = InMemoryStore()
        
        linking_agent = LinkingAgent(openai_manager, es_manager, memory_store)
        
        documents = state.get("documents", [])
        cross_links = state.get("cross_links", [])
        
        if not documents:
            logger.warning("No documents found for linking")
            state["processing_stage"] = "completed"
            return state
        
        logger.info(f"Updating references for {len(documents)} chunks using {len(cross_links)} global cross-links...")
        
        updated_chunks = []
        successful_updates = 0
        cross_jurisdiction_references = 0
        
        for i, chunk in enumerate(documents):
            try:
                logger.info(f"Updating global references for chunk {i+1}/{len(documents)}")
                
                chunk_links = [
                    link for link in cross_links 
                    if link.source_chunk_id == chunk.chunk_id or link.target_chunk_id == chunk.chunk_id
                ]
                
                updated_chunk = await linking_agent.update_chunk_references(chunk, chunk_links)
                updated_chunks.append(updated_chunk)
                successful_updates += 1
                
                # Count cross-jurisdiction references
                cross_jurisdiction_references += sum(
                    1 for ref in updated_chunk.supporting_references 
                    if ref.jurisdiction != chunk.jurisdiction
                )
                
                if (i + 1) % 25 == 0:
                    logger.info(f"Progress: {i+1}/{len(documents)} chunks updated")
                
                if i < len(documents) - 1 and i % 50 == 0:
                    await asyncio.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"Error updating chunk {i+1} ({chunk.chunk_id}): {e}")
                updated_chunks.append(chunk)
                continue
        
        state["documents"] = updated_chunks
        state["processing_stage"] = "completed"
        
        logger.info(f"Global comprehensive linking completed:")
        logger.info(f"  Successfully updated: {successful_updates}/{len(documents)} chunks")
        logger.info(f"  Cross-jurisdiction references: {cross_jurisdiction_references}")
        
        success_message = AIMessage(
            content=f"Global linking completed successfully. Updated {successful_updates} chunks with "
                   f"cross-references including {cross_jurisdiction_references} cross-jurisdiction links."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in global linking node: {e}")
        error_message = AIMessage(content=f"Global linking process failed: {str(e)}")
        state["messages"].append(error_message)
        state["processing_stage"] = "completed_with_errors"
        return state


async def react_query_node(state: ProcessingState) -> ProcessingState:
    """ReAct agent node for intelligent global privacy querying and analysis"""
    logger.info("Global Privacy ReAct agent available for intelligent multi-jurisdiction querying...")
    
    state["processing_stage"] = "react_ready"
    
    react_message = AIMessage(content="Global Privacy ReAct agent initialized and ready for intelligent multi-jurisdiction privacy analysis queries.")
    state["messages"].append(react_message)
    
    return state


# Create the enhanced multi-jurisdiction workflow
def create_global_privacy_processing_workflow():
    """Create the enhanced LangGraph workflow for global privacy document processing"""
    
    checkpointer = MemorySaver()
    memory_store = InMemoryStore()
    
    tools = [search_similar_chunks, search_full_articles, search_both_levels, 
             get_chunk_relationships, store_agent_memory, react_query_global_privacy]
    tool_node = ToolNode(tools)
    
    workflow = StateGraph(ProcessingState)
    
    workflow.add_node("document_processing", document_processing_node)
    workflow.add_node("cross_reference", cross_reference_node)
    workflow.add_node("linking", linking_node)
    workflow.add_node("react_query", react_query_node)
    workflow.add_node("tools", tool_node)
    
    workflow.add_edge(START, "document_processing")
    workflow.add_edge("document_processing", "cross_reference")
    workflow.add_edge("cross_reference", "linking")
    workflow.add_edge("linking", "react_query")
    workflow.add_edge("react_query", END)
    
    app = workflow.compile(
        checkpointer=checkpointer,
        store=memory_store
    )
    
    return app


async def run_global_comprehensive_analysis(final_state: ProcessingState, es_manager: ElasticsearchManager) -> None:
    """Run comprehensive analysis with enhanced metrics and global ReAct agent demonstration"""
    try:
        # Global privacy sample queries
        sample_queries = [
            "data processing lawful basis across jurisdictions",
            "consent requirements GDPR vs CCPA",
            "cross-border data transfer restrictions",
            "data subject rights comparison",
            "breach notification requirements globally",
            "data protection officer obligations",
            "automated decision making rules",
            "privacy by design implementation",
            "data retention requirements across frameworks",
            "controller processor relationships globally"
        ]
        
        print(f"\n=== Enhanced Global Privacy Performance Benchmark ===")
        
        openai_manager = OpenAIManager()
        total_time = 0
        successful_queries = 0
        
        for query in sample_queries[:5]:
            import time
            start_time = time.time()
            
            embedding = await openai_manager.create_embedding(query)
            search_results = es_manager.hybrid_search(
                query=query,
                embedding=embedding,
                search_level="both"
            )
            
            end_time = time.time()
            query_time = (end_time - start_time) * 1000
            total_time += query_time
            successful_queries += 1
            
            print(f"Query '{query}' took {query_time:.2f}ms")
        
        avg_latency = total_time / successful_queries if successful_queries > 0 else 0
        print(f"Average Query Latency: {avg_latency:.2f}ms")
        print(f"Successful Queries: {successful_queries}/{len(sample_queries[:5])}")
        
        # Enhanced global search demonstration
        if final_state['documents']:
            print(f"\n=== Enhanced Global Hybrid Search Demo ===")
            sample_query = "data processing consent requirements comparison"
            
            sample_embedding = await openai_manager.create_embedding(sample_query)
            
            search_results = es_manager.hybrid_search(
                query=sample_query,
                embedding=sample_embedding,
                search_level="both"
            )
            
            print(f"Search query: '{sample_query}'")
            print(f"Articles found: {len(search_results.get('articles', []))}")
            print(f"Chunks found: {len(search_results.get('chunks', []))}")
            
            # Show jurisdiction diversity
            jurisdictions = set()
            for chunk in search_results.get('chunks', [])[:5]:
                jurisdictions.add(chunk.get('jurisdiction', 'Unknown'))
            print(f"Jurisdictions represented: {', '.join(jurisdictions)}")
            
            if search_results.get('chunks'):
                top_chunk = search_results['chunks'][0]
                print(f"Top result: {top_chunk.get('title', 'N/A')} (Score: {top_chunk.get('_score', 0):.3f})")
                print(f"Jurisdiction: {top_chunk.get('jurisdiction', 'N/A')}")
                print(f"Document type: {top_chunk.get('document_type', 'N/A')}")
                print(f"Framework: {top_chunk.get('framework_type', 'N/A')}")
        
        # Global jurisdiction summary
        print(f"\n=== Global Jurisdiction Coverage ===")
        try:
            jurisdiction_summary = es_manager.get_jurisdiction_summary()
            if jurisdiction_summary:
                print(f"Total documents indexed: {jurisdiction_summary.get('total_documents', 0):,}")
                print(f"Jurisdictions covered: {len(jurisdiction_summary.get('jurisdictions', []))}")
                print(f"Document types: {len(jurisdiction_summary.get('document_types', []))}")
                print(f"Framework types: {len(jurisdiction_summary.get('framework_types', []))}")
                
                # Show some examples
                jurisdictions = jurisdiction_summary.get('jurisdictions', [])
                if jurisdictions:
                    print(f"Sample jurisdictions: {', '.join(jurisdictions[:5])}")
                
                doc_types = jurisdiction_summary.get('document_types', [])
                if doc_types:
                    print(f"Sample document types: {', '.join(doc_types[:5])}")
        except Exception as e:
            print(f"Error getting jurisdiction summary: {e}")
        
        # Global ReAct Agent Demonstration
        print(f"\n=== Global Privacy ReAct Agent Demonstration ===")
        memory_store = InMemoryStore()
        react_agent = ReActQueryAgent(openai_manager, es_manager, memory_store)
        
        try:
            react_agent.create_react_agent()
            print("Global Privacy ReAct agent successfully created with o3-mini model")
            
            demo_queries = [
                "What are the key differences between GDPR and CCPA regarding data subject rights?",
                "How do breach notification requirements compare across major privacy frameworks?",
                "What are the cross-border data transfer restrictions across different jurisdictions?",
                "How do consent requirements vary between EU GDPR and Brazil LGPD?",
                "What are the common elements across all major privacy frameworks?"
            ]
            
            print(f"\nGlobal Privacy ReAct Agent Tools Available:")
            print(f"  - search_articles_tool: Search articles across all jurisdictions")
            print(f"  - search_chunks_tool: Search specific sections globally")
            print(f"  - search_both_tool: Comprehensive global search")
            print(f"  - get_relationships_tool: Find cross-jurisdictional relationships")
            print(f"  - compare_jurisdictions_tool: Compare specific topics across jurisdictions")
            print(f"  - analyze_global_concept_tool: Deep analysis across all frameworks")
            print(f"  - compliance_gap_analysis_tool: Identify gaps for global operations")
            print(f"  - jurisdiction_summary_tool: Get system coverage summary")
            
            print(f"\nExample Global Privacy ReAct queries:")
            for i, query in enumerate(demo_queries, 1):
                print(f"  {i}. {query}")
            
            if final_state['documents']:
                print(f"\n--- Global Privacy ReAct Agent Demo Query ---")
                demo_query = "How do data processing lawful basis requirements compare between GDPR and CCPA?"
                print(f"Query: {demo_query}")
                
                print("Global Privacy ReAct agent would:")
                print("1. THINK: I need to compare lawful basis requirements between EU and California frameworks")
                print("2. ACT: Use compare_jurisdictions_tool to analyze GDPR vs CCPA requirements")
                print("3. OBSERVE: Review the comparative results and identify key differences")
                print("4. ACT: Use analyze_global_concept_tool for deeper analysis of 'lawful basis'")
                print("5. OBSERVE: Gather comprehensive information from both frameworks")
                print("6. THINK: Synthesize findings into a comparative analysis")
                print("7. RESPOND: Provide detailed comparison with specific regulatory citations")
                
                print("\nGlobal Privacy ReAct agent is ready for intelligent multi-jurisdiction analysis!")
            
        except Exception as e:
            print(f"Global Privacy ReAct agent creation demo error: {e}")
            print("Global Privacy ReAct agent framework is implemented and ready for use")
                
    except Exception as e:
        logger.error(f"Error in global comprehensive analysis: {e}")


# Enhanced main execution function for multi-jurisdiction support
async def main():
    """Main execution function with comprehensive global privacy support"""
    
    try:
        logger.info("Validating configuration for global privacy processing...")
        
        if not Config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        if not Config.ES_PASSWORD:
            raise ValueError("ES_PASSWORD environment variable is required")
        
        # Validate model availability
        try:
            test_client = openai.OpenAI(
                api_key=Config.OPENAI_API_KEY,
                base_url=Config.OPENAI_BASE_URL
            )
            test_response = test_client.chat.completions.create(
                model=Config.O3_MINI_MODEL,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            logger.info(f"Successfully validated OpenAI API access with model: {Config.O3_MINI_MODEL}")
            
        except Exception as e:
            logger.error(f"OpenAI API validation failed: {e}")
            raise ValueError(f"Cannot access OpenAI API with model {Config.O3_MINI_MODEL}: {e}")
        
        logger.info(f"Using OpenAI base URL: {Config.OPENAI_BASE_URL}")
        logger.info(f"Using Elasticsearch: {Config.ES_HOST}:{Config.ES_PORT}")
        logger.info(f"Text splitting configuration: chunk_size={Config.CHUNK_SIZE}, overlap={Config.CHUNK_OVERLAP}")
        logger.info(f"Global privacy frameworks supported: {len(jurisdiction_mapper.mappings)} frameworks")
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        raise
    
    # Validate data directory and files
    try:
        data_dir = "data"
        if not os.path.exists(data_dir):
            logger.error(f"Data directory '{data_dir}' not found")
            raise FileNotFoundError(f"Please create a '{data_dir}' directory and add your privacy framework PDF files")
        
        pdf_files = glob.glob(os.path.join(data_dir, "*.pdf"))
        if not pdf_files:
            logger.error(f"No PDF files found in '{data_dir}' directory")
            raise FileNotFoundError(f"Please add privacy framework PDF files to the '{data_dir}' directory")
        
        logger.info(f"Found {len(pdf_files)} PDF files for global privacy processing")
        
        # Validate PDF files and predict jurisdictions
        readable_files = []
        predicted_jurisdictions = set()
        
        for pdf_file in pdf_files:
            try:
                with pymupdf.open(pdf_file) as doc:
                    if doc.page_count > 0:
                        readable_files.append(pdf_file)
                        # Quick content sample for jurisdiction prediction
                        sample_text = doc[0].get_text()[:1000] if doc.page_count > 0 else ""
                        predicted_type = detect_document_type(pdf_file, sample_text)
                        jurisdiction_info = jurisdiction_mapper.get_jurisdiction_info(predicted_type)
                        predicted_jurisdictions.add(jurisdiction_info["region"])
                        logger.info(f"  ✓ {os.path.basename(pdf_file)} ({doc.page_count} pages) - Predicted: {predicted_type}")
                    else:
                        logger.warning(f"  ✗ {os.path.basename(pdf_file)} (no pages)")
            except Exception as e:
                logger.warning(f"  ✗ {os.path.basename(pdf_file)} (error: {e})")
        
        if not readable_files:
            raise ValueError("No readable PDF files found")
        
        logger.info(f"Successfully validated {len(readable_files)} readable PDF files")
        logger.info(f"Predicted jurisdictions: {', '.join(sorted(predicted_jurisdictions))}")
        
    except Exception as e:
        logger.error(f"Data validation failed: {e}")
        raise
    
    # Test Elasticsearch connection
    try:
        logger.info("Testing Elasticsearch connection...")
        test_es = ElasticsearchManager()
        test_info = test_es.client.info()
        logger.info(f"Successfully connected to Elasticsearch: {test_info.get('version', {}).get('number', 'unknown')}")
    except Exception as e:
        logger.error(f"Elasticsearch connection failed: {e}")
        raise ValueError(f"Cannot connect to Elasticsearch: {e}")
    
    # Create and run the enhanced workflow
    try:
        logger.info("Creating global privacy processing workflow...")
        app = create_global_privacy_processing_workflow()
        
        initial_state = ProcessingState(
            messages=[HumanMessage(content="Process global privacy documents with comprehensive multi-jurisdiction chunking and ReAct agent")],
            full_articles=[],
            documents=[],
            current_chunk=None,
            cross_links=[],
            processing_stage="initializing",
            agent_memories=[],
            elasticsearch_client=None
        )
        
        config = {
            "configurable": {
                "thread_id": f"global_privacy_comprehensive_processing_{int(time.time())}",
                "elasticsearch_manager": ElasticsearchManager(),
                "openai_manager": OpenAIManager(),
                "memory_store": InMemoryStore()
            }
        }
        
        logger.info("Starting comprehensive global privacy document processing workflow with ReAct agent...")
        
        final_state = await app.ainvoke(initial_state, config)
        
        # Validate results
        if not final_state:
            raise ValueError("Workflow returned empty state")
        
        articles_count = len(final_state.get('full_articles', []))
        chunks_count = len(final_state.get('documents', []))
        links_count = len(final_state.get('cross_links', []))
        
        if articles_count == 0 and chunks_count == 0:
            raise ValueError("No content was processed successfully")
        
        logger.info(f"Comprehensive global privacy processing with ReAct agent completed!")
        logger.info(f"Processed {articles_count} full articles")
        logger.info(f"Created {chunks_count} comprehensive chunks")
        logger.info(f"Established {links_count} cross-document links")
        logger.info(f"ReAct agent status: {final_state.get('processing_stage', 'unknown')}")
        
        es_manager = config["configurable"]["elasticsearch_manager"]
        
        print("\n" + "="*80)
        print("COMPREHENSIVE GLOBAL PRIVACY PROCESSING WITH REACT AGENT SUMMARY")
        print("="*80)
        print(f"PDF files processed: {len(readable_files)}")
        print(f"Full articles processed: {articles_count}")
        print(f"Total comprehensive chunks: {chunks_count}")
        print(f"Cross-document links created: {links_count}")
        print(f"Processing stage: {final_state.get('processing_stage', 'unknown')}")
        
        react_status = "✓ Ready for intelligent global privacy querying" if final_state.get('processing_stage') == 'react_ready' else "✗ Not ready"
        print(f"Global Privacy ReAct agent: {react_status}")
        
        # Enhanced jurisdiction analysis
        try:
            jurisdiction_counts = {}
            framework_counts = {}
            cross_jurisdiction_links = 0
            
            for article in final_state.get('full_articles', []):
                jurisdiction = getattr(article, 'jurisdiction', 'Unknown')
                framework_type = getattr(article, 'framework_type', 'Unknown')
                jurisdiction_counts[jurisdiction] = jurisdiction_counts.get(jurisdiction, 0) + 1
                framework_counts[framework_type] = framework_counts.get(framework_type, 0) + 1
            
            for link in final_state.get('cross_links', []):
                if hasattr(link, 'source_jurisdiction') and hasattr(link, 'target_jurisdiction'):
                    if link.source_jurisdiction != link.target_jurisdiction:
                        cross_jurisdiction_links += 1
            
            if jurisdiction_counts:
                print(f"\nJurisdictions Processed:")
                for jurisdiction, count in jurisdiction_counts.items():
                    print(f"  {jurisdiction}: {count} articles")
                
                print(f"\nFramework Types:")
                for framework_type, count in framework_counts.items():
                    print(f"  {framework_type}: {count} articles")
                
                print(f"\nCross-Jurisdiction Analysis:")
                print(f"  Total cross-document links: {links_count}")
                print(f"  Cross-jurisdiction links: {cross_jurisdiction_links}")
                print(f"  Same-jurisdiction links: {links_count - cross_jurisdiction_links}")
                
        except Exception as e:
            logger.error(f"Error in jurisdiction analysis: {e}")
        
        # Text splitting metrics
        try:
            page_covered_chunks = sum(1 for chunk in final_state.get('documents', []) 
                                    if hasattr(chunk, 'page_number') and chunk.page_number is not None)
            print(f"\nText Splitting Metrics:")
            print(f"  Chunk size: {Config.CHUNK_SIZE} characters")
            print(f"  Chunk overlap: {Config.CHUNK_OVERLAP} characters")
            print(f"  Chunks with page tracking: {page_covered_chunks}/{chunks_count}")
        except Exception as e:
            logger.error(f"Error in text splitting metrics: {e}")
        
        # HNSW and optimization details
        print(f"\n" + "="*60)
        print("ENHANCED HNSW & QUANTIZATION OPTIMIZATIONS")
        print("="*60)
        print(f"Vector Index Type: {Config.VECTOR_INDEX_TYPE}")
        print(f"HNSW Parameters: M={Config.HNSW_M}, EF_Construction={Config.HNSW_EF_CONSTRUCTION}")
        print(f"Embedding Dimensions: {Config.EMBEDDING_DIMENSIONS}")
        print(f"Preload Cache: {'Enabled' if Config.ENABLE_PRELOAD else 'Disabled'}")
        
        # Index statistics
        try:
            index_stats = es_manager.get_index_stats()
            for index_name, stats in index_stats.items():
                print(f"\n{index_name.upper()} Index:")
                print(f"  Documents: {stats.get('documents', 0):,}")
                print(f"  Size: {stats.get('size_bytes', 0) / (1024**2):.1f} MB")
                print(f"  Segments: {stats.get('segments', 0)}")
                
                vector_stats = stats.get('vector_size_estimate', {})
                if vector_stats.get('estimated_memory', 'Unknown') != 'Unknown':
                    print(f"  Vector Memory: {vector_stats.get('estimated_memory')}")
                    print(f"  Quantization Savings: {vector_stats.get('quantization_savings')}")
                    print(f"  HNSW Overhead: {vector_stats.get('hnsw_overhead')}")
        except Exception as e:
            logger.error(f"Error getting index statistics: {e}")
        
        # Index optimization
        try:
            print(f"\n" + "="*50)
            print("OPTIMIZING INDICES FOR GLOBAL PRODUCTION")
            print("="*50)
            es_manager.optimize_indices()
            print("Index optimization completed")
        except Exception as e:
            logger.error(f"Error optimizing indices: {e}")
        
        # Enhanced examples
        try:
            if final_state.get('full_articles'):
                article = final_state['full_articles'][0]
                print(f"\nExample global privacy article: {getattr(article, 'title', 'N/A')[:100]}...")
                print(f"Jurisdiction: {getattr(article, 'jurisdiction', 'N/A')}")
                print(f"Framework type: {getattr(article, 'framework_type', 'N/A')}")
                print(f"Article chunks: {len(getattr(article, 'chunk_ids', []))}")
                print(f"Key concepts: {len(getattr(article, 'key_concepts', []))}")
        except Exception as e:
            logger.error(f"Error displaying article example: {e}")
        
        try:
            if final_state.get('documents'):
                chunk = final_state['documents'][0]
                print(f"\nExample comprehensive chunk: {getattr(chunk, 'title', 'N/A')[:50]}...")
                print(f"Jurisdiction: {getattr(chunk, 'jurisdiction', 'N/A')}")
                print(f"Framework type: {getattr(chunk, 'framework_type', 'N/A')}")
                print(f"Chunk index: {getattr(chunk, 'chunk_index', 'N/A')}")
                print(f"Page number: {getattr(chunk, 'page_number', 'N/A')}")
                print(f"Content length: {len(getattr(chunk, 'content', ''))}")
                print(f"Supporting references: {len(getattr(chunk, 'supporting_references', []))}")
        except Exception as e:
            logger.error(f"Error displaying chunk example: {e}")
        
        # Run comprehensive analysis
        try:
            await run_global_comprehensive_analysis(final_state, es_manager)
        except Exception as e:
            logger.error(f"Error in global comprehensive analysis: {e}")
        
        print(f"\n" + "="*80)
        print("GLOBAL PRIVACY PROCESSING COMPLETED SUCCESSFULLY!")
        print("="*80)
        
        return final_state
        
    except Exception as e:
        logger.error(f"Error in comprehensive global privacy workflow execution: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
