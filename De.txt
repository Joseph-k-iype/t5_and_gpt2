import streamlit as st
import duckdb
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
import io
from typing import List, Dict, Optional, Union, Any, Tuple
import warnings
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import base64
from pathlib import Path

# Suppress warnings
warnings.filterwarnings('ignore')

# Configure Streamlit page
st.set_page_config(
    page_title="DuckDB Excel Analysis Dashboard",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .stButton > button {
        width: 100%;
    }
    .main > div {
        padding-top: 1rem;
    }
    .block-container {
        padding-top: 2rem;
    }
    .stProgress > div > div > div > div {
        background-color: #00cc00;
    }
    .styled-metric {
        padding: 10px;
        border-radius: 5px;
        background-color: #f0f2f6;
        margin: 5px;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

class DataValidator:
    """Class to handle data validation and error checking."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
    
    @staticmethod
    def validate_file(file) -> bool:
        """Validate uploaded file."""
        try:
            if file is None:
                return False
            
            # Check file size (max 200MB)
            if file.size > 200 * 1024 * 1024:
                st.error(f"File {file.name} is too large. Maximum size is 200MB.")
                return False
            
            # Check file extension
            if not file.name.lower().endswith(('.xlsx', '.xls')):
                st.error(f"File {file.name} is not an Excel file.")
                return False
            
            return True
        except Exception as e:
            st.error(f"Error validating file {file.name}: {str(e)}")
            return False
    
    def validate_table(self, table_name: str) -> bool:
        """Validate table structure."""
        try:
            # Check if table exists
            table_exists = self.con.execute(f"""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='{table_name}'
            """).fetchone() is not None

            if not table_exists:
                st.error(f"Table {table_name} does not exist.")
                return False
            
            # Check if table has data
            row_count = self.con.execute(f"""
                SELECT COUNT(*) FROM {table_name}
            """).fetchone()[0]
            
            if row_count == 0:
                st.error(f"Table {table_name} has no rows.")
                return False
            
            # Check if table has columns
            col_count = self.con.execute(f"""
                SELECT COUNT(*) FROM pragma_table_info('{table_name}')
            """).fetchone()[0]
            
            if col_count == 0:
                st.error(f"Table {table_name} has no columns.")
                return False
            
            return True
            
        except Exception as e:
            st.error(f"Error validating table {table_name}: {str(e)}")
            return False

class DataLoader:
    """Class to handle data loading and initial processing."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
        self.validator = DataValidator(connection)
    
    def _check_table_exists(self, table_name: str) -> bool:
        """Check if a table exists in DuckDB."""
        try:
            result = self.con.execute(f"""
                SELECT name 
                FROM sqlite_master 
                WHERE type='table' AND name='{table_name}'
            """).fetchone()
            return result is not None
        except Exception:
            return False

    def _drop_table_if_exists(self, table_name: str):
        """Safely drop a table if it exists."""
        try:
            self.con.execute(f"DROP TABLE IF EXISTS {table_name}")
        except Exception as e:
            st.error(f"Error dropping table {table_name}: {str(e)}")

    def load_excel(self, file) -> Optional[str]:
        """Load Excel file into DuckDB with comprehensive error handling."""
        try:
            if not self.validator.validate_file(file):
                return None
            
            # Read Excel into temporary pandas DataFrame
            df = pd.read_excel(
                file,
                engine='openpyxl',
                na_values=['NA', 'N/A', '', ' ']
            )
            
            # Create table name from file name (sanitized)
            table_name = f"table_{file.name.split('.')[0].lower().replace(' ', '_').replace('-', '_')}"
            
            # Drop existing table if it exists
            self._drop_table_if_exists(table_name)
            
            # Create new table directly from DataFrame
            try:
                self.con.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df")
            except Exception as e:
                st.error(f"Error creating table {table_name}: {str(e)}")
                return None
            
            # Verify table was created
            if not self._check_table_exists(table_name):
                st.error(f"Failed to create table {table_name}")
                return None
            
            # Clean column names
            try:
                # Get current column names
                columns = self.con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
                
                # Create new names and rename columns
                for col in columns:
                    old_name = col[1]
                    new_name = old_name.strip().replace(' ', '_').lower()
                    
                    if old_name != new_name:
                        self.con.execute(f"""
                            ALTER TABLE {table_name} 
                            RENAME COLUMN "{old_name}" TO "{new_name}"
                        """)
            except Exception as e:
                st.warning(f"Warning during column renaming: {str(e)}")
            
            if not self.validator.validate_table(table_name):
                return None
            
            return table_name
            
        except Exception as e:
            st.error(f"Error loading {file.name}: {str(e)}")
            return None

    def _process_table(self, table_name: str):
        """Process and clean table in DuckDB."""
        try:
            # Get column information
            columns = self.con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
            
            # Process each column
            for col in columns:
                col_name = col[1]
                col_type = col[2]
                
                # Handle data type conversions if needed
                if col_type == 'VARCHAR':
                    # Try converting to numeric if possible
                    try:
                        self.con.execute(f"""
                            UPDATE {table_name}
                            SET "{col_name}" = CAST("{col_name}" AS DOUBLE)
                            WHERE "{col_name}" ~ '^[0-9]+\.?[0-9]*$'
                        """)
                    except:
                        pass
                
                # Handle boolean conversions
                if col_type == 'BOOLEAN':
                    self.con.execute(f"""
                        UPDATE {table_name}
                        SET "{col_name}" = CASE 
                            WHEN "{col_name}" THEN 1
                            ELSE 0
                        END
                    """)
            
        except Exception as e:
            st.error(f"Error processing table {table_name}: {str(e)}")

class DataProcessor:
    """Class to handle data merging, transformations, and cleaning with DuckDB."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
        self.cleaning_report = {
            'missing_values': {},
            'outliers': {},
            'duplicates': {},
            'summary': {}
        }

    def validate_merge_keys(self, tables: List[str], merge_keys: List[str]) -> bool:
        """Validate merge keys exist in all tables and have compatible types."""
        try:
            if not tables or not merge_keys:
                st.error("No tables or merge keys provided.")
                return False
            
            # Check keys exist in all tables
            for table in tables:
                for key in merge_keys:
                    exists = self.con.execute(f"""
                        SELECT COUNT(*) 
                        FROM pragma_table_info('{table}')
                        WHERE name = '{key}'
                    """).fetchone()[0]
                    
                    if not exists:
                        st.error(f"Key {key} not found in table {table}")
                        return False
            
            # Check key types are compatible
            base_table = tables[0]
            for key in merge_keys:
                base_type = self.con.execute(f"""
                    SELECT type 
                    FROM pragma_table_info('{base_table}')
                    WHERE name = '{key}'
                """).fetchone()[0]
                
                for table in tables[1:]:
                    curr_type = self.con.execute(f"""
                        SELECT type 
                        FROM pragma_table_info('{table}')
                        WHERE name = '{key}'
                    """).fetchone()[0]
                    
                    if base_type != curr_type:
                        st.warning(f"Column '{key}' has different types in {base_table} ({base_type}) and {table} ({curr_type})")
                        # Try converting to string if types don't match
                        st.info(f"Converting {key} to string type in all tables for compatibility")
                        for t in tables:
                            try:
                                self.con.execute(f"""
                                    ALTER TABLE {t} ALTER COLUMN "{key}" TYPE VARCHAR
                                """)
                            except Exception as e:
                                st.error(f"Error converting column type: {str(e)}")
                                return False
            
            return True
            
        except Exception as e:
            st.error(f"Error validating merge keys: {str(e)}")
            return False

    def merge_tables(self, tables: List[str], 
                    merge_keys: List[str], 
                    merge_type: str = 'left') -> Optional[str]:
        """Merge multiple tables with validation and error handling."""
        try:
            if not tables:
                st.error("No tables to merge.")
                return None
            
            # Validate merge keys
            if not self.validate_merge_keys(tables, merge_keys):
                return None
            
            # Initialize progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Create JOIN type mapping
            join_types = {
                'left': 'LEFT JOIN',
                'right': 'RIGHT JOIN',
                'inner': 'INNER JOIN',
                'outer': 'FULL OUTER JOIN'
            }
            
            join_type = join_types.get(merge_type, 'LEFT JOIN')
            
            # Drop merged table if it exists
            self.con.execute("DROP TABLE IF EXISTS merged_data")
            
            # Build the merge query
            base_table = tables[0]
            joins = []
            
            for i, table in enumerate(tables[1:], 1):
                # Update progress
                progress = i / len(tables[1:])
                progress_bar.progress(progress)
                status_text.text(f"Merging table {i + 1}...")
                
                # Create JOIN conditions
                join_conditions = ' AND '.join([
                    f't0."{key}" = t{i}."{key}"'
                    for key in merge_keys
                ])
                
                # Add JOIN clause
                joins.append(f"{join_type} {table} t{i} ON {join_conditions}")
            
            # Create merged table query
            merge_query = f"""
            CREATE TABLE merged_data AS
            SELECT DISTINCT *
            FROM {base_table} t0
            {' '.join(joins)}
            """
            
            # Execute merge
            self.con.execute(merge_query)
            
            # Log merge results
            pre_merge_count = self.con.execute(f"SELECT COUNT(*) FROM {base_table}").fetchone()[0]
            post_merge_count = self.con.execute("SELECT COUNT(*) FROM merged_data").fetchone()[0]
            
            if merge_type in ['left', 'right'] and post_merge_count != pre_merge_count:
                st.warning(f"Row count changed in {merge_type} merge: {pre_merge_count:,} â†’ {post_merge_count:,}")
            elif merge_type == 'inner' and post_merge_count < pre_merge_count:
                st.info(f"Inner merge reduced rows: {pre_merge_count:,} â†’ {post_merge_count:,}")
            
            # Check for duplicates in merge keys
            for key in merge_keys:
                dupe_count = self.con.execute(f"""
                    SELECT COUNT(*) - COUNT(DISTINCT "{key}")
                    FROM merged_data
                """).fetchone()[0]
                
                if dupe_count > 0:
                    st.info(f"Found {dupe_count:,} duplicate values in key '{key}' after merge.")
            
            progress_bar.progress(1.0)
            status_text.text("Merge completed successfully!")
            
            return "merged_data"
            
        except Exception as e:
            st.error(f"Error during merge: {str(e)}")
            return None

    def handle_missing_values(self, table_name: str) -> str:
        """Handle missing values and report details using DuckDB."""
        try:
            # Get column information
            columns = self.con.execute(f"""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = '{table_name}'
            """).fetchall()
            
            # Initialize report
            missing_report = {}
            total_cells = self.con.execute(f"""
                SELECT COUNT(*) * {len(columns)} 
                FROM {table_name}
            """).fetchone()[0]
            
            total_missing = 0
            
            # Create new table for cleaned data
            cleaned_table = f"{table_name}_cleaned"
            
            # Process each column
            column_definitions = []
            for (column,) in columns:
                # Count missing values
                missing_count = self.con.execute(f"""
                    SELECT COUNT(*) 
                    FROM {table_name} 
                    WHERE "{column}" IS NULL
                """).fetchone()[0]
                
                total_missing += missing_count
                
                if missing_count > 0:
                    missing_report[column] = {
                        'count': missing_count,
                        'percentage': (missing_count / total_cells) * 100
                    }
                    
                    # Create column definition with COALESCE
                    column_definitions.append(f"""
                        COALESCE("{column}", 'Data Not Available') as "{column}"
                    """)
                else:
                    column_definitions.append(f'"{column}"')
            
            # Create new table with handled missing values
            create_query = f"""
            CREATE TABLE {cleaned_table} AS
            SELECT {', '.join(column_definitions)}
            FROM {table_name}
            """
            self.con.execute(create_query)
            
            # Update cleaning report
            self.cleaning_report['missing_values'] = {
                'columns_with_missing': len(missing_report),
                'total_missing_cells': total_missing,
                'total_cells': total_cells,
                'overall_missing_percentage': (total_missing / total_cells * 100) if total_cells > 0 else 0,
                'details_by_column': missing_report
            }
            
            return cleaned_table
            
        except Exception as e:
            st.error(f"Error handling missing values: {str(e)}")
            return table_name

    def handle_outliers(self, table_name: str) -> str:
        """Handle outliers using IQR method for numeric columns."""
        try:
            # Get numeric columns
            numeric_columns = self.con.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns 
                WHERE table_name = '{table_name}'
                AND data_type IN ('INTEGER', 'DOUBLE', 'DECIMAL')
            """).fetchall()
            
            if not numeric_columns:
                return table_name
            
            outlier_report = {}
            total_outliers = 0
            
            # Create new table for cleaned data
            cleaned_table = f"{table_name}_no_outliers"
            
            # Process each numeric column
            column_definitions = []
            for column, _ in numeric_columns:
                # Calculate quartiles and IQR
                stats = self.con.execute(f"""
                    SELECT 
                        percentile_cont(0.25) WITHIN GROUP (ORDER BY "{column}") as Q1,
                        percentile_cont(0.75) WITHIN GROUP (ORDER BY "{column}") as Q3
                    FROM {table_name}
                    WHERE "{column}" IS NOT NULL
                """).fetchone()
                
                if stats:
                    Q1, Q3 = stats
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # Count outliers
                    outlier_count = self.con.execute(f"""
                        SELECT COUNT(*)
                        FROM {table_name}
                        WHERE "{column}" < {lower_bound} OR "{column}" > {upper_bound}
                    """).fetchone()[0]
                    
                    total_outliers += outlier_count
                    
                    if outlier_count > 0:
                        # Store outlier information
                        outlier_report[column] = {
                            'count': outlier_count,
                            'percentage': (outlier_count / self.con.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]) * 100,
                            'boundaries': {
                                'lower': lower_bound,
                                'upper': upper_bound
                            },
                            'statistics': {
                                'Q1': Q1,
                                'Q3': Q3,
                                'IQR': IQR
                            }
                        }
                        
                        # Create column definition with clipping
                        column_definitions.append(f"""
                            CASE
                                WHEN "{column}" < {lower_bound} THEN {lower_bound}
                                WHEN "{column}" > {upper_bound} THEN {upper_bound}
                                ELSE "{column}"
                            END as "{column}"
                        """)
                    else:
                        column_definitions.append(f'"{column}"')
            
            # Get non-numeric columns
            non_numeric_cols = [col for col, _ in self.con.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns 
                WHERE table_name = '{table_name}'
                AND data_type NOT IN ('INTEGER', 'DOUBLE', 'DECIMAL')
            """).fetchall()]
            
            # Add non-numeric columns to column definitions
            column_definitions.extend([f'"{col}"' for col in non_numeric_cols])
            
            # Create new table with handled outliers
            create_query = f"""
            CREATE TABLE {cleaned_table} AS
            SELECT {', '.join(column_definitions)}
            FROM {table_name}
            """
            self.con.execute(create_query)
            
            # Update cleaning report
            self.cleaning_report['outliers'] = {
                'columns_processed': len(numeric_columns),
                'columns_with_outliers': len(outlier_report),
                'total_outliers': total_outliers,
                'details_by_column': outlier_report
            }
            
            return cleaned_table
            
        except Exception as e:
            st.error(f"Error handling outliers: {str(e)}")
            return table_name
    def handle_duplicates(self, table_name: str) -> str:
        """Remove exact duplicate rows using DuckDB."""
        try:
            # Get initial row count
            initial_count = self.con.execute(f"""
                SELECT COUNT(*) FROM {table_name}
            """).fetchone()[0]
            
            # Create new table without duplicates
            deduped_table = f"{table_name}_unique"
            self.con.execute(f"""
                CREATE TABLE {deduped_table} AS
                SELECT DISTINCT *
                FROM {table_name}
            """)
            
            # Get final row count
            final_count = self.con.execute(f"""
                SELECT COUNT(*) FROM {deduped_table}
            """).fetchone()[0]
            
            duplicate_count = initial_count - final_count
            
            # Get sample of duplicate rows
            if duplicate_count > 0:
                duplicate_examples = self.con.execute(f"""
                    WITH RowCounts AS (
                        SELECT *, COUNT(*) as row_count
                        FROM {table_name}
                        GROUP BY *
                        HAVING COUNT(*) > 1
                    )
                    SELECT *
                    FROM RowCounts
                    LIMIT 5
                """).df()
            else:
                duplicate_examples = pd.DataFrame()
            
            # Update cleaning report
            self.cleaning_report['duplicates'] = {
                'rows_before': initial_count,
                'rows_after': final_count,
                'duplicates_removed': duplicate_count,
                'percentage_duplicate': (duplicate_count / initial_count * 100) if initial_count > 0 else 0,
                'duplicate_examples': duplicate_examples.to_dict('records') if not duplicate_examples.empty else []
            }
            
            return deduped_table
            
        except Exception as e:
            st.error(f"Error handling duplicates: {str(e)}")
            return table_name

    def clean_data(self, table_name: str) -> str:
        """Main method to clean data with comprehensive reporting."""
        try:
            # Reset cleaning report
            self.cleaning_report = {
                'missing_values': {},
                'outliers': {},
                'duplicates': {},
                'summary': {}
            }
            
            # Store initial state
            initial_state = {
                'rows': self.con.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0],
                'columns': len(self.con.execute(f"PRAGMA table_info('{table_name}')").fetchall())
            }
            initial_state['total_cells'] = initial_state['rows'] * initial_state['columns']
            
            # Step 1: Handle Missing Values
            st.write("Step 1: Handling Missing Values...")
            table_name = self.handle_missing_values(table_name)
            
            # Step 2: Handle Outliers
            st.write("Step 2: Processing Outliers...")
            table_name = self.handle_outliers(table_name)
            
            # Step 3: Remove Duplicates
            st.write("Step 3: Removing Duplicates...")
            final_table = self.handle_duplicates(table_name)
            
            # Final state
            final_state = {
                'rows': self.con.execute(f"SELECT COUNT(*) FROM {final_table}").fetchone()[0],
                'columns': len(self.con.execute(f"PRAGMA table_info('{final_table}')").fetchall())
            }
            final_state['total_cells'] = final_state['rows'] * final_state['columns']
            
            # Generate final summary
            self.cleaning_report['summary'] = {
                'initial_state': initial_state,
                'final_state': final_state,
                'changes': {
                    'rows_removed': initial_state['rows'] - final_state['rows'],
                    'missing_values_handled': self.cleaning_report['missing_values'].get('total_missing_cells', 0),
                    'outliers_handled': self.cleaning_report['outliers'].get('total_outliers', 0),
                    'duplicates_removed': self.cleaning_report['duplicates'].get('duplicates_removed', 0)
                },
                'performance_metrics': {
                    'data_reduction_percentage': ((initial_state['rows'] - final_state['rows']) / initial_state['rows'] * 100) if initial_state['rows'] > 0 else 0
                }
            }
            
            return final_table
            
        except Exception as e:
            st.error(f"Error during data cleaning: {str(e)}")
            return table_name

    def display_cleaning_report(self):
        """Display detailed cleaning report."""
        st.header("ðŸ§¹ Data Cleaning Report")
        
        # Overall Summary
        st.subheader("ðŸ“Š Summary of Changes")
        summary = self.cleaning_report['summary']
        
        # Create summary metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="Total Rows",
                value=f"{summary['final_state']['rows']:,}",
                delta=f"-{summary['changes']['rows_removed']:,}",
                delta_color="inverse"
            )
        
        with col2:
            st.metric(
                label="Missing Values Handled",
                value=f"{summary['changes']['missing_values_handled']:,}"
            )
        
        with col3:
            st.metric(
                label="Data Reduction",
                value=f"{summary['performance_metrics']['data_reduction_percentage']:.1f}%",
                delta="reduction" if summary['performance_metrics']['data_reduction_percentage'] > 0 else "no change"
            )
        
        # Missing Values Section
        st.subheader("ðŸ” Missing Values")
        missing = self.cleaning_report['missing_values']
        if missing:
            st.write(f"Total Missing Cells: {missing['total_missing_cells']:,} ({missing['overall_missing_percentage']:.2f}%)")
            st.write(f"Columns with Missing Values: {missing['columns_with_missing']}")
            
            if missing.get('details_by_column'):
                missing_df = pd.DataFrame.from_dict(
                    {k: {'count': v['count'], 'percentage': v['percentage']} 
                     for k, v in missing['details_by_column'].items()},
                    orient='index'
                )
                if not missing_df.empty:
                    missing_df = missing_df.sort_values('count', ascending=False)
                    st.dataframe(
                        missing_df.style.format({
                            'count': '{:,.0f}',
                            'percentage': '{:.2f}%'
                        })
                    )
        
        # Outliers Section
        st.subheader("ðŸ“Š Outliers")
        outliers = self.cleaning_report['outliers']
        if outliers:
            st.write(f"Numeric Columns Processed: {outliers['columns_processed']}")
            st.write(f"Columns with Outliers: {outliers['columns_with_outliers']}")
            st.write(f"Total Outliers Found: {outliers['total_outliers']:,}")
            
            if outliers.get('details_by_column'):
                for col, info in outliers['details_by_column'].items():
                    with st.expander(f"{col} - {info['count']:,} outliers ({info['percentage']:.2f}%)"):
                        st.write("### Boundaries")
                        st.write(f"- Lower: {info['boundaries']['lower']:.2f}")
                        st.write(f"- Upper: {info['boundaries']['upper']:.2f}")
                        
                        st.write("### Statistics")
                        st.write(f"- Q1: {info['statistics']['Q1']:.2f}")
                        st.write(f"- Q3: {info['statistics']['Q3']:.2f}")
                        st.write(f"- IQR: {info['statistics']['IQR']:.2f}")
        
        # Duplicates Section
        st.subheader("ðŸ”„ Duplicates")
        dupes = self.cleaning_report['duplicates']
        if dupes:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    label="Duplicates Removed",
                    value=f"{dupes['duplicates_removed']:,}",
                    delta=f"-{dupes['duplicates_removed']:,}",
                    delta_color="inverse"
                )
            
            with col2:
                st.metric(
                    label="Duplicate Percentage",
                    value=f"{dupes['percentage_duplicate']:.2f}%"
                )
            
            with col3:
                st.metric(
                    label="Final Row Count",
                    value=f"{dupes['rows_after']:,}",
                    delta=f"-{dupes['rows_before'] - dupes['rows_after']:,}",
                    delta_color="inverse"
                )
            
            if dupes.get('duplicate_examples'):
                with st.expander("View Sample Duplicate Rows"):
                    st.dataframe(pd.DataFrame(dupes['duplicate_examples']))
                    
class Analytics:
    """Class to handle all analytical operations using DuckDB."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
    
    def create_pivot(self, 
                    table_name: str,
                    index_cols: List[str],
                    value_cols: List[str],
                    agg_funcs: List[str],
                    filters: Dict = None) -> Optional[pd.DataFrame]:
        """Create pivot table with proper type handling."""
        try:
            if not index_cols or not value_cols or not agg_funcs:
                st.error("Missing required parameters for pivot table.")
                return None
            
            # Apply filters if provided
            base_table = table_name
            if filters:
                filtered_table = f"{table_name}_filtered"
                filter_conditions = []
                for col, values in filters.items():
                    if values:
                        placeholders = ', '.join([f"'{str(v)}'" for v in values])
                        filter_conditions.append(f'"{col}"::VARCHAR IN ({placeholders})')
                
                if filter_conditions:
                    filter_query = f"""
                    CREATE TABLE {filtered_table} AS
                    SELECT *
                    FROM {table_name}
                    WHERE {' AND '.join(filter_conditions)}
                    """
                    self.con.execute(filter_query)
                    base_table = filtered_table
            
            # Create aggregation expressions
            agg_expressions = []
            for col in value_cols:
                for func in agg_funcs:
                    try:
                        # Handle different aggregation functions
                        if func == 'distinct_count':
                            agg_expr = f'COUNT(DISTINCT "{col}") as "{col}_distinct_count"'
                        elif func == 'count':
                            agg_expr = f'COUNT("{col}") as "{col}_count"'
                        elif func == 'sum':
                            # Try to cast to numeric for sum
                            agg_expr = f'SUM(CAST("{col}" AS DOUBLE)) as "{col}_sum"'
                        elif func == 'mean':
                            # Try to cast to numeric for mean
                            agg_expr = f'AVG(CAST("{col}" AS DOUBLE)) as "{col}_mean"'
                        elif func in ['max', 'min']:
                            agg_expr = f'{func.upper()}("{col}") as "{col}_{func}"'
                        elif func == 'first':
                            agg_expr = f'FIRST_VALUE("{col}") as "{col}_first"'
                        elif func == 'last':
                            agg_expr = f'LAST_VALUE("{col}") as "{col}_last"'
                        elif func == 'list':
                            agg_expr = f'STRING_AGG(CAST("{col}" AS VARCHAR), \', \') as "{col}_list"'
                        else:
                            continue
                        
                        agg_expressions.append(agg_expr)
                        
                    except Exception as e:
                        st.warning(f"Error calculating {func} for {col}: {str(e)}")
                        continue
            
            if not agg_expressions:
                st.warning("No valid aggregations could be created.")
                return None
            
            # Create final query
            query = f"""
            WITH base_data AS (
                SELECT 
                    {', '.join([f'"{col}"' for col in index_cols])},
                    {', '.join(agg_expressions)}
                FROM {base_table}
                GROUP BY {', '.join([f'"{col}"' for col in index_cols])}
            )
            SELECT * FROM base_data
            """
            
            # Execute query
            result = self.con.execute(query).df()
            
            # Add totals row if possible
            try:
                totals = pd.DataFrame(index=['Total'])
                
                # Get numeric columns
                numeric_cols = result.select_dtypes(include=[np.number]).columns
                
                for col in result.columns:
                    if col in numeric_cols:
                        if 'count' in col.lower() or 'sum' in col.lower():
                            totals[col] = result[col].sum()
                        elif 'mean' in col.lower():
                            totals[col] = result[col].mean()
                        elif 'max' in col.lower():
                            totals[col] = result[col].max()
                        elif 'min' in col.lower():
                            totals[col] = result[col].min()
                        else:
                            totals[col] = result[col].sum()
                    else:
                        totals[col] = ''
                
                result = pd.concat([result, totals])
            except Exception as e:
                st.warning(f"Could not add totals row: {str(e)}")
            
            return result
            
        except Exception as e:
            st.error(f"Error creating pivot table: {str(e)}")
            return None
    def format_pivot_table(self, df: pd.DataFrame) -> pd.DataFrame.style:
        """Format pivot table with proper handling of numeric and string values."""
        # Create the style
        styler = df.style
        
        # Get numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if not numeric_cols.empty:
            # Create format dictionary only for numeric columns
            format_dict = {}
            for col in numeric_cols:
                format_dict[col] = "{:,.2f}"
            
            # Apply formatting only to numeric columns
            styler = styler.format(format_dict)
        
        return styler
    
    def calculate_statistics(self, table_name: str, column: str) -> Dict:
        """Calculate comprehensive statistics using DuckDB."""
        try:
            # Get column type
            col_type = self.con.execute(f"""
                SELECT data_type 
                FROM information_schema.columns
                WHERE table_name = '{table_name}'
                AND column_name = '{column}'
            """).fetchone()[0]
            
            # Basic stats for all types
            basic_stats = self.con.execute(f"""
                SELECT 
                    COUNT(*) as count,
                    COUNT(DISTINCT "{column}") as unique_count,
                    COUNT(*) - COUNT("{column}") as missing_count,
                    (COUNT(*) - COUNT("{column}"))::FLOAT / COUNT(*) * 100 as missing_pct
                FROM {table_name}
            """).fetchone()
            
            stats = {
                'count': int(basic_stats[0]),
                'unique': int(basic_stats[1]),
                'missing': int(basic_stats[2]),
                'missing_pct': float(basic_stats[3])
            }
            
            # Additional stats for numeric columns
            if col_type in ['INTEGER', 'DOUBLE', 'DECIMAL']:
                numeric_stats = self.con.execute(f"""
                    SELECT 
                        AVG("{column}") as mean,
                        MEDIAN("{column}") as median,
                        STDDEV("{column}") as std,
                        MIN("{column}") as min,
                        MAX("{column}") as max,
                        percentile_cont(0.25) WITHIN GROUP (ORDER BY "{column}") as q1,
                        percentile_cont(0.75) WITHIN GROUP (ORDER BY "{column}") as q3
                    FROM {table_name}
                    WHERE "{column}" IS NOT NULL
                """).fetchone()
                
                stats.update({
                    'mean': float(numeric_stats[0]) if numeric_stats[0] is not None else None,
                    'median': float(numeric_stats[1]) if numeric_stats[1] is not None else None,
                    'std': float(numeric_stats[2]) if numeric_stats[2] is not None else None,
                    'min': float(numeric_stats[3]) if numeric_stats[3] is not None else None,
                    'max': float(numeric_stats[4]) if numeric_stats[4] is not None else None,
                    'percentiles': {
                        '25%': float(numeric_stats[5]) if numeric_stats[5] is not None else None,
                        '75%': float(numeric_stats[6]) if numeric_stats[6] is not None else None
                    }
                })
                
                # Calculate skewness and kurtosis
                moments = self.con.execute(f"""
                    WITH MomentCalc AS (
                        SELECT 
                            AVG("{column}") as mean,
                            STDDEV("{column}") as std,
                            COUNT(*) as n
                        FROM {table_name}
                        WHERE "{column}" IS NOT NULL
                    )
                    SELECT 
                        SUM(POWER(("{column}" - mean) / std, 3)) / (n - 1) as skewness,
                        SUM(POWER(("{column}" - mean) / std, 4)) / (n - 1) - 3 as kurtosis
                    FROM {table_name}, MomentCalc
                    WHERE "{column}" IS NOT NULL
                """).fetchone()
                
                stats.update({
                    'skew': float(moments[0]) if moments[0] is not None else None,
                    'kurtosis': float(moments[1]) if moments[1] is not None else None
                })
            
            return stats
            
        except Exception as e:
            st.error(f"Error calculating statistics for {column}: {str(e)}")
            return {}
    
    def create_correlation_matrix(self, table_name: str, columns: List[str] = None) -> Tuple[Optional[pd.DataFrame], Optional[go.Figure]]:
        """Create correlation analysis with both matrix and heatmap."""
        try:
            # Get numeric columns if not specified
            if not columns:
                columns = [col[0] for col in self.con.execute(f"""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_name = '{table_name}'
                    AND data_type IN ('INTEGER', 'DOUBLE', 'DECIMAL')
                """).fetchall()]
            
            if not columns:
                st.warning("No numeric columns available for correlation analysis.")
                return None, None
            
            # Create correlation matrix using DuckDB
            corr_expressions = []
            for col1 in columns:
                for col2 in columns:
                    corr_expressions.append(f"""
                        CORR("{col1}", "{col2}") as "{col1}_{col2}"
                    """)
            
            corr_query = f"""
            SELECT {', '.join(corr_expressions)}
            FROM {table_name}
            """
            
            # Execute query and reshape results
            corr_results = self.con.execute(corr_query).fetchone()
            corr_matrix = np.zeros((len(columns), len(columns)))
            
            for i, col1 in enumerate(columns):
                for j, col2 in enumerate(columns):
                    idx = i * len(columns) + j
                    corr_matrix[i, j] = corr_results[idx] if corr_results[idx] is not None else 0
            
            # Create DataFrame
            corr_df = pd.DataFrame(corr_matrix, columns=columns, index=columns)
            
            # Create heatmap
            fig = go.Figure(data=go.Heatmap(
                z=corr_matrix,
                x=columns,
                y=columns,
                text=[[f"{val:.3f}" for val in row] for row in corr_matrix],
                texttemplate="%{text}",
                textfont={"size": 10},
                hoverongaps=False,
                colorscale='RdBu_r',
                zmid=0
            ))
            
            fig.update_layout(
                title='Correlation Heatmap',
                title_x=0.5,
                height=700,
                width=800,
                xaxis_title="Features",
                yaxis_title="Features",
                xaxis={'side': 'bottom'}
            )
            
            return corr_df, fig
            
        except Exception as e:
            st.error(f"Error creating correlation analysis: {str(e)}")
            return None, None

class Visualizer:
    """Class to handle all visualization functionality."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
    
    def create_pivot_chart(self, 
                          table_name: str,
                          pivot_df: pd.DataFrame,
                          chart_type: str = 'bar',
                          color_theme: str = 'default',
                          show_values: bool = True,
                          normalize: bool = False) -> Optional[go.Figure]:
        """Create visualization for pivot table results."""
        try:
            if pivot_df is None or pivot_df.empty:
                st.error("No data available for visualization.")
                return None
            
            # Remove total row for visualization
            plot_df = pivot_df[~pivot_df.index.isin(['Total'])].copy() if not isinstance(pivot_df.index, pd.MultiIndex) else pivot_df.copy()
            
            # Handle MultiIndex if present
            if isinstance(plot_df.index, pd.MultiIndex):
                plot_df = plot_df.reset_index()
            
            if isinstance(plot_df.columns, pd.MultiIndex):
                plot_df.columns = [f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col 
                                 for col in plot_df.columns]
            
            # Get numeric columns
            numeric_cols = plot_df.select_dtypes(include=[np.number]).columns.tolist()
            
            # Normalize values if requested
            if normalize and numeric_cols:
                for col in numeric_cols:
                    plot_df[col] = plot_df[col] / plot_df[col].max() * 100 if plot_df[col].max() != 0 else 0
            
            # Get index column name
            index_cols = plot_df.columns.difference(numeric_cols)
            if len(index_cols) == 0:
                st.error("No non-numeric columns available for axes.")
                return None
            
            index_col = index_cols[0]
            
            fig = go.Figure()
            
            if chart_type == 'bar':
                for col in numeric_cols:
                    fig.add_trace(
                        go.Bar(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='auto',
                        )
                    )
                fig.update_layout(barmode='group')
                
            elif chart_type == 'line':
                for col in numeric_cols:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='lines+markers',
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'scatter':
                for col in numeric_cols:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='markers',
                            marker=dict(size=10),
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'area':
                for col in numeric_cols:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='lines',
                            fill='tonexty',
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'heatmap':
                # Reshape data for heatmap if needed
                if len(index_cols) >= 2:
                    pivot_data = plot_df.pivot(
                        index=index_cols[0],
                        columns=index_cols[1],
                        values=numeric_cols[0] if numeric_cols else None
                    )
                    
                    fig = go.Figure(data=go.Heatmap(
                        z=pivot_data.values,
                        x=pivot_data.columns,
                        y=pivot_data.index,
                        text=np.round(pivot_data.values, 2),
                        texttemplate="%{text}",
                        textfont={"size": 10},
                        hoverongaps=False,
                        colorscale='RdBu_r'
                    ))
                else:
                    st.warning("Heatmap requires at least two categorical columns.")
                    return None
            
            # Update color theme if specified
            if color_theme != 'default':
                if hasattr(fig, 'data'):
                    for trace in fig.data:
                        if hasattr(trace, 'marker'):
                            trace.marker.color = color_theme
                        if hasattr(trace, 'line'):
                            trace.line.color = color_theme
            
            # Update layout
            fig.update_layout(
                title='Pivot Table Visualization',
                title_x=0.5,
                height=600,
                showlegend=True,
                template='plotly_white',
                xaxis_title=index_col,
                yaxis_title="Values",
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating pivot visualization: {str(e)}")
            return None
    
    def create_time_series_chart(self,
                               table_name: str,
                               time_col: str,
                               value_cols: List[str],
                               agg_func: str = 'SUM',
                               interval: str = 'month') -> Optional[go.Figure]:
        """Create time series visualization directly from DuckDB."""
        try:
            # Create time series query
            date_trunc = f"date_trunc('{interval}', {time_col})"
            value_expressions = [f"{agg_func}({col}) as {col}" for col in value_cols]
            
            query = f"""
            SELECT 
                {date_trunc} as time_period,
                {', '.join(value_expressions)}
            FROM {table_name}
            GROUP BY {date_trunc}
            ORDER BY time_period
            """
            
            # Execute query
            df = self.con.execute(query).df()
            
            # Create figure
            fig = go.Figure()
            
            for col in value_cols:
                fig.add_trace(
                    go.Scatter(
                        x=df['time_period'],
                        y=df[col],
                        name=col,
                        mode='lines+markers',
                        hovertemplate=f"{col}: %{{y:,.2f}}<br>Date: %{{x|%Y-%m-%d}}<extra></extra>"
                    )
                )
            
            # Update layout
            fig.update_layout(
                title=f"Time Series Analysis ({interval.capitalize()} Intervals)",
                title_x=0.5,
                height=500,
                template='plotly_white',
                xaxis_title="Time Period",
                yaxis_title=f"{agg_func.capitalize()}ed Values",
                hovermode='x unified',
                showlegend=True
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating time series chart: {str(e)}")
            return None
    
    def create_distribution_plot(self,
                               table_name: str,
                               column: str,
                               plot_type: str = 'histogram',
                               bins: int = 30) -> Optional[go.Figure]:
        """Create distribution visualization for a numeric column."""
        try:
            # Get column statistics
            stats = self.con.execute(f"""
                SELECT 
                    MIN("{column}") as min_val,
                    MAX("{column}") as max_val,
                    AVG("{column}") as mean,
                    MEDIAN("{column}") as median,
                    STDDEV("{column}") as std
                FROM {table_name}
                WHERE "{column}" IS NOT NULL
            """).fetchone()
            
            min_val, max_val, mean, median, std = stats
            
            # Create figure based on plot type
            if plot_type == 'histogram':
                # Calculate bin width
                bin_width = (max_val - min_val) / bins
                
                query = f"""
                SELECT width_bucket("{column}", {min_val}, {max_val}, {bins}) as bucket,
                       COUNT(*) as count,
                       MIN("{column}") as bin_start,
                       MAX("{column}") as bin_end
                FROM {table_name}
                WHERE "{column}" IS NOT NULL
                GROUP BY bucket
                ORDER BY bucket
                """
                
                hist_data = self.con.execute(query).df()
                
                fig = go.Figure(data=[
                    go.Bar(
                        x=[(row['bin_start'] + row['bin_end']) / 2 for _, row in hist_data.iterrows()],
                        y=hist_data['count'],
                        width=bin_width,
                        name='Frequency'
                    )
                ])
                
                # Add mean and median lines
                fig.add_vline(x=mean, line_dash="dash", line_color="red",
                            annotation_text="Mean")
                fig.add_vline(x=median, line_dash="dash", line_color="green",
                            annotation_text="Median")
                
            elif plot_type == 'box':
                # Calculate quartiles
                quartiles = self.con.execute(f"""
                    SELECT 
                        percentile_cont(0.25) WITHIN GROUP (ORDER BY "{column}") as q1,
                        percentile_cont(0.75) WITHIN GROUP (ORDER BY "{column}") as q3
                    FROM {table_name}
                    WHERE "{column}" IS NOT NULL
                """).fetchone()
                
                q1, q3 = quartiles
                
                fig = go.Figure(data=[
                    go.Box(
                        y=self.con.execute(f'SELECT "{column}" FROM {table_name} WHERE "{column}" IS NOT NULL').df()[column],
                        name=column,
                        boxpoints='outliers'
                    )
                ])
            
            # Update layout
            fig.update_layout(
                title=f'Distribution of {column}',
                title_x=0.5,
                height=500,
                template='plotly_white',
                showlegend=True,
                annotations=[
                    dict(
                        x=0.95,
                        y=0.95,
                        xref='paper',
                        yref='paper',
                        text=f'Mean: {mean:.2f}<br>Median: {median:.2f}<br>Std Dev: {std:.2f}',
                        showarrow=False,
                        bgcolor='white',
                        bordercolor='black',
                        borderwidth=1
                    )
                ]
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating distribution plot: {str(e)}")
            return None
class ExportManager:
    """Class to handle all export functionality."""
    
    def __init__(self, connection: duckdb.DuckDBPyConnection):
        self.con = connection
    
    def to_excel(self,
                table_name: str,
                pivot_df: Optional[pd.DataFrame] = None,
                stats: Optional[Dict] = None,
                cleaning_report: Optional[Dict] = None) -> Optional[bytes]:
        """Export data to Excel with formatting."""
        try:
            output = io.BytesIO()
            
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                # Get workbook and add formats
                workbook = writer.book
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#D3D3D3',
                    'border': 1,
                    'text_wrap': True
                })
                
                number_format = workbook.add_format({
                    'num_format': '#,##0.00',
                    'border': 1
                })
                
                # Write original data
                # Use DuckDB's streaming capabilities for large datasets
                chunk_size = 10000
                total_rows = self.con.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
                
                for offset in range(0, total_rows, chunk_size):
                    chunk_query = f"""
                    SELECT *
                    FROM {table_name}
                    LIMIT {chunk_size}
                    OFFSET {offset}
                    """
                    chunk_df = self.con.execute(chunk_query).df()
                    
                    if offset == 0:
                        # First chunk - write with header
                        chunk_df.to_excel(writer, sheet_name='Data', index=False, startrow=offset)
                    else:
                        # Subsequent chunks - append without header
                        chunk_df.to_excel(writer, sheet_name='Data', index=False, 
                                        startrow=offset + 1, header=False)
                
                # Format data sheet
                worksheet = writer.sheets['Data']
                worksheet.set_row(0, None, header_format)
                worksheet.set_column(0, 50, 15)
                worksheet.freeze_panes(1, 0)
                
                # Write pivot results if available
                if pivot_df is not None and not pivot_df.empty:
                    pivot_df.to_excel(writer, sheet_name='Pivot Analysis')
                    worksheet = writer.sheets['Pivot Analysis']
                    worksheet.set_row(0, None, header_format)
                    worksheet.set_column(0, 50, 15)
                    worksheet.freeze_panes(1, 0)
                
                # Write statistics if available
                if stats:
                    stats_sheet = workbook.add_worksheet('Statistics')
                    stats_sheet.set_column(0, 0, 30)  # Column name width
                    stats_sheet.set_column(1, 5, 15)  # Stats width
                    
                    row = 0
                    for column, column_stats in stats.items():
                        stats_sheet.write(row, 0, f"Statistics for {column}", header_format)
                        row += 1
                        
                        for stat_name, stat_value in column_stats.items():
                            if isinstance(stat_value, dict):
                                stats_sheet.write(row, 0, stat_name)
                                col = 1
                                for sub_name, sub_value in stat_value.items():
                                    stats_sheet.write(row, col, f"{sub_name}: {sub_value}")
                                    col += 1
                            else:
                                stats_sheet.write(row, 0, stat_name)
                                stats_sheet.write(row, 1, stat_value, 
                                                number_format if isinstance(stat_value, (int, float)) else None)
                            row += 1
                        row += 1  # Add space between columns
                
                # Write cleaning report if available
                if cleaning_report:
                    cleaning_sheet = workbook.add_worksheet('Cleaning Report')
                    cleaning_sheet.set_column(0, 0, 30)
                    cleaning_sheet.set_column(1, 5, 15)
                    
                    row = 0
                    # Write summary
                    cleaning_sheet.write(row, 0, "Cleaning Summary", header_format)
                    row += 1
                    
                    summary = cleaning_report['summary']
                    cleaning_sheet.write(row, 0, "Initial Rows")
                    cleaning_sheet.write(row, 1, summary['initial_state']['rows'])
                    row += 1
                    cleaning_sheet.write(row, 0, "Final Rows")
                    cleaning_sheet.write(row, 1, summary['final_state']['rows'])
                    row += 1
                    cleaning_sheet.write(row, 0, "Rows Removed")
                    cleaning_sheet.write(row, 1, summary['changes']['rows_removed'])
                    row += 2
                    
                    # Write missing values section
                    if cleaning_report.get('missing_values'):
                        cleaning_sheet.write(row, 0, "Missing Values", header_format)
                        row += 1
                        missing = cleaning_report['missing_values']
                        cleaning_sheet.write(row, 0, "Total Missing Cells")
                        cleaning_sheet.write(row, 1, missing['total_missing_cells'])
                        row += 1
                        cleaning_sheet.write(row, 0, "Missing Percentage")
                        cleaning_sheet.write(row, 1, f"{missing['overall_missing_percentage']:.2f}%")
                        row += 2
                    
                    # Write outliers section
                    if cleaning_report.get('outliers'):
                        cleaning_sheet.write(row, 0, "Outliers", header_format)
                        row += 1
                        outliers = cleaning_report['outliers']
                        cleaning_sheet.write(row, 0, "Total Outliers")
                        cleaning_sheet.write(row, 1, outliers['total_outliers'])
                        row += 1
                        cleaning_sheet.write(row, 0, "Columns with Outliers")
                        cleaning_sheet.write(row, 1, outliers['columns_with_outliers'])
                        row += 2
                
            return output.getvalue()
            
        except Exception as e:
            st.error(f"Error exporting to Excel: {str(e)}")
            return None
    
    def to_csv(self, 
               table_name: str,
               include_index: bool = False,
               compression: Optional[str] = None) -> Tuple[Optional[bytes], str]:
        """Export data to CSV with optional compression."""
        try:
            output = io.StringIO()
            
            # Use DuckDB's CSV writer
            self.con.execute(f"""
                COPY (SELECT * FROM {table_name}) 
                TO '{output}' 
                WITH (FORMAT CSV, HEADER)
            """)
            
            data = output.getvalue().encode('utf-8')
            
            if compression == 'gzip':
                import gzip
                compressed = io.BytesIO()
                with gzip.GzipFile(fileobj=compressed, mode='wb') as gz:
                    gz.write(data)
                return compressed.getvalue(), 'gz'
            
            elif compression == 'zip':
                import zipfile
                compressed = io.BytesIO()
                with zipfile.ZipFile(compressed, 'w', zipfile.ZIP_DEFLATED) as zf:
                    zf.writestr('data.csv', data)
                return compressed.getvalue(), 'zip'
            
            else:
                return data, 'csv'
            
        except Exception as e:
            st.error(f"Error exporting to CSV: {str(e)}")
            return None, ''
    
    def to_html(self,
                table_name: str,
                include_stats: bool = True,
                include_viz: bool = True) -> Optional[str]:
        """Export data and analysis to HTML."""
        try:
            # Get basic table info
            table_info = self.con.execute(f"""
                SELECT 
                    COUNT(*) as row_count,
                    (SELECT COUNT(*) FROM pragma_table_info('{table_name}')) as col_count
                FROM {table_name}
            """).fetchone()
            
            row_count, col_count = table_info
            
            # Start building HTML
            html = f"""
            <html>
            <head>
                <title>Data Analysis Report</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    .metric {{ background-color: #f8f9fa; padding: 10px; margin: 5px; border-radius: 5px; }}
                </style>
            </head>
            <body>
                <h1>Data Analysis Report</h1>
                <h2>Dataset Overview</h2>
                <div class="metric">
                    <p>Total Rows: {row_count:,}</p>
                    <p>Total Columns: {col_count}</p>
                </div>
            """
            
            if include_stats:
                html += "<h2>Column Statistics</h2>"
                # Get statistics for each column
                for col_info in self.con.execute(f"PRAGMA table_info('{table_name}')").fetchall():
                    col_name = col_info[1]
                    col_type = col_info[2]
                    
                    html += f"<h3>Column: {col_name} ({col_type})</h3>"
                    
                    # Basic stats for all columns
                    basic_stats = self.con.execute(f"""
                        SELECT 
                            COUNT(*) as count,
                            COUNT(DISTINCT "{col_name}") as unique_count,
                            COUNT(*) - COUNT("{col_name}") as null_count
                        FROM {table_name}
                    """).fetchone()
                    
                    html += f"""
                    <div class="metric">
                        <p>Total Values: {basic_stats[0]:,}</p>
                        <p>Unique Values: {basic_stats[1]:,}</p>
                        <p>Missing Values: {basic_stats[2]:,}</p>
                    </div>
                    """
                    
                    # Additional stats for numeric columns
                    if col_type in ['INTEGER', 'DOUBLE', 'DECIMAL']:
                        num_stats = self.con.execute(f"""
                            SELECT 
                                MIN("{col_name}") as min_val,
                                MAX("{col_name}") as max_val,
                                AVG("{col_name}") as mean,
                                MEDIAN("{col_name}") as median
                            FROM {table_name}
                            WHERE "{col_name}" IS NOT NULL
                        """).fetchone()
                        
                        html += f"""
                        <div class="metric">
                            <p>Minimum: {num_stats[0]:,.2f}</p>
                            <p>Maximum: {num_stats[1]:,.2f}</p>
                            <p>Mean: {num_stats[2]:,.2f}</p>
                            <p>Median: {num_stats[3]:,.2f}</p>
                        </div>
                        """
            
            # Add sample data table
            html += "<h2>Sample Data</h2>"
            sample_data = self.con.execute(f"""
                SELECT * FROM {table_name} LIMIT 10
            """).df()
            
            html += sample_data.to_html(index=False, classes='styled-table')
            
            html += """
            </body>
            </html>
            """
            
            return html
            
        except Exception as e:
            st.error(f"Error exporting to HTML: {str(e)}")
            return None

def main():
    """Main application function."""
    
    # Initialize session state variables
    if 'duckdb_con' not in st.session_state:
        st.session_state.duckdb_con = duckdb.connect(database=':memory:')
    
    if 'initial_table' not in st.session_state:
        st.session_state.initial_table = None
    
    if 'current_table' not in st.session_state:
        st.session_state.current_table = None
    
    if 'pivot_results' not in st.session_state:
        st.session_state.pivot_results = None
    
    if 'cleaning_report' not in st.session_state:
        st.session_state.cleaning_report = None

    st.title("ðŸ“Š DuckDB Excel Analysis Dashboard")

    # Initialize components
    data_loader = DataLoader(st.session_state.duckdb_con)
    data_processor = DataProcessor(st.session_state.duckdb_con)
    analytics = Analytics(st.session_state.duckdb_con)
    visualizer = Visualizer(st.session_state.duckdb_con)
    export_manager = ExportManager(st.session_state.duckdb_con)

    # File Upload Section
    with st.sidebar:
        st.header("ðŸ“ File Upload")
        uploaded_files = st.file_uploader(
            "Upload Excel Files",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="Upload one or more Excel files for analysis"
        )

        if not uploaded_files:
            st.info("ðŸ‘† Please upload Excel files to begin analysis.")
            return

        # Data Cleaning Options
        st.header("âš™ï¸ Data Processing Settings")
        handle_missing = st.checkbox("Handle Missing Values", value=True,
                                   help="Replace missing values with 'Data Not Available'")
        handle_outliers = st.checkbox("Handle Outliers", value=True,
                                    help="Detect and clip outliers using IQR method")
        handle_duplicates = st.checkbox("Remove Duplicates", value=True,
                                      help="Remove exact duplicate rows")

    # Load and process files
    loaded_tables = []
    
    with st.spinner('Processing uploaded files...'):
        for file in uploaded_files:
            table_name = data_loader.load_excel(file)
            if table_name:
                loaded_tables.append(table_name)
                st.success(f"âœ“ Successfully loaded: {file.name}")
                
                # File preview
                with st.expander(f"ðŸ“„ Preview: {file.name}"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("Data Sample:")
                        sample_query = f"SELECT * FROM {table_name} LIMIT 5"
                        st.dataframe(st.session_state.duckdb_con.execute(sample_query).df())
                    
                    with col2:
                        st.write("Summary:")
                        stats_query = f"""
                            SELECT 
                                COUNT(*) as row_count,
                                (SELECT COUNT(*) FROM pragma_table_info('{table_name}')) as col_count
                            FROM {table_name}
                        """
                        basic_stats = st.session_state.duckdb_con.execute(stats_query).fetchone()
                        
                        st.write(f"- Rows: {basic_stats[0]:,}")
                        st.write(f"- Columns: {basic_stats[1]:,}")
                        
                        numeric_query = f"""
                            SELECT COUNT(*) 
                            FROM pragma_table_info('{table_name}')
                            WHERE type IN ('INTEGER', 'DOUBLE', 'DECIMAL')
                        """
                        numeric_count = st.session_state.duckdb_con.execute(numeric_query).fetchone()[0]
                        st.write(f"- Numeric Columns: {numeric_count:,}")
                        
                        columns = st.session_state.duckdb_con.execute(f"PRAGMA table_info('{table_name}')").fetchall()
                        column_names = [col[1] for col in columns]
                        
                        null_counts = []
                        for col in column_names:
                            null_counts.append(f"COUNT(*) FILTER (WHERE \"{col}\" IS NULL)")
                        
                        missing_query = f"""
                            SELECT {' + '.join(null_counts)} as total_nulls
                            FROM {table_name}
                        """
                        missing_count = st.session_state.duckdb_con.execute(missing_query).fetchone()[0]
                        st.write(f"- Missing Values: {missing_count:,}")

    if not loaded_tables:
        st.error("âŒ No valid data loaded.")
        return

    # Get common columns for merge
    common_columns = st.session_state.duckdb_con.execute(f"""
        WITH ColumnLists AS (
            {' UNION ALL '.join([
                f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table}'"
                for table in loaded_tables
            ])}
        )
        SELECT column_name
        FROM ColumnLists
        GROUP BY column_name
        HAVING COUNT(*) = {len(loaded_tables)}
    """).fetchall()
    
    common_columns = [col[0] for col in common_columns]

    if not common_columns:
        st.error("âŒ No common columns found between files!")
        for table in loaded_tables:
            cols = st.session_state.duckdb_con.execute(f"""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = '{table}'
            """).fetchall()
            st.write(f"Columns in {table}:", [col[0] for col in cols])
        return

    # Main Analysis Tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "ðŸ”„ Merge & Clean",
        "ðŸ“Š Analysis",
        "ðŸ“ˆ Visualization",
        "ðŸ’¾ Export"
    ])

    # Tab 1: Merge & Clean
    with tab1:
        st.header("Data Merge & Filter")
        
        # Step 1: Merge Configuration
        st.subheader("1ï¸âƒ£ Merge Configuration")
        merge_keys = st.multiselect(
            "Select columns to merge on:",
            common_columns,
            help="Select one or more columns to use as merge keys",
            key="merge_keys_main"
        )

        merge_type = st.selectbox(
            "Select merge type:",
            ['left', 'right', 'inner', 'outer'],
            help="Choose how to merge the files",
            key="merge_type_main"
        )

        if st.button("ðŸ”„ Merge Data", use_container_width=True, key="merge_button"):
            if not merge_keys:
                st.warning("âš ï¸ Please select merge columns.")
                return

            with st.spinner("Merging data..."):
                merged_table = data_processor.merge_tables(loaded_tables, merge_keys, merge_type)

                if merged_table:
                    st.session_state.initial_table = merged_table
                    st.session_state.current_table = merged_table
                    st.success("âœ… Data merged successfully!")

        # Step 2: Data Filtering
        if st.session_state.current_table:
            st.subheader("2ï¸âƒ£ Filter Data")
            
            with st.expander("Configure Filters", expanded=True):
                columns = st.session_state.duckdb_con.execute(f"""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = '{st.session_state.current_table}'
                """).fetchall()
                columns = [col[0] for col in columns]
                
                filter_cols = st.multiselect(
                    "Select columns to filter:",
                    columns,
                    help="Choose columns you want to filter on",
                    key="filter_cols_main"
                )

                active_filters = {}
                if filter_cols:
                    cols_per_row = 2
                    for i in range(0, len(filter_cols), cols_per_row):
                        cols = st.columns(cols_per_row)
                        for j, col in enumerate(filter_cols[i:i + cols_per_row]):
                            with cols[j]:
                                unique_vals = st.session_state.duckdb_con.execute(f"""
                                    SELECT DISTINCT "{col}"
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                    ORDER BY "{col}"
                                """).fetchall()
                                unique_vals = [val[0] for val in unique_vals]
                                
                                selected = st.multiselect(
                                    f"Filter {col}:",
                                    unique_vals,
                                    help=f"Select values to include for {col}",
                                    key=f"filter_select_{col}"
                                )
                                if selected:
                                    active_filters[col] = selected

                col1, col2 = st.columns(2)
                with col1:
                    if active_filters and st.button("Apply Filters", use_container_width=True, key="apply_filters"):
                        filtered_table = f"{st.session_state.current_table}_filtered"
                        filter_conditions = []
                        for col, values in active_filters.items():
                            placeholders = ', '.join([f"'{str(v)}'" for v in values])
                            filter_conditions.append(f'"{col}"::VARCHAR IN ({placeholders})')
                        
                        filter_query = f"""
                        CREATE TABLE {filtered_table} AS
                        SELECT *
                        FROM {st.session_state.current_table}
                        WHERE {' AND '.join(filter_conditions)}
                        """
                        
                        st.session_state.duckdb_con.execute(filter_query)
                        
                        original_count = st.session_state.duckdb_con.execute(
                            f"SELECT COUNT(*) FROM {st.session_state.current_table}"
                        ).fetchone()[0]
                        
                        filtered_count = st.session_state.duckdb_con.execute(
                            f"SELECT COUNT(*) FROM {filtered_table}"
                        ).fetchone()[0]
                        
                        st.session_state.current_table = filtered_table
                        
                        st.success(f"""
                            âœ… Filters applied successfully!
                            - Original rows: {original_count:,}
                            - Filtered rows: {filtered_count:,}
                            - Rows removed: {original_count - filtered_count:,}
                        """)

                with col2:
                    if st.button("Reset Filters", use_container_width=True, key="reset_filters"):
                        st.session_state.current_table = st.session_state.initial_table
                        st.success("ðŸ”„ Filters reset to original data")

            with st.expander("ðŸ‘€ Preview Filtered Data", expanded=False):
                if st.session_state.current_table:
                    preview_query = f"SELECT * FROM {st.session_state.current_table} LIMIT 5"
                    st.dataframe(st.session_state.duckdb_con.execute(preview_query).df())
                    
                    row_count = st.session_state.duckdb_con.execute(
                        f"SELECT COUNT(*) FROM {st.session_state.current_table}"
                    ).fetchone()[0]
                    
                    col_count = st.session_state.duckdb_con.execute(
                        f"SELECT COUNT(*) FROM pragma_table_info('{st.session_state.current_table}')"
                    ).fetchone()[0]
                    
                    st.write(f"Total rows: {row_count:,}")
                    st.write(f"Total columns: {col_count:,}")

            # Step 3: Data Cleaning
            st.subheader("3ï¸âƒ£ Clean Data")
            if st.button("ðŸ§¹ Clean Data", use_container_width=True, key="clean_data"):
                with st.spinner("Cleaning data..."):
                    if handle_missing or handle_outliers or handle_duplicates:
                        cleaned_table = data_processor.clean_data(st.session_state.current_table)
                        data_processor.display_cleaning_report()
                        st.session_state.cleaning_report = data_processor.cleaning_report
                        st.session_state.current_table = cleaned_table
                        st.success("âœ… Data cleaned successfully!")

    # Tab 2: Analysis
    with tab2:
        if st.session_state.current_table:
            st.header("Analysis Options")

            # Get column types
            columns = st.session_state.duckdb_con.execute(f"""
                SELECT column_name, data_type
                FROM information_schema.columns 
                WHERE table_name = '{st.session_state.current_table}'
            """).fetchall()
            
            # Separate columns by type
            numeric_cols = [col[0] for col in columns if col[1].upper() in ('INTEGER', 'DOUBLE', 'DECIMAL', 'NUMERIC', 'FLOAT')]
            datetime_cols = [col[0] for col in columns if 'TIME' in col[1].upper() or 'DATE' in col[1].upper()]
            categorical_cols = [col[0] for col in columns if col[1].upper() not in 
                            ('INTEGER', 'DOUBLE', 'DECIMAL', 'NUMERIC', 'FLOAT') and 
                            'TIME' not in col[1].upper() and 'DATE' not in col[1].upper()]
            all_columns = [col[0] for col in columns]

            analysis_type = st.multiselect(
                "Select Analysis Types:",
                ["Pivot Table", "Statistical Analysis", "Correlation Analysis"],
                key="analysis_type_select"
            )

            # Pivot Table Analysis
            # Pivot Table Analysis
            if "Pivot Table" in analysis_type:
                st.subheader("Pivot Table Analysis")
                
                pivot_type = st.radio(
                    "Select Pivot Table Type:",
                    ["Basic Pivot Table", "Advanced Excel-Style Pivot Table"],
                    key="pivot_type_radio"
                )
                
                if pivot_type == "Basic Pivot Table":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        group_cols = st.multiselect(
                            "Select Group By Columns:",
                            all_columns,
                            key="group_cols_pivot"
                        )
                    
                    with col2:
                        value_cols = st.multiselect(
                            "Select Value Columns:",
                            all_columns,
                            key="value_cols_pivot"
                        )
                    
                    with col3:
                        agg_funcs = st.multiselect(
                            "Select Aggregations:",
                            ['count', 'distinct_count', 'sum', 'mean', 'max', 'min', 'first', 'last'],
                            key="agg_funcs_pivot"
                        )

                    # Filters section
                    if group_cols:
                        st.subheader("Filters")
                        filter_cols = st.multiselect(
                            "Select columns to filter:",
                            group_cols,
                            key="filter_cols_pivot"
                        )

                        filters = {}
                        if filter_cols:
                            cols_per_row = 3
                            for i in range(0, len(filter_cols), cols_per_row):
                                cols = st.columns(cols_per_row)
                                for j, col in enumerate(filter_cols[i:i + cols_per_row]):
                                    with cols[j]:
                                        distinct_vals = st.session_state.duckdb_con.execute(f"""
                                            SELECT DISTINCT "{col}"
                                            FROM {st.session_state.current_table}
                                            WHERE "{col}" IS NOT NULL
                                            ORDER BY "{col}"
                                        """).fetchall()
                                        distinct_vals = [str(val[0]) for val in distinct_vals]
                                        
                                        selected_vals = st.multiselect(
                                            f"Filter {col}:",
                                            distinct_vals,
                                            key=f"filter_vals_pivot_{col}"
                                        )
                                        
                                        if selected_vals:
                                            filters[col] = selected_vals

                    if st.button("Generate Basic Pivot Table", key="gen_basic_pivot"):
                        if not (group_cols and value_cols and agg_funcs):
                            st.warning("Please select all required pivot table parameters.")
                        else:
                            with st.spinner("Creating pivot table..."):
                                pivot_df = analytics.create_pivot(
                                    st.session_state.current_table,
                                    group_cols,
                                    value_cols,
                                    agg_funcs,
                                    filters=filters
                                )
                                
                                if pivot_df is not None:
                                    st.session_state.pivot_results = pivot_df
                                    st.success("Pivot table created successfully!")
                                    
                                    st.write("##### Pivot Table Results")
                                    st.dataframe(analytics.format_pivot_table(pivot_df))
                                    
                                    st.write("##### Visualization")
                                    viz_type = st.selectbox(
                                        "Select visualization type:",
                                        ['bar', 'line', 'area', 'scatter'],
                                        key="viz_type_basic"
                                    )
                                    
                                    fig = visualizer.create_pivot_chart(
                                        st.session_state.current_table,
                                        pivot_df,
                                        chart_type=viz_type
                                    )
                                    
                                    if fig:
                                        st.plotly_chart(fig, use_container_width=True)

                else:  # Advanced Excel-Style Pivot Table
                    col1, col2 = st.columns([2, 3])
                    
                    with col1:
                        st.write("##### Pivot Table Fields")
                        
                        filter_cols = st.multiselect(
                            "Filters (Optional):",
                            all_columns,
                            key="filter_cols_adv"
                        )

                        filters = {}
                        if filter_cols:
                            for col in filter_cols:
                                distinct_vals = st.session_state.duckdb_con.execute(f"""
                                    SELECT DISTINCT "{col}"
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                    ORDER BY "{col}"
                                """).fetchall()
                                distinct_vals = [str(val[0]) for val in distinct_vals]
                                
                                selected_vals = st.multiselect(
                                    f"Select values for {col}:",
                                    distinct_vals,
                                    key=f"filter_vals_adv_{col}"
                                )
                                
                                if selected_vals:
                                    filters[col] = selected_vals
                        
                        row_cols = st.multiselect(
                            "Row Labels:",
                            all_columns,
                            key="row_cols_adv"
                        )
                        
                        column_cols = st.multiselect(
                            "Column Labels:",
                            all_columns,
                            key="column_cols_adv"
                        )
                        
                        value_cols = st.multiselect(
                            "Values:",
                            all_columns,
                            key="value_cols_adv"
                        )
                    
                    with col2:
                        if value_cols:
                            st.write("##### Value Settings")
                            value_settings = {}
                            
                            for col in value_cols:
                                st.write(f"Settings for {col}:")
                                settings_col1, settings_col2 = st.columns(2)
                                
                                with settings_col1:
                                    agg_func = st.selectbox(
                                        f"Aggregation for {col}:",
                                        ['Sum', 'Average', 'Count', 'Min', 'Max',
                                        'Distinct Count', 'First', 'Last'],
                                        key=f"agg_func_adv_{col}"
                                    )
                                
                                with settings_col2:
                                    show_as = st.selectbox(
                                        "Show values as:",
                                        ['Values',
                                        '% of Row Total',
                                        '% of Column Total',
                                        '% of Grand Total',
                                        'Difference From',
                                        '% Difference From',
                                        'Running Total'],
                                        key=f"show_as_adv_{col}"
                                    )
                                
                                format_col1, format_col2 = st.columns(2)
                                with format_col1:
                                    decimal_places = st.number_input(
                                        "Decimal places:",
                                        min_value=0,
                                        max_value=6,
                                        value=2,
                                        key=f"decimal_adv_{col}"
                                    )
                                
                                with format_col2:
                                    number_format = st.selectbox(
                                        "Number format:",
                                        ['Number',
                                        'Currency',
                                        'Percentage',
                                        'Scientific'],
                                        key=f"format_adv_{col}"
                                    )
                                
                                value_settings[col] = {
                                    'aggregation': agg_func.upper(),
                                    'show_as': show_as,
                                    'decimal_places': decimal_places,
                                    'number_format': number_format
                                }

                    if st.button("Generate Advanced Pivot Table", key="gen_adv_pivot"):
                        if not (row_cols or column_cols) or not value_cols:
                            st.warning("Please select at least row/column fields and value fields.")
                        else:
                            with st.spinner("Creating advanced pivot table..."):
                                try:
                                    # Process filters
                                    filtered_table = st.session_state.current_table
                                    if filters:
                                        filtered_table = f"{st.session_state.current_table}_filtered"
                                        filter_conditions = []
                                        for col, values in filters.items():
                                            placeholders = ', '.join([f"'{str(v)}'" for v in values])
                                            filter_conditions.append(f'"{col}"::VARCHAR IN ({placeholders})')
                                        
                                        filter_query = f"""
                                        CREATE TABLE {filtered_table} AS
                                        SELECT *
                                        FROM {st.session_state.current_table}
                                        WHERE {' AND '.join(filter_conditions)}
                                        """
                                        st.session_state.duckdb_con.execute(filter_query)
                                    
                                    # Create pivot query
                                    dimensions = row_cols + column_cols
                                    dimension_cols = ', '.join([f'"{col}"' for col in dimensions])
                                    
                                    agg_expressions = []
                                    for col, settings in value_settings.items():
                                        agg_func = settings['aggregation']
                                        if agg_func == 'DISTINCT COUNT':
                                            agg_expr = f'COUNT(DISTINCT "{col}") as "{col}_{agg_func}"'
                                        elif agg_func in ['FIRST', 'LAST']:
                                            agg_expr = f'FIRST_VALUE("{col}") as "{col}_{agg_func}"'
                                        else:
                                            agg_expr = f'{agg_func}("{col}") as "{col}_{agg_func}"'
                                        agg_expressions.append(agg_expr)
                                    
                                    query = f"""
                                    SELECT 
                                        {dimension_cols},
                                        {', '.join(agg_expressions)}
                                    FROM {filtered_table}
                                    GROUP BY {dimension_cols}
                                    """
                                    
                                    pivot_df = st.session_state.duckdb_con.execute(query).df()
                                    
                                    if column_cols:
                                        pivot_df = pivot_df.pivot_table(
                                            index=row_cols,
                                            columns=column_cols,
                                            values=[f"{col}_{settings['aggregation']}" 
                                                for col, settings in value_settings.items()]
                                        )
                                    
                                    st.session_state.pivot_results = pivot_df
                                    
                                    # Inside the main() function, replace the pivot visualization section with this:

                                    if pivot_df is not None:
                                        st.write("##### Advanced Pivot Table Results")
                                        formatted_pivot = pivot_df.style
                                        
                                        # Handle MultiIndex columns if they exist
                                        if isinstance(pivot_df.columns, pd.MultiIndex):
                                            numeric_cols = pivot_df.select_dtypes(include=[np.number]).columns
                                        else:
                                            numeric_cols = pivot_df.select_dtypes(include=[np.number]).columns.tolist()
                                        
                                        format_dict = {}
                                        
                                        for col, settings in value_settings.items():
                                            # Handle both MultiIndex and regular columns
                                            if isinstance(pivot_df.columns, pd.MultiIndex):
                                                col_names = [c for c in pivot_df.columns if str(c[0]).startswith(f"{col}_")]
                                            else:
                                                col_names = [c for c in pivot_df.columns if str(c).startswith(f"{col}_")]
                                            
                                            for col_name in col_names:
                                                # For MultiIndex, check if the column tuple contains numeric data
                                                if isinstance(col_name, tuple):
                                                    col_to_check = col_name[0]  # Get first level of MultiIndex
                                                else:
                                                    col_to_check = col_name
                                                
                                                # Check if the column contains numeric data
                                                try:
                                                    if pd.api.types.is_numeric_dtype(pivot_df[col_name]):
                                                        if settings['number_format'] == 'Percentage':
                                                            format_dict[col_name] = "{:.%}".format
                                                        elif settings['number_format'] == 'Currency':
                                                            format_dict[col_name] = "${:,.2f}".format
                                                        elif settings['number_format'] == 'Scientific':
                                                            format_dict[col_name] = "{:.2e}".format
                                                        else:  # Number format
                                                            format_dict[col_name] = "{:,.2f}".format
                                                except (KeyError, ValueError):
                                                    continue
                                        
                                        # Apply formatting if any format specifications exist
                                        if format_dict:
                                            try:
                                                formatted_pivot = formatted_pivot.format(format_dict)
                                            except Exception as e:
                                                st.warning(f"Could not apply all formatting: {str(e)}")
                                        
                                        st.dataframe(formatted_pivot)
                                        
                                        # Visualization section
                                        st.write("##### Visualization")
                                        chart_type = st.selectbox(
                                            "Select chart type:",
                                            ['Bar Chart', 'Line Chart', 'Area Chart', 'Heatmap'],
                                            key="chart_type_adv"
                                        )
                                        
                                        # Reset index for visualization if it's a MultiIndex
                                        plot_df = pivot_df.copy()
                                        if isinstance(plot_df.index, pd.MultiIndex):
                                            plot_df = plot_df.reset_index()
                                        
                                        # Handle MultiIndex columns for visualization
                                        if isinstance(plot_df.columns, pd.MultiIndex):
                                            plot_df.columns = [f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col 
                                                             for col in plot_df.columns]
                                        
                                        # Create the visualization
                                        try:
                                            fig = visualizer.create_pivot_chart(
                                                st.session_state.current_table,
                                                plot_df,
                                                chart_type=chart_type.lower().replace(' ', '')
                                            )
                                            
                                            if fig:
                                                st.plotly_chart(fig, use_container_width=True)
                                        except Exception as e:
                                            st.error(f"Error creating visualization: {str(e)}")
                                            st.info("Try a different chart type or data selection.")
                                            
                                except Exception as e:
                                    st.error(f"Error creating pivot table: {str(e)}")
                                    st.error("Some columns might not support the selected aggregation functions.")
                                    st.info("Try different combinations of columns and aggregations.")

            # Statistical Analysis Section
            # Statistical Analysis Section
            # Inside the Statistical Analysis section, update the string analysis part:
            if "Statistical Analysis" in analysis_type:
                st.subheader("Statistical Analysis")
                
                # Get all columns with their types
                all_cols_query = f"""
                    SELECT column_name, data_type
                    FROM information_schema.columns 
                    WHERE table_name = '{st.session_state.current_table}'
                """
                all_cols_result = st.session_state.duckdb_con.execute(all_cols_query).fetchall()
                all_cols_list = [col[0] for col in all_cols_result]
                
                stat_columns = st.multiselect(
                    "Select columns for statistical analysis:",
                    options=all_cols_list,
                    help="Select columns for detailed statistics (including text/string analysis)",
                    key="stat_cols_select_new"
                )

                if stat_columns and st.button("Calculate Statistics", key="calc_stats_new"):
                    with st.spinner("Calculating statistics..."):
                        stats_results = {}
                        
                        for col in stat_columns:
                            # Get column type
                            col_type = [c[1] for c in all_cols_result if c[0] == col][0]
                            
                            # Basic counts for all types
                            basic_stats = st.session_state.duckdb_con.execute(f"""
                                SELECT 
                                    COUNT(*) as total_count,
                                    COUNT(DISTINCT "{col}") as unique_count,
                                    COUNT(*) - COUNT("{col}") as null_count
                                FROM {st.session_state.current_table}
                            """).fetchone()
                            
                            stats = {
                                'count': basic_stats[0],
                                'unique': basic_stats[1],
                                'missing': basic_stats[2],
                                'missing_pct': (basic_stats[2] / basic_stats[0] * 100) if basic_stats[0] > 0 else 0,
                                'data_type': col_type
                            }
                            
                            # String/Text specific analysis
                            if col_type.upper() in ('VARCHAR', 'TEXT', 'CHAR', 'STRING'):
                                # Length statistics
                                length_stats = st.session_state.duckdb_con.execute(f"""
                                    SELECT 
                                        AVG(LENGTH("{col}")) as avg_length,
                                        MIN(LENGTH("{col}")) as min_length,
                                        MAX(LENGTH("{col}")) as max_length,
                                        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH("{col}")) as median_length
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                """).fetchone()
                                
                                stats['text_stats'] = {
                                    'avg_length': length_stats[0],
                                    'min_length': length_stats[1],
                                    'max_length': length_stats[2],
                                    'median_length': length_stats[3]
                                }
                                
                                # Pattern analysis
                                pattern_stats = st.session_state.duckdb_con.execute(f"""
                                    SELECT 
                                        SUM(CASE WHEN "{col}" ~ '^[0-9]+$' THEN 1 ELSE 0 END) as numeric_only,
                                        SUM(CASE WHEN "{col}" ~ '^[A-Za-z]+$' THEN 1 ELSE 0 END) as alpha_only,
                                        SUM(CASE WHEN "{col}" ~ '^[A-Za-z0-9]+$' THEN 1 ELSE 0 END) as alphanumeric,
                                        SUM(CASE WHEN "{col}" ~ '[,]' THEN 1 ELSE 0 END) as contains_comma,
                                        SUM(CASE WHEN "{col}" ~ '[@]' THEN 1 ELSE 0 END) as contains_email
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                """).fetchone()
                                
                                stats['pattern_stats'] = {
                                    'numeric_only': pattern_stats[0],
                                    'alpha_only': pattern_stats[1],
                                    'alphanumeric': pattern_stats[2],
                                    'contains_comma': pattern_stats[3],
                                    'contains_email': pattern_stats[4]
                                }
                                
                                # Top values with frequencies
                                freq_stats = st.session_state.duckdb_con.execute(f"""
                                    SELECT 
                                        "{col}" as value,
                                        COUNT(*) as frequency,
                                        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                    GROUP BY "{col}"
                                    ORDER BY frequency DESC
                                    LIMIT 10
                                """).fetchall()
                                
                                stats['value_counts'] = [
                                    {
                                        'value': str(row[0]),
                                        'count': row[1],
                                        'percentage': row[2]
                                    }
                                    for row in freq_stats
                                ]
                                
                            # For numeric types
                            elif col_type.upper() in ('INTEGER', 'DOUBLE', 'DECIMAL', 'NUMERIC', 'FLOAT'):
                                num_stats = st.session_state.duckdb_con.execute(f"""
                                    SELECT 
                                        AVG(CAST("{col}" AS DOUBLE)) as mean,
                                        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY CAST("{col}" AS DOUBLE)) as median,
                                        STDDEV(CAST("{col}" AS DOUBLE)) as std,
                                        MIN(CAST("{col}" AS DOUBLE)) as min,
                                        MAX(CAST("{col}" AS DOUBLE)) as max,
                                        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY CAST("{col}" AS DOUBLE)) as q1,
                                        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY CAST("{col}" AS DOUBLE)) as q3
                                    FROM {st.session_state.current_table}
                                    WHERE "{col}" IS NOT NULL
                                """).fetchone()
                                
                                stats.update({
                                    'mean': num_stats[0],
                                    'median': num_stats[1],
                                    'std': num_stats[2],
                                    'min': num_stats[3],
                                    'max': num_stats[4],
                                    'percentiles': {
                                        '25%': num_stats[5],
                                        '75%': num_stats[6]
                                    }
                                })
                            
                            stats_results[col] = stats
                        
                        # Display results for each column
                        for col, stats in stats_results.items():
                            with st.expander(f"ðŸ“Š {col} Statistics ({stats['data_type']})", expanded=True):
                                # Basic stats for all types
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.write("Basic Statistics")
                                    st.write(f"- Total Count: {stats['count']:,}")
                                    st.write(f"- Unique Values: {stats['unique']:,}")
                                    st.write(f"- Missing Values: {stats['missing']:,}")
                                    st.write(f"- Missing %: {stats['missing_pct']:.2f}%")

                                with col2:
                                    if 'mean' in stats:  # Numeric column
                                        st.write("Numeric Statistics")
                                        st.write(f"- Mean: {stats['mean']:,.2f}")
                                        st.write(f"- Median: {stats['median']:,.2f}")
                                        st.write(f"- Std Dev: {stats['std']:,.2f}")
                                        st.write(f"- Min: {stats['min']:,.2f}")
                                        st.write(f"- Max: {stats['max']:,.2f}")
                                    
                                    elif 'text_stats' in stats:  # String/Text column
                                        st.write("Text Statistics")
                                        text_stats = stats['text_stats']
                                        st.write(f"- Average Length: {text_stats['avg_length']:.1f}")
                                        st.write(f"- Min Length: {text_stats['min_length']}")
                                        st.write(f"- Max Length: {text_stats['max_length']}")
                                        st.write(f"- Median Length: {text_stats['median_length']}")
                                
                                # Additional text analysis section
                                if 'pattern_stats' in stats:
                                    st.write("### Pattern Analysis")
                                    pattern_stats = stats['pattern_stats']
                                    total_values = stats['count'] - stats['missing']
                                    
                                    pattern_col1, pattern_col2 = st.columns(2)
                                    with pattern_col1:
                                        st.write("Content Type Distribution")
                                        st.write(f"- Numeric only: {pattern_stats['numeric_only']:,} ({pattern_stats['numeric_only']*100/total_values:.1f}%)")
                                        st.write(f"- Alphabetic only: {pattern_stats['alpha_only']:,} ({pattern_stats['alpha_only']*100/total_values:.1f}%)")
                                        st.write(f"- Alphanumeric: {pattern_stats['alphanumeric']:,} ({pattern_stats['alphanumeric']*100/total_values:.1f}%)")
                                    
                                    with pattern_col2:
                                        st.write("Special Patterns")
                                        st.write(f"- Contains comma: {pattern_stats['contains_comma']:,} ({pattern_stats['contains_comma']*100/total_values:.1f}%)")
                                        st.write(f"- Possible emails: {pattern_stats['contains_email']:,} ({pattern_stats['contains_email']*100/total_values:.1f}%)")
                                
                                # Value distribution
                                if 'value_counts' in stats:
                                    st.write("### Top Values Distribution")
                                    value_data = pd.DataFrame(stats['value_counts'])
                                    
                                    # Create bar chart
                                    fig = go.Figure(data=[
                                        go.Bar(
                                            x=value_data['value'],
                                            y=value_data['count'],
                                            text=value_data['percentage'].apply(lambda x: f'{x:.1f}%'),
                                            textposition='auto'
                                        )
                                    ])
                                    
                                    fig.update_layout(
                                        title=f'Top Values in {col}',
                                        xaxis_title='Values',
                                        yaxis_title='Frequency',
                                        height=400
                                    )
                                    
                                    st.plotly_chart(fig, use_container_width=True)
                                    
                                    # Show value counts table
                                    st.write("Top Values Table")
                                    value_display = pd.DataFrame(stats['value_counts'])
                                    value_display['percentage'] = value_display['percentage'].apply(lambda x: f'{x:.1f}%')
                                    st.dataframe(value_display)
                                
                                elif 'mean' in stats:  # Show distribution plot for numeric columns
                                    fig = visualizer.create_distribution_plot(
                                        st.session_state.current_table,
                                        col
                                    )
                                    if fig:
                                        st.plotly_chart(fig, use_container_width=True)

            # Correlation Analysis Section
            # Inside the Correlation Analysis section:
            if "Correlation Analysis" in analysis_type:
                st.subheader("Correlation Analysis")

                # Get all columns with their types
                all_cols_query = f"""
                    SELECT column_name, data_type
                    FROM information_schema.columns 
                    WHERE table_name = '{st.session_state.current_table}'
                """
                all_cols_result = st.session_state.duckdb_con.execute(all_cols_query).fetchall()
                all_cols_list = [col[0] for col in all_cols_result]
                cols_types = {col[0]: col[1] for col in all_cols_result}

                # Select columns for correlation
                corr_cols = st.multiselect(
                    "Select columns for correlation analysis:",
                    options=all_cols_list,
                    help="Select columns to analyze relationships (all data types supported)",
                    key="corr_cols_select_new"
                )

                if corr_cols and len(corr_cols) > 1:
                    analysis_method = st.selectbox(
                        "Select analysis method:",
                        [
                            'Auto (Based on data type)', 
                            'Numeric Correlation',
                            'Categorical Association',
                            'String Similarity',
                            'Mixed Analysis'
                        ],
                        help="""
                        - Auto: Automatically chooses appropriate method based on data type
                        - Numeric Correlation: Pearson correlation for numeric data
                        - Categorical Association: Cramer's V for categorical data
                        - String Similarity: Text-based similarity measures
                        - Mixed Analysis: Combines different methods based on data types
                        """,
                        key="analysis_method"
                    )

                    # Initialize similarity_metric with a default value
                    similarity_metric = None
                    
                    # Only show similarity metric selection when String Similarity is selected
                    if analysis_method == 'String Similarity':
                        similarity_metric = st.selectbox(
                            "Select string similarity metric:",
                            ['Overlap Coefficient', 'Length Ratio', 'Pattern Matching'],
                            help="Choose how to measure similarity between string columns",
                            key="similarity_metric"
                        )

                    if st.button("Generate Correlation Analysis", key="gen_corr_new"):
                        with st.spinner("Calculating correlations..."):
                            try:
                                # Initialize correlation matrix
                                corr_df = pd.DataFrame(index=corr_cols, columns=corr_cols)
                                
                                # Process each pair of columns
                                for i, col1 in enumerate(corr_cols):
                                    for j, col2 in enumerate(corr_cols):
                                        if i <= j:  # Process upper triangle (including diagonal)
                                            if i == j:
                                                corr_df.iloc[i, j] = 1.0
                                                continue
                                                
                                            type1 = cols_types[col1].upper()
                                            type2 = cols_types[col2].upper()
                                            
                                            # Decide analysis method
                                            method = analysis_method
                                            if method == 'Auto (Based on data type)':
                                                if all(t in ('INTEGER', 'DOUBLE', 'DECIMAL', 'NUMERIC', 'FLOAT') 
                                                      for t in (type1, type2)):
                                                    method = 'Numeric Correlation'
                                                elif any(t in ('VARCHAR', 'TEXT', 'CHAR', 'STRING') 
                                                        for t in (type1, type2)):
                                                    method = 'String Similarity'
                                                    if similarity_metric is None:
                                                        similarity_metric = 'Overlap Coefficient'
                                                else:
                                                    method = 'Categorical Association'

                                            # Calculate correlation based on method
                                            if method == 'Numeric Correlation':
                                                # Convert to numeric and calculate Pearson correlation
                                                corr_val = st.session_state.duckdb_con.execute(f"""
                                                    SELECT CORR(
                                                        CAST("{col1}" AS DOUBLE),
                                                        CAST("{col2}" AS DOUBLE)
                                                    )
                                                    FROM {st.session_state.current_table}
                                                    WHERE "{col1}" IS NOT NULL AND "{col2}" IS NOT NULL
                                                """).fetchone()[0]
                                                
                                            elif method == 'Categorical Association':
                                                # Calculate Cramer's V
                                                contingency = st.session_state.duckdb_con.execute(f"""
                                                    SELECT "{col1}", "{col2}", COUNT(*)
                                                    FROM {st.session_state.current_table}
                                                    GROUP BY "{col1}", "{col2}"
                                                """).df().pivot(index=col1, columns=col2, values='count')
                                                
                                                chi2 = stats.chi2_contingency(contingency.fillna(0))[0]
                                                n = contingency.sum().sum()
                                                min_dim = min(contingency.shape) - 1
                                                corr_val = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0
                                                
                                            elif method == 'String Similarity':
                                                # Use default similarity metric if none selected
                                                current_metric = similarity_metric or 'Overlap Coefficient'
                                                
                                                if current_metric == 'Overlap Coefficient':
                                                    # Calculate overlap between unique tokens
                                                    query = f"""
                                                    WITH Tokens AS (
                                                        SELECT DISTINCT 
                                                            "{col1}" as val1,
                                                            "{col2}" as val2
                                                        FROM {st.session_state.current_table}
                                                        WHERE "{col1}" IS NOT NULL 
                                                        AND "{col2}" IS NOT NULL
                                                    )
                                                    SELECT 
                                                        COUNT(DISTINCT val1) as unique1,
                                                        COUNT(DISTINCT val2) as unique2,
                                                        COUNT(DISTINCT CASE WHEN val1 = val2 THEN val1 END) as overlap
                                                    FROM Tokens
                                                    """
                                                    result = st.session_state.duckdb_con.execute(query).fetchone()
                                                    min_size = min(result[0], result[1])
                                                    corr_val = result[2] / min_size if min_size > 0 else 0
                                                    
                                                elif current_metric == 'Length Ratio':
                                                    # Compare length distributions
                                                    query = f"""
                                                    SELECT CORR(
                                                        LENGTH(CAST("{col1}" AS VARCHAR)),
                                                        LENGTH(CAST("{col2}" AS VARCHAR))
                                                    )
                                                    FROM {st.session_state.current_table}
                                                    WHERE "{col1}" IS NOT NULL AND "{col2}" IS NOT NULL
                                                    """
                                                    corr_val = st.session_state.duckdb_con.execute(query).fetchone()[0]
                                                    
                                                else:  # Pattern Matching
                                                    # Compare character pattern similarities
                                                    query = f"""
                                                    WITH Patterns AS (
                                                        SELECT 
                                                            CASE 
                                                                WHEN "{col1}" ~ '^[0-9]+$' THEN 'numeric'
                                                                WHEN "{col1}" ~ '^[A-Za-z]+$' THEN 'alpha'
                                                                ELSE 'mixed'
                                                            END as pattern1,
                                                            CASE 
                                                                WHEN "{col2}" ~ '^[0-9]+$' THEN 'numeric'
                                                                WHEN "{col2}" ~ '^[A-Za-z]+$' THEN 'alpha'
                                                                ELSE 'mixed'
                                                            END as pattern2
                                                        FROM {st.session_state.current_table}
                                                        WHERE "{col1}" IS NOT NULL AND "{col2}" IS NOT NULL
                                                    )
                                                    SELECT 
                                                        COUNT(*) FILTER (WHERE pattern1 = pattern2)::FLOAT / 
                                                        NULLIF(COUNT(*), 0)
                                                    FROM Patterns
                                                    """
                                                    corr_val = st.session_state.duckdb_con.execute(query).fetchone()[0]
                                                    
                                            else:  # Mixed Analysis
                                                # Attempt numeric correlation first
                                                try:
                                                    corr_val = st.session_state.duckdb_con.execute(f"""
                                                        SELECT CORR(
                                                            TRY_CAST("{col1}" AS DOUBLE),
                                                            TRY_CAST("{col2}" AS DOUBLE)
                                                        )
                                                        FROM {st.session_state.current_table}
                                                        WHERE "{col1}" IS NOT NULL AND "{col2}" IS NOT NULL
                                                    """).fetchone()[0]
                                                except:
                                                    # Fallback to categorical association
                                                    contingency = st.session_state.duckdb_con.execute(f"""
                                                        SELECT "{col1}", "{col2}", COUNT(*)
                                                        FROM {st.session_state.current_table}
                                                        GROUP BY "{col1}", "{col2}"
                                                    """).df().pivot(index=col1, columns=col2, values='count')
                                                    
                                                    chi2 = stats.chi2_contingency(contingency.fillna(0))[0]
                                                    n = contingency.sum().sum()
                                                    min_dim = min(contingency.shape) - 1
                                                    corr_val = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0

                                            # Store correlation value
                                            corr_df.iloc[i, j] = corr_val if corr_val is not None else 0
                                            corr_df.iloc[j, i] = corr_df.iloc[i, j]  # Mirror value

                                # Create visualization
                                corr_fig = go.Figure(data=go.Heatmap(
                                    z=corr_df.values,
                                    x=corr_df.columns,
                                    y=corr_df.index,
                                    text=np.round(corr_df.values, 3),
                                    texttemplate="%{text}",
                                    textfont={"size": 10},
                                    colorscale='RdBu_r',
                                    zmid=0.5 if analysis_method in ['Categorical Association', 'String Similarity'] else 0
                                ))

                                corr_fig.update_layout(
                                    title=f'Correlation/Association Matrix ({analysis_method})',
                                    title_x=0.5,
                                    width=800,
                                    height=800
                                )

                                # Display results
                                st.write(f"### {analysis_method} Matrix")
                                st.write("Note: Values range from:")
                                if analysis_method == 'Numeric Correlation':
                                    st.write("- -1 to 1 (negative to positive correlation)")
                                elif analysis_method in ['Categorical Association', 'String Similarity']:
                                    st.write("- 0 to 1 (no association to perfect association)")
                                
                                st.dataframe(
                                    corr_df.style
                                    .background_gradient(
                                        cmap='RdBu_r',
                                        vmin=-1 if analysis_method == 'Numeric Correlation' else 0,
                                        vmax=1
                                    )
                                    .format("{:.3f}")
                                )

                                st.write("### Visualization")
                                st.plotly_chart(corr_fig, use_container_width=True)

                                # Add insights section
                                st.write("### Key Insights")
                                insights = []
                                
                                # Different thresholds based on analysis method
                                if analysis_method == 'Numeric Correlation':
                                    strong_threshold = 0.7
                                    moderate_threshold = 0.4
                                else:
                                    strong_threshold = 0.6
                                    moderate_threshold = 0.3

                                for i in range(len(corr_cols)):
                                    for j in range(i + 1, len(corr_cols)):
                                        corr_val = abs(corr_df.iloc[i, j])
                                        if corr_val >= strong_threshold:
                                            insights.append({
                                                'type': 'Strong',
                                                'col1': corr_cols[i],
                                                'col2': corr_cols[j],
                                                'value': corr_df.iloc[i, j]
                                            })
                                        elif corr_val >= moderate_threshold:
                                            insights.append({
                                                'type': 'Moderate',
                                                'col1': corr_cols[i],
                                                'col2': corr_cols[j],
                                                'value': corr_df.iloc[i, j]
                                            })

                                if insights:
                                    for strength in ['Strong', 'Moderate']:
                                        strength_insights = [i for i in insights if i['type'] == strength]
                                        if strength_insights:
                                            st.write(f"{strength} Relationships:")
                                            for insight in strength_insights:
                                                st.write(
                                                    f"- {insight['col1']} vs {insight['col2']}: "
                                                    f"{insight['value']:.3f}"
                                                )
                                else:
                                    st.write("No significant relationships found between the selected columns.")

                            except Exception as e:
                                st.error(f"Error in correlation analysis: {str(e)}")
                                st.info("Try selecting different columns or a different analysis method.")

    # Tab 3: Visualization
    with tab3:
        if st.session_state.current_table:
            st.header("Visualization Options")

            # Time Series Analysis
            st.subheader("ðŸ•’ Time Series Analysis")
            
            if datetime_cols:
                time_col = st.selectbox(
                    "Select time column:",
                    datetime_cols,
                    key="time_col_select"
                )
                
                if numeric_cols:
                    ts_metrics = st.multiselect(
                        "Select metrics to plot:",
                        numeric_cols,
                        key="ts_metrics_select"
                    )
                    
                    interval = st.selectbox(
                        "Select time interval:",
                        ['day', 'week', 'month', 'quarter', 'year'],
                        key="interval_select"
                    )
                    
                    if ts_metrics and st.button("Generate Time Series Plot", key="gen_ts_plot"):
                        with st.spinner("Creating time series visualization..."):
                            fig = visualizer.create_time_series_chart(
                                st.session_state.current_table,
                                time_col,
                                ts_metrics,
                                interval=interval
                            )
                            
                            if fig:
                                st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("No numeric columns available for time series analysis.")
            else:
                st.warning("No datetime columns found for time series analysis.")

            # Distribution Analysis
            st.subheader("ðŸ“Š Distribution Analysis")
            if numeric_cols:
                dist_col = st.selectbox(
                    "Select column for distribution analysis:",
                    numeric_cols,
                    key="dist_col_select"
                )
                
                plot_type = st.selectbox(
                    "Select plot type:",
                    ['histogram', 'box'],
                    key="dist_plot_type"
                )
                
                if st.button("Generate Distribution Plot", key="gen_dist_plot"):
                    with st.spinner("Creating distribution plot..."):
                        fig = visualizer.create_distribution_plot(
                            st.session_state.current_table,
                            dist_col,
                            plot_type=plot_type
                        )
                        
                        if fig:
                            st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("No numeric columns available for distribution analysis.")

            # Categorical Analysis
            if categorical_cols:
                st.subheader("ðŸ“Š Categorical Analysis")
                cat_col = st.selectbox(
                    "Select categorical column:",
                    categorical_cols,
                    key="cat_col_select"
                )
                
                if numeric_cols:
                    value_col = st.selectbox(
                        "Select value column (optional):",
                        ['Count'] + numeric_cols,
                        key="value_col_select"
                    )
                    
                    agg_func = st.selectbox(
                        "Select aggregation function:",
                        ['COUNT', 'SUM', 'AVG', 'MAX', 'MIN'] if value_col != 'Count' else ['COUNT'],
                        key="cat_agg_func"
                    )
                    
                    limit = st.slider(
                        "Number of categories to show:",
                        min_value=5,
                        max_value=50,
                        value=10,
                        key="cat_limit"
                    )
                    
                    if st.button("Generate Category Analysis", key="gen_cat_analysis"):
                        with st.spinner("Analyzing categories..."):
                            if value_col == 'Count':
                                query = f"""
                                SELECT "{cat_col}", COUNT(*) as count
                                FROM {st.session_state.current_table}
                                GROUP BY "{cat_col}"
                                ORDER BY count DESC
                                LIMIT {limit}
                                """
                            else:
                                query = f"""
                                SELECT 
                                    "{cat_col}", 
                                    {agg_func}("{value_col}") as value
                                FROM {st.session_state.current_table}
                                GROUP BY "{cat_col}"
                                ORDER BY value DESC
                                LIMIT {limit}
                                """
                            
                            result = st.session_state.duckdb_con.execute(query).df()
                            
                            fig = go.Figure(data=[
                                go.Bar(
                                    x=result[cat_col],
                                    y=result['count' if value_col == 'Count' else 'value'],
                                    text=result['count' if value_col == 'Count' else 'value'].round(2),
                                    textposition='auto',
                                )
                            ])
                            
                            fig.update_layout(
                                title=f'{cat_col} Distribution',
                                xaxis_title=cat_col,
                                yaxis_title='Count' if value_col == 'Count' else f'{agg_func} of {value_col}',
                                height=500
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No categorical columns available for analysis.")

    # Tab 4: Export
    with tab4:
        if st.session_state.current_table:
            st.header("Export Options")

            export_format = st.selectbox(
                "Select Export Format:",
                ["Excel Report", "CSV Data", "HTML Report"],
                key="export_format"
            )

            if export_format == "Excel Report":
                st.write("Excel report will include:")
                st.write("- Processed data")
                st.write("- Pivot tables (if created)")
                st.write("- Statistical analysis (if performed)")
                st.write("- Data cleaning report")

                if st.button("Generate Excel Report", key="gen_excel"):
                    with st.spinner("Generating Excel report..."):
                        excel_data = export_manager.to_excel(
                            st.session_state.current_table,
                            st.session_state.pivot_results,
                            stats_results if 'stats_results' in locals() else None,
                            st.session_state.cleaning_report
                        )

                        if excel_data:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                label="ðŸ“¥ Download Excel Report",
                                data=excel_data,
                                file_name=f"analysis_report_{timestamp}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key="download_excel"
                            )

            elif export_format == "CSV Data":
                st.write("Export options:")
                include_index = st.checkbox("Include Row Index", value=False, key="include_index")
                compression = st.selectbox(
                    "Compression:",
                    [None, 'zip', 'gzip'],
                    key="compression"
                )

                if st.button("Export to CSV", key="export_csv"):
                    with st.spinner("Preparing CSV export..."):
                        csv_data, extension = export_manager.to_csv(
                            st.session_state.current_table,
                            include_index=include_index,
                            compression=compression
                        )
                        
                        if csv_data:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                label="ðŸ“¥ Download CSV",
                                data=csv_data,
                                file_name=f"data_{timestamp}.{extension}",
                                mime=("application/zip" if extension == 'zip' else
                                     "application/gzip" if extension == 'gz' else
                                     "text/csv"),
                                key="download_csv"
                            )

            elif export_format == "HTML Report":
                st.write("HTML report will include:")
                st.write("- Dataset overview")
                st.write("- Statistical summaries")
                st.write("- Sample data")

                include_stats = st.checkbox("Include Statistics", value=True, key="include_stats")
                include_viz = st.checkbox("Include Visualizations", value=True, key="include_viz")

                if st.button("Generate HTML Report", key="gen_html"):
                    with st.spinner("Generating HTML report..."):
                        html_content = export_manager.to_html(
                            st.session_state.current_table,
                            include_stats=include_stats,
                            include_viz=include_viz
                        )
                        
                        if html_content:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                label="ðŸ“¥ Download HTML Report",
                                data=html_content,
                                file_name=f"analysis_report_{timestamp}.html",
                                mime="text/html",
                                key="download_html"
                            )

if __name__ == "__main__":
    main()
