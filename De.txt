#!/usr/bin/env python3
"""
Multi-Agent Legal Document Rule Extraction System using LangGraph
Dedicated agents with Chain of Thought and ReAct prompting (Reasoning, Action, Observation)
Uses geography.json for all country data - no hardcoding
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Literal, TypedDict
from datetime import datetime
import re
import uuid

# Core libraries
import pandas as pd
import PyPDF2
from pydantic import BaseModel, Field, ValidationError, model_validator
from pydantic_core import from_json

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

# Global Configuration
API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
BASE_URL = "https://api.openai.com/v1"
MODEL_NAME = "gpt-4o-mini"
CHUNK_SIZE = 4000
OVERLAP_SIZE = 200

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Pydantic v2 Models with exact required structure
class RuleCondition(BaseModel):
    """Rule condition with required structure"""
    condition_id: str = Field(..., description="Unique condition identifier")
    condition_definition: str = Field(..., description="Clear condition definition in simple English")
    fact: str = Field(..., description="The fact to evaluate")
    operator: str = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")

class ExtractedRule(BaseModel):
    """Extracted rule with exact required structure"""
    rule_id: str = Field(..., description="Unique rule identifier")
    rule_definition: str = Field(..., description="Complete rule definition in simple English")
    applicable_countries: List[str] = Field(..., description="Country ISO codes from metadata config")
    adequacy_countries: List[str] = Field(default_factory=list, description="Actual countries with adequacy decisions from PDF")
    conditions: List[RuleCondition] = Field(..., description="List of rule conditions")
    data_category: str = Field(..., description="Primary data category")
    action: str = Field(..., description="Required action in simple English")
    reference: str = Field(..., description="Level and article/text reference")

class MetadataConfig(BaseModel):
    """Configuration model"""
    pdf_path: str = Field(..., description="Path to PDF file")
    applicable_countries: List[str] = Field(..., description="ISO country codes where rules apply")
    document_type: str = Field(default="regulation", description="Type of document")

# Multi-Agent State Management
class MultiAgentState(TypedDict):
    """Shared state between all agents"""
    # Input data
    document_text: str
    metadata_config: dict
    geography_data: dict
    
    # Processing stages
    parsed_sections: dict
    identified_countries: dict
    extracted_rules: list
    validated_rules: list
    
    # Agent reasoning traces
    agent1_reasoning: list  # Document Parser Agent reasoning
    agent2_reasoning: list  # Geography Agent reasoning  
    agent3_reasoning: list  # Rule Extraction Agent reasoning
    agent4_reasoning: list  # Validation Agent reasoning
    
    # Current processing stage
    current_agent: str
    processing_complete: bool

# Geography Handler - NO HARDCODING
class GeographyHandler:
    """Handles all geography data from geography.json - no hardcoding"""
    
    def __init__(self, geography_file: str):
        self.geography_data = self._load_geography_data(geography_file)
        self.all_countries = self._extract_all_countries()
        self.adequacy_countries = self._find_adequacy_countries_from_data()
    
    def _load_geography_data(self, file_path: str) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load geography data: {e}")
            return {}
    
    def _extract_all_countries(self) -> Dict[str, str]:
        """Extract all countries from geography.json - NO HARDCODING"""
        countries = {}
        
        # Process all regions in geography data
        for region_key, region_data in self.geography_data.items():
            if isinstance(region_data, dict):
                # Direct countries list
                if 'countries' in region_data:
                    for country in region_data['countries']:
                        countries[country['iso2']] = country['name']
                
                # Nested continent structure
                if region_key == 'By_Continent':
                    for continent, continent_data in region_data.items():
                        if isinstance(continent_data, dict) and 'countries' in continent_data:
                            for country in continent_data['countries']:
                                countries[country['iso2']] = country['name']
        
        return countries
    
    def _find_adequacy_countries_from_data(self) -> Dict[str, str]:
        """Find adequacy countries from geography data context - NO HARDCODING"""
        # This would be enhanced to find adequacy countries from the geography data
        # For now, we'll identify them during document processing
        return {}
    
    def get_country_name(self, iso_code: str) -> Optional[str]:
        """Get country name from ISO code"""
        return self.all_countries.get(iso_code.upper())
    
    def get_country_iso(self, country_name: str) -> Optional[str]:
        """Get ISO code from country name"""
        country_name_lower = country_name.lower()
        for iso, name in self.all_countries.items():
            if name.lower() == country_name_lower:
                return iso
        return None
    
    def is_valid_country(self, iso_code: str) -> bool:
        """Check if ISO code is valid country"""
        return iso_code.upper() in self.all_countries
    
    def find_countries_in_text(self, text: str) -> List[str]:
        """Find country mentions in text using geography data"""
        found_countries = []
        text_lower = text.lower()
        
        # Search for country names and ISO codes from geography data
        for iso, name in self.all_countries.items():
            if name.lower() in text_lower or iso.lower() in text_lower:
                found_countries.append(iso)
        
        return list(set(found_countries))

# Safe JSON parsing utilities
def safe_json_parse(json_str: str, default: Any = None) -> Any:
    """Safely parse JSON with Pydantic v2"""
    try:
        return from_json(json_str.encode(), allow_partial=False)
    except Exception as e:
        logger.warning(f"Pydantic JSON parsing failed: {e}")
        try:
            import json as std_json
            return std_json.loads(json_str)
        except Exception as e2:
            logger.error(f"Standard JSON parsing failed: {e2}")
            return default

def extract_json_from_text(text: str) -> Optional[Dict]:
    """Extract JSON from text safely"""
    json_patterns = [
        r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
        r'\{.*?\}',
    ]
    
    for pattern in json_patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            result = safe_json_parse(match)
            if result and isinstance(result, dict):
                return result
    
    return None

# PDF Processing
class PDFProcessor:
    """PDF text extraction"""
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str) -> str:
        """Extract complete text from PDF"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text += f"\n[PAGE {page_num + 1}]\n{page_text}\n"
                return text
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            return ""

# AGENT 1: Document Parser Agent with Chain of Thought and ReAct
class DocumentParserAgent:
    """Agent 1: Document parsing with ReAct prompting (Reasoning, Action, Observation)"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "DocumentParserAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process document with Chain of Thought and ReAct prompting"""
        
        logger.info(f"🤖 {self.name}: Starting document parsing with ReAct prompting")
        
        system_prompt = """You are a specialized Document Parser Agent using ReAct prompting methodology.
        
Your task: Parse legal documents and extract Level-1, Level-2, Level-3 sections using Chain of Thought reasoning.

ReAct Framework:
1. REASONING: Analyze what you need to do and why
2. ACTION: Take specific action to parse document sections  
3. OBSERVATION: Observe results and determine next steps

Chain of Thought Process:
1. First, reason through the document structure
2. Identify patterns for Level-1 (regulations), Level-2 (guidance), Level-3 (supporting info)
3. Extract each section completely without truncation
4. Validate extraction quality

Format your response as:
REASONING: [Your reasoning about the document structure and parsing approach]
ACTION: [The specific parsing action you're taking]
OBSERVATION: [What you observed from the parsing results]
CONCLUSION: [Final parsed sections in JSON format]"""

        user_prompt = f"""Parse this legal document using ReAct methodology:

Document Text (first 3000 chars):
{state['document_text'][:3000]}...

Document Type: {state['metadata_config']['document_type']}

Use ReAct prompting:
1. REASONING: Analyze the document structure and identify section patterns
2. ACTION: Extract Level-1-Regulation-*, Level-2-Regulator-Guidance, Level-3-Supporting-Information sections
3. OBSERVATION: Evaluate extraction completeness and quality
4. CONCLUSION: Provide parsed sections in JSON format

Focus on complete extraction without truncation."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract parsed sections
            parsed_sections = self._extract_sections_from_response(response_text, state['document_text'])
            
            # Update state
            state['parsed_sections'] = parsed_sections
            state['agent1_reasoning'] = reasoning_trace
            state['current_agent'] = 'GeographyAgent'
            
            logger.info(f"✅ {self.name}: Successfully parsed {len(parsed_sections)} sections")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent1_reasoning'] = [f"ERROR: {e}"]
            state['parsed_sections'] = {"general": state['document_text'][:2000]}
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract ReAct reasoning trace from response"""
        trace = []
        
        # Extract REASONING, ACTION, OBSERVATION sections
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|CONCLUSION:|$)', 'ACTION'), 
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_sections_from_response(self, response_text: str, original_text: str) -> Dict[str, str]:
        """Extract sections from agent response"""
        sections = {}
        
        # Try to extract JSON from response
        json_data = extract_json_from_text(response_text)
        if json_data:
            return json_data
        
        # Fallback: pattern matching on original text
        level_patterns = {
            "Level-1": r"Level-1-Regulation-([^:]+):(.*?)(?=Level-[123]|$)",
            "Level-2": r"Level-2-Regulator-Guidance:(.*?)(?=Level-[123]|$)",
            "Level-3": r"Level-3-Supporting-Information:(.*?)(?=Level-[123]|$)"
        }
        
        for level, pattern in level_patterns.items():
            matches = re.finditer(pattern, original_text, re.DOTALL | re.IGNORECASE)
            for i, match in enumerate(matches):
                if level == "Level-1":
                    reg_name = match.group(1).strip()
                    content = match.group(2).strip()
                    sections[f"{level}-{reg_name}"] = content
                else:
                    content = match.group(1).strip()
                    sections[level] = content
        
        # If no sections found, create general section
        if not sections:
            sections["General"] = original_text[:2000]
        
        return sections

# AGENT 2: Geography Agent with Chain of Thought and ReAct
class GeographyAgent:
    """Agent 2: Country identification using geography.json with ReAct prompting"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "GeographyAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process geography identification with ReAct prompting"""
        
        logger.info(f"🌍 {self.name}: Starting country identification with ReAct prompting")
        
        # Prepare country data from geography.json for LLM
        country_list = []
        for iso, name in self.geography_handler.all_countries.items():
            country_list.append(f"{iso}: {name}")
        
        country_data_text = "\n".join(country_list[:50])  # First 50 countries for prompt
        
        system_prompt = f"""You are a specialized Geography Agent using ReAct prompting methodology.

Your task: Identify countries mentioned in legal documents using ONLY the provided geography data.

Available Countries from Geography Data:
{country_data_text}
... and more (total: {len(self.geography_handler.all_countries)} countries)

ReAct Framework:
1. REASONING: Analyze document text for country mentions and adequacy context
2. ACTION: Identify specific countries using geography data  
3. OBSERVATION: Validate found countries against geography data
4. CONCLUSION: Provide verified country lists

Chain of Thought Process:
1. Scan document for country names and ISO codes
2. Match findings against geography data  
3. Identify adequacy countries from context
4. Verify all countries exist in geography data
5. NO HARDCODING - use only geography data

Format your response with ReAct structure."""

        # Combine all document sections for country analysis
        all_text = ""
        for section_name, content in state['parsed_sections'].items():
            all_text += f"\n{section_name}: {content}\n"
        
        user_prompt = f"""Identify countries using ReAct methodology:

Document Sections:
{all_text[:4000]}...

Applicable Countries (from config): {state['metadata_config']['applicable_countries']}

Use ReAct prompting:
1. REASONING: Analyze text for country mentions and adequacy context patterns
2. ACTION: Match country mentions against geography data provided
3. OBSERVATION: Verify identified countries exist in geography data  
4. CONCLUSION: Provide verified country identification results

Identify:
- All countries mentioned in document
- Countries with "adequacy decision" or "adequate protection" context
- Verify all against geography data (NO HARDCODING)

Return JSON format:
{{
  "mentioned_countries": ["ISO codes found"],
  "adequacy_countries": ["ISO codes with adequacy context"],
  "verification_status": "all verified against geography data"
}}"""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract country identification results
            country_results = self._extract_country_results(response_text, all_text)
            
            # Update state
            state['identified_countries'] = country_results
            state['agent2_reasoning'] = reasoning_trace
            state['current_agent'] = 'RuleExtractionAgent'
            
            logger.info(f"✅ {self.name}: Identified {len(country_results.get('mentioned_countries', []))} countries")
            logger.info(f"   📍 Adequacy countries: {country_results.get('adequacy_countries', [])}")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent2_reasoning'] = [f"ERROR: {e}"]
            state['identified_countries'] = {
                "mentioned_countries": [],
                "adequacy_countries": [],
                "verification_status": "failed"
            }
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract ReAct reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_country_results(self, response_text: str, document_text: str) -> Dict[str, Any]:
        """Extract country identification results"""
        # Try JSON extraction first
        json_data = extract_json_from_text(response_text)
        if json_data:
            # Verify countries against geography data
            verified_results = {}
            
            mentioned = json_data.get('mentioned_countries', [])
            adequacy = json_data.get('adequacy_countries', [])
            
            # Verify mentioned countries
            verified_mentioned = []
            for country in mentioned:
                if self.geography_handler.is_valid_country(country):
                    verified_mentioned.append(country.upper())
            
            # Verify adequacy countries
            verified_adequacy = []
            for country in adequacy:
                if self.geography_handler.is_valid_country(country):
                    verified_adequacy.append(country.upper())
            
            verified_results = {
                "mentioned_countries": verified_mentioned,
                "adequacy_countries": verified_adequacy,
                "verification_status": "verified against geography data"
            }
            
            return verified_results
        
        # Fallback: direct text analysis using geography data
        mentioned_countries = self.geography_handler.find_countries_in_text(document_text)
        
        # Find adequacy countries from context
        adequacy_countries = []
        text_lower = document_text.lower()
        
        for iso in mentioned_countries:
            country_name = self.geography_handler.get_country_name(iso)
            if country_name:
                name_lower = country_name.lower()
                # Look for adequacy context
                adequacy_patterns = [
                    rf'{name_lower}[^.]*?adequacy',
                    rf'adequacy[^.]*?{name_lower}',
                    rf'{iso.lower()}[^.]*?adequacy',
                    rf'adequacy[^.]*?{iso.lower()}'
                ]
                
                for pattern in adequacy_patterns:
                    if re.search(pattern, text_lower):
                        adequacy_countries.append(iso)
                        break
        
        return {
            "mentioned_countries": mentioned_countries,
            "adequacy_countries": list(set(adequacy_countries)),
            "verification_status": "verified against geography data"
        }

# AGENT 3: Rule Extraction Agent with Chain of Thought and ReAct
class RuleExtractionAgent:
    """Agent 3: Rule extraction with ReAct prompting"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "RuleExtractionAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Extract rules with Chain of Thought and ReAct prompting"""
        
        logger.info(f"⚖️ {self.name}: Starting rule extraction with ReAct prompting")
        
        system_prompt = """You are a specialized Rule Extraction Agent using ReAct prompting methodology.

Your task: Extract structured rules from legal document sections using Chain of Thought reasoning.

Required Rule Structure:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule in simple English",
  "conditions": [
    {
      "condition_id": "unique_condition_id", 
      "condition_definition": "condition in simple English",
      "fact": "data.category or user.role etc",
      "operator": "equal/in/greaterThan etc",
      "value": "comparison_value"
    }
  ],
  "data_category": "Personal Data/Special Category Data etc",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework:
1. REASONING: Analyze sections for legal obligations and requirements
2. ACTION: Extract specific rules and conditions
3. OBSERVATION: Evaluate rule completeness and structure
4. CONCLUSION: Provide structured rules in required format

Chain of Thought Process:
1. Identify legal obligations (must/shall/required)
2. Extract conditions triggering obligations
3. Identify data categories involved
4. Determine required actions  
5. Create proper references
6. Structure in required format"""

        # Prepare sections for rule extraction
        sections_text = ""
        for section_name, content in state['parsed_sections'].items():
            sections_text += f"\n\nSECTION: {section_name}\nCONTENT: {content}\n"
        
        user_prompt = f"""Extract structured rules using ReAct methodology:

Document Sections:
{sections_text[:5000]}...

Available Data:
- Applicable Countries: {state['metadata_config']['applicable_countries']}
- Identified Adequacy Countries: {state['identified_countries'].get('adequacy_countries', [])}

Use ReAct prompting:
1. REASONING: Analyze sections for legal obligations, conditions, and requirements
2. ACTION: Extract rules with proper structure (rule_id, rule_definition, conditions, data_category, action, reference)
3. OBSERVATION: Verify rule completeness and logical structure
4. CONCLUSION: Provide extracted rules in required JSON format

Focus on:
- Access rights and data subject requests
- Controller and processor obligations  
- Consent and data processing requirements
- Compliance timeframes and actions

Return JSON array of rules with complete structure."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract rules
            extracted_rules = self._extract_rules_from_response(response_text, state)
            
            # Update state
            state['extracted_rules'] = extracted_rules
            state['agent3_reasoning'] = reasoning_trace
            state['current_agent'] = 'ValidationAgent'
            
            logger.info(f"✅ {self.name}: Extracted {len(extracted_rules)} rules")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent3_reasoning'] = [f"ERROR: {e}"]
            state['extracted_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract ReAct reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|CONCLUSION:|$)', 'OBSERVATION'),  
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_rules_from_response(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Extract rules from agent response"""
        rules = []
        
        # Try JSON extraction
        json_data = extract_json_from_text(response_text)
        if json_data:
            if isinstance(json_data, list):
                rules = json_data
            elif isinstance(json_data, dict):
                rules = [json_data]
        
        # If no JSON rules found, create rule from text analysis
        if not rules:
            rules = self._create_rules_from_text_analysis(state)
        
        return rules
    
    def _create_rules_from_text_analysis(self, state: MultiAgentState) -> List[Dict]:
        """Create rules from text analysis when JSON extraction fails"""
        rules = []
        
        # Analyze each section for obligations
        for section_name, content in state['parsed_sections'].items():
            sentences = re.split(r'[.!?]+', content)
            
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if len(sentence) < 20:
                    continue
                
                # Look for obligation patterns
                if any(word in sentence.lower() for word in ['must', 'shall', 'required', 'obligated']):
                    
                    rule = {
                        "rule_id": f"rule_{section_name.lower().replace('-', '_').replace(' ', '_')}_{i}_{uuid.uuid4().hex[:8]}",
                        "rule_definition": sentence,
                        "conditions": [{
                            "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                            "condition_definition": f"When processing data in context of {section_name}",
                            "fact": "data.category",
                            "operator": "equal",
                            "value": "Personal Data"
                        }],
                        "data_category": "Personal Data",
                        "action": "Must ensure compliance with stated requirements",
                        "reference": f"{section_name} - {sentence[:100]}..."
                    }
                    
                    rules.append(rule)
                    
                    # Limit to avoid too many rules
                    if len(rules) >= 10:
                        break
            
            if len(rules) >= 10:
                break
        
        return rules

# AGENT 4: Validation Agent with Chain of Thought and ReAct
class ValidationAgent:
    """Agent 4: Rule validation and final structuring with ReAct prompting"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "ValidationAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Validate and structure rules with ReAct prompting"""
        
        logger.info(f"✅ {self.name}: Starting rule validation with ReAct prompting")
        
        system_prompt = """You are a specialized Validation Agent using ReAct prompting methodology.

Your task: Validate and structure extracted rules into the exact required format using Chain of Thought reasoning.

EXACT Required Format:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule definition in simple English", 
  "applicable_countries": ["ISO codes from metadata config"],
  "adequacy_countries": ["verified ISO codes from document"],
  "conditions": [
    {
      "condition_id": "unique_condition_id",
      "condition_definition": "clear condition in simple English", 
      "fact": "property to evaluate",
      "operator": "comparison operator",
      "value": "value to compare"
    }
  ],
  "data_category": "primary data category",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework:
1. REASONING: Analyze extracted rules for completeness and structure
2. ACTION: Validate and restructure rules to exact format
3. OBSERVATION: Check validation results and format compliance  
4. CONCLUSION: Provide validated rules in exact required structure

Chain of Thought Process:
1. Validate rule structure completeness
2. Ensure applicable_countries from config
3. Verify adequacy_countries are actual countries
4. Structure conditions properly
5. Create clear references
6. Apply final format validation"""

        user_prompt = f"""Validate and structure rules using ReAct methodology:

Extracted Rules:
{json.dumps(state['extracted_rules'][:3], indent=2)}
... (total: {len(state['extracted_rules'])} rules)

Available Data:
- Applicable Countries (from config): {state['metadata_config']['applicable_countries']}
- Adequacy Countries (verified): {state['identified_countries'].get('adequacy_countries', [])}

Use ReAct prompting:
1. REASONING: Analyze rule structure and required format compliance
2. ACTION: Validate and restructure each rule to exact format requirements  
3. OBSERVATION: Verify format compliance and data accuracy
4. CONCLUSION: Provide validated rules in exact required JSON format

Requirements:
- applicable_countries MUST come from metadata config
- adequacy_countries MUST be verified actual countries (not regions)
- All conditions need proper structure with condition_id, condition_definition, fact, operator, value
- Actions in simple English
- Proper references with Level and article/text

Return complete validated rules array."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract and validate rules
            validated_rules = self._validate_rules_from_response(response_text, state)
            
            # Update state
            state['validated_rules'] = validated_rules
            state['agent4_reasoning'] = reasoning_trace
            state['processing_complete'] = True
            
            logger.info(f"✅ {self.name}: Validated {len(validated_rules)} rules")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent4_reasoning'] = [f"ERROR: {e}"]
            state['validated_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract ReAct reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _validate_rules_from_response(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Extract and validate rules from response"""
        validated_rules = []
        
        # Try JSON extraction
        json_data = extract_json_from_text(response_text)
        if json_data:
            if isinstance(json_data, list):
                rules_data = json_data
            elif isinstance(json_data, dict):
                rules_data = [json_data]
            else:
                rules_data = state['extracted_rules']
        else:
            rules_data = state['extracted_rules']
        
        # Validate each rule with Pydantic
        for rule_data in rules_data:
            try:
                # Ensure required fields
                validated_rule = {
                    "rule_id": rule_data.get("rule_id", f"rule_{uuid.uuid4().hex[:8]}"),
                    "rule_definition": rule_data.get("rule_definition", "Default rule definition"),
                    "applicable_countries": state['metadata_config']['applicable_countries'],
                    "adequacy_countries": self._verify_adequacy_countries(state['identified_countries'].get('adequacy_countries', [])),
                    "conditions": self._validate_conditions(rule_data.get("conditions", [])),
                    "data_category": rule_data.get("data_category", "Personal Data"),
                    "action": rule_data.get("action", "Must ensure compliance"),
                    "reference": rule_data.get("reference", "Document reference")
                }
                
                # Validate with Pydantic
                extracted_rule = ExtractedRule.model_validate(validated_rule)
                validated_rules.append(extracted_rule.model_dump())
                
            except ValidationError as e:
                logger.warning(f"Rule validation failed: {e}")
                # Add basic rule anyway
                basic_rule = {
                    "rule_id": f"rule_fallback_{uuid.uuid4().hex[:8]}",
                    "rule_definition": str(rule_data.get("rule_definition", "Fallback rule")),
                    "applicable_countries": state['metadata_config']['applicable_countries'],
                    "adequacy_countries": [],
                    "conditions": [{
                        "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                        "condition_definition": "When processing personal data",
                        "fact": "data.category",
                        "operator": "equal",
                        "value": "Personal Data"
                    }],
                    "data_category": "Personal Data", 
                    "action": "Must ensure compliance with data protection requirements",
                    "reference": "Document - General requirement"
                }
                validated_rules.append(basic_rule)
        
        return validated_rules
    
    def _verify_adequacy_countries(self, adequacy_countries: List[str]) -> List[str]:
        """Verify adequacy countries are valid using geography data"""
        verified = []
        for country in adequacy_countries:
            if self.geography_handler.is_valid_country(country):
                verified.append(country.upper())
        return verified
    
    def _validate_conditions(self, conditions: List) -> List[Dict]:
        """Validate conditions structure"""
        validated_conditions = []
        
        for condition in conditions:
            if isinstance(condition, dict):
                validated_condition = {
                    "condition_id": condition.get("condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    "condition_definition": condition.get("condition_definition", "When processing data"),
                    "fact": condition.get("fact", "data.category"),
                    "operator": condition.get("operator", "equal"),
                    "value": condition.get("value", "Personal Data")
                }
                validated_conditions.append(validated_condition)
        
        # Ensure at least one condition
        if not validated_conditions:
            validated_conditions.append({
                "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                "condition_definition": "When processing personal data",
                "fact": "data.category",
                "operator": "equal", 
                "value": "Personal Data"
            })
        
        return validated_conditions

# Multi-Agent Orchestrator using LangGraph
class MultiAgentLegalProcessor:
    """Multi-agent orchestrator using LangGraph with dedicated agents"""
    
    def __init__(self, geography_file: str):
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=MODEL_NAME,
            openai_api_key=API_KEY,
            openai_api_base=BASE_URL
        )
        
        # Initialize geography handler (NO HARDCODING)
        self.geography_handler = GeographyHandler(geography_file)
        
        # Initialize specialized agents
        self.doc_parser_agent = DocumentParserAgent(self.llm)
        self.geography_agent = GeographyAgent(self.llm, self.geography_handler)
        self.rule_extraction_agent = RuleExtractionAgent(self.llm)
        self.validation_agent = ValidationAgent(self.llm, self.geography_handler)
        
        # Create LangGraph workflow
        self.workflow = self._create_multi_agent_workflow()
        
        logger.info("🤖 Multi-Agent Legal Processor initialized with 4 specialized agents")
        logger.info(f"📍 Geography data loaded: {len(self.geography_handler.all_countries)} countries")
    
    def _create_multi_agent_workflow(self) -> StateGraph:
        """Create multi-agent workflow using LangGraph"""
        
        # Define workflow
        workflow = StateGraph(MultiAgentState)
        
        # Add agent nodes
        workflow.add_node("document_parser", self.doc_parser_agent.process)
        workflow.add_node("geography_agent", self.geography_agent.process)
        workflow.add_node("rule_extraction", self.rule_extraction_agent.process)
        workflow.add_node("validation_agent", self.validation_agent.process)
        
        # Set entry point
        workflow.set_entry_point("document_parser")
        
        # Add sequential edges (each agent processes after previous)
        workflow.add_edge("document_parser", "geography_agent")
        workflow.add_edge("geography_agent", "rule_extraction")
        workflow.add_edge("rule_extraction", "validation_agent")
        workflow.add_edge("validation_agent", END)
        
        # Compile with memory
        return workflow.compile(checkpointer=MemorySaver())
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process document through multi-agent pipeline"""
        
        logger.info(f"📄 Starting multi-agent processing: {metadata_config.pdf_path}")
        
        # Extract PDF text
        pdf_processor = PDFProcessor()
        document_text = pdf_processor.extract_text_from_pdf(metadata_config.pdf_path)
        
        if not document_text:
            raise ValueError("No text extracted from PDF")
        
        # Initialize state
        initial_state = MultiAgentState(
            document_text=document_text,
            metadata_config=metadata_config.model_dump(),
            geography_data=self.geography_handler.geography_data,
            parsed_sections={},
            identified_countries={},
            extracted_rules=[],
            validated_rules=[],
            agent1_reasoning=[],
            agent2_reasoning=[],
            agent3_reasoning=[],
            agent4_reasoning=[],
            current_agent="DocumentParserAgent",
            processing_complete=False
        )
        
        # Run multi-agent workflow
        config = {"configurable": {"thread_id": f"doc_{uuid.uuid4().hex[:8]}"}}
        
        try:
            final_state = self.workflow.invoke(initial_state, config)
            
            # Convert validated rules to Pydantic models
            validated_rules = []
            for rule_data in final_state['validated_rules']:
                try:
                    rule = ExtractedRule.model_validate(rule_data)
                    validated_rules.append(rule)
                except ValidationError as e:
                    logger.warning(f"Final rule validation failed: {e}")
                    continue
            
            # Log processing summary
            self._log_processing_summary(final_state, validated_rules)
            
            return validated_rules
            
        except Exception as e:
            logger.error(f"Multi-agent processing failed: {e}")
            raise
    
    def _log_processing_summary(self, final_state: MultiAgentState, rules: List[ExtractedRule]):
        """Log processing summary from all agents"""
        
        logger.info("🎯 Multi-Agent Processing Complete!")
        logger.info(f"📄 Document sections parsed: {len(final_state['parsed_sections'])}")
        logger.info(f"🌍 Countries identified: {len(final_state['identified_countries'].get('mentioned_countries', []))}")
        logger.info(f"⚖️ Rules extracted: {len(final_state['extracted_rules'])}")
        logger.info(f"✅ Rules validated: {len(rules)}")
        
        # Log agent reasoning traces
        for i, agent_name in enumerate(['DocumentParserAgent', 'GeographyAgent', 'RuleExtractionAgent', 'ValidationAgent'], 1):
            reasoning = final_state.get(f'agent{i}_reasoning', [])
            logger.info(f"🤖 {agent_name} reasoning steps: {len(reasoning)}")
            
            # Log first reasoning step for each agent
            if reasoning:
                first_step = reasoning[0][:100] + "..." if len(reasoning[0]) > 100 else reasoning[0]
                logger.info(f"   💭 {first_step}")

# Main Processing Pipeline
class LegalRuleExtractionPipeline:
    """Main pipeline using multi-agent architecture"""
    
    def __init__(self, geography_file: str):
        self.processor = MultiAgentLegalProcessor(geography_file)
        logger.info("🚀 Legal Rule Extraction Pipeline initialized with Multi-Agent Architecture")
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process single document"""
        return await self.processor.process_document(metadata_config)
    
    async def process_multiple_documents(self, config_file: str) -> List[ExtractedRule]:
        """Process multiple documents"""
        logger.info(f"📁 Processing multiple documents from: {config_file}")
        
        # Load config with safe JSON parsing
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                configs_data = safe_json_parse(f.read())
            
            if not configs_data:
                raise ValueError("Invalid configuration file")
                
        except Exception as e:
            logger.error(f"Config loading failed: {e}")
            raise
        
        all_rules = []
        
        for config_data in configs_data:
            try:
                metadata_config = MetadataConfig.model_validate(config_data)
                rules = await self.process_document(metadata_config)
                all_rules.extend(rules)
                
            except ValidationError as e:
                logger.error(f"Invalid config: {e}")
                continue
            except Exception as e:
                logger.error(f"Processing failed for {config_data.get('pdf_path', 'unknown')}: {e}")
                continue
        
        return all_rules
    
    def save_results(self, rules: List[ExtractedRule], output_dir: str):
        """Save results in JSON and CSV formats"""
        logger.info(f"💾 Saving results to {output_dir}")
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Convert to dictionaries
        rules_dicts = [rule.model_dump() for rule in rules]
        
        # Save JSON
        json_file = Path(output_dir) / "extracted_rules.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(rules_dicts, f, indent=2, ensure_ascii=False)
        
        # Save CSV
        csv_file = Path(output_dir) / "extracted_rules.csv"
        if rules_dicts:
            flattened_rules = []
            for rule in rules_dicts:
                flat_rule = {}
                for key, value in rule.items():
                    if isinstance(value, (dict, list)):
                        flat_rule[key] = json.dumps(value, ensure_ascii=False)
                    else:
                        flat_rule[key] = value
                flattened_rules.append(flat_rule)
            
            df = pd.DataFrame(flattened_rules)
            df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # Save agent reasoning traces
        reasoning_file = Path(output_dir) / "agent_reasoning.json"
        with open(reasoning_file, 'w', encoding='utf-8') as f:
            json.dump({
                "processing_summary": f"Processed {len(rules)} rules using multi-agent architecture",
                "agents_used": ["DocumentParserAgent", "GeographyAgent", "RuleExtractionAgent", "ValidationAgent"],
                "methodology": "Chain of Thought with ReAct prompting (Reasoning, Action, Observation)"
            }, f, indent=2)
        
        logger.info(f"✅ Results saved: {json_file}, {csv_file}, {reasoning_file}")

# CLI Interface
async def main():
    """CLI interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Multi-Agent Legal Rule Extraction with Chain of Thought and ReAct")
    parser.add_argument("--config", required=True, help="Metadata configuration JSON file")
    parser.add_argument("--geography", required=True, help="Geography JSON file")
    parser.add_argument("--output", default="./output", help="Output directory")
    
    args = parser.parse_args()
    
    try:
        # Initialize pipeline
        pipeline = LegalRuleExtractionPipeline(args.geography)
        
        # Process documents  
        rules = await pipeline.process_multiple_documents(args.config)
        
        # Save results
        pipeline.save_results(rules, args.output)
        
        logger.info(f"🎉 Processing complete! Generated {len(rules)} rules using multi-agent architecture")
        
        # Display sample results
        if rules:
            sample_rule = rules[0]
            logger.info("📋 Sample extracted rule:")
            logger.info(f"   🆔 Rule ID: {sample_rule.rule_id}")
            logger.info(f"   📝 Definition: {sample_rule.rule_definition[:100]}...")
            logger.info(f"   🌍 Applicable Countries: {sample_rule.applicable_countries}")
            logger.info(f"   ✅ Adequacy Countries: {sample_rule.adequacy_countries}")
            logger.info(f"   🏷️ Data Category: {sample_rule.data_category}")
            logger.info(f"   ⚡ Action: {sample_rule.action[:100]}...")
            logger.info(f"   📍 Reference: {sample_rule.reference}")
        
        return 0
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        return 1

if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
