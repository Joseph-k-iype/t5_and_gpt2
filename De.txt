"""
TermMatchingAgent - Agent responsible for finding and ranking business terms.

Enhanced with LangGraph for agentic RAG capabilities to improve matching accuracy.
"""
import logging
import json
from typing import List, Dict, Any, Optional, Tuple
import time

from app.core.embedding import MyDocument
from app.config.settings import get_llm
from app.agents.agentic_rag.graph import AgenticRagGraph

logger = logging.getLogger(__name__)

# Constants for default configuration
CANDIDATE_FETCH_MULTIPLIER = 5  # Fetch 5x the number of final results needed
MIN_CANDIDATES_TO_FETCH = 20  # Fetch at least 20 candidates
DEFAULT_INITIAL_THRESHOLD = 0.2  # Lower threshold for broader retrieval

class TermMatchingAgent:
    """
    Agent for matching terms using an agentic RAG approach with LangGraph.
    
    This agent enhances term matching by combining vector search, keyword matching,
    synonym expansion, and LLM-based relevance evaluation in a graph-based workflow.
    """
    
    def __init__(self, business_term_manager):
        """
        Initialize the TermMatchingAgent.

        Args:
            business_term_manager: An instance of BusinessTermManager
        """
        self.bt_manager = business_term_manager
        self.embedding_client = business_term_manager.embedding_client
        
        try:
            # Initialize LLM
            self.llm = get_llm()
            logger.info("TermMatchingAgent initialized with LLM.")
            
            # Initialize Agentic RAG graph
            self.rag_graph = AgenticRagGraph(business_term_manager)
            logger.info("Agentic RAG graph initialized.")
            
        except Exception as e:
            logger.error(f"Failed to initialize TermMatchingAgent: {e}", exc_info=True)
            self.llm = None
            self.rag_graph = None
            logger.warning("TermMatchingAgent will fall back to basic vector search due to initialization failure.")
    
    async def find_matching_terms(
        self,
        element_id: str,
        element_name: str,
        element_description: str,
        top_k: int,
        cdm_context: Optional[str] = None,
        example_context: Optional[str] = None,
        process_name_context: Optional[str] = None,
        process_description_context: Optional[str] = None,
        initial_threshold: float = DEFAULT_INITIAL_THRESHOLD
    ) -> Tuple[List[Dict[str, Any]], List[float]]:
        """
        Find matching business terms using the agentic RAG approach.
        
        Falls back to simpler methods if agentic RAG isn't available.

        Args:
            element_id: Unique identifier for the element
            element_name: Name of the element to match
            element_description: Description of the element
            top_k: Number of top matches to return
            cdm_context: Optional CDM context
            example_context: Optional example context
            process_name_context: Optional process name context
            process_description_context: Optional process description context
            initial_threshold: Minimum similarity threshold
            
        Returns:
            Tuple containing:
            - List of matched term dictionaries
            - List of confidence scores
        """
        logger.info(f"Finding terms for '{element_name}' (ID: {element_id})")
        
        # Record start time for performance tracking
        start_time = time.time()
        
        # Use agentic RAG if available
        if self.rag_graph:
            try:
                matching_terms, confidence_scores, modeling_required, message = await self.rag_graph.run(
                    element_id=element_id,
                    element_name=element_name,
                    element_description=element_description,
                    top_k=top_k,
                    threshold=initial_threshold,
                    cdm_context=cdm_context,
                    example_context=example_context,
                    process_name_context=process_name_context,
                    process_description_context=process_description_context
                )
                
                execution_time = time.time() - start_time
                logger.info(f"Agentic RAG found {len(matching_terms)} terms in {execution_time:.2f}s, modeling_required={modeling_required}")
                
                return matching_terms, confidence_scores
                
            except Exception as e:
                logger.error(f"Agentic RAG failed, falling back to simple search: {e}", exc_info=True)
                # Continue with fallback methods
        
        # Fallback to basic vector search if agentic RAG fails or is unavailable
        logger.info("Using basic vector search fallback")
        fallback_start = time.time()
        
        # Build query
        query_parts = [
            f"Item Name: {element_name}",
            f"Description: {element_description}"
        ]
        
        # Add context if available
        if example_context: 
            query_parts.append(f"Examples: {example_context}")
        if process_name_context: 
            query_parts.append(f"Related Process: {process_name_context}")
        if process_description_context: 
            query_parts.append(f"Process Description: {process_description_context}")
        if cdm_context: 
            query_parts.append(f"CDM: {cdm_context}")
        
        query_text = ". ".join(query_parts)
        
        # Create document and get embedding
        query_doc = MyDocument(id=f"query_{element_id}", text=query_text)
        try:
            embedded_query_doc = self.embedding_client.generate_embeddings(query_doc)
            if not embedded_query_doc.embedding:
                logger.error("Failed to generate embedding for the query.")
                return [], []
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return [], []
            
        # Perform vector search
        try:
            num_candidates = max(top_k * CANDIDATE_FETCH_MULTIPLIER, MIN_CANDIDATES_TO_FETCH)
            
            results = self.bt_manager.vector_store.find_similar_vectors(
                query_vector=embedded_query_doc.embedding,
                top_k=num_candidates,
                threshold=initial_threshold
            )
            
            # Sort by similarity
            results.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
            
            # Take top_k
            top_results = results[:top_k]
            
            confidence_scores = [result.get("similarity", 0.0) for result in top_results]
            
            fallback_time = time.time() - fallback_start
            logger.info(f"Basic vector search fallback found {len(top_results)} results in {fallback_time:.2f}s")
            
            return top_results, confidence_scores
            
        except Exception as e:
            logger.error(f"Error performing vector search: {e}")
            return [], []

    async def evaluate_term_match(self, 
                                element_name: str, 
                                element_description: str, 
                                term_name: str, 
                                term_description: str) -> Tuple[float, str]:
        """
        Evaluate how well a specific term matches the given element.
        
        Args:
            element_name: Element name
            element_description: Element description
            term_name: Business term name
            term_description: Business term description
            
        Returns:
            Tuple containing:
            - Match score (0-1)
            - Reasoning for the score
        """
        if not self.llm:
            # Calculate basic similarity if LLM is not available
            from app.utils.text_processing import semantic_similarity
            
            combined_element = f"{element_name} {element_description}"
            combined_term = f"{term_name} {term_description}"
            score = semantic_similarity(combined_element, combined_term)
            
            return score, "Score based on text similarity (LLM not available)"
        
        try:
            # Use LLM to evaluate the match
            prompt = f"""
            You are a data governance expert evaluating how well a business term matches a data element.
            
            Data Element:
              Name: {element_name}
              Description: {element_description}
            
            Business Term:
              Name: {term_name}
              Description: {term_description}
            
            Evaluate how well this business term matches the data element semantically.
            Consider:
            1. How well the term's name matches the element's meaning
            2. How well the term's description aligns with the element's purpose
            3. Whether the concept is the same, even if expressed differently
            
            Rate the match on a scale of 0-10 and provide brief reasoning.
            
            Return your evaluation as a JSON object:
            {{
              "score": 8.5,
              "reasoning": "Brief explanation of why this is a good/bad match"
            }}
            
            Ensure your response is ONLY the JSON object.
            """
            
            response = await self.llm.ainvoke(prompt)
            
            # Parse response
            try:
                # Try direct JSON parsing
                eval_data = json.loads(response)
                score = eval_data.get("score", 0) / 10.0  # Convert to 0-1 scale
                reasoning = eval_data.get("reasoning", "")
                
                return score, reasoning
                
            except json.JSONDecodeError:
                # Try to extract JSON from text
                import re
                json_match = re.search(r'{.*}', response, re.DOTALL)
                if json_match:
                    try:
                        eval_data = json.loads(json_match.group(0))
                        score = eval_data.get("score", 0) / 10.0
                        reasoning = eval_data.get("reasoning", "")
                        
                        return score, reasoning
                    except:
                        pass
                
                # Fallback to basic similarity
                from app.utils.text_processing import semantic_similarity
                
                combined_element = f"{element_name} {element_description}"
                combined_term = f"{term_name} {term_description}"
                score = semantic_similarity(combined_element, combined_term)
                
                return score, "Score based on text similarity (LLM response parsing failed)"
                
        except Exception as e:
            logger.error(f"Error evaluating term match: {e}")
            
            # Fallback to basic similarity
            from app.utils.text_processing import semantic_similarity
            
            combined_element = f"{element_name} {element_description}"
            combined_term = f"{term_name} {term_description}"
            score = semantic_similarity(combined_element, combined_term)
            
            return score, f"Score based on text similarity (LLM evaluation failed: {str(e)})"
