"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Using o3-mini with Advanced Reasoning: Mixture of Thought + Mixture of Chains + Chain of Thought

This system converts legislation text into JSON rules compatible with json-rules-engine
using advanced prompting strategies for superior reasoning capabilities.
"""

import json
import re
import asyncio
from typing import List, Dict, Any, Optional, Annotated, Sequence
from dataclasses import dataclass
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# State Management
class AgentState(BaseModel):
    """Comprehensive state for the legislation processing agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str = ""
    
    # Multi-pathway analysis results
    reasoning_pathways: List[Dict[str, Any]] = []
    pathway_analyses: Dict[str, Any] = {}
    
    # Chain processing results
    chain_results: Dict[str, Any] = {}
    extraction_results: Dict[str, List[Dict[str, Any]]] = {}
    
    # Rule processing
    raw_rules: List[Dict[str, Any]] = []
    extracted_rules: List[Dict[str, Any]] = []
    synthesized_rules: List[Dict[str, Any]] = []
    json_rules: List[Dict[str, Any]] = []
    
    # Processing metadata
    processing_stage: str = "initial"
    processing_step: str = "initialization"
    current_pathway: str = ""
    current_chain: str = ""
    
    # Quality and validation
    validation_results: Dict[str, Any] = {}
    quality_metrics: Dict[str, Any] = {}
    confidence_scores: Dict[str, float] = {}
    
    # Metadata and context
    metadata: Dict[str, Any] = {}
    processing_context: Dict[str, Any] = {}
    reasoning_traces: List[Dict[str, Any]] = []
    
    # Configuration
    synthesis_strategy: str = "weighted"
    validation_depth: str = "comprehensive"
    reasoning_effort: str = "medium"


# Pydantic Input Models for Strict Tool Definitions

class LegislationAnalysisInput(BaseModel):
    """Input for comprehensive legislation analysis using multiple reasoning pathways"""
    legislation_text: str = Field(..., description="Raw legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Type of reasoning pathway: 'structural', 'semantic', 'logical', or 'contextual'")


class RuleExtractionInput(BaseModel):
    """Input for extracting rules using chain of thought reasoning"""
    text_segment: str = Field(..., description="Legislation text segment to process")
    analysis_context: Dict[str, Any] = Field(..., description="Context from previous analysis")
    extraction_focus: str = Field(..., description="Focus area: 'requirements', 'prohibitions', 'conditions', 'exceptions', or 'penalties'")


class RuleSynthesisInput(BaseModel):
    """Input for synthesizing rules from multiple chains"""
    pathway_results: List[Dict[str, Any]] = Field(..., description="Results from different reasoning pathways")
    synthesis_strategy: str = Field(..., description="Strategy: 'consensus', 'weighted', or 'hierarchical'")


class JSONConversionInput(BaseModel):
    """Input for converting synthesized rules to JSON format"""
    synthesized_rules: List[Dict[str, Any]] = Field(..., description="Rules from synthesis process")
    conversion_standard: str = Field(default="json-rules-engine", description="Target JSON rules format")


class ValidationInput(BaseModel):
    """Input for validating JSON rules"""
    json_rules: List[Dict[str, Any]] = Field(..., description="JSON rules to validate")
    validation_depth: str = Field(default="comprehensive", description="Validation depth: 'basic', 'standard', or 'comprehensive'")


# Advanced Tools with Mixture of Thought and Chain Reasoning

@tool(args_schema=LegislationAnalysisInput)
def analyze_legislation_pathway(legislation_text: str, reasoning_pathway: str) -> Dict[str, Any]:
    """
    Analyze legislation using a specific reasoning pathway in the Mixture of Thought approach.
    Each pathway explores different aspects of the legal text.
    
    Args:
        legislation_text: Raw legislation text
        reasoning_pathway: Type of reasoning pathway to use
        
    Returns:
        Analysis results from the specific pathway
    """
    
    if reasoning_pathway == "structural":
        # Structural analysis: hierarchy, organization, relationships
        structure_patterns = {
            'sections': r'(?:Section|Sec\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Section|Sec\.?)\s+\d+|$)',
            'articles': r'(?:Article|Art\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Article|Art\.?)\s+\d+|$)',
            'subsections': r'(?:^|\n)\s*\(([a-z]|\d+)\)\s+(.*?)(?=\n\s*\([a-z]|\d+\)|$)',
            'clauses': r'(?:^|\n)\s*\(i+\)\s+(.*?)(?=\n\s*\(i+\)|$)'
        }
        
        analysis = {
            'pathway': 'structural',
            'hierarchy': {},
            'relationships': [],
            'organization_quality': 'high'
        }
        
        # Extract structural elements
        for element_type, pattern in structure_patterns.items():
            matches = re.findall(pattern, legislation_text, re.DOTALL | re.MULTILINE)
            analysis['hierarchy'][element_type] = [
                {'id': match[0] if isinstance(match, tuple) else f"{element_type}_{i+1}", 
                 'content': match[1] if isinstance(match, tuple) else match}
                for i, match in enumerate(matches)
            ]
        
        # Identify cross-references
        cross_refs = re.findall(r'(?:Section|Article)\s+(\d+(?:\.\d+)*)', legislation_text)
        analysis['relationships'] = [{'type': 'cross_reference', 'target': ref} for ref in set(cross_refs)]
        
        return analysis
    
    elif reasoning_pathway == "semantic":
        # Semantic analysis: meaning, definitions, intent
        semantic_patterns = {
            'definitions': [
                r'"([^"]+)"\s+means\s+([^.]+)',
                r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+means\s+([^.]+)',
                r'(?:For the purposes of|In this)\s+[^,]*,\s*"([^"]+)"\s+([^.]+)'
            ],
            'intent_indicators': [
                r'\b(?:purpose|intent|objective|goal|aim)\b',
                r'\b(?:whereas|considering|recognizing)\b'
            ],
            'scope_indicators': [
                r'\b(?:applies to|covers|includes|excludes|except)\b'
            ]
        }
        
        analysis = {
            'pathway': 'semantic',
            'definitions': [],
            'intent_phrases': [],
            'scope_elements': [],
            'semantic_density': 'medium'
        }
        
        # Extract definitions
        for pattern in semantic_patterns['definitions']:
            matches = re.findall(pattern, legislation_text, re.MULTILINE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    analysis['definitions'].append({
                        'term': match[0].strip(),
                        'definition': match[1].strip()
                    })
        
        # Extract intent and scope
        for pattern in semantic_patterns['intent_indicators']:
            matches = re.findall(f'[^.]*{pattern}[^.]*', legislation_text, re.IGNORECASE)
            analysis['intent_phrases'].extend(matches)
        
        for pattern in semantic_patterns['scope_indicators']:
            matches = re.findall(f'[^.]*{pattern}[^.]*', legislation_text, re.IGNORECASE)
            analysis['scope_elements'].extend(matches)
        
        return analysis
    
    elif reasoning_pathway == "logical":
        # Logical analysis: rules, conditions, consequences
        logical_patterns = {
            'conditionals': r'(?:If|When|Where)\s+([^,]+),\s*(?:then\s+)?([^.]+)',
            'requirements': r'([^.]+?)\s+(?:shall|must)\s+([^.]+)',
            'prohibitions': r'([^.]+?)\s+(?:shall not|must not|may not|prohibited from)\s+([^.]+)',
            'exceptions': r'(?:except|unless|provided that|notwithstanding)\s+([^,]+),\s*([^.]+)',
            'penalties': r'(?:violation|breach|failure)[^.]*?(?:shall result in|subject to|punishable by)\s+([^.]+)'
        }
        
        analysis = {
            'pathway': 'logical',
            'conditionals': [],
            'requirements': [],
            'prohibitions': [],
            'exceptions': [],
            'penalties': [],
            'logical_complexity': 'medium'
        }
        
        # Extract logical structures
        for rule_type, pattern in logical_patterns.items():
            matches = re.findall(pattern, legislation_text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple):
                    analysis[rule_type].append({
                        'condition': match[0].strip() if len(match) > 1 else '',
                        'consequence': match[1].strip() if len(match) > 1 else match[0].strip(),
                        'confidence': 0.8
                    })
                else:
                    analysis[rule_type].append({
                        'content': match.strip(),
                        'confidence': 0.7
                    })
        
        return analysis
    
    elif reasoning_pathway == "contextual":
        # Contextual analysis: actors, actions, temporal elements
        contextual_patterns = {
            'actors': [
                r'\b(?:person|individual|entity|organization|company|corporation|agency|department|officer|employee|citizen|resident)\b',
                r'\b(?:applicant|licensee|registrant|holder|owner|operator|provider|contractor)\b'
            ],
            'actions': [
                r'\b(?:shall|must|may|should|will|can)\s+([^.]+)',
                r'\b(?:provide|submit|maintain|ensure|comply|notify|report|register|apply)\b'
            ],
            'temporal': [
                r'\b(?:within|before|after|by|no later than)\s+\d+\s+(?:days|weeks|months|years)\b',
                r'\b(?:immediately|promptly|forthwith|annually|monthly|quarterly)\b'
            ]
        }
        
        analysis = {
            'pathway': 'contextual',
            'actors': [],
            'actions': [],
            'temporal_elements': [],
            'context_richness': 'high'
        }
        
        # Extract contextual elements
        for category, patterns in contextual_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, legislation_text, re.IGNORECASE)
                if category == 'actors':
                    analysis[category].extend(list(set(matches)))
                elif category == 'actions':
                    analysis[category].extend([match if isinstance(match, str) else match[0] for match in matches])
                else:
                    analysis[category].extend(matches)
        
        # Remove duplicates
        for key in analysis:
            if isinstance(analysis[key], list):
                analysis[key] = list(set(analysis[key]))
        
        return analysis
    
    else:
        return {
            'pathway': reasoning_pathway,
            'error': 'Unknown reasoning pathway',
            'available_pathways': ['structural', 'semantic', 'logical', 'contextual']
        }


@tool(args_schema=RuleExtractionInput)
def extract_rules_chain(text_segment: str, analysis_context: Dict[str, Any], extraction_focus: str) -> Dict[str, Any]:
    """
    Extract specific types of rules using chain of thought reasoning.
    Each chain focuses on a specific aspect of rule extraction.
    
    Args:
        text_segment: Text segment to process
        analysis_context: Context from pathway analysis
        extraction_focus: Focus area for extraction
        
    Returns:
        Extracted rules with reasoning chain
    """
    
    reasoning_chain = []
    extracted_rules = []
    
    # Step 1: Context Assessment
    reasoning_chain.append({
        'step': 1,
        'action': 'context_assessment',
        'reasoning': f'Analyzing text segment for {extraction_focus} extraction with context from {analysis_context.get("pathway", "unknown")} pathway',
        'findings': f'Text length: {len(text_segment)} characters, Focus: {extraction_focus}'
    })
    
    if extraction_focus == "requirements":
        # Chain of thought for requirements extraction
        requirement_patterns = [
            r'([^.]+?)\s+(?:shall|must|required to|obligated to|duty to)\s+([^.]+)',
            r'(?:must|shall)\s+([^.]+)',
            r'([^.]+?)\s+(?:is required|are required)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying requirement patterns using modal verbs and obligation indicators',
            'patterns_used': requirement_patterns
        })
        
        for i, pattern in enumerate(requirement_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'requirement',
                        'subject': match[0].strip(),
                        'obligation': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9
                    })
                else:
                    extracted_rules.append({
                        'type': 'requirement',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} requirements using pattern matching',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "prohibitions":
        # Chain of thought for prohibitions extraction
        prohibition_patterns = [
            r'([^.]+?)\s+(?:shall not|must not|may not|cannot|prohibited from|forbidden from)\s+([^.]+)',
            r'(?:no\s+)?([^.]+?)\s+(?:shall|may)\s+([^.]+)',
            r'(?:it is prohibited|forbidden|illegal)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying prohibition patterns using negative modal constructions',
            'patterns_used': prohibition_patterns
        })
        
        for i, pattern in enumerate(prohibition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'prohibition',
                        'subject': match[0].strip(),
                        'prohibited_action': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9
                    })
                else:
                    extracted_rules.append({
                        'type': 'prohibition',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} prohibitions using negative modal analysis',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "conditions":
        # Chain of thought for conditions extraction
        condition_patterns = [
            r'(?:If|When|Where|Provided that|In case)\s+([^,]+),\s*([^.]+)',
            r'([^.]+?)\s+(?:if|when|where|provided)\s+([^.]+)',
            r'(?:Subject to|Conditional upon)\s+([^,]+),\s*([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying conditional patterns using temporal and logical connectors',
            'patterns_used': condition_patterns
        })
        
        for i, pattern in enumerate(condition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'condition',
                        'trigger': match[0].strip(),
                        'consequence': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} conditions using conditional logic patterns',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "exceptions":
        # Chain of thought for exceptions extraction
        exception_patterns = [
            r'(?:except|unless|save|but|however|notwithstanding)\s+([^,]+),\s*([^.]+)',
            r'([^.]+)\s+(?:except|unless|save for)\s+([^.]+)',
            r'(?:this does not apply|shall not apply)\s+(?:to|when|if)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying exception patterns using adversative and conditional expressions',
            'patterns_used': exception_patterns
        })
        
        for i, pattern in enumerate(exception_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'exception',
                        'exception_condition': match[0].strip(),
                        'modified_rule': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
                else:
                    extracted_rules.append({
                        'type': 'exception',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.6
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} exceptions using adversative pattern analysis',
            'extraction_quality': 'medium' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "penalties":
        # Chain of thought for penalties extraction
        penalty_patterns = [
            r'(?:violation|breach|failure to comply|non-compliance)[^.]*?(?:shall result in|subject to|punishable by|penalty of)\s+([^.]+)',
            r'(?:fine|penalty|sanction|imprisonment|suspension|revocation)\s+(?:of|not exceeding|up to)\s+([^.]+)',
            r'([^.]+)\s+(?:shall be fined|shall be subject to|shall result in)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying penalty patterns using consequence and sanction indicators',
            'patterns_used': penalty_patterns
        })
        
        for i, pattern in enumerate(penalty_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'penalty',
                        'violation': match[0].strip(),
                        'sanction': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8
                    })
                else:
                    extracted_rules.append({
                        'type': 'penalty',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} penalties using sanction pattern analysis',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    # Step 4: Quality Assessment
    reasoning_chain.append({
        'step': 4,
        'action': 'quality_assessment',
        'reasoning': f'Assessing extraction quality based on rule count and pattern diversity',
        'total_rules': len(extracted_rules),
        'assessment': 'successful' if len(extracted_rules) > 0 else 'limited_success'
    })
    
    return {
        'extraction_focus': extraction_focus,
        'extracted_rules': extracted_rules,
        'reasoning_chain': reasoning_chain,
        'context_used': analysis_context.get('pathway', 'unknown'),
        'extraction_metadata': {
            'text_length': len(text_segment),
            'rules_found': len(extracted_rules),
            'confidence_avg': sum([rule.get('confidence', 0) for rule in extracted_rules]) / len(extracted_rules) if extracted_rules else 0
        }
    }


@tool(args_schema=RuleSynthesisInput)
def synthesize_rules(pathway_results: List[Dict[str, Any]], synthesis_strategy: str) -> Dict[str, Any]:
    """
    Synthesize rules from multiple reasoning pathways using mixture of thought approach.
    
    Args:
        pathway_results: Results from different reasoning pathways
        synthesis_strategy: Strategy for synthesis
        
    Returns:
        Synthesized rules with rationale
    """
    
    synthesized_rules = []
    synthesis_rationale = []
    
    # Collect all rules from different pathways
    all_rules = []
    pathway_weights = {
        'structural': 0.2,
        'semantic': 0.25,
        'logical': 0.35,
        'contextual': 0.2
    }
    
    for result in pathway_results:
        if 'extracted_rules' in result:
            for rule in result['extracted_rules']:
                rule['source_pathway'] = result.get('extraction_focus', 'unknown')
                rule['pathway_weight'] = pathway_weights.get(result.get('context_used', 'unknown'), 0.1)
                all_rules.append(rule)
    
    synthesis_rationale.append({
        'step': 'collection',
        'reasoning': f'Collected {len(all_rules)} rules from {len(pathway_results)} pathways',
        'pathway_distribution': {result.get('extraction_focus', 'unknown'): len(result.get('extracted_rules', [])) for result in pathway_results}
    })
    
    if synthesis_strategy == "consensus":
        # Find rules that appear across multiple pathways
        rule_signatures = {}
        for rule in all_rules:
            # Create signature based on rule content
            signature = f"{rule.get('type', 'unknown')}_{rule.get('subject', rule.get('content', ''))[:50]}"
            if signature not in rule_signatures:
                rule_signatures[signature] = []
            rule_signatures[signature].append(rule)
        
        # Select rules with consensus (appear in multiple pathways or high confidence)
        for signature, rules in rule_signatures.items():
            if len(rules) > 1 or (len(rules) == 1 and rules[0].get('confidence', 0) > 0.8):
                # Take the rule with highest confidence
                best_rule = max(rules, key=lambda r: r.get('confidence', 0))
                best_rule['synthesis_support'] = len(rules)
                best_rule['synthesis_confidence'] = min(1.0, best_rule.get('confidence', 0) * len(rules) * 0.3)
                synthesized_rules.append(best_rule)
        
        synthesis_rationale.append({
            'step': 'consensus_filtering',
            'reasoning': f'Applied consensus filtering, selected {len(synthesized_rules)} rules with cross-pathway support',
            'strategy': 'consensus'
        })
    
    elif synthesis_strategy == "weighted":
        # Weight rules based on pathway confidence and rule quality
        for rule in all_rules:
            weighted_confidence = rule.get('confidence', 0) * rule.get('pathway_weight', 0.1)
            if weighted_confidence > 0.4:  # Threshold for inclusion
                rule['weighted_confidence'] = weighted_confidence
                synthesized_rules.append(rule)
        
        # Sort by weighted confidence
        synthesized_rules.sort(key=lambda r: r.get('weighted_confidence', 0), reverse=True)
        
        synthesis_rationale.append({
            'step': 'weighted_selection',
            'reasoning': f'Applied weighted selection, retained {len(synthesized_rules)} high-quality rules',
            'strategy': 'weighted'
        })
    
    elif synthesis_strategy == "hierarchical":
        # Prioritize by rule type hierarchy: requirements > prohibitions > conditions > exceptions > penalties
        type_priority = {
            'requirement': 5,
            'prohibition': 4,
            'condition': 3,
            'exception': 2,
            'penalty': 1
        }
        
        # Sort by type priority and confidence
        sorted_rules = sorted(all_rules, 
                            key=lambda r: (type_priority.get(r.get('type', 'unknown'), 0), 
                                         r.get('confidence', 0)), 
                            reverse=True)
        
        # Take top rules from each category
        rules_by_type = {}
        for rule in sorted_rules:
            rule_type = rule.get('type', 'unknown')
            if rule_type not in rules_by_type:
                rules_by_type[rule_type] = []
            if len(rules_by_type[rule_type]) < 10:  # Limit per type
                rules_by_type[rule_type].append(rule)
        
        # Flatten back to single list
        for type_rules in rules_by_type.values():
            synthesized_rules.extend(type_rules)
        
        synthesis_rationale.append({
            'step': 'hierarchical_prioritization',
            'reasoning': f'Applied hierarchical prioritization, organized {len(synthesized_rules)} rules by type importance',
            'strategy': 'hierarchical',
            'type_distribution': {t: len(rules) for t, rules in rules_by_type.items()}
        })
    
    # Final quality check
    synthesis_rationale.append({
        'step': 'quality_assessment',
        'reasoning': f'Final synthesis produced {len(synthesized_rules)} high-quality rules',
        'quality_metrics': {
            'total_rules': len(synthesized_rules),
            'avg_confidence': sum([r.get('confidence', 0) for r in synthesized_rules]) / len(synthesized_rules) if synthesized_rules else 0,
            'type_diversity': len(set([r.get('type', 'unknown') for r in synthesized_rules]))
        }
    })
    
    return {
        'synthesized_rules': synthesized_rules,
        'synthesis_strategy': synthesis_strategy,
        'synthesis_rationale': synthesis_rationale,
        'original_rule_count': len(all_rules),
        'final_rule_count': len(synthesized_rules),
        'synthesis_quality': 'high' if len(synthesized_rules) > 0 else 'low'
    }


@tool(args_schema=JSONConversionInput)
def convert_to_json_rules(synthesized_rules: List[Dict[str, Any]], conversion_standard: str = "json-rules-engine") -> List[Dict[str, Any]]:
    """
    Convert synthesized rules into json-rules-engine compatible format.
    
    Args:
        synthesized_rules: Rules from synthesis process
        conversion_standard: Target JSON rules format
        
    Returns:
        JSON rules compatible with json-rules-engine
    """
    json_rules = []
    
    for i, rule in enumerate(synthesized_rules):
        rule_id = f"legislation_rule_{i+1}"
        rule_type = rule.get('type', 'unknown')
        
        if rule_type == 'requirement':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "compliance_required",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "requirement_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "obligation": rule.get('obligation', rule.get('content', 'unknown')),
                        "mandatory": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 80
            }
        
        elif rule_type == 'prohibition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "prohibited_action_attempted",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "prohibition_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "prohibited_action": rule.get('prohibited_action', rule.get('content', 'unknown')),
                        "violation_level": "high",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 90
            }
        
        elif rule_type == 'condition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "trigger_condition",
                            "operator": "contains",
                            "value": rule.get('trigger', 'condition').lower()[:20]
                        },
                        {
                            "fact": "condition_met",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "conditional_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "trigger": rule.get('trigger', 'unknown'),
                        "consequence": rule.get('consequence', rule.get('content', 'unknown')),
                        "conditional": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 70
            }
        
        elif rule_type == 'exception':
            json_rule = {
                "conditions": {
                    "any": [
                        {
                            "fact": "exception_condition",
                            "operator": "contains",
                            "value": rule.get('exception_condition', 'exception').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "exception_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "exception_condition": rule.get('exception_condition', 'unknown'),
                        "modified_rule": rule.get('modified_rule', rule.get('content', 'unknown')),
                        "overrides_default": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 60
            }
        
        elif rule_type == 'penalty':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "violation_detected",
                            "operator": "equal",
                            "value": True
                        },
                        {
                            "fact": "violation_type",
                            "operator": "contains",
                            "value": rule.get('violation', 'violation').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "penalty_applied",
                    "params": {
                        "rule_id": rule_id,
                        "violation": rule.get('violation', 'unknown'),
                        "sanction": rule.get('sanction', rule.get('content', 'unknown')),
                        "severity": "medium",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 50
            }
        
        else:
            # Generic rule for unknown types
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "rule_applicable",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "generic_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "content": rule.get('content', 'unknown rule'),
                        "rule_type": rule_type,
                        "confidence": rule.get('confidence', 0.3),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 40
            }
        
        # Add metadata
        json_rule["metadata"] = {
            "source": "legislation_analysis",
            "extraction_method": "mixture_of_thought_chains",
            "reasoning_pathway": rule.get('source_pathway', 'unknown'),
            "synthesis_strategy": "advanced",
            "created_by": "o3_mini_agent"
        }
        
        json_rules.append(json_rule)
    
    return json_rules


@tool(args_schema=ValidationInput)
def validate_json_rules(json_rules: List[Dict[str, Any]], validation_depth: str = "comprehensive") -> Dict[str, Any]:
    """
    Validate JSON rules for json-rules-engine compatibility.
    
    Args:
        json_rules: JSON rules to validate
        validation_depth: Depth of validation
        
    Returns:
        Comprehensive validation report
    """
    validation_report = {
        "valid": True,
        "validation_depth": validation_depth,
        "total_rules": len(json_rules),
        "errors": [],
        "warnings": [],
        "recommendations": [],
        "rule_analysis": [],
        "quality_score": 0.0
    }
    
    required_fields = ["conditions", "event"]
    valid_operators = [
        "equal", "notEqual", "lessThan", "lessThanInclusive", 
        "greaterThan", "greaterThanInclusive", "in", "notIn", 
        "contains", "doesNotContain", "regex"
    ]
    
    quality_points = 0
    max_possible_points = len(json_rules) * 10
    
    for i, rule in enumerate(json_rules):
        rule_errors = []
        rule_warnings = []
        rule_recommendations = []
        rule_quality = 0
        
        # Basic structure validation
        for field in required_fields:
            if field not in rule:
                rule_errors.append(f"Missing required field: {field}")
            else:
                rule_quality += 2
        
        # Conditions validation
        if "conditions" in rule:
            conditions = rule["conditions"]
            if isinstance(conditions, dict):
                logical_ops = ["all", "any", "not"]
                has_logical_op = any(op in conditions for op in logical_ops)
                
                if has_logical_op:
                    rule_quality += 2
                    
                    # Validate condition structure
                    for logical_op in logical_ops:
                        if logical_op in conditions:
                            condition_list = conditions[logical_op]
                            
                            if logical_op == "not":
                                if isinstance(condition_list, dict):
                                    rule_quality += 1
                                else:
                                    rule_errors.append("'not' operator must contain a single condition object")
                            else:
                                if isinstance(condition_list, list):
                                    rule_quality += 1
                                    
                                    # Validate individual conditions
                                    for j, condition in enumerate(condition_list):
                                        if isinstance(condition, dict):
                                            rule_quality += 0.5
                                            
                                            # Check condition fields
                                            if "fact" in condition:
                                                rule_quality += 0.5
                                            else:
                                                rule_errors.append(f"Condition {j} missing 'fact' field")
                                            
                                            if "operator" in condition:
                                                if condition["operator"] in valid_operators:
                                                    rule_quality += 1
                                                else:
                                                    rule_warnings.append(f"Condition {j} uses unknown operator: {condition['operator']}")
                                            else:
                                                rule_errors.append(f"Condition {j} missing 'operator' field")
                                            
                                            if "value" in condition:
                                                rule_quality += 0.5
                                            else:
                                                rule_warnings.append(f"Condition {j} missing 'value' field")
                                        else:
                                            rule_errors.append(f"Condition {j} must be an object")
                                else:
                                    rule_errors.append(f"'{logical_op}' operator must contain an array of conditions")
                else:
                    rule_errors.append("Conditions must contain 'all', 'any', or 'not' operator")
            else:
                rule_errors.append("Conditions must be an object")
        
        # Event validation
        if "event" in rule:
            event = rule["event"]
            if isinstance(event, dict):
                rule_quality += 1
                
                if "type" in event:
                    rule_quality += 1
                else:
                    rule_warnings.append("Event missing 'type' field")
                
                if "params" in event:
                    rule_quality += 1
                    params = event["params"]
                    if isinstance(params, dict) and len(params) > 0:
                        rule_quality += 1
            else:
                rule_errors.append("Event must be an object")
        
        # Priority validation
        if "priority" in rule:
            priority = rule["priority"]
            if isinstance(priority, (int, float)):
                if 0 <= priority <= 100:
                    rule_quality += 1
                else:
                    rule_warnings.append("Priority should be between 0 and 100")
            else:
                rule_errors.append("Priority must be a number")
        else:
            rule_recommendations.append("Consider adding priority field for better rule ordering")
        
        # Metadata validation (if present)
        if "metadata" in rule:
            metadata = rule["metadata"]
            if isinstance(metadata, dict):
                rule_quality += 0.5
                if len(metadata) > 0:
                    rule_quality += 0.5
        
        # Additional quality checks for comprehensive validation
        if validation_depth == "comprehensive":
            # Check for meaningful fact names
            if "conditions" in rule:
                conditions = rule["conditions"]
                fact_names = []
                
                def extract_facts(cond_obj):
                    if isinstance(cond_obj, dict):
                        if "fact" in cond_obj:
                            fact_names.append(cond_obj["fact"])
                        for key, value in cond_obj.items():
                            if key in ["all", "any"]:
                                if isinstance(value, list):
                                    for item in value:
                                        extract_facts(item)
                            elif key == "not":
                                extract_facts(value)
                
                extract_facts(conditions)
                
                if fact_names:
                    # Check for descriptive fact names
                    descriptive_facts = [f for f in fact_names if len(f) > 5 and '_' in f]
                    if descriptive_facts:
                        rule_quality += 1
                        rule_recommendations.append("Good use of descriptive fact names")
                    else:
                        rule_recommendations.append("Consider using more descriptive fact names")
            
            # Check for rule complexity balance
            if "conditions" in rule:
                condition_count = 0
                
                def count_conditions(cond_obj):
                    nonlocal condition_count
                    if isinstance(cond_obj, dict):
                        if "fact" in cond_obj:
                            condition_count += 1
                        for key, value in cond_obj.items():
                            if key in ["all", "any"]:
                                if isinstance(value, list):
                                    for item in value:
                                        count_conditions(item)
                            elif key == "not":
                                count_conditions(value)
                
                count_conditions(rule["conditions"])
                
                if 1 <= condition_count <= 5:
                    rule_quality += 1
                elif condition_count > 5:
                    rule_recommendations.append("Consider simplifying complex conditions")
                elif condition_count == 0:
                    rule_warnings.append("No conditions found in rule")
        
        # Compile rule analysis
        rule_analysis = {
            "rule_index": i,
            "valid": len(rule_errors) == 0,
            "quality_score": min(10.0, rule_quality),
            "errors": rule_errors,
            "warnings": rule_warnings,
            "recommendations": rule_recommendations
        }
        
        validation_report["rule_analysis"].append(rule_analysis)
        validation_report["errors"].extend(rule_errors)
        validation_report["warnings"].extend(rule_warnings)
        validation_report["recommendations"].extend(rule_recommendations)
        
        quality_points += rule_quality
        
        if rule_errors:
            validation_report["valid"] = False
    
    # Calculate overall quality score
    if max_possible_points > 0:
        validation_report["quality_score"] = min(100.0, (quality_points / max_possible_points) * 100)
    
    # Add summary statistics
    validation_report["summary"] = {
        "total_errors": len(validation_report["errors"]),
        "total_warnings": len(validation_report["warnings"]),
        "total_recommendations": len(validation_report["recommendations"]),
        "valid_rules": sum(1 for r in validation_report["rule_analysis"] if r["valid"]),
        "average_rule_quality": sum(r["quality_score"] for r in validation_report["rule_analysis"]) / len(json_rules) if json_rules else 0
    }
    
    return validation_report


# Advanced o3-mini Agent with Mixture of Thought + Mixture of Chains + Chain of Thought

def create_advanced_o3_mini_agent():
    """
    Create a LangGraph agent using o3-mini with advanced prompting strategies:
    - Mixture of Thought: Multiple reasoning pathways
    - Mixture of Chains: Different processing chains for different aspects
    - Chain of Thought: Step-by-step reasoning within each chain
    """
    
    # Initialize o3-mini with optimized reasoning configuration
    model = ChatOpenAI(
        model="o3-mini",
        temperature=0,
        model_kwargs={
            "reasoning_effort": "medium"
        }
    )
    
    # Available tools for advanced processing
    tools = [
        analyze_legislation_pathway,
        extract_rules_chain,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    
    # Advanced system message combining all three techniques
    system_message = """You are an elite legal analyst and rules engineer with advanced reasoning capabilities, specializing in converting legislation into precise machine-readable JSON rules using cutting-edge AI reasoning techniques.

## REASONING FRAMEWORK: MIXTURE OF THOUGHT + MIXTURE OF CHAINS + CHAIN OF THOUGHT

### MIXTURE OF THOUGHT APPROACH:
Explore multiple reasoning pathways simultaneously:
1. **Structural Pathway**: Analyze hierarchy, organization, and document structure
2. **Semantic Pathway**: Focus on meaning, definitions, and legal intent
3. **Logical Pathway**: Examine rules, conditions, and logical relationships
4. **Contextual Pathway**: Identify actors, actions, and temporal elements

Each pathway offers unique insights. DO NOT settle for a single perspective.

### MIXTURE OF CHAINS STRATEGY:
Process different aspects through specialized chains:
- **Chain 1 (Analysis)**: Multi-pathway legislation analysis
- **Chain 2 (Extraction)**: Focused rule extraction by type
- **Chain 3 (Synthesis)**: Cross-pathway rule synthesis
- **Chain 4 (Conversion)**: JSON rules generation
- **Chain 5 (Validation)**: Comprehensive rule validation

### CHAIN OF THOUGHT EXECUTION:
Within each chain, think step-by-step:
1. **Context Assessment**: What am I analyzing and why?
2. **Pattern Recognition**: What patterns and structures do I see?
3. **Information Extraction**: What specific information can I extract?
4. **Quality Evaluation**: How confident am I in these findings?
5. **Integration Planning**: How does this connect to other insights?

## PROCESSING METHODOLOGY:

**PHASE 1 - DIVERGENT ANALYSIS (Mixture of Thought)**
- Analyze the same legislation through 4 different reasoning pathways
- Each pathway focuses on different aspects but covers the complete text
- Generate diverse insights and perspectives

**PHASE 2 - SPECIALIZED EXTRACTION (Mixture of Chains)**
- Extract requirements, prohibitions, conditions, exceptions, and penalties
- Use chain of thought reasoning within each extraction process
- Maintain context from the multi-pathway analysis

**PHASE 3 - CONVERGENT SYNTHESIS (Advanced Integration)**
- Synthesize findings from all pathways and chains
- Resolve conflicts and redundancies
- Create comprehensive rule set

**PHASE 4 - PRECISION CONVERSION (JSON Translation)**
- Convert synthesized rules to json-rules-engine format
- Ensure technical accuracy and compliance
- Maintain semantic fidelity

**PHASE 5 - RIGOROUS VALIDATION (Quality Assurance)**
- Comprehensive validation of generated rules
- Quality scoring and improvement recommendations
- Final optimization

## QUALITY STANDARDS:
- **Completeness**: Capture all significant legal rules
- **Accuracy**: Maintain precise legal meaning
- **Consistency**: Ensure logical coherence across rules
- **Usability**: Generate executable machine-readable rules
- **Traceability**: Maintain clear reasoning paths

Use the provided tools systematically, leveraging your advanced reasoning to produce comprehensive, high-quality JSON rules that accurately represent the legislation's requirements."""
    
    # Create memory for conversation state
    memory = MemorySaver()
    
    # Create the react agent with advanced capabilities
    agent = create_react_agent(
        model=model,
        tools=tools,
        checkpointer=memory,
        prompt=system_message
    )
    
    return agent


# Main Processing Function

async def process_legislation_advanced(legislation_text: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Process legislation text using advanced o3-mini reasoning with mixture of thought,
    mixture of chains, and chain of thought approaches.
    
    Args:
        legislation_text: The legislation text to process
        config: Optional configuration for the agent
        
    Returns:
        Dictionary containing comprehensive processing results
    """
    
    # Create the advanced o3-mini agent
    agent = create_advanced_o3_mini_agent()
    
    # Default config
    if config is None:
        config = {"configurable": {"thread_id": "advanced_o3_mini_session"}}
    
    # Create comprehensive prompt using all three techniques
    prompt = f"""Process the following legislation using the advanced reasoning framework (Mixture of Thought + Mixture of Chains + Chain of Thought):

LEGISLATION TEXT:
{legislation_text}

## PROCESSING INSTRUCTIONS:

**PHASE 1 - DIVERGENT ANALYSIS (Mixture of Thought)**
Analyze this legislation through multiple reasoning pathways:

1. Use `analyze_legislation_pathway` with pathway="structural" to understand document organization
2. Use `analyze_legislation_pathway` with pathway="semantic" to extract meaning and definitions  
3. Use `analyze_legislation_pathway` with pathway="logical" to identify rule structures
4. Use `analyze_legislation_pathway` with pathway="contextual" to understand actors and actions

**PHASE 2 - SPECIALIZED EXTRACTION (Mixture of Chains)**
For each rule type, use `extract_rules_chain` with the analysis context:

1. Extract requirements (obligations, duties, mandates)
2. Extract prohibitions (forbidden actions, restrictions)
3. Extract conditions (if-then structures, triggers)
4. Extract exceptions (exemptions, special cases)
5. Extract penalties (sanctions, consequences)

**PHASE 3 - CONVERGENT SYNTHESIS**
Use `synthesize_rules` to combine findings from all pathways using an appropriate synthesis strategy.

**PHASE 4 - PRECISION CONVERSION**
Use `convert_to_json_rules` to generate json-rules-engine compatible rules.

**PHASE 5 - RIGOROUS VALIDATION**
Use `validate_json_rules` with validation_depth="comprehensive" for quality assurance.

## REASONING REQUIREMENTS:
- Think step-by-step within each tool usage
- Explain your reasoning for tool selection and parameter choices
- Integrate insights across different pathways and chains
- Ensure comprehensive coverage of all legislative elements
- Maintain high quality standards throughout

Provide detailed reasoning for each phase and explain how the different techniques contribute to the final result."""

    # Process with advanced reasoning
    response = await agent.ainvoke(
        {"messages": [HumanMessage(content=prompt)]},
        config=config
    )
    
    return {
        "status": "completed",
        "model_used": "o3-mini",
        "reasoning_framework": "mixture_of_thought_chains_cot",
        "original_text": legislation_text,
        "messages": response["messages"],
        "processing_phases": [
            "divergent_analysis",
            "specialized_extraction", 
            "convergent_synthesis",
            "precision_conversion",
            "rigorous_validation"
        ]
    }


# Utility Functions for Rule Management

def save_rules_to_file(rules: List[Dict[str, Any]], filename: str, include_metadata: bool = True):
    """Save JSON rules to a file with optional metadata"""
    output_data = {
        "rules": rules,
        "metadata": {
            "created_by": "advanced_o3_mini_agent",
            "created_at": "2025-08-10",
            "rule_count": len(rules),
            "processing_framework": "mixture_of_thought_chains_cot",
            "engine_compatibility": "json-rules-engine"
        } if include_metadata else {}
    }
    
    with open(filename, 'w') as f:
        json.dump(output_data, f, indent=2)
    print(f" Rules saved to {filename}")


def load_rules_from_file(filename: str) -> List[Dict[str, Any]]:
    """Load JSON rules from a file"""
    try:
        with open(filename, 'r') as f:
            data = json.load(f)
        
        # Handle both formats: direct rules array or wrapped with metadata
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and 'rules' in data:
            return data['rules']
        else:
            raise ValueError("Invalid file format")
    except Exception as e:
        print(f" Error loading rules from {filename}: {e}")
        return []


def create_rules_engine_config(rules: List[Dict[str, Any]], config_name: str = "legislation_rules") -> Dict[str, Any]:
    """Create a configuration object for json-rules-engine"""
    return {
        "name": config_name,
        "rules": rules,
        "metadata": {
            "created_by": "advanced_o3_mini_agent",
            "processing_method": "mixture_of_thought_chains_cot",
            "rule_count": len(rules),
            "engine_version": "json-rules-engine",
            "quality_validated": True
        },
        "settings": {
            "allowUndefinedFacts": False,
            "replaceFactsInEventParams": True,
            "pathResolver": "jsonpath"
        }
    }


def extract_rules_summary(processing_result: Dict[str, Any]) -> Dict[str, Any]:
    """Extract a summary of rules from processing result"""
    summary = {
        "total_rules": 0,
        "rules_by_type": {},
        "rules_by_pathway": {},
        "average_confidence": 0.0,
        "processing_phases": [],
        "quality_score": 0.0
    }
    
    # Extract rules from different sources in the result
    all_rules = []
    
    # From messages (if rules are embedded in conversation)
    if 'messages' in processing_result:
        for msg in processing_result['messages']:
            if hasattr(msg, 'content') and 'json_rules' in str(msg.content).lower():
                # Try to extract JSON from message content
                pass
    
    # From direct rule fields
    for field in ['json_rules', 'final_rules', 'synthesized_rules']:
        if field in processing_result:
            all_rules.extend(processing_result[field])
    
    if all_rules:
        summary["total_rules"] = len(all_rules)
        
        # Analyze by type
        for rule in all_rules:
            rule_type = rule.get('event', {}).get('type', 'unknown')
            summary["rules_by_type"][rule_type] = summary["rules_by_type"].get(rule_type, 0) + 1
            
            # Analyze by pathway
            pathway = rule.get('metadata', {}).get('reasoning_pathway', 'unknown')
            summary["rules_by_pathway"][pathway] = summary["rules_by_pathway"].get(pathway, 0) + 1
        
        # Calculate average confidence
        confidences = []
        for rule in all_rules:
            conf = rule.get('event', {}).get('params', {}).get('confidence', 0)
            if conf > 0:
                confidences.append(conf)
        
        if confidences:
            summary["average_confidence"] = sum(confidences) / len(confidences)
    
    # Extract processing phases
    if 'processing_phases' in processing_result:
        summary["processing_phases"] = processing_result['processing_phases']
    
    return summary


def validate_rule_engine_compatibility(rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Quick validation for json-rules-engine compatibility"""
    compatibility_report = {
        "compatible": True,
        "issues": [],
        "warnings": [],
        "rule_count": len(rules)
    }
    
    required_fields = ["conditions", "event"]
    
    for i, rule in enumerate(rules):
        rule_issues = []
        
        # Check required fields
        for field in required_fields:
            if field not in rule:
                rule_issues.append(f"Rule {i+1}: Missing required field '{field}'")
        
        # Check conditions structure
        if "conditions" in rule:
            conditions = rule["conditions"]
            if not isinstance(conditions, dict):
                rule_issues.append(f"Rule {i+1}: Conditions must be an object")
            else:
                logical_ops = ["all", "any", "not"]
                if not any(op in conditions for op in logical_ops):
                    rule_issues.append(f"Rule {i+1}: Conditions must contain 'all', 'any', or 'not'")
        
        # Check event structure
        if "event" in rule:
            event = rule["event"]
            if not isinstance(event, dict):
                rule_issues.append(f"Rule {i+1}: Event must be an object")
            elif "type" not in event:
                compatibility_report["warnings"].append(f"Rule {i+1}: Event missing 'type' field")
        
        compatibility_report["issues"].extend(rule_issues)
    
    if compatibility_report["issues"]:
        compatibility_report["compatible"] = False
    
    return compatibility_report


# Advanced Rule Processing Classes

class RuleQualityAnalyzer:
    """Analyze and score rule quality"""
    
    def __init__(self):
        self.quality_weights = {
            'completeness': 0.25,
            'clarity': 0.20,
            'consistency': 0.20,
            'accuracy': 0.20,
            'usability': 0.15
        }
    
    def analyze_rule_quality(self, rule: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze the quality of a single rule"""
        scores = {
            'completeness': self._score_completeness(rule),
            'clarity': self._score_clarity(rule),
            'consistency': self._score_consistency(rule),
            'accuracy': self._score_accuracy(rule),
            'usability': self._score_usability(rule)
        }
        
        # Calculate weighted overall score
        overall_score = sum(
            scores[aspect] * self.quality_weights[aspect] 
            for aspect in scores
        )
        
        return {
            'overall_score': overall_score,
            'aspect_scores': scores,
            'grade': self._score_to_grade(overall_score),
            'recommendations': self._generate_recommendations(scores)
        }
    
    def _score_completeness(self, rule: Dict[str, Any]) -> float:
        """Score rule completeness (0-1)"""
        required_fields = ['conditions', 'event', 'priority']
        optional_fields = ['metadata']
        
        score = 0.0
        
        # Check required fields (80% of score)
        for field in required_fields:
            if field in rule:
                score += 0.8 / len(required_fields)
        
        # Check optional fields (20% of score)
        for field in optional_fields:
            if field in rule:
                score += 0.2 / len(optional_fields)
        
        return min(1.0, score)
    
    def _score_clarity(self, rule: Dict[str, Any]) -> float:
        """Score rule clarity and readability (0-1)"""
        score = 0.0
        
        # Check if event has descriptive type
        event_type = rule.get('event', {}).get('type', '')
        if event_type and len(event_type) > 5:
            score += 0.3
        
        # Check if conditions use meaningful fact names
        conditions = rule.get('conditions', {})
        fact_names = self._extract_fact_names(conditions)
        if fact_names:
            meaningful_facts = [f for f in fact_names if len(f) > 5 and ('_' in f or any(c.isupper() for c in f[1:]))]
            score += 0.4 * (len(meaningful_facts) / len(fact_names))
        
        # Check for metadata documentation
        metadata = rule.get('metadata', {})
        if metadata and len(metadata) > 2:
            score += 0.3
        
        return min(1.0, score)
    
    def _score_consistency(self, rule: Dict[str, Any]) -> float:
        """Score internal consistency (0-1)"""
        score = 1.0  # Start with perfect score, deduct for issues
        
        # Check priority range
        priority = rule.get('priority', 50)
        if not isinstance(priority, (int, float)) or priority < 0 or priority > 100:
            score -= 0.3
        
        # Check condition-event alignment
        event_type = rule.get('event', {}).get('type', '')
        conditions = rule.get('conditions', {})
        
        # Simple heuristic: event type should relate to condition facts
        fact_names = self._extract_fact_names(conditions)
        if fact_names and event_type:
            # Check if event type semantically relates to facts
            related_terms = any(term in event_type.lower() for fact in fact_names for term in fact.lower().split('_'))
            if not related_terms:
                score -= 0.2
        
        return max(0.0, score)
    
    def _score_accuracy(self, rule: Dict[str, Any]) -> float:
        """Score technical accuracy (0-1)"""
        score = 1.0
        
        # Check operator validity
        valid_operators = ["equal", "notEqual", "lessThan", "lessThanInclusive", 
                          "greaterThan", "greaterThanInclusive", "in", "notIn", 
                          "contains", "doesNotContain", "regex"]
        
        operators_used = self._extract_operators(rule.get('conditions', {}))
        invalid_operators = [op for op in operators_used if op not in valid_operators]
        
        if invalid_operators:
            score -= 0.5 * (len(invalid_operators) / len(operators_used))
        
        # Check for proper condition structure
        conditions = rule.get('conditions', {})
        if not self._validate_condition_structure(conditions):
            score -= 0.3
        
        return max(0.0, score)
    
    def _score_usability(self, rule: Dict[str, Any]) -> float:
        """Score practical usability (0-1)"""
        score = 0.0
        
        # Check for reasonable complexity
        condition_count = self._count_conditions(rule.get('conditions', {}))
        if 1 <= condition_count <= 5:
            score += 0.4
        elif condition_count > 5:
            score += 0.2  # Too complex
        
        # Check for actionable events
        event_params = rule.get('event', {}).get('params', {})
        if event_params and len(event_params) > 0:
            score += 0.3
        
        # Check for confidence information
        confidence = rule.get('event', {}).get('params', {}).get('confidence', 0)
        if confidence > 0:
            score += 0.3
        
        return min(1.0, score)
    
    def _extract_fact_names(self, conditions: Dict[str, Any]) -> List[str]:
        """Extract fact names from conditions"""
        facts = []
        
        def extract_recursive(obj):
            if isinstance(obj, dict):
                if 'fact' in obj:
                    facts.append(obj['fact'])
                for value in obj.values():
                    extract_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_recursive(item)
        
        extract_recursive(conditions)
        return facts
    
    def _extract_operators(self, conditions: Dict[str, Any]) -> List[str]:
        """Extract operators from conditions"""
        operators = []
        
        def extract_recursive(obj):
            if isinstance(obj, dict):
                if 'operator' in obj:
                    operators.append(obj['operator'])
                for value in obj.values():
                    extract_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_recursive(item)
        
        extract_recursive(conditions)
        return operators
    
    def _validate_condition_structure(self, conditions: Dict[str, Any]) -> bool:
        """Validate condition structure"""
        if not isinstance(conditions, dict):
            return False
        
        logical_ops = ["all", "any", "not"]
        return any(op in conditions for op in logical_ops)
    
    def _count_conditions(self, conditions: Dict[str, Any]) -> int:
        """Count individual conditions"""
        count = 0
        
        def count_recursive(obj):
            nonlocal count
            if isinstance(obj, dict):
                if 'fact' in obj and 'operator' in obj:
                    count += 1
                for value in obj.values():
                    count_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    count_recursive(item)
        
        count_recursive(conditions)
        return count
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def _generate_recommendations(self, scores: Dict[str, float]) -> List[str]:
        """Generate improvement recommendations"""
        recommendations = []
        
        if scores['completeness'] < 0.8:
            recommendations.append("Add missing required fields (priority, metadata)")
        
        if scores['clarity'] < 0.7:
            recommendations.append("Use more descriptive fact names and event types")
        
        if scores['consistency'] < 0.8:
            recommendations.append("Ensure conditions and events are semantically aligned")
        
        if scores['accuracy'] < 0.9:
            recommendations.append("Verify operators and condition structure")
        
        if scores['usability'] < 0.7:
            recommendations.append("Simplify complex conditions and add confidence scores")
        
        return recommendations


class LegislationRulesTester:
    """Enhanced testing framework for generated rules"""
    
    def __init__(self, rules: List[Dict[str, Any]]):
        self.rules = rules
        self.quality_analyzer = RuleQualityAnalyzer()
    
    def create_comprehensive_test_facts(self) -> List[Dict[str, Any]]:
        """Create comprehensive test facts covering different scenarios"""
        return [
            # Requirement testing
            {
                "subject_type": "person",
                "compliance_required": True,
                "action_required": True,
                "context": "requirement_scenario"
            },
            # Prohibition testing
            {
                "subject_type": "vehicle",
                "prohibited_action_attempted": True,
                "violation_detected": True,
                "context": "prohibition_scenario"
            },
            # Condition testing
            {
                "trigger_condition": "emergency response",
                "condition_met": True,
                "special_circumstances": True,
                "context": "conditional_scenario"
            },
            # Exception testing
            {
                "exception_condition": "emergency vehicle",
                "standard_rule_applies": False,
                "override_conditions": True,
                "context": "exception_scenario"
            },
            # Penalty testing
            {
                "violation_detected": True,
                "violation_type": "speed limit exceeded",
                "enforcement_active": True,
                "context": "penalty_scenario"
            }
        ]
    
    def test_rules_comprehensive(self) -> Dict[str, Any]:
        """Comprehensive testing of rules"""
        test_facts = self.create_comprehensive_test_facts()
        results = {
            "test_summary": {
                "total_rules": len(self.rules),
                "total_test_scenarios": len(test_facts),
                "rules_triggered": 0,
                "average_quality": 0.0
            },
            "rule_tests": [],
            "scenario_tests": [],
            "quality_analysis": [],
            "recommendations": []
        }
        
        # Test each rule
        for i, rule in enumerate(self.rules):
            rule_test = {
                "rule_index": i,
                "rule_id": rule.get('metadata', {}).get('rule_id', f'rule_{i+1}'),
                "scenarios_triggered": 0,
                "quality_score": 0.0,
                "test_results": []
            }
            
            # Quality analysis
            quality_analysis = self.quality_analyzer.analyze_rule_quality(rule)
            rule_test["quality_score"] = quality_analysis["overall_score"]
            results["quality_analysis"].append(quality_analysis)
            
            # Test against scenarios
            for j, facts in enumerate(test_facts):
                triggered = self._evaluate_rule_simple(rule, facts)
                rule_test["test_results"].append({
                    "scenario": j + 1,
                    "facts": facts["context"],
                    "triggered": triggered
                })
                
                if triggered:
                    rule_test["scenarios_triggered"] += 1
            
            results["rule_tests"].append(rule_test)
            if rule_test["scenarios_triggered"] > 0:
                results["test_summary"]["rules_triggered"] += 1
        
        # Test each scenario
        for j, facts in enumerate(test_facts):
            scenario_test = {
                "scenario": j + 1,
                "context": facts["context"],
                "rules_triggered": 0,
                "triggered_rules": []
            }
            
            for i, rule in enumerate(self.rules):
                if self._evaluate_rule_simple(rule, facts):
                    scenario_test["rules_triggered"] += 1
                    scenario_test["triggered_rules"].append(i + 1)
            
            results["scenario_tests"].append(scenario_test)
        
        # Calculate averages
        if results["quality_analysis"]:
            avg_quality = sum(qa["overall_score"] for qa in results["quality_analysis"]) / len(results["quality_analysis"])
            results["test_summary"]["average_quality"] = avg_quality
        
        # Generate recommendations
        results["recommendations"] = self._generate_test_recommendations(results)
        
        return results
    
    def _evaluate_rule_simple(self, rule: Dict[str, Any], facts: Dict[str, Any]) -> bool:
        """Simplified rule evaluation for testing"""
        conditions = rule.get("conditions", {})
        
        if "all" in conditions:
            return all(self._evaluate_condition_simple(cond, facts) for cond in conditions["all"])
        elif "any" in conditions:
            return any(self._evaluate_condition_simple(cond, facts) for cond in conditions["any"])
        elif "not" in conditions:
            return not self._evaluate_condition_simple(conditions["not"], facts)
        
        return False
    
    def _evaluate_condition_simple(self, condition: Dict[str, Any], facts: Dict[str, Any]) -> bool:
        """Simplified condition evaluation"""
        fact_name = condition.get("fact")
        operator = condition.get("operator")
        expected_value = condition.get("value")
        
        if fact_name not in facts:
            return False
        
        fact_value = facts[fact_name]
        
        if operator == "equal":
            return fact_value == expected_value
        elif operator == "contains":
            return str(expected_value).lower() in str(fact_value).lower()
        elif operator == "greaterThan":
            return fact_value > expected_value
        elif operator == "lessThan":
            return fact_value < expected_value
        
        return False
    
    def _generate_test_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generate testing recommendations"""
        recommendations = []
        
        rules_triggered = results["test_summary"]["rules_triggered"]
        total_rules = results["test_summary"]["total_rules"]
        avg_quality = results["test_summary"]["average_quality"]
        
        if rules_triggered == 0:
            recommendations.append(" No rules triggered in testing - check fact names and condition logic")
        elif rules_triggered < total_rules * 0.5:
            recommendations.append(" Less than 50% of rules triggered - consider adding more diverse test scenarios")
        
        if avg_quality < 0.7:
            recommendations.append(" Average rule quality below 70% - focus on improving rule clarity and completeness")
        
        # Analyze quality distribution
        quality_scores = [qa["overall_score"] for qa in results["quality_analysis"]]
        if quality_scores:
            low_quality_rules = sum(1 for score in quality_scores if score < 0.6)
            if low_quality_rules > 0:
                recommendations.append(f" {low_quality_rules} rules have quality scores below 60% - prioritize improvements")
        
        return recommendations
    """
    Synchronous wrapper for advanced o3-mini legislation processing.
    
    Args:
        legislation_text: The legislation text to process
        config: Optional configuration for the agent
        
    Returns:
        Dictionary containing the processed results
    """
    
    async def _process_async():
        return await process_legislation_advanced(legislation_text, config)
    
    # Handle event loop management
    try:
        loop = asyncio.get_running_loop()
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, _process_async())
            return future.result()
    except RuntimeError:
        return asyncio.run(_process_async())


# Main execution with comprehensive capabilities
if __name__ == "__main__":
    print(" ADVANCED o3-mini LEGISLATION PROCESSOR")
    print("=" * 60)
    print(" Reasoning: Mixture of Thought + Mixture of Chains + Chain of Thought")
    print(" State Management: Comprehensive with Quality Analysis")
    print(" Features: Full Rule Testing & Validation Framework")
    print()
    
    # Run comprehensive demo
    result = run_comprehensive_demo()
    
    if result:
        print("\n" + "=" * 60)
        print(" TESTING ADVANCED CAPABILITIES")
        print("=" * 60)
        
        # Test quality analysis
        quality_result = test_rule_quality_analysis()
        
        # Test comprehensive testing framework
        testing_result = test_comprehensive_rule_testing()
        
        # Test utility functions
        utility_result = demo_utility_functions()
        
        print("\n" + "=" * 60)
        print(" COMPREHENSIVE DEMO COMPLETED")
        print("=" * 60)
        print(" All systems operational!")
        print(f" Quality Analysis: {' Passed' if quality_result else ' Failed'}")
        print(f" Rule Testing: {' Passed' if testing_result else ' Failed'}")
        print(f" Utilities: {' Passed' if all(utility_result.values()) else ' Failed'}")
        
        print(f"\n Quick Start Commands:")
        print(f"    process_legislation_sync('Your text here...')")
        print(f"    run_comprehensive_demo()")
        print(f"    test_rule_quality_analysis()")
        print(f"    test_comprehensive_rule_testing()")
    else:
        print(" Demo failed - check o3-mini API access")


"""
COMPREHENSIVE o3-mini LEGISLATION PROCESSOR

 ADVANCED REASONING FRAMEWORK:
    Mixture of Thought: Multi-pathway analysis (Structural, Semantic, Logical, Contextual)
    Mixture of Chains: Specialized processing chains for different aspects
    Chain of Thought: Step-by-step reasoning within each process

 COMPREHENSIVE STATE MANAGEMENT:
    Full AgentState with all processing stages
    LegislationRule dataclass with reasoning traces
    ProcessingPhase and ReasoningPathway enums
    Complete metadata and context tracking

 ADVANCED FEATURES:
    RuleQualityAnalyzer: Comprehensive rule quality scoring
    LegislationRulesTester: Full testing framework with scenarios
    Utility functions: Save/load, config generation, validation
    Compatibility checking for json-rules-engine

## USAGE EXAMPLES:

### Basic Processing:
```python
result = process_legislation_sync("Your legislation text here...")
```

### Comprehensive Demo:
```python
run_comprehensive_demo()  # Full feature demonstration
```

### Quality Analysis:
```python
analyzer = RuleQualityAnalyzer()
quality = analyzer.analyze_rule_quality(rule)
print(f"Quality Score: {quality['overall_score']}")
```

### Rule Testing:
```python
tester = LegislationRulesTester(rules)
results = tester.test_rules_comprehensive()
```

### Utility Functions:
```python
save_rules_to_file(rules, "output.json")
config = create_rules_engine_config(rules)
compatibility = validate_rule_engine_compatibility(rules)
```

## PROCESSING PHASES:
1. **Initialization**: Setup and context preparation
2. **Divergent Analysis**: Multi-pathway exploration
3. **Specialized Extraction**: Focused rule extraction
4. **Convergent Synthesis**: Cross-pathway integration
5. **Precision Conversion**: JSON rules generation
6. **Rigorous Validation**: Quality assurance

## REQUIREMENTS:
- OpenAI API key with o3-mini access
- pip install langgraph langchain-openai langchain-core

## SETUP:
export OPENAI_API_KEY="your-openai-api-key"

This system provides the most advanced legislation processing capabilities
with comprehensive state management, quality analysis, and testing frameworks.
"""
