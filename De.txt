"""
Confidence evaluation service for assessing match quality between business terms and PBTs.
"""

import logging
import json
import uuid
import asyncio
import time
from typing import Dict, Any, Optional, Union
from pydantic import BaseModel

from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import empty_checkpoint

from app.config.settings import get_settings
from app.config.environment import get_os_env
from app.core.auth.auth_helper import get_azure_token_cached, refresh_token_if_needed
from app.core.models.pbt import ConfidenceScore, MatchedPBT

logger = logging.getLogger(__name__)

class ConfidenceService:
    """Service for evaluating confidence of matches between business terms and PBTs."""
    
    _instance = None
    
    def __new__(cls):
        """Implement singleton pattern."""
        if cls._instance is None:
            cls._instance = super(ConfidenceService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize the confidence service."""
        if self._initialized:
            return
            
        self._initialized = True
        self.settings = get_settings()
        self.env = get_os_env()
        
        # LLM for confidence evaluation
        self.llm = self._init_llm()
        
        # Chain for confidence evaluation
        self.chain = self._create_confidence_chain()
        
        # Memory for caching confidence evaluations
        self.memory_saver = MemorySaver()
        
        # In-memory cache for fast lookups
        self.cache = {}
        
        logger.info("Confidence service initialized")
    
    def _init_llm(self) -> AzureChatOpenAI:
        """Initialize the LLM for confidence evaluation."""
        try:
            # Get Azure token
            token = get_azure_token_cached(
                tenant_id=self.settings.azure.tenant_id,
                client_id=self.settings.azure.client_id,
                client_secret=self.settings.azure.client_secret,
                scope="https://cognitiveservices.azure.com/.default"
            )
            
            if not token:
                logger.error("Failed to get Azure token for confidence service")
                raise ValueError("Failed to get Azure token")
            
            # Create token provider function
            token_provider = lambda: get_azure_token_cached(
                tenant_id=self.settings.azure.tenant_id,
                client_id=self.settings.azure.client_id,
                client_secret=self.settings.azure.client_secret,
                scope="https://cognitiveservices.azure.com/.default"
            ) or token
            
            # Initialize Azure OpenAI client for LLM
            llm = AzureChatOpenAI(
                model_name=self.settings.azure.openai_model_name,  # Use the renamed field
                temperature=0.2,  # Lower temperature for more consistent confidence scores
                api_version=self.settings.azure.api_version,
                azure_endpoint=self.settings.azure.azure_endpoint,
                azure_ad_token_provider=token_provider
            )
            
            return llm
        
        except Exception as e:
            logger.error(f"Error initializing LLM for confidence evaluation: {e}")
            raise
    
    def _create_confidence_chain(self):
        """Create the confidence evaluation chain."""
        # Define prompt for confidence evaluation
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
            You are an expert system that evaluates the confidence of matches between user-provided terms and standard
            Preferred Business Terms (PBT). Analyze the semantic similarity, contextual relevance, and overall 
            appropriateness of the match.
            
            Provide a confidence score between 0 and 100, where:
            - 0-20: Very low confidence. The match seems arbitrary or incorrect.
            - 21-40: Low confidence. There's a vague relationship but likely not the best match.
            - 41-60: Moderate confidence. There's a reasonable connection but potentially better alternatives.
            - 61-80: High confidence. The match is strong and likely appropriate.
            - 81-100: Very high confidence. The match is excellent and almost certainly correct.
            
            Return your evaluation as a JSON object with the following fields:
            - score: (number between 0-100)
            - explanation: (string explaining your reasoning)
            """),
            HumanMessage(content="""
            User Input: {user_input}
            
            Matched PBT: 
            ID: {pbt_id}
            Name: {pbt_name}
            Definition: {pbt_definition}
            CDM: {pbt_cdm}
            
            Additional Information:
            - Match Type: {match_type}
            - Was matched via synonym: {synonym_match}
            - Matched synonym (if any): {matched_synonym}
            - Similarity Score: {similarity_score}
            
            Evaluate the confidence of this match.
            """)
        ])
        
        # Create the chain
        return prompt | self.llm
    
    def _get_memory_key(self, user_input: str, pbt_id: str) -> str:
        """Generate a consistent memory key for retrieving past evaluations."""
        # Normalize input to ensure consistent keys
        normalized_input = user_input.strip().lower()
        normalized_id = str(pbt_id).strip()
        return f"confidence:{normalized_input}:{normalized_id}"
    
    async def evaluate_confidence(self, user_input: str, 
                                 pbt_match: Union[Dict[str, Any], MatchedPBT]) -> ConfidenceScore:
        """
        Evaluate the confidence of a match between user input and a PBT.
        
        Args:
            user_input: User input (name and description)
            pbt_match: Matched PBT (either a dict or MatchedPBT object)
            
        Returns:
            ConfidenceScore with score and explanation
        """
        try:
            # Extract fields based on the match object type
            pbt_id = None
            pbt_name = None
            pbt_definition = None
            pbt_cdm = None
            match_type = None
            similarity_score = None
            synonym_match = False
            matched_synonym = None
            
            # Extract fields from input based on its type
            if isinstance(pbt_match, MatchedPBT):
                pbt_id = pbt_match.id
                pbt_name = pbt_match.name  # Uses the property to get PBT_NAME
                pbt_definition = pbt_match.definition  # Uses the property to get PBT_DEFINITION
                pbt_cdm = pbt_match.CDM
                match_type = pbt_match.match_type
                similarity_score = pbt_match.similarity_score
                synonym_match = pbt_match.synonym_match
                matched_synonym = pbt_match.matched_synonym
            elif isinstance(pbt_match, dict):
                pbt_id = pbt_match.get("id")
                # Try both field naming conventions
                pbt_name = pbt_match.get("PBT_NAME", pbt_match.get("name"))
                pbt_definition = pbt_match.get("PBT_DEFINITION", pbt_match.get("definition"))
                pbt_cdm = pbt_match.get("CDM", pbt_match.get("cdm"))
                match_type = pbt_match.get("match_type", "specific")
                similarity_score = pbt_match.get("similarity_score", 0.5)
                synonym_match = pbt_match.get("synonym_match", False)
                matched_synonym = pbt_match.get("matched_synonym")
            else:
                logger.error(f"Invalid PBT match type: {type(pbt_match)}")
                return ConfidenceScore(
                    score=50,
                    explanation=f"Error: Invalid PBT match type: {type(pbt_match)}"
                )
            
            # Validate required fields
            if not pbt_id or not pbt_name or not pbt_definition:
                logger.error(f"Missing required fields in PBT match: {pbt_match}")
                
                # Use default values for missing fields rather than failing
                if not pbt_id:
                    pbt_id = "unknown_id"
                if not pbt_name:
                    pbt_name = "Unknown Term"
                if not pbt_definition:
                    pbt_definition = "No definition available"
            
            # Generate a memory key for this evaluation
            memory_key = self._get_memory_key(user_input, str(pbt_id))
            
            # Check in-memory cache first
            if memory_key in self.cache:
                logger.info(f"Found cached confidence evaluation in memory: {memory_key}")
                return self.cache[memory_key]
            
            # Create a session ID for memory storage
            session_id = "confidence_evaluations"
            
            # Create a proper config object with thread_id and checkpoint_ns
            config = {
                "configurable": {
                    "thread_id": session_id,
                    "checkpoint_ns": ""
                }
            }
            
            # Try to get from persistent memory
            try:
                checkpoint = self.memory_saver.get(config)
                
                if checkpoint and memory_key in checkpoint.get("channel_values", {}):
                    cached_result = checkpoint["channel_values"][memory_key]
                    logger.info(f"Found cached confidence evaluation in persistent memory: {memory_key}")
                    
                    # Parse the cached result
                    score = cached_result.get("score", 0)
                    explanation = cached_result.get("explanation", "")
                    
                    # Create ConfidenceScore and cache it
                    confidence_score = ConfidenceScore(score=score, explanation=explanation)
                    self.cache[memory_key] = confidence_score
                    
                    return confidence_score
            except Exception as mem_error:
                logger.warning(f"Error retrieving from memory: {mem_error}")
            
            # Ensure we have valid token before calling LLM
            refresh_token_if_needed(
                tenant_id=self.settings.azure.tenant_id,
                client_id=self.settings.azure.client_id,
                client_secret=self.settings.azure.client_secret,
                scope="https://cognitiveservices.azure.com/.default",
                min_validity_seconds=600  # 10 minutes
            )
            
            # Make sure token is valid and LLM is properly initialized
            self.llm = self._init_llm()
            self.chain = self._create_confidence_chain()
            
            # Call the LLM chain for confidence evaluation with retry mechanism
            max_retries = 3
            retry_delay = 2  # seconds
            
            for attempt in range(max_retries):
                try:
                    # Call LLM with timeout
                    llm_response = await asyncio.wait_for(
                        self.chain.ainvoke({
                            "user_input": user_input,
                            "pbt_id": pbt_id,
                            "pbt_name": pbt_name,
                            "pbt_definition": pbt_definition,
                            "pbt_cdm": pbt_cdm or "N/A",
                            "match_type": match_type,
                            "synonym_match": "Yes" if synonym_match else "No",
                            "matched_synonym": matched_synonym or "N/A",
                            "similarity_score": f"{similarity_score:.2f}" if similarity_score is not None else "N/A"
                        }),
                        timeout=30.0  # 30 second timeout
                    )
                    
                    # Extract content from response
                    content = llm_response.content
                    break  # Success, exit retry loop
                except asyncio.TimeoutError:
                    logger.warning(f"LLM call timed out on attempt {attempt+1}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                    else:
                        # Fallback on timeout
                        logger.error("LLM confidence evaluation timed out after all retries")
                        return ConfidenceScore(
                            score=50,
                            explanation="Confidence evaluation timed out. Using default medium confidence."
                        )
                except Exception as llm_error:
                    logger.warning(f"LLM call failed on attempt {attempt+1}: {llm_error}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                        # Reinitialize LLM
                        self.llm = self._init_llm()
                        self.chain = self._create_confidence_chain()
                    else:
                        # Fallback on error
                        logger.error(f"LLM confidence evaluation failed after all retries: {llm_error}")
                        return ConfidenceScore(
                            score=50,
                            explanation=f"Error evaluating confidence: {str(llm_error)}. Using default medium confidence."
                        )
            
            # Parse the JSON response
            try:
                # Try direct JSON parsing first
                result = json.loads(content)
            except json.JSONDecodeError:
                # If that fails, try to extract JSON using regex
                import re
                json_pattern = r'\{.*\}'
                match = re.search(json_pattern, content, re.DOTALL)
                
                if match:
                    try:
                        result = json.loads(match.group(0))
                    except json.JSONDecodeError:
                        # If regex extraction fails, create a default result
                        result = {
                            "score": 50,
                            "explanation": "Could not parse confidence score from LLM output. Using default medium confidence."
                        }
                else:
                    # If no JSON-like structure found, create a default result
                    result = {
                        "score": 50,
                        "explanation": "Could not parse confidence score from LLM output. Using default medium confidence."
                    }
            
            # Ensure the result has the expected fields
            score = result.get("score", result.get("confidence_score", 50))
            if isinstance(score, str):
                try:
                    score = int(score)
                except ValueError:
                    score = 50
            
            # Clamp the score to valid range
            score = max(0, min(100, score))
            
            explanation = result.get("explanation", "No explanation provided")
            
            # Create a ConfidenceScore object
            confidence_score = ConfidenceScore(score=score, explanation=explanation)
            
            # Save to memory
            try:
                # Create or update the checkpoint
                checkpoint = self.memory_saver.get(config) or empty_checkpoint()
                checkpoint["channel_values"][memory_key] = {
                    "score": score,
                    "explanation": explanation
                }
                
                # Use the put method with the correct signature
                self.memory_saver.put(
                    config=config,
                    checkpoint=checkpoint,
                    metadata={"source": "input", "step": -1},
                    new_versions={}
                )
                
                logger.info(f"Saved confidence evaluation to memory: {memory_key}")
            except Exception as mem_error:
                logger.warning(f"Error saving to memory: {mem_error}")
            
            # Cache the result in memory
            self.cache[memory_key] = confidence_score
            
            # Prune cache if too large
            if len(self.cache) > 1000:
                # Remove 20% of entries (oldest first)
                items_to_remove = sorted(self.cache.keys())[:int(len(self.cache) * 0.2)]
                for key in items_to_remove:
                    del self.cache[key]
            
            return confidence_score
            
        except Exception as e:
            logger.error(f"Error in confidence evaluation: {e}")
            # Return a default confidence score rather than failing completely
            return ConfidenceScore(
                score=50,
                explanation=f"Error evaluating confidence: {str(e)}. Using default medium confidence."
            )
    
    def clear_cache(self):
        """Clear the in-memory cache."""
        self.cache = {}
        logger.info("Cleared confidence evaluation cache")


# Get the confidence service instance
def get_confidence_service() -> ConfidenceService:
    """
    Get the confidence service instance.
    
    Returns:
        ConfidenceService: Confidence service instance
    """
    return ConfidenceService()
