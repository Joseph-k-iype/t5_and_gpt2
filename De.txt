"""
Legal Document Analyzer - COMPLETE FIXED VERSION
- Fixed combined variable scope issue
- Removed temperature parameter
- ALL methods included - no truncation
- Production-ready with comprehensive error handling

Location: src/analyzers/legal_document_analyzer.py
"""

from typing import Dict, List, Optional, Any
import json
from dataclasses import dataclass, field
import logging
import re

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from src.prompting.advanced_strategies import AdvancedPromptingStrategies
from src.services.openai_service import OpenAIService
from src.utils.document_chunker import DocumentChunker
from src.config import Config

logger = logging.getLogger(__name__)


# SIMPLIFIED ACTION TAXONOMY
class DataActionType:
    """Simplified data action taxonomy"""
    DATA_SHARING_AND_ACCESS = "data_sharing_and_access"
    DATA_STORAGE_AND_HOSTING = "data_storage_and_hosting"
    DATA_USAGE = "data_usage"


class RuleClassification:
    """Rule classification"""
    CONDITION = "condition"
    RESTRICTION = "restriction"


@dataclass
class Citation:
    """Citation for extracted information"""
    text_excerpt: str
    chunk_id: int
    document_level: int
    reasoning: str = ""


@dataclass
class Evidence:
    """Evidence supporting a rule or requirement"""
    description: str
    citations: List[Citation] = field(default_factory=list)


class LegalDocumentAnalyzer:
    """
    COMPLETE Legal Document Analyzer
    - No complex workflow graphs
    - Direct sequential processing
    - Robust JSON extraction
    - Clear state management
    - Fixed combined variable issue
    - All methods included
    """
    
    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        if not self.config.API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        self.openai_service = OpenAIService()
        self.llm = ChatOpenAI(
            model=self.config.CHAT_MODEL,
            openai_api_key=self.config.API_KEY,
            openai_api_base=self.config.BASE_URL
        )
        
        chunk_size = getattr(self.config, 'CHUNK_SIZE', 3000)
        overlap_size = getattr(self.config, 'OVERLAP_SIZE', 200)
        
        self.chunker = DocumentChunker(
            chunk_size=chunk_size,
            chunk_overlap=overlap_size,
            respect_boundaries=True
        )
    
    def analyze_document(
        self,
        rule_name: str,
        jurisdiction: str,
        document_text: str,
        level: int,
        enterprise_context: Optional[Dict[str, Any]] = None,
        previous_level_analysis: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Main document analysis method - analyzes document without complex workflow
        """
        print(f"\n{'='*60}")
        print(f"Analyzing: {rule_name} (Level {level})")
        print(f"Document length: {len(document_text)} chars")
        print(f"{'='*60}")
        
        # Initialize strategies
        strategies = AdvancedPromptingStrategies(rule_name, jurisdiction)
        
        # Chunk document
        chunks = self.chunker.chunk_document(
            text=document_text,
            metadata={
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "level": level
            }
        )
        
        print(f"Created {len(chunks)} chunks")
        
        # Process each chunk
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            print(f"\nProcessing chunk {i+1}/{len(chunks)}...")
            
            try:
                analysis = self._analyze_single_chunk(
                    chunk=chunk,
                    strategies=strategies,
                    level=level,
                    enterprise_context=enterprise_context
                )
                
                if analysis:
                    chunk_analyses.append(analysis)
                    print(f"  ✓ Extracted: {len(analysis.get('data_actions', []))} actions, "
                          f"{len(analysis.get('citations', []))} citations")
                else:
                    print(f"  ⚠ No data extracted from chunk {i+1}")
                    
            except Exception as e:
                print(f"  ✗ Error processing chunk {i+1}: {e}")
                logger.error(f"Chunk {i+1} error: {e}", exc_info=True)
                continue
        
        # Merge all chunk analyses
        print(f"\nMerging {len(chunk_analyses)} chunk analyses...")
        final_analysis = self._merge_chunk_analyses(
            chunk_analyses=chunk_analyses,
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            level=level,
            enterprise_context=enterprise_context
        )
        
        return final_analysis
    
    def _analyze_single_chunk(
        self,
        chunk: Dict[str, Any],
        strategies: AdvancedPromptingStrategies,
        level: int,
        enterprise_context: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """
        Analyze a single chunk with robust extraction
        """
        chunk_text = chunk['text']
        chunk_id = chunk.get('chunk_id', 0)
        chunk_context = self.chunker.get_chunk_context(chunk)
        
        # Create prompt
        prompt = strategies.get_chain_of_thought_prompt(
            document_text=chunk_text,
            analysis_type="Extract all requirements with citations",
            context=enterprise_context,
            chunk_info=chunk_context
        )
        
        # Get LLM response
        messages = [
            SystemMessage(content=strategies.get_react_agent_system_prompt()),
            HumanMessage(content=prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Log raw response for debugging
            logger.info(f"Chunk {chunk_id} raw response length: {len(response_text)}")
            logger.debug(f"Chunk {chunk_id} response: {response_text[:500]}...")
            
            # Extract structured data
            extracted = self._extract_from_response(
                response_text=response_text,
                chunk_id=chunk_id,
                level=level
            )
            
            return extracted
            
        except Exception as e:
            logger.error(f"Error in LLM call for chunk {chunk_id}: {e}")
            return None
    
    def _extract_from_response(
        self,
        response_text: str,
        chunk_id: int,
        level: int
    ) -> Dict[str, Any]:
        """
        ROBUST extraction from LLM response with multiple fallback strategies
        """
        # Strategy 1: Try to parse as pure JSON
        try:
            parsed = json.loads(response_text)
            if isinstance(parsed, dict):
                logger.info(f"Chunk {chunk_id}: Extracted via direct JSON parsing")
                return self._normalize_extracted_data(parsed, chunk_id, level)
        except:
            pass
        
        # Strategy 2: Extract JSON from markdown code blocks
        try:
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
                if isinstance(parsed, dict):
                    logger.info(f"Chunk {chunk_id}: Extracted via markdown JSON block")
                    return self._normalize_extracted_data(parsed, chunk_id, level)
        except:
            pass
        
        # Strategy 3: Find any JSON object in the text
        try:
            # Find the largest JSON object
            json_objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
            for json_str in sorted(json_objects, key=len, reverse=True):
                try:
                    parsed = json.loads(json_str)
                    if isinstance(parsed, dict) and len(parsed) > 2:
                        logger.info(f"Chunk {chunk_id}: Extracted via JSON object search")
                        return self._normalize_extracted_data(parsed, chunk_id, level)
                except:
                    continue
        except:
            pass
        
        # Strategy 4: Text-based extraction as last resort
        logger.warning(f"Chunk {chunk_id}: Falling back to text-based extraction")
        return self._extract_from_text(response_text, chunk_id, level)
    
    def _normalize_extracted_data(
        self,
        data: Dict[str, Any],
        chunk_id: int,
        level: int
    ) -> Dict[str, Any]:
        """
        Normalize extracted data to consistent format
        """
        normalized = {
            "description": "",
            "citations": [],
            "data_actions": [],
            "user_evidence": [],
            "system_evidence": [],
            "constraints": [],
            "classification": "condition",
            "classification_reasoning": ""
        }
        
        # Extract description
        normalized["description"] = str(data.get("description", ""))
        
        # Extract citations
        for cite in data.get("citations", []):
            if isinstance(cite, dict):
                normalized["citations"].append({
                    "text": str(cite.get("text", ""))[:200],
                    "chunk_id": chunk_id,
                    "level": level,
                    "reasoning": str(cite.get("reasoning", ""))
                })
            elif isinstance(cite, str):
                normalized["citations"].append({
                    "text": cite[:200],
                    "chunk_id": chunk_id,
                    "level": level,
                    "reasoning": ""
                })
        
        # Extract data actions
        for action in data.get("data_actions", []):
            if isinstance(action, dict):
                action_type = self._map_to_taxonomy(action.get("type", ""))
                normalized["data_actions"].append({
                    "type": action_type,
                    "description": str(action.get("description", "")),
                    "citations": action.get("citations", [])
                })
            elif isinstance(action, str):
                normalized["data_actions"].append({
                    "type": DataActionType.DATA_USAGE,
                    "description": action,
                    "citations": []
                })
        
        # Extract user evidence
        for evidence in data.get("user_evidence", []):
            if isinstance(evidence, dict):
                normalized["user_evidence"].append({
                    "description": str(evidence.get("description", "")),
                    "citations": evidence.get("citations", [])
                })
            elif isinstance(evidence, str):
                normalized["user_evidence"].append({
                    "description": evidence,
                    "citations": []
                })
        
        # Extract system evidence
        for evidence in data.get("system_evidence", []):
            if isinstance(evidence, dict):
                normalized["system_evidence"].append({
                    "description": str(evidence.get("description", "")),
                    "citations": evidence.get("citations", [])
                })
            elif isinstance(evidence, str):
                normalized["system_evidence"].append({
                    "description": evidence,
                    "citations": []
                })
        
        # Extract constraints
        for constraint in data.get("constraints", []):
            if isinstance(constraint, dict):
                normalized["constraints"].append(constraint)
            elif isinstance(constraint, str) and len(constraint) > 5:
                normalized["constraints"].append({
                    "type": "general",
                    "description": constraint
                })
        
        # Extract classification
        classification = str(data.get("classification", "condition")).lower()
        if "restriction" in classification or "prohibit" in classification:
            normalized["classification"] = "restriction"
        else:
            normalized["classification"] = "condition"
        
        normalized["classification_reasoning"] = str(data.get("classification_reasoning", ""))
        
        return normalized
    
    def _map_to_taxonomy(self, action_type: str) -> str:
        """
        Map action type to simplified taxonomy
        """
        action_lower = action_type.lower().strip()
        
        # Sharing/Access keywords
        if any(word in action_lower for word in ['share', 'transfer', 'disclose', 'access', 'provide', 'send']):
            return DataActionType.DATA_SHARING_AND_ACCESS
        
        # Storage/Hosting keywords
        if any(word in action_lower for word in ['store', 'host', 'retain', 'keep', 'maintain', 'archive']):
            return DataActionType.DATA_STORAGE_AND_HOSTING
        
        # Default to usage
        return DataActionType.DATA_USAGE
    
    def _extract_from_text(
        self,
        text: str,
        chunk_id: int,
        level: int
    ) -> Dict[str, Any]:
        """
        COMPLETE: Fallback text-based extraction when JSON parsing fails
        """
        result = {
            "description": "",
            "citations": [],
            "data_actions": [],
            "user_evidence": [],
            "system_evidence": [],
            "constraints": [],
            "classification": "condition",
            "classification_reasoning": ""
        }
        
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line or len(line) < 3:
                continue
            
            line_lower = line.lower()
            
            # Identify sections
            if "description:" in line_lower:
                current_section = "description"
                desc = line.split(":", 1)[1].strip() if ":" in line else ""
                if desc:
                    result["description"] = desc
            elif "citation" in line_lower:
                current_section = "citations"
            elif "data action" in line_lower or "action:" in line_lower:
                current_section = "data_actions"
            elif "user evidence" in line_lower or "user:" in line_lower:
                current_section = "user_evidence"
            elif "system evidence" in line_lower or "system:" in line_lower:
                current_section = "system_evidence"
            elif "constraint" in line_lower:
                current_section = "constraints"
            elif "classification:" in line_lower:
                if "restriction" in line_lower or "prohibit" in line_lower:
                    result["classification"] = "restriction"
                else:
                    result["classification"] = "condition"
            elif current_section and (line.startswith("-") or line.startswith("•") or line.startswith("*")):
                # List item
                item = line.lstrip("-•* ").strip()
                if len(item) > 5:
                    if current_section == "data_actions":
                        action_type = self._map_to_taxonomy(item)
                        result["data_actions"].append({
                            "type": action_type,
                            "description": item,
                            "citations": []
                        })
                    elif current_section == "citations":
                        result["citations"].append({
                            "text": item[:200],
                            "chunk_id": chunk_id,
                            "level": level,
                            "reasoning": "Text extraction"
                        })
                    elif current_section == "user_evidence":
                        result["user_evidence"].append({
                            "description": item,
                            "citations": []
                        })
                    elif current_section == "system_evidence":
                        result["system_evidence"].append({
                            "description": item,
                            "citations": []
                        })
                    elif current_section == "constraints":
                        result["constraints"].append({
                            "type": "general",
                            "description": item
                        })
        
        # Fallback: if nothing extracted, use the first 500 chars as description
        if not result["description"] and text:
            result["description"] = text[:500]
            result["citations"].append({
                "text": text[:200],
                "chunk_id": chunk_id,
                "level": level,
                "reasoning": "Fallback citation"
            })
        
        logger.info(f"Text extraction complete: {len(result['data_actions'])} actions, "
                   f"{len(result['citations'])} citations")
        
        return result
    
    def _merge_chunk_analyses(
        self,
        chunk_analyses: List[Dict[str, Any]],
        rule_name: str,
        jurisdiction: str,
        level: int,
        enterprise_context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        COMPLETE: Merge multiple chunk analyses into single comprehensive analysis
        """
        if not chunk_analyses:
            return {
                "description": "",
                "citations": [],
                "data_actions": [],
                "user_evidence": [],
                "system_evidence": [],
                "constraints": [],
                "classification": "condition",
                "classification_reasoning": "",
                "metadata": {
                    "rule_name": rule_name,
                    "jurisdiction": jurisdiction,
                    "level": level,
                    "chunks_processed": 0
                }
            }
        
        # Merge descriptions
        descriptions = [c.get("description", "") for c in chunk_analyses if c.get("description")]
        merged_description = " ".join(descriptions).strip()
        
        # Collect all citations
        all_citations = []
        for chunk in chunk_analyses:
            all_citations.extend(chunk.get("citations", []))
        
        # Deduplicate citations by text
        unique_citations = []
        seen_texts = set()
        for cite in all_citations:
            text = cite.get("text", "")[:50]
            if text and text not in seen_texts:
                unique_citations.append(cite)
                seen_texts.add(text)
        
        # Collect and deduplicate data actions
        all_actions = []
        seen_actions = set()
        for chunk in chunk_analyses:
            for action in chunk.get("data_actions", []):
                key = (action.get("type", ""), action.get("description", "")[:50])
                if key not in seen_actions:
                    all_actions.append(action)
                    seen_actions.add(key)
        
        # Collect and deduplicate user evidence
        all_user_evidence = []
        seen_user = set()
        for chunk in chunk_analyses:
            for evidence in chunk.get("user_evidence", []):
                desc = evidence.get("description", "")[:50]
                if desc and desc not in seen_user:
                    all_user_evidence.append(evidence)
                    seen_user.add(desc)
        
        # Collect and deduplicate system evidence
        all_system_evidence = []
        seen_system = set()
        for chunk in chunk_analyses:
            for evidence in chunk.get("system_evidence", []):
                desc = evidence.get("description", "")[:50]
                if desc and desc not in seen_system:
                    all_system_evidence.append(evidence)
                    seen_system.add(desc)
        
        # Collect constraints
        all_constraints = []
        for chunk in chunk_analyses:
            for constraint in chunk.get("constraints", []):
                if isinstance(constraint, dict):
                    all_constraints.append(constraint)
        
        # Determine classification
        classifications = [c.get("classification", "") for c in chunk_analyses if c.get("classification")]
        final_classification = "condition"
        if "restriction" in classifications:
            final_classification = "restriction"
        elif classifications:
            final_classification = classifications[0]
        
        # Combine reasoning
        reasonings = [c.get("classification_reasoning", "") for c in chunk_analyses if c.get("classification_reasoning")]
        combined_reasoning = "; ".join(reasonings).strip()
        
        # Create final analysis
        final = {
            "description": merged_description,
            "citations": unique_citations,
            "data_actions": all_actions,
            "user_evidence": all_user_evidence,
            "system_evidence": all_system_evidence,
            "constraints": all_constraints,
            "classification": final_classification,
            "classification_reasoning": combined_reasoning,
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "level": level,
                "document_length": sum(len(c.get("description", "")) for c in chunk_analyses),
                "chunks_processed": len(chunk_analyses),
                "enterprise_context": enterprise_context,
                "total_citations": len(unique_citations),
                "total_actions": len(all_actions),
                "total_user_evidence": len(all_user_evidence),
                "total_system_evidence": len(all_system_evidence)
            }
        }
        
        # Log summary
        print(f"\n✓ Merge complete:")
        print(f"  Description: {len(merged_description)} chars")
        print(f"  Citations: {len(unique_citations)}")
        print(f"  Actions: {len(all_actions)}")
        print(f"  User evidence: {len(all_user_evidence)}")
        print(f"  System evidence: {len(all_system_evidence)}")
        print(f"  Constraints: {len(all_constraints)}")
        print(f"  Classification: {final_classification}")
        
        return final
    
    def analyze_multi_level(
        self,
        rule_name: str,
        jurisdiction: str,
        level_1_text: str,
        level_2_text: str,
        level_3_text: str,
        enterprise_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        COMPLETE & FIXED: Analyze all three levels sequentially and combine results
        """
        print(f"\n{'#'*60}")
        print(f"# MULTI-LEVEL ANALYSIS: {rule_name}")
        print(f"{'#'*60}")
        
        # Level 1 Analysis
        print(f"\n>>> LEVEL 1: Legislation ({len(level_1_text)} chars)")
        level_1_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_1_text,
            level=1,
            enterprise_context=enterprise_context
        )
        
        # Level 2 Analysis
        print(f"\n>>> LEVEL 2: Guidance ({len(level_2_text)} chars)")
        level_2_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_2_text,
            level=2,
            enterprise_context=enterprise_context
        )
        
        # Level 3 Analysis
        print(f"\n>>> LEVEL 3: Enterprise Policies ({len(level_3_text)} chars)")
        level_3_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_3_text,
            level=3,
            enterprise_context=enterprise_context
        )
        
        # FIXED: Combine all levels - define combined variable before using it
        print(f"\n>>> COMBINING ALL LEVELS")
        
        # Build combined dict with all data from all three levels
        combined = {
            "description": " ".join([
                level_1_analysis.get("description", ""),
                level_2_analysis.get("description", ""),
                level_3_analysis.get("description", "")
            ]).strip(),
            
            "citations": (
                level_1_analysis.get("citations", []) +
                level_2_analysis.get("citations", []) +
                level_3_analysis.get("citations", [])
            ),
            
            "data_actions": (
                level_1_analysis.get("data_actions", []) +
                level_2_analysis.get("data_actions", []) +
                level_3_analysis.get("data_actions", [])
            ),
            
            "user_evidence": (
                level_1_analysis.get("user_evidence", []) +
                level_2_analysis.get("user_evidence", []) +
                level_3_analysis.get("user_evidence", [])
            ),
            
            "system_evidence": (
                level_1_analysis.get("system_evidence", []) +
                level_2_analysis.get("system_evidence", []) +
                level_3_analysis.get("system_evidence", [])
            ),
            
            "constraints": (
                level_1_analysis.get("constraints", []) +
                level_2_analysis.get("constraints", []) +
                level_3_analysis.get("constraints", [])
            ),
            
            "classification": level_1_analysis.get("classification", "condition"),
            
            "classification_reasoning": "; ".join([
                level_1_analysis.get("classification_reasoning", ""),
                level_2_analysis.get("classification_reasoning", ""),
                level_3_analysis.get("classification_reasoning", "")
            ]).strip(),
            
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "enterprise_context": enterprise_context,
                "level_1_chunks": level_1_analysis.get("metadata", {}).get("chunks_processed", 0),
                "level_2_chunks": level_2_analysis.get("metadata", {}).get("chunks_processed", 0),
                "level_3_chunks": level_3_analysis.get("metadata", {}).get("chunks_processed", 0),
                "level_3_enterprise_specific": True,
                "total_citations": 0,  # Will be updated below
                "total_actions": 0     # Will be updated below
            }
        }
        
        # Update metadata counts after combined is fully defined
        combined["metadata"]["total_citations"] = len(combined["citations"])
        combined["metadata"]["total_actions"] = len(combined["data_actions"])
        combined["metadata"]["total_user_evidence"] = len(combined["user_evidence"])
        combined["metadata"]["total_system_evidence"] = len(combined["system_evidence"])
        
        # Print summary
        print(f"\n{'#'*60}")
        print(f"# MULTI-LEVEL ANALYSIS COMPLETE")
        print(f"{'#'*60}")
        print(f"Total description: {len(combined['description'])} chars")
        print(f"Total citations: {len(combined['citations'])}")
        print(f"Total actions: {len(combined['data_actions'])}")
        print(f"Total user evidence: {len(combined['user_evidence'])}")
        print(f"Total system evidence: {len(combined['system_evidence'])}")
        print(f"Classification: {combined['classification']}")
        
        return combined
