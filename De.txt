import asyncio
import json
import logging
import os
import csv
import glob
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from datetime import datetime

# Core dependencies
import openai
from openai import OpenAI
from pydantic import BaseModel, Field, validator
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI

# PDF processing
try:
    import pymupdf  # Modern PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    try:
        import pdfplumber
        PDF_AVAILABLE = True
    except ImportError:
        PDF_AVAILABLE = False
        print("Warning: No PDF library found. Install PyMuPDF or pdfplumber: pip install PyMuPDF pdfplumber")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================
# GLOBAL CONFIGURATION
# ===============================

class Config:
    """Global configuration for the legislation rules converter."""
    BASE_URL = "https://api.openai.com/v1"
    API_KEY = os.getenv("OPENAI_API_KEY")
    CHAT_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Paths
    LEGISLATION_PDF_PATH = "./legislation_pdfs/"
    RULES_OUTPUT_PATH = "./extracted_rules/"
    EMBEDDINGS_PATH = "./embeddings/"
    LOGS_PATH = "./logs/"
    EXISTING_RULES_FILE = "./extracted_rules/all_rules.json"
    METADATA_CONFIG_FILE = "./legislation_metadata.json"

# Validate API key
if not Config.API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is required")

# ===============================
# PYDANTIC MODELS
# ===============================

class DataDomain(str, Enum):
    """Data domains as per privacy regulations."""
    DATA_TRANSFER = "data_transfer"
    DATA_USAGE = "data_usage" 
    DATA_STORAGE = "data_storage"

class DataRole(str, Enum):
    """Roles in data processing."""
    CONTROLLER = "controller"
    PROCESSOR = "processor"
    JOINT_CONTROLLER = "joint_controller"

class DataCategory(str, Enum):
    """Categories of personal data."""
    PERSONAL_DATA = "personal_data"
    SENSITIVE_DATA = "sensitive_data"
    BIOMETRIC_DATA = "biometric_data"
    HEALTH_DATA = "health_data"
    FINANCIAL_DATA = "financial_data"
    LOCATION_DATA = "location_data"
    BEHAVIORAL_DATA = "behavioral_data"
    IDENTIFICATION_DATA = "identification_data"

class ConditionOperator(str, Enum):
    """Operators for rule conditions."""
    EQUAL = "equal"
    NOT_EQUAL = "notEqual"
    GREATER_THAN = "greaterThan"
    LESS_THAN = "lessThan"
    GREATER_THAN_EQUAL = "greaterThanInclusive"
    LESS_THAN_EQUAL = "lessThanInclusive"
    CONTAINS = "contains"
    NOT_CONTAINS = "doesNotContain"
    IN = "in"
    NOT_IN = "notIn"

class RuleCondition(BaseModel):
    """Individual condition within a rule."""
    fact: str = Field(..., description="The fact/data point to evaluate")
    operator: ConditionOperator = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    path: Optional[str] = Field(None, description="JSONPath to navigate nested objects")
    description: str = Field(..., description="Human-readable description of this condition")
    data_domain: List[DataDomain] = Field(..., description="Applicable data domains")
    role: DataRole = Field(..., description="Role this condition applies to")
    reasoning: str = Field(..., description="LLM reasoning for why this condition was extracted")

class RuleEvent(BaseModel):
    """Event triggered when rule conditions are met."""
    type: str = Field(..., description="Type of event/action")
    params: Dict[str, Any] = Field(default_factory=dict, description="Event parameters")

class LegislationRule(BaseModel):
    """Complete rule structure aligned with json-rules-engine format."""
    id: str = Field(..., description="Unique rule identifier")
    name: str = Field(..., description="Rule name")
    description: str = Field(..., description="Human-readable rule description")
    source_article: str = Field(..., description="Source legislation article/section")
    source_file: str = Field(..., description="Source PDF filename")
    
    conditions: Dict[str, List[RuleCondition]] = Field(
        ..., 
        description="Rule conditions with 'all', 'any', or 'not' logic"
    )
    event: RuleEvent = Field(..., description="Event triggered when conditions are met")
    priority: int = Field(default=1, description="Rule priority (1-10)")
    
    # New required fields
    primary_impacted_role: DataRole = Field(..., description="Primary role most impacted by this rule")
    secondary_impacted_role: Optional[DataRole] = Field(None, description="Secondary role impacted by this rule")
    data_category: List[DataCategory] = Field(..., description="Categories of data this rule applies to")
    
    # Country metadata
    applicable_countries: List[str] = Field(default_factory=list, description="Countries where this rule applies")
    adequacy_countries: List[str] = Field(default_factory=list, description="Adequacy countries (optional)")
    
    # Metadata
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    extraction_method: str = Field(default="llm_analysis")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Extraction confidence")
    
    @validator('conditions')
    def validate_conditions_structure(cls, v):
        """Ensure conditions follow json-rules-engine format."""
        valid_keys = {'all', 'any', 'not'}
        if not isinstance(v, dict) or not any(key in valid_keys for key in v.keys()):
            raise ValueError("Conditions must contain 'all', 'any', or 'not' keys")
        return v

class ExtractionResult(BaseModel):
    """Complete result of legislation analysis."""
    rules: List[LegislationRule] = Field(..., description="Extracted rules")
    summary: str = Field(..., description="Summary of extraction")
    total_rules: int = Field(..., description="Total number of rules extracted")
    processing_time: float = Field(..., description="Processing time in seconds")
    embeddings: Optional[List[List[float]]] = Field(None, description="Rule embeddings")
    
    def save_json(self, filepath: str):
        """Save rules to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in self.rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
    
    def save_csv(self, filepath: str):
        """Save rules to CSV file with all JSON fields."""
        if not self.rules:
            return
            
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # Write comprehensive header with all fields
            headers = [
                'id', 'name', 'description', 'source_article', 'source_file',
                'primary_impacted_role', 'secondary_impacted_role', 'data_category',
                'applicable_countries', 'adequacy_countries',
                'conditions_logic_type', 'conditions_count', 'conditions_details',
                'event_type', 'event_params', 'priority', 'confidence_score',
                'extraction_method', 'extracted_at'
            ]
            writer.writerow(headers)
            
            # Write data rows with all information
            for rule in self.rules:
                # Process conditions into readable format
                conditions_details = []
                conditions_logic_types = []
                total_conditions = 0
                
                for logic_type, conditions in rule.conditions.items():
                    conditions_logic_types.append(logic_type)
                    total_conditions += len(conditions)
                    
                    for condition in conditions:
                        condition_detail = (
                            f"[{logic_type.upper()}] {condition.description} | "
                            f"Fact: {condition.fact} | Operator: {condition.operator} | "
                            f"Value: {condition.value} | Role: {condition.role.value} | "
                            f"Domains: {', '.join([d.value for d in condition.data_domain])} | "
                            f"Reasoning: {condition.reasoning}"
                        )
                        conditions_details.append(condition_detail)
                
                # Serialize event params
                event_params_str = json.dumps(rule.event.params) if rule.event.params else ""
                
                writer.writerow([
                    rule.id,
                    rule.name,
                    rule.description,
                    rule.source_article,
                    rule.source_file,
                    rule.primary_impacted_role.value,
                    rule.secondary_impacted_role.value if rule.secondary_impacted_role else "",
                    ", ".join([cat.value for cat in rule.data_category]),
                    ", ".join(rule.applicable_countries),
                    ", ".join(rule.adequacy_countries),
                    "; ".join(conditions_logic_types),
                    total_conditions,
                    " || ".join(conditions_details),
                    rule.event.type,
                    event_params_str,
                    rule.priority,
                    rule.confidence_score,
                    rule.extraction_method,
                    rule.extracted_at.isoformat()
                ])

# ===============================
# METADATA MANAGEMENT
# ===============================

class MetadataManager:
    """Manages legislation metadata configuration."""
    
    def __init__(self, config_file: str = Config.METADATA_CONFIG_FILE):
        self.config_file = config_file
        self.metadata = {}
        self.load_metadata()
    
    def load_metadata(self):
        """Load metadata from config file."""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                logger.info(f"Loaded metadata for {len(self.metadata)} files")
            else:
                logger.info("No metadata config file found. Creating default structure.")
                self.create_default_config()
        except Exception as e:
            logger.error(f"Error loading metadata: {e}")
            self.metadata = {}
    
    def create_default_config(self):
        """Create a default metadata configuration file."""
        default_config = {
            "metadata_info": {
                "description": "This file contains metadata for legislation PDFs",
                "format": {
                    "filename.pdf": {
                        "applicable_countries": ["Country1", "Country2"],
                        "adequacy_countries": ["Country3", "Country4"]
                    }
                }
            },
            "example_files": {
                "gdpr_article_28.pdf": {
                    "applicable_countries": ["Germany", "France", "Italy", "Spain", "Netherlands"],
                    "adequacy_countries": ["Canada", "Japan", "United Kingdom", "Switzerland"]
                },
                "ccpa_regulation.pdf": {
                    "applicable_countries": ["United States", "California"],
                    "adequacy_countries": []
                }
            }
        }
        
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"Created default metadata config at: {self.config_file}")
        except Exception as e:
            logger.error(f"Error creating default config: {e}")
    
    def get_file_metadata(self, filename: str) -> Dict[str, List[str]]:
        """Get metadata for a specific PDF file."""
        # Check in example_files and other top-level keys
        for section in self.metadata.values():
            if isinstance(section, dict) and filename in section:
                file_meta = section[filename]
                return {
                    'applicable_countries': file_meta.get('applicable_countries', []),
                    'adequacy_countries': file_meta.get('adequacy_countries', [])
                }
        
        # Return empty lists if no metadata found
        logger.warning(f"No metadata found for {filename}, using empty country lists")
        return {
            'applicable_countries': [],
            'adequacy_countries': []
        }
    
    def add_file_metadata(self, filename: str, applicable_countries: List[str], adequacy_countries: List[str] = None):
        """Add metadata for a new file."""
        if adequacy_countries is None:
            adequacy_countries = []
        
        # Add to example_files section or create new section
        if 'example_files' not in self.metadata:
            self.metadata['example_files'] = {}
        
        self.metadata['example_files'][filename] = {
            'applicable_countries': applicable_countries,
            'adequacy_countries': adequacy_countries
        }
        
        self.save_metadata()
    
    def save_metadata(self):
        """Save metadata to config file."""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
            logger.info("Metadata config saved successfully")
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
    
    def list_configured_files(self) -> List[str]:
        """Get list of files that have metadata configured."""
        configured_files = []
        for section in self.metadata.values():
            if isinstance(section, dict):
                for key in section.keys():
                    if key.endswith('.pdf'):
                        configured_files.append(key)
        return configured_files

# ===============================
# PDF READER
# ===============================

class PDFReader:
    """PDF reader for extracting text from legislation documents."""
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str) -> str:
        """Extract text from PDF file."""
        if not PDF_AVAILABLE:
            raise ImportError("No PDF library available. Install PyMuPDF or pdfplumber")
        
        try:
            # Try modern PyMuPDF first
            if 'pymupdf' in globals():
                return PDFReader._extract_with_pymupdf(pdf_path)
            else:
                return PDFReader._extract_with_pdfplumber(pdf_path)
        except Exception as e:
            logger.error(f"Error reading PDF {pdf_path}: {e}")
            raise
    
    @staticmethod
    def _extract_with_pymupdf(pdf_path: str) -> str:
        """Extract text using modern PyMuPDF with context manager."""
        text = ""
        try:
            # Use modern PyMuPDF API with context manager (best practice)
            with pymupdf.open(pdf_path) as doc:
                for page in doc:
                    page_text = page.get_text()
                    if page_text:
                        text += page_text + "\n"
            # Document is automatically closed when exiting context manager
        except Exception as e:
            logger.error(f"PyMuPDF extraction failed: {e}")
            raise
        return text
    
    @staticmethod
    def _extract_with_pdfplumber(pdf_path: str) -> str:
        """Extract text using pdfplumber."""
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    
    @staticmethod
    def get_pdf_files(directory: str) -> List[str]:
        """Get all PDF files from directory."""
        pdf_pattern = os.path.join(directory, "*.pdf")
        return glob.glob(pdf_pattern)

# ===============================
# RULE MANAGEMENT SYSTEM
# ===============================

class RuleManager:
    """Manages existing rules and provides context for new extractions."""
    
    def __init__(self, rules_file: str = Config.EXISTING_RULES_FILE):
        self.rules_file = rules_file
        self.existing_rules: List[LegislationRule] = []
        self.load_existing_rules()
    
    def load_existing_rules(self):
        """Load existing rules from file."""
        try:
            if os.path.exists(self.rules_file):
                with open(self.rules_file, 'r', encoding='utf-8') as f:
                    rules_data = json.load(f)
                
                for rule_data in rules_data:
                    try:
                        rule = LegislationRule(**rule_data)
                        self.existing_rules.append(rule)
                    except Exception as e:
                        logger.warning(f"Skipping invalid existing rule: {e}")
                
                logger.info(f"Loaded {len(self.existing_rules)} existing rules")
            else:
                logger.info("No existing rules file found. Starting fresh.")
        except Exception as e:
            logger.error(f"Error loading existing rules: {e}")
            self.existing_rules = []
    
    def save_rules(self, new_rules: List[LegislationRule]):
        """Save new rules, appending to existing ones."""
        # Combine existing and new rules
        all_rules = self.existing_rules + new_rules
        
        # Remove duplicates based on ID
        unique_rules = []
        seen_ids = set()
        for rule in all_rules:
            if rule.id not in seen_ids:
                unique_rules.append(rule)
                seen_ids.add(rule.id)
        
        # Save to file
        os.makedirs(os.path.dirname(self.rules_file), exist_ok=True)
        with open(self.rules_file, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in unique_rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
        
        # Update internal state
        self.existing_rules = unique_rules
        logger.info(f"Saved {len(unique_rules)} total rules ({len(new_rules)} new)")
    
    def get_context_summary(self) -> str:
        """Get a summary of existing rules for context."""
        if not self.existing_rules:
            return "No existing rules found."
        
        summary = f"Existing Rules Context ({len(self.existing_rules)} rules):\n\n"
        
        # Group by source
        sources = {}
        for rule in self.existing_rules:
            source = rule.source_article
            if source not in sources:
                sources[source] = []
            sources[source].append(rule)
        
        for source, rules in sources.items():
            summary += f"Source: {source} ({len(rules)} rules)\n"
            for rule in rules[:3]:  # Show first 3 rules per source
                summary += f"  - {rule.name}: {rule.description[:100]}...\n"
            if len(rules) > 3:
                summary += f"  ... and {len(rules) - 3} more rules\n"
            summary += "\n"
        
        return summary
    
    def get_processed_files(self) -> List[str]:
        """Get list of already processed PDF files."""
        processed = set()
        for rule in self.existing_rules:
            if hasattr(rule, 'source_file'):
                processed.add(rule.source_file)
        return list(processed)

# ===============================
# ADVANCED PROMPTING STRATEGIES
# ===============================

class PromptingStrategies:
    """Advanced prompting strategies for rule extraction."""
    
    @staticmethod
    def chain_of_thought_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Chain of Thought prompting for step-by-step reasoning."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        You are an expert legal analyst specializing in converting legislation into machine-readable rules.
        {context_section}
        Analyze the following legislation text step by step:
        
        LEGISLATION TEXT:
        {legislation_text}
        
        CHAIN OF THOUGHT ANALYSIS:
        
        Step 1: Identify Key Legal Obligations
        - What are the main obligations stated in this text?
        - Who has these obligations (controller, processor, joint_controller)?
        
        Step 2: Extract Conditional Logic
        - What conditions trigger these obligations?
        - Are there any "if-then" relationships?
        - What are the specific criteria that must be met?
        
        Step 3: Determine Data Domains
        - Does this relate to data_transfer, data_usage, or data_storage?
        - Which specific data activities are covered?
        
        Step 4: Identify Roles and Data Categories
        - Who are the key actors (controller, processor, joint_controller)?
        - What is the primary impacted role and any secondary impacted role?
        - What data categories are involved (personal_data, sensitive_data, biometric_data, health_data, financial_data, location_data, behavioral_data, identification_data)?
        
        Step 5: Structure as Machine-Readable Rules
        - Convert each obligation into a conditional rule
        - Define clear facts, operators, and values
        - Ensure alignment with json-rules-engine format
        - Consider existing rules to maintain consistency
        
        Let's work through this step by step...
        """
    
    @staticmethod
    def mixture_of_experts_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Experts prompting with specialized perspectives."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        We need to analyze legislation from multiple expert perspectives. Each expert will contribute their specialized knowledge.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === EXPERT PANEL ANALYSIS ===
        
        EXPERT 1 - PRIVACY LAW SPECIALIST:
        As a privacy law expert, I will focus on:
        - Data protection obligations and rights
        - Cross-border transfer requirements
        - Consent and lawful basis considerations
        - Individual rights and freedoms
        - Primary and secondary impacted roles
        
        EXPERT 2 - TECHNICAL COMPLIANCE SPECIALIST:
        As a technical expert, I will focus on:
        - Technical and organizational measures
        - Security requirements and safeguards
        - Data processing procedures and controls
        - Risk assessment and mitigation
        - Data categories and classification
        
        EXPERT 3 - REGULATORY INTERPRETATION SPECIALIST:
        As a regulatory expert, I will focus on:
        - Supervisory authority requirements
        - Penalty and enforcement mechanisms
        - Compliance documentation needs
        - Audit and accountability measures
        - Consistency with existing regulatory framework
        
        Each expert will identify different aspects of the legislation and contribute to a comprehensive rule extraction that builds upon existing rules.
        
        Now, let each expert analyze the text and provide their perspective...
        """
    
    @staticmethod
    def mixture_of_thought_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Thought prompting for diverse reasoning approaches."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        Apply multiple thinking approaches to analyze this legislation comprehensively.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === MULTIPLE THINKING APPROACHES ===
        
        ANALYTICAL THINKING:
        - Break down the text into component obligations
        - Identify logical relationships and dependencies
        - Create systematic categorization of requirements
        - Analyze consistency with existing rules
        
        CREATIVE THINKING:
        - Consider edge cases and alternative interpretations
        - Think about practical implementation scenarios
        - Explore different ways obligations could be triggered
        - Identify gaps in existing rule coverage
        
        CRITICAL THINKING:
        - Question assumptions and implicit requirements
        - Evaluate the necessity and sufficiency of conditions
        - Consider potential conflicts or ambiguities
        - Assess impact on different roles and data categories
        
        PRACTICAL THINKING:
        - Focus on real-world application and compliance
        - Consider operational feasibility and implementation
        - Think about monitoring and enforcement mechanisms
        - Evaluate primary vs secondary role impacts
        
        SYSTEMATIC THINKING:
        - Consider the broader regulatory framework
        - Understand interconnections with other provisions
        - Map relationships between different roles and responsibilities
        - Ensure consistency with existing rule patterns
        
        Apply each thinking approach to extract comprehensive, actionable rules...
        """
    
    @staticmethod
    def mixture_of_reasoning_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Reasoning prompting for comprehensive analysis."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        Use multiple reasoning strategies to thoroughly analyze this legislation.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === REASONING STRATEGIES ===
        
        DEDUCTIVE REASONING:
        - Start with general legal principles
        - Apply them to specific provisions
        - Derive specific obligations and requirements
        - Maintain consistency with established rule patterns
        
        INDUCTIVE REASONING:
        - Examine specific examples and cases mentioned
        - Identify patterns and common elements
        - Generalize to broader rules and principles
        - Learn from existing rule structures
        
        ABDUCTIVE REASONING:
        - Observe the intended outcomes and goals
        - Infer the most likely requirements to achieve them
        - Hypothesize necessary conditions and controls
        - Consider primary and secondary role impacts
        
        ANALOGICAL REASONING:
        - Compare to similar regulations and provisions
        - Draw parallels from established legal frameworks
        - Apply proven compliance patterns
        - Leverage existing rule precedents
        
        CAUSAL REASONING:
        - Identify cause-and-effect relationships
        - Map triggers to required actions
        - Understand consequences of non-compliance
        - Analyze data category implications
        
        Apply each reasoning strategy to extract precise, enforceable rules that complement existing ones...
        """

# ===============================
# OPENAI CLIENT AND EMBEDDINGS
# ===============================

class OpenAIService:
    """Service for OpenAI API interactions."""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
    
    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI's text-embedding-3-large model."""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=texts,
                encoding_format="float"
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Union[Dict[str, str], SystemMessage, HumanMessage, AIMessage]]) -> str:
        """Generate chat completion using OpenAI's API."""
        try:
            # Convert LangChain messages to dict format for OpenAI API
            formatted_messages = []
            for msg in messages:
                if isinstance(msg, (SystemMessage, HumanMessage, AIMessage)):
                    if isinstance(msg, SystemMessage):
                        formatted_messages.append({"role": "system", "content": msg.content})
                    elif isinstance(msg, HumanMessage):
                        formatted_messages.append({"role": "user", "content": msg.content})
                    elif isinstance(msg, AIMessage):
                        formatted_messages.append({"role": "assistant", "content": msg.content})
                elif isinstance(msg, dict):
                    formatted_messages.append(msg)
                else:
                    formatted_messages.append({"role": "user", "content": str(msg)})
            
            response = self.client.chat.completions.create(
                model=Config.CHAT_MODEL,
                messages=formatted_messages
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            raise

# ===============================
# SAFE JSON PARSING
# ===============================

class SafeJsonParser:
    """Safe JSON parsing with error handling and validation."""
    
    @staticmethod
    def parse_json_response(response: str) -> Dict[str, Any]:
        """Safely parse JSON response from LLM."""
        try:
            # Clean the response
            cleaned = response.strip()
            
            # Handle code blocks
            if "```json" in cleaned:
                start = cleaned.find("```json") + 7
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            elif "```" in cleaned:
                start = cleaned.find("```") + 3
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            
            # Try to parse JSON
            parsed = json.loads(cleaned)
            return parsed
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON decode error: {e}. Attempting to fix...")
            
            # Try to fix common JSON issues
            try:
                # Remove trailing commas
                import re
                fixed = re.sub(r',(\s*[}\]])', r'\1', cleaned)
                parsed = json.loads(fixed)
                return parsed
            except Exception:
                logger.error(f"Could not parse JSON response: {cleaned[:200]}...")
                return {"error": "Failed to parse JSON", "raw_response": cleaned}
    
    @staticmethod
    def validate_rule_structure(data: Dict[str, Any]) -> bool:
        """Validate that parsed data follows expected rule structure."""
        required_fields = ['id', 'name', 'description', 'conditions', 'event']
        return all(field in data for field in required_fields)

# ===============================
# LANGGRAPH TOOLS
# ===============================

@tool
def extract_rule_conditions(legislation_text: str, focus_area: str) -> str:
    """Extract specific rule conditions from legislation text."""
    
    prompt = f"""
    Extract specific rule conditions from the following legislation text, focusing on {focus_area}.
    
    Return a JSON object with conditions in json-rules-engine format.
    
    Text: {legislation_text}
    
    Focus on identifying:
    - Specific facts that can be evaluated
    - Comparison operators (equal, greaterThan, contains, etc.)
    - Values to compare against
    - Data domains (data_transfer, data_usage, data_storage) and roles (controller, processor, joint_controller)
    
    Return valid JSON only.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error extracting conditions: {str(e)}"

@tool
def analyze_data_domains(legislation_text: str) -> str:
    """Analyze and identify relevant data domains in legislation."""
    
    prompt = f"""
    Analyze the following legislation text and identify which data domains are relevant:
    - data_transfer
    - data_usage
    - data_storage
    
    Text: {legislation_text}
    
    Return a JSON object mapping each identified domain to its relevance and reasoning.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error analyzing domains: {str(e)}"

@tool
def identify_roles_responsibilities(legislation_text: str) -> str:
    """Identify roles and responsibilities in legislation."""
    
    prompt = f"""
    Identify the roles and responsibilities mentioned in this legislation:
    - controller
    - processor 
    - joint_controller
    
    Text: {legislation_text}
    
    For each role, identify their specific obligations and responsibilities.
    Return a JSON object with role mappings.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error identifying roles: {str(e)}"

# ===============================
# MAIN LEGISLATION ANALYZER
# ===============================

class LegislationAnalyzer:
    """Main analyzer for converting legislation to machine-readable rules."""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.json_parser = SafeJsonParser()
        self.rule_manager = RuleManager()
        self.pdf_reader = PDFReader()
        self.metadata_manager = MetadataManager()
        
        # Initialize LangChain model
        self.llm = ChatOpenAI(
            model=Config.CHAT_MODEL,
            openai_api_key=Config.API_KEY,
            openai_api_base=Config.BASE_URL
        )
        
        # Create react agent with tools
        self.tools = [
            extract_rule_conditions,
            analyze_data_domains, 
            identify_roles_responsibilities
        ]
        
        # Memory for conversation state
        self.memory = MemorySaver()
        
        # Create react agent
        self.agent = create_react_agent(
            self.llm,
            self.tools,
            checkpointer=self.memory
        )
    
    async def process_legislation_folder(self, folder_path: str = None) -> ExtractionResult:
        """Process all PDF files in the legislation folder."""
        if folder_path is None:
            folder_path = Config.LEGISLATION_PDF_PATH
        
        # Ensure folder exists
        os.makedirs(folder_path, exist_ok=True)
        
        # Get PDF files
        pdf_files = self.pdf_reader.get_pdf_files(folder_path)
        
        if not pdf_files:
            logger.warning(f"No PDF files found in {folder_path}")
            return ExtractionResult(
                rules=[],
                summary="No PDF files to process",
                total_rules=0,
                processing_time=0.0
            )
        
        # Get already processed files
        processed_files = self.rule_manager.get_processed_files()
        
        # Filter to only new files
        new_files = [f for f in pdf_files if os.path.basename(f) not in processed_files]
        
        if not new_files:
            logger.info("All PDF files have already been processed")
            return ExtractionResult(
                rules=[],
                summary="All files already processed",
                total_rules=0,
                processing_time=0.0
            )
        
        logger.info(f"Processing {len(new_files)} new PDF files")
        
        all_new_rules = []
        start_time = datetime.utcnow()
        
        for pdf_file in new_files:
            try:
                logger.info(f"Processing: {os.path.basename(pdf_file)}")
                
                # Extract text from PDF
                text = self.pdf_reader.extract_text_from_pdf(pdf_file)
                
                # Get metadata for this file
                filename = os.path.basename(pdf_file)
                file_metadata = self.metadata_manager.get_file_metadata(filename)
                
                # Analyze the legislation
                result = await self.analyze_legislation(
                    legislation_text=text,
                    article_reference=f"Document: {filename}",
                    source_file=filename,
                    applicable_countries=file_metadata['applicable_countries'],
                    adequacy_countries=file_metadata['adequacy_countries']
                )
                
                all_new_rules.extend(result.rules)
                
            except Exception as e:
                logger.error(f"Error processing {pdf_file}: {e}")
                continue
        
        # Calculate total processing time
        end_time = datetime.utcnow()
        total_processing_time = (end_time - start_time).total_seconds()
        
        # Generate embeddings for all new rules
        if all_new_rules:
            rule_texts = [f"{rule.description} {rule.source_article}" for rule in all_new_rules]
            embeddings = await self.openai_service.get_embeddings(rule_texts)
        else:
            embeddings = []
        
        # Save new rules
        if all_new_rules:
            self.rule_manager.save_rules(all_new_rules)
        
        # Create result
        result = ExtractionResult(
            rules=all_new_rules,
            summary=f"Processed {len(new_files)} PDF files, extracted {len(all_new_rules)} new rules",
            total_rules=len(all_new_rules),
            processing_time=total_processing_time,
            embeddings=embeddings
        )
        
        return result
    
    async def analyze_legislation(
        self, 
        legislation_text: str, 
        article_reference: str = "", 
        source_file: str = "",
        applicable_countries: List[str] = None,
        adequacy_countries: List[str] = None
    ) -> ExtractionResult:
        """Analyze legislation text and extract machine-readable rules."""
        start_time = datetime.utcnow()
        
        # Default to empty lists if not provided
        if applicable_countries is None:
            applicable_countries = []
        if adequacy_countries is None:
            adequacy_countries = []
        
        try:
            logger.info(f"Starting analysis of legislation: {article_reference}")
            logger.info(f"Applicable countries: {applicable_countries}")
            logger.info(f"Adequacy countries: {adequacy_countries}")
            
            # Get existing rules context
            existing_context = self.rule_manager.get_context_summary()
            
            # Create metadata context for LLM
            metadata_context = f"""
            LEGISLATION METADATA:
            - Applicable Countries: {', '.join(applicable_countries) if applicable_countries else 'Not specified'}
            - Adequacy Countries: {', '.join(adequacy_countries) if adequacy_countries else 'None specified'}
            """
            
            # Step 1: Apply advanced prompting strategies with context
            cot_analysis = await self._apply_chain_of_thought(legislation_text, existing_context + metadata_context)
            moe_analysis = await self._apply_mixture_of_experts(legislation_text, existing_context + metadata_context)
            mot_analysis = await self._apply_mixture_of_thought(legislation_text, existing_context + metadata_context)
            mor_analysis = await self._apply_mixture_of_reasoning(legislation_text, existing_context + metadata_context)
            
            # Step 2: Use react agent for comprehensive analysis
            agent_analysis = await self._run_react_agent(legislation_text, article_reference)
            
            # Step 3: Synthesize all analyses into structured rules
            rules = await self._synthesize_rules(
                legislation_text, 
                article_reference,
                source_file,
                existing_context,
                metadata_context,
                applicable_countries,
                adequacy_countries,
                cot_analysis,
                moe_analysis, 
                mot_analysis,
                mor_analysis,
                agent_analysis
            )
            
            # Step 4: Generate embeddings for rules
            if rules:
                rule_texts = [f"{rule.description} {rule.source_article}" for rule in rules]
                embeddings = await self.openai_service.get_embeddings(rule_texts)
            else:
                embeddings = []
            
            # Step 5: Calculate processing time
            end_time = datetime.utcnow()
            processing_time = (end_time - start_time).total_seconds()
            
            # Create result
            result = ExtractionResult(
                rules=rules,
                summary=f"Extracted {len(rules)} rules from {article_reference}",
                total_rules=len(rules),
                processing_time=processing_time,
                embeddings=embeddings
            )
            
            logger.info(f"Analysis completed: {len(rules)} rules extracted in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing legislation: {e}")
            raise
    
    async def _apply_chain_of_thought(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Chain of Thought prompting strategy."""
        prompt = PromptingStrategies.chain_of_thought_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="You are an expert legal analyst. Use step-by-step reasoning."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_experts(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Experts prompting strategy."""
        prompt = PromptingStrategies.mixture_of_experts_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="You are a panel of legal experts with different specializations."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_thought(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Thought prompting strategy."""
        prompt = PromptingStrategies.mixture_of_thought_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="Apply diverse thinking approaches to comprehensive analysis."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_reasoning(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Reasoning prompting strategy.""" 
        prompt = PromptingStrategies.mixture_of_reasoning_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="Use multiple reasoning strategies for thorough analysis."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _run_react_agent(self, legislation_text: str, article_reference: str) -> str:
        """Run the react agent for comprehensive analysis."""
        try:
            config = {"configurable": {"thread_id": f"analysis_{datetime.utcnow().timestamp()}"}}
            
            message = f"""
            Analyze the following legislation and use all available tools to extract comprehensive information:
            
            Article: {article_reference}
            Text: {legislation_text}
            
            Use the tools to:
            1. Extract specific rule conditions
            2. Analyze data domains
            3. Identify roles and responsibilities
            
            Provide a comprehensive analysis that can be used to create machine-readable rules.
            """
            
            result = self.agent.invoke(
                {"messages": [HumanMessage(content=message)]},
                config
            )
            
            # Extract the final message content
            if result and "messages" in result:
                last_message = result["messages"][-1]
                if hasattr(last_message, 'content'):
                    return last_message.content
                elif isinstance(last_message, dict) and 'content' in last_message:
                    return last_message['content']
            
            return "Agent analysis completed but no content returned"
            
        except Exception as e:
            logger.error(f"Error running react agent: {e}")
            return f"Error in agent analysis: {str(e)}"
    
    async def _synthesize_rules(
        self, 
        legislation_text: str,
        article_reference: str,
        source_file: str,
        existing_context: str,
        metadata_context: str,
        applicable_countries: List[str],
        adequacy_countries: List[str],
        cot_analysis: str,
        moe_analysis: str, 
        mot_analysis: str,
        mor_analysis: str,
        agent_analysis: str
    ) -> List[LegislationRule]:
        """Synthesize all analyses into structured rules."""
        
        synthesis_prompt = f"""
        Based on the comprehensive analyses below, create machine-readable rules in JSON format that align with json-rules-engine structure.
        
        EXISTING RULES CONTEXT:
        {existing_context}
        
        METADATA CONTEXT:
        {metadata_context}
        
        ORIGINAL LEGISLATION:
        Article: {article_reference}
        Source File: {source_file}
        Text: {legislation_text}
        
        ANALYSIS RESULTS:
        
        Chain of Thought Analysis:
        {cot_analysis}
        
        Mixture of Experts Analysis:
        {moe_analysis}
        
        Mixture of Thought Analysis:
        {mot_analysis}
        
        Mixture of Reasoning Analysis:
        {mor_analysis}
        
        Agent Tool Analysis:
        {agent_analysis}
        
        REQUIREMENTS:
        1. Create rules in json-rules-engine format with conditions containing 'all', 'any', or 'not' keys
        2. Each condition must have: fact, operator, value, description, data_domain, role, reasoning
        3. Infer appropriate data domains from: data_transfer, data_usage, data_storage
        4. Assign roles from: controller, processor, joint_controller
        5. Determine primary_impacted_role and secondary_impacted_role (optional)
        6. Identify data_category from: personal_data, sensitive_data, biometric_data, health_data, financial_data, location_data, behavioral_data, identification_data
        7. Use the provided country metadata for applicable_countries and adequacy_countries
        8. Use LLM reasoning capabilities to infer domains, roles, and categories not explicitly stated
        9. Provide confidence scores (0.0-1.0) for each rule
        10. Consider existing rules context to maintain consistency and avoid duplication
        
        Return a JSON array of rules. Each rule must follow this exact structure:
        {{
            "id": "unique_id_based_on_content",
            "name": "rule_name", 
            "description": "human_readable_description",
            "source_article": "{article_reference}",
            "source_file": "{source_file}",
            "conditions": {{
                "all": [
                    {{
                        "fact": "fact_name",
                        "operator": "equal",
                        "value": "comparison_value",
                        "path": "$.optional.json.path",
                        "description": "condition_description",
                        "data_domain": ["data_transfer"],
                        "role": "controller",
                        "reasoning": "why_this_condition_was_extracted"
                    }}
                ]
            }},
            "event": {{
                "type": "compliance_required",
                "params": {{
                    "action": "specific_action_required"
                }}
            }},
            "priority": 1,
            "primary_impacted_role": "controller",
            "secondary_impacted_role": "processor",
            "data_category": ["personal_data", "sensitive_data"],
            "applicable_countries": {json.dumps(applicable_countries)},
            "adequacy_countries": {json.dumps(adequacy_countries)},
            "confidence_score": 0.85
        }}
        
        Return ONLY valid JSON array, no other text.
        """
        
        messages = [
            SystemMessage(content="You are a legal-tech expert. Return only valid JSON."),
            HumanMessage(content=synthesis_prompt)
        ]
        
        response = await self.openai_service.chat_completion(messages)
        
        # Parse JSON response safely
        parsed_data = self.json_parser.parse_json_response(response)
        
        if "error" in parsed_data:
            logger.error(f"Failed to parse rules JSON: {parsed_data}")
            return []
        
        # Convert to Pydantic models
        rules = []
        try:
            if isinstance(parsed_data, list):
                rule_data_list = parsed_data
            elif isinstance(parsed_data, dict) and "rules" in parsed_data:
                rule_data_list = parsed_data["rules"]
            else:
                rule_data_list = [parsed_data]
            
            for rule_data in rule_data_list:
                try:
                    # Ensure required fields have defaults
                    rule_data.setdefault("priority", 1)
                    rule_data.setdefault("confidence_score", 0.8)
                    rule_data.setdefault("source_article", article_reference)
                    rule_data.setdefault("source_file", source_file)
                    rule_data.setdefault("primary_impacted_role", "controller")
                    rule_data.setdefault("data_category", ["personal_data"])
                    rule_data.setdefault("applicable_countries", applicable_countries)
                    rule_data.setdefault("adequacy_countries", adequacy_countries)
                    
                    # Create rule
                    rule = LegislationRule(**rule_data)
                    rules.append(rule)
                    
                except Exception as e:
                    logger.warning(f"Skipping invalid rule: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error creating rule objects: {e}")
            
        return rules

# ===============================
# MAIN EXECUTION FUNCTION
# ===============================

async def main():
    """Main execution function demonstrating the system."""
    
    # Initialize analyzer
    analyzer = LegislationAnalyzer()
    
    try:
        print("\n=== LEGISLATION RULES CONVERTER ===")
        print("Processing legislation PDFs and extracting machine-readable rules...\n")
        
        # Show metadata configuration status
        print(" METADATA CONFIGURATION:")
        configured_files = analyzer.metadata_manager.list_configured_files()
        if configured_files:
            print(f" Metadata configured for {len(configured_files)} files:")
            for file in configured_files:
                metadata = analyzer.metadata_manager.get_file_metadata(file)
                print(f"    {file}")
                print(f"       Countries: {', '.join(metadata['applicable_countries']) if metadata['applicable_countries'] else 'None'}")
                print(f"       Adequacy: {', '.join(metadata['adequacy_countries']) if metadata['adequacy_countries'] else 'None'}")
        else:
            print(" No metadata configured yet.")
        
        print(f" Metadata config file: {Config.METADATA_CONFIG_FILE}")
        print()
        
        # Check if PDF processing is available
        if not PDF_AVAILABLE:
            print(" Warning: PDF processing libraries not available.")
            print("Install with: pip install PyMuPDF pdfplumber")
            print("PyMuPDF is recommended for better performance and modern API.")
            
            # Fallback to example text processing
            print("Using example GDPR Article 28 for demonstration...\n")
            
            gdpr_article_28 = """
            Article 28 - Processor
            
            1. Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing sufficient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject.
            
            2. The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.
            
            3. Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller.
            """
            
            # Analyze the example legislation
            result = await analyzer.analyze_legislation(
                legislation_text=gdpr_article_28,
                article_reference="GDPR Article 28",
                source_file="example_gdpr_article_28.txt"
            )
        else:
            # Process PDF folder
            print(" Scanning for PDF files in legislation folder...")
            
            # Ensure directories exist
            os.makedirs(Config.LEGISLATION_PDF_PATH, exist_ok=True)
            
            # Check if there are PDFs to process
            pdf_files = analyzer.pdf_reader.get_pdf_files(Config.LEGISLATION_PDF_PATH)
            if not pdf_files:
                print(f" No PDF files found in {Config.LEGISLATION_PDF_PATH}")
                print("Add PDF files to the folder and run again.")
                
                # Create a sample with example text for demonstration
                print("\nUsing example GDPR Article 28 for demonstration...\n")
                
                gdpr_article_28 = """
                Article 28 - Processor
                
                1. Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing sufficient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject.
                
                2. The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.
                
                3. Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller.
                """
                
                result = await analyzer.analyze_legislation(
                    legislation_text=gdpr_article_28,
                    article_reference="GDPR Article 28",
                    source_file="example_gdpr_article_28.txt",
                    applicable_countries=["Germany", "France", "Italy", "Spain"],
                    adequacy_countries=["Canada", "Japan", "United Kingdom"]
                )
            else:
                # Process all PDFs in the folder
                result = await analyzer.process_legislation_folder()
        
        # Print results
        print(f"\n=== EXTRACTION RESULTS ===")
        print(f" Summary: {result.summary}")
        print(f" Total Rules: {result.total_rules}")
        print(f" Processing Time: {result.processing_time:.2f} seconds")
        
        if result.rules:
            print(f"\n=== EXTRACTED RULES DETAILS ===")
            for i, rule in enumerate(result.rules, 1):
                print(f"\n Rule {i}: {rule.name}")
                print(f"    Description: {rule.description}")
                print(f"    Source: {rule.source_article} ({rule.source_file})")
                print(f"    Primary Role: {rule.primary_impacted_role.value}")
                if rule.secondary_impacted_role:
                    print(f"    Secondary Role: {rule.secondary_impacted_role.value}")
                print(f"    Data Categories: {', '.join([cat.value for cat in rule.data_category])}")
                print(f"    Applicable Countries: {', '.join(rule.applicable_countries) if rule.applicable_countries else 'Not specified'}")
                print(f"    Adequacy Countries: {', '.join(rule.adequacy_countries) if rule.adequacy_countries else 'None'}")
                print(f"    Confidence: {rule.confidence_score}")
                print(f"    Priority: {rule.priority}")
                
                print(f"    Conditions:")
                for logic_type, conditions in rule.conditions.items():
                    print(f"      {logic_type.upper()}:")
                    for condition in conditions:
                        print(f"        - {condition.description}")
                        print(f"          Fact: {condition.fact} | Operator: {condition.operator} | Value: {condition.value}")
                        print(f"          Role: {condition.role.value} | Domains: {', '.join([d.value for d in condition.data_domain])}")
                
                print(f"    Event: {rule.event.type}")
                print("-" * 80)
        
        # Save results in both formats
        if result.rules:
            print(f"\n=== SAVING RESULTS ===")
            
            # Ensure output directory exists
            os.makedirs(Config.RULES_OUTPUT_PATH, exist_ok=True)
            
            # Generate timestamp for unique filenames
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            
            # Save JSON format
            json_file = os.path.join(Config.RULES_OUTPUT_PATH, f"extracted_rules_{timestamp}.json")
            result.save_json(json_file)
            print(f" JSON saved to: {json_file}")
            
            # Save CSV format
            csv_file = os.path.join(Config.RULES_OUTPUT_PATH, f"extracted_rules_{timestamp}.csv")
            result.save_csv(csv_file)
            print(f" CSV saved to: {csv_file}")
            
            # Show existing rules summary
            total_existing = len(analyzer.rule_manager.existing_rules)
            print(f"\n=== RULE DATABASE STATUS ===")
            print(f" Total rules in database: {total_existing}")
            print(f" New rules added: {len(result.rules)}")
            print(f" Database file: {Config.EXISTING_RULES_FILE}")
            
        else:
            print("\n No rules were extracted.")
        
        print(f"\n Processing complete!")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise

if __name__ == "__main__":
    # Run the main function
    asyncio.run(main())
