import os
import json
import logging
import requests
from dotenv import load_dotenv
from azure.identity import ClientSecretCredential, get_bearer_token_provider

# ------------------------------
# Load Environment Variables
# ------------------------------
load_dotenv("config/dev")         # Loads general settings
load_dotenv("config/dev.creds")    # Loads credentials

# Retrieve Azure OpenAI endpoints and deployment names
chat_endpoint = os.getenv("AZURE_OPENAI_CHAT_ENDPOINT")
embedding_endpoint = os.getenv("AZURE_OPENAI_EMBEDDINGS_ENDPOINT")
chat_deployment = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "gpt-4o-mini")
embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
# If using API key for embeddings, it would be here:
embedding_api_key = os.getenv("AZURE_OPENAI_EMBEDDINGS_API_KEY")
openai_api_version = os.getenv("OPENAI_API_VERSION", "2023-05-15")

if not chat_endpoint or not embedding_endpoint:
    raise ValueError("Please set both AZURE_OPENAI_CHAT_ENDPOINT and AZURE_OPENAI_EMBEDDINGS_ENDPOINT.")

# Ensure endpoints include the required "/openai" suffix
if "/openai" not in chat_endpoint:
    chat_endpoint = chat_endpoint.rstrip("/") + "/openai"
if "/openai" not in embedding_endpoint:
    embedding_endpoint = embedding_endpoint.rstrip("/") + "/openai"

# ------------------------------
# Proxy and SSL Configuration
# ------------------------------
proxy_url = "http://44444:unuou123.lts@abc.uk.systems:80"
os.environ['HTTP_PROXY'] = proxy_url
os.environ['HTTPS_PROXY'] = proxy_url
os.environ["REQUESTS_CA_BUNDLE"] = 'cacert.pem'

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Create a requests session that forces our custom proxy and uses our CA bundle.
session = requests.Session()
session.verify = 'cacert.pem'
session.trust_env = False  # Disable use of system environment proxies.
session.proxies = {"http": proxy_url, "https": proxy_url}

# ------------------------------
# Initialize Azure Client Secret Credential and Token Provider
# ------------------------------
# Retrieve credentials from environment variables (loaded from dev.creds)
client_id = os.getenv("AZURE_CLIENT_ID")
tenant_id = os.getenv("AZURE_TENANT_ID")
client_secret = os.getenv("AZURE_CLIENT_SECRET")

if not client_id or not tenant_id or not client_secret:
    raise ValueError("Please ensure AZURE_CLIENT_ID, AZURE_TENANT_ID, and AZURE_CLIENT_SECRET are set in your credentials.")

# Create a ClientSecretCredential using your provided credentials
credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)
# Use get_bearer_token_provider to create a token provider function for the desired scope
token_provider = get_bearer_token_provider(credential, "https://cognitiveservices.azure.com/.default")

# ------------------------------
# Function to Get Embeddings
# ------------------------------
def get_embeddings(texts, endpoint, deployment_name="text-embedding-3-large", batch_size=100):
    # Use API key if provided; otherwise, use bearer token from our token_provider.
    if embedding_api_key:
        headers = {
            'api-key': embedding_api_key,
            'Content-Type': 'application/json'
        }
    else:
        token = token_provider()
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json'
        }
    
    api_url = f"{endpoint}/deployments/{deployment_name}/embeddings?api-version={openai_api_version}"
    logger.info(f"Requesting embeddings from URL: {api_url}")
    
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        try:
            payload = {"input": batch}
            response = session.post(api_url, headers=headers, json=payload)
            if response.status_code == 200:
                data = response.json()
                batch_embeddings = [item['embedding'] for item in data.get("data", [])]
                embeddings.extend(batch_embeddings)
                logger.info(f"Received embeddings for batch {i+1}-{min(i+batch_size, len(texts))}")
            else:
                logger.error(f"Failed to receive embeddings for batch {i+1}-{min(i+batch_size, len(texts))}, status code: {response.status_code}")
                logger.error(response.text)
                embeddings.extend([None] * len(batch))
        except Exception as e:
            logger.error(f"Error processing batch {i+1}-{min(i+batch_size, len(texts))}: {str(e)}")
            embeddings.extend([None] * len(batch))
    return embeddings

def test_connection(endpoint):
    try:
        test_texts = ['Hello World']
        embeds = get_embeddings(test_texts, endpoint)
        if embeds and embeds[0]:
            print(f"Embedding Dimension: {len(embeds[0])}")
            return True
        else:
            print("Failed to retrieve embeddings")
            return False
    except Exception as e:
        print(f"Failed to connect to {endpoint}: {str(e)}")
        return False

# ------------------------------
# Main Execution
# ------------------------------
if __name__ == "__main__":
    # Replace with your actual Azure Cognitive Services endpoint
    azure_endpoint = "https://your-azure-endpoint.cognitiveservices.azure.com"
    if test_connection(azure_endpoint):
        print("Connection successful")
    else:
        print("Connection failed")
