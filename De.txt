#!/usr/bin/env python3
"""
Enhanced IT Incident Data Quality Analysis with LangGraph Multi-Agent Architecture
This script uses ReAct agents and supervisor orchestration for comprehensive incident analysis.

Architecture:
- Supervisor Agent: Orchestrates the entire workflow
- Data Quality Analyzer Agent: Identifies DQ issues and dimensions
- Evidence Assessment Agent: Evaluates supporting/contradicting evidence
- Confidence Scoring Agent: Calculates weighted confidence scores  
- Validation Agent: Ensures output quality and consistency

Enhanced with:
- LangGraph ReAct agents for reasoning and acting
- Multi-agent supervisor orchestration
- Hierarchical agent teams
- Advanced state management
- Comprehensive validation pipeline

Required packages:
pip install openai pandas pydantic tenacity langgraph langchain langchain-openai langchain-community

Usage:
1. Set your OpenAI API key in the API_KEY variable below
2. Set the correct path to your CSV file in INPUT_CSV_PATH
3. Run the script: python enhanced_incident_analysis.py
"""

import pandas as pd
import json
import time
import logging
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Literal, TypedDict, Annotated
from datetime import datetime

# Core imports
from openai import OpenAI
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator
from pydantic.types import PositiveFloat
from pydantic import ValidationError

# Tenacity retry imports
from tenacity import (
    retry, 
    stop_after_attempt, 
    wait_exponential, 
    retry_if_exception_type,
    before_log,
    after_log,
    RetryError
)

# LangGraph and LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate

# LangGraph imports
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.types import Command
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Global Configuration
# ===================
MODEL = "gpt-4o"  # Using latest GPT-4o for better performance
BASE_URL = "https://api.openai.com/v1"
API_KEY = "your-openai-api-key-here"  # REQUIRED: Replace with your actual OpenAI API key

# File paths
INPUT_CSV_PATH = "incidents.csv"
OUTPUT_JSON_PATH = "enhanced_incident_analysis_results.json"
OUTPUT_CSV_PATH = "enhanced_incident_analysis_results.csv"

# Agent configuration
MAX_RETRIES = 3
RETRY_MULTIPLIER = 1
RETRY_MIN_WAIT = 1
RETRY_MAX_WAIT = 10
API_RATE_LIMIT_DELAY = 0.2

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_incident_analysis.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Initialize LLM for agents
llm = ChatOpenAI(
    model=MODEL,
    api_key=API_KEY,
    base_url=BASE_URL if BASE_URL != "https://api.openai.com/v1" else None,
    temperature=0.1,  # Low temperature for consistent analysis
    max_tokens=2000
)

# Pydantic Models for Enhanced Validation
# ======================================

class IncidentData(BaseModel):
    """Enhanced incident data model with validation."""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        extra='allow'
    )
    
    INCIDENT_ID: str = Field(default="", description="Unique incident identifier")
    IT_INCIDENT_DESC: str = Field(default="", description="Incident description")
    IT_INCIDENT_AREA_CATEGORY_NAME: str = Field(default="", description="Area category")
    IT_INCIDENT_AREA_SUBCATEGORY_NAME: str = Field(default="", description="Area subcategory")
    IT_INCIDENT_CAUSE_TEXT: str = Field(default="", description="Incident cause")
    MAJOR_IT_INCIDENT_SUMMARY_TEXT: str = Field(default="", description="Major incident summary")
    MAJOR_IT_INCIDENT_FINDINGS_TEXT: str = Field(default="", description="Major incident findings")
    IT_INCIDENT_RESOLUTION_DESC: str = Field(default="", description="Resolution description")

class DQAnalysis(BaseModel):
    """Data Quality analysis result."""
    is_data_related: bool = Field(description="Whether incident is data-related")
    dq_dimensions: List[str] = Field(default_factory=list, description="Applicable DQ dimensions")
    primary_indicators: List[str] = Field(default_factory=list, description="Primary data quality indicators")
    reasoning: str = Field(description="Analysis reasoning")

class EvidenceAssessment(BaseModel):
    """Evidence assessment result."""
    supporting_evidence: List[str] = Field(default_factory=list, description="Supporting evidence")
    contradicting_evidence: List[str] = Field(default_factory=list, description="Contradicting evidence")
    evidence_strength: Dict[str, int] = Field(default_factory=dict, description="Evidence strength scores")
    assessment_reasoning: str = Field(description="Evidence assessment reasoning")

class ConfidenceScore(BaseModel):
    """Confidence scoring result."""
    confidence_score: float = Field(ge=0, le=100, description="Confidence score 0-100")
    calculation_method: str = Field(description="How confidence was calculated")
    contributing_factors: List[str] = Field(default_factory=list, description="Factors affecting confidence")

class ValidationResult(BaseModel):
    """Final validation result."""
    is_valid: bool = Field(description="Whether analysis is valid")
    validation_issues: List[str] = Field(default_factory=list, description="Any validation issues")
    final_recommendation: str = Field(description="Final recommendation")

class MultiAgentState(TypedDict):
    """State shared across all agents."""
    messages: List[BaseMessage]
    incident_data: Dict[str, Any]
    dq_analysis: Optional[DQAnalysis]
    evidence_assessment: Optional[EvidenceAssessment]
    confidence_score: Optional[ConfidenceScore]
    validation_result: Optional[ValidationResult]
    current_agent: str
    workflow_step: int
    errors: List[str]

class FinalAnalysisResult(BaseModel):
    """Final comprehensive analysis result."""
    model_config = ConfigDict(extra='allow')
    
    # Original incident data
    INCIDENT_ID: str = ""
    IT_INCIDENT_DESC: str = ""
    IT_INCIDENT_AREA_CATEGORY_NAME: str = ""
    IT_INCIDENT_AREA_SUBCATEGORY_NAME: str = ""
    IT_INCIDENT_CAUSE_TEXT: str = ""
    MAJOR_IT_INCIDENT_SUMMARY_TEXT: str = ""
    MAJOR_IT_INCIDENT_FINDINGS_TEXT: str = ""
    IT_INCIDENT_RESOLUTION_DESC: str = ""
    
    # Enhanced analysis results
    IS_DATA_ISSUE: str = Field(description="Y or N flag")
    DQ_DIMENSIONS: str = Field(default="", description="Comma-separated DQ dimensions")
    PRIMARY_INDICATORS: str = Field(default="", description="Primary DQ indicators")
    SUPPORTING_EVIDENCE: str = Field(default="", description="Supporting evidence")
    CONTRADICTING_EVIDENCE: str = Field(default="", description="Contradicting evidence")
    EVIDENCE_STRENGTH_SCORES: str = Field(default="", description="Evidence strength scores")
    CONFIDENCE_SCORE: float = Field(description="Confidence score")
    CONFIDENCE_CALCULATION: str = Field(default="", description="Confidence calculation method")
    VALIDATION_STATUS: str = Field(default="", description="Validation status")
    FINAL_REASONING: str = Field(description="Final comprehensive reasoning")
    PROCESSING_STATUS: str = Field(default="SUCCESS", description="Processing status")
    PROCESSING_TIMESTAMP: str = Field(default_factory=lambda: datetime.now().isoformat())
    WORKFLOW_STEPS: int = Field(default=0, description="Number of workflow steps")

# Agent Tools
# ===========

@tool
def analyze_dq_patterns(incident_text: str) -> str:
    """Analyze text for data quality patterns and indicators."""
    dq_patterns = {
        "Accuracy": ["incorrect", "wrong", "inaccurate", "error", "mismatch", "discrepancy"],
        "Completeness": ["missing", "incomplete", "null", "empty", "blank", "absent"],
        "Consistency": ["inconsistent", "conflicting", "contradiction", "sync", "duplicate"],
        "Validity": ["invalid", "format", "schema", "constraint", "rule violation"],
        "Uniqueness": ["duplicate", "repeated", "multiple", "redundant"],
        "Timeliness": ["outdated", "stale", "delayed", "late", "timestamp"],
        "Integrity": ["corrupted", "damaged", "broken", "altered", "modified"],
        "Availability": ["unavailable", "inaccessible", "down", "offline"],
        "Reasonableness": ["unreasonable", "outlier", "anomaly", "suspicious"]
    }
    
    found_patterns = {}
    text_lower = incident_text.lower()
    
    for dimension, keywords in dq_patterns.items():
        matches = [kw for kw in keywords if kw in text_lower]
        if matches:
            found_patterns[dimension] = matches
    
    return json.dumps(found_patterns)

@tool
def assess_evidence_strength(evidence_text: str, evidence_type: str) -> int:
    """Assess the strength of evidence (1-3 scale)."""
    strong_indicators = ["corruption", "missing records", "data loss", "invalid format", "constraint violation"]
    medium_indicators = ["sync issue", "mismatch", "incorrect value", "duplicate", "inconsistent"]
    weak_indicators = ["possible", "might", "could be", "potentially", "appears to"]
    
    text_lower = evidence_text.lower()
    
    if any(indicator in text_lower for indicator in strong_indicators):
        return 3
    elif any(indicator in text_lower for indicator in medium_indicators):
        return 2
    elif any(indicator in text_lower for indicator in weak_indicators):
        return 1
    else:
        return 1  # Default weak

@tool
def calculate_weighted_confidence(supporting_weights: List[int], contradicting_weights: List[int]) -> float:
    """Calculate weighted confidence score based on evidence."""
    if not supporting_weights and not contradicting_weights:
        return 50.0
    
    total_supporting = sum(supporting_weights) if supporting_weights else 0
    total_contradicting = sum(contradicting_weights) if contradicting_weights else 0
    total_evidence = total_supporting + total_contradicting
    
    if total_evidence == 0:
        return 50.0
    
    confidence = (total_supporting / total_evidence) * 100
    
    # Adjustments for edge cases
    if total_contradicting == 0 and total_supporting >= 6:  # Strong supporting evidence only
        confidence = min(95.0, confidence + 10)
    elif total_supporting == 0 and total_contradicting >= 6:  # Strong contradicting evidence only
        confidence = max(5.0, confidence - 10)
    
    return round(confidence, 1)

# Specialized Agents
# ==================

class DataQualityAnalyzerAgent:
    """Specialized agent for data quality analysis."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [analyze_dq_patterns]
        self.agent = create_react_agent(
            model=llm,
            tools=self.tools,
            prompt="""You are a Data Quality Analysis Expert. Your job is to:

1. Analyze incident descriptions for data quality issues
2. Identify relevant DQ dimensions (Accuracy, Completeness, Consistency, Validity, Uniqueness, Timeliness, Integrity, Availability, Reasonableness)
3. Find primary indicators that suggest data quality problems
4. Provide clear reasoning for your analysis

Use the analyze_dq_patterns tool to systematically examine the incident text.

Always respond with a JSON object containing:
{
    "is_data_related": boolean,
    "dq_dimensions": [list of applicable dimensions],
    "primary_indicators": [list of key indicators found],
    "reasoning": "detailed explanation"
}"""
        )
    
    def analyze(self, state: MultiAgentState) -> MultiAgentState:
        """Perform data quality analysis."""
        try:
            incident_data = state["incident_data"]
            
            # Combine all incident text for analysis
            combined_text = " ".join([
                incident_data.get("IT_INCIDENT_DESC", ""),
                incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
                incident_data.get("IT_INCIDENT_RESOLUTION_DESC", "")
            ]).strip()
            
            if not combined_text:
                # Handle empty incident data
                state["dq_analysis"] = DQAnalysis(
                    is_data_related=False,
                    dq_dimensions=[],
                    primary_indicators=[],
                    reasoning="No incident text available for analysis"
                )
                return state
            
            # Use ReAct agent for analysis
            messages = [HumanMessage(content=f"Analyze this IT incident for data quality issues:\n\n{combined_text}")]
            response = self.agent.invoke({"messages": messages})
            
            # Extract JSON from response
            response_text = response["messages"][-1].content
            
            # Parse the analysis result
            try:
                # Try to extract JSON from the response
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    analysis_data = json.loads(json_match.group())
                    state["dq_analysis"] = DQAnalysis(**analysis_data)
                else:
                    raise ValueError("No JSON found in response")
            
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Failed to parse DQ analysis JSON: {e}. Using fallback analysis.")
                # Fallback analysis
                state["dq_analysis"] = DQAnalysis(
                    is_data_related="data" in combined_text.lower(),
                    dq_dimensions=[],
                    primary_indicators=[],
                    reasoning=f"Fallback analysis due to parsing error: {str(e)}"
                )
            
            state["current_agent"] = "evidence_assessor"
            state["workflow_step"] = state.get("workflow_step", 0) + 1
            
        except Exception as e:
            logger.error(f"DQ Analyzer error: {e}")
            state["errors"].append(f"DQ Analysis failed: {str(e)}")
            state["dq_analysis"] = DQAnalysis(
                is_data_related=False,
                dq_dimensions=[],
                primary_indicators=[],
                reasoning=f"Analysis failed due to error: {str(e)}"
            )
        
        return state

class EvidenceAssessmentAgent:
    """Specialized agent for evidence assessment."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [assess_evidence_strength]
        self.agent = create_react_agent(
            model=llm,
            tools=self.tools,
            prompt="""You are an Evidence Assessment Expert. Your job is to:

1. Identify supporting evidence for data quality issues
2. Identify contradicting evidence against data quality issues
3. Assess the strength of each piece of evidence (1-3 scale)
4. Provide reasoning for your assessment

Use the assess_evidence_strength tool to evaluate evidence strength.

Always respond with a JSON object containing:
{
    "supporting_evidence": [list of supporting evidence],
    "contradicting_evidence": [list of contradicting evidence],
    "evidence_strength": {"evidence_item": strength_score},
    "assessment_reasoning": "detailed explanation"
}"""
        )
    
    def assess(self, state: MultiAgentState) -> MultiAgentState:
        """Perform evidence assessment."""
        try:
            incident_data = state["incident_data"]
            dq_analysis = state.get("dq_analysis")
            
            if not dq_analysis:
                state["evidence_assessment"] = EvidenceAssessment(
                    supporting_evidence=[],
                    contradicting_evidence=[],
                    evidence_strength={},
                    assessment_reasoning="No DQ analysis available"
                )
                return state
            
            combined_text = " ".join([
                incident_data.get("IT_INCIDENT_DESC", ""),
                incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
                incident_data.get("IT_INCIDENT_RESOLUTION_DESC", "")
            ]).strip()
            
            # Create assessment prompt
            assessment_prompt = f"""
            Based on the DQ analysis that found: {dq_analysis.reasoning}
            
            Analyze this incident text for supporting and contradicting evidence:
            {combined_text}
            
            Look for:
            - Supporting evidence: indicators that confirm this is a data quality issue
            - Contradicting evidence: indicators that suggest this is NOT a data quality issue (hardware, network, user error, etc.)
            """
            
            messages = [HumanMessage(content=assessment_prompt)]
            response = self.agent.invoke({"messages": messages})
            
            response_text = response["messages"][-1].content
            
            try:
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    assessment_data = json.loads(json_match.group())
                    state["evidence_assessment"] = EvidenceAssessment(**assessment_data)
                else:
                    raise ValueError("No JSON found in response")
            
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Failed to parse evidence assessment JSON: {e}")
                state["evidence_assessment"] = EvidenceAssessment(
                    supporting_evidence=[],
                    contradicting_evidence=[],
                    evidence_strength={},
                    assessment_reasoning=f"Fallback assessment due to parsing error: {str(e)}"
                )
            
            state["current_agent"] = "confidence_scorer"
            state["workflow_step"] = state.get("workflow_step", 0) + 1
            
        except Exception as e:
            logger.error(f"Evidence Assessment error: {e}")
            state["errors"].append(f"Evidence Assessment failed: {str(e)}")
            state["evidence_assessment"] = EvidenceAssessment(
                supporting_evidence=[],
                contradicting_evidence=[],
                evidence_strength={},
                assessment_reasoning=f"Assessment failed due to error: {str(e)}"
            )
        
        return state

class ConfidenceScoringAgent:
    """Specialized agent for confidence scoring."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [calculate_weighted_confidence]
        self.agent = create_react_agent(
            model=llm,
            tools=self.tools,
            prompt="""You are a Confidence Scoring Expert. Your job is to:

1. Calculate confidence scores based on evidence strength
2. Use weighted scoring methodology
3. Consider contributing factors
4. Explain calculation method

Use the calculate_weighted_confidence tool with evidence strength weights.

Always respond with a JSON object containing:
{
    "confidence_score": float (0-100),
    "calculation_method": "explanation of how score was calculated",
    "contributing_factors": [list of factors that influenced the score]
}"""
        )
    
    def score(self, state: MultiAgentState) -> MultiAgentState:
        """Calculate confidence score."""
        try:
            evidence_assessment = state.get("evidence_assessment")
            
            if not evidence_assessment:
                state["confidence_score"] = ConfidenceScore(
                    confidence_score=50.0,
                    calculation_method="Default score due to missing evidence assessment",
                    contributing_factors=["No evidence assessment available"]
                )
                return state
            
            # Extract evidence strengths
            supporting_weights = []
            contradicting_weights = []
            
            for evidence in evidence_assessment.supporting_evidence:
                strength = evidence_assessment.evidence_strength.get(evidence, 1)
                supporting_weights.append(strength)
            
            for evidence in evidence_assessment.contradicting_evidence:
                strength = evidence_assessment.evidence_strength.get(evidence, 1)
                contradicting_weights.append(strength)
            
            # Create scoring prompt
            scoring_prompt = f"""
            Calculate confidence score for data quality assessment:
            
            Supporting evidence weights: {supporting_weights}
            Contradicting evidence weights: {contradicting_weights}
            
            Use the calculate_weighted_confidence tool and explain your methodology.
            """
            
            messages = [HumanMessage(content=scoring_prompt)]
            response = self.agent.invoke({"messages": messages})
            
            response_text = response["messages"][-1].content
            
            try:
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    scoring_data = json.loads(json_match.group())
                    state["confidence_score"] = ConfidenceScore(**scoring_data)
                else:
                    # Fallback calculation
                    score = calculate_weighted_confidence.func(supporting_weights, contradicting_weights)
                    state["confidence_score"] = ConfidenceScore(
                        confidence_score=score,
                        calculation_method="Weighted average of evidence strengths",
                        contributing_factors=[f"Supporting: {len(supporting_weights)}", f"Contradicting: {len(contradicting_weights)}"]
                    )
            
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Failed to parse confidence scoring JSON: {e}")
                score = calculate_weighted_confidence.func(supporting_weights, contradicting_weights)
                state["confidence_score"] = ConfidenceScore(
                    confidence_score=score,
                    calculation_method="Fallback calculation due to parsing error",
                    contributing_factors=[f"Parse error: {str(e)}"]
                )
            
            state["current_agent"] = "validator"
            state["workflow_step"] = state.get("workflow_step", 0) + 1
            
        except Exception as e:
            logger.error(f"Confidence Scoring error: {e}")
            state["errors"].append(f"Confidence Scoring failed: {str(e)}")
            state["confidence_score"] = ConfidenceScore(
                confidence_score=0.0,
                calculation_method=f"Error in calculation: {str(e)}",
                contributing_factors=["Calculation error"]
            )
        
        return state

class ValidationAgent:
    """Specialized agent for final validation."""
    
    def __init__(self, llm):
        self.llm = llm
        self.agent = create_react_agent(
            model=llm,
            tools=[],
            prompt="""You are a Validation Expert. Your job is to:

1. Review the complete analysis for consistency
2. Identify any validation issues
3. Provide final recommendations
4. Ensure logical coherence

Always respond with a JSON object containing:
{
    "is_valid": boolean,
    "validation_issues": [list of any issues found],
    "final_recommendation": "comprehensive final recommendation"
}"""
        )
    
    def validate(self, state: MultiAgentState) -> MultiAgentState:
        """Perform final validation."""
        try:
            dq_analysis = state.get("dq_analysis")
            evidence_assessment = state.get("evidence_assessment")
            confidence_score = state.get("confidence_score")
            
            validation_issues = []
            
            # Check for consistency
            if dq_analysis and evidence_assessment:
                if dq_analysis.is_data_related and not evidence_assessment.supporting_evidence:
                    validation_issues.append("Data issue identified but no supporting evidence found")
                
                if not dq_analysis.is_data_related and evidence_assessment.supporting_evidence:
                    validation_issues.append("No data issue identified but supporting evidence exists")
            
            # Check confidence score reasonableness
            if confidence_score:
                if confidence_score.confidence_score > 90 and evidence_assessment and evidence_assessment.contradicting_evidence:
                    validation_issues.append("High confidence score despite contradicting evidence")
                
                if confidence_score.confidence_score < 20 and evidence_assessment and evidence_assessment.supporting_evidence:
                    validation_issues.append("Low confidence score despite supporting evidence")
            
            # Generate final recommendation
            final_recommendation = self._generate_recommendation(dq_analysis, evidence_assessment, confidence_score, validation_issues)
            
            state["validation_result"] = ValidationResult(
                is_valid=len(validation_issues) == 0,
                validation_issues=validation_issues,
                final_recommendation=final_recommendation
            )
            
            state["current_agent"] = "completed"
            state["workflow_step"] = state.get("workflow_step", 0) + 1
            
        except Exception as e:
            logger.error(f"Validation error: {e}")
            state["errors"].append(f"Validation failed: {str(e)}")
            state["validation_result"] = ValidationResult(
                is_valid=False,
                validation_issues=[f"Validation error: {str(e)}"],
                final_recommendation="Unable to complete validation due to error"
            )
        
        return state
    
    def _generate_recommendation(self, dq_analysis, evidence_assessment, confidence_score, validation_issues):
        """Generate final recommendation based on all analysis."""
        if not dq_analysis:
            return "Unable to provide recommendation due to missing DQ analysis"
        
        if dq_analysis.is_data_related:
            confidence_level = "high" if confidence_score and confidence_score.confidence_score >= 80 else \
                             "medium" if confidence_score and confidence_score.confidence_score >= 60 else "low"
            
            recommendation = f"This incident appears to be data-related with {confidence_level} confidence"
            
            if dq_analysis.dq_dimensions:
                recommendation += f" affecting {', '.join(dq_analysis.dq_dimensions)} dimensions"
            
            if validation_issues:
                recommendation += f". However, validation identified concerns: {'; '.join(validation_issues)}"
        else:
            recommendation = "This incident does not appear to be primarily data-related"
            
            if evidence_assessment and evidence_assessment.contradicting_evidence:
                recommendation += f". Non-data factors include: {', '.join(evidence_assessment.contradicting_evidence[:3])}"
        
        return recommendation

# Supervisor Agent and Orchestration
# ==================================

class SupervisorAgent:
    """Main supervisor agent that orchestrates the multi-agent workflow."""
    
    def __init__(self, llm):
        self.llm = llm
        self.dq_analyzer = DataQualityAnalyzerAgent(llm)
        self.evidence_assessor = EvidenceAssessmentAgent(llm)
        self.confidence_scorer = ConfidenceScoringAgent(llm)
        self.validator = ValidationAgent(llm)
        
        # Build the workflow graph
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the multi-agent workflow graph."""
        
        def supervisor_node(state: MultiAgentState) -> Command[Literal["dq_analyzer", "evidence_assessor", "confidence_scorer", "validator", "__end__"]]:
            """Supervisor decision logic."""
            current_agent = state.get("current_agent", "start")
            
            if current_agent == "start":
                return Command(goto="dq_analyzer")
            elif current_agent == "evidence_assessor":
                return Command(goto="evidence_assessor")
            elif current_agent == "confidence_scorer":
                return Command(goto="confidence_scorer")
            elif current_agent == "validator":
                return Command(goto="validator")
            elif current_agent == "completed":
                return Command(goto="__end__")
            else:
                return Command(goto="__end__")
        
        def dq_analyzer_node(state: MultiAgentState) -> MultiAgentState:
            """DQ Analyzer node."""
            return self.dq_analyzer.analyze(state)
        
        def evidence_assessor_node(state: MultiAgentState) -> MultiAgentState:
            """Evidence Assessor node."""
            return self.evidence_assessor.assess(state)
        
        def confidence_scorer_node(state: MultiAgentState) -> MultiAgentState:
            """Confidence Scorer node."""
            return self.confidence_scorer.score(state)
        
        def validator_node(state: MultiAgentState) -> MultiAgentState:
            """Validator node."""
            return self.validator.validate(state)
        
        # Create the workflow graph
        workflow = StateGraph(MultiAgentState)
        
        # Add nodes
        workflow.add_node("supervisor", supervisor_node)
        workflow.add_node("dq_analyzer", dq_analyzer_node)
        workflow.add_node("evidence_assessor", evidence_assessor_node)
        workflow.add_node("confidence_scorer", confidence_scorer_node)
        workflow.add_node("validator", validator_node)
        
        # Add edges
        workflow.add_edge(START, "supervisor")
        workflow.add_conditional_edges("supervisor", lambda x: x["current_agent"])
        workflow.add_edge("dq_analyzer", "supervisor")
        workflow.add_edge("evidence_assessor", "supervisor")
        workflow.add_edge("confidence_scorer", "supervisor")
        workflow.add_edge("validator", "supervisor")
        
        return workflow
    
    def analyze_incident(self, incident_data: Dict[str, Any]) -> FinalAnalysisResult:
        """Analyze a single incident using the multi-agent workflow."""
        try:
            # Initialize state
            initial_state = MultiAgentState(
                messages=[],
                incident_data=incident_data,
                dq_analysis=None,
                evidence_assessment=None,
                confidence_score=None,
                validation_result=None,
                current_agent="start",
                workflow_step=0,
                errors=[]
            )
            
            # Add memory for stateful processing
            memory = MemorySaver()
            app = self.workflow.compile(checkpointer=memory)
            
            # Run the workflow
            config = {"configurable": {"thread_id": f"incident_{incident_data.get('INCIDENT_ID', 'unknown')}"}}
            final_state = app.invoke(initial_state, config)
            
            # Convert to final result
            return self._create_final_result(incident_data, final_state)
            
        except Exception as e:
            logger.error(f"Multi-agent workflow error: {e}")
            return self._create_error_result(incident_data, str(e))
    
    def _create_final_result(self, incident_data: Dict[str, Any], state: MultiAgentState) -> FinalAnalysisResult:
        """Create final analysis result from workflow state."""
        
        dq_analysis = state.get("dq_analysis")
        evidence_assessment = state.get("evidence_assessment")
        confidence_score = state.get("confidence_score")
        validation_result = state.get("validation_result")
        
        # Determine if it's a data issue
        is_data_issue = "Y" if dq_analysis and dq_analysis.is_data_related else "N"
        
        # Create comprehensive final result
        result = FinalAnalysisResult(
            INCIDENT_ID=incident_data.get("INCIDENT_ID", ""),
            IT_INCIDENT_DESC=incident_data.get("IT_INCIDENT_DESC", ""),
            IT_INCIDENT_AREA_CATEGORY_NAME=incident_data.get("IT_INCIDENT_AREA_CATEGORY_NAME", ""),
            IT_INCIDENT_AREA_SUBCATEGORY_NAME=incident_data.get("IT_INCIDENT_AREA_SUBCATEGORY_NAME", ""),
            IT_INCIDENT_CAUSE_TEXT=incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
            MAJOR_IT_INCIDENT_SUMMARY_TEXT=incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
            MAJOR_IT_INCIDENT_FINDINGS_TEXT=incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
            IT_INCIDENT_RESOLUTION_DESC=incident_data.get("IT_INCIDENT_RESOLUTION_DESC", ""),
            
            IS_DATA_ISSUE=is_data_issue,
            DQ_DIMENSIONS=", ".join(dq_analysis.dq_dimensions) if dq_analysis else "",
            PRIMARY_INDICATORS=", ".join(dq_analysis.primary_indicators) if dq_analysis else "",
            SUPPORTING_EVIDENCE=" | ".join(evidence_assessment.supporting_evidence) if evidence_assessment else "",
            CONTRADICTING_EVIDENCE=" | ".join(evidence_assessment.contradicting_evidence) if evidence_assessment else "",
            EVIDENCE_STRENGTH_SCORES=json.dumps(evidence_assessment.evidence_strength) if evidence_assessment else "",
            CONFIDENCE_SCORE=confidence_score.confidence_score if confidence_score else 0.0,
            CONFIDENCE_CALCULATION=confidence_score.calculation_method if confidence_score else "",
            VALIDATION_STATUS="VALID" if validation_result and validation_result.is_valid else "ISSUES_FOUND",
            FINAL_REASONING=validation_result.final_recommendation if validation_result else "Analysis incomplete",
            PROCESSING_STATUS="SUCCESS" if not state.get("errors") else "PARTIAL_SUCCESS",
            WORKFLOW_STEPS=state.get("workflow_step", 0)
        )
        
        # Add extra fields from original data
        for key, value in incident_data.items():
            if not hasattr(result, key) and key not in result.model_fields:
                setattr(result, key, value)
        
        return result
    
    def _create_error_result(self, incident_data: Dict[str, Any], error_msg: str) -> FinalAnalysisResult:
        """Create error result when workflow fails."""
        return FinalAnalysisResult(
            INCIDENT_ID=incident_data.get("INCIDENT_ID", ""),
            IT_INCIDENT_DESC=incident_data.get("IT_INCIDENT_DESC", ""),
            IT_INCIDENT_AREA_CATEGORY_NAME=incident_data.get("IT_INCIDENT_AREA_CATEGORY_NAME", ""),
            IT_INCIDENT_AREA_SUBCATEGORY_NAME=incident_data.get("IT_INCIDENT_AREA_SUBCATEGORY_NAME", ""),
            IT_INCIDENT_CAUSE_TEXT=incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
            MAJOR_IT_INCIDENT_SUMMARY_TEXT=incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
            MAJOR_IT_INCIDENT_FINDINGS_TEXT=incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
            IT_INCIDENT_RESOLUTION_DESC=incident_data.get("IT_INCIDENT_RESOLUTION_DESC", ""),
            
            IS_DATA_ISSUE="N",
            DQ_DIMENSIONS="",
            PRIMARY_INDICATORS="",
            SUPPORTING_EVIDENCE="",
            CONTRADICTING_EVIDENCE="",
            EVIDENCE_STRENGTH_SCORES="",
            CONFIDENCE_SCORE=0.0,
            CONFIDENCE_CALCULATION="",
            VALIDATION_STATUS="ERROR",
            FINAL_REASONING=f"Multi-agent analysis failed: {error_msg}",
            PROCESSING_STATUS="ERROR",
            WORKFLOW_STEPS=0
        )

# Enhanced Analysis Pipeline
# ==========================

class EnhancedIncidentAnalyzer:
    """Enhanced incident analyzer using multi-agent architecture."""
    
    def __init__(self):
        self.supervisor = SupervisorAgent(llm)
        self.total_incidents = 0
        self.successful_analyses = 0
        self.failed_analyses = 0
        
    def process_csv(self, input_path: str) -> List[FinalAnalysisResult]:
        """Process CSV file with enhanced multi-agent analysis."""
        try:
            # Read and validate CSV
            df = self._read_and_validate_csv(input_path)
            
            results = []
            self.total_incidents = len(df)
            
            for index, row in df.iterrows():
                try:
                    incident_id = row.get('INCIDENT_ID', f'ROW_{index}')
                    logger.info(f"Processing incident {index + 1}/{len(df)}: {incident_id}")
                    
                    # Convert row to dictionary
                    incident_data = row.fillna("").to_dict()
                    
                    # Analyze with multi-agent system
                    result = self.supervisor.analyze_incident(incident_data)
                    results.append(result)
                    
                    if result.PROCESSING_STATUS == "SUCCESS":
                        self.successful_analyses += 1
                    else:
                        self.failed_analyses += 1
                    
                    # Rate limiting
                    time.sleep(API_RATE_LIMIT_DELAY)
                    
                except Exception as e:
                    logger.error(f"Error processing row {index}: {e}")
                    self.failed_analyses += 1
                    
                    # Create error result
                    error_result = self.supervisor._create_error_result(
                        row.fillna("").to_dict(),
                        f"Row processing error: {str(e)}"
                    )
                    results.append(error_result)
            
            logger.info(f"Analysis complete: {self.successful_analyses} successful, {self.failed_analyses} failed")
            return results
            
        except Exception as e:
            logger.error(f"CSV processing error: {e}")
            raise
    
    def _read_and_validate_csv(self, input_path: str) -> pd.DataFrame:
        """Read and validate CSV file."""
        try:
            df = pd.read_csv(input_path, encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(input_path, encoding='latin-1')
            logger.warning(f"Used latin-1 encoding for {input_path}")
        
        if df.empty:
            raise ValueError("CSV file is empty")
        
        logger.info(f"Loaded {len(df)} incidents from {input_path}")
        return df
    
    def save_results(self, results: List[FinalAnalysisResult], json_path: str, csv_path: str):
        """Save enhanced results with comprehensive analytics."""
        try:
            # Convert to dictionaries for serialization
            results_dict = [result.model_dump() for result in results]
            
            # Save to JSON
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            logger.info(f"Results saved to JSON: {json_path}")
            
            # Save to CSV
            df = pd.DataFrame(results_dict)
            df.to_csv(csv_path, index=False, encoding='utf-8')
            logger.info(f"Results saved to CSV: {csv_path}")
            
            # Generate analytics
            self._generate_comprehensive_analytics(results)
            
        except Exception as e:
            logger.error(f"Error saving results: {e}")
            raise
    
    def _generate_comprehensive_analytics(self, results: List[FinalAnalysisResult]):
        """Generate comprehensive analytics from multi-agent results."""
        total = len(results)
        successful = sum(1 for r in results if r.PROCESSING_STATUS == "SUCCESS")
        data_issues = sum(1 for r in results if r.IS_DATA_ISSUE == "Y")
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ENHANCED MULTI-AGENT ANALYSIS SUMMARY")
        logger.info(f"{'='*80}")
        logger.info(f"Total incidents processed: {total}")
        logger.info(f"Successfully analyzed: {successful}")
        logger.info(f"Processing errors: {total - successful}")
        logger.info(f"Success rate: {(successful/total*100):.1f}%")
        
        if successful > 0:
            logger.info(f"\nDATA QUALITY INSIGHTS:")
            logger.info(f"Data-related incidents: {data_issues}")
            logger.info(f"Data issue rate: {(data_issues/successful*100):.1f}%")
            
            # Confidence distribution
            confidence_scores = [r.CONFIDENCE_SCORE for r in results if r.CONFIDENCE_SCORE > 0]
            if confidence_scores:
                avg_confidence = sum(confidence_scores) / len(confidence_scores)
                logger.info(f"Average confidence score: {avg_confidence:.1f}%")
                
                high_conf = sum(1 for s in confidence_scores if s >= 80)
                med_conf = sum(1 for s in confidence_scores if 60 <= s < 80)
                low_conf = sum(1 for s in confidence_scores if s < 60)
                
                logger.info(f"High confidence (≥80%): {high_conf}")
                logger.info(f"Medium confidence (60-79%): {med_conf}")
                logger.info(f"Low confidence (<60%): {low_conf}")
            
            # DQ dimensions analysis
            all_dimensions = []
            for result in results:
                if result.DQ_DIMENSIONS:
                    dims = [d.strip() for d in result.DQ_DIMENSIONS.split(",") if d.strip()]
                    all_dimensions.extend(dims)
            
            if all_dimensions:
                from collections import Counter
                dim_counts = Counter(all_dimensions)
                logger.info(f"\nTOP DQ DIMENSIONS:")
                for dim, count in dim_counts.most_common(5):
                    percentage = (count / data_issues * 100) if data_issues > 0 else 0
                    logger.info(f"{dim}: {count} incidents ({percentage:.1f}%)")
            
            # Workflow efficiency
            avg_steps = sum(r.WORKFLOW_STEPS for r in results) / len(results)
            logger.info(f"\nWORKFLOW EFFICIENCY:")
            logger.info(f"Average workflow steps: {avg_steps:.1f}")
            
            validation_success = sum(1 for r in results if r.VALIDATION_STATUS == "VALID")
            logger.info(f"Validation success rate: {(validation_success/total*100):.1f}%")
        
        logger.info(f"{'='*80}")

# Main Functions
# ==============

def validate_configuration():
    """Validate configuration and dependencies."""
    errors = []
    
    if API_KEY == "your-openai-api-key-here" or not API_KEY:
        errors.append("OpenAI API key not configured")
    
    if not os.path.exists(INPUT_CSV_PATH):
        errors.append(f"Input CSV file not found: {INPUT_CSV_PATH}")
    
    # Test LangGraph dependencies
    try:
        from langgraph.graph import StateGraph
        from langgraph.checkpoint.memory import MemorySaver
    except ImportError as e:
        errors.append(f"LangGraph dependency missing: {e}")
    
    if errors:
        for error in errors:
            logger.error(f"Configuration error: {error}")
        return False
    
    logger.info("Configuration validation passed")
    return True

@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=1, max=5)
)
def test_llm_connection():
    """Test LLM connection."""
    try:
        logger.info("Testing LLM connection...")
        response = llm.invoke([HumanMessage(content="Test connection. Respond with 'OK'.")])
        
        if response.content.strip():
            logger.info("LLM connection test successful")
            return True
        else:
            logger.error("LLM connection test failed: Empty response")
            return False
    except Exception as e:
        logger.error(f"LLM connection test failed: {e}")
        return False

def main():
    """Main execution function for enhanced multi-agent analysis."""
    start_time = time.time()
    
    try:
        logger.info("Starting Enhanced Multi-Agent IT Incident Analysis")
        logger.info(f"Model: {MODEL}")
        logger.info(f"Input: {INPUT_CSV_PATH}")
        logger.info(f"Outputs: {OUTPUT_JSON_PATH}, {OUTPUT_CSV_PATH}")
        
        # Validation
        if not validate_configuration():
            sys.exit(1)
        
        if not test_llm_connection():
            logger.error("LLM connection failed")
            sys.exit(1)
        
        # Initialize enhanced analyzer
        analyzer = EnhancedIncidentAnalyzer()
        
        # Process incidents with multi-agent system
        logger.info("Starting multi-agent incident analysis...")
        results = analyzer.process_csv(INPUT_CSV_PATH)
        
        # Save enhanced results
        analyzer.save_results(results, OUTPUT_JSON_PATH, OUTPUT_CSV_PATH)
        
        # Completion summary
        end_time = time.time()
        execution_time = end_time - start_time
        
        logger.info(f"\n🎉 Enhanced analysis completed successfully in {execution_time:.1f} seconds!")
        logger.info(f"📊 Multi-agent workflow processed {len(results)} incidents")
        logger.info(f"📁 Results saved to:")
        logger.info(f"   JSON: {OUTPUT_JSON_PATH}")
        logger.info(f"   CSV: {OUTPUT_CSV_PATH}")
        
    except KeyboardInterrupt:
        logger.info("Analysis interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Enhanced analysis failed: {e}")
        logger.error("Stack trace:", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
