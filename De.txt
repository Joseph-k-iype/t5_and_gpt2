"""
Document Chunker Utility
Intelligently chunks documents while respecting semantic boundaries
Location: src/utils/document_chunker.py
"""
import re
from typing import List, Dict, Any
from dataclasses import dataclass


@dataclass
class DocumentChunk:
    """Represents a chunk of document text"""
    content: str
    chunk_index: int
    start_position: int
    end_position: int
    
    # For compatibility with dict access
    def get(self, key: str, default=None):
        """Dict-like get method"""
        if key == "content" or key == "text":
            return self.content
        elif key == "chunk_id" or key == "index":
            return self.chunk_index
        elif key == "start":
            return self.start_position
        elif key == "end":
            return self.end_position
        return default
    
    @property
    def text(self):
        """Alias for content"""
        return self.content


class DocumentChunker:
    """
    Intelligent document chunker that respects semantic boundaries
    """
    
    def __init__(
        self,
        chunk_size: int = 3000,
        chunk_overlap: int = 200,
        respect_boundaries: bool = True
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_boundaries = respect_boundaries
        
        # Regex patterns for semantic boundaries
        self.section_pattern = re.compile(r'\n\s*(?:Section|Article|Chapter|ยง)\s+\d+', re.IGNORECASE)
        self.paragraph_pattern = re.compile(r'\n\s*\(\d+\)|^\d+\.\s+', re.MULTILINE)
        self.sentence_pattern = re.compile(r'(?<=[.!?])\s+(?=[A-Z])')
    
    def chunk_text(self, text: str) -> List[DocumentChunk]:
        """
        Chunk text into segments while respecting semantic boundaries
        
        Args:
            text: Text to chunk
            
        Returns:
            List of DocumentChunk objects
        """
        if not text or len(text.strip()) == 0:
            return []
        
        # If text is smaller than chunk_size, return as single chunk
        if len(text) <= self.chunk_size:
            return [DocumentChunk(
                content=text,
                chunk_index=0,
                start_position=0,
                end_position=len(text)
            )]
        
        chunks = []
        current_position = 0
        chunk_index = 0
        
        while current_position < len(text):
            # Determine end position for this chunk
            end_position = min(current_position + self.chunk_size, len(text))
            
            # If respecting boundaries and not at end of text, find good break point
            if self.respect_boundaries and end_position < len(text):
                end_position = self._find_break_point(text, current_position, end_position)
            
            # Extract chunk text
            chunk_text = text[current_position:end_position].strip()
            
            if chunk_text:
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    chunk_index=chunk_index,
                    start_position=current_position,
                    end_position=end_position
                ))
                chunk_index += 1
            
            # Move to next position with overlap
            current_position = end_position - self.chunk_overlap
            
            # Ensure we make progress
            if current_position <= chunks[-1].start_position if chunks else 0:
                current_position = end_position
        
        return chunks
    
    def _find_break_point(self, text: str, start: int, end: int) -> int:
        """
        Find a good break point that respects semantic boundaries
        
        Priority:
        1. Section/Article boundaries
        2. Paragraph boundaries
        3. Sentence boundaries
        4. Whitespace
        """
        # Define search window (last 20% of chunk)
        search_start = end - int(self.chunk_size * 0.2)
        search_window = text[search_start:end]
        
        # Try to find section boundary
        section_matches = list(self.section_pattern.finditer(search_window))
        if section_matches:
            # Use last section match in window
            match = section_matches[-1]
            return search_start + match.start()
        
        # Try to find paragraph boundary
        paragraph_matches = list(self.paragraph_pattern.finditer(search_window))
        if paragraph_matches:
            match = paragraph_matches[-1]
            return search_start + match.start()
        
        # Try to find sentence boundary
        sentence_matches = list(self.sentence_pattern.finditer(search_window))
        if sentence_matches:
            match = sentence_matches[-1]
            return search_start + match.end()
        
        # Fall back to last whitespace
        last_space = search_window.rfind(' ')
        if last_space != -1:
            return search_start + last_space
        
        # No good break point found, use original end
        return end
    
    def get_chunk_metadata(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """Get metadata about the chunking"""
        if not chunks:
            return {
                "total_chunks": 0,
                "total_length": 0,
                "avg_chunk_size": 0,
                "min_chunk_size": 0,
                "max_chunk_size": 0
            }
        
        chunk_sizes = [len(chunk.content) for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_length": sum(chunk_sizes),
            "avg_chunk_size": sum(chunk_sizes) / len(chunks),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes),
            "chunk_overlap": self.chunk_overlap,
            "target_chunk_size": self.chunk_size
        }
