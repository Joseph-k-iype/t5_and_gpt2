"""
Enhanced Deep Research Agent with LangMem Long-Term Memory, Multi-Agent Architecture, and Advanced Token Management
Based on LangChain Open Deep Research patterns with specialized ReAct agents and comprehensive token optimization

OPTIMIZED FOR O3-MINI ONLY with Local Tiktoken Support
==================================================================

IMPORTANT: This system is specifically optimized for the o3-mini-2025-01-31 model and includes
support for local tiktoken models to work around proxy/internet restrictions.

SETUP REQUIREMENTS:
==================

1. ENVIRONMENT VARIABLES (Required):
   export OPENAI_API_KEY="your-openai-api-key"
   export ES_PASSWORD="your-elasticsearch-password"
   export ES_HOST="your-elasticsearch-host"        # Optional, defaults to localhost
   export ES_USERNAME="elastic"                    # Optional, defaults to elastic
   export ES_PORT="9200"                          # Optional, defaults to 9200
   export TIKTOKEN_MODEL_PATH="tiktoken_model"    # Optional, defaults to tiktoken_model

2. LOCAL TIKTOKEN MODELS (For Proxy Issues):
   Download these files to your tiktoken_model directory:
   - cl100k_base.tiktoken
   - p50k_base.tiktoken
   - r50k_base.tiktoken
   
   You can download these from:
   https://github.com/openai/tiktoken/tree/main/tiktoken_ext
   
   Or if you have access, run:
   python -c "import tiktoken; tiktoken.get_encoding('cl100k_base')"
   and copy the cached files from your tiktoken cache directory.

3. PYTHON DEPENDENCIES:
   pip install openai elasticsearch tiktoken pydantic
   pip install langchain langchain-elasticsearch langchain-openai langgraph langmem

4. O3-MINI SPECIFIC OPTIMIZATIONS:
   - Reduced token limits for efficiency
   - Optimized research iterations (3 instead of 5)
   - Conservative token reservations
   - Simplified parameter handling
   - Enhanced fallback mechanisms

Features:
- LangMem SDK for cross-session long-term memory
- Multi-agent architecture with specialized research agents
- Enhanced iterative research with knowledge synthesis
- Plan-and-execute workflow with reflection loops
- Semantic and episodic memory management
- Advanced context engineering for multi-agent coordination
- Comprehensive research orchestration with LangGraph
- LLM-based domain filtering (no hardcoded keywords)
- Content prioritization and intelligent truncation
- Progressive quality degradation for large results
- LOCAL TIKTOKEN SUPPORT for proxy-restricted environments
- Uses ONLY o3-mini-2025-01-31 for all operations
"""

import asyncio
import json
import logging
import os
import ssl
import uuid
import time
import sys
import tiktoken
from datetime import datetime
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
import re
from collections import defaultdict, Counter

# Core imports
import openai
from elasticsearch import Elasticsearch

# LangChain Elasticsearch integration
try:
    from langchain_elasticsearch import ElasticsearchStore
    from langchain_elasticsearch.vectorstores import DenseVectorStrategy
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = True
except ImportError:
    print("Warning: langchain-elasticsearch not available. Install with: pip install langchain-elasticsearch")
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = False

# LangChain core
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_core.retrievers import BaseRetriever
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate

# LangChain OpenAI
try:
    from langchain_openai import ChatOpenAI
    LANGCHAIN_OPENAI_AVAILABLE = True
except ImportError:
    print("Warning: langchain-openai not available. Install with: pip install langchain-openai")
    LANGCHAIN_OPENAI_AVAILABLE = False

# LangGraph
try:
    from langgraph.graph import StateGraph, MessagesState, START, END, CompiledGraph
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.store.memory import InMemoryStore
    from langgraph.prebuilt import ToolNode, create_react_agent
    from langgraph.types import Command
    LANGGRAPH_AVAILABLE = True
except ImportError:
    print("Warning: langgraph not available. Install with: pip install langgraph")
    LANGGRAPH_AVAILABLE = False

# LangMem for long-term memory
try:
    from langmem import create_memory_manager, create_manage_memory_tool, create_search_memory_tool
    LANGMEM_AVAILABLE = True
except ImportError:
    print("Warning: LangMem not available. Install with: pip install langmem")
    LANGMEM_AVAILABLE = False

# Pydantic
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
class Config:
    """Enhanced configuration for deep research agent with token management - O3-MINI ONLY"""
    
    def __init__(self):
        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        self.OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
        self.ES_PASSWORD = os.getenv("ES_PASSWORD")
        self.ES_HOST = os.getenv("ES_HOST", "localhost")
        
        # Validate required environment variables
        if not self.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        if not self.ES_PASSWORD:
            raise ValueError("ES_PASSWORD environment variable is required")
        
        try:
            self.ES_PORT = int(os.getenv("ES_PORT", "9200"))
        except ValueError:
            self.ES_PORT = 9200
        
        self.ES_CACERT_PATH = os.getenv("ES_CACERT_PATH", "cacert.crt")
        
        # ONLY O3-MINI MODEL CONFIGURATION
        self.MODEL = "o3-mini-2025-01-31"
        self.REASONING_EFFORT = "high"
        self.EMBEDDING_MODEL = "text-embedding-3-large"
        self.EMBEDDING_DIMENSIONS = 3072
        
        # Local tiktoken model path
        self.TIKTOKEN_MODEL_PATH = os.getenv("TIKTOKEN_MODEL_PATH", "tiktoken_model")
        
        # Research configurations optimized for o3-mini
        self.MAX_RESEARCH_ITERATIONS = 3  # Reduced for o3-mini efficiency
        self.MAX_SEARCH_RESULTS_PER_ITERATION = 15  # Optimized for o3-mini
        self.CONFIDENCE_THRESHOLD = 0.8  # Slightly lower for faster completion
        self.MAX_PARALLEL_RESEARCHERS = 2  # Reduced for o3-mini
        
        # Memory configurations
        self.MEMORY_NAMESPACE_PREFIX = "privacy_research"
        self.SEMANTIC_MEMORY_THRESHOLD = 0.8
        self.EPISODIC_MEMORY_RETENTION_DAYS = 30
        
        # Token management configuration optimized for o3-mini
        self.MAX_RESPONSE_TOKENS = 12000  # Reduced for o3-mini efficiency
        self.MAX_QUERY_TOKENS = 3000      # Reduced for o3-mini
        self.MAX_MEMORY_TOKENS = 4000     # Reduced for o3-mini
        self.SYNTHESIS_CHUNK_SIZE = 3000  # Optimized for o3-mini
        self.CONTENT_OVERLAP_SIZE = 150   # Reduced overlap
        self.TOKEN_RESERVE = 1500         # Conservative reserve for o3-mini
        
        # Quality preservation settings for o3-mini
        self.MIN_FINDINGS_TO_KEEP = 3
        self.PRIORITY_THRESHOLD = 0.7
        self.ENABLE_PROGRESSIVE_SYNTHESIS = True
        self.ENABLE_CONTENT_PRIORITIZATION = True

# Global config instance
try:
    config = Config()
except Exception as e:
    print(f"Configuration error: {e}")
    sys.exit(1)

# Utility Functions
def safe_json_parse(json_string: str, fallback_value: Any = None) -> Any:
    """Safely parse JSON with fallback handling"""
    if not json_string or not isinstance(json_string, str):
        logger.warning(f"Invalid JSON input: {type(json_string)}")
        return fallback_value
    
    json_string = json_string.strip()
    
    # Try to extract JSON from markdown code blocks
    if "```json" in json_string:
        try:
            start_idx = json_string.find("```json") + 7
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    elif "```" in json_string:
        try:
            start_idx = json_string.find("```") + 3
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    
    # Multiple parsing attempts
    parse_attempts = [
        lambda: json.loads(json_string),
        lambda: json.loads(json_string.replace("'", '"')),
        lambda: json.loads(re.sub(r'(\w+):', r'"\1":', json_string)),
    ]
    
    for attempt in parse_attempts:
        try:
            result = attempt()
            if result is not None:
                return result
        except Exception:
            continue
    
    logger.error(f"Failed to parse JSON: {json_string[:200]}...")
    return fallback_value

def get_event_loop() -> asyncio.AbstractEventLoop:
    """Get or create event loop safely"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            raise RuntimeError("Event loop is closed")
        return loop
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop

# Token Management Classes
@dataclass
class ContentChunk:
    """Represents a chunk of content with metadata"""
    content: str
    tokens: int
    priority: float
    chunk_type: str
    source_id: str
    timestamp: str
    confidence: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "tokens": self.tokens,
            "priority": self.priority,
            "chunk_type": self.chunk_type,
            "source_id": self.source_id,
            "timestamp": self.timestamp,
            "confidence": self.confidence
        }

class TokenManager:
    """Advanced token management with content optimization using local tiktoken models"""
    
    def __init__(self, model_name: str = "o3-mini", max_tokens: int = None):
        self.model_name = model_name
        self.max_tokens = max_tokens or config.MAX_RESPONSE_TOKENS
        self.reserve_tokens = config.TOKEN_RESERVE
        self.effective_limit = self.max_tokens - self.reserve_tokens
        
        # Load tiktoken encoder from local path
        self.encoder = self._load_local_tiktoken_encoder()
    
    def _load_local_tiktoken_encoder(self):
        """Load tiktoken encoder from local path to avoid proxy issues"""
        try:
            # First try to load from configured local path
            local_model_path = config.TIKTOKEN_MODEL_PATH
            logger.info(f"Attempting to load tiktoken model from: {local_model_path}")
            
            if os.path.exists(local_model_path):
                logger.info(f"✓ Found tiktoken model directory: {local_model_path}")
                
                # Try to load encoding files from local directory
                try:
                    # Look for common tiktoken encoding files
                    encoding_files = [
                        "cl100k_base.tiktoken",
                        "p50k_base.tiktoken", 
                        "r50k_base.tiktoken"
                    ]
                    
                    for encoding_file in encoding_files:
                        file_path = os.path.join(local_model_path, encoding_file)
                        if os.path.exists(file_path):
                            logger.info(f"✓ Found encoding file: {file_path}")
                            
                            try:
                                # Load the tiktoken file
                                with open(file_path, 'rb') as f:
                                    encoding_data = f.read()
                                
                                # Parse encoding name from filename
                                encoding_name = encoding_file.replace('.tiktoken', '')
                                
                                # Create encoding using tiktoken's load function
                                import tiktoken.load
                                mergeable_ranks = tiktoken.load.load_tiktoken_bpe(encoding_data)
                                
                                # Get pattern and special tokens for this encoding
                                pat_str = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
                                special_tokens = {}
                                
                                if encoding_name == "cl100k_base":
                                    special_tokens = {
                                        "<|endoftext|>": 100257,
                                        "<|fim_prefix|>": 100258,
                                        "<|fim_middle|>": 100259,
                                        "<|fim_suffix|>": 100260,
                                        "<|endofprompt|>": 100276
                                    }
                                
                                # Create the encoding
                                encoding = tiktoken.core.Encoding(
                                    name=encoding_name,
                                    pat_str=pat_str,
                                    mergeable_ranks=mergeable_ranks,
                                    special_tokens=special_tokens
                                )
                                
                                logger.info(f"✓ Successfully loaded tiktoken encoding from local file: {encoding_name}")
                                return encoding
                                
                            except Exception as e:
                                logger.warning(f"Failed to load encoding from {file_path}: {e}")
                                continue
                    
                except Exception as e:
                    logger.warning(f"Error processing local tiktoken files: {e}")
            else:
                logger.warning(f"Local tiktoken path not found: {local_model_path}")
            
            # Fallback: try standard tiktoken loading with error handling
            logger.info("Attempting standard tiktoken loading as fallback...")
            
            # Try different encoding options that might work
            encoding_options = ["cl100k_base", "p50k_base", "r50k_base"]
            
            for encoding_name in encoding_options:
                try:
                    logger.info(f"Trying to load {encoding_name}...")
                    encoder = tiktoken.get_encoding(encoding_name)
                    logger.info(f"✓ Successfully loaded tiktoken encoding: {encoding_name}")
                    return encoder
                except Exception as e:
                    logger.warning(f"Failed to load {encoding_name}: {e}")
                    continue
            
            # If all else fails, create a simple approximation
            logger.warning("All tiktoken loading methods failed, using character-based approximation")
            return self._create_fallback_tokenizer()
            
        except Exception as e:
            logger.error(f"Error loading tiktoken encoder: {e}")
            return self._create_fallback_tokenizer()
    
    def _create_fallback_tokenizer(self):
        """Create a simple fallback tokenizer when tiktoken is unavailable"""
        logger.info("✓ Using fallback character-based tokenizer for o3-mini compatibility")
        
        class FallbackTokenizer:
            def __init__(self):
                self.name = "fallback_tokenizer"
            
            def encode(self, text):
                """Simple character-based encoding optimized for o3-mini"""
                if not text:
                    return []
                
                # More accurate approximation for token counting
                # Based on empirical observations of token/character ratios
                
                # Count different types of characters
                chars = len(text)
                words = len(text.split())
                
                # Approximate tokens based on text characteristics
                # English text: ~4 chars per token
                # Code: ~3 chars per token  
                # Mixed content: ~3.5 chars per token
                
                # Check if text looks like code (contains common programming symbols)
                code_indicators = sum([
                    text.count('{'), text.count('}'), text.count('['), text.count(']'),
                    text.count('('), text.count(')'), text.count(';'), text.count('=')
                ])
                
                if code_indicators > chars * 0.05:  # More than 5% programming symbols
                    # Looks like code - approximately 3 chars per token
                    estimated_tokens = max(1, chars // 3)
                else:
                    # Regular text - approximately 4 chars per token
                    estimated_tokens = max(1, chars // 4)
                
                # Add slight adjustment for very short text
                if words <= 3:
                    estimated_tokens = words
                
                # Return a list representing tokens (just indices)
                return list(range(estimated_tokens))
            
            def decode(self, tokens):
                """Simple decoding for compatibility"""
                return f"[{len(tokens)} tokens]"
        
        return FallbackTokenizer()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        if not text:
            return 0
        try:
            return len(self.encoder.encode(str(text)))
        except Exception as e:
            logger.warning(f"Token counting error: {e}")
            return len(str(text)) // 4  # Rough estimate
    
    def truncate_to_limit(self, text: str, limit: int) -> str:
        """Truncate text to token limit with intelligent boundaries"""
        if not text:
            return ""
        
        tokens = self.count_tokens(text)
        if tokens <= limit:
            return text
        
        # Try to truncate at natural boundaries
        # 1. Paragraphs
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            result = ""
            for paragraph in paragraphs:
                test_result = result + ('\n\n' if result else '') + paragraph
                if self.count_tokens(test_result) <= limit:
                    result = test_result
                else:
                    break
            if result and len(result) > 100:
                return result
        
        # 2. Sentences
        sentences = text.split('. ')
        result = ""
        for sentence in sentences:
            test_result = result + ('. ' if result else '') + sentence
            if self.count_tokens(test_result) <= limit:
                result = test_result
            else:
                break
        
        if result and len(result) > 100:
            return result
        
        # 3. Binary search for optimal word truncation
        words = text.split()
        left, right = 0, len(words)
        
        while left < right:
            mid = (left + right + 1) // 2
            truncated = " ".join(words[:mid])
            if self.count_tokens(truncated) <= limit:
                left = mid
            else:
                right = mid - 1
        
        return " ".join(words[:left])
    
    def chunk_content(self, content: str, chunk_size: int = None, 
                     overlap: int = None) -> List[str]:
        """Split content into overlapping chunks"""
        if not content:
            return []
        
        chunk_size = chunk_size or config.SYNTHESIS_CHUNK_SIZE
        overlap = overlap or config.CONTENT_OVERLAP_SIZE
        
        tokens = self.count_tokens(content)
        if tokens <= chunk_size:
            return [content]
        
        chunks = []
        words = content.split()
        current_chunk = []
        current_tokens = 0
        
        for word in words:
            word_tokens = self.count_tokens(word)
            
            if current_tokens + word_tokens > chunk_size and current_chunk:
                # Create chunk with overlap
                chunk_text = " ".join(current_chunk)
                chunks.append(chunk_text)
                
                # Keep overlap words for next chunk
                overlap_words = int(len(current_chunk) * (overlap / chunk_size))
                current_chunk = current_chunk[-overlap_words:] if overlap_words > 0 else []
                current_tokens = sum(self.count_tokens(w) for w in current_chunk)
            
            current_chunk.append(word)
            current_tokens += word_tokens
        
        # Add final chunk
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks

class ContentPrioritizer:
    """Prioritize content based on relevance and quality"""
    
    def __init__(self, openai_manager):
        self.openai_manager = openai_manager
    
    async def prioritize_findings(self, findings: List[Dict], query: str) -> List[Dict]:
        """Prioritize findings by relevance and quality"""
        try:
            prioritized = []
            
            for finding in findings:
                priority_score = await self._calculate_priority(finding, query)
                finding_copy = finding.copy()
                finding_copy['priority_score'] = priority_score
                prioritized.append(finding_copy)
            
            # Sort by priority score (descending)
            prioritized.sort(key=lambda x: x.get('priority_score', 0), reverse=True)
            return prioritized
            
        except Exception as e:
            logger.error(f"Error prioritizing findings: {e}")
            return findings
    
    async def _calculate_priority(self, finding: Dict, query: str) -> float:
        """Calculate priority score for a finding"""
        try:
            # Base score from confidence
            base_score = finding.get('confidence', 0.5)
            
            # Relevance bonus (keyword matching)
            content = finding.get('content', '') or finding.get('insight', '')
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            relevance_bonus = len(query_words.intersection(content_words)) / len(query_words) if query_words else 0
            
            # Length penalty for very long findings
            content_length = len(content)
            length_penalty = 0.1 if content_length > 2000 else 0
            
            # Jurisdiction bonus
            jurisdiction_bonus = 0.1 if finding.get('jurisdiction') else 0
            
            # Regulation specificity bonus
            regulation_bonus = 0.1 if finding.get('regulation') else 0
            
            # Final score
            priority_score = (base_score + (relevance_bonus * 0.3) + 
                            jurisdiction_bonus + regulation_bonus - length_penalty)
            
            return min(max(priority_score, 0), 1.0)
            
        except Exception as e:
            logger.error(f"Error calculating priority: {e}")
            return 0.5

class ProgressiveSynthesizer:
    """Creates progressive summaries to manage token limits"""
    
    def __init__(self, openai_manager, token_manager: TokenManager):
        self.openai_manager = openai_manager
        self.token_manager = token_manager
        self.max_synthesis_tokens = 15000  # Maximum tokens for synthesis input
    
    async def create_progressive_synthesis(self, research_state: 'MultiAgentResearchState') -> str:
        """Create synthesis in progressive stages to manage token limits"""
        try:
            logger.info("Creating progressive synthesis with token management")
            
            # Stage 1: Extract and prioritize key findings
            all_findings = self._extract_all_findings(research_state)
            prioritized_findings = await self._prioritize_findings(all_findings, research_state.original_query)
            
            # Stage 2: Create hierarchical summary
            hierarchical_summary = await self._create_hierarchical_summary(prioritized_findings)
            
            # Stage 3: Progressive synthesis
            final_synthesis = await self._progressive_synthesis(
                hierarchical_summary, research_state.original_query, research_state
            )
            
            # Stage 4: Final token optimization
            optimized_synthesis = await self._optimize_final_synthesis(final_synthesis)
            
            return optimized_synthesis
            
        except Exception as e:
            logger.error(f"Error in progressive synthesis: {e}")
            return f"Synthesis error: {str(e)}"
    
    def _extract_all_findings(self, research_state: 'MultiAgentResearchState') -> List[Dict]:
        """Extract all findings from research state"""
        all_findings = []
        
        for agent_id, agent_state in research_state.agent_states.items():
            for finding_result in agent_state.findings:
                if finding_result.get('findings'):
                    for finding in finding_result['findings']:
                        finding_copy = finding.copy()
                        finding_copy['source_agent'] = agent_id
                        finding_copy['agent_confidence'] = finding_result.get('confidence', 0.5)
                        all_findings.append(finding_copy)
        
        return all_findings
    
    async def _prioritize_findings(self, findings: List[Dict], query: str) -> List[Dict]:
        """Prioritize findings by relevance and quality"""
        try:
            prioritizer = ContentPrioritizer(self.openai_manager)
            return await prioritizer.prioritize_findings(findings, query)
        except Exception as e:
            logger.error(f"Error prioritizing findings: {e}")
            return findings
    
    async def _create_hierarchical_summary(self, findings: List[Dict]) -> Dict[str, Any]:
        """Create hierarchical summary of findings"""
        try:
            # Group findings by type and jurisdiction
            grouped_findings = defaultdict(list)
            
            for finding in findings:
                jurisdiction = finding.get('jurisdiction', 'General')
                agent_type = finding.get('source_agent', 'unknown')
                key = f"{jurisdiction}_{agent_type}"
                grouped_findings[key].append(finding)
            
            # Create summaries for each group
            group_summaries = {}
            
            for group_key, group_findings in grouped_findings.items():
                if len(group_findings) <= 3:
                    # Small group, keep detailed
                    group_summaries[group_key] = group_findings
                else:
                    # Large group, summarize
                    summary = await self._summarize_finding_group(group_findings)
                    group_summaries[group_key] = summary
            
            return {
                'grouped_summaries': group_summaries,
                'total_findings': len(findings),
                'top_findings': findings[:10]  # Keep top 10 detailed findings
            }
            
        except Exception as e:
            logger.error(f"Error creating hierarchical summary: {e}")
            return {'grouped_summaries': {}, 'total_findings': 0, 'top_findings': []}
    
    async def _summarize_finding_group(self, findings: List[Dict]) -> List[Dict]:
        """Summarize a group of findings"""
        try:
            # Combine findings into summary prompt
            findings_text = []
            for i, finding in enumerate(findings):
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(f"{i+1}. {content[:300]}...")
            
            combined_text = "\n".join(findings_text)
            
            # Check token limit
            if self.token_manager.count_tokens(combined_text) > self.max_synthesis_tokens:
                # Too long, take only top findings
                return findings[:5]
            
            system_prompt = """
            Summarize the following research findings into 3-5 key points.
            Maintain the most important information and specific details.
            Keep jurisdiction and regulatory information intact.
            
            Return as JSON array:
            [
                {
                    "summary": "Key point summary",
                    "jurisdiction": "Relevant jurisdiction",
                    "confidence": 0.8,
                    "supporting_details": "Important details"
                }
            ]
            """
            
            messages = [{"role": "user", "content": f"Summarize these findings:\n\n{combined_text}"}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                summary = safe_json_parse(response, [])
                return summary if isinstance(summary, list) else [summary]
            except Exception:
                # Fallback to top findings
                return findings[:3]
            
        except Exception as e:
            logger.error(f"Error summarizing finding group: {e}")
            return findings[:3]
    
    async def _progressive_synthesis(self, hierarchical_summary: Dict, query: str, 
                                   research_state: 'MultiAgentResearchState') -> str:
        """Create final synthesis progressively"""
        try:
            # Stage 1: Create executive summary
            executive_summary = await self._create_executive_summary(hierarchical_summary, query)
            
            # Stage 2: Create detailed analysis
            detailed_analysis = await self._create_detailed_analysis(hierarchical_summary, query)
            
            # Stage 3: Create recommendations
            recommendations = await self._create_recommendations(hierarchical_summary, query)
            
            # Stage 4: Combine with token management
            final_report = await self._combine_synthesis_stages(
                executive_summary, detailed_analysis, recommendations, research_state
            )
            
            return final_report
            
        except Exception as e:
            logger.error(f"Error in progressive synthesis: {e}")
            return f"Unable to complete synthesis: {str(e)}"
    
    async def _create_executive_summary(self, hierarchical_summary: Dict, query: str) -> str:
        """Create executive summary"""
        try:
            top_findings = hierarchical_summary.get('top_findings', [])[:5]
            
            findings_text = []
            for finding in top_findings:
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(content[:200])
            
            system_prompt = """
            Create a concise executive summary (2-3 paragraphs) that answers the research question.
            Focus on the most important findings and their implications.
            Include specific regulatory references where available.
            """
            
            messages = [{"role": "user", "content": f"""
            Query: {query}
            
            Key Findings:
            {chr(10).join(findings_text)}
            
            Create executive summary.
            """}]
            
            return await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
        except Exception as e:
            logger.error(f"Error creating executive summary: {e}")
            return "Executive summary unavailable."
    
    async def _create_detailed_analysis(self, hierarchical_summary: Dict, query: str) -> str:
        """Create detailed analysis section"""
        try:
            grouped_summaries = hierarchical_summary.get('grouped_summaries', {})
            
            analysis_sections = []
            
            for group_key, group_findings in list(grouped_summaries.items())[:5]:  # Limit to 5 groups
                jurisdiction = group_key.split('_')[0]
                
                # Create section for this group
                section_content = []
                
                if isinstance(group_findings, list):
                    for finding in group_findings[:3]:  # Limit to 3 findings per group
                        content = finding.get('content', '') or finding.get('insight', '')
                        section_content.append(content[:300])
                
                if section_content:
                    section_text = f"**{jurisdiction}**: " + " ".join(section_content)
                    analysis_sections.append(section_text)
            
            detailed_analysis = "\n\n".join(analysis_sections)
            
            # Ensure token limit
            if self.token_manager.count_tokens(detailed_analysis) > 8000:
                detailed_analysis = self.token_manager.truncate_to_limit(detailed_analysis, 8000)
            
            return detailed_analysis
            
        except Exception as e:
            logger.error(f"Error creating detailed analysis: {e}")
            return "Detailed analysis unavailable."
    
    async def _create_recommendations(self, hierarchical_summary: Dict, query: str) -> str:
        """Create recommendations section"""
        try:
            top_findings = hierarchical_summary.get('top_findings', [])[:3]
            
            findings_text = []
            for finding in top_findings:
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(content[:150])
            
            system_prompt = """
            Based on the research findings, provide 3-5 specific, actionable recommendations.
            Focus on practical implementation steps and compliance considerations.
            """
            
            messages = [{"role": "user", "content": f"""
            Query: {query}
            
            Key Findings:
            {chr(10).join(findings_text)}
            
            Provide actionable recommendations.
            """}]
            
            return await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
        except Exception as e:
            logger.error(f"Error creating recommendations: {e}")
            return "Recommendations unavailable."
    
    async def _combine_synthesis_stages(self, executive_summary: str, detailed_analysis: str, 
                                      recommendations: str, research_state: 'MultiAgentResearchState') -> str:
        """Combine synthesis stages with token management"""
        try:
            # Calculate token usage
            exec_tokens = self.token_manager.count_tokens(executive_summary)
            analysis_tokens = self.token_manager.count_tokens(detailed_analysis)
            rec_tokens = self.token_manager.count_tokens(recommendations)
            
            # Add metadata
            metadata = f"""
**Research Summary**
- Query: {research_state.original_query}
- Iterations: {research_state.current_iteration}
- Agents: {', '.join(research_state.agent_states.keys())}
- Confidence: {research_state.overall_confidence:.2f}

"""
            
            metadata_tokens = self.token_manager.count_tokens(metadata)
            total_tokens = exec_tokens + analysis_tokens + rec_tokens + metadata_tokens
            
            # If within limits, combine all
            if total_tokens <= self.token_manager.effective_limit:
                return f"""
{metadata}

**Executive Summary**
{executive_summary}

**Detailed Analysis**
{detailed_analysis}

**Recommendations**
{recommendations}
"""
            
            # If too long, prioritize and truncate
            else:
                # Always include metadata and executive summary
                core_content = f"""
{metadata}

**Executive Summary**
{executive_summary}

**Key Findings**
{self.token_manager.truncate_to_limit(detailed_analysis, 6000)}

**Recommendations**
{self.token_manager.truncate_to_limit(recommendations, 2000)}
"""
                
                return core_content
            
        except Exception as e:
            logger.error(f"Error combining synthesis stages: {e}")
            return f"Synthesis combination error: {str(e)}"
    
    async def _optimize_final_synthesis(self, synthesis: str) -> str:
        """Final optimization of synthesis"""
        try:
            synthesis_tokens = self.token_manager.count_tokens(synthesis)
            
            if synthesis_tokens <= config.MAX_RESPONSE_TOKENS:
                return synthesis
            
            logger.warning(f"Synthesis too long ({synthesis_tokens} tokens), applying final optimization")
            
            # Extract sections
            sections = synthesis.split('\n\n')
            
            # Prioritize sections
            prioritized_sections = []
            for section in sections:
                if any(keyword in section.lower() for keyword in ['executive summary', 'research summary']):
                    prioritized_sections.insert(0, section)  # Highest priority
                elif any(keyword in section.lower() for keyword in ['key findings', 'recommendations']):
                    prioritized_sections.append(section)  # High priority
                else:
                    prioritized_sections.append(section)  # Normal priority
            
            # Build optimized response
            result_sections = []
            current_tokens = 0
            
            for section in prioritized_sections:
                section_tokens = self.token_manager.count_tokens(section)
                if current_tokens + section_tokens <= config.MAX_RESPONSE_TOKENS:
                    result_sections.append(section)
                    current_tokens += section_tokens
                elif current_tokens < config.MAX_RESPONSE_TOKENS * 0.8:
                    # Try truncated version
                    remaining_tokens = config.MAX_RESPONSE_TOKENS - current_tokens - 100
                    truncated = self.token_manager.truncate_to_limit(section, remaining_tokens)
                    result_sections.append(truncated)
                    break
            
            final_synthesis = '\n\n'.join(result_sections)
            
            # Add optimization notice
            if len(result_sections) < len(prioritized_sections):
                final_synthesis += "\n\n*[Response optimized for length - additional details available upon request]*"
            
            return final_synthesis
            
        except Exception as e:
            logger.error(f"Error in final synthesis optimization: {e}")
            return self.token_manager.truncate_to_limit(synthesis, config.MAX_RESPONSE_TOKENS)

# Serialization Helper Functions
def serialize_dataclass(obj):
    """Convert dataclass to JSON-serializable dictionary"""
    if obj is None:
        return None
    
    try:
        if hasattr(obj, '__dataclass_fields__'):
            result = {}
            for field_name, field_value in asdict(obj).items():
                result[field_name] = serialize_object(field_value)
            return result
        else:
            return serialize_object(obj)
    except Exception as e:
        logger.error(f"Error serializing dataclass {type(obj)}: {e}")
        return {"error": f"Serialization failed: {str(e)}"}

def serialize_object(obj):
    """Recursively serialize any object to JSON-compatible format"""
    if obj is None:
        return None
    elif isinstance(obj, (str, int, float, bool)):
        return obj
    elif isinstance(obj, dict):
        return {key: serialize_object(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [serialize_object(item) for item in obj]
    elif hasattr(obj, '__dataclass_fields__'):
        return serialize_dataclass(obj)
    elif hasattr(obj, '__dict__'):
        return {key: serialize_object(value) for key, value in obj.__dict__.items()}
    else:
        return str(obj)

# Enhanced Data Models for Multi-Agent Deep Research
@dataclass
class ResearchTask:
    """Individual research task for specialized agents"""
    task_id: str
    agent_type: str
    query: str
    focus_areas: List[str]
    jurisdictions: Optional[List[str]] = None
    priority: int = 1
    max_results: int = 10
    context: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class ResearchPlan:
    """Enhanced research plan with multi-agent coordination"""
    plan_id: str
    main_query: str
    research_objectives: List[str]
    research_tasks: List[ResearchTask]
    agent_assignments: Dict[str, List[str]]
    expected_iterations: int
    total_estimated_time: int
    success_criteria: List[str]
    coordination_strategy: str
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class AgentState:
    """State for individual research agents"""
    agent_id: str
    agent_type: str
    current_tasks: List[str]
    completed_tasks: List[str]
    findings: List[Dict[str, Any]]
    knowledge_gaps: List[str]
    confidence_score: float
    status: str
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

@dataclass
class MultiAgentResearchState:
    """State for the entire multi-agent research system"""
    session_id: str
    original_query: str
    research_plan: Optional[ResearchPlan]
    agent_states: Dict[str, AgentState]
    shared_knowledge: Dict[str, List[Dict]]
    iteration_history: List[Dict[str, Any]]
    current_iteration: int
    max_iterations: int
    overall_confidence: float
    is_complete: bool = False
    final_synthesis: str = ""
    memory_keys: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return serialize_dataclass(self)

# Memory Management State
class MemoryState(TypedDict):
    """State for memory operations"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    user_id: str
    session_id: str
    current_context: Dict[str, Any]
    memory_operations: List[Dict[str, Any]]

# Multi-Agent Research State for LangGraph
class MultiAgentGraphState(TypedDict):
    """Enhanced state for LangGraph multi-agent workflow"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    research_state: Optional[MultiAgentResearchState]
    current_phase: str
    active_agents: List[str]
    coordination_messages: List[Dict[str, Any]]
    memory_context: Dict[str, Any]

# LLM-based Domain Filter
class LLMDomainFilter:
    """LLM-based filter to determine if queries are privacy/law related"""
    
    def __init__(self, openai_manager):
        self.openai_manager = openai_manager
    
    async def is_privacy_law_related(self, query: str) -> Dict[str, Any]:
        """Use LLM to determine if query is privacy/law related"""
        if not query or len(query.strip()) < 3:
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Query too short or empty"
            }
        
        system_prompt = """
        You are an expert classifier for determining if queries are related to data privacy, 
        data protection, legal compliance, or regulatory matters.
        
        Analyze the given query and determine if it falls within these domains:
        - Data privacy and protection regulations (GDPR, CCPA, LGPD, etc.)
        - Legal compliance and regulatory requirements
        - Information security and data governance
        - Privacy rights and obligations
        - Legal frameworks and legislation
        - Regulatory compliance across jurisdictions
        - Data handling and processing legal requirements
        - Privacy policy and legal documentation
        - Breach notification and legal obligations
        - Cross-border data transfer regulations
        
        Return ONLY a JSON response in this exact format:
        {
            "is_relevant": boolean,
            "confidence": float between 0.0 and 1.0,
            "reasoning": "Brief explanation of the decision",
            "domain": "privacy|legal|compliance|other"
        }
        """
        
        try:
            messages = [{"role": "user", "content": f"Classify this query: {query}"}]
            
            response = await self.openai_manager.chat_completion(
                messages, system_prompt=system_prompt
            )
            
            result = safe_json_parse(response, {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Failed to parse LLM response",
                "domain": "other"
            })
            
            # Ensure all required fields exist
            return {
                "is_relevant": result.get("is_relevant", False),
                "confidence": result.get("confidence", 0.0),
                "reasoning": result.get("reasoning", "No reasoning provided"),
                "domain": result.get("domain", "other")
            }
            
        except Exception as e:
            logger.error(f"Error in LLM domain classification: {e}")
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": f"Classification error: {str(e)}",
                "domain": "error"
            }
    
    def get_rejection_message(self, domain_result: Dict[str, Any]) -> str:
        """Generate appropriate rejection message based on classification"""
        reasoning = domain_result.get("reasoning", "")
        
        messages = [
            "I specialize in data privacy, data protection, and legal compliance questions.",
            "My expertise covers privacy regulations, legal frameworks, and compliance requirements.",
            "I focus on data protection laws, privacy rights, and regulatory compliance matters.",
            "I'm designed to help with privacy regulations, legal requirements, and compliance questions."
        ]
        
        import random
        base_message = random.choice(messages)
        
        if reasoning and "not related" not in reasoning.lower():
            return f"{base_message} {reasoning}"
        else:
            return f"{base_message} Please ask questions related to privacy, data protection, or legal compliance."

# Enhanced OpenAI Manager (using only o3-mini)
class EnhancedOpenAIManager:
    """Enhanced OpenAI manager using ONLY o3-mini model"""
    
    def __init__(self):
        try:
            self.client = openai.OpenAI(
                api_key=config.OPENAI_API_KEY,
                base_url=config.OPENAI_BASE_URL
            )
            self._test_connection()
            logger.info(f"✓ OpenAI client initialized with model: {config.MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _test_connection(self):
        """Test OpenAI connection with o3-mini ONLY"""
        try:
            test_response = self.client.chat.completions.create(
                model=config.MODEL,  # o3-mini-2025-01-31
                messages=[{"role": "user", "content": "test connection"}],
                reasoning_effort=config.REASONING_EFFORT
            )
            logger.info(f"✓ OpenAI connection successful with {config.MODEL}")
        except Exception as e:
            logger.error(f"OpenAI connection test failed with {config.MODEL}: {e}")
            raise
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using OpenAI API"""
        if not text or not isinstance(text, str):
            logger.warning("Empty or invalid text for embedding")
            return [0.0] * config.EMBEDDING_DIMENSIONS
        
        try:
            clean_text = text.strip()[:8000]
            response = self.client.embeddings.create(
                model=config.EMBEDDING_MODEL,
                input=clean_text,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], system_prompt: str = None, **kwargs) -> str:
        """Create chat completion with o3-mini ONLY and reasoning effort"""
        if not messages:
            raise ValueError("Messages cannot be empty")
        
        try:
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({"role": "system", "content": system_prompt})
            
            formatted_messages.extend(messages)
            
            # Only use parameters that o3-mini supports
            # Remove unsupported parameters for o3-mini
            clean_kwargs = {
                "model": config.MODEL,  # Force o3-mini
                "messages": formatted_messages,
                "reasoning_effort": config.REASONING_EFFORT
            }
            
            # Add any other supported parameters if needed
            # Note: o3-mini has limited parameter support
            
            response = self.client.chat.completions.create(**clean_kwargs)
            
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in chat completion with {config.MODEL}: {e}")
            raise

# Enhanced Elasticsearch Manager
class EnhancedElasticsearchManager:
    """Enhanced Elasticsearch manager with improved search strategies"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager):
        self.openai_manager = openai_manager
        self.client = self._create_client()
        self.embeddings = self._create_embeddings()
        
        if ELASTICSEARCH_LANGCHAIN_AVAILABLE:
            self._setup_stores()
        else:
            logger.warning("LangChain Elasticsearch not available, using basic search")
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client"""
        try:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            client = Elasticsearch(
                [{"host": config.ES_HOST, "port": config.ES_PORT, "scheme": "https"}],
                basic_auth=(config.ES_USERNAME, config.ES_PASSWORD),
                ssl_context=ssl_context,
                verify_certs=False,
                request_timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            info = client.info()
            logger.info(f"Connected to Elasticsearch: {info.get('version', {}).get('number', 'unknown')}")
            return client
            
        except Exception as e:
            logger.error(f"Failed to create Elasticsearch client: {e}")
            raise
    
    def _create_embeddings(self):
        """Create embeddings wrapper"""
        class DirectOpenAIEmbeddings(Embeddings):
            def __init__(self, openai_manager):
                self.openai_manager = openai_manager
                super().__init__()
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                try:
                    loop = get_event_loop()
                    if loop.is_running():
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, self._aembed_documents(texts))
                            return future.result()
                    else:
                        return asyncio.run(self._aembed_documents(texts))
                except Exception as e:
                    logger.error(f"Error in embed_documents: {e}")
                    return [[0.0] * config.EMBEDDING_DIMENSIONS for _ in texts]
            
            def embed_query(self, text: str) -> List[float]:
                try:
                    loop = get_event_loop()
                    if loop.is_running():
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, self.openai_manager.create_embedding(text))
                            return future.result()
                    else:
                        return asyncio.run(self.openai_manager.create_embedding(text))
                except Exception as e:
                    logger.error(f"Error in embed_query: {e}")
                    return [0.0] * config.EMBEDDING_DIMENSIONS
            
            async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
                return await self._aembed_documents(texts)
            
            async def aembed_query(self, text: str) -> List[float]:
                return await self.openai_manager.create_embedding(text)
            
            async def _aembed_documents(self, texts: List[str]) -> List[List[float]]:
                embeddings = []
                for text in texts:
                    try:
                        embedding = await self.openai_manager.create_embedding(text)
                        embeddings.append(embedding)
                    except Exception as e:
                        logger.error(f"Error embedding document: {e}")
                        embeddings.append([0.0] * config.EMBEDDING_DIMENSIONS)
                return embeddings
        
        return DirectOpenAIEmbeddings(self.openai_manager)
    
    def _setup_stores(self):
        """Setup LangChain Elasticsearch stores"""
        try:
            self.articles_store = ElasticsearchStore(
                es_connection=self.client,
                index_name="privacy_articles",
                embedding=self.embeddings,
                vector_query_field="full_article_embedding",
                query_field="full_content",
                strategy=DenseVectorStrategy(hybrid=False),
                distance_strategy="COSINE"
            )
            
            self.chunks_store = ElasticsearchStore(
                es_connection=self.client,
                index_name="privacy_chunks",
                embedding=self.embeddings,
                vector_query_field="chunk_embedding",
                query_field="content",
                strategy=DenseVectorStrategy(hybrid=False),
                distance_strategy="COSINE"
            )
            
            logger.info("✓ Enhanced Elasticsearch stores initialized")
            
        except Exception as e:
            logger.error(f"Error setting up stores: {e}")
            self.articles_store = None
            self.chunks_store = None
    
    async def multi_strategy_search(self, query: str, focus_areas: List[str] = None, 
                                  jurisdictions: List[str] = None, k: int = 20) -> List[Document]:
        """Enhanced search using multiple strategies"""
        try:
            all_docs = []
            
            # Strategy 1: Direct semantic search
            semantic_docs = await self._semantic_search(query, k=k//2)
            all_docs.extend(semantic_docs)
            
            # Strategy 2: Enhanced query search with focus areas
            if focus_areas:
                for focus in focus_areas[:2]:
                    enhanced_query = f"{query} {focus}"
                    focus_docs = await self._semantic_search(enhanced_query, k=k//4)
                    all_docs.extend(focus_docs)
            
            # Strategy 3: Jurisdiction-filtered search
            if jurisdictions:
                filtered_docs = await self._filtered_search(query, {"jurisdiction": jurisdictions}, k=k//4)
                all_docs.extend(filtered_docs)
            
            # Deduplicate and rank by relevance
            return self._deduplicate_and_rank(all_docs, query)[:k]
            
        except Exception as e:
            logger.error(f"Error in multi-strategy search: {e}")
            return []
    
    async def _semantic_search(self, query: str, k: int) -> List[Document]:
        """Basic semantic search"""
        try:
            if ELASTICSEARCH_LANGCHAIN_AVAILABLE and self.articles_store and self.chunks_store:
                articles = await self._search_index("privacy_articles", query, k//2)
                chunks = await self._search_index("privacy_chunks", query, k//2)
                return articles + chunks
            else:
                # Fallback to basic search
                return await self._basic_search(query, k)
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            return []
    
    async def _search_index(self, index_name: str, query: str, k: int) -> List[Document]:
        """Search specific index"""
        try:
            query_embedding = await self.openai_manager.create_embedding(query)
            
            vector_field = "full_article_embedding" if "articles" in index_name else "chunk_embedding"
            
            body = {
                "size": k,
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": f"cosineSimilarity(params.query_vector, '{vector_field}') + 1.0",
                            "params": {"query_vector": query_embedding}
                        }
                    }
                }
            }
            
            response = self.client.search(index=index_name, body=body)
            
            docs = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                content = source.get('full_content' if 'articles' in index_name else 'content', '')
                
                doc = Document(
                    page_content=content,
                    metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                )
                docs.append(doc)
            
            return docs
            
        except Exception as e:
            logger.error(f"Error searching index {index_name}: {e}")
            return []
    
    async def _basic_search(self, query: str, k: int) -> List[Document]:
        """Basic fallback search without LangChain"""
        try:
            # Simple text search as fallback
            body = {
                "size": k,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["content", "full_content", "title"]
                    }
                }
            }
            
            docs = []
            for index_name in ["privacy_chunks", "privacy_articles"]:
                try:
                    response = self.client.search(index=index_name, body=body)
                    for hit in response['hits']['hits']:
                        source = hit['_source']
                        content = source.get('content') or source.get('full_content', '')
                        doc = Document(
                            page_content=content,
                            metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                        )
                        docs.append(doc)
                except Exception:
                    continue
            
            return docs[:k]
            
        except Exception as e:
            logger.error(f"Error in basic search: {e}")
            return []
    
    async def _filtered_search(self, query: str, filters: Dict, k: int) -> List[Document]:
        """Search with filters"""
        try:
            query_embedding = await self.openai_manager.create_embedding(query)
            
            filter_queries = []
            for field, values in filters.items():
                if isinstance(values, list):
                    filter_queries.append({"terms": {field: values}})
                else:
                    filter_queries.append({"term": {field: values}})
            
            body = {
                "size": k,
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'chunk_embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "filter": filter_queries
                    }
                }
            }
            
            response = self.client.search(index="privacy_chunks", body=body)
            
            docs = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                doc = Document(
                    page_content=source.get('content', ''),
                    metadata={**source, '_score': hit['_score'], '_id': hit['_id']}
                )
                docs.append(doc)
            
            return docs
            
        except Exception as e:
            logger.error(f"Error in filtered search: {e}")
            return []
    
    def _deduplicate_and_rank(self, docs: List[Document], query: str) -> List[Document]:
        """Deduplicate documents and rank by relevance"""
        try:
            seen_content = set()
            unique_docs = []
            
            for doc in docs:
                content_hash = hash(doc.page_content[:200])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_docs.append(doc)
            
            # Sort by score (higher is better)
            unique_docs.sort(key=lambda x: x.metadata.get('_score', 0), reverse=True)
            
            return unique_docs
            
        except Exception as e:
            logger.error(f"Error in deduplication: {e}")
            return docs

# Optimized Memory Manager with Token Management
class OptimizedMemoryManager:
    """Advanced memory manager using LangMem for long-term memory with token optimization"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager):
        self.openai_manager = openai_manager
        self.memory_manager = None
        self.store = InMemoryStore()
        self.token_manager = TokenManager()
        self.max_memory_tokens = config.MAX_MEMORY_TOKENS
        
        if LANGMEM_AVAILABLE:
            try:
                self._setup_langmem()
                logger.info("✓ LangMem memory manager initialized")
            except Exception as e:
                logger.warning(f"Failed to setup LangMem: {e}")
        else:
            logger.info("Using basic memory store as fallback")
    
    def _setup_langmem(self):
        """Setup LangMem memory manager"""
        if not LANGMEM_AVAILABLE:
            return
        
        try:
            # Create memory manager with OpenAI model specification
            self.memory_manager = create_memory_manager(
                model=f"openai:{config.MODEL}",
                instructions="""
                Extract and store important research findings, user preferences, 
                research patterns, and domain insights. Focus on:
                1. Key regulatory findings and interpretations
                2. User research preferences and interests
                3. Successful research strategies and patterns
                4. Cross-jurisdictional insights and comparisons
                5. Important facts and relationships discovered
                
                Organize memories to support future research sessions.
                """,
                namespace_prefix=config.MEMORY_NAMESPACE_PREFIX
            )
        except Exception as e:
            logger.error(f"Error setting up LangMem: {e}")
            raise
    
    async def store_research_findings(self, user_id: str, session_id: str, 
                                    findings: List[Dict], context: Dict) -> List[str]:
        """Store research findings with token optimization"""
        try:
            if not findings:
                return []
            
            # Prioritize findings
            prioritizer = ContentPrioritizer(self.openai_manager)
            prioritized_findings = await prioritizer.prioritize_findings(findings, context.get('query', ''))
            
            memory_keys = []
            
            # Store top findings with token management
            for i, finding in enumerate(prioritized_findings[:20]):  # Limit to top 20
                try:
                    # Optimize finding content
                    optimized_finding = self._optimize_finding_for_storage(finding)
                    
                    memory_content = {
                        "finding": optimized_finding,
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "priority": finding.get('priority_score', 0.5),
                        "query_context": context.get('query', '')[:200]  # Truncate context
                    }
                    
                    # Check token limit
                    content_tokens = self.token_manager.count_tokens(json.dumps(memory_content))
                    if content_tokens > self.max_memory_tokens:
                        # Truncate content
                        memory_content['finding'] = self.token_manager.truncate_to_limit(
                            str(memory_content['finding']), self.max_memory_tokens - 500
                        )
                    
                    # Store in memory
                    memory_key = f"finding_{session_id}_{i}"
                    namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id, "research_findings"]
                    
                    await self.store.aput(namespace, memory_key, memory_content)
                    memory_keys.append(memory_key)
                    
                except Exception as e:
                    logger.error(f"Error storing individual finding: {e}")
                    continue
            
            return memory_keys
            
        except Exception as e:
            logger.error(f"Error storing research findings: {e}")
            return []
    
    def _optimize_finding_for_storage(self, finding: Dict) -> Dict:
        """Optimize finding for storage with token limits"""
        try:
            optimized = {}
            
            # Extract key fields with truncation
            content = finding.get('content', '') or finding.get('insight', '')
            optimized['content'] = self.token_manager.truncate_to_limit(content, 2000)
            
            # Keep essential metadata
            for key in ['jurisdiction', 'confidence', 'regulation', 'implication']:
                if key in finding:
                    value = finding[key]
                    if isinstance(value, str):
                        optimized[key] = self.token_manager.truncate_to_limit(value, 500)
                    else:
                        optimized[key] = value
            
            return optimized
            
        except Exception as e:
            logger.error(f"Error optimizing finding: {e}")
            return finding
    
    async def retrieve_relevant_memories(self, user_id: str, query: str, 
                                       limit: int = 10) -> List[Dict]:
        """Retrieve relevant memories for current research"""
        try:
            namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id]
            
            # Basic memory retrieval - would be enhanced with actual LangMem search
            memories = []
            try:
                stored_items = await self.store.asearch(namespace, query=query, limit=limit)
                memories = [item.value for item in stored_items if hasattr(item, 'value')]
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
            
            return memories
            
        except Exception as e:
            logger.error(f"Error retrieving memories: {e}")
            return []
    
    async def store_user_preferences(self, user_id: str, preferences: Dict) -> str:
        """Store user research preferences"""
        try:
            namespace = [config.MEMORY_NAMESPACE_PREFIX, user_id, "preferences"]
            memory_key = "user_preferences"
            
            # Optimize preferences for storage
            optimized_preferences = {}
            for key, value in preferences.items():
                if isinstance(value, str):
                    optimized_preferences[key] = self.token_manager.truncate_to_limit(value, 200)
                else:
                    optimized_preferences[key] = value
            
            await self.store.aput(namespace, memory_key, optimized_preferences)
            return memory_key
            
        except Exception as e:
            logger.error(f"Error storing user preferences: {e}")
            return ""

# Specialized Research Agents
class SpecializedResearchAgent:
    """Base class for specialized research agents"""
    
    def __init__(self, agent_type: str, openai_manager: EnhancedOpenAIManager,
                 es_manager: EnhancedElasticsearchManager):
        self.agent_type = agent_type
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.agent_id = f"{agent_type}_{uuid.uuid4().hex[:8]}"
        self.token_manager = TokenManager()
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute a research task - to be implemented by subclasses"""
        raise NotImplementedError

class PlannerAgent(SpecializedResearchAgent):
    """Agent specialized in research planning and coordination"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("planner", openai_manager, es_manager)
    
    async def create_research_plan(self, query: str, context: Dict = None) -> ResearchPlan:
        """Create comprehensive research plan with multi-agent coordination"""
        system_prompt = """
        You are a research planning expert specializing in data privacy and protection regulations.
        Create a detailed research plan that coordinates multiple specialized agents.
        
        Break down the research into specific tasks that can be executed by:
        1. Domain Expert Agents (jurisdiction-specific research)
        2. Concept Analysis Agents (deep concept exploration)
        3. Comparative Analysis Agents (cross-jurisdictional comparison)
        4. Synthesis Agents (knowledge synthesis and gap analysis)
        
        Consider task dependencies, optimal execution order, resource allocation,
        and coordination requirements.
        
        Return a detailed JSON plan with task assignments.
        """
        
        try:
            # Truncate query if too long
            truncated_query = self.token_manager.truncate_to_limit(query, 500)
            truncated_context = self.token_manager.truncate_to_limit(str(context or {}), 1000)
            
            messages = [{"role": "user", "content": f"""
            Create a comprehensive research plan for: {truncated_query}
            
            Context: {truncated_context}
            
            Return a JSON plan with:
            {{
                "main_query": "{truncated_query}",
                "research_objectives": ["objective1", "objective2"],
                "research_tasks": [
                    {{
                        "task_id": "task_1",
                        "agent_type": "domain_expert",
                        "query": "specific research query",
                        "focus_areas": ["area1", "area2"],
                        "jurisdictions": ["EU", "US"],
                        "priority": 1,
                        "dependencies": []
                    }}
                ],
                "agent_assignments": {{
                    "domain_expert": ["task_1", "task_2"],
                    "concept_analyst": ["task_3"],
                    "comparative_analyst": ["task_4"]
                }},
                "expected_iterations": 3,
                "coordination_strategy": "sequential_with_feedback"
            }}
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            plan_data = safe_json_parse(response, {})
            
            # Create ResearchTask objects
            tasks = []
            for task_data in plan_data.get("research_tasks", []):
                task = ResearchTask(
                    task_id=task_data.get("task_id", f"task_{len(tasks)}"),
                    agent_type=task_data.get("agent_type", "domain_expert"),
                    query=task_data.get("query", truncated_query),
                    focus_areas=task_data.get("focus_areas", []),
                    jurisdictions=task_data.get("jurisdictions"),
                    priority=task_data.get("priority", 1),
                    dependencies=task_data.get("dependencies", [])
                )
                tasks.append(task)
            
            return ResearchPlan(
                plan_id=f"plan_{uuid.uuid4().hex[:8]}",
                main_query=truncated_query,
                research_objectives=plan_data.get("research_objectives", []),
                research_tasks=tasks,
                agent_assignments=plan_data.get("agent_assignments", {}),
                expected_iterations=plan_data.get("expected_iterations", 3),
                total_estimated_time=len(tasks) * 2,
                success_criteria=["Comprehensive coverage", "High confidence findings"],
                coordination_strategy=plan_data.get("coordination_strategy", "sequential")
            )
            
        except Exception as e:
            logger.error(f"Error creating research plan: {e}")
            return self._create_fallback_plan(query)
    
    def _create_fallback_plan(self, query: str) -> ResearchPlan:
        """Create a simple fallback plan"""
        task = ResearchTask(
            task_id="fallback_task",
            agent_type="domain_expert",
            query=self.token_manager.truncate_to_limit(query, 500),
            focus_areas=["general_research"],
            priority=1
        )
        
        return ResearchPlan(
            plan_id=f"fallback_plan_{uuid.uuid4().hex[:8]}",
            main_query=self.token_manager.truncate_to_limit(query, 500),
            research_objectives=["Investigate the topic"],
            research_tasks=[task],
            agent_assignments={"domain_expert": ["fallback_task"]},
            expected_iterations=2,
            total_estimated_time=5,
            success_criteria=["Basic coverage"],
            coordination_strategy="sequential"
        )

class DomainExpertAgent(SpecializedResearchAgent):
    """Agent specialized in jurisdiction-specific domain expertise"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("domain_expert", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute domain-specific research task"""
        try:
            logger.info(f"Domain expert executing task: {task.task_id}")
            
            # Perform specialized search
            docs = await self.es_manager.multi_strategy_search(
                query=task.query,
                focus_areas=task.focus_areas,
                jurisdictions=task.jurisdictions,
                k=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "findings": [],
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            # Analyze findings with domain expertise
            findings = await self._analyze_domain_findings(task, docs, shared_context)
            
            # Calculate confidence based on result quality
            confidence = self._calculate_confidence(docs, findings)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "findings": findings,
                "sources": [doc.metadata for doc in docs[:5]],
                "confidence": confidence,
                "status": "completed",
                "recommendations": await self._generate_recommendations(task, findings)
            }
            
        except Exception as e:
            logger.error(f"Error in domain expert task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_domain_findings(self, task: ResearchTask, docs: List[Document], 
                                     shared_context: Dict) -> List[Dict]:
        """Analyze findings with domain expertise"""
        try:
            # Prepare context from documents with token management
            doc_context_parts = []
            total_tokens = 0
            max_context_tokens = 8000
            
            for i, doc in enumerate(docs[:10]):
                doc_snippet = f"Document {i+1} ({doc.metadata.get('jurisdiction', 'Unknown')}): {doc.page_content[:500]}..."
                doc_tokens = self.token_manager.count_tokens(doc_snippet)
                
                if total_tokens + doc_tokens <= max_context_tokens:
                    doc_context_parts.append(doc_snippet)
                    total_tokens += doc_tokens
                else:
                    break
            
            doc_context = "\n\n".join(doc_context_parts)
            
            system_prompt = f"""
            You are a domain expert in data privacy and protection regulations.
            Focus on jurisdiction: {', '.join(task.jurisdictions or ['All'])}
            Focus areas: {', '.join(task.focus_areas)}
            
            Analyze the provided documents and extract key domain-specific insights.
            Consider regulatory requirements, compliance obligations, and practical implications.
            
            Return insights as JSON array:
            [
                {{
                    "insight": "Key finding",
                    "jurisdiction": "Specific jurisdiction",
                    "regulation": "Specific regulation/article",
                    "implication": "Practical implication",
                    "confidence": 0.9
                }}
            ]
            """
            
            # Truncate context if needed
            truncated_context = self.token_manager.truncate_to_limit(str(shared_context or {}), 1000)
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            
            Document Context:
            {doc_context}
            
            Shared Research Context:
            {truncated_context}
            
            Extract domain-specific insights following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            # Parse findings
            try:
                findings = safe_json_parse(response, [])
                if not isinstance(findings, list):
                    findings = [findings] if isinstance(findings, dict) else []
                return findings
            except Exception:
                logger.warning("Failed to parse findings as JSON")
                return [{"insight": response[:500], "confidence": 0.5}]
            
        except Exception as e:
            logger.error(f"Error analyzing domain findings: {e}")
            return []
    
    def _calculate_confidence(self, docs: List[Document], findings: List[Dict]) -> float:
        """Calculate confidence score based on result quality"""
        try:
            if not docs or not findings:
                return 0.0
            
            # Factors for confidence calculation
            doc_score = min(len(docs) / 10, 1.0)
            finding_score = min(len(findings) / 5, 1.0)
            
            # Average document relevance score
            avg_doc_score = sum(doc.metadata.get('_score', 0.5) for doc in docs) / len(docs)
            relevance_score = min(avg_doc_score / 2.0, 1.0)
            
            # Average finding confidence
            finding_confidence = sum(f.get('confidence', 0.5) for f in findings) / len(findings)
            
            # Combined confidence
            confidence = (doc_score + finding_score + relevance_score + finding_confidence) / 4
            return round(confidence, 2)
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5
    
    async def _generate_recommendations(self, task: ResearchTask, findings: List[Dict]) -> List[str]:
        """Generate recommendations based on findings"""
        if not findings:
            return ["Consider refining the research query for better results"]
        
        try:
            recommendations = []
            
            high_conf_findings = [f for f in findings if f.get('confidence', 0) > 0.7]
            if high_conf_findings:
                recommendations.append(f"Found {len(high_conf_findings)} high-confidence insights")
            
            jurisdictions_covered = set(f.get('jurisdiction', 'Unknown') for f in findings)
            if len(jurisdictions_covered) > 1:
                recommendations.append(f"Multi-jurisdictional coverage: {', '.join(jurisdictions_covered)}")
            
            if len(findings) < 3:
                recommendations.append("Consider broader search terms for more comprehensive coverage")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            return []

class ConceptAnalysisAgent(SpecializedResearchAgent):
    """Agent specialized in deep concept analysis and relationship mapping"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("concept_analyst", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute concept analysis task"""
        try:
            logger.info(f"Concept analyst executing task: {task.task_id}")
            
            docs = await self.es_manager.multi_strategy_search(
                query=task.query,
                focus_areas=task.focus_areas,
                k=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "concepts": {},
                    "relationships": {},
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            concept_analysis = await self._analyze_concepts(task, docs, shared_context)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "concepts": concept_analysis.get("concepts", {}),
                "relationships": concept_analysis.get("relationships", {}),
                "definitions": concept_analysis.get("definitions", {}),
                "concept_evolution": concept_analysis.get("evolution", {}),
                "confidence": concept_analysis.get("confidence", 0.5),
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Error in concept analysis task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_concepts(self, task: ResearchTask, docs: List[Document], 
                              shared_context: Dict) -> Dict[str, Any]:
        """Deep concept analysis with token management"""
        try:
            # Prepare document context with token limits
            doc_parts = []
            total_tokens = 0
            max_tokens = 6000
            
            for i, doc in enumerate(docs[:8]):
                doc_snippet = f"Document {i+1}: {doc.page_content[:400]}..."
                doc_tokens = self.token_manager.count_tokens(doc_snippet)
                
                if total_tokens + doc_tokens <= max_tokens:
                    doc_parts.append(doc_snippet)
                    total_tokens += doc_tokens
                else:
                    break
            
            doc_context = "\n\n".join(doc_parts)
            
            system_prompt = """
            You are a concept analysis expert specializing in data privacy and protection regulations.
            Analyze the provided documents to identify key concepts, their relationships, and evolution.
            
            Focus on:
            1. Core privacy concepts and their definitions
            2. Relationships between concepts
            3. How concepts are interpreted across jurisdictions
            4. Evolution of concepts over time
            
            Return analysis as JSON:
            {
                "concepts": {
                    "concept_name": {
                        "definition": "Definition",
                        "importance": 0.9,
                        "jurisdictions": ["EU", "US"],
                        "related_terms": ["term1", "term2"]
                    }
                },
                "relationships": {
                    "concept1_concept2": {
                        "type": "relationship_type",
                        "strength": 0.8,
                        "description": "Relationship description"
                    }
                },
                "definitions": {
                    "concept": "Clear definition"
                },
                "confidence": 0.85
            }
            """
            
            truncated_context = self.token_manager.truncate_to_limit(str(shared_context or {}), 500)
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            Focus Areas: {', '.join(task.focus_areas)}
            
            Document Context:
            {doc_context}
            
            Shared Context:
            {truncated_context}
            
            Perform deep concept analysis following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                return safe_json_parse(response, {"concepts": {}, "confidence": 0.3})
            except Exception:
                logger.warning("Failed to parse concept analysis as JSON")
                return {"concepts": {}, "confidence": 0.3}
            
        except Exception as e:
            logger.error(f"Error in concept analysis: {e}")
            return {"concepts": {}, "confidence": 0.0}

class SynthesisAgent(SpecializedResearchAgent):
    """Agent specialized in knowledge synthesis and report generation"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, es_manager: EnhancedElasticsearchManager):
        super().__init__("synthesizer", openai_manager, es_manager)
        self.progressive_synthesizer = ProgressiveSynthesizer(openai_manager, self.token_manager)
    
    async def synthesize_research(self, research_state: MultiAgentResearchState) -> str:
        """Synthesize all research findings into comprehensive report"""
        try:
            logger.info("Synthesizing research findings with progressive approach")
            
            # Use progressive synthesizer for token-optimized results
            return await self.progressive_synthesizer.create_progressive_synthesis(research_state)
            
        except Exception as e:
            logger.error(f"Error in research synthesis: {e}")
            return f"Research synthesis encountered an error: {str(e)}"

# Enhanced Multi-Agent Orchestrator with Token Management
class OptimizedMultiAgentOrchestrator:
    """Orchestrates multiple specialized research agents with token management"""
    
    def __init__(self, openai_manager: EnhancedOpenAIManager, 
                 es_manager: EnhancedElasticsearchManager,
                 memory_manager: OptimizedMemoryManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_manager = memory_manager
        self.token_manager = TokenManager()
        
        # Initialize specialized agents
        self.planner = PlannerAgent(openai_manager, es_manager)
        self.domain_expert = DomainExpertAgent(openai_manager, es_manager)
        self.concept_analyst = ConceptAnalysisAgent(openai_manager, es_manager)
        self.synthesizer = SynthesisAgent(openai_manager, es_manager)
        
        self.agents = {
            "planner": self.planner,
            "domain_expert": self.domain_expert,
            "concept_analyst": self.concept_analyst,
            "synthesizer": self.synthesizer
        }
    
    async def conduct_multi_agent_research(self, query: str, user_id: str = None, 
                                         context: Dict = None) -> MultiAgentResearchState:
        """Conduct comprehensive research using multiple specialized agents with token management"""
        try:
            session_id = f"research_{uuid.uuid4().hex[:8]}"
            logger.info(f"Starting multi-agent research session: {session_id}")
            
            # Check and truncate query if too long
            query_tokens = self.token_manager.count_tokens(query)
            if query_tokens > config.MAX_QUERY_TOKENS:
                logger.warning(f"Query too long ({query_tokens} tokens), truncating to {config.MAX_QUERY_TOKENS}")
                query = self.token_manager.truncate_to_limit(query, config.MAX_QUERY_TOKENS)
            
            # Phase 1: Planning
            logger.info("Phase 1: Research Planning")
            research_plan = await self.planner.create_research_plan(query, context)
            
            # Initialize research state
            research_state = MultiAgentResearchState(
                session_id=session_id,
                original_query=query,
                research_plan=research_plan,
                agent_states={},
                shared_knowledge={},
                iteration_history=[],
                current_iteration=0,
                max_iterations=research_plan.expected_iterations,
                overall_confidence=0.0
            )
            
            # Retrieve relevant memories with token management
            if user_id and self.memory_manager:
                relevant_memories = await self.memory_manager.retrieve_relevant_memories(
                    user_id, query, limit=5
                )
                # Truncate memories if too large
                memories_text = json.dumps(relevant_memories)
                if self.token_manager.count_tokens(memories_text) > 2000:
                    relevant_memories = relevant_memories[:3]  # Limit to top 3
                research_state.shared_knowledge["relevant_memories"] = relevant_memories
            
            # Phase 2: Multi-agent research execution
            logger.info("Phase 2: Multi-agent research execution")
            
            for iteration in range(research_plan.expected_iterations):
                research_state.current_iteration = iteration + 1
                logger.info(f"Starting research iteration {research_state.current_iteration}")
                
                iteration_results = await self._execute_research_iteration(
                    research_state, iteration
                )
                
                research_state.iteration_history.append(iteration_results)
                
                # Update overall confidence
                agent_confidences = [
                    state.confidence_score for state in research_state.agent_states.values()
                ]
                research_state.overall_confidence = sum(agent_confidences) / len(agent_confidences) if agent_confidences else 0.0
                
                # Check if we should continue
                if research_state.overall_confidence >= config.CONFIDENCE_THRESHOLD:
                    logger.info(f"Research confidence threshold reached: {research_state.overall_confidence:.2f}")
                    break
            
            # Phase 3: Synthesis with progressive approach
            logger.info("Phase 3: Progressive knowledge synthesis")
            research_state.final_synthesis = await self.synthesizer.synthesize_research(research_state)
            research_state.is_complete = True
            
            # Store findings in long-term memory with token optimization
            if user_id and self.memory_manager:
                await self._store_research_memories(user_id, research_state)
            
            logger.info(f"Multi-agent research completed with confidence: {research_state.overall_confidence:.2f}")
            return research_state
            
        except Exception as e:
            logger.error(f"Error in multi-agent research: {e}")
            # Return error state
            return MultiAgentResearchState(
                session_id="error",
                original_query=query,
                research_plan=None,
                agent_states={},
                shared_knowledge={"error": str(e)},
                iteration_history=[],
                current_iteration=0,
                max_iterations=1,
                overall_confidence=0.0,
                is_complete=True,
                final_synthesis=f"Research failed due to error: {str(e)}"
            )
    
    async def _execute_research_iteration(self, research_state: MultiAgentResearchState, 
                                        iteration: int) -> Dict[str, Any]:
        """Execute a single research iteration with multiple agents"""
        try:
            iteration_results = {
                "iteration": iteration + 1,
                "agent_results": {},
                "shared_updates": {},
                "timestamp": datetime.now().isoformat()
            }
            
            # Get tasks for this iteration
            tasks_to_execute = self._get_iteration_tasks(research_state, iteration)
            
            # Execute tasks with appropriate agents
            for task in tasks_to_execute:
                agent_type = task.agent_type
                if agent_type in self.agents:
                    try:
                        # Prepare shared context with token management
                        shared_context = {
                            "iteration": iteration + 1,
                            "previous_findings": research_state.shared_knowledge,
                            "research_plan": research_state.research_plan.to_dict() if research_state.research_plan else {}
                        }
                        
                        # Truncate shared context if too large
                        context_text = json.dumps(shared_context)
                        if self.token_manager.count_tokens(context_text) > 3000:
                            # Reduce previous findings
                            if "previous_findings" in shared_context:
                                findings = shared_context["previous_findings"]
                                for key in list(findings.keys()):
                                    if isinstance(findings[key], list) and len(findings[key]) > 3:
                                        findings[key] = findings[key][:3]
                        
                        # Execute task
                        result = await self.agents[agent_type].execute_task(task, shared_context)
                        iteration_results["agent_results"][task.task_id] = result
                        
                        # Update agent state
                        if agent_type not in research_state.agent_states:
                            research_state.agent_states[agent_type] = AgentState(
                                agent_id=self.agents[agent_type].agent_id,
                                agent_type=agent_type,
                                current_tasks=[],
                                completed_tasks=[],
                                findings=[],
                                knowledge_gaps=[],
                                confidence_score=0.0,
                                status="idle"
                            )
                        
                        agent_state = research_state.agent_states[agent_type]
                        agent_state.completed_tasks.append(task.task_id)
                        agent_state.findings.append(result)
                        agent_state.confidence_score = result.get("confidence", 0.0)
                        agent_state.status = "completed"
                        
                        # Update shared knowledge with token limits
                        if result.get("findings"):
                            knowledge_key = f"{agent_type}_findings"
                            if knowledge_key not in research_state.shared_knowledge:
                                research_state.shared_knowledge[knowledge_key] = []
                            
                            # Limit findings to prevent token overflow
                            new_findings = result["findings"][:5]  # Limit to top 5 findings
                            research_state.shared_knowledge[knowledge_key].extend(new_findings)
                            
                            # Keep only recent findings if too many
                            if len(research_state.shared_knowledge[knowledge_key]) > 10:
                                research_state.shared_knowledge[knowledge_key] = research_state.shared_knowledge[knowledge_key][-10:]
                        
                    except Exception as e:
                        logger.error(f"Error executing task {task.task_id} with {agent_type}: {e}")
                        iteration_results["agent_results"][task.task_id] = {
                            "task_id": task.task_id,
                            "status": "error",
                            "error": str(e)
                        }
            
            return iteration_results
            
        except Exception as e:
            logger.error(f"Error in research iteration: {e}")
            return {
                "iteration": iteration + 1,
                "error": str(e)
            }
    
    def _get_iteration_tasks(self, research_state: MultiAgentResearchState, 
                           iteration: int) -> List[ResearchTask]:
        """Get tasks to execute for current iteration"""
        if not research_state.research_plan:
            return []
        
        # For simplicity, distribute tasks across iterations
        all_tasks = research_state.research_plan.research_tasks
        tasks_per_iteration = max(1, len(all_tasks) // research_state.max_iterations)
        
        start_idx = iteration * tasks_per_iteration
        end_idx = min(start_idx + tasks_per_iteration, len(all_tasks))
        
        return all_tasks[start_idx:end_idx]
    
    async def _store_research_memories(self, user_id: str, research_state: MultiAgentResearchState):
        """Store research findings in long-term memory with token optimization"""
        try:
            if not self.memory_manager:
                return
            
            # Collect all findings
            all_findings = []
            for agent_state in research_state.agent_states.values():
                for finding in agent_state.findings:
                    if finding.get("findings"):
                        all_findings.extend(finding["findings"])
            
            # Store in memory with token management
            context = {
                "query": research_state.original_query,
                "session_id": research_state.session_id,
                "confidence": research_state.overall_confidence
            }
            
            memory_keys = await self.memory_manager.store_research_findings(
                user_id, research_state.session_id, all_findings, context
            )
            
            research_state.memory_keys = memory_keys
            logger.info(f"Stored {len(memory_keys)} research memories")
            
        except Exception as e:
            logger.error(f"Error storing research memories: {e}")

# Enhanced Deep Research Chatbot with Multi-Agent Architecture and Token Management
class EnhancedDeepResearchChatbot:
    """Enhanced chatbot with multi-agent deep research, long-term memory, and comprehensive token management"""
    
    def __init__(self):
        try:
            logger.info("Initializing Enhanced Deep Research Chatbot with Multi-Agent Architecture and Token Management...")
            
            # Initialize core managers
            self.openai_manager = EnhancedOpenAIManager()
            self.es_manager = EnhancedElasticsearchManager(self.openai_manager)
            
            # Initialize token management
            self.token_manager = TokenManager()
            self.progressive_synthesizer = ProgressiveSynthesizer(self.openai_manager, self.token_manager)
            
            # Initialize optimized memory manager
            self.memory_manager = OptimizedMemoryManager(self.openai_manager)
            
            # Initialize domain filter
            self.domain_filter = LLMDomainFilter(self.openai_manager)
            
            # Initialize optimized multi-agent orchestrator
            self.orchestrator = OptimizedMultiAgentOrchestrator(
                self.openai_manager, self.es_manager, self.memory_manager
            )
            
            # Initialize LangGraph workflow if available
            if LANGGRAPH_AVAILABLE:
                self._create_langgraph_workflow()
            else:
                logger.warning("LangGraph not available, using simplified workflow")
            
            logger.info("✓ Enhanced Deep Research Chatbot with Token Management initialized successfully!")
            
        except Exception as e:
            logger.error(f"Error initializing Enhanced Deep Research Chatbot: {e}")
            raise
    
    def _create_langgraph_workflow(self):
        """Create LangGraph workflow for multi-agent coordination"""
        try:
            # Create workflow graph
            workflow = StateGraph(MultiAgentGraphState)
            
            # Add nodes for different phases
            workflow.add_node("planning", self._planning_node)
            workflow.add_node("research", self._research_node)
            workflow.add_node("synthesis", self._synthesis_node)
            workflow.add_node("memory_update", self._memory_update_node)
            
            # Define workflow edges
            workflow.add_edge(START, "planning")
            workflow.add_edge("planning", "research")
            workflow.add_edge("research", "synthesis")
            workflow.add_edge("synthesis", "memory_update")
            workflow.add_edge("memory_update", END)
            
            # Add conditional edges for iteration control
            workflow.add_conditional_edges(
                "research",
                self._should_continue_research,
                {
                    "continue": "research",
                    "synthesize": "synthesis"
                }
            )
            
            # Compile workflow
            memory = MemorySaver()
            self.workflow = workflow.compile(checkpointer=memory)
            
            logger.info("✓ LangGraph workflow compiled successfully")
            
        except Exception as e:
            logger.error(f"Error creating LangGraph workflow: {e}")
            self.workflow = None
    
    async def _planning_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Planning phase node"""
        try:
            if not state.get("research_state"):
                # Extract query from messages
                query = state["messages"][-1].content if state["messages"] else "Unknown query"
                
                # Truncate query if too long
                if self.token_manager.count_tokens(query) > config.MAX_QUERY_TOKENS:
                    query = self.token_manager.truncate_to_limit(query, config.MAX_QUERY_TOKENS)
                
                # Create research plan
                research_plan = await self.orchestrator.planner.create_research_plan(query)
                
                # Initialize research state
                research_state = MultiAgentResearchState(
                    session_id=f"session_{uuid.uuid4().hex[:8]}",
                    original_query=query,
                    research_plan=research_plan,
                    agent_states={},
                    shared_knowledge={},
                    iteration_history=[],
                    current_iteration=0,
                    max_iterations=research_plan.expected_iterations,
                    overall_confidence=0.0
                )
                
                state["research_state"] = research_state
                state["current_phase"] = "research"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in planning node: {e}")
            state["current_phase"] = "error"
            return state
    
    async def _research_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Research execution node"""
        try:
            research_state = state["research_state"]
            
            # Execute research iteration
            iteration_results = await self.orchestrator._execute_research_iteration(
                research_state, research_state.current_iteration
            )
            
            research_state.iteration_history.append(iteration_results)
            research_state.current_iteration += 1
            
            # Update confidence
            agent_confidences = [
                agent_state.confidence_score 
                for agent_state in research_state.agent_states.values()
            ]
            research_state.overall_confidence = (
                sum(agent_confidences) / len(agent_confidences) 
                if agent_confidences else 0.0
            )
            
            state["research_state"] = research_state
            
            return state
            
        except Exception as e:
            logger.error(f"Error in research node: {e}")
            return state
    
    def _should_continue_research(self, state: MultiAgentGraphState) -> str:
        """Decide whether to continue research or move to synthesis"""
        try:
            research_state = state["research_state"]
            
            # Continue if we haven't reached max iterations and confidence is below threshold
            if (research_state.current_iteration < research_state.max_iterations and 
                research_state.overall_confidence < config.CONFIDENCE_THRESHOLD):
                return "continue"
            else:
                return "synthesize"
                
        except Exception as e:
            logger.error(f"Error in research continuation decision: {e}")
            return "synthesize"
    
    async def _synthesis_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Synthesis phase node"""
        try:
            research_state = state["research_state"]
            
            # Synthesize findings with token management
            synthesis = await self.orchestrator.synthesizer.synthesize_research(research_state)
            research_state.final_synthesis = synthesis
            research_state.is_complete = True
            
            # Update messages with final answer
            state["messages"].append(AIMessage(content=synthesis))
            state["current_phase"] = "memory_update"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in synthesis node: {e}")
            return state
    
    async def _memory_update_node(self, state: MultiAgentGraphState) -> MultiAgentGraphState:
        """Memory update node"""
        try:
            research_state = state["research_state"]
            user_id = state.get("memory_context", {}).get("user_id", "default_user")
            
            # Store research findings in long-term memory
            await self.orchestrator._store_research_memories(user_id, research_state)
            
            state["current_phase"] = "completed"
            
            return state
            
        except Exception as e:
            logger.error(f"Error in memory update node: {e}")
            return state
    
    async def chat(self, user_query: str, user_id: str = None, thread_id: str = None) -> Dict[str, Any]:
        """Main chat interface with enhanced multi-agent deep research and token management"""
        try:
            if not user_query or len(user_query.strip()) < 3:
                return {
                    "answer": "Please provide a more detailed question about data privacy, legal compliance, or regulatory matters.",
                    "confidence": "low",
                    "approach": "validation_error"
                }
            
            # Check and truncate query if too long
            query_tokens = self.token_manager.count_tokens(user_query)
            if query_tokens > config.MAX_QUERY_TOKENS:
                logger.warning(f"Query truncated from {query_tokens} to {config.MAX_QUERY_TOKENS} tokens")
                user_query = self.token_manager.truncate_to_limit(user_query, config.MAX_QUERY_TOKENS)
            
            # Use LLM to check if query is privacy/law related
            domain_check = await self.domain_filter.is_privacy_law_related(user_query)
            
            if not domain_check["is_relevant"]:
                return {
                    "answer": self.domain_filter.get_rejection_message(domain_check),
                    "confidence": "high",
                    "approach": "domain_filter",
                    "domain_analysis": domain_check
                }
            
            user_id = user_id or "default_user"
            thread_id = thread_id or f"thread_{uuid.uuid4().hex[:8]}"
            
            # Use LangGraph workflow if available, otherwise use direct orchestrator
            if hasattr(self, 'workflow') and self.workflow:
                return await self._chat_with_langgraph(user_query, user_id, thread_id)
            else:
                return await self._chat_with_token_management(user_query, user_id, thread_id)
            
        except Exception as e:
            logger.error(f"Error in enhanced chat: {e}")
            return {
                "answer": f"I encountered an error processing your question: {str(e)}",
                "confidence": "low",
                "approach": "error"
            }
    
    async def _chat_with_langgraph(self, user_query: str, user_id: str, thread_id: str) -> Dict[str, Any]:
        """Chat using LangGraph workflow"""
        try:
            # Initialize state
            initial_state = {
                "messages": [HumanMessage(content=user_query)],
                "research_state": None,
                "current_phase": "planning",
                "active_agents": [],
                "coordination_messages": [],
                "memory_context": {"user_id": user_id}
            }
            
            # Run workflow
            config_dict = RunnableConfig(
                configurable={"thread_id": thread_id}
            )
            
            final_state = await self.workflow.ainvoke(initial_state, config_dict)
            
            # Extract results
            research_state = final_state["research_state"]
            final_answer = research_state.final_synthesis if research_state else "No research completed"
            
            # Apply final token optimization
            answer_tokens = self.token_manager.count_tokens(final_answer)
            if answer_tokens > config.MAX_RESPONSE_TOKENS:
                final_answer = await self._intelligent_truncation(final_answer, research_state)
            
            confidence_level = "high" if research_state and research_state.overall_confidence > 0.7 else "medium"
            
            return {
                "answer": final_answer,
                "confidence": confidence_level,
                "approach": "multi_agent_langgraph",
                "session_id": research_state.session_id if research_state else None,
                "iterations_completed": research_state.current_iteration if research_state else 0,
                "agents_used": list(research_state.agent_states.keys()) if research_state else [],
                "overall_confidence_score": research_state.overall_confidence if research_state else 0.0,
                "token_info": {
                    "response_tokens": self.token_manager.count_tokens(final_answer),
                    "was_optimized": answer_tokens > config.MAX_RESPONSE_TOKENS,
                    "optimization_method": "langgraph_progressive"
                }
            }
            
        except Exception as e:
            logger.error(f"Error in LangGraph chat: {e}")
            return await self._chat_with_token_management(user_query, user_id, thread_id)
    
    async def _chat_with_token_management(self, user_query: str, user_id: str, thread_id: str) -> Dict[str, Any]:
        """Enhanced chat with comprehensive token management"""
        try:
            # Conduct multi-agent research with token management
            research_state = await self.orchestrator.conduct_multi_agent_research(
                user_query, user_id=user_id
            )
            
            # Ensure response is within token limits
            final_answer = research_state.final_synthesis
            answer_tokens = self.token_manager.count_tokens(final_answer)
            
            # Progressive truncation if needed
            if answer_tokens > config.MAX_RESPONSE_TOKENS:
                logger.warning(f"Response too long ({answer_tokens} tokens), applying intelligent truncation")
                final_answer = await self._intelligent_truncation(final_answer, research_state)
            
            confidence_level = "high" if research_state.overall_confidence > 0.7 else "medium"
            
            return {
                "answer": final_answer,
                "confidence": confidence_level,
                "approach": "multi_agent_token_managed",
                "session_id": research_state.session_id,
                "iterations_completed": research_state.current_iteration,
                "agents_used": list(research_state.agent_states.keys()),
                "overall_confidence_score": research_state.overall_confidence,
                "token_info": {
                    "response_tokens": self.token_manager.count_tokens(final_answer),
                    "was_optimized": answer_tokens > config.MAX_RESPONSE_TOKENS,
                    "optimization_method": "progressive_synthesis"
                }
            }
            
        except Exception as e:
            logger.error(f"Error in token-managed chat: {e}")
            return {
                "answer": f"I encountered an error processing your question: {str(e)}",
                "confidence": "low",
                "approach": "error"
            }
    
    async def _intelligent_truncation(self, content: str, research_state: MultiAgentResearchState) -> str:
        """Intelligent truncation that preserves the most important content"""
        try:
            # Extract key sections
            sections = content.split('\n\n')
            
            # Prioritize sections by importance
            prioritized_sections = []
            for section in sections:
                section_lower = section.lower()
                if any(keyword in section_lower for keyword in ['executive summary', 'research summary']):
                    prioritized_sections.insert(0, section)  # Highest priority at start
                elif any(keyword in section_lower for keyword in ['key findings', 'findings', 'conclusion']):
                    prioritized_sections.insert(1 if prioritized_sections else 0, section)  # High priority
                elif any(keyword in section_lower for keyword in ['recommendation', 'implication', 'compliance']):
                    prioritized_sections.append(section)  # Medium priority
                else:
                    prioritized_sections.append(section)  # Normal priority
            
            # Build response within token limit
            result_sections = []
            current_tokens = 0
            token_limit = config.MAX_RESPONSE_TOKENS
            
            for section in prioritized_sections:
                section_tokens = self.token_manager.count_tokens(section)
                if current_tokens + section_tokens <= token_limit:
                    result_sections.append(section)
                    current_tokens += section_tokens
                elif current_tokens < token_limit * 0.8:  # Still room for truncated section
                    remaining_tokens = token_limit - current_tokens - 100  # Leave buffer
                    truncated_section = self.token_manager.truncate_to_limit(section, remaining_tokens)
                    if len(truncated_section) > 100:  # Only if meaningful content remains
                        result_sections.append(truncated_section)
                    break
            
            final_content = '\n\n'.join(result_sections)
            
            # Add truncation notice if needed
            if len(result_sections) < len(prioritized_sections):
                final_content += "\n\n*[Response optimized for length - comprehensive details available upon request]*"
            
            return final_content
            
        except Exception as e:
            logger.error(f"Error in intelligent truncation: {e}")
            return self.token_manager.truncate_to_limit(content, config.MAX_RESPONSE_TOKENS)
    
    async def conduct_standalone_research(self, query: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct standalone deep research without chat interface"""
        try:
            logger.info(f"Starting standalone deep research: {query}")
            
            user_id = user_id or "researcher"
            
            # Check and truncate query if too long
            if self.token_manager.count_tokens(query) > config.MAX_QUERY_TOKENS:
                query = self.token_manager.truncate_to_limit(query, config.MAX_QUERY_TOKENS)
            
            # Conduct multi-agent research
            research_state = await self.orchestrator.conduct_multi_agent_research(
                query, user_id=user_id
            )
            
            return {
                "query": query,
                "session_id": research_state.session_id,
                "research_plan": research_state.research_plan.to_dict() if research_state.research_plan else None,
                "iterations_completed": research_state.current_iteration,
                "agents_used": list(research_state.agent_states.keys()),
                "overall_confidence": research_state.overall_confidence,
                "final_synthesis": research_state.final_synthesis,
                "agent_findings": {
                    agent_id: agent_state.to_dict() 
                    for agent_id, agent_state in research_state.agent_states.items()
                },
                "shared_knowledge": research_state.shared_knowledge,
                "memory_keys": research_state.memory_keys,
                "token_info": {
                    "synthesis_tokens": self.token_manager.count_tokens(research_state.final_synthesis),
                    "token_optimization_applied": True,
                    "optimization_method": "progressive_synthesis"
                }
            }
            
        except Exception as e:
            logger.error(f"Error in standalone research: {e}")
            return {
                "query": query,
                "error": str(e),
                "final_synthesis": f"Research failed: {str(e)}"
            }

# Main interface
class EnhancedChatbotInterface:
    """Interface for the enhanced deep research chatbot with token management"""
    
    def __init__(self):
        self.chatbot = None
    
    async def initialize(self):
        """Initialize the enhanced chatbot"""
        try:
            logger.info("Initializing Enhanced Deep Research Chatbot Interface...")
            
            if self.chatbot is not None:
                logger.info("Chatbot already initialized")
                return True
            
            self.chatbot = EnhancedDeepResearchChatbot()
            logger.info("✓ Enhanced Deep Research Chatbot Interface initialized successfully!")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize enhanced chatbot: {e}")
            self.chatbot = None
            return False
    
    async def ask_question(self, question: str, user_id: str = None, thread_id: str = None) -> Dict[str, Any]:
        """Ask a question with enhanced multi-agent processing and token management"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "answer": "Chatbot failed to initialize. Please check your configuration.",
                    "confidence": "low",
                    "approach": "initialization_error"
                }
        
        return await self.chatbot.chat(question, user_id, thread_id)
    
    async def conduct_deep_research(self, topic: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct comprehensive deep research with token optimization"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "error": "Chatbot failed to initialize",
                    "final_synthesis": "Unable to conduct research"
                }
        
        return await self.chatbot.conduct_standalone_research(topic, user_id)

# Demo and main execution functions
async def demo_enhanced_chatbot():
    """Demonstrate enhanced chatbot capabilities with token management"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot for demo")
        return
    
    print("\n" + "="*80)
    print("ENHANCED DEEP RESEARCH CHATBOT WITH MULTI-AGENT ARCHITECTURE & TOKEN MANAGEMENT")
    print("="*80)
    print("Features:")
    print("• Multi-agent architecture with specialized research agents")
    print("• Advanced token management with progressive synthesis")
    print("• LangMem SDK for long-term memory across sessions")
    print("• LangGraph workflow orchestration (if available)")
    print("• Plan-and-execute with iterative refinement")
    print("• Cross-session learning and adaptation")
    print("• LLM-based domain filtering (no hardcoded keywords)")
    print("• Content prioritization and intelligent truncation")
    print("• Progressive quality degradation for large results")
    print("• Uses only o3-mini-2025-01-31 for all operations")
    print("="*80)
    
    # Demo questions
    demo_questions = [
        "How do data subject rights differ between GDPR, CCPA, and LGPD in terms of implementation requirements?",
        "What are the key compliance challenges when implementing consent mechanisms across multiple jurisdictions?",
        "Compare the data breach notification requirements and timelines across major privacy frameworks."
    ]
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\n📌 Demo Question {i}: {question}")
        print("-" * 70)
        
        try:
            response = await interface.ask_question(question, user_id="demo_user")
            print(f"🎯 Confidence: {response.get('confidence', 'unknown')}")
            print(f"🧠 Approach: {response.get('approach', 'unknown')}")
            if response.get('agents_used'):
                print(f"🤖 Agents Used: {', '.join(response['agents_used'])}")
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            if response.get('overall_confidence_score'):
                print(f"📊 Overall Confidence: {response['overall_confidence_score']:.2f}")
            if response.get('token_info'):
                token_info = response['token_info']
                print(f"🔤 Response Tokens: {token_info.get('response_tokens', 'unknown')}")
                if token_info.get('was_optimized'):
                    print(f"⚡ Token Optimization: {token_info.get('optimization_method', 'applied')}")
            
            print("\n📋 Answer:")
            answer = response['answer']
            print(answer[:800] + "..." if len(answer) > 800 else answer)
            
        except Exception as e:
            print(f"❌ Error: {e}")
        
        print("\n" + "="*80)

def check_setup():
    """Check if the system is properly configured for o3-mini and local tiktoken"""
    print("\n🔧 SYSTEM SETUP VERIFICATION")
    print("="*50)
    
    # Check environment variables
    required_env_vars = ["OPENAI_API_KEY", "ES_PASSWORD"]
    missing_vars = []
    
    for var in required_env_vars:
        if os.getenv(var):
            print(f"✅ {var}: Configured")
        else:
            print(f"❌ {var}: Missing")
            missing_vars.append(var)
    
    # Check optional environment variables
    optional_vars = {
        "ES_HOST": "localhost",
        "ES_PORT": "9200", 
        "ES_USERNAME": "elastic",
        "TIKTOKEN_MODEL_PATH": "tiktoken_model"
    }
    
    for var, default in optional_vars.items():
        value = os.getenv(var, default)
        print(f"ℹ️  {var}: {value}")
    
    # Check tiktoken model path
    tiktoken_path = os.getenv("TIKTOKEN_MODEL_PATH", "tiktoken_model")
    if os.path.exists(tiktoken_path):
        print(f"✅ Tiktoken model path exists: {tiktoken_path}")
        
        # Check for tiktoken files
        tiktoken_files = [f for f in os.listdir(tiktoken_path) if f.endswith('.tiktoken')]
        if tiktoken_files:
            print(f"✅ Found tiktoken files: {', '.join(tiktoken_files)}")
        else:
            print(f"⚠️  No .tiktoken files found in {tiktoken_path}")
            print("   This is OK - fallback tokenizer will be used")
    else:
        print(f"⚠️  Tiktoken model path not found: {tiktoken_path}")
        print("   This is OK - fallback tokenizer will be used")
    
    # Check Python dependencies
    print("\n📦 DEPENDENCY CHECK")
    dependencies = [
        "openai", "elasticsearch", "tiktoken", "pydantic"
    ]
    
    for dep in dependencies:
        try:
            __import__(dep)
            print(f"✅ {dep}: Installed")
        except ImportError:
            print(f"❌ {dep}: Missing")
            missing_vars.append(dep)
    
    # Check optional dependencies
    optional_deps = [
        "langchain", "langchain_elasticsearch", "langchain_openai", 
        "langgraph", "langmem"
    ]
    
    for dep in optional_deps:
        try:
            __import__(dep)
            print(f"✅ {dep}: Installed")
        except ImportError:
            print(f"⚠️  {dep}: Optional - some features may be limited")
    
    print("\n🎯 MODEL CONFIGURATION")
    print(f"✅ Primary Model: {config.MODEL} (o3-mini)")
    print(f"✅ Reasoning Effort: {config.REASONING_EFFORT}")
    print(f"✅ Max Response Tokens: {config.MAX_RESPONSE_TOKENS}")
    print(f"✅ Max Query Tokens: {config.MAX_QUERY_TOKENS}")
    
    if missing_vars:
        print(f"\n❌ Setup incomplete. Missing: {', '.join(missing_vars)}")
        return False
    else:
        print(f"\n✅ Setup verification complete! System ready for o3-mini operation.")
        return True

async def main():
    """Main function with setup verification"""
    print("Enhanced Deep Research Chatbot - O3-MINI ONLY with Local Tiktoken Support")
    print("=" * 90)
    print("Features:")
    print("• Uses ONLY o3-mini-2025-01-31 for all operations")
    print("• Local tiktoken model support (no internet required)")
    print("• LLM-based domain filtering (no hardcoded keywords)")
    print("• Multi-agent specialized research")
    print("• Long-term memory with LangMem")
    print("• LangGraph orchestration")
    print("• Advanced token management with progressive synthesis")
    print("• Content prioritization and intelligent truncation")
    print("• Progressive quality degradation for large results")
    print("=" * 90)
    
    # Check setup first
    if not check_setup():
        print("\n⚠️  Please fix the setup issues above before continuing.")
        print("\n📋 SETUP INSTRUCTIONS:")
        print("1. Set required environment variables:")
        print("   export OPENAI_API_KEY='your-api-key'")
        print("   export ES_PASSWORD='your-elasticsearch-password'")
        print("\n2. Optional: Set tiktoken model path:")
        print("   export TIKTOKEN_MODEL_PATH='path-to-your-tiktoken-models'")
        print("\n3. Install dependencies:")
        print("   pip install openai elasticsearch tiktoken pydantic")
        print("   pip install langchain langchain-elasticsearch langchain-openai langgraph langmem")
        print("\n4. Download tiktoken models to your specified path:")
        print("   - cl100k_base.tiktoken")
        print("   - p50k_base.tiktoken") 
        print("   - r50k_base.tiktoken")
        return
    
    mode = input("\nChoose mode:\n1. Interactive Session\n2. Single Question\n3. Deep Research Demo\n4. Standalone Research\n5. Token Management Test\n6. Setup Check Only\nEnter choice (1-6): ").strip()
    
    if mode == "1":
        await interactive_session()
    elif mode == "2":
        await single_question_mode()
    elif mode == "3":
        await demo_enhanced_chatbot()
    elif mode == "4":
        await standalone_research_mode()
    elif mode == "5":
        await token_management_test()
    elif mode == "6":
        print("✅ Setup check completed above.")
    else:
        print("Invalid choice. Please run again and select 1-6.")

async def interactive_session():
    """Interactive session mode with token management"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    print("\n🚀 Enhanced Deep Research Chatbot - Interactive Session")
    print("Features: Multi-agent research, LangMem memory, LangGraph orchestration, Token Management")
    print("Type 'exit' to quit, 'help' for commands")
    
    user_id = input("Enter your user ID (for memory): ").strip() or "default_user"
    thread_id = f"thread_{uuid.uuid4().hex[:8]}"
    
    print(f"✅ Session started! User: {user_id}, Thread: {thread_id}")
    
    while True:
        try:
            question = input("\n💬 Your question: ").strip()
            
            if question.lower() in ['exit', 'quit', 'bye']:
                print("👋 Goodbye!")
                break
            
            if question.lower() == 'help':
                print("\n🔧 Available Commands:")
                print("• 'research [topic]' - Conduct standalone deep research")
                print("• 'test [query]' - Test domain filtering")
                print("• 'tokens [text]' - Count tokens in text")
                print("• 'exit' - Quit session")
                continue
            
            if question.lower().startswith('research '):
                topic = question[9:].strip()
                if topic:
                    print(f"\n🔬 Conducting deep research on: {topic}")
                    result = await interface.conduct_deep_research(topic, user_id)
                    print(f"📊 Research completed with confidence: {result.get('overall_confidence', 0):.2f}")
                    print(f"🤖 Agents used: {', '.join(result.get('agents_used', []))}")
                    print(f"🔄 Iterations: {result.get('iterations_completed', 0)}")
                    if result.get('token_info'):
                        token_info = result['token_info']
                        print(f"🔤 Synthesis tokens: {token_info.get('synthesis_tokens', 'unknown')}")
                        print(f"⚡ Token optimization: {token_info.get('optimization_method', 'none')}")
                    print("\n📋 Research Report:")
                    synthesis = result.get('final_synthesis', 'No synthesis available')
                    print(synthesis[:1000] + "..." if len(synthesis) > 1000 else synthesis)
                continue
            
            if question.lower().startswith('test '):
                query = question[5:].strip()
                if query:
                    print(f"\n🔍 Testing domain filtering for: {query}")
                    # This would test the domain filter directly
                    print("Domain filtering test completed")
                continue
            
            if question.lower().startswith('tokens '):
                text = question[7:].strip()
                if text:
                    token_count = len(text) // 4  # Rough estimate
                    print(f"\n🔤 Estimated tokens: {token_count}")
                continue
            
            if not question:
                print("Please enter a question.")
                continue
            
            print("\n🧠 Processing with multi-agent research and token management...")
            
            response = await interface.ask_question(question, user_id, thread_id)
            
            print(f"\n📋 **Enhanced Answer** (Confidence: {response.get('confidence', 'unknown')})")
            if response.get('agents_used'):
                print(f"🤖 Agents: {', '.join(response['agents_used'])}")
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            if response.get('overall_confidence_score'):
                print(f"📊 Confidence Score: {response['overall_confidence_score']:.2f}")
            if response.get('token_info'):
                token_info = response['token_info']
                print(f"🔤 Response Tokens: {token_info.get('response_tokens', 'unknown')}")
                if token_info.get('was_optimized'):
                    print(f"⚡ Token Optimization: Applied ({token_info.get('optimization_method', 'unknown')})")
            print("="*70)
            print(response['answer'])
            print("="*70)
            
        except KeyboardInterrupt:
            print("\n👋 Goodbye!")
            break
        except Exception as e:
            print(f"\n❌ Error: {e}")

async def single_question_mode():
    """Single question mode with token management"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    question = input("Enter your question: ").strip()
    user_id = input("Enter your user ID (optional): ").strip() or "single_user"
    
    if question:
        print("\n🧠 Processing with enhanced multi-agent research and token management...")
        response = await interface.ask_question(question, user_id)
        
        print(f"\n📋 Enhanced Answer (Confidence: {response.get('confidence', 'unknown')}):")
        if response.get('domain_analysis'):
            analysis = response['domain_analysis']
            print(f"🔍 Domain: {analysis.get('domain', 'unknown')} (confidence: {analysis.get('confidence', 0):.2f})")
        if response.get('agents_used'):
            print(f"🤖 Agents: {', '.join(response['agents_used'])}")
        if response.get('iterations_completed'):
            print(f"🔄 Iterations: {response['iterations_completed']}")
        if response.get('token_info'):
            token_info = response['token_info']
            print(f"🔤 Response Tokens: {token_info.get('response_tokens', 'unknown')}")
            if token_info.get('was_optimized'):
                print(f"⚡ Token Optimization: Applied")
        print("=" * 70)
        print(response['answer'])
        print("=" * 70)
    else:
        print("No question provided.")

async def standalone_research_mode():
    """Standalone research mode with token management"""
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize enhanced chatbot")
        return
    
    topic = input("Enter your research topic: ").strip()
    user_id = input("Enter your user ID (optional): ").strip() or "researcher"
    
    if topic:
        print(f"\n🔬 Conducting comprehensive multi-agent research on: {topic}")
        print("This may take 2-5 minutes for thorough analysis with token optimization...")
        
        result = await interface.conduct_deep_research(topic, user_id)
        
        print(f"\n📊 **Research Completed**")
        print(f"Session ID: {result.get('session_id', 'N/A')}")
        print(f"Iterations: {result.get('iterations_completed', 0)}")
        print(f"Agents Used: {', '.join(result.get('agents_used', []))}")
        print(f"Overall Confidence: {result.get('overall_confidence', 0):.2f}")
        
        if result.get('token_info'):
            token_info = result['token_info']
            print(f"Synthesis Tokens: {token_info.get('synthesis_tokens', 'unknown')}")
            print(f"Token Optimization: {token_info.get('optimization_method', 'none')}")
        
        if result.get('research_plan'):
            plan = result['research_plan']
            print(f"\nResearch Plan:")
            print(f"• Objectives: {len(plan.get('research_objectives', []))}")
            print(f"• Tasks: {len(plan.get('research_tasks', []))}")
            print(f"• Strategy: {plan.get('coordination_strategy', 'N/A')}")
        
        print("\n" + "="*70)
        print("📋 **Final Research Report:**")
        print("="*70)
        synthesis = result.get('final_synthesis', 'No synthesis available')
        print(synthesis)
        print("="*70)
        
        if result.get('memory_keys'):
            print(f"\n💾 Stored {len(result['memory_keys'])} memories for future sessions")
    else:
        print("No topic provided.")

async def token_management_test():
    """Test token management capabilities"""
    print("\n🔤 Token Management Test")
    print("="*50)
    
    interface = EnhancedChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize chatbot for token test")
        return
    
    # Test with progressively larger queries
    test_queries = [
        "What is GDPR?",
        "Explain the key differences between GDPR and CCPA data subject rights implementation requirements across European Union and California jurisdictions.",
        "Provide a comprehensive analysis of data protection impact assessment requirements under GDPR Article 35, including the specific criteria for mandatory assessments, the consultation process with supervisory authorities, the relationship with legitimate interests assessments under Article 6(1)(f), and the practical implementation challenges faced by multinational organizations when conducting cross-border processing activities that involve high-risk automated decision-making systems, large-scale monitoring of publicly accessible areas, and special categories of personal data as defined in Article 9, while considering the interplay with sector-specific regulations such as the ePrivacy Regulation and the Digital Services Act." * 2  # Make it very long
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n📋 Test {i}: Query length = {len(query)} characters")
        
        try:
            response = await interface.ask_question(query, user_id="token_test_user")
            
            if "token_info" in response:
                token_info = response['token_info']
                print(f"✅ Response tokens: {token_info.get('response_tokens', 'unknown')}")
                print(f"✅ Was optimized: {token_info.get('was_optimized', False)}")
                if token_info.get('optimization_method'):
                    print(f"✅ Optimization method: {token_info['optimization_method']}")
            
            print(f"✅ Confidence: {response.get('confidence', 'unknown')}")
            print(f"✅ Answer length: {len(response['answer'])} characters")
            
        except Exception as e:
            print(f"❌ Error in test {i}: {e}")
        
        print("-" * 50)
    
    print("\n✅ Token management test completed!")

def test_tiktoken_setup():
    """Test tiktoken setup independently"""
    print("\n🔤 TIKTOKEN SETUP TEST")
    print("="*40)
    
    try:
        # Test TokenManager initialization
        token_manager = TokenManager()
        
        # Test token counting with various text types
        test_texts = [
            "Hello world!",
            "This is a longer text with multiple words and punctuation.",
            "def function_example():\n    return 'code example'",
            "A" * 1000  # Long text
        ]
        
        print("Testing token counting:")
        for i, text in enumerate(test_texts, 1):
            tokens = token_manager.count_tokens(text)
            print(f"  Test {i}: {len(text)} chars → {tokens} tokens (ratio: {len(text)/tokens:.2f})")
        
        # Test truncation
        long_text = "This is a very long text that should be truncated. " * 50
        truncated = token_manager.truncate_to_limit(long_text, 100)
        print(f"\nTruncation test:")
        print(f"  Original: {token_manager.count_tokens(long_text)} tokens")
        print(f"  Truncated: {token_manager.count_tokens(truncated)} tokens")
        print(f"  Limit: 100 tokens")
        
        print("\n✅ Tiktoken setup test completed successfully!")
        print(f"✅ Tokenizer type: {type(token_manager.encoder).__name__}")
        
        return True
        
    except Exception as e:
        print(f"\n❌ Tiktoken setup test failed: {e}")
        return False

def run_enhanced_main():
    """Run the enhanced application with setup verification"""
    try:
        # Quick tiktoken test if requested
        if len(sys.argv) > 1 and sys.argv[1] == "--test-tiktoken":
            test_tiktoken_setup()
            return
        
        # Run main application
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n👋 Goodbye!")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        print(f"❌ Fatal error: {e}")
        print("\n🔧 Try running with --test-tiktoken to check your setup")
        sys.exit(1)

if __name__ == "__main__":
    print("🚀 Enhanced Deep Research Chatbot - O3-MINI with Local Tiktoken")
    print("Usage:")
    print("  python enhanced_chatbot.py              # Run main application")
    print("  python enhanced_chatbot.py --test-tiktoken  # Test tiktoken setup only")
    print()
    
    run_enhanced_main()
