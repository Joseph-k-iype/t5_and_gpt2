"""
Advanced Schema Mapping System with Graph RAG and Sophisticated Reasoning
WITH COMPLETE DEDUPLICATION - Cleans existing duplicates AND prevents new ones
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any, TypedDict, Annotated
from operator import add

from falkordb import FalkorDB
from openai import OpenAI

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
FALKORDB_HOST = os.getenv("FALKORDB_HOST", "localhost")
FALKORDB_PORT = int(os.getenv("FALKORDB_PORT", 6379))

# Initialize OpenAI client
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# Configure LangChain LLMs
llm = ChatOpenAI(
    model="gpt-4o",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

reasoning_llm = ChatOpenAI(
    model="o1",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# ============================================================================
# EMBEDDING UTILITIES
# ============================================================================

def create_embedding(text: str) -> List[float]:
    """Create embedding using OpenAI text-embedding-3-large model"""
    response = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return response.data[0].embedding


def batch_create_embeddings(texts: List[str]) -> List[List[float]]:
    """Create embeddings for multiple texts in batch"""
    response = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=texts
    )
    return [item.embedding for item in response.data]


def escape_string(s: Any) -> str:
    """Escape string for Cypher query"""
    if s is None or pd.isna(s):
        return ''
    return str(s).replace("'", "\\'").replace('"', '\\"')


# ============================================================================
# GRAPH DEDUPLICATION UTILITY
# ============================================================================

class GraphDeduplicator:
    """Deduplicate existing nodes and relationships in the graph"""
    
    def __init__(self, graph):
        self.graph = graph
        self.dedup_stats = {
            'duplicate_tables_found': 0,
            'tables_merged': 0,
            'duplicate_columns_found': 0,
            'columns_merged': 0,
            'duplicate_relationships_found': 0,
            'relationships_merged': 0
        }
    
    def deduplicate_graph(self):
        """Run complete deduplication on the graph"""
        print("\n" + "="*60)
        print("DEDUPLICATING EXISTING GRAPH")
        print("="*60)
        
        print("\n1. Finding and merging duplicate tables...")
        self._deduplicate_tables()
        
        print("\n2. Finding and merging duplicate columns...")
        self._deduplicate_columns()
        
        print("\n3. Finding and merging duplicate relationships...")
        self._deduplicate_relationships()
        
        print("\n" + "="*60)
        print("DEDUPLICATION STATISTICS")
        print("="*60)
        print(f"Duplicate Tables Found: {self.dedup_stats['duplicate_tables_found']}")
        print(f"Tables Merged: {self.dedup_stats['tables_merged']}")
        print(f"Duplicate Columns Found: {self.dedup_stats['duplicate_columns_found']}")
        print(f"Columns Merged: {self.dedup_stats['columns_merged']}")
        print(f"Duplicate Relationships Found: {self.dedup_stats['duplicate_relationships_found']}")
        print(f"Relationships Merged: {self.dedup_stats['relationships_merged']}")
        print("="*60)
    
    def _deduplicate_tables(self):
        """Find and merge duplicate table nodes"""
        # Find tables with same table_id
        query = """
            MATCH (t:Table)
            WITH t.table_id AS table_id, collect(t) AS tables, count(t) AS cnt
            WHERE cnt > 1
            RETURN table_id, tables, cnt
        """
        result = self.graph.query(query)
        
        if not result.result_set:
            print("   ✓ No duplicate tables found")
            return
        
        for record in result.result_set:
            table_id = record[0]
            duplicate_count = record[2]
            
            print(f"   ⚠ Found {duplicate_count} duplicates for table_id: {table_id}")
            self.dedup_stats['duplicate_tables_found'] += duplicate_count - 1
            
            # Merge duplicates - keep the one with most complete data
            table_id_esc = escape_string(table_id)
            
            # Get all duplicate nodes with their properties
            get_dupes = f"""
                MATCH (t:Table {{table_id: '{table_id_esc}'}})
                RETURN id(t) AS node_id, 
                       t.table_name AS name,
                       t.data_model AS model,
                       t.name_embedding AS embedding
                ORDER BY 
                    CASE WHEN t.name_embedding IS NOT NULL THEN 1 ELSE 0 END DESC,
                    CASE WHEN t.data_model IS NOT NULL THEN 1 ELSE 0 END DESC
            """
            dupes = self.graph.query(get_dupes)
            
            if dupes.result_set and len(dupes.result_set) > 1:
                # Keep the first (most complete) node
                keeper_id = dupes.result_set[0][0]
                keeper_name = dupes.result_set[0][1]
                keeper_model = dupes.result_set[0][2]
                keeper_embedding = dupes.result_set[0][3]
                
                # Merge properties from duplicates into keeper
                for i in range(1, len(dupes.result_set)):
                    dup_id = dupes.result_set[i][0]
                    dup_name = dupes.result_set[i][1]
                    dup_model = dupes.result_set[i][2]
                    dup_embedding = dupes.result_set[i][3]
                    
                    # Update keeper with any missing properties
                    update_keeper = f"""
                        MATCH (keeper:Table)
                        WHERE id(keeper) = {keeper_id}
                        SET keeper.table_name = COALESCE(keeper.table_name, '{escape_string(dup_name)}'),
                            keeper.data_model = COALESCE(keeper.data_model, '{escape_string(dup_model)}')
                    """
                    if keeper_embedding is None and dup_embedding is not None:
                        update_keeper += f", keeper.name_embedding = vecf32({dup_embedding})"
                    
                    self.graph.query(update_keeper)
                    
                    # Transfer all relationships from duplicate to keeper
                    transfer_rels = f"""
                        MATCH (dup:Table)-[r:HAS_COLUMN]->(c:Column)
                        WHERE id(dup) = {dup_id}
                        MATCH (keeper:Table)
                        WHERE id(keeper) = {keeper_id}
                        MERGE (keeper)-[:HAS_COLUMN]->(c)
                        DELETE r
                    """
                    self.graph.query(transfer_rels)
                    
                    # Transfer outgoing relationships
                    transfer_out = f"""
                        MATCH (dup:Table)-[r:RELATES_TO]->(other:Table)
                        WHERE id(dup) = {dup_id}
                        MATCH (keeper:Table)
                        WHERE id(keeper) = {keeper_id}
                        WITH keeper, other, r
                        MERGE (keeper)-[new_r:RELATES_TO]->(other)
                        ON CREATE SET new_r = properties(r)
                        DELETE r
                    """
                    self.graph.query(transfer_out)
                    
                    # Transfer incoming relationships
                    transfer_in = f"""
                        MATCH (other:Table)-[r:RELATES_TO]->(dup:Table)
                        WHERE id(dup) = {dup_id}
                        MATCH (keeper:Table)
                        WHERE id(keeper) = {keeper_id}
                        WITH keeper, other, r
                        MERGE (other)-[new_r:RELATES_TO]->(keeper)
                        ON CREATE SET new_r = properties(r)
                        DELETE r
                    """
                    self.graph.query(transfer_in)
                    
                    # Delete the duplicate node
                    delete_dup = f"""
                        MATCH (dup:Table)
                        WHERE id(dup) = {dup_id}
                        DELETE dup
                    """
                    self.graph.query(delete_dup)
                    
                    self.dedup_stats['tables_merged'] += 1
                    print(f"      ✓ Merged duplicate table node (id: {dup_id}) into keeper (id: {keeper_id})")
    
    def _deduplicate_columns(self):
        """Find and merge duplicate column nodes"""
        # Find columns with same table_id + column_name
        query = """
            MATCH (t:Table)-[:HAS_COLUMN]->(c:Column)
            WITH t.table_id AS table_id, c.column_name AS column_name, 
                 collect(c) AS columns, count(c) AS cnt
            WHERE cnt > 1
            RETURN table_id, column_name, cnt
        """
        result = self.graph.query(query)
        
        if not result.result_set:
            print("   ✓ No duplicate columns found")
            return
        
        for record in result.result_set:
            table_id = record[0]
            column_name = record[1]
            duplicate_count = record[2]
            
            print(f"   ⚠ Found {duplicate_count} duplicates for column: {table_id}.{column_name}")
            self.dedup_stats['duplicate_columns_found'] += duplicate_count - 1
            
            table_id_esc = escape_string(table_id)
            column_name_esc = escape_string(column_name)
            
            # Get all duplicate column nodes
            get_dupes = f"""
                MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column {{column_name: '{column_name_esc}'}})
                RETURN id(c) AS node_id,
                       c.column_id AS col_id,
                       c.type AS type,
                       c.primary_key AS pk,
                       c.foreign_key AS fk,
                       c.name_embedding AS embedding
                ORDER BY 
                    CASE WHEN c.name_embedding IS NOT NULL THEN 1 ELSE 0 END DESC,
                    CASE WHEN c.type IS NOT NULL AND c.type <> '' THEN 1 ELSE 0 END DESC
            """
            dupes = self.graph.query(get_dupes)
            
            if dupes.result_set and len(dupes.result_set) > 1:
                # Keep the first (most complete) node
                keeper_id = dupes.result_set[0][0]
                
                # Collect all properties from duplicates
                all_props = {
                    'column_id': dupes.result_set[0][1],
                    'type': dupes.result_set[0][2],
                    'primary_key': dupes.result_set[0][3],
                    'foreign_key': dupes.result_set[0][4],
                    'embedding': dupes.result_set[0][5]
                }
                
                # Merge properties from duplicates
                for i in range(1, len(dupes.result_set)):
                    dup_id = dupes.result_set[i][0]
                    
                    # Collect non-null properties from duplicate
                    if all_props['column_id'] is None or all_props['column_id'] == '':
                        all_props['column_id'] = dupes.result_set[i][1]
                    if all_props['type'] is None or all_props['type'] == '':
                        all_props['type'] = dupes.result_set[i][2]
                    if all_props['primary_key'] is None:
                        all_props['primary_key'] = dupes.result_set[i][3]
                    if all_props['foreign_key'] is None:
                        all_props['foreign_key'] = dupes.result_set[i][4]
                    if all_props['embedding'] is None:
                        all_props['embedding'] = dupes.result_set[i][5]
                    
                    # Delete duplicate
                    delete_dup = f"""
                        MATCH (c:Column)
                        WHERE id(c) = {dup_id}
                        DETACH DELETE c
                    """
                    self.graph.query(delete_dup)
                    
                    self.dedup_stats['columns_merged'] += 1
                    print(f"      ✓ Merged duplicate column (id: {dup_id}) into keeper (id: {keeper_id})")
                
                # Update keeper with all collected properties
                update_query = f"""
                    MATCH (c:Column)
                    WHERE id(c) = {keeper_id}
                    SET c.column_id = '{escape_string(all_props['column_id'])}',
                        c.type = '{escape_string(all_props['type'])}',
                        c.primary_key = '{escape_string(all_props['primary_key'])}',
                        c.foreign_key = '{escape_string(all_props['foreign_key'])}'
                """
                if all_props['embedding'] is not None:
                    update_query += f", c.name_embedding = vecf32({all_props['embedding']})"
                
                self.graph.query(update_query)
    
    def _deduplicate_relationships(self):
        """Find and merge duplicate relationships"""
        query = """
            MATCH (t1:Table)-[r:RELATES_TO]->(t2:Table)
            WITH t1.table_id AS from_id, t2.table_id AS to_id, 
                 collect(r) AS rels, count(r) AS cnt
            WHERE cnt > 1
            RETURN from_id, to_id, cnt
        """
        result = self.graph.query(query)
        
        if not result.result_set:
            print("   ✓ No duplicate relationships found")
            return
        
        for record in result.result_set:
            from_id = record[0]
            to_id = record[1]
            duplicate_count = record[2]
            
            print(f"   ⚠ Found {duplicate_count} duplicate relationships: {from_id} -> {to_id}")
            self.dedup_stats['duplicate_relationships_found'] += duplicate_count - 1
            
            from_id_esc = escape_string(from_id)
            to_id_esc = escape_string(to_id)
            
            # Get all duplicate relationships
            get_dupes = f"""
                MATCH (t1:Table {{table_id: '{from_id_esc}'}})-[r:RELATES_TO]->(t2:Table {{table_id: '{to_id_esc}'}})
                RETURN id(r) AS rel_id,
                       r.relationship_id AS relationship_id,
                       r.from_multiplicity AS from_mult,
                       r.to_multiplicity AS to_mult,
                       r.identifying AS identifying
                ORDER BY 
                    CASE WHEN r.relationship_id IS NOT NULL THEN 1 ELSE 0 END DESC
            """
            dupes = self.graph.query(get_dupes)
            
            if dupes.result_set and len(dupes.result_set) > 1:
                # Keep first relationship
                keeper_id = dupes.result_set[0][0]
                
                # Collect all properties
                all_props = {
                    'relationship_id': dupes.result_set[0][1],
                    'from_multiplicity': dupes.result_set[0][2],
                    'to_multiplicity': dupes.result_set[0][3],
                    'identifying': dupes.result_set[0][4]
                }
                
                # Delete duplicates
                for i in range(1, len(dupes.result_set)):
                    dup_id = dupes.result_set[i][0]
                    
                    # Collect non-null properties
                    if all_props['relationship_id'] is None:
                        all_props['relationship_id'] = dupes.result_set[i][1]
                    if all_props['from_multiplicity'] is None:
                        all_props['from_multiplicity'] = dupes.result_set[i][2]
                    if all_props['to_multiplicity'] is None:
                        all_props['to_multiplicity'] = dupes.result_set[i][3]
                    if all_props['identifying'] is None:
                        all_props['identifying'] = dupes.result_set[i][4]
                    
                    # Delete duplicate relationship
                    delete_dup = f"""
                        MATCH ()-[r:RELATES_TO]->()
                        WHERE id(r) = {dup_id}
                        DELETE r
                    """
                    self.graph.query(delete_dup)
                    
                    self.dedup_stats['relationships_merged'] += 1
                    print(f"      ✓ Deleted duplicate relationship (id: {dup_id})")
                
                # Update keeper with collected properties
                update_query = f"""
                    MATCH ()-[r:RELATES_TO]->()
                    WHERE id(r) = {keeper_id}
                    SET r.relationship_id = '{escape_string(all_props['relationship_id'])}',
                        r.from_multiplicity = '{escape_string(all_props['from_multiplicity'])}',
                        r.to_multiplicity = '{escape_string(all_props['to_multiplicity'])}',
                        r.identifying = '{escape_string(all_props['identifying'])}'
                """
                self.graph.query(update_query)


# ============================================================================
# FALKORDB SCHEMA LOADER WITH SMART DEDUPLICATION
# ============================================================================

class SchemaLoader:
    """Load schema into FalkorDB with smart deduplication"""
    
    def __init__(self, host: str = FALKORDB_HOST, port: int = FALKORDB_PORT):
        self.db = FalkorDB(host=host, port=port)
        self.graph = self.db.select_graph('schema_graph')
        self.deduplicator = GraphDeduplicator(self.graph)
        self.stats = {
            'tables_created': 0,
            'tables_updated': 0,
            'tables_skipped': 0,
            'columns_created': 0,
            'columns_updated': 0,
            'columns_skipped': 0,
            'relationships_created': 0,
            'relationships_skipped': 0
        }
        
    def load_schema(self, attributes_csv: str, relationships_csv: str, deduplicate_existing: bool = True):
        """Load schema from CSV files into FalkorDB with deduplication"""
        
        # STEP 1: Deduplicate existing graph first
        if deduplicate_existing:
            self.deduplicator.deduplicate_graph()
        
        # STEP 2: Load new data
        print("\n" + "="*60)
        print("LOADING SCHEMA WITH DEDUPLICATION")
        print("="*60)
        
        print("\nLoading schema attributes...")
        df_attrs = pd.read_csv(attributes_csv)
        
        print("Loading schema relationships...")
        df_rels = pd.read_csv(relationships_csv)
        
        print("\nCreating vector indices...")
        self._create_vector_indices()
        
        print("\nLoading tables and columns...")
        self._load_tables_and_columns(df_attrs)
        
        print("\nLoading relationships...")
        self._load_relationships(df_rels)
        
        print("\n" + "="*60)
        print("SCHEMA LOADING STATISTICS")
        print("="*60)
        print(f"Tables - Created: {self.stats['tables_created']}, "
              f"Updated: {self.stats['tables_updated']}, "
              f"Skipped: {self.stats['tables_skipped']}")
        print(f"Columns - Created: {self.stats['columns_created']}, "
              f"Updated: {self.stats['columns_updated']}, "
              f"Skipped: {self.stats['columns_skipped']}")
        print(f"Relationships - Created: {self.stats['relationships_created']}, "
              f"Skipped: {self.stats['relationships_skipped']}")
        print("="*60)
        print("Schema loaded successfully!")
        
    def _create_vector_indices(self):
        """Create vector indices for semantic search"""
        try:
            self.graph.query("""
                CREATE VECTOR INDEX FOR (t:Table) ON (t.name_embedding) 
                OPTIONS {dimension:3072, similarityFunction:'cosine'}
            """)
            print("✓ Created vector index for Table nodes")
        except Exception:
            print("✓ Table vector index already exists")
        
        try:
            self.graph.query("""
                CREATE VECTOR INDEX FOR (c:Column) ON (c.name_embedding) 
                OPTIONS {dimension:3072, similarityFunction:'cosine'}
            """)
            print("✓ Created vector index for Column nodes")
        except Exception:
            print("✓ Column vector index already exists")
    
    def _check_table_exists(self, table_id: str, table_name: str, embedding: List[float]) -> str:
        """Check if table exists and return status: 'skip', 'update', or 'create'"""
        table_id_esc = escape_string(table_id)
        
        query = f"""
            MATCH (t:Table {{table_id: '{table_id_esc}'}})
            RETURN t.table_name AS name, 
                   t.name_embedding AS embedding,
                   t.data_model AS data_model
        """
        result = self.graph.query(query)
        
        if not result.result_set:
            return 'create'
        
        existing_embedding = result.result_set[0][1]
        
        if existing_embedding is not None:
            if len(existing_embedding) == len(embedding):
                return 'skip'
        
        return 'update'
    
    def _check_column_exists(self, table_id: str, column_name: str, embedding: List[float]) -> str:
        """Check if column exists and return status: 'skip', 'update', or 'create'"""
        table_id_esc = escape_string(table_id)
        column_name_esc = escape_string(column_name)
        
        query = f"""
            MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column {{column_name: '{column_name_esc}'}})
            RETURN c.column_id AS col_id,
                   c.type AS type,
                   c.name_embedding AS embedding
        """
        result = self.graph.query(query)
        
        if not result.result_set:
            return 'create'
        
        existing_data = result.result_set[0]
        existing_embedding = existing_data[2]
        
        if existing_embedding is not None and existing_data[0] and existing_data[1]:
            return 'skip'
        
        return 'update'
    
    def _load_tables_and_columns(self, df_attrs: pd.DataFrame):
        """Load tables and columns with smart deduplication"""
        tables = df_attrs.groupby(['table_id', 'table_name', 'data_model'])
        
        for (table_id, table_name, data_model), columns in tables:
            table_id_esc = escape_string(table_id)
            table_name_esc = escape_string(table_name)
            data_model_esc = escape_string(data_model)
            
            table_embedding = create_embedding(str(table_name))
            table_status = self._check_table_exists(table_id, table_name, table_embedding)
            
            if table_status == 'skip':
                print(f"⊘ Skipping table {table_name}")
                self.stats['tables_skipped'] += 1
            elif table_status == 'create':
                print(f"+ Creating table {table_name}")
                query = f"""
                    CREATE (t:Table {{
                        table_id: '{table_id_esc}',
                        table_name: '{table_name_esc}',
                        data_model: '{data_model_esc}',
                        name_embedding: vecf32({table_embedding})
                    }})
                """
                self.graph.query(query)
                self.stats['tables_created'] += 1
            else:
                print(f"↻ Updating table {table_name}")
                query = f"""
                    MATCH (t:Table {{table_id: '{table_id_esc}'}})
                    SET t.table_name = COALESCE(t.table_name, '{table_name_esc}'),
                        t.data_model = COALESCE(t.data_model, '{data_model_esc}'),
                        t.name_embedding = CASE 
                            WHEN t.name_embedding IS NULL 
                            THEN vecf32({table_embedding})
                            ELSE t.name_embedding
                        END
                """
                self.graph.query(query)
                self.stats['tables_updated'] += 1
            
            # Process columns
            for _, col in columns.iterrows():
                col_name = str(col['column_name'])
                col_embedding = create_embedding(col_name)
                col_status = self._check_column_exists(table_id, col_name, col_embedding)
                
                col_id_esc = escape_string(col['column_id'])
                col_name_esc = escape_string(col_name)
                type_esc = escape_string(col.get('type', ''))
                length_esc = escape_string(col.get('length', ''))
                scale_esc = escape_string(col.get('scale', ''))
                nullable_esc = escape_string(col.get('nullable', ''))
                pk_esc = escape_string(col.get('primary_key', ''))
                unique_esc = escape_string(col.get('unique', ''))
                fk_esc = escape_string(col.get('foreign_key', ''))
                
                if col_status == 'skip':
                    self.stats['columns_skipped'] += 1
                elif col_status == 'create':
                    query = f"""
                        MATCH (t:Table {{table_id: '{table_id_esc}'}})
                        CREATE (c:Column {{
                            column_id: '{col_id_esc}',
                            column_name: '{col_name_esc}',
                            type: '{type_esc}',
                            length: '{length_esc}',
                            scale: '{scale_esc}',
                            nullable: '{nullable_esc}',
                            primary_key: '{pk_esc}',
                            unique: '{unique_esc}',
                            foreign_key: '{fk_esc}',
                            name_embedding: vecf32({col_embedding})
                        }})
                        MERGE (t)-[:HAS_COLUMN]->(c)
                    """
                    self.graph.query(query)
                    self.stats['columns_created'] += 1
                else:
                    query = f"""
                        MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column {{column_name: '{col_name_esc}'}})
                        SET c.column_id = COALESCE(c.column_id, '{col_id_esc}'),
                            c.type = CASE WHEN c.type IS NULL OR c.type = '' THEN '{type_esc}' ELSE c.type END,
                            c.length = CASE WHEN c.length IS NULL OR c.length = '' THEN '{length_esc}' ELSE c.length END,
                            c.scale = CASE WHEN c.scale IS NULL OR c.scale = '' THEN '{scale_esc}' ELSE c.scale END,
                            c.nullable = COALESCE(c.nullable, '{nullable_esc}'),
                            c.primary_key = COALESCE(c.primary_key, '{pk_esc}'),
                            c.unique = COALESCE(c.unique, '{unique_esc}'),
                            c.foreign_key = COALESCE(c.foreign_key, '{fk_esc}'),
                            c.name_embedding = CASE 
                                WHEN c.name_embedding IS NULL 
                                THEN vecf32({col_embedding})
                                ELSE c.name_embedding
                            END
                    """
                    self.graph.query(query)
                    self.stats['columns_updated'] += 1
    
    def _load_relationships(self, df_rels: pd.DataFrame):
        """Load relationships with deduplication"""
        for _, rel in df_rels.iterrows():
            rel_id_esc = escape_string(rel['relationship_id'])
            from_table_esc = escape_string(rel['from_table_id'])
            to_table_esc = escape_string(rel['to_table_id'])
            from_mult_esc = escape_string(rel.get('from_multiplicity', ''))
            to_mult_esc = escape_string(rel.get('to_multiplicity', ''))
            ident_esc = escape_string(rel.get('identifying', ''))
            
            check_query = f"""
                MATCH (t1:Table {{table_id: '{from_table_esc}'}})-[r:RELATES_TO]->(t2:Table {{table_id: '{to_table_esc}'}})
                WHERE r.relationship_id = '{rel_id_esc}'
                RETURN count(r) AS rel_count
            """
            result = self.graph.query(check_query)
            
            if result.result_set and result.result_set[0][0] > 0:
                self.stats['relationships_skipped'] += 1
                continue
            
            query = f"""
                MATCH (t1:Table {{table_id: '{from_table_esc}'}}),
                      (t2:Table {{table_id: '{to_table_esc}'}})
                MERGE (t1)-[r:RELATES_TO {{
                    relationship_id: '{rel_id_esc}'
                }}]->(t2)
                ON CREATE SET 
                    r.from_multiplicity = '{from_mult_esc}',
                    r.to_multiplicity = '{to_mult_esc}',
                    r.identifying = '{ident_esc}'
                ON MATCH SET
                    r.from_multiplicity = COALESCE(r.from_multiplicity, '{from_mult_esc}'),
                    r.to_multiplicity = COALESCE(r.to_multiplicity, '{to_mult_esc}'),
                    r.identifying = COALESCE(r.identifying, '{ident_esc}')
            """
            self.graph.query(query)
            self.stats['relationships_created'] += 1


# ============================================================================
# REST OF THE CODE (EnhancedGraphRAGRetriever, Agents, etc.) UNCHANGED
# ... [Include all the remaining code from the previous version]
# ============================================================================

# [COPY ALL THE CODE FROM EnhancedGraphRAGRetriever through AdvancedSchemaMappingPipeline from previous version]

class EnhancedGraphRAGRetriever:
    """Retrieve schema with full context: relationships, lineage, transformations"""
    
    def __init__(self, host: str = FALKORDB_HOST, port: int = FALKORDB_PORT):
        self.db = FalkorDB(host=host, port=port)
        self.graph = self.db.select_graph('schema_graph')
    
    def semantic_search_with_context(self, query_text: str, top_k: int = 10) -> List[Dict]:
        """Search for columns with full contextual information"""
        query_embedding = create_embedding(query_text)
        
        query = f"""
            CALL db.idx.vector.queryNodes('Column', 'name_embedding', {top_k}, vecf32({query_embedding}))
            YIELD node, score
            MATCH (t:Table)-[:HAS_COLUMN]->(node)
            RETURN 
                node.column_id AS column_id,
                node.column_name AS column_name,
                node.type AS type,
                node.primary_key AS primary_key,
                node.foreign_key AS foreign_key,
                node.nullable AS nullable,
                t.table_name AS table_name,
                t.table_id AS table_id,
                t.data_model AS data_model,
                score
            ORDER BY score DESC
        """
        result = self.graph.query(query)
        
        columns = []
        for record in result.result_set:
            column_info = {
                'column_id': record[0],
                'column_name': record[1],
                'type': record[2],
                'primary_key': record[3],
                'foreign_key': record[4],
                'nullable': record[5],
                'table_name': record[6],
                'table_id': record[7],
                'data_model': record[8],
                'similarity_score': record[9]
            }
            
            column_info['relationships'] = self.get_table_relationships(record[7])
            column_info['related_columns'] = self.get_related_columns_in_table(record[7])
            column_info['lineage'] = self.get_column_lineage(record[7], record[1])
            column_info['table_context'] = self.get_table_context(record[7])
            
            columns.append(column_info)
        
        return columns
    
    def get_table_relationships(self, table_id: str) -> Dict[str, List[Dict]]:
        """Get both incoming and outgoing relationships for a table"""
        table_id_esc = escape_string(table_id)
        
        query_out = f"""
            MATCH (t1:Table {{table_id: '{table_id_esc}'}})-[r:RELATES_TO]->(t2:Table)
            RETURN 
                t2.table_name AS related_table_name,
                r.from_multiplicity AS from_multiplicity,
                r.to_multiplicity AS to_multiplicity
        """
        outgoing = self.graph.query(query_out)
        
        query_in = f"""
            MATCH (t1:Table)-[r:RELATES_TO]->(t2:Table {{table_id: '{table_id_esc}'}})
            RETURN 
                t1.table_name AS related_table_name,
                r.from_multiplicity AS from_multiplicity,
                r.to_multiplicity AS to_multiplicity
        """
        incoming = self.graph.query(query_in)
        
        relationships = {
            'outgoing': [],
            'incoming': []
        }
        
        for record in outgoing.result_set:
            relationships['outgoing'].append({
                'related_table': record[0],
                'cardinality': f"{record[1]}:{record[2]}"
            })
        
        for record in incoming.result_set:
            relationships['incoming'].append({
                'related_table': record[0],
                'cardinality': f"{record[1]}:{record[2]}"
            })
        
        return relationships
    
    def get_related_columns_in_table(self, table_id: str) -> List[Dict]:
        """Get all columns in the same table for context"""
        table_id_esc = escape_string(table_id)
        
        query = f"""
            MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column)
            RETURN 
                c.column_name AS column_name,
                c.type AS type,
                c.primary_key AS primary_key,
                c.foreign_key AS foreign_key
            ORDER BY c.column_name
        """
        result = self.graph.query(query)
        
        columns = []
        for record in result.result_set:
            columns.append({
                'column_name': record[0],
                'type': record[1],
                'is_pk': record[2] == 'Y',
                'is_fk': record[3] == 'Y'
            })
        
        return columns
    
    def get_column_lineage(self, table_id: str, column_name: str) -> Dict[str, Any]:
        """Get data lineage through foreign key relationships"""
        table_id_esc = escape_string(table_id)
        column_name_esc = escape_string(column_name)
        
        query = f"""
            MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column {{column_name: '{column_name_esc}'}})
            WHERE c.foreign_key = 'Y'
            RETURN c.foreign_key
        """
        fk_result = self.graph.query(query)
        
        if not fk_result.result_set:
            return {'is_foreign_key': False, 'references': None}
        
        query2 = f"""
            MATCH (source:Table {{table_id: '{table_id_esc}'}})-[r:RELATES_TO]->(target:Table)
            MATCH (target)-[:HAS_COLUMN]->(target_col:Column)
            WHERE target_col.primary_key = 'Y'
            RETURN 
                target.table_name AS referenced_table,
                target_col.column_name AS referenced_column,
                r.from_multiplicity AS from_card,
                r.to_multiplicity AS to_card
        """
        lineage_result = self.graph.query(query2)
        
        references = []
        for record in lineage_result.result_set:
            references.append({
                'referenced_table': record[0],
                'referenced_column': record[1],
                'relationship': f"{record[2]}:{record[3]}"
            })
        
        return {
            'is_foreign_key': True,
            'references': references
        }
    
    def get_table_context(self, table_id: str) -> Dict[str, Any]:
        """Get comprehensive table context"""
        table_id_esc = escape_string(table_id)
        
        query = f"""
            MATCH (t:Table {{table_id: '{table_id_esc}'}})-[:HAS_COLUMN]->(c:Column)
            WITH t, count(c) AS column_count,
                 sum(CASE WHEN c.primary_key = 'Y' THEN 1 ELSE 0 END) AS pk_count,
                 sum(CASE WHEN c.foreign_key = 'Y' THEN 1 ELSE 0 END) AS fk_count
            OPTIONAL MATCH (t)-[r:RELATES_TO]-()
            WITH t, column_count, pk_count, fk_count, count(DISTINCT r) AS relationship_count
            RETURN 
                t.table_name AS table_name,
                t.data_model AS data_model,
                column_count,
                pk_count,
                fk_count,
                relationship_count
        """
        result = self.graph.query(query)
        
        if result.result_set:
            record = result.result_set[0]
            return {
                'table_name': record[0],
                'data_model': record[1],
                'column_count': record[2],
                'primary_keys': record[3],
                'foreign_keys': record[4],
                'relationships': record[5]
            }
        
        return {}


# [NOTE: Include ALL remaining code from the advanced agents, workflow, and pipeline]
# For brevity, I'm indicating this should include all the MappingAgents, workflow, and pipeline code

if __name__ == "__main__":
    from schema_mapper_previous import (
        EnhancedMappingState, CHAIN_OF_THOUGHT_PROMPT, MIXTURE_OF_EXPERTS_PROMPTS,
        AdvancedMappingAgents, create_advanced_mapping_workflow, AdvancedSchemaMappingPipeline
    )
    # Or simply copy all remaining classes and functions here
    
    pipeline = AdvancedSchemaMappingPipeline()
    
    print("Loading schema with FULL deduplication...")
    pipeline.load_schema(
        attributes_csv="schema_attributes.csv",
        relationships_csv="schema_relationships.csv",
        deduplicate_existing=True  # Set to False to skip deduplication
    )
    
    print("\nMapping transaction fields...")
    results = pipeline.map_transaction_fields(
        transaction_csv="transaction_fields.csv",
        output_csv="advanced_mapping_results.csv"
    )
    
    if not results.empty:
        print("\nADVANCED MAPPING STATISTICS")
        print(f"Total Fields: {len(results)}")
        print(f"Approved: {len(results[results['status'] == 'APPROVED'])}")
        print(f"Average Confidence: {results['confidence'].mean():.4f}")
