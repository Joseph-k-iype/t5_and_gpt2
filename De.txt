"""
ISO 11179 Compliant Data Enrichment and Mapping System
=====================================================
This system enriches field names according to ISO/IEC 11179-1:2023 standards,
generates logical descriptions, maps fields to object-property combinations,
and identifies PII using LangGraph ReAct agents with mixture of experts reasoning.
"""

import json
import os
import pandas as pd
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Sequence
from operator import add
import numpy as np
from openai import OpenAI

# LangChain and LangGraph imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_community.vectorstores import InMemoryVectorStore

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI Configuration - DECLARE GLOBALLY AS REQUESTED
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_REASONING_MODEL = "o3-mini"  # For complex reasoning tasks
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"

# Initialize OpenAI client for direct API calls (embeddings)
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

# Initialize LangChain ChatOpenAI for LangGraph agents
llm = ChatOpenAI(
    model=OPENAI_REASONING_MODEL,
    openai_api_key=OPENAI_API_KEY,
    openai_api_base=OPENAI_BASE_URL
    # NO temperature or max_tokens as requested
)

# File paths
INPUT_JSON_PATH = "cib_long_json.json"
EXCEL_PATH = "pbt.xlsx"
OUTPUT_CSV_PATH = "enriched_mapped_output.csv"


# ============================================================================
# EMBEDDING FUNCTION USING OPENAI API DIRECTLY
# ============================================================================

def get_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Generate embeddings using OpenAI API directly (not tiktoken via langchain).
    Uses text-embedding-3-large model.
    
    Args:
        texts: List of text strings to embed
        
    Returns:
        List of embedding vectors
    """
    try:
        response = openai_client.embeddings.create(
            model=OPENAI_EMBEDDING_MODEL,
            input=texts
        )
        return [item.embedding for item in response.data]
    except Exception as e:
        print(f"Error generating embeddings: {e}")
        raise


# ============================================================================
# ISO 11179 STANDARDS SYSTEM PROMPTS
# ============================================================================

ISO_11179_NAMING_PRINCIPLES = """
ISO/IEC 11179-1:2023 and ISO/IEC 11179-5:2015 NAMING PRINCIPLES FOR METADATA REGISTRIES

Core Principles for Data Element Naming:

1. SEMANTIC CLARITY AND PRECISION:
   - Names must be semantically precise and unambiguous
   - Use natural language terms that clearly convey meaning
   - Each word should contribute meaningful information
   - Avoid abbreviations, acronyms, and technical jargon unless universally understood
   - Names should be understandable by both technical and non-technical stakeholders

2. NAMING CONVENTION STRUCTURE:
   - Use proper spacing between words (NO camelCase, NO snake_case, NO kebab-case)
   - Capitalize first letter of each significant word (Title Case)
   - Avoid special characters, underscores, hyphens in the semantic name
   - Example: "Customer Primary Email Address" not "customer_primary_email_address" or "customerPrimaryEmailAddress"

3. CONCEPT-BASED NAMING:
   - Names should reflect the conceptual meaning, not physical implementation
   - Focus on WHAT the data represents, not HOW it is stored
   - Example: "Customer Birth Date" not "CUST_DOB_FLD" or "dateOfBirth"

4. CONSISTENT TERMINOLOGY:
   - Use consistent terms across the organization
   - Align with business glossary and domain ontologies
   - Maintain consistency with ISO/IEC 11179-3 metamodel concepts

5. QUALIFIER TERMS:
   - Use qualifiers to differentiate similar data elements
   - Common qualifiers: Primary, Secondary, Preferred, Alternative, Current, Previous
   - Example: "Customer Primary Phone Number" vs "Customer Secondary Phone Number"

6. OBJECT CLASS - PROPERTY - REPRESENTATION STRUCTURE:
   - Object Class: The entity or concept being described (e.g., Customer, Account, Transaction)
   - Property: The characteristic or attribute (e.g., Name, Address, Balance)
   - Representation: The form or data type (usually implicit in naming)
   - Standard structure: [Object Class] [Qualifier(s)] [Property] [Representation]
   - Example: "Customer Mailing Address Text" or "Account Current Balance Amount"

7. AVOID REDUNDANCY:
   - Don't repeat information already implicit in context
   - Remove unnecessary words that don't add semantic value

8. MEANINGFUL AND DESCRIPTIVE:
   - Names should be self-documenting
   - A reader should understand what data is stored without additional documentation
   - Balance between brevity and completeness

9. STANDARDS ALIGNMENT:
   - Names should align with international standards (ISO, NIST, etc.)
   - Use terms from recognized vocabularies and ontologies
   - Consider domain-specific standards (e.g., FIBO for finance, HL7 for healthcare)

10. CASE SENSITIVITY AND FORMATTING:
    - Use Title Case for readability
    - First letter of each significant word capitalized
    - Articles (a, an, the) and short prepositions (of, in, on) lowercase unless at start

EXAMPLES OF PROPER ISO 11179 NAMING:

Poor Name → Good ISO 11179 Name
------------------------------
cust_id → Customer Identifier
emp_addr_1 → Employee Primary Street Address
DOB → Person Birth Date
phone_num → Contact Phone Number
acc_bal → Account Current Balance Amount
email_addr → Contact Email Address
SSN → Person Social Security Number
start_dt → Employment Start Date
cust_first_nm → Customer Given Name
prod_desc_txt → Product Description Text

Remember: The goal is to create names that are clear, consistent, and meaningful
to all stakeholders while following standardized principles for data element naming.
"""

ISO_11179_DEFINITION_PRINCIPLES = """
ISO/IEC 11179-4 PRINCIPLES FOR FORMULATING DATA DEFINITIONS

Core Principles for Data Element Definitions:

1. DEFINITION STRUCTURE:
   - State what the concept IS, not what it is not
   - Begin with the concept itself or a noun phrase that categorizes it
   - Use complete sentences with proper grammar and punctuation
   - Definitions must be precise, unambiguous, and concise

2. ESSENTIAL CHARACTERISTICS:
   - Include only essential characteristics that distinguish this element
   - Specify the concept domain or classification
   - Describe the semantic meaning, not implementation details
   - Avoid circular definitions

3. CONTEXT AND SCOPE:
   - Provide context for understanding the element within the domain
   - Specify any constraints, valid ranges, or business rules
   - Indicate relationships to other data elements if relevant
   - Clarify any ambiguous terms used in the definition

4. DEFINITION CONTENT RULES:
   - Start with a descriptive phrase identifying the category
   - Use present tense
   - Use singular form for countable concepts
   - Be technology and implementation neutral
   - Avoid embedding multiple concepts in one definition

5. WHAT TO AVOID:
   - Avoid using the term being defined in the definition (circularity)
   - Don't describe how data is captured, stored, or processed
   - Don't include system-specific information
   - Don't use abbreviations or acronyms without spelling out
   - Avoid vague terms like "information about" or "data concerning"

6. SEMANTIC PRECISION:
   - Each word must contribute to precision
   - Remove ambiguous or unnecessary words
   - Use established terminology from domain ontologies
   - Ensure consistency with organizational vocabulary

7. RELATIONSHIP TO NAME:
   - Definition should elaborate on the name
   - Must be consistent with the data element name
   - Provide additional context not captured in the name

8. EXAMPLES OF PROPER DEFINITIONS:

Data Element: Customer Birth Date
Good Definition: "The calendar date on which a customer was born, used for age verification, 
demographic analysis, and regulatory compliance purposes. Format: YYYY-MM-DD."

Data Element: Account Current Balance Amount
Good Definition: "The monetary value representing the current financial position of an account, 
calculated as the sum of all credits minus all debits up to the present moment. Expressed in 
the account's designated currency with two decimal places for fractional units."

Data Element: Customer Primary Email Address
Good Definition: "The principal electronic mail address designated by a customer for official 
communications and correspondence. Must conform to RFC 5322 email address specification format."

Data Element: Transaction Authorization Code
Good Definition: "A unique alphanumeric identifier assigned by the payment processor to confirm 
approval of a transaction request. Serves as proof of authorization and is used for transaction 
reconciliation and dispute resolution."

Data Element: Employee Social Security Number
Good Definition: "The unique nine-digit identification number issued by the Social Security 
Administration to an employee for tax reporting and benefits administration purposes. Format: 
XXX-XX-XXXX. Classified as sensitive personally identifiable information requiring strict 
access controls and encryption."

9. DEFINITION TEMPLATE:
   "The [category/type] [that/which] [essential characteristics], [used for purpose]. 
   [Additional context, constraints, or format specifications as needed]."

Remember: Definitions must be clear enough that someone unfamiliar with the domain can 
understand what the data element represents and how it should be interpreted.
"""

PII_CLASSIFICATION_FRAMEWORK = """
COMPREHENSIVE PII (PERSONALLY IDENTIFIABLE INFORMATION) CLASSIFICATION FRAMEWORK

Based on NIST SP 800-122, GDPR, ISO/IEC 27001, and industry best practices.

PII CLASSIFICATION LEVELS:

1. NOT PII (Level 0):
   - Definition: Data that cannot identify an individual directly or indirectly
   - Examples: Aggregate statistics, anonymized data, system metadata
   - Risk: None
   - Protection Required: Standard data security practices

2. LOW SENSITIVITY PII (Level 1):
   - Definition: Non-sensitive information that is publicly available or has minimal privacy impact
   - Examples: 
     * Business email addresses (generic)
     * Office phone numbers
     * Job titles
     * Department names
     * Public organization information
   - Risk: Minimal privacy impact if disclosed
   - Protection Required: Basic access controls

3. MODERATE SENSITIVITY PII (Level 2):
   - Definition: Information that can identify individuals when combined with other data
   - Examples:
     * First name or last name alone (not together)
     * Zip code / postal code
     * Gender
     * Date of birth (without other identifiers)
     * Age range
     * Ethnicity or race
     * Religion
     * Education level
     * Place of birth (city/country)
     * IP addresses
     * Device identifiers
     * Cookie IDs
   - Risk: Could enable identification when linked with other data
   - Protection Required: Restricted access, encryption in transit

4. HIGH SENSITIVITY PII (Level 3):
   - Definition: Direct identifiers that can uniquely identify an individual
   - Examples:
     * Full name (first + last name together)
     * Personal email address
     * Personal phone number
     * Home address / mailing address
     * Photograph or image with identifiable face
     * Employee identification number
     * Customer identification number
     * Account numbers
     * Vehicle registration number
     * License plate number
     * Mother's maiden name
     * Biometric data (fingerprints, facial recognition, iris scan, voice print)
   - Risk: High - Direct identification possible
   - Protection Required: Strong access controls, encryption at rest and in transit, audit logging

5. CRITICAL/HIGHLY SENSITIVE PII (Level 4):
   - Definition: Extremely sensitive data that if compromised could cause severe harm
   - Examples:
     * Social Security Number (SSN)
     * Passport number
     * Driver's license number
     * National identification number
     * Tax identification number
     * Financial account numbers (bank account, credit card)
     * Financial PINs or passwords
     * Medical record numbers
     * Health insurance numbers
     * Medical/health information
     * Mental health data
     * Genetic information
     * Sexual orientation
     * Criminal history
     * Credentials (username + password combinations)
     * Security questions and answers
     * Immigration status
   - Risk: Critical - Identity theft, fraud, severe privacy harm
   - Protection Required: Maximum security - encryption at rest and in transit, 
     strict access controls, multi-factor authentication, comprehensive audit logging, 
     data loss prevention, tokenization where possible

SPECIAL CATEGORIES REQUIRING ADDITIONAL CONSIDERATION:

A. LINKED PII:
   - Multiple moderate sensitivity items together that create high sensitivity
   - Example: Name + Date of Birth + City = High Sensitivity PII
   - Apply the highest classification level among linked elements

B. CONTEXT-DEPENDENT PII:
   - Some data elements may vary in sensitivity based on context
   - Consider: Who has access? What system? What purpose?
   - Example: Employee ID in HR system (High) vs. Employee ID in company directory (Moderate)

C. DERIVED OR INFERRED PII:
   - Data that doesn't directly identify but can be used to infer identity
   - Examples: Behavioral patterns, usage patterns, location history
   - Classification: Typically Moderate to High sensitivity

D. METADATA AS PII:
   - Timestamps, geolocation tags, access logs can be PII
   - Consider implications of metadata exposure

PII IDENTIFICATION DECISION TREE:

1. Can this data element directly identify a specific individual?
   → YES: Minimum Level 3 (High Sensitivity)
   → NO: Continue to question 2

2. Is this data element regulated by law (GDPR, HIPAA, CCPA, etc.)?
   → YES: Likely Level 3 or 4 (High/Critical)
   → NO: Continue to question 3

3. Would disclosure cause harm, embarrassment, or discrimination to an individual?
   → YES: Level 3 or 4 (High/Critical)
   → NO: Continue to question 4

4. Can this data be combined with other readily available information to identify someone?
   → YES: Level 2 or 3 (Moderate/High)
   → NO: Continue to question 5

5. Is this data publicly available or commonly shared?
   → YES: Level 1 (Low Sensitivity)
   → NO: Level 2 (Moderate)

REGULATORY FRAMEWORK CONSIDERATIONS:

- GDPR (EU): Broad definition of personal data
- CCPA (California): Consumer personal information
- HIPAA (US Healthcare): Protected Health Information (PHI)
- GLBA (US Financial): Nonpublic Personal Information (NPI)
- FERPA (US Education): Education records
- PIPEDA (Canada): Personal information

PII CLASSIFICATION OUTPUT FORMAT:

For each data element, provide:
1. PII Classification Level: [0-4]
2. PII Category: [NOT PII / LOW / MODERATE / HIGH / CRITICAL]
3. Specific PII Type: [e.g., "Direct Identifier - Financial", "Quasi-Identifier"]
4. Rationale: Detailed explanation of why this classification was assigned
5. Regulatory Considerations: Any relevant regulations (GDPR, HIPAA, etc.)
6. Risk Assessment: What risks exist if this data is compromised
7. Recommended Controls: Security measures appropriate for this classification level

Remember: When in doubt, classify at a higher sensitivity level. It's better to 
over-protect data than to under-protect it. Always consider the context and 
potential for linkability with other data elements.
"""


# ============================================================================
# AGENT STATE DEFINITION
# ============================================================================

class EnrichmentState(TypedDict):
    """
    State schema for the enrichment and mapping agent system.
    Tracks messages, field being processed, and accumulated results.
    """
    messages: Annotated[Sequence[BaseMessage], add_messages]
    current_field: Dict[str, Any]
    enriched_name: str
    enriched_description: str
    object_name: str
    property_name: str
    pii_classification: Dict[str, Any]
    enrichment_rationale: str
    mapping_rationale: str
    vector_store: Any
    objects_dict: Dict[str, List[str]]
    all_objects: List[str]
    all_properties: List[str]
    expert_opinions: List[str]
    reasoning_chain: List[str]


# ============================================================================
# TOOLS FOR REACT AGENTS
# ============================================================================

@tool
def iso_11179_name_enrichment_expert(field_name: str, field_context: str) -> str:
    """
    Expert agent for enriching field names according to ISO 11179 standards.
    Transforms technical field names into semantically precise, human-readable names
    with proper spacing and clear meaning.
    
    Args:
        field_name: The original field name to enrich
        field_context: Context about the field (application, description)
        
    Returns:
        Enriched field name following ISO 11179 naming principles
    """
    prompt = f"""You are an ISO 11179 metadata naming standards expert with deep expertise in 
data governance, semantic modeling, and enterprise metadata management.

TASK: Transform the given technical field name into a semantically precise, ISO 11179 compliant name.

ORIGINAL FIELD NAME: {field_name}

CONTEXT: {field_context}

APPLY THESE ISO 11179 PRINCIPLES:

{ISO_11179_NAMING_PRINCIPLES}

REASONING PROCESS (Think step-by-step):

1. ANALYZE ORIGINAL NAME:
   - Identify the core concept or entity (Object Class)
   - Identify the property or characteristic being described
   - Recognize any qualifiers or modifiers
   - Understand the semantic intent behind abbreviations or technical notation

2. DECONSTRUCT TECHNICAL ELEMENTS:
   - Break down camelCase: "customerEmailAddr" → Customer + Email + Address
   - Break down snake_case: "cust_birth_dt" → Customer + Birth + Date
   - Expand abbreviations with domain knowledge
   - Identify implicit meanings in codes or technical terms

3. CONSTRUCT ISO 11179 COMPLIANT NAME:
   - Start with Object Class (the entity): Customer, Account, Transaction, etc.
   - Add qualifiers if needed: Primary, Secondary, Current, Previous, etc.
   - Add the property: Name, Address, Date, Amount, Status, etc.
   - Add representation term if helpful: Text, Code, Amount, Date, Indicator, etc.
   - Use Title Case with proper spacing

4. VALIDATE SEMANTIC CLARITY:
   - Is the name self-documenting?
   - Would a non-technical business user understand it?
   - Is it unambiguous and precise?
   - Does it avoid jargon and abbreviations?

5. CROSS-CHECK AGAINST CONTEXT:
   - Does the enriched name align with the application context?
   - Is it appropriate for the business domain?

ENRICHED NAME REQUIREMENTS:
- Use natural language with spaces (NOT camelCase, snake_case, or kebab-case)
- Title Case capitalization
- No special characters, underscores, or hyphens
- 2-6 words typically (balance brevity and clarity)
- Clear semantic meaning
- Business-friendly terminology

OUTPUT FORMAT:
Provide ONLY the enriched field name, nothing else. No explanation, no additional text.

Example Transformations:
- "cust_id" → "Customer Identifier"
- "acctBal" → "Account Balance Amount"
- "empStartDt" → "Employee Start Date"
- "primaryPhNum" → "Primary Phone Number"
- "userEmailAddr" → "User Email Address"

Now provide the enriched name:"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        enriched_name = response.content.strip()
        return enriched_name
    except Exception as e:
        return f"Error in name enrichment: {str(e)}"


@tool
def iso_11179_definition_expert(enriched_name: str, original_name: str, context: str) -> str:
    """
    Expert agent for creating ISO 11179 compliant data element definitions.
    Generates precise, formal definitions that explain what the data element represents.
    
    Args:
        enriched_name: The ISO 11179 enriched field name
        original_name: The original technical field name
        context: Business and application context
        
    Returns:
        ISO 11179 compliant definition
    """
    prompt = f"""You are an ISO 11179 metadata definition standards expert specializing in 
formulating precise, formal data element definitions.

TASK: Create a comprehensive, ISO 11179 compliant definition for the given data element.

ENRICHED FIELD NAME: {enriched_name}
ORIGINAL FIELD NAME: {original_name}
CONTEXT: {context}

APPLY THESE ISO 11179 DEFINITION PRINCIPLES:

{ISO_11179_DEFINITION_PRINCIPLES}

REASONING PROCESS FOR DEFINITION FORMULATION:

1. IDENTIFY THE CONCEPT CATEGORY:
   - What type of thing is this? (a date, an amount, an identifier, a description, etc.)
   - What is the superordinate category?

2. SPECIFY ESSENTIAL CHARACTERISTICS:
   - What distinguishes this element from similar elements?
   - What are the key semantic properties?
   - What business purpose does it serve?

3. PROVIDE NECESSARY CONTEXT:
   - How is this element used in business processes?
   - What are valid values or constraints?
   - What is the relationship to other data elements?

4. ADD TECHNICAL SPECIFICATIONS (if relevant):
   - Format specifications (e.g., date format, decimal precision)
   - Valid ranges or enumerated values
   - Measurement units

5. INCLUDE PURPOSE AND USAGE:
   - Why is this data collected?
   - How is it used in business operations?
   - What decisions or processes depend on it?

DEFINITION STRUCTURE TEMPLATE:
"The [category/type] [that/which] [essential characteristics describing what it represents]. 
[Purpose and business context]. [Additional constraints, formats, or specifications]. 
[Security/privacy considerations if applicable]."

QUALITY CRITERIA:
- Uses complete sentences with proper grammar
- Starts with a clear categorization
- Avoids circular definitions (doesn't use the term being defined)
- Technology and implementation neutral
- Semantic precision without unnecessary verbosity
- Understandable by business users
- Includes relevant context and constraints

EXAMPLES OF WELL-FORMED DEFINITIONS:

Example 1:
Name: Customer Birth Date
Definition: "The calendar date on which a customer was born, expressed in ISO 8601 format 
(YYYY-MM-DD). Used for age verification, eligibility determination, demographic analysis, 
and regulatory compliance. Must be a valid past date not earlier than 120 years from the 
current date."

Example 2:
Name: Account Current Balance Amount
Definition: "The monetary value representing the net financial position of an account at 
the present moment, calculated as the sum of all credit transactions minus all debit 
transactions from account inception to the current date and time. Expressed in the 
account's designated currency with precision to two decimal places. Used for financial 
reporting, transaction authorization, and customer account statements."

Example 3:
Name: Employee Primary Email Address
Definition: "The principal electronic mail address officially assigned to or designated by 
an employee for corporate communications, notifications, and official correspondence. Must 
conform to RFC 5322 standard email address format. Serves as the primary digital contact 
method and may be used for system authentication and access control."

Now create the ISO 11179 compliant definition for the given data element.
Provide ONLY the definition text, no preamble or additional commentary:"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        definition = response.content.strip()
        return definition
    except Exception as e:
        return f"Error in definition creation: {str(e)}"


@tool
def semantic_mapping_expert(
    enriched_name: str, 
    enriched_description: str,
    all_objects: str,
    all_properties: str,
    context: str
) -> str:
    """
    Expert agent for mapping enriched field names to object-property combinations
    using semantic similarity and domain reasoning.
    
    Args:
        enriched_name: The enriched field name
        enriched_description: The enriched description
        all_objects: Comma-separated list of available objects
        all_properties: Comma-separated list of available properties
        context: Business context
        
    Returns:
        JSON string with best_object and best_property
    """
    prompt = f"""You are a semantic data modeling expert specializing in ontology alignment 
and metadata mapping.

TASK: Map the given enriched data element to the most semantically appropriate Object and 
Property from the available vocabulary.

ENRICHED FIELD NAME: {enriched_name}
ENRICHED DESCRIPTION: {enriched_description}
CONTEXT: {context}

AVAILABLE OBJECTS:
{all_objects}

AVAILABLE PROPERTIES:
{all_properties}

REASONING PROCESS FOR SEMANTIC MAPPING:

1. UNDERSTAND THE DATA ELEMENT:
   - What is the primary entity or concept? (This suggests the Object)
   - What characteristic or attribute is being described? (This suggests the Property)
   - Parse the enriched name: [Object Class] [Qualifiers] [Property] [Representation]

2. IDENTIFY CANDIDATE OBJECTS:
   - From the enriched name, what is the subject/entity?
   - Look for objects that semantically match this entity
   - Consider synonyms and related terms
   - Example: "Customer Birth Date" → Object is likely "Customer" or "Person"

3. IDENTIFY CANDIDATE PROPERTIES:
   - What attribute/characteristic is being captured?
   - Look for properties that match the attribute
   - Consider synonyms: "Birth Date" could map to "BirthDate", "DateOfBirth", "DOB"
   - Think about property granularity and specificity

4. SEMANTIC SIMILARITY ANALYSIS:
   - For each candidate object-property pair, assess:
     * Exact match? (highest confidence)
     * Synonym or variant? (high confidence)
     * Related term? (medium confidence)
     * Conceptual fit? (lower confidence)

5. CONTEXTUAL VALIDATION:
   - Does this mapping make sense in the business domain?
   - Is the object-property relationship logical?
   - Would this mapping be useful for data integration?

6. HANDLE AMBIGUITY:
   - If multiple objects could apply, choose the most specific one
   - If no exact property match, choose the closest semantic match
   - Document reasoning for choices

7. EDGE CASES:
   - If no good object match exists, recommend the closest general category
   - If no good property match exists, recommend the closest semantic match
   - Explain the confidence level of the mapping

MAPPING QUALITY CRITERIA:
- Semantic coherence: Object-Property pair must be logically related
- Specificity: Prefer specific matches over generic ones
- Business alignment: Mapping should make sense in the business domain
- Standard compliance: Prefer standard terminology over non-standard

OUTPUT FORMAT (JSON):
{{
    "best_object": "The most appropriate object from the available list",
    "best_property": "The most appropriate property from the available list",
    "confidence": "HIGH/MEDIUM/LOW",
    "reasoning": "Detailed explanation of why this mapping was chosen, including semantic analysis"
}}

Provide ONLY valid JSON, no additional text before or after."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except Exception as e:
        return json.dumps({
            "best_object": "Unknown",
            "best_property": "Unknown",
            "confidence": "ERROR",
            "reasoning": f"Error in semantic mapping: {str(e)}"
        })


@tool
def pii_classification_expert(
    enriched_name: str, 
    enriched_description: str,
    object_name: str,
    property_name: str
) -> str:
    """
    Expert agent for classifying data elements according to PII sensitivity levels
    based on NIST, GDPR, and industry standards.
    
    Args:
        enriched_name: The enriched field name
        enriched_description: The field description
        object_name: Mapped object name
        property_name: Mapped property name
        
    Returns:
        JSON string with comprehensive PII classification
    """
    prompt = f"""You are a data privacy and PII classification expert with deep knowledge of 
NIST SP 800-122, GDPR, CCPA, HIPAA, and global data protection regulations.

TASK: Classify the given data element according to PII sensitivity levels and provide 
comprehensive privacy analysis.

DATA ELEMENT INFORMATION:
- Enriched Name: {enriched_name}
- Description: {enriched_description}
- Object: {object_name}
- Property: {property_name}

APPLY THIS PII CLASSIFICATION FRAMEWORK:

{PII_CLASSIFICATION_FRAMEWORK}

DETAILED REASONING PROCESS FOR PII CLASSIFICATION:

1. INITIAL PII DETERMINATION:
   - Does this data element relate to an identified or identifiable person?
   - Can it be used to distinguish or trace an individual's identity?
   - Is it linked or linkable to a specific person?

2. DIRECT IDENTIFIER ANALYSIS:
   - Is this a direct identifier (SSN, passport, email, full name)?
   - Can it uniquely identify someone on its own?
   → If YES: Minimum Level 3 (High Sensitivity)

3. REGULATORY CLASSIFICATION:
   - Is this data protected by specific regulations?
     * GDPR Personal Data?
     * HIPAA Protected Health Information (PHI)?
     * CCPA Personal Information?
     * Financial data under GLBA?
     * Education records under FERPA?
   - What specific regulations apply?

4. SENSITIVITY ASSESSMENT:
   - What harm could result from unauthorized disclosure?
     * Identity theft risk?
     * Financial fraud risk?
     * Discrimination risk?
     * Reputational harm?
     * Physical safety concerns?
   - Severity: None / Minimal / Moderate / High / Critical

5. LINKABILITY ANALYSIS:
   - Can this data be combined with other readily available data to identify someone?
   - What other data elements would be needed?
   - How easily could such linking occur?
   - Context matters: In isolation vs. in combination

6. SPECIAL CATEGORY DETERMINATION:
   - Is this a special category under GDPR?
     * Racial or ethnic origin
     * Political opinions
     * Religious beliefs
     * Trade union membership
     * Genetic data
     * Biometric data for identification
     * Health data
     * Sex life or sexual orientation
   → If YES: Level 4 (Critical Sensitivity)

7. CONTEXT CONSIDERATIONS:
   - How is this data typically used?
   - Who has access to it?
   - What system contains it?
   - Public vs. private context?

8. CLASSIFICATION ASSIGNMENT:
   - Based on all factors above, assign classification level 0-4
   - Provide specific PII type/category
   - When in doubt, classify higher for safety

CLASSIFICATION OUTPUT REQUIREMENTS:

Provide a comprehensive JSON object with:

{{
    "pii_level": <integer 0-4>,
    "pii_category": "<NOT PII | LOW | MODERATE | HIGH | CRITICAL>",
    "pii_type": "<Specific type, e.g., 'Direct Identifier - Financial' or 'Quasi-Identifier'>",
    "is_pii": <boolean>,
    "is_sensitive_pii": <boolean>,
    "is_direct_identifier": <boolean>,
    "regulatory_considerations": [
        "<List of applicable regulations: GDPR, HIPAA, CCPA, etc.>"
    ],
    "risk_assessment": {{
        "identity_theft_risk": "<NONE | LOW | MEDIUM | HIGH | CRITICAL>",
        "financial_fraud_risk": "<NONE | LOW | MEDIUM | HIGH | CRITICAL>",
        "discrimination_risk": "<NONE | LOW | MEDIUM | HIGH | CRITICAL>",
        "reputational_harm_risk": "<NONE | LOW | MEDIUM | HIGH | CRITICAL>",
        "overall_risk": "<NONE | LOW | MEDIUM | HIGH | CRITICAL>"
    }},
    "linkability_assessment": "<Description of how this data could be linked to identify individuals>",
    "recommended_controls": [
        "<List of specific security controls appropriate for this classification level>"
    ],
    "detailed_rationale": "<Comprehensive explanation of classification decision with specific reasoning>"
}}

CLASSIFICATION EXAMPLES:

Example 1 - High Sensitivity:
Name: "Customer Primary Email Address"
Classification: Level 3 (HIGH)
Rationale: Direct identifier that uniquely identifies an individual. Personal email addresses 
are considered PII under GDPR and CCPA. Can be used for account access, password resets, and 
linked to online behavior.

Example 2 - Critical Sensitivity:
Name: "Employee Social Security Number"
Classification: Level 4 (CRITICAL)
Rationale: Highly sensitive direct identifier regulated under multiple laws. Primary risk 
factor for identity theft. Subject to strict protection requirements including encryption, 
access controls, and audit logging.

Example 3 - Moderate Sensitivity:
Name: "Transaction Postal Code"
Classification: Level 2 (MODERATE)
Rationale: Quasi-identifier that can be linked with other data to identify individuals. 
Limited PII on its own but increases in sensitivity when combined with other demographic data.

Now analyze and classify the given data element. Provide ONLY valid JSON, no additional text:"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except Exception as e:
        return json.dumps({
            "pii_level": 0,
            "pii_category": "ERROR",
            "pii_type": "Classification Error",
            "is_pii": False,
            "is_sensitive_pii": False,
            "is_direct_identifier": False,
            "regulatory_considerations": [],
            "risk_assessment": {
                "identity_theft_risk": "UNKNOWN",
                "financial_fraud_risk": "UNKNOWN",
                "discrimination_risk": "UNKNOWN",
                "reputational_harm_risk": "UNKNOWN",
                "overall_risk": "UNKNOWN"
            },
            "linkability_assessment": "Error during classification",
            "recommended_controls": [],
            "detailed_rationale": f"Error during PII classification: {str(e)}"
        })


@tool
def vector_similarity_search(query: str, vector_store_obj: str, top_k: int = 5) -> str:
    """
    Perform semantic similarity search using embeddings to find best matches.
    
    Args:
        query: The search query
        vector_store_obj: Placeholder (actual vector store passed via state)
        top_k: Number of top results to return
        
    Returns:
        JSON string with top similar matches
    """
    # This is a placeholder - actual implementation uses state
    return json.dumps({"matches": [], "note": "Implemented via state in agent workflow"})


# ============================================================================
# SUPERVISOR AGENT
# ============================================================================

def supervisor_validation_agent(state: EnrichmentState) -> EnrichmentState:
    """
    Supervisor agent that validates the enrichment and mapping results,
    ensures quality, consistency, and provides final rationale.
    """
    prompt = f"""You are a SUPERVISOR AGENT responsible for validating and ensuring quality 
of data enrichment and mapping results. You have oversight of all expert agents and must 
verify their work meets the highest standards.

REVIEW THE FOLLOWING RESULTS:

ORIGINAL FIELD: {state['current_field'].get('Field Name', 'N/A')}
APPLICATION: {state['current_field'].get('Application Name', 'N/A')}
DESCRIPTION: {state['current_field'].get('Application Description', 'N/A')}

EXPERT AGENT RESULTS:
- Enriched Name: {state['enriched_name']}
- Enriched Description: {state['enriched_description']}
- Mapped Object: {state['object_name']}
- Mapped Property: {state['property_name']}
- PII Classification: {state['pii_classification']}

EXPERT OPINIONS RECEIVED:
{chr(10).join(f"- {opinion}" for opinion in state['expert_opinions'])}

REASONING CHAIN:
{chr(10).join(f"{i+1}. {step}" for i, step in enumerate(state['reasoning_chain']))}

YOUR SUPERVISORY VALIDATION TASKS:

1. VALIDATE ENRICHMENT QUALITY:
   - Does the enriched name follow ISO 11179 standards?
   - Is it clear, unambiguous, and semantically precise?
   - Are there any remaining technical artifacts (abbreviations, underscores)?
   - Is the description comprehensive and well-formed?

2. VALIDATE MAPPING CORRECTNESS:
   - Is the Object-Property mapping logically sound?
   - Does it make semantic sense?
   - Is there strong alignment between the enriched name and the mapping?

3. VALIDATE PII CLASSIFICATION:
   - Is the PII classification appropriate and well-reasoned?
   - Are all privacy considerations accounted for?
   - Are recommended controls appropriate?

4. CONSISTENCY CHECK:
   - Do all components (name, description, mapping, PII) align coherently?
   - Are there any contradictions or inconsistencies?

5. QUALITY ASSURANCE:
   - Would this result be useful for data governance?
   - Could it be understood by stakeholders?
   - Is it production-ready?

PROVIDE YOUR SUPERVISOR ASSESSMENT:

Respond with a JSON object:
{{
    "validation_passed": <boolean>,
    "quality_score": <1-10>,
    "enrichment_rationale": "<Comprehensive explanation validating the enrichment decisions>",
    "mapping_rationale": "<Comprehensive explanation validating the mapping decisions>",
    "concerns": ["<Any concerns or areas for improvement>"],
    "recommendations": ["<Any recommendations for enhancement>"],
    "final_approval": <boolean>
}}

Provide ONLY valid JSON:"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        supervisor_result = json.loads(response.content.strip())
        
        # Update state with supervisor rationale
        state['enrichment_rationale'] = supervisor_result.get('enrichment_rationale', '')
        state['mapping_rationale'] = supervisor_result.get('mapping_rationale', '')
        
        # Add supervisor validation to reasoning chain
        state['reasoning_chain'].append(
            f"SUPERVISOR VALIDATION: Quality Score {supervisor_result.get('quality_score', 0)}/10. "
            f"Approval: {supervisor_result.get('final_approval', False)}"
        )
        
        return state
    except Exception as e:
        print(f"Supervisor validation error: {e}")
        state['enrichment_rationale'] = "Automated validation completed"
        state['mapping_rationale'] = "Automated validation completed"
        return state


# ============================================================================
# REACT AGENT WORKFLOW NODES
# ============================================================================

def enrichment_coordinator_node(state: EnrichmentState) -> EnrichmentState:
    """
    Coordinator node that orchestrates the enrichment process using dynamic
    chain of thought and mixture of experts reasoning.
    """
    current_field = state['current_field']
    field_name = current_field.get('Field Name', '')
    context = f"""
    Application: {current_field.get('Application Name', '')}
    Description: {current_field.get('Application Description', '')}
    Current Classification: {current_field.get('ISR Classification', '')}
    """
    
    print(f"\n{'='*80}")
    print(f"PROCESSING FIELD: {field_name}")
    print(f"{'='*80}")
    
    # Initialize reasoning chain
    state['reasoning_chain'] = []
    state['expert_opinions'] = []
    
    # STEP 1: Name Enrichment using expert agent
    print("\n[STEP 1] ISO 11179 Name Enrichment...")
    state['reasoning_chain'].append(f"ANALYZING: Original field name '{field_name}'")
    
    enriched_name = iso_11179_name_enrichment_expert.invoke({
        "field_name": field_name,
        "field_context": context
    })
    state['enriched_name'] = enriched_name
    state['expert_opinions'].append(f"Name Enrichment Expert: '{enriched_name}'")
    state['reasoning_chain'].append(f"ENRICHED NAME: '{enriched_name}'")
    print(f"   → Enriched Name: {enriched_name}")
    
    # STEP 2: Definition Generation
    print("\n[STEP 2] ISO 11179 Definition Generation...")
    enriched_description = iso_11179_definition_expert.invoke({
        "enriched_name": enriched_name,
        "original_name": field_name,
        "context": context
    })
    state['enriched_description'] = enriched_description
    state['expert_opinions'].append(f"Definition Expert: Generated formal definition")
    state['reasoning_chain'].append(f"DEFINITION CREATED: {enriched_description[:100]}...")
    print(f"   → Definition: {enriched_description[:100]}...")
    
    # STEP 3: Semantic Mapping with Vector Search
    print("\n[STEP 3] Semantic Mapping Analysis...")
    
    # Perform vector similarity search for best matches
    query_text = f"{enriched_name} {enriched_description}"
    query_embedding = get_embeddings([query_text])[0]
    
    # Search in objects and properties vector stores
    state['reasoning_chain'].append("VECTOR SIMILARITY SEARCH: Finding semantic matches")
    
    mapping_result = semantic_mapping_expert.invoke({
        "enriched_name": enriched_name,
        "enriched_description": enriched_description,
        "all_objects": ", ".join(state['all_objects'][:50]),  # Limit for context
        "all_properties": ", ".join(state['all_properties'][:50]),
        "context": context
    })
    
    try:
        mapping_data = json.loads(mapping_result)
        state['object_name'] = mapping_data.get('best_object', 'Unknown')
        state['property_name'] = mapping_data.get('best_property', 'Unknown')
        state['expert_opinions'].append(
            f"Mapping Expert: {state['object_name']}.{state['property_name']} "
            f"(Confidence: {mapping_data.get('confidence', 'UNKNOWN')})"
        )
        state['reasoning_chain'].append(
            f"MAPPED TO: Object='{state['object_name']}', Property='{state['property_name']}'"
        )
        print(f"   → Mapped to: {state['object_name']}.{state['property_name']}")
    except:
        state['object_name'] = 'Unknown'
        state['property_name'] = 'Unknown'
        print(f"   → Mapping failed, using defaults")
    
    # STEP 4: PII Classification
    print("\n[STEP 4] PII Classification Analysis...")
    pii_result = pii_classification_expert.invoke({
        "enriched_name": enriched_name,
        "enriched_description": enriched_description,
        "object_name": state['object_name'],
        "property_name": state['property_name']
    })
    
    try:
        pii_data = json.loads(pii_result)
        state['pii_classification'] = pii_data
        state['expert_opinions'].append(
            f"PII Expert: Level {pii_data.get('pii_level', 0)} - "
            f"{pii_data.get('pii_category', 'UNKNOWN')}"
        )
        state['reasoning_chain'].append(
            f"PII CLASSIFIED: Level {pii_data.get('pii_level', 0)} - "
            f"{pii_data.get('pii_category', 'UNKNOWN')}"
        )
        print(f"   → PII Level: {pii_data.get('pii_level', 0)} ({pii_data.get('pii_category', 'UNKNOWN')})")
    except:
        state['pii_classification'] = {
            "pii_level": 0,
            "pii_category": "ERROR",
            "detailed_rationale": "Classification failed"
        }
        print(f"   → PII classification failed")
    
    # STEP 5: Supervisor Validation
    print("\n[STEP 5] Supervisor Validation...")
    state = supervisor_validation_agent(state)
    print(f"   → Validation completed")
    
    print(f"\n{'='*80}\n")
    
    return state


def should_continue(state: EnrichmentState) -> Literal["continue", "end"]:
    """
    Conditional edge to determine if more processing is needed.
    """
    # For this workflow, we process once per field
    return "end"


# ============================================================================
# BUILD LANGGRAPH WORKFLOW
# ============================================================================

def create_enrichment_graph():
    """
    Create the LangGraph workflow for data enrichment and mapping.
    """
    workflow = StateGraph(EnrichmentState)
    
    # Add nodes
    workflow.add_node("enrichment_coordinator", enrichment_coordinator_node)
    
    # Set entry point
    workflow.set_entry_point("enrichment_coordinator")
    
    # Add conditional edge
    workflow.add_conditional_edges(
        "enrichment_coordinator",
        should_continue,
        {
            "continue": "enrichment_coordinator",
            "end": END
        }
    )
    
    return workflow.compile()


# ============================================================================
# MAIN PROCESSING FUNCTION
# ============================================================================

def process_data_enrichment_and_mapping():
    """
    Main function to process the entire dataset.
    """
    print("="*80)
    print("ISO 11179 DATA ENRICHMENT AND MAPPING SYSTEM")
    print("="*80)
    
    # Load input JSON
    print("\n[1] Loading input JSON...")
    with open(INPUT_JSON_PATH, 'r') as f:
        input_data = json.load(f)
    print(f"   → Loaded {len(input_data)} records")
    
    # Load Excel and create optimized dictionaries
    print("\n[2] Loading Excel and creating optimized dictionaries...")
    excel_df = pd.read_excel(EXCEL_PATH)
    
    # Create three dictionaries as requested
    objects_dict = {}  # Object → [Properties]
    all_objects = []
    all_properties = []
    
    for _, row in excel_df.iterrows():
        obj = str(row['Object name']).strip()
        prop = str(row['Property name']).strip()
        
        if obj not in objects_dict:
            objects_dict[obj] = []
            all_objects.append(obj)
        
        if prop not in objects_dict[obj]:
            objects_dict[obj].append(prop)
        
        if prop not in all_properties:
            all_properties.append(prop)
    
    print(f"   → Found {len(all_objects)} unique objects")
    print(f"   → Found {len(all_properties)} unique properties")
    
    # Create vector store for semantic search
    print("\n[3] Creating vector embeddings for semantic search...")
    
    # Combine object-property pairs for embedding
    object_property_texts = []
    for obj in all_objects:
        for prop in objects_dict[obj]:
            object_property_texts.append(f"{obj} {prop}")
    
    # Generate embeddings
    print(f"   → Generating embeddings for {len(object_property_texts)} object-property combinations...")
    embeddings = get_embeddings(object_property_texts)
    
    # Create in-memory vector store
    # Note: Using a simple numpy-based approach since LangChain's InMemoryVectorStore 
    # requires specific embedding function format
    vector_store = {
        "texts": object_property_texts,
        "embeddings": np.array(embeddings)
    }
    print(f"   → Vector store created")
    
    # Create LangGraph workflow
    print("\n[4] Initializing LangGraph workflow...")
    app = create_enrichment_graph()
    print("   → Workflow initialized")
    
    # Process each field
    print(f"\n[5] Processing {len(input_data)} fields...")
    results = []
    
    for idx, record in enumerate(input_data):
        print(f"\n{'='*80}")
        print(f"Processing Record {idx + 1}/{len(input_data)}")
        print(f"{'='*80}")
        
        # Initialize state
        initial_state = {
            "messages": [],
            "current_field": record,
            "enriched_name": "",
            "enriched_description": "",
            "object_name": "",
            "property_name": "",
            "pii_classification": {},
            "enrichment_rationale": "",
            "mapping_rationale": "",
            "vector_store": vector_store,
            "objects_dict": objects_dict,
            "all_objects": all_objects,
            "all_properties": all_properties,
            "expert_opinions": [],
            "reasoning_chain": []
        }
        
        # Run the workflow
        final_state = app.invoke(initial_state)
        
        # Extract PII classification details
        pii_class = final_state['pii_classification']
        
        # Create result record
        result = {
            "EIM ID": record.get("EIM ID", ""),
            "Application Name": record.get("Application Name", ""),
            "Application Description": record.get("Application Description", ""),
            "Original Field Name": record.get("Field Name", ""),
            "Enriched Field Name": final_state['enriched_name'],
            "Enriched Description": final_state['enriched_description'],
            "Mapped Object": final_state['object_name'],
            "Mapped Property": final_state['property_name'],
            "PII Level": pii_class.get('pii_level', 0),
            "PII Category": pii_class.get('pii_category', ''),
            "PII Type": pii_class.get('pii_type', ''),
            "Is PII": pii_class.get('is_pii', False),
            "Is Sensitive PII": pii_class.get('is_sensitive_pii', False),
            "Is Direct Identifier": pii_class.get('is_direct_identifier', False),
            "Overall Risk": pii_class.get('risk_assessment', {}).get('overall_risk', ''),
            "PII Rationale": pii_class.get('detailed_rationale', ''),
            "Enrichment Rationale": final_state['enrichment_rationale'],
            "Mapping Rationale": final_state['mapping_rationale'],
            "Original ISR Classification": record.get("ISR Classification", "")
        }
        
        results.append(result)
    
    # Save to CSV
    print(f"\n[6] Saving results to CSV...")
    results_df = pd.DataFrame(results)
    results_df.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"   → Results saved to {OUTPUT_CSV_PATH}")
    
    print("\n" + "="*80)
    print("PROCESSING COMPLETE!")
    print("="*80)
    print(f"\nTotal records processed: {len(results)}")
    print(f"Output file: {OUTPUT_CSV_PATH}")


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    process_data_enrichment_and_mapping()
