"""
Semantic Matching Script for CSV Files
This script performs semantic matching between two CSV files using Azure OpenAI embeddings.
"""

import os
import time
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import faiss
from sklearn.preprocessing import normalize
from openai import AzureOpenAI
from azure.identity import ClientSecretCredential
from tqdm import tqdm
import json

# Set up logging with both file and console handlers
def setup_logging():
    """Configure logging with both file and console output."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"semantic_matching_{time.strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert string to boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

class OSEnv:
    """Environment and certificate management class."""
    
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        """Initialize with configuration files and certificate path."""
        self.var_list = []
        
        # Load configurations
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        
        # Set up certificates
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")
        
        # Configure proxy if enabled
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")
        
        # Set up Azure token if secure endpoints enabled
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        """Set up the certificate path for SSL verification."""
        try:
            if is_file_readable(certificate_path):
                cert_path = str(Path(certificate_path))
                self.set("REQUESTS_CA_BUNDLE", cert_path)
                self.set("SSL_CERT_FILE", cert_path)
                self.set("CURL_CA_BUNDLE", cert_path)
                logger.info(f"Certificate path set to: {cert_path}")
        except Exception as e:
            logger.error(f"Certificate configuration failed: {str(e)}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        """Read and set environment variables from a dotenv file."""
        try:
            if is_file_readable(dotenvfile):
                logger.info(f"Loading environment variables from {dotenvfile}")
                with open(dotenvfile) as f:
                    lines = f.readlines()
                
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip("'").strip('"')
                            self.set(key, value, print_val)
                        except ValueError:
                            continue
                            
                logger.info(f"Successfully loaded variables from {dotenvfile}")
                
        except Exception as e:
            logger.error(f"Failed to load environment file {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        """Set an environment variable."""
        try:
            os.environ[var_name] = val
            if var_name not in self.var_list:
                self.var_list.append(var_name)
            if print_val and var_name not in ['AZURE_CLIENT_SECRET', 'AD_USER_PW']:
                logger.info(f"Set {var_name}={val}")
        except Exception as e:
            logger.error(f"Failed to set environment variable {var_name}: {str(e)}")
            raise

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        """Get an environment variable value."""
        return os.getenv(var_name, default)

    def set_proxy(self) -> None:
        """Set up proxy configuration with authentication."""
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Missing proxy credentials")
            
            proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            
            # Set no_proxy for Azure services
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains))
            
            logger.info("Proxy configuration completed")
            
        except Exception as e:
            logger.error(f"Proxy configuration failed: {str(e)}")
            raise

    def get_azure_token(self) -> str:
        """Get Azure authentication token."""
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token acquired successfully")
            return token.token
            
        except Exception as e:
            logger.error(f"Failed to get Azure token: {str(e)}")
            raise

class SemanticMatcher:
    """Handles semantic matching between CSV entries using embeddings."""
    
    def __init__(self, env_setup: OSEnv):
        """Initialize with environment setup."""
        self.env = env_setup
        self._setup_openai_client()
        
    def _setup_openai_client(self):
        """Configure OpenAI client with Azure settings."""
        self.client = AzureOpenAI(
            api_key=self.env.token,
            api_version=self.env.get("API_VERSION", "2024-02-01"),
            azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT")
        )

    def _prepare_text(self, name: str, description: str, max_length: int = 6000) -> str:
        """Combine name and description for embedding."""
        # Ensure name is included by limiting description length
        max_desc_length = max_length - len(name) - 2  # 2 for ": "
        if len(description) > max_desc_length:
            logger.warning(f"Truncating description for {name} from {len(description)} to {max_desc_length} characters")
            description = description[:max_desc_length] + "..."
        return f"{name}: {description}".strip()

    def load_csv_data(self, source_csv: str, target_csv: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load and validate CSV files."""
        def try_read_csv(file_path: str) -> pd.DataFrame:
            """Try different encodings to read CSV file."""
            encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
            errors = []
            
            for encoding in encodings:
                try:
                    logger.info(f"Trying to read {file_path} with {encoding} encoding...")
                    return pd.read_csv(file_path, encoding=encoding, engine='python')
                except Exception as e:
                    errors.append(f"{encoding}: {str(e)}")
                    continue
            
            error_msg = f"Failed to read {file_path} with any encoding. Errors:\n" + "\n".join(errors)
            logger.error(error_msg)
            raise ValueError(error_msg)

        try:
            source_df = try_read_csv(source_csv)
            target_df = try_read_csv(target_csv)
            
            # Validate columns
            if not {'name', 'description'}.issubset(source_df.columns):
                raise ValueError("Source CSV must have 'name' and 'description' columns")
            if not {'pbt-name', 'pbt-description'}.issubset(target_df.columns):
                raise ValueError("Target CSV must have 'pbt-name' and 'pbt-description' columns")
            
            # Clean data
            for df in [source_df, target_df]:
                for col in df.columns:
                    df[col] = df[col].fillna('')
                    df[col] = df[col].astype(str).str.strip()
            
            logger.info(f"\nLoaded CSV files:")
            logger.info(f"Source: {len(source_df)} entries from {source_csv}")
            logger.info(f"Target: {len(target_df)} entries from {target_csv}")
            
            return source_df, target_df
            
        except Exception as e:
            logger.error(f"Failed to load CSV data: {str(e)}")
            raise

    def process_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame, 
                       similarity_threshold: float = 0.75,
                       batch_size: int = 10) -> List[Dict]:
        """Process and find matches between dataframes with batched processing."""
        try:
            matches = []
            
            # Prepare target texts
            logger.info("Preparing target data...")
            target_texts = [
                self._prepare_text(row['pbt-name'], row['pbt-description'])
                for _, row in target_df.iterrows()
            ]
            
            # Process target embeddings in batches
            logger.info("Generating embeddings for target texts...")
            target_embeddings = []
            for i in tqdm(range(0, len(target_texts), batch_size), desc="Processing target texts"):
                batch = target_texts[i:i + batch_size]
                try:
                    response = self.client.embeddings.create(
                        model="text-embedding-ada-002",
                        input=batch
                    )
                    target_embeddings.extend([d.embedding for d in response.data])
                except Exception as e:
                    logger.error(f"Error processing batch {i//batch_size + 1}: {str(e)}")
                    raise

            # Convert to numpy array and normalize
            target_embeddings_array = np.array(target_embeddings).astype('float32')
            target_embeddings_array = normalize(target_embeddings_array)
            
            # Build FAISS index
            dimension = len(target_embeddings[0])
            index = faiss.IndexFlatIP(dimension)
            index.add(target_embeddings_array)
            
            # Process source texts in batches
            logger.info("Processing source texts and finding matches...")
            for i in tqdm(range(0, len(source_df), batch_size), desc="Processing source texts"):
                batch_df = source_df.iloc[i:i + batch_size]
                source_texts = [
                    self._prepare_text(row['name'], row['description'])
                    for _, row in batch_df.iterrows()
                ]
                
                try:
                    # Get embeddings for batch
                    response = self.client.embeddings.create(
                        model="text-embedding-ada-002",
                        input=source_texts
                    )
                    source_embeddings = [d.embedding for d in response.data]
                    
                    # Convert to numpy and normalize
                    source_embeddings_array = np.array(source_embeddings).astype('float32')
                    source_embeddings_array = normalize(source_embeddings_array)
                    
                    # Search in FAISS index
                    k = 3  # Number of nearest neighbors
                    similarities, indices = index.search(source_embeddings_array, k)
                    
                    # Process results for this batch
                    for j, (sims, idxs) in enumerate(zip(similarities, indices)):
                        source_row = batch_df.iloc[j]
                        for sim, idx in zip(sims, idxs):
                            if sim >= similarity_threshold:
                                matches.append({
                                    'source_name': source_row['name'],
                                    'source_description': source_row['description'],
                                    'target_name': target_df.iloc[idx]['pbt-name'],
                                    'target_description': target_df.iloc[idx]['pbt-description'],
                                    'similarity_score': float(sim)
                                })
                
                except Exception as e:
                    logger.error(f"Error processing batch {i//batch_size + 1}: {str(e)}")
                    raise
            
            # Sort matches by similarity score
            matches.sort(key=lambda x: x['similarity_score'], reverse=True)
            logger.info(f"Found {len(matches)} matches above threshold")
            return matches
            
        except Exception as e:
            logger.error(f"Failed to process matches: {str(e)}")
            raise

    def save_results(self, matches: List[Dict], output_file: str) -> None:
        """Save matches to output files."""
        try:
            # Create output directory if it doesn't exist
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            
            # Convert to DataFrame for CSV
            results_df = pd.DataFrame(matches)
            results_df.to_csv(output_file, index=False)
            
            # Save detailed JSON
            json_file = output_file.rsplit('.', 1)[0] + '.json'
            with open(json_file, 'w', encoding='
