"""
Enhanced Legal Document Analyzer with Graph RAG, Vector Embeddings & Semantic Search
Complete implementation with vector storage, semantic search, and graph reasoning
Location: src/analyzers/legal_document_analyzer_enhanced.py
"""

from typing import Dict, List, Optional, Any, TypedDict, Annotated, Tuple
import json
from dataclasses import dataclass
import logging
import re
import operator
import os
import math

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langchain_community.graphs import FalkorDBGraph

from openai import OpenAI

from src.utils.document_chunker import DocumentChunker
from src.config import Config

logger = logging.getLogger(__name__)


# ============================================================================
# EMBEDDING SERVICE (Direct OpenAI API)
# ============================================================================

class EmbeddingService:
    """Direct OpenAI embedding service"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-large"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.dimension = 3072
    
    def embed_text(self, text: str) -> List[float]:
        """Generate embedding for single text"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return [0.0] * self.dimension
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        try:
            response = self.client.embeddings.create(
                input=texts,
                model=self.model
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Batch embedding error: {e}")
            return [[0.0] * self.dimension for _ in texts]


# ============================================================================
# FALKORDB KNOWLEDGE GRAPH WITH VECTOR EMBEDDINGS & GRAPH RAG
# ============================================================================

class FalkorDBKnowledgeGraph:
    """FalkorDB with vector embeddings, semantic search, and Graph RAG"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, 
                 graph_name: str = "legal_knowledge_graph",
                 embedding_service: EmbeddingService = None):
        """Initialize FalkorDB with vector capabilities"""
        try:
            self.graph = FalkorDBGraph(
                database=graph_name,
                host=host,
                port=port
            )
            self.graph_name = graph_name
            self.embedding_service = embedding_service
            self.embedding_dim = 3072 if embedding_service else 0
            
            # Statistics
            self.stats = {
                "requirements": 0,
                "actions": 0,
                "evidence": 0,
                "constraints": 0
            }
            
            # Create vector index for semantic search
            self._create_vector_indexes()
            
            logger.info(f"Connected to FalkorDB with vector embeddings: {host}:{port}/{graph_name}")
            
        except Exception as e:
            logger.error(f"FalkorDB connection error: {e}")
            self.graph = None
            self.stats = {}
    
    def _create_vector_indexes(self):
        """Create indexes for vector search and full-text search"""
        if not self.graph:
            return
        
        try:
            # Standard indexes for efficient querying
            self.graph.query("CREATE INDEX FOR (r:Requirement) ON (r.id)")
            self.graph.query("CREATE INDEX FOR (a:Action) ON (a.type)")
            self.graph.query("CREATE INDEX FOR (e:Evidence) ON (e.perspective)")
            self.graph.query("CREATE INDEX FOR (c:Constraint) ON (c.type)")
            
            # Full-text indexes
            self.graph.query("CREATE FULLTEXT INDEX FOR (r:Requirement) ON (r.description)")
            self.graph.query("CREATE FULLTEXT INDEX FOR (a:Action) ON (a.description)")
            self.graph.query("CREATE FULLTEXT INDEX FOR (e:Evidence) ON (e.description)")
            
            logger.info("Created vector and full-text indexes")
        except Exception as e:
            logger.warning(f"Index creation warning (may already exist): {e}")
    
    def _escape_text(self, text: str) -> str:
        """Escape text for Cypher query"""
        if not text:
            return ""
        text = text.replace("'", "\\'")
        text = text.replace('"', '\\"')
        text = text.replace('\n', ' ')
        return text[:2000]
    
    def _serialize_embedding(self, embedding: List[float]) -> str:
        """Serialize embedding vector for storage (store subset)"""
        if not embedding or len(embedding) == 0:
            return ""
        # Store first 100 dimensions as representative sample
        sample = embedding[:100]
        return ",".join([f"{x:.6f}" for x in sample])
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def add_requirement(self, req_id: str, description: str, level: int,
                       classification: str, citations: List[str],
                       embedding: Optional[List[float]] = None):
        """Add requirement node with vector embedding"""
        if not self.graph:
            self.stats["requirements"] = self.stats.get("requirements", 0) + 1
            return
        
        try:
            # Generate embedding if not provided
            if embedding is None and self.embedding_service and description:
                embedding = self.embedding_service.embed_text(description)
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:3]])
            
            # Store embedding sample
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            
            query = f"""
            CREATE (r:Requirement {{
                id: '{req_id}',
                description: '{desc_escaped}',
                level: {level},
                classification: '{classification}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {1 if embedding_str else 0},
                timestamp: timestamp()
            }})
            RETURN r.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["requirements"] = self.stats.get("requirements", 0) + 1
                logger.info(f"Created requirement node with embedding: {req_id}")
                
                # Store full embedding separately for semantic search
                if embedding:
                    self._store_full_embedding(req_id, "Requirement", embedding)
            
        except Exception as e:
            logger.error(f"Error adding requirement {req_id}: {e}")
    
    def add_action(self, action_id: str, action_type: str, description: str,
                  actor: str, req_id: str, citations: List[str],
                  embedding: Optional[List[float]] = None):
        """Add action node with vector embedding"""
        if not self.graph:
            self.stats["actions"] = self.stats.get("actions", 0) + 1
            return
        
        try:
            # Generate embedding
            if embedding is None and self.embedding_service and description:
                embedding = self.embedding_service.embed_text(description)
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:2]])
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (a:Action {{
                id: '{action_id}',
                type: '{action_type}',
                description: '{desc_escaped}',
                actor: '{actor}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {1 if embedding_str else 0},
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_ACTION]->(a)
            RETURN a.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["actions"] = self.stats.get("actions", 0) + 1
                logger.info(f"Created action node with embedding: {action_id}")
                
                if embedding:
                    self._store_full_embedding(action_id, "Action", embedding)
            
        except Exception as e:
            logger.error(f"Error adding action {action_id}: {e}")
    
    def add_evidence(self, evidence_id: str, evidence_type: str, 
                    description: str, perspective: str, req_id: str,
                    citations: List[str], embedding: Optional[List[float]] = None):
        """Add evidence node with vector embedding"""
        if not self.graph:
            self.stats["evidence"] = self.stats.get("evidence", 0) + 1
            return
        
        try:
            if embedding is None and self.embedding_service and description:
                embedding = self.embedding_service.embed_text(description)
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:2]])
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (e:Evidence {{
                id: '{evidence_id}',
                type: '{evidence_type}',
                description: '{desc_escaped}',
                perspective: '{perspective}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {1 if embedding_str else 0},
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_EVIDENCE]->(e)
            RETURN e.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["evidence"] = self.stats.get("evidence", 0) + 1
                logger.info(f"Created evidence node with embedding: {evidence_id}")
                
                if embedding:
                    self._store_full_embedding(evidence_id, "Evidence", embedding)
            
        except Exception as e:
            logger.error(f"Error adding evidence {evidence_id}: {e}")
    
    def add_constraint(self, constraint_id: str, constraint_type: str,
                      description: str, operator: str, value: Any, req_id: str,
                      embedding: Optional[List[float]] = None):
        """Add constraint node with vector embedding"""
        if not self.graph:
            self.stats["constraints"] = self.stats.get("constraints", 0) + 1
            return
        
        try:
            if embedding is None and self.embedding_service and description:
                embedding = self.embedding_service.embed_text(description)
            
            desc_escaped = self._escape_text(description)
            value_str = self._escape_text(str(value))
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (c:Constraint {{
                id: '{constraint_id}',
                type: '{constraint_type}',
                description: '{desc_escaped}',
                operator: '{operator}',
                value: '{value_str}',
                embedding: '{embedding_str}',
                has_embedding: {1 if embedding_str else 0},
                timestamp: timestamp()
            }})
            CREATE (r)-[:HAS_CONSTRAINT]->(c)
            RETURN c.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["constraints"] = self.stats.get("constraints", 0) + 1
                logger.info(f"Created constraint node with embedding: {constraint_id}")
                
                if embedding:
                    self._store_full_embedding(constraint_id, "Constraint", embedding)
            
        except Exception as e:
            logger.error(f"Error adding constraint {constraint_id}: {e}")
    
    def _store_full_embedding(self, node_id: str, node_type: str, embedding: List[float]):
        """Store full embedding vector in separate embedding cache (in-memory for now)"""
        if not hasattr(self, '_embedding_cache'):
            self._embedding_cache = {}
        
        cache_key = f"{node_type}:{node_id}"
        self._embedding_cache[cache_key] = embedding
        logger.debug(f"Cached full embedding for {cache_key}")
    
    def _get_full_embedding(self, node_id: str, node_type: str) -> Optional[List[float]]:
        """Retrieve full embedding vector"""
        if not hasattr(self, '_embedding_cache'):
            return None
        
        cache_key = f"{node_type}:{node_id}"
        return self._embedding_cache.get(cache_key)
    
    def semantic_search(self, query_text: str, node_types: List[str] = None, 
                       top_k: int = 5) -> List[Dict[str, Any]]:
        """Semantic search using vector embeddings"""
        if not self.embedding_service or not self.graph:
            return []
        
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.embed_text(query_text)
            
            if node_types is None:
                node_types = ["Requirement", "Action", "Evidence", "Constraint"]
            
            results = []
            
            for node_type in node_types:
                # Get all nodes with embeddings
                query = f"""
                MATCH (n:{node_type})
                WHERE n.has_embedding = 1
                RETURN n.id AS id, n.description AS description, 
                       n.embedding AS embedding_sample
                LIMIT 100
                """
                
                nodes = self.graph.query(query)
                
                if not nodes:
                    continue
                
                # Calculate similarity scores
                for node_data in nodes:
                    # Handle different result formats
                    if isinstance(node_data, dict):
                        node_id = node_data.get('id', '')
                        description = node_data.get('description', '')
                    elif isinstance(node_data, (list, tuple)) and len(node_data) >= 2:
                        node_id = node_data[0]
                        description = node_data[1]
                    else:
                        continue
                    
                    # Get full embedding from cache
                    node_embedding = self._get_full_embedding(node_id, node_type)
                    
                    if node_embedding:
                        similarity = self._cosine_similarity(query_embedding, node_embedding)
                        
                        results.append({
                            "node_id": node_id,
                            "node_type": node_type,
                            "description": description,
                            "similarity_score": similarity
                        })
            
            # Sort by similarity and return top_k
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Semantic search error: {e}")
            return []
    
    def full_text_search(self, query_text: str, node_types: List[str] = None,
                        top_k: int = 5) -> List[Dict[str, Any]]:
        """Full-text search on node descriptions"""
        if not self.graph:
            return []
        
        try:
            if node_types is None:
                node_types = ["Requirement", "Action", "Evidence"]
            
            results = []
            
            for node_type in node_types:
                # Simple text matching (FalkorDB may not support full FULLTEXT yet)
                # Using CONTAINS for text matching
                query_escaped = self._escape_text(query_text)
                
                query = f"""
                MATCH (n:{node_type})
                WHERE n.description CONTAINS '{query_escaped}'
                RETURN n.id AS id, n.description AS description,
                       n.level AS level, n.type AS type
                LIMIT {top_k}
                """
                
                nodes = self.graph.query(query)
                
                if not nodes:
                    continue
                
                for node_data in nodes:
                    if isinstance(node_data, dict):
                        results.append({
                            "node_id": node_data.get('id', ''),
                            "node_type": node_type,
                            "description": node_data.get('description', ''),
                            "level": node_data.get('level'),
                            "search_method": "full_text"
                        })
                    elif isinstance(node_data, (list, tuple)) and len(node_data) >= 2:
                        results.append({
                            "node_id": node_data[0],
                            "node_type": node_type,
                            "description": node_data[1],
                            "level": node_data[2] if len(node_data) > 2 else None,
                            "search_method": "full_text"
                        })
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Full-text search error: {e}")
            return []
    
    def graph_rag_search(self, query_text: str, top_k: int = 5,
                        include_related: bool = True) -> Dict[str, Any]:
        """Graph RAG: Combines semantic search + graph traversal + full-text search"""
        if not self.graph:
            return {"results": [], "related_nodes": [], "graph_paths": []}
        
        try:
            # 1. Semantic search for most relevant nodes
            semantic_results = self.semantic_search(query_text, top_k=top_k)
            
            # 2. Full-text search for keyword matches
            fulltext_results = self.full_text_search(query_text, top_k=top_k)
            
            # 3. Combine and deduplicate
            combined_results = {}
            
            for result in semantic_results:
                node_id = result['node_id']
                combined_results[node_id] = {
                    **result,
                    "search_method": "semantic",
                    "score": result['similarity_score']
                }
            
            for result in fulltext_results:
                node_id = result['node_id']
                if node_id not in combined_results:
                    combined_results[node_id] = {
                        **result,
                        "search_method": "full_text",
                        "score": 0.5  # Default score for text match
                    }
            
            # 4. Graph traversal to find related nodes
            related_nodes = []
            graph_paths = []
            
            if include_related:
                for node_id in list(combined_results.keys())[:3]:  # Top 3 results
                    # Find connected nodes
                    related = self._find_related_nodes(node_id, max_depth=2)
                    related_nodes.extend(related)
                    
                    # Find graph paths
                    paths = self._find_citation_paths(node_id)
                    graph_paths.extend(paths)
            
            # 5. Sort final results by score
            final_results = sorted(
                combined_results.values(),
                key=lambda x: x['score'],
                reverse=True
            )
            
            return {
                "results": final_results[:top_k],
                "related_nodes": related_nodes[:10],
                "graph_paths": graph_paths[:5],
                "search_methods": ["semantic", "full_text", "graph_traversal"]
            }
            
        except Exception as e:
            logger.error(f"Graph RAG search error: {e}")
            return {"results": [], "related_nodes": [], "graph_paths": []}
    
    def _find_related_nodes(self, node_id: str, max_depth: int = 2) -> List[Dict[str, Any]]:
        """Find nodes connected to given node through graph traversal"""
        if not self.graph:
            return []
        
        try:
            query = f"""
            MATCH path = (n {{id: '{node_id}'}})-[*1..{max_depth}]-(related)
            RETURN DISTINCT related.id AS id, 
                   labels(related)[0] AS type,
                   related.description AS description,
                   length(path) AS distance
            LIMIT 10
            """
            
            result = self.graph.query(query)
            
            related = []
            if result:
                for row in result:
                    if isinstance(row, dict):
                        related.append({
                            "node_id": row.get('id', ''),
                            "node_type": row.get('type', ''),
                            "description": row.get('description', ''),
                            "distance": row.get('distance', 0)
                        })
                    elif isinstance(row, (list, tuple)) and len(row) >= 3:
                        related.append({
                            "node_id": row[0],
                            "node_type": row[1],
                            "description": row[2],
                            "distance": row[3] if len(row) > 3 else 0
                        })
            
            return related
            
        except Exception as e:
            logger.error(f"Error finding related nodes: {e}")
            return []
    
    def _find_citation_paths(self, node_id: str) -> List[Dict[str, Any]]:
        """Find citation paths through the graph"""
        if not self.graph:
            return []
        
        try:
            query = f"""
            MATCH path = (n {{id: '{node_id}'}})-[:REQUIRES_ACTION|REQUIRES_EVIDENCE|HAS_CONSTRAINT*1..3]-(related)
            RETURN [node IN nodes(path) | {{id: node.id, type: labels(node)[0]}}] AS path_nodes,
                   [rel IN relationships(path) | type(rel)] AS path_rels
            LIMIT 5
            """
            
            result = self.graph.query(query)
            
            paths = []
            if result:
                for row in result:
                    if isinstance(row, dict):
                        paths.append({
                            "nodes": row.get('path_nodes', []),
                            "relationships": row.get('path_rels', [])
                        })
                    elif isinstance(row, (list, tuple)) and len(row) >= 2:
                        paths.append({
                            "nodes": row[0],
                            "relationships": row[1]
                        })
            
            return paths
            
        except Exception as e:
            logger.error(f"Error finding citation paths: {e}")
            return []
    
    def find_supporting_citations(self, claim: str, node_id: str = None) -> List[Dict[str, Any]]:
        """Find citations that support a specific claim using Graph RAG"""
        if not self.graph:
            return []
        
        try:
            # Use Graph RAG to find relevant nodes
            rag_results = self.graph_rag_search(claim, top_k=5, include_related=True)
            
            citations = []
            
            # Extract citations from results
            for result in rag_results.get('results', []):
                node_id_result = result.get('node_id', '')
                
                # Get the node's citations
                query = f"""
                MATCH (n {{id: '{node_id_result}'}})
                RETURN n.citations AS citations, n.description AS description
                """
                
                node_data = self.graph.query(query)
                
                if node_data:
                    row = node_data[0]
                    if isinstance(row, dict):
                        citation_str = row.get('citations', '')
                        description = row.get('description', '')
                    elif isinstance(row, (list, tuple)) and len(row) >= 2:
                        citation_str = row[0]
                        description = row[1]
                    else:
                        continue
                    
                    if citation_str:
                        # Parse citations
                        citation_texts = citation_str.split('|')
                        for cite_text in citation_texts:
                            if cite_text:
                                citations.append({
                                    "text": cite_text,
                                    "supporting_node": node_id_result,
                                    "node_description": description,
                                    "relevance_score": result.get('score', 0)
                                })
            
            # Sort by relevance
            citations.sort(key=lambda x: x['relevance_score'], reverse=True)
            return citations[:10]
            
        except Exception as e:
            logger.error(f"Error finding supporting citations: {e}")
            return []
    
    def validate_citation_consistency(self, node_id: str) -> Dict[str, Any]:
        """Validate that citations are consistent across related nodes"""
        if not self.graph:
            return {"consistent": True, "conflicts": []}
        
        try:
            # Get node and its related nodes
            related = self._find_related_nodes(node_id, max_depth=1)
            
            # Get citations from all nodes
            all_citations = {}
            
            query = f"""
            MATCH (n {{id: '{node_id}'}})
            RETURN n.citations AS citations, n.description AS description
            """
            
            result = self.graph.query(query)
            
            if result:
                row = result[0]
                if isinstance(row, dict):
                    all_citations[node_id] = row.get('citations', '')
                elif isinstance(row, (list, tuple)):
                    all_citations[node_id] = row[0]
            
            # Compare citations
            conflicts = []
            # Simple validation - check if related nodes have contradicting info
            
            return {
                "consistent": len(conflicts) == 0,
                "conflicts": conflicts,
                "related_nodes_checked": len(related)
            }
            
        except Exception as e:
            logger.error(f"Error validating citations: {e}")
            return {"consistent": True, "conflicts": [], "error": str(e)}
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get graph statistics - FIXED for FalkorDB result structure"""
        if not self.graph:
            return self.stats
        
        try:
            stats = {}
            
            # Count each node type
            for node_type in ["Requirement", "Action", "Evidence", "Constraint"]:
                try:
                    query = f"MATCH (n:{node_type}) RETURN count(n) AS count"
                    result = self.graph.query(query)
                    
                    # FalkorDB returns list of lists or list of Row objects
                    if result and len(result) > 0:
                        first_row = result[0]
                        
                        # Handle different result types
                        if isinstance(first_row, dict):
                            count_value = first_row.get('count', 0)
                        elif isinstance(first_row, (list, tuple)):
                            count_value = first_row[0] if len(first_row) > 0 else 0
                        elif hasattr(first_row, 'count'):
                            count_value = first_row.count
                        else:
                            # Try to access as attribute or index
                            try:
                                count_value = first_row[0]
                            except:
                                count_value = 0
                        
                        stats[node_type.lower()] = int(count_value) if count_value else 0
                    else:
                        stats[node_type.lower()] = 0
                        
                except Exception as e:
                    logger.warning(f"Error counting {node_type}: {e}")
                    stats[node_type.lower()] = 0
            
            # Count relationships
            try:
                query = "MATCH ()-[r]->() RETURN count(r) AS count"
                result = self.graph.query(query)
                
                if result and len(result) > 0:
                    first_row = result[0]
                    
                    # Handle different result types
                    if isinstance(first_row, dict):
                        count_value = first_row.get('count', 0)
                    elif isinstance(first_row, (list, tuple)):
                        count_value = first_row[0] if len(first_row) > 0 else 0
                    elif hasattr(first_row, 'count'):
                        count_value = first_row.count
                    else:
                        try:
                            count_value = first_row[0]
                        except:
                            count_value = 0
                    
                    stats['relationships'] = int(count_value) if count_value else 0
                else:
                    stats['relationships'] = 0
                    
            except Exception as e:
                logger.warning(f"Error counting relationships: {e}")
                stats['relationships'] = 0
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return self.stats


# ============================================================================
# STATE DEFINITIONS FOR LANGGRAPH
# ============================================================================

class AgentState(TypedDict):
    """State for LangGraph agent"""
    messages: Annotated[List[BaseMessage], operator.add]
    chunk_text: str
    chunk_id: int
    level: int
    rule_name: str
    jurisdiction: str
    enterprise_context: Optional[Dict[str, Any]]
    
    description: str
    citations: List[Dict[str, Any]]
    data_actions: List[Dict[str, Any]]
    user_evidence: List[Dict[str, Any]]
    system_evidence: List[Dict[str, Any]]
    constraints: List[Dict[str, Any]]
    enterprise_policies: List[Dict[str, Any]]
    classification: str
    classification_reasoning: str
    
    chain_of_thought: List[str]
    expert_opinions: Dict[str, Any]
    thought_tree: Dict[str, Any]
    
    kg_nodes: List[Dict[str, Any]]
    kg_edges: List[Dict[str, Any]]
    
    next_step: str
    iteration: int
    max_iterations: int


# ============================================================================
# LANGGRAPH AGENT NODES (FIXED ROUTING)
# ============================================================================

class LegalAnalysisAgent:
    """LangGraph agent with advanced reasoning - FIXED"""
    
    def __init__(self, llm: ChatOpenAI, kg: FalkorDBKnowledgeGraph):
        self.llm = llm
        self.kg = kg
    
    def chain_of_thought_reasoning(self, state: AgentState) -> AgentState:
        """Chain of Thought reasoning node"""
        level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
        level_name = level_names.get(state['level'], "Document")
        
        prompt = f"""Analyze this legal text from {level_name} using systematic step-by-step reasoning.

TEXT:
{state['chunk_text'][:2500]}

Rule: {state['rule_name']}
Jurisdiction: {state['jurisdiction']}
Document Level: Level {state['level']} - {level_name}

IMPORTANT NOTES:
- "DataVisa" is an internal data governance tool
- Ignore classification markers like "INTERNAL" at document end

Think through systematically:
1. What is the core legal requirement stated in this text?
2. Who is affected - users performing actions, or systems that must be implemented?
3. What specific data operations are mentioned (sharing/accessing, storing/hosting, or using/processing)?
4. What conditions or limitations apply?
5. Is this describing actions that ARE allowed under conditions (CONDITION), or actions that are NOT allowed (RESTRICTION)?

For each point, provide exact citations from the text (max 150 characters each) with source reference.

Return valid JSON:
{{
    "thought_steps": ["clear step 1", "clear step 2", "clear step 3", "clear step 4", "clear step 5"],
    "main_requirement": "Complete sentence describing the requirement",
    "affected_parties": ["user", "system"],
    "data_operations": ["specific operation"],
    "conditions": ["condition 1", "condition 2"],
    "classification": "condition" or "restriction",
    "citations": [
        {{
            "text": "exact quote from text",
            "reasoning": "why this supports the requirement",
            "source_reference": "Level {state['level']} - {level_name}, Section/Article reference if visible"
        }}
    ]
}}"""
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        try:
            result = self._extract_json(response.content)
            state['chain_of_thought'] = result.get('thought_steps', [])
            
            if result.get('main_requirement'):
                state['description'] += result['main_requirement'] + " "
            
            for cite in result.get('citations', []):
                if cite.get('text') and len(cite['text']) > 20:
                    state['citations'].append({
                        "text": cite['text'],
                        "reasoning": cite.get('reasoning', ''),
                        "source_reference": cite.get('source_reference', f"Level {state['level']} - {level_name}"),
                        "chunk_id": state['chunk_id'],
                        "level": state['level'],
                        "level_name": level_name
                    })
            
            if not state.get('classification') and result.get('classification'):
                state['classification'] = result['classification']
            
        except Exception as e:
            logger.error(f"Chain of thought error: {e}")
            state['chain_of_thought'].append(f"Error in reasoning: {str(e)}")
        
        state['next_step'] = 'mixture_of_experts'
        return state
    
    def mixture_of_experts(self, state: AgentState) -> AgentState:
        """Mixture of Experts reasoning"""
        level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
        level_name = level_names.get(state['level'], "Document")
        
        experts = {
            "legal_expert": "Extract legal obligations with precise citations",
            "data_privacy_specialist": "Identify data handling operations",
            "technical_architect": "Determine system requirements",
            "compliance_officer": "Identify user compliance actions"
        }
        
        expert_results = {}
        
        for expert_role, expert_task in experts.items():
            prompt = f"""You are a {expert_role.replace('_', ' ').title()}.

Task: {expert_task}

TEXT (from {level_name}):
{state['chunk_text'][:2000]}

Rule: {state['rule_name']}

Provide JSON:
{{
    "key_findings": ["complete finding 1", "complete finding 2"],
    "data_actions": [
        {{
            "type": "data_sharing_and_access" or "data_storage_and_hosting" or "data_usage",
            "description": "Complete sentence",
            "actor": "user" or "system",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ],
    "requirements": [
        {{
            "description": "Complete sentence",
            "perspective": "user" or "system",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ],
    "constraints": [
        {{
            "type": "temporal" or "technical" or "procedural",
            "description": "Complete sentence",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ]
}}"""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                expert_results[expert_role] = self._extract_json(response.content)
            except Exception as e:
                logger.error(f"Expert {expert_role} error: {e}")
                expert_results[expert_role] = {"error": str(e)}
        
        state['expert_opinions'] = expert_results
        self._consolidate_expert_findings(state, expert_results, level_name)
        
        state['next_step'] = 'tree_of_thought'
        return state
    
    def tree_of_thought(self, state: AgentState) -> AgentState:
        """Tree of Thought exploration"""
        prompt = f"""Explore alternative interpretations of this rule.

TEXT:
{state['chunk_text'][:2000]}

Current understanding:
- Description: {state['description'][:300]}
- Actions: {len(state['data_actions'])}
- Evidence: {len(state['user_evidence']) + len(state['system_evidence'])}

Generate 3 interpretation paths:

Path 1 - Conservative: Strictest reading
Path 2 - Balanced: Reasonable middle-ground
Path 3 - Expansive: Broadest reasonable interpretation

Return JSON:
{{
    "paths": [
        {{
            "name": "conservative",
            "interpretation": "Complete explanation",
            "implications": ["implication 1", "implication 2"],
            "required_actions": ["action 1", "action 2"],
            "confidence": "high" or "medium" or "low"
        }}
    ],
    "recommended_path": "conservative" or "balanced" or "expansive",
    "reasoning": "Complete explanation"
}}"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = self._extract_json(response.content)
            state['thought_tree'] = result
            
            state['chain_of_thought'].append(
                f"Explored {len(result.get('paths', []))} interpretation paths. "
                f"Recommended: {result.get('recommended_path', 'balanced')}"
            )
        except Exception as e:
            logger.error(f"Tree of thought error: {e}")
            state['thought_tree'] = {"error": str(e)}
        
        # FIXED: Correct next step name
        state['next_step'] = 'knowledge_graph'
        return state
    
    def knowledge_graph_integration(self, state: AgentState) -> AgentState:
        """Integrate into FalkorDB with vector embeddings and validation"""
        chunk_id = state['chunk_id']
        level = state['level']
        
        req_id = f"req_{state['rule_name']}_{level}_{chunk_id}".replace(" ", "_").replace("/", "_")
        
        # Generate embedding for requirement
        req_embedding = None
        if self.kg.embedding_service and state['description']:
            req_embedding = self.kg.embedding_service.embed_text(state['description'])
        
        # Add requirement node with embedding
        self.kg.add_requirement(
            req_id,
            state['description'],
            level,
            state['classification'],
            [c['text'] for c in state['citations'][:5]],
            embedding=req_embedding
        )
        
        # Add action nodes with embeddings
        for i, action in enumerate(state['data_actions']):
            action_id = f"{req_id}_action_{i}"
            action_desc = action.get('description', '')
            
            # Generate embedding for action
            action_embedding = None
            if self.kg.embedding_service and action_desc:
                action_embedding = self.kg.embedding_service.embed_text(action_desc)
            
            self.kg.add_action(
                action_id,
                action.get('type', 'data_usage'),
                action_desc,
                action.get('actor', 'unknown'),
                req_id,
                [c.get('text', '') for c in action.get('citations', [])],
                embedding=action_embedding
            )
        
        # Add user evidence nodes with embeddings
        for i, evidence in enumerate(state['user_evidence']):
            evidence_id = f"{req_id}_evidence_user_{i}"
            evidence_desc = evidence.get('description', '')
            
            evidence_embedding = None
            if self.kg.embedding_service and evidence_desc:
                evidence_embedding = self.kg.embedding_service.embed_text(evidence_desc)
            
            self.kg.add_evidence(
                evidence_id,
                'user_requirement',
                evidence_desc,
                'user',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])],
                embedding=evidence_embedding
            )
        
        # Add system evidence nodes with embeddings
        for i, evidence in enumerate(state['system_evidence']):
            evidence_id = f"{req_id}_evidence_system_{i}"
            evidence_desc = evidence.get('description', '')
            
            evidence_embedding = None
            if self.kg.embedding_service and evidence_desc:
                evidence_embedding = self.kg.embedding_service.embed_text(evidence_desc)
            
            self.kg.add_evidence(
                evidence_id,
                'system_requirement',
                evidence_desc,
                'system',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])],
                embedding=evidence_embedding
            )
        
        # Add constraint nodes with embeddings
        for i, constraint in enumerate(state['constraints']):
            constraint_id = f"{req_id}_constraint_{i}"
            constraint_desc = constraint.get('description', '')
            
            constraint_embedding = None
            if self.kg.embedding_service and constraint_desc:
                constraint_embedding = self.kg.embedding_service.embed_text(constraint_desc)
            
            self.kg.add_constraint(
                constraint_id,
                constraint.get('type', 'general'),
                constraint_desc,
                constraint.get('operator', 'eq'),
                constraint.get('right_operand', ''),
                req_id,
                embedding=constraint_embedding
            )
        
        # Validate citations using Graph RAG
        if state['description']:
            supporting_citations = self.kg.find_supporting_citations(
                state['description'], 
                node_id=req_id
            )
            
            if supporting_citations:
                state['chain_of_thought'].append(
                    f"Found {len(supporting_citations)} supporting citations through graph analysis"
                )
        
        # Get statistics
        stats = self.kg.get_statistics()
        state['kg_nodes'] = [{"type": k, "count": v} for k, v in stats.items()]
        state['chain_of_thought'].append(f"Graph integration with embeddings: {stats}")
        
        state['next_step'] = 'validate'
        return state
    
    def validate_and_refine(self, state: AgentState) -> AgentState:
        """Validation and refinement with Graph RAG verification"""
        issues = []
        
        if len(state['description']) < 100:
            issues.append("Description too short")
        
        if len(state['citations']) == 0:
            issues.append("No citations")
        
        if len(state['data_actions']) == 0:
            issues.append("No data actions")
        
        if len(state['user_evidence']) == 0 and len(state['system_evidence']) == 0:
            issues.append("No evidence")
        
        if not state.get('classification'):
            issues.append("No classification")
        
        # Use Graph RAG to validate citations
        if state['description'] and state['iteration'] == 0:
            # Search for similar requirements in the graph
            similar_reqs = self.kg.semantic_search(
                state['description'],
                node_types=["Requirement"],
                top_k=3
            )
            
            if similar_reqs:
                state['chain_of_thought'].append(
                    f"Found {len(similar_reqs)} semantically similar requirements for validation"
                )
                
                # Check for potential citation gaps
                for sim_req in similar_reqs:
                    if sim_req.get('similarity_score', 0) > 0.8:
                        state['chain_of_thought'].append(
                            f"High similarity ({sim_req['similarity_score']:.2f}) with {sim_req['node_id']}"
                        )
        
        if issues and state['iteration'] < state['max_iterations']:
            level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
            level_name = level_names.get(state['level'], "Document")
            
            # Use Graph RAG to find additional context
            rag_context = ""
            if state['description']:
                rag_results = self.kg.graph_rag_search(state['description'], top_k=2, include_related=False)
                if rag_results.get('results'):
                    rag_context = "\n\nRELATED FINDINGS FROM GRAPH:\n"
                    for result in rag_results['results']:
                        rag_context += f"- {result.get('description', '')[:150]}\n"
            
            prompt = f"""Refine this analysis to address: {', '.join(issues)}

Current state:
- Description: {state['description'][:400]}
- Citations: {len(state['citations'])}
- Actions: {len(state['data_actions'])}
{rag_context}

Original text (from {level_name}):
{state['chunk_text'][:2000]}

Extract what was missed. Include source references for all citations (Level {state['level']} - {level_name}). Return JSON."""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                refinements = self._extract_json(response.content)
                
                if refinements.get('description'):
                    state['description'] += " " + refinements['description']
                
                for cite in refinements.get('citations', []):
                    if cite.get('text'):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                        state['citations'].append(cite)
                
                for action in refinements.get('data_actions', []):
                    state['data_actions'].append(action)
                
                for evidence in refinements.get('user_evidence', []):
                    state['user_evidence'].append(evidence)
                
                for evidence in refinements.get('system_evidence', []):
                    state['system_evidence'].append(evidence)
                
                if refinements.get('classification') and not state.get('classification'):
                    state['classification'] = refinements['classification']
                
                state['iteration'] += 1
                state['next_step'] = 'validate'
            except Exception as e:
                logger.error(f"Refinement error: {e}")
                state['next_step'] = 'end'
        else:
            state['next_step'] = 'end'
        
        return state
    
    def _extract_json(self, text: str) -> Dict[str, Any]:
        """Extract JSON from response"""
        try:
            return json.loads(text)
        except:
            pass
        
        match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
        
        pattern = r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}'
        matches = re.findall(pattern, text, re.DOTALL)
        for match in sorted(matches, key=len, reverse=True):
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and len(parsed) > 2:
                    return parsed
            except:
                continue
        
        return {}
    
    def _consolidate_expert_findings(self, state: AgentState, expert_results: Dict[str, Any], level_name: str):
        """Consolidate expert findings"""
        for expert_role, results in expert_results.items():
            if 'error' in results:
                continue
            
            for action in results.get('data_actions', []):
                if action.get('description'):
                    # Ensure source reference
                    for cite in action.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['data_actions'].append(action)
            
            for req in results.get('requirements', []):
                if req.get('description'):
                    # Ensure source reference
                    for cite in req.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    
                    perspective = req.get('perspective', 'user')
                    if perspective == 'user':
                        state['user_evidence'].append(req)
                    else:
                        state['system_evidence'].append(req)
            
            for constraint in results.get('constraints', []):
                if constraint.get('description'):
                    # Ensure source reference
                    for cite in constraint.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['constraints'].append(constraint)


# ============================================================================
# ENHANCED ANALYZER (FIXED WORKFLOW)
# ============================================================================

class EnhancedLegalDocumentAnalyzer:
    """Enhanced analyzer with graph database - FIXED"""
    
    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        if not self.config.API_KEY:
            raise ValueError("OPENAI_API_KEY required")
        
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=self.config.CHAT_MODEL,
            openai_api_key=self.config.API_KEY,
            openai_api_base=self.config.BASE_URL
        )
        
        self.chunker = DocumentChunker(
            chunk_size=getattr(self.config, 'CHUNK_SIZE', 4000),
            chunk_overlap=getattr(self.config, 'OVERLAP_SIZE', 300),
            respect_boundaries=True
        )
        
        # Initialize embedding service
        self.embedding_service = EmbeddingService(
            api_key=self.config.API_KEY,
            model=self.config.EMBEDDING_MODEL
        )
        
        # Initialize FalkorDB
        self.kg = FalkorDBKnowledgeGraph(
            host=getattr(self.config, 'FALKORDB_HOST', 'localhost'),
            port=getattr(self.config, 'FALKORDB_PORT', 6379),
            embedding_service=self.embedding_service
        )
        
        self.agent = LegalAnalysisAgent(self.llm, self.kg)
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build LangGraph workflow - FIXED ROUTING"""
        workflow = StateGraph(AgentState)
        
        workflow.add_node("chain_of_thought", self.agent.chain_of_thought_reasoning)
        workflow.add_node("mixture_of_experts", self.agent.mixture_of_experts)
        workflow.add_node("tree_of_thought", self.agent.tree_of_thought)
        workflow.add_node("knowledge_graph", self.agent.knowledge_graph_integration)
        workflow.add_node("validate", self.agent.validate_and_refine)
        
        workflow.set_entry_point("chain_of_thought")
        
        # FIXED: Proper edge mappings
        workflow.add_conditional_edges(
            "chain_of_thought",
            lambda state: state['next_step'],
            {
                "mixture_of_experts": "mixture_of_experts",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "mixture_of_experts",
            lambda state: state['next_step'],
            {
                "tree_of_thought": "tree_of_thought",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "tree_of_thought",
            lambda state: state['next_step'],
            {
                "knowledge_graph": "knowledge_graph",  # FIXED: was "knowledge_graph_integration"
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "knowledge_graph",
            lambda state: state['next_step'],
            {
                "validate": "validate",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "validate",
            lambda state: state['next_step'],
            {
                "validate": "validate",
                "end": END
            }
        )
        
        return workflow.compile()
    
    def analyze_chunk(self, chunk: Dict[str, Any], rule_name: str,
                     jurisdiction: str, level: int,
                     enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze chunk with LangGraph"""
        
        initial_state = AgentState(
            messages=[],
            chunk_text=chunk['text'],
            chunk_id=chunk['chunk_id'],
            level=level,
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            enterprise_context=enterprise_context,
            description="",
            citations=[],
            data_actions=[],
            user_evidence=[],
            system_evidence=[],
            constraints=[],
            enterprise_policies=[],
            classification="",
            classification_reasoning="",
            chain_of_thought=[],
            expert_opinions={},
            thought_tree={},
            kg_nodes=[],
            kg_edges=[],
            next_step="chain_of_thought",
            iteration=0,
            max_iterations=2
        )
        
        try:
            final_state = self.workflow.invoke(initial_state)
            
            return {
                "description": final_state['description'].strip(),
                "citations": final_state['citations'],
                "data_actions": final_state['data_actions'],
                "user_evidence": final_state['user_evidence'],
                "system_evidence": final_state['system_evidence'],
                "constraints": final_state['constraints'],
                "enterprise_policies": final_state['enterprise_policies'],
                "classification": final_state['classification'] or "condition",
                "classification_reasoning": final_state['classification_reasoning'],
                "metadata": {
                    "chunk_id": chunk['chunk_id'],
                    "level": level,
                    "kg_integration": {
                        "nodes": final_state['kg_nodes']
                    }
                }
            }
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            import traceback
            traceback.print_exc()
            return self._empty_analysis(chunk['chunk_id'], level)
    
    def analyze_document(self, rule_name: str, jurisdiction: str,
                        document_text: str, level: int,
                        enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze document"""
        print(f"\n{'='*60}")
        print(f"Level {level}: {rule_name}")
        print(f"Document: {len(document_text)} characters")
        print(f"{'='*60}")
        
        chunks = self.chunker.chunk_document(
            text=document_text,
            metadata={"rule_name": rule_name, "jurisdiction": jurisdiction, "level": level}
        )
        
        print(f"Created {len(chunks)} chunks")
        
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            print(f"\nChunk {i+1}/{len(chunks)}...")
            
            analysis = self.analyze_chunk(
                chunk, rule_name, jurisdiction, level, enterprise_context
            )
            
            if analysis:
                chunk_analyses.append(analysis)
                print(f"   Description: {len(analysis['description'])} chars")
                print(f"   Citations: {len(analysis['citations'])}")
                print(f"   Actions: {len(analysis['data_actions'])}")
        
        print(f"\nMerging {len(chunk_analyses)} analyses...")
        final = self._merge_analyses(chunk_analyses, rule_name, jurisdiction, level)
        
        return final
    
    def _merge_analyses(self, analyses: List[Dict[str, Any]], rule_name: str,
                       jurisdiction: str, level: int) -> Dict[str, Any]:
        """Merge analyses"""
        if not analyses:
            return self._empty_analysis(0, level)
        
        descriptions = [a['description'] for a in analyses if a['description']]
        combined_desc = " ".join(descriptions).strip()
        if combined_desc and not combined_desc.endswith('.'):
            combined_desc += '.'
        
        all_citations = []
        seen_cites = set()
        for analysis in analyses:
            for cite in analysis.get('citations', []):
                cite_key = cite.get('text', '')[:50]
                if cite_key and cite_key not in seen_cites:
                    all_citations.append(cite)
                    seen_cites.add(cite_key)
        
        all_actions = []
        seen_actions = set()
        for analysis in analyses:
            for action in analysis.get('data_actions', []):
                action_key = (action.get('type', ''), action.get('description', '')[:50])
                if action_key[1] and action_key not in seen_actions:
                    all_actions.append(action)
                    seen_actions.add(action_key)
        
        all_user_evidence = []
        seen_user = set()
        for analysis in analyses:
            for evidence in analysis.get('user_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_user:
                    all_user_evidence.append(evidence)
                    seen_user.add(evidence_key)
        
        all_system_evidence = []
        seen_system = set()
        for analysis in analyses:
            for evidence in analysis.get('system_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_system:
                    all_system_evidence.append(evidence)
                    seen_system.add(evidence_key)
        
        all_constraints = []
        seen_constraints = set()
        for analysis in analyses:
            for constraint in analysis.get('constraints', []):
                constraint_key = (constraint.get('type', ''), constraint.get('description', '')[:50])
                if constraint_key[1] and constraint_key not in seen_constraints:
                    all_constraints.append(constraint)
                    seen_constraints.add(constraint_key)
        
        classifications = [a.get('classification', '') for a in analyses if a.get('classification')]
        final_classification = "condition"
        if "restriction" in classifications:
            final_classification = "restriction"
        elif classifications:
            final_classification = classifications[0]
        
        reasonings = [a.get('classification_reasoning', '') for a in analyses if a.get('classification_reasoning')]
        combined_reasoning = " ".join(reasonings)
        
        return {
            "description": combined_desc,
            "citations": all_citations,
            "data_actions": all_actions,
            "user_evidence": all_user_evidence,
            "system_evidence": all_system_evidence,
            "constraints": all_constraints,
            "enterprise_policies": [],
            "classification": final_classification,
            "classification_reasoning": combined_reasoning,
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "level": level,
                "chunks_processed": len(analyses),
                "total_citations": len(all_citations),
                "total_actions": len(all_actions),
                "total_user_evidence": len(all_user_evidence),
                "total_system_evidence": len(all_system_evidence),
                "total_constraints": len(all_constraints),
                "kg_stats": self.kg.get_statistics()
            }
        }
    
    def _empty_analysis(self, chunk_id: int, level: int) -> Dict[str, Any]:
        """Empty analysis"""
        return {
            "description": "",
            "citations": [],
            "data_actions": [],
            "user_evidence": [],
            "system_evidence": [],
            "constraints": [],
            "enterprise_policies": [],
            "classification": "condition",
            "classification_reasoning": "",
            "metadata": {"chunk_id": chunk_id, "level": level}
        }
    
    def analyze_multi_level(self, rule_name: str, jurisdiction: str,
                           level_1_text: str, level_2_text: str, level_3_text: str,
                           enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Multi-level analysis with Graph RAG capabilities"""
        print(f"\n{'#'*60}")
        print(f"# MULTI-LEVEL ANALYSIS: {rule_name}")
        print(f"# Graph RAG with Vector Embeddings & Semantic Search")
        print(f"{'#'*60}")
        
        print(f"\n>>> LEVEL 1: PRIMARY LEGISLATION ({len(level_1_text)} chars)")
        level_1 = self.analyze_document(rule_name, jurisdiction, level_1_text, 1, enterprise_context)
        
        print(f"\n>>> LEVEL 2: REGULATORY GUIDANCE ({len(level_2_text)} chars)")
        level_2 = self.analyze_document(rule_name, jurisdiction, level_2_text, 2, enterprise_context)
        
        print(f"\n>>> LEVEL 3: ENTERPRISE POLICY ({len(level_3_text)} chars)")
        level_3 = self.analyze_document(rule_name, jurisdiction, level_3_text, 3, enterprise_context)
        
        combined = {
            "description": " ".join([
                level_1['description'],
                level_2['description'],
                level_3['description']
            ]).strip(),
            "citations": level_1['citations'] + level_2['citations'] + level_3['citations'],
            "data_actions": level_1['data_actions'] + level_2['data_actions'] + level_3['data_actions'],
            "user_evidence": level_1['user_evidence'] + level_2['user_evidence'] + level_3['user_evidence'],
            "system_evidence": level_1['system_evidence'] + level_2['system_evidence'] + level_3['system_evidence'],
            "constraints": level_1['constraints'] + level_2['constraints'] + level_3['constraints'],
            "enterprise_policies": level_3.get('enterprise_policies', []),
            "classification": level_1['classification'],
            "classification_reasoning": " ".join([
                level_1.get('classification_reasoning', ''),
                level_2.get('classification_reasoning', ''),
                level_3.get('classification_reasoning', '')
            ]).strip(),
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "enterprise_context": enterprise_context,
                "level_1_chunks": level_1['metadata']['chunks_processed'],
                "level_2_chunks": level_2['metadata']['chunks_processed'],
                "level_3_chunks": level_3['metadata']['chunks_processed'],
                "total_citations": len(level_1['citations']) + len(level_2['citations']) + len(level_3['citations']),
                "total_actions": len(level_1['data_actions']) + len(level_2['data_actions']) + len(level_3['data_actions']),
                "kg_stats": self.kg.get_statistics()
            }
        }
        
        print(f"\n{'#'*60}")
        print(f"# GRAPH RAG ANALYSIS COMPLETE")
        print(f"{'#'*60}")
        print(f"  Description: {len(combined['description'])} chars")
        print(f"  Citations: {len(combined['citations'])}")
        print(f"  Actions: {len(combined['data_actions'])}")
        print(f"  User Evidence: {len(combined['user_evidence'])}")
        print(f"  System Evidence: {len(combined['system_evidence'])}")
        
        # Demonstrate Graph RAG capabilities
        print(f"\n{'='*60}")
        print(f"GRAPH RAG CAPABILITIES DEMONSTRATION")
        print(f"{'='*60}")
        
        # 1. Semantic Search Demo
        if combined['description']:
            print(f"\n1. SEMANTIC SEARCH:")
            query = combined['description'][:200]
            semantic_results = self.kg.semantic_search(query, top_k=3)
            if semantic_results:
                print(f"   Found {len(semantic_results)} semantically similar nodes:")
                for result in semantic_results[:3]:
                    print(f"   - {result['node_type']}: {result['node_id']}")
                    print(f"     Similarity: {result['similarity_score']:.3f}")
                    print(f"     Text: {result.get('description', '')[:100]}...")
            else:
                print(f"   No semantic matches found (graph being populated)")
        
        # 2. Full-text Search Demo
        print(f"\n2. FULL-TEXT SEARCH:")
        if "data" in combined['description'].lower():
            fulltext_results = self.kg.full_text_search("data", top_k=3)
            if fulltext_results:
                print(f"   Found {len(fulltext_results)} text matches for 'data':")
                for result in fulltext_results[:3]:
                    print(f"   - {result['node_type']}: {result['node_id']}")
            else:
                print(f"   No text matches found")
        
        # 3. Graph RAG Combined Search Demo
        print(f"\n3. GRAPH RAG COMBINED SEARCH:")
        if combined['description']:
            rag_results = self.kg.graph_rag_search(
                combined['description'][:200],
                top_k=3,
                include_related=True
            )
            print(f"   Combined results:")
            print(f"   - Direct matches: {len(rag_results.get('results', []))}")
            print(f"   - Related nodes: {len(rag_results.get('related_nodes', []))}")
            print(f"   - Graph paths: {len(rag_results.get('graph_paths', []))}")
            print(f"   - Search methods used: {', '.join(rag_results.get('search_methods', []))}")
        
        # 4. Citation Validation
        print(f"\n4. CITATION VALIDATION:")
        if combined['description']:
            supporting_cites = self.kg.find_supporting_citations(
                combined['description'][:200]
            )
            if supporting_cites:
                print(f"   Found {len(supporting_cites)} supporting citations through graph:")
                for cite in supporting_cites[:3]:
                    print(f"   - Relevance: {cite['relevance_score']:.3f}")
                    print(f"     Text: {cite['text'][:80]}...")
            else:
                print(f"   Citation validation in progress")
        
        kg_stats = combined['metadata']['kg_stats']
        if kg_stats:
            print(f"\n{'='*60}")
            print(f"GRAPH DATABASE STATISTICS:")
            print(f"{'='*60}")
            for key, value in kg_stats.items():
                print(f"  {key.title()}: {value}")
        
        return combined
