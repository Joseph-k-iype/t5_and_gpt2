"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Using o3-mini with Advanced Reasoning: Mixture of Thought + Mixture of Chains + Chain of Thought

This system converts legislation text into JSON rules compatible with json-rules-engine
using advanced prompting strategies for superior reasoning capabilities.
"""

import json
import re
import asyncio
from typing import List, Dict, Any, Optional, Annotated, Sequence
from dataclasses import dataclass
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# State Management
class AgentState(BaseModel):
    """State for the legislation processing agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str = ""
    reasoning_pathways: List[Dict[str, Any]] = []
    chain_results: Dict[str, Any] = {}
    final_rules: List[Dict[str, Any]] = []
    processing_stage: str = "initial"


# Pydantic Input Models for Strict Tool Definitions

class LegislationAnalysisInput(BaseModel):
    """Input for comprehensive legislation analysis using multiple reasoning pathways"""
    legislation_text: str = Field(..., description="Raw legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Type of reasoning pathway: 'structural', 'semantic', 'logical', or 'contextual'")


class RuleExtractionInput(BaseModel):
    """Input for extracting rules using chain of thought reasoning"""
    text_segment: str = Field(..., description="Legislation text segment to process")
    analysis_context: Dict[str, Any] = Field(..., description="Context from previous analysis")
    extraction_focus: str = Field(..., description="Focus area: 'requirements', 'prohibitions', 'conditions', 'exceptions', or 'penalties'")


class RuleSynthesisInput(BaseModel):
    """Input for synthesizing rules from multiple chains"""
    pathway_results: List[Dict[str, Any]] = Field(..., description="Results from different reasoning pathways")
    synthesis_strategy: str = Field(..., description="Strategy: 'consensus', 'weighted', or 'hierarchical'")


class JSONConversionInput(BaseModel):
    """Input for converting synthesized rules to JSON format"""
    synthesized_rules: List[Dict[str, Any]] = Field(..., description="Rules from synthesis process")
    conversion_standard: str = Field(default="json-rules-engine", description="Target JSON rules format")


class ValidationInput(BaseModel):
    """Input for validating JSON rules"""
    json_rules: List[Dict[str, Any]] = Field(..., description="JSON rules to validate")
    validation_depth: str = Field(default="comprehensive", description="Validation depth: 'basic', 'standard', or 'comprehensive'")


# Advanced Tools with Mixture of Thought and Chain Reasoning

@tool(args_schema=LegislationAnalysisInput)
def analyze_legislation_pathway(legislation_text: str, reasoning_pathway: str) -> Dict[str, Any]:
    """
    Analyze legislation using a specific reasoning pathway in the Mixture of Thought approach.
    Each pathway explores different aspects of the legal text.
    
    Args:
        legislation_text: Raw legislation text
        reasoning_pathway: Type of reasoning pathway to use
        
    Returns:
        Analysis results from the specific pathway
    """
    
    if reasoning_pathway == "structural":
        # Structural analysis: hierarchy, organization, relationships
        structure_patterns = {
            'sections': r'(?:Section|Sec\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Section|Sec\.?)\s+\d+|$)',
            'articles': r'(?:Article|Art\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Article|Art\.?)\s+\d+|$)',
            'subsections': r'(?:^|\n)\s*\(([a-z]|\d+)\)\s+(.*?)(?=\n\s*\([a-z]|\d+\)|$)',
            'clauses': r'(?:^|\n)\s*\(i+\)\s+(.*?)(?=\n\s*\(i+\)|$)'
        }
        
        analysis = {
            'pathway': 'structural',
            'hierarchy': {},
            'relationships': [],
            'organization_quality': 'high'
        }
        
        # Extract structural elements
        for element_type, pattern in structure_patterns.items():
            matches = re.findall(pattern, legislation_text, re.DOTALL | re.MULTILINE)
            analysis['hierarchy'][element_type] = [
                {'id': match[0] if isinstance(match, tuple) else f"{element_type}_{i+1}", 
                 'content': match[1] if isinstance(match, tuple) else match}
                for i, match in enumerate(matches)
            ]
        
        # Identify cross-references
        cross_refs = re.findall(r'(?:Section|Article)\s+(\d+(?:\.\d+)*)', legislation_text)
        analysis['relationships'] = [{'type': 'cross_reference', 'target': ref} for ref in set(cross_refs)]
        
        return analysis
    
    elif reasoning_pathway == "semantic":
        # Semantic analysis: meaning, definitions, intent
        semantic_patterns = {
            'definitions': [
                r'"([^"]+)"\s+means\s+([^.]+)',
                r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+means\s+([^.]+)',
                r'(?:For the purposes of|In this)\s+[^,]*,\s*"([^"]+)"\s+([^.]+)'
            ],
            'intent_indicators': [
                r'\b(?:purpose|intent|objective|goal|aim)\b',
                r'\b(?:whereas|considering|recognizing)\b'
            ],
            'scope_indicators': [
                r'\b(?:applies to|covers|includes|excludes|except)\b'
            ]
        }
        
        analysis = {
            'pathway': 'semantic',
            'definitions': [],
            'intent_phrases': [],
            'scope_elements': [],
            'semantic_density': 'medium'
        }
        
        # Extract definitions
        for pattern in semantic_patterns['definitions']:
            matches = re.findall(pattern, legislation_text, re.MULTILINE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    analysis['definitions'].append({
                        'term': match[0].strip(),
                        'definition': match[1].strip()
                    })
        
        # Extract intent and scope
        for pattern in semantic_patterns['intent_indicators']:
            matches = re.findall(f'[^.]*{pattern}[^.]*', legislation_text, re.IGNORECASE)
            analysis['intent_phrases'].extend(matches)
        
        for pattern in semantic_patterns['scope_indicators']:
            matches = re.findall(f'[^.]*{pattern}[^.]*', legislation_text, re.IGNORECASE)
            analysis['scope_elements'].extend(matches)
        
        return analysis
    
    elif reasoning_pathway == "logical":
        # Logical analysis: rules, conditions, consequences
        logical_patterns = {
            'conditionals': r'(?:If|When|Where)\s+([^,]+),\s*(?:then\s+)?([^.]+)',
            'requirements': r'([^.]+?)\s+(?:shall|must)\s+([^.]+)',
            'prohibitions': r'([^.]+?)\s+(?:shall not|must not|may not|prohibited from)\s+([^.]+)',
            'exceptions': r'(?:except|unless|provided that|notwithstanding)\s+([^,]+),\s*([^.]+)',
            'penalties': r'(?:violation|breach|failure)[^.]*?(?:shall result in|subject to|punishable by)\s+([^.]+)'
        }
        
        analysis = {
            'pathway': 'logical',
            'conditionals': [],
            'requirements': [],
            'prohibitions': [],
            'exceptions': [],
            'penalties': [],
            'logical_complexity': 'medium'
        }
        
        # Extract logical structures
        for rule_type, pattern in logical_patterns.items():
            matches = re.findall(pattern, legislation_text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple):
                    analysis[rule_type].append({
                        'condition': match[0].strip() if len(match) > 1 else '',
                        'consequence': match[1].strip() if len(match) > 1 else match[0].strip(),
                        'confidence': 0.8
                    })
                else:
                    analysis[rule_type].append({
                        'content': match.strip(),
                        'confidence': 0.7
                    })
        
        return analysis
    
    elif reasoning_pathway == "contextual":
        # Contextual analysis: actors, actions, temporal elements
        contextual_patterns = {
            'actors': [
                r'\b(?:person|individual|entity|organization|company|corporation|agency|department|officer|employee|citizen|resident)\b',
                r'\b(?:applicant|licensee|registrant|holder|owner|operator|provider|contractor)\b'
            ],
            'actions': [
                r'\b(?:shall|must|may|should|will|can)\s+([^.]+)',
                r'\b(?:provide|submit|maintain|ensure|comply|notify|report|register|apply)\b'
            ],
            'temporal': [
                r'\b(?:within|before|after|by|no later than)\s+\d+\s+(?:days|weeks|months|years)\b',
                r'\b(?:immediately|promptly|forthwith|annually|monthly|quarterly)\b'
            ]
        }
        
        analysis = {
            'pathway': 'contextual',
            'actors': [],
            'actions': [],
            'temporal_elements': [],
            'context_richness': 'high'
        }
        
        # Extract contextual elements
        for category, patterns in contextual_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, legislation_text, re.IGNORECASE)
                if category == 'actors':
                    analysis[category].extend(list(set(matches)))
                elif category == 'actions':
                    analysis[category].extend([match if isinstance(match, str) else match[0] for match in matches])
                else:
                    analysis[category].extend(matches)
        
        # Remove duplicates
        for key in analysis:
            if isinstance(analysis[key], list):
                analysis[key] = list(set(analysis[key]))
        
        return analysis
    
    else:
        return {
            'pathway': reasoning_pathway,
            'error': 'Unknown reasoning pathway',
            'available_pathways': ['structural', 'semantic', 'logical', 'contextual']
        }


@tool(args_schema=RuleExtractionInput)
def extract_rules_chain(text_segment: str, analysis_context: Dict[str, Any], extraction_focus: str) -> Dict[str, Any]:
    """
    Extract specific types of rules using chain of thought reasoning.
    Each chain focuses on a specific aspect of rule extraction.
    
    Args:
        text_segment: Text segment to process
        analysis_context: Context from pathway analysis
        extraction_focus: Focus area for extraction
        
    Returns:
        Extracted rules with reasoning chain
    """
    
    reasoning_chain = []
    extracted_rules = []
    
    # Step 1: Context Assessment
    reasoning_chain.append({
        'step': 1,
        'action': 'context_assessment',
        'reasoning': f'Analyzing text segment for {extraction_focus} extraction with context from {analysis_context.get("pathway", "unknown")} pathway',
        'findings': f'Text length: {len(text_segment)} characters, Focus: {extraction_focus}'
    })
    
    if extraction_focus == "requirements":
        # Chain of thought for requirements extraction
        requirement_patterns = [
            r'([^.]+?)\s+(?:shall|must|required to|obligated to|duty to)\s+([^.]+)',
            r'(?:must|shall)\s+([^.]+)',
            r'([^.]+?)\s+(?:is required|are required)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying requirement patterns using modal verbs and obligation indicators',
            'patterns_used': requirement_patterns
        })
        
        for i, pattern in enumerate(requirement_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'requirement',
                        'subject': match[0].strip(),
                        'obligation': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9
                    })
                else:
                    extracted_rules.append({
                        'type': 'requirement',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} requirements using pattern matching',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "prohibitions":
        # Chain of thought for prohibitions extraction
        prohibition_patterns = [
            r'([^.]+?)\s+(?:shall not|must not|may not|cannot|prohibited from|forbidden from)\s+([^.]+)',
            r'(?:no\s+)?([^.]+?)\s+(?:shall|may)\s+([^.]+)',
            r'(?:it is prohibited|forbidden|illegal)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying prohibition patterns using negative modal constructions',
            'patterns_used': prohibition_patterns
        })
        
        for i, pattern in enumerate(prohibition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'prohibition',
                        'subject': match[0].strip(),
                        'prohibited_action': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9
                    })
                else:
                    extracted_rules.append({
                        'type': 'prohibition',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} prohibitions using negative modal analysis',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "conditions":
        # Chain of thought for conditions extraction
        condition_patterns = [
            r'(?:If|When|Where|Provided that|In case)\s+([^,]+),\s*([^.]+)',
            r'([^.]+?)\s+(?:if|when|where|provided)\s+([^.]+)',
            r'(?:Subject to|Conditional upon)\s+([^,]+),\s*([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying conditional patterns using temporal and logical connectors',
            'patterns_used': condition_patterns
        })
        
        for i, pattern in enumerate(condition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'condition',
                        'trigger': match[0].strip(),
                        'consequence': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} conditions using conditional logic patterns',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "exceptions":
        # Chain of thought for exceptions extraction
        exception_patterns = [
            r'(?:except|unless|save|but|however|notwithstanding)\s+([^,]+),\s*([^.]+)',
            r'([^.]+)\s+(?:except|unless|save for)\s+([^.]+)',
            r'(?:this does not apply|shall not apply)\s+(?:to|when|if)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying exception patterns using adversative and conditional expressions',
            'patterns_used': exception_patterns
        })
        
        for i, pattern in enumerate(exception_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'exception',
                        'exception_condition': match[0].strip(),
                        'modified_rule': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
                else:
                    extracted_rules.append({
                        'type': 'exception',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.6
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} exceptions using adversative pattern analysis',
            'extraction_quality': 'medium' if len(extracted_rules) > 0 else 'low'
        })
    
    elif extraction_focus == "penalties":
        # Chain of thought for penalties extraction
        penalty_patterns = [
            r'(?:violation|breach|failure to comply|non-compliance)[^.]*?(?:shall result in|subject to|punishable by|penalty of)\s+([^.]+)',
            r'(?:fine|penalty|sanction|imprisonment|suspension|revocation)\s+(?:of|not exceeding|up to)\s+([^.]+)',
            r'([^.]+)\s+(?:shall be fined|shall be subject to|shall result in)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning': 'Identifying penalty patterns using consequence and sanction indicators',
            'patterns_used': penalty_patterns
        })
        
        for i, pattern in enumerate(penalty_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'penalty',
                        'violation': match[0].strip(),
                        'sanction': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8
                    })
                else:
                    extracted_rules.append({
                        'type': 'penalty',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7
                    })
        
        reasoning_chain.append({
            'step': 3,
            'action': 'rule_extraction',
            'reasoning': f'Extracted {len(extracted_rules)} penalties using sanction pattern analysis',
            'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
        })
    
    # Step 4: Quality Assessment
    reasoning_chain.append({
        'step': 4,
        'action': 'quality_assessment',
        'reasoning': f'Assessing extraction quality based on rule count and pattern diversity',
        'total_rules': len(extracted_rules),
        'assessment': 'successful' if len(extracted_rules) > 0 else 'limited_success'
    })
    
    return {
        'extraction_focus': extraction_focus,
        'extracted_rules': extracted_rules,
        'reasoning_chain': reasoning_chain,
        'context_used': analysis_context.get('pathway', 'unknown'),
        'extraction_metadata': {
            'text_length': len(text_segment),
            'rules_found': len(extracted_rules),
            'confidence_avg': sum([rule.get('confidence', 0) for rule in extracted_rules]) / len(extracted_rules) if extracted_rules else 0
        }
    }


@tool(args_schema=RuleSynthesisInput)
def synthesize_rules(pathway_results: List[Dict[str, Any]], synthesis_strategy: str) -> Dict[str, Any]:
    """
    Synthesize rules from multiple reasoning pathways using mixture of thought approach.
    
    Args:
        pathway_results: Results from different reasoning pathways
        synthesis_strategy: Strategy for synthesis
        
    Returns:
        Synthesized rules with rationale
    """
    
    synthesized_rules = []
    synthesis_rationale = []
    
    # Collect all rules from different pathways
    all_rules = []
    pathway_weights = {
        'structural': 0.2,
        'semantic': 0.25,
        'logical': 0.35,
        'contextual': 0.2
    }
    
    for result in pathway_results:
        if 'extracted_rules' in result:
            for rule in result['extracted_rules']:
                rule['source_pathway'] = result.get('extraction_focus', 'unknown')
                rule['pathway_weight'] = pathway_weights.get(result.get('context_used', 'unknown'), 0.1)
                all_rules.append(rule)
    
    synthesis_rationale.append({
        'step': 'collection',
        'reasoning': f'Collected {len(all_rules)} rules from {len(pathway_results)} pathways',
        'pathway_distribution': {result.get('extraction_focus', 'unknown'): len(result.get('extracted_rules', [])) for result in pathway_results}
    })
    
    if synthesis_strategy == "consensus":
        # Find rules that appear across multiple pathways
        rule_signatures = {}
        for rule in all_rules:
            # Create signature based on rule content
            signature = f"{rule.get('type', 'unknown')}_{rule.get('subject', rule.get('content', ''))[:50]}"
            if signature not in rule_signatures:
                rule_signatures[signature] = []
            rule_signatures[signature].append(rule)
        
        # Select rules with consensus (appear in multiple pathways or high confidence)
        for signature, rules in rule_signatures.items():
            if len(rules) > 1 or (len(rules) == 1 and rules[0].get('confidence', 0) > 0.8):
                # Take the rule with highest confidence
                best_rule = max(rules, key=lambda r: r.get('confidence', 0))
                best_rule['synthesis_support'] = len(rules)
                best_rule['synthesis_confidence'] = min(1.0, best_rule.get('confidence', 0) * len(rules) * 0.3)
                synthesized_rules.append(best_rule)
        
        synthesis_rationale.append({
            'step': 'consensus_filtering',
            'reasoning': f'Applied consensus filtering, selected {len(synthesized_rules)} rules with cross-pathway support',
            'strategy': 'consensus'
        })
    
    elif synthesis_strategy == "weighted":
        # Weight rules based on pathway confidence and rule quality
        for rule in all_rules:
            weighted_confidence = rule.get('confidence', 0) * rule.get('pathway_weight', 0.1)
            if weighted_confidence > 0.4:  # Threshold for inclusion
                rule['weighted_confidence'] = weighted_confidence
                synthesized_rules.append(rule)
        
        # Sort by weighted confidence
        synthesized_rules.sort(key=lambda r: r.get('weighted_confidence', 0), reverse=True)
        
        synthesis_rationale.append({
            'step': 'weighted_selection',
            'reasoning': f'Applied weighted selection, retained {len(synthesized_rules)} high-quality rules',
            'strategy': 'weighted'
        })
    
    elif synthesis_strategy == "hierarchical":
        # Prioritize by rule type hierarchy: requirements > prohibitions > conditions > exceptions > penalties
        type_priority = {
            'requirement': 5,
            'prohibition': 4,
            'condition': 3,
            'exception': 2,
            'penalty': 1
        }
        
        # Sort by type priority and confidence
        sorted_rules = sorted(all_rules, 
                            key=lambda r: (type_priority.get(r.get('type', 'unknown'), 0), 
                                         r.get('confidence', 0)), 
                            reverse=True)
        
        # Take top rules from each category
        rules_by_type = {}
        for rule in sorted_rules:
            rule_type = rule.get('type', 'unknown')
            if rule_type not in rules_by_type:
                rules_by_type[rule_type] = []
            if len(rules_by_type[rule_type]) < 10:  # Limit per type
                rules_by_type[rule_type].append(rule)
        
        # Flatten back to single list
        for type_rules in rules_by_type.values():
            synthesized_rules.extend(type_rules)
        
        synthesis_rationale.append({
            'step': 'hierarchical_prioritization',
            'reasoning': f'Applied hierarchical prioritization, organized {len(synthesized_rules)} rules by type importance',
            'strategy': 'hierarchical',
            'type_distribution': {t: len(rules) for t, rules in rules_by_type.items()}
        })
    
    # Final quality check
    synthesis_rationale.append({
        'step': 'quality_assessment',
        'reasoning': f'Final synthesis produced {len(synthesized_rules)} high-quality rules',
        'quality_metrics': {
            'total_rules': len(synthesized_rules),
            'avg_confidence': sum([r.get('confidence', 0) for r in synthesized_rules]) / len(synthesized_rules) if synthesized_rules else 0,
            'type_diversity': len(set([r.get('type', 'unknown') for r in synthesized_rules]))
        }
    })
    
    return {
        'synthesized_rules': synthesized_rules,
        'synthesis_strategy': synthesis_strategy,
        'synthesis_rationale': synthesis_rationale,
        'original_rule_count': len(all_rules),
        'final_rule_count': len(synthesized_rules),
        'synthesis_quality': 'high' if len(synthesized_rules) > 0 else 'low'
    }


@tool(args_schema=JSONConversionInput)
def convert_to_json_rules(synthesized_rules: List[Dict[str, Any]], conversion_standard: str = "json-rules-engine") -> List[Dict[str, Any]]:
    """
    Convert synthesized rules into json-rules-engine compatible format.
    
    Args:
        synthesized_rules: Rules from synthesis process
        conversion_standard: Target JSON rules format
        
    Returns:
        JSON rules compatible with json-rules-engine
    """
    json_rules = []
    
    for i, rule in enumerate(synthesized_rules):
        rule_id = f"legislation_rule_{i+1}"
        rule_type = rule.get('type', 'unknown')
        
        if rule_type == 'requirement':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "compliance_required",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "requirement_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "obligation": rule.get('obligation', rule.get('content', 'unknown')),
                        "mandatory": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 80
            }
        
        elif rule_type == 'prohibition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "prohibited_action_attempted",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "prohibition_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "prohibited_action": rule.get('prohibited_action', rule.get('content', 'unknown')),
                        "violation_level": "high",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 90
            }
        
        elif rule_type == 'condition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "trigger_condition",
                            "operator": "contains",
                            "value": rule.get('trigger', 'condition').lower()[:20]
                        },
                        {
                            "fact": "condition_met",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "conditional_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "trigger": rule.get('trigger', 'unknown'),
                        "consequence": rule.get('consequence', rule.get('content', 'unknown')),
                        "conditional": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 70
            }
        
        elif rule_type == 'exception':
            json_rule = {
                "conditions": {
                    "any": [
                        {
                            "fact": "exception_condition",
                            "operator": "contains",
                            "value": rule.get('exception_condition', 'exception').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "exception_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "exception_condition": rule.get('exception_condition', 'unknown'),
                        "modified_rule": rule.get('modified_rule', rule.get('content', 'unknown')),
                        "overrides_default": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 60
            }
        
        elif rule_type == 'penalty':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "violation_detected",
                            "operator": "equal",
                            "value": True
                        },
                        {
                            "fact": "violation_type",
                            "operator": "contains",
                            "value": rule.get('violation', 'violation').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "penalty_applied",
                    "params": {
                        "rule_id": rule_id,
                        "violation": rule.get('violation', 'unknown'),
                        "sanction": rule.get('sanction', rule.get('content', 'unknown')),
                        "severity": "medium",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 50
            }
        
        else:
            # Generic rule for unknown types
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "rule_applicable",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "generic_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "content": rule.get('content', 'unknown rule'),
                        "rule_type": rule_type,
                        "confidence": rule.get('confidence', 0.3),
                        "source_pathway": rule.get('source_pathway', 'unknown')
                    }
                },
                "priority": 40
            }
        
        # Add metadata
        json_rule["metadata"] = {
            "source": "legislation_analysis",
            "extraction_method": "mixture_of_thought_chains",
            "reasoning_pathway": rule.get('source_pathway', 'unknown'),
            "synthesis_strategy": "advanced",
            "created_by": "o3_mini_agent"
        }
        
        json_rules.append(json_rule)
    
    return json_rules


@tool(args_schema=ValidationInput)
def validate_json_rules(json_rules: List[Dict[str, Any]], validation_depth: str = "comprehensive") -> Dict[str, Any]:
    """
    Validate JSON rules for json-rules-engine compatibility.
    
    Args:
        json_rules: JSON rules to validate
        validation_depth: Depth of validation
        
    Returns:
        Comprehensive validation report
    """
    validation_report = {
        "valid": True,
        "validation_depth": validation_depth,
        "total_rules": len(json_rules),
        "errors": [],
        "warnings": [],
        "recommendations": [],
        "rule_analysis": [],
        "quality_score": 0.0
    }
    
    required_fields = ["conditions", "event"]
    valid_operators = [
        "equal", "notEqual", "lessThan", "lessThanInclusive", 
        "greaterThan", "greaterThanInclusive", "in", "notIn", 
        "contains", "doesNotContain", "regex"
    ]
    
    quality_points = 0
    max_possible_points = len(json_rules) * 10
    
    for i, rule in enumerate(json_rules):
        rule_errors = []
        rule_warnings = []
        rule_recommendations = []
        rule_quality = 0
        
        # Basic structure validation
        for field in required_fields:
            if field not in rule:
                rule_errors.append(f"Missing required field: {field}")
            else:
                rule_quality += 2
        
        # Conditions validation
        if "conditions" in rule:
            conditions = rule["conditions"]
            if isinstance(conditions, dict):
                logical_ops = ["all", "any", "not"]
                has_logical_op = any(op in conditions for op in logical_ops)
                
                if has_logical_op:
                    rule_quality += 2
                    
                    # Validate condition structure
                    for logical_op in logical_ops:
                        if logical_op in conditions:
                            condition_list = conditions[logical_op]
                            
                            if logical_op == "not":
                                if isinstance(condition_list, dict):
                                    rule_quality += 1
                                else:
                                    rule_errors.append("'not' operator must contain a single condition object")
                            else:
                                if isinstance(condition_list, list):
                                    rule_quality += 1
                                    
                                    # Validate individual conditions
                                    for j, condition in enumerate(condition_list):
                                        if isinstance(condition, dict):
                                            rule_quality += 0.5
                                            
                                            # Check condition fields
                                            if "fact" in condition:
                                                rule_quality += 0.5
                                            else:
                                                rule_errors.append(f"Condition {j} missing 'fact' field")
                                            
                                            if "operator" in condition:
                                                if condition["operator"] in valid_operators:
                                                    rule_quality += 1
                                                else:
                                                    rule_warnings.append(f"Condition {j} uses unknown operator: {condition['operator']}")
                                            else:
                                                rule_errors.append(f"Condition {j} missing 'operator' field")
                                            
                                            if "value" in condition:
                                                rule_quality += 0.5
                                            else:
                                                rule_warnings.append(f"Condition {j} missing 'value' field")
                                        else:
                                            rule_errors.append(f"Condition {j} must be an object")
                                else:
                                    rule_errors.append(f"'{logical_op}' operator must contain an array of conditions")
                else:
                    rule_errors.append("Conditions must contain 'all', 'any', or 'not' operator")
            else:
                rule_errors.append("Conditions must be an object")
        
        # Event validation
        if "event" in rule:
            event = rule["event"]
            if isinstance(event, dict):
                rule_quality += 1
                
                if "type" in event:
                    rule_quality += 1
                else:
                    rule_warnings.append("Event missing 'type' field")
                
                if "params" in event:
                    rule_quality += 1
                    params = event["params"]
                    if isinstance(params, dict) and len(params) > 0:
                        rule_quality += 1
            else:
                rule_errors.append("Event must be an object")
        
        # Priority validation
        if "priority" in rule:
            priority = rule["priority"]
            if isinstance(priority, (int, float)):
                if 0 <= priority <= 100:
                    rule_quality += 1
                else:
                    rule_warnings.append("Priority should be between 0 and 100")
            else:
                rule_errors.append("Priority must be a number")
        else:
            rule_recommendations.append("Consider adding priority field for better rule ordering")
        
        # Metadata validation (if present)
        if "metadata" in rule:
            metadata = rule["metadata"]
            if isinstance(metadata, dict):
                rule_quality += 0.5
                if len(metadata) > 0:
                    rule_quality += 0.5
        
        # Additional quality checks for comprehensive validation
        if validation_depth == "comprehensive":
            # Check for meaningful fact names
            if "conditions" in rule:
                conditions = rule["conditions"]
                fact_names = []
                
                def extract_facts(cond_obj):
                    if isinstance(cond_obj, dict):
                        if "fact" in cond_obj:
                            fact_names.append(cond_obj["fact"])
                        for key, value in cond_obj.items():
                            if key in ["all", "any"]:
                                if isinstance(value, list):
                                    for item in value:
                                        extract_facts(item)
                            elif key == "not":
                                extract_facts(value)
                
                extract_facts(conditions)
                
                if fact_names:
                    # Check for descriptive fact names
                    descriptive_facts = [f for f in fact_names if len(f) > 5 and '_' in f]
                    if descriptive_facts:
                        rule_quality += 1
                        rule_recommendations.append("Good use of descriptive fact names")
                    else:
                        rule_recommendations.append("Consider using more descriptive fact names")
            
            # Check for rule complexity balance
            if "conditions" in rule:
                condition_count = 0
                
                def count_conditions(cond_obj):
                    nonlocal condition_count
                    if isinstance(cond_obj, dict):
                        if "fact" in cond_obj:
                            condition_count += 1
                        for key, value in cond_obj.items():
                            if key in ["all", "any"]:
                                if isinstance(value, list):
                                    for item in value:
                                        count_conditions(item)
                            elif key == "not":
                                count_conditions(value)
                
                count_conditions(rule["conditions"])
                
                if 1 <= condition_count <= 5:
                    rule_quality += 1
                elif condition_count > 5:
                    rule_recommendations.append("Consider simplifying complex conditions")
                elif condition_count == 0:
                    rule_warnings.append("No conditions found in rule")
        
        # Compile rule analysis
        rule_analysis = {
            "rule_index": i,
            "valid": len(rule_errors) == 0,
            "quality_score": min(10.0, rule_quality),
            "errors": rule_errors,
            "warnings": rule_warnings,
            "recommendations": rule_recommendations
        }
        
        validation_report["rule_analysis"].append(rule_analysis)
        validation_report["errors"].extend(rule_errors)
        validation_report["warnings"].extend(rule_warnings)
        validation_report["recommendations"].extend(rule_recommendations)
        
        quality_points += rule_quality
        
        if rule_errors:
            validation_report["valid"] = False
    
    # Calculate overall quality score
    if max_possible_points > 0:
        validation_report["quality_score"] = min(100.0, (quality_points / max_possible_points) * 100)
    
    # Add summary statistics
    validation_report["summary"] = {
        "total_errors": len(validation_report["errors"]),
        "total_warnings": len(validation_report["warnings"]),
        "total_recommendations": len(validation_report["recommendations"]),
        "valid_rules": sum(1 for r in validation_report["rule_analysis"] if r["valid"]),
        "average_rule_quality": sum(r["quality_score"] for r in validation_report["rule_analysis"]) / len(json_rules) if json_rules else 0
    }
    
    return validation_report


# Advanced o3-mini Agent with Mixture of Thought + Mixture of Chains + Chain of Thought

def create_advanced_o3_mini_agent():
    """
    Create a LangGraph agent using o3-mini with advanced prompting strategies:
    - Mixture of Thought: Multiple reasoning pathways
    - Mixture of Chains: Different processing chains for different aspects
    - Chain of Thought: Step-by-step reasoning within each chain
    """
    
    # Initialize o3-mini with optimized reasoning configuration
    model = ChatOpenAI(
        model="o3-mini",
        temperature=0,
        model_kwargs={
            "reasoning_effort": "medium"
        }
    )
    
    # Available tools for advanced processing
    tools = [
        analyze_legislation_pathway,
        extract_rules_chain,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    
    # Advanced system message combining all three techniques
    system_message = """You are an elite legal analyst and rules engineer with advanced reasoning capabilities, specializing in converting legislation into precise machine-readable JSON rules using cutting-edge AI reasoning techniques.

## REASONING FRAMEWORK: MIXTURE OF THOUGHT + MIXTURE OF CHAINS + CHAIN OF THOUGHT

### MIXTURE OF THOUGHT APPROACH:
Explore multiple reasoning pathways simultaneously:
1. **Structural Pathway**: Analyze hierarchy, organization, and document structure
2. **Semantic Pathway**: Focus on meaning, definitions, and legal intent
3. **Logical Pathway**: Examine rules, conditions, and logical relationships
4. **Contextual Pathway**: Identify actors, actions, and temporal elements

Each pathway offers unique insights. DO NOT settle for a single perspective.

### MIXTURE OF CHAINS STRATEGY:
Process different aspects through specialized chains:
- **Chain 1 (Analysis)**: Multi-pathway legislation analysis
- **Chain 2 (Extraction)**: Focused rule extraction by type
- **Chain 3 (Synthesis)**: Cross-pathway rule synthesis
- **Chain 4 (Conversion)**: JSON rules generation
- **Chain 5 (Validation)**: Comprehensive rule validation

### CHAIN OF THOUGHT EXECUTION:
Within each chain, think step-by-step:
1. **Context Assessment**: What am I analyzing and why?
2. **Pattern Recognition**: What patterns and structures do I see?
3. **Information Extraction**: What specific information can I extract?
4. **Quality Evaluation**: How confident am I in these findings?
5. **Integration Planning**: How does this connect to other insights?

## PROCESSING METHODOLOGY:

**PHASE 1 - DIVERGENT ANALYSIS (Mixture of Thought)**
- Analyze the same legislation through 4 different reasoning pathways
- Each pathway focuses on different aspects but covers the complete text
- Generate diverse insights and perspectives

**PHASE 2 - SPECIALIZED EXTRACTION (Mixture of Chains)**
- Extract requirements, prohibitions, conditions, exceptions, and penalties
- Use chain of thought reasoning within each extraction process
- Maintain context from the multi-pathway analysis

**PHASE 3 - CONVERGENT SYNTHESIS (Advanced Integration)**
- Synthesize findings from all pathways and chains
- Resolve conflicts and redundancies
- Create comprehensive rule set

**PHASE 4 - PRECISION CONVERSION (JSON Translation)**
- Convert synthesized rules to json-rules-engine format
- Ensure technical accuracy and compliance
- Maintain semantic fidelity

**PHASE 5 - RIGOROUS VALIDATION (Quality Assurance)**
- Comprehensive validation of generated rules
- Quality scoring and improvement recommendations
- Final optimization

## QUALITY STANDARDS:
- **Completeness**: Capture all significant legal rules
- **Accuracy**: Maintain precise legal meaning
- **Consistency**: Ensure logical coherence across rules
- **Usability**: Generate executable machine-readable rules
- **Traceability**: Maintain clear reasoning paths

Use the provided tools systematically, leveraging your advanced reasoning to produce comprehensive, high-quality JSON rules that accurately represent the legislation's requirements."""
    
    # Create memory for conversation state
    memory = MemorySaver()
    
    # Create the react agent with advanced capabilities
    agent = create_react_agent(
        model=model,
        tools=tools,
        checkpointer=memory,
        prompt=system_message
    )
    
    return agent


# Main Processing Function

async def process_legislation_advanced(legislation_text: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Process legislation text using advanced o3-mini reasoning with mixture of thought,
    mixture of chains, and chain of thought approaches.
    
    Args:
        legislation_text: The legislation text to process
        config: Optional configuration for the agent
        
    Returns:
        Dictionary containing comprehensive processing results
    """
    
    # Create the advanced o3-mini agent
    agent = create_advanced_o3_mini_agent()
    
    # Default config
    if config is None:
        config = {"configurable": {"thread_id": "advanced_o3_mini_session"}}
    
    # Create comprehensive prompt using all three techniques
    prompt = f"""Process the following legislation using the advanced reasoning framework (Mixture of Thought + Mixture of Chains + Chain of Thought):

LEGISLATION TEXT:
{legislation_text}

## PROCESSING INSTRUCTIONS:

**PHASE 1 - DIVERGENT ANALYSIS (Mixture of Thought)**
Analyze this legislation through multiple reasoning pathways:

1. Use `analyze_legislation_pathway` with pathway="structural" to understand document organization
2. Use `analyze_legislation_pathway` with pathway="semantic" to extract meaning and definitions  
3. Use `analyze_legislation_pathway` with pathway="logical" to identify rule structures
4. Use `analyze_legislation_pathway` with pathway="contextual" to understand actors and actions

**PHASE 2 - SPECIALIZED EXTRACTION (Mixture of Chains)**
For each rule type, use `extract_rules_chain` with the analysis context:

1. Extract requirements (obligations, duties, mandates)
2. Extract prohibitions (forbidden actions, restrictions)
3. Extract conditions (if-then structures, triggers)
4. Extract exceptions (exemptions, special cases)
5. Extract penalties (sanctions, consequences)

**PHASE 3 - CONVERGENT SYNTHESIS**
Use `synthesize_rules` to combine findings from all pathways using an appropriate synthesis strategy.

**PHASE 4 - PRECISION CONVERSION**
Use `convert_to_json_rules` to generate json-rules-engine compatible rules.

**PHASE 5 - RIGOROUS VALIDATION**
Use `validate_json_rules` with validation_depth="comprehensive" for quality assurance.

## REASONING REQUIREMENTS:
- Think step-by-step within each tool usage
- Explain your reasoning for tool selection and parameter choices
- Integrate insights across different pathways and chains
- Ensure comprehensive coverage of all legislative elements
- Maintain high quality standards throughout

Provide detailed reasoning for each phase and explain how the different techniques contribute to the final result."""

    # Process with advanced reasoning
    response = await agent.ainvoke(
        {"messages": [HumanMessage(content=prompt)]},
        config=config
    )
    
    return {
        "status": "completed",
        "model_used": "o3-mini",
        "reasoning_framework": "mixture_of_thought_chains_cot",
        "original_text": legislation_text,
        "messages": response["messages"],
        "processing_phases": [
            "divergent_analysis",
            "specialized_extraction", 
            "convergent_synthesis",
            "precision_conversion",
            "rigorous_validation"
        ]
    }


def process_legislation_sync(legislation_text: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Synchronous wrapper for advanced o3-mini legislation processing.
    
    Args:
        legislation_text: The legislation text to process
        config: Optional configuration for the agent
        
    Returns:
        Dictionary containing the processed results
    """
    
    async def _process_async():
        return await process_legislation_advanced(legislation_text, config)
    
    # Handle event loop management
    try:
        loop = asyncio.get_running_loop()
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, _process_async())
            return future.result()
    except RuntimeError:
        return asyncio.run(_process_async())


# Main execution
if __name__ == "__main__":
    # Example legislation for demonstration
    sample_legislation = """
    SECTION 1. DEFINITIONS
    
    For the purposes of this Act:
    (a) "Vehicle" means any motorized conveyance designed for transportation on public roads.
    (b) "Driver" means any person operating a vehicle on a public road.
    (c) "Speed limit" means the maximum lawful speed for vehicles on a particular road segment.
    
    SECTION 2. SPEED REGULATIONS
    
    (a) No person shall operate a vehicle at a speed greater than the posted speed limit.
    (b) If weather conditions are hazardous, drivers must reduce speed to maintain safe operation.
    (c) Emergency vehicles may exceed speed limits when responding to emergencies, provided warning signals are activated.
    
    SECTION 3. PENALTIES
    
    (a) Any person who violates Section 2(a) shall be subject to a fine of not less than $100 and not more than $500.
    (b) Repeat violations within 12 months shall result in license suspension for 30 days.
    """
    
    print("🚀 Advanced o3-mini Legislation Processing")
    print("=" * 60)
    print("🧠 Using: Mixture of Thought + Mixture of Chains + Chain of Thought")
    print("📄 Processing sample legislation...")
    
    try:
        result = process_legislation_sync(sample_legislation)
        
        print(f"\n✅ Processing completed successfully!")
        print(f"📊 Status: {result['status']}")
        print(f"🧠 Model: {result['model_used']}")
        print(f"⚙️ Framework: {result['reasoning_framework']}")
        print(f"📝 Phases: {', '.join(result['processing_phases'])}")
        print(f"💬 Messages exchanged: {len(result['messages'])}")
        
        print(f"\n🔍 Sample conversation excerpt:")
        if result['messages']:
            last_msg = result['messages'][-1]
            if hasattr(last_msg, 'content'):
                content_preview = last_msg.content[:500] + "..." if len(last_msg.content) > 500 else last_msg.content
                print(f"Agent: {content_preview}")
        
    except Exception as e:
        print(f"❌ Processing failed: {e}")
        print(f"🔧 Error type: {type(e).__name__}")


"""
ADVANCED o3-mini LEGISLATION PROCESSOR

This system combines three cutting-edge prompting techniques:

1. **Mixture of Thought (MoT)**: Explores multiple reasoning pathways simultaneously
   - Structural analysis (document organization)
   - Semantic analysis (meaning and definitions) 
   - Logical analysis (rules and conditions)
   - Contextual analysis (actors and actions)

2. **Mixture of Chains**: Specialized processing chains for different aspects
   - Analysis chain (multi-pathway exploration)
   - Extraction chain (focused rule extraction)
   - Synthesis chain (cross-pathway integration)
   - Conversion chain (JSON generation)
   - Validation chain (quality assurance)

3. **Chain of Thought (CoT)**: Step-by-step reasoning within each process
   - Context assessment
   - Pattern recognition
   - Information extraction
   - Quality evaluation
   - Integration planning

## USAGE:

```python
# Process legislation text
result = process_legislation_sync("Your legislation text here...")

# Results include:
# - Multiple reasoning pathways
# - Specialized extraction chains
# - Synthesized rule sets
# - JSON-compatible rules
# - Comprehensive validation
```

## REQUIREMENTS:
- OpenAI API key with o3-mini access
- pip install langgraph langchain-openai langchain-core

## SETUP:
export OPENAI_API_KEY="your-openai-api-key"

The system produces high-quality, machine-readable JSON rules that accurately 
represent complex legislation through advanced AI reasoning techniques.
"""
