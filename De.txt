"""
Enhanced Legal Document Analyzer with Graph RAG, Vector Embeddings & Semantic Search
COMPLETE VERSION - ALL METHODS INCLUDED
UPDATED: Preserves exact document terminology, machine-readable output, duplicate detection
Complete implementation with vector storage, semantic search, and graph reasoning
Location: src/analyzers/legal_document_analyzer_enhanced.py
"""

from typing import Dict, List, Optional, Any, TypedDict, Annotated, Tuple
import json
from dataclasses import dataclass
import logging
import re
import operator
import os
import math
import asyncio

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langchain_community.graphs import FalkorDBGraph

from src.utils.document_chunker import DocumentChunker
from src.config import Config
from src.services.openai_service import OpenAIService

logger = logging.getLogger(__name__)


# ============================================================================
# EMBEDDING SERVICE (Integrated with OpenAIService and HSBC Auth)
# ============================================================================

class EmbeddingService:
    """
    Embedding service using OpenAIService with full HSBC auth support
    FIXED: Proper client usage for embeddings
    """
    
    def __init__(self, config: Config = None):
        """
        Initialize embedding service with HSBC auth support
        
        Args:
            config: Configuration object with all settings including HSBC auth
        """
        self.config = config or Config()
        self.model = self.config.EMBEDDING_MODEL
        self.dimension = 3072
        self.base_url = self.config.BASE_URL
        
        logger.info(f"Initializing EmbeddingService:")
        logger.info(f"  Model: {self.model}")
        logger.info(f"  Dimension: {self.dimension}")
        logger.info(f"  Base URL: {self.base_url}")
        logger.info(f"  HSBC Auth: {self.config.USE_HSBC_AUTH}")
        
        # Initialize OpenAI service with full config (HSBC or standard)
        self.openai_service = OpenAIService(
            api_key=self.config.API_KEY if not self.config.USE_HSBC_AUTH else None,
            base_url=self.base_url,
            use_hsbc_auth=self.config.USE_HSBC_AUTH
        )
        
        logger.info("✓ EmbeddingService initialized successfully")
    
    def embed_text(self, text: str) -> List[float]:
        """
        Generate embedding for single text
        Handles both HSBC and standard authentication transparently
        
        Args:
            text: Text to embed
            
        Returns:
            List of floats representing the embedding vector (3072 dimensions)
        """
        if not text or not text.strip():
            logger.warning("Empty text provided for embedding")
            return [0.0] * self.dimension
        
        try:
            # Use asyncio event loop to run async method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                embeddings = loop.run_until_complete(
                    self.openai_service.get_embeddings([text])
                )
            finally:
                loop.close()
            
            if embeddings and len(embeddings) > 0:
                embedding = embeddings[0]
                if embedding and len(embedding) == self.dimension:
                    logger.debug(f"Generated embedding: {self.dimension}D")
                    return embedding
                else:
                    logger.warning(f"Unexpected embedding dimension: {len(embedding) if embedding else 0}")
                    return [0.0] * self.dimension
            else:
                logger.warning("No embeddings returned from service")
                return [0.0] * self.dimension
            
        except Exception as e:
            logger.error(f"Embedding error for text (len={len(text)}): {e}")
            import traceback
            traceback.print_exc()
            return [0.0] * self.dimension
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for multiple texts (batch processing)
        More efficient than calling embed_text multiple times
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        if not texts:
            logger.warning("Empty text list provided for batch embedding")
            return []
        
        # Filter out empty texts but track their positions
        filtered_texts = []
        empty_indices = []
        for i, text in enumerate(texts):
            if text and text.strip():
                filtered_texts.append(text)
            else:
                empty_indices.append(i)
        
        if not filtered_texts:
            logger.warning("All texts are empty")
            return [[0.0] * self.dimension for _ in texts]
        
        try:
            # Use asyncio event loop to run async method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                embeddings = loop.run_until_complete(
                    self.openai_service.get_embeddings(filtered_texts)
                )
            finally:
                loop.close()
            
            if embeddings and len(embeddings) == len(filtered_texts):
                # Insert zero embeddings for empty texts at correct positions
                result = []
                filtered_idx = 0
                for i in range(len(texts)):
                    if i in empty_indices:
                        result.append([0.0] * self.dimension)
                    else:
                        result.append(embeddings[filtered_idx])
                        filtered_idx += 1
                
                logger.info(f"Generated {len(result)} embeddings (batch)")
                return result
            else:
                logger.warning(f"Embedding count mismatch: expected {len(filtered_texts)}, got {len(embeddings) if embeddings else 0}")
                return [[0.0] * self.dimension for _ in texts]
            
        except Exception as e:
            logger.error(f"Batch embedding error for {len(texts)} texts: {e}")
            import traceback
            traceback.print_exc()
            return [[0.0] * self.dimension for _ in texts]


# ============================================================================
# FALKORDB KNOWLEDGE GRAPH WITH VECTOR EMBEDDINGS & GRAPH RAG
# COMPLETE VERSION - ALL METHODS INCLUDED
# ============================================================================

class FalkorDBKnowledgeGraph:
    """FalkorDB with vector embeddings, semantic search, and Graph RAG - COMPLETE"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, 
                 graph_name: str = "legal_knowledge_graph",
                 embedding_service: EmbeddingService = None):
        """Initialize FalkorDB with vector capabilities"""
        try:
            self.graph = FalkorDBGraph(
                database=graph_name,
                host=host,
                port=port
            )
            self.graph_name = graph_name
            self.embedding_service = embedding_service
            self.embedding_dim = 3072 if embedding_service else 0
            
            # Statistics
            self.stats = {
                "requirements": 0,
                "actions": 0,
                "evidence": 0,
                "constraints": 0
            }
            
            # Create vector index for semantic search
            self._create_vector_indexes()
            
            logger.info(f"Connected to FalkorDB with vector embeddings: {host}:{port}/{graph_name}")
            
        except Exception as e:
            logger.error(f"FalkorDB connection error: {e}")
            self.graph = None
            self.stats = {}
    
    def _create_vector_indexes(self):
        """Create indexes for vector search and full-text search"""
        if not self.graph:
            return
        
        try:
            # Standard indexes for efficient querying
            self.graph.query("CREATE INDEX FOR (r:Requirement) ON (r.id)")
            self.graph.query("CREATE INDEX FOR (a:Action) ON (a.type)")
            self.graph.query("CREATE INDEX FOR (e:Evidence) ON (e.perspective)")
            self.graph.query("CREATE INDEX FOR (c:Constraint) ON (c.type)")
            
            # Full-text indexes
            self.graph.query("CREATE FULLTEXT INDEX FOR (r:Requirement) ON (r.description)")
            self.graph.query("CREATE FULLTEXT INDEX FOR (a:Action) ON (a.description)")
            self.graph.query("CREATE FULLTEXT INDEX FOR (e:Evidence) ON (e.description)")
            
            logger.info("Created vector and full-text indexes")
        except Exception as e:
            logger.warning(f"Index creation warning (may already exist): {e}")
    
    def _escape_text(self, text: str) -> str:
        """Escape text for Cypher query"""
        if not text:
            return ""
        text = text.replace("'", "\\'")
        text = text.replace('"', '\\"')
        text = text.replace('\n', ' ')
        return text[:2000]
    
    def _serialize_embedding(self, embedding: List[float]) -> str:
        """Serialize embedding vector for storage (store subset)"""
        if not embedding or len(embedding) == 0:
            return ""
        # Store first 100 dimensions as representative sample
        sample = embedding[:100]
        return ",".join([f"{x:.6f}" for x in sample])
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def add_requirement(self, req_id: str, description: str, level: int,
                       classification: str, citations: List[str],
                       embedding: Optional[List[float]] = None):
        """Add requirement node with vector embedding"""
        if not self.graph:
            self.stats["requirements"] = self.stats.get("requirements", 0) + 1
            return
        
        try:
            # Generate embedding if not provided and service is available
            if embedding is None and self.embedding_service and description:
                try:
                    logger.info(f"Generating embedding for requirement: {req_id}")
                    embedding = self.embedding_service.embed_text(description)
                    
                    # Verify embedding is valid
                    if embedding and sum(embedding) != 0:
                        logger.info(f"✓ Generated embedding: {len(embedding)} dimensions")
                    else:
                        logger.warning(f"⚠ Zero embedding generated for {req_id}")
                        embedding = None
                        
                except Exception as e:
                    logger.error(f"Error generating embedding for {req_id}: {e}")
                    embedding = None
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:3]])
            
            # Store embedding sample (first 100 dimensions for storage efficiency)
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            has_embedding = 1 if (embedding and sum(embedding) != 0) else 0
            
            query = f"""
            CREATE (r:Requirement {{
                id: '{req_id}',
                description: '{desc_escaped}',
                level: {level},
                classification: '{classification}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {has_embedding},
                timestamp: timestamp()
            }})
            RETURN r.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["requirements"] = self.stats.get("requirements", 0) + 1
                
                # Store full embedding in cache if valid
                if embedding and sum(embedding) != 0:
                    self._store_full_embedding(req_id, "Requirement", embedding)
                    logger.info(f"✓ Created requirement node with embedding: {req_id}")
                else:
                    logger.info(f"✓ Created requirement node (no embedding): {req_id}")
            
        except Exception as e:
            logger.error(f"Error adding requirement {req_id}: {e}")
            import traceback
            traceback.print_exc()
    
    def add_action(self, action_id: str, action_type: str, description: str,
                  actor: str, req_id: str, citations: List[str],
                  embedding: Optional[List[float]] = None):
        """Add action node with vector embedding"""
        if not self.graph:
            self.stats["actions"] = self.stats.get("actions", 0) + 1
            return
        
        try:
            # Generate embedding if not provided
            if embedding is None and self.embedding_service and description:
                try:
                    embedding = self.embedding_service.embed_text(description)
                    if not embedding or sum(embedding) == 0:
                        embedding = None
                except Exception as e:
                    logger.error(f"Error generating embedding for action {action_id}: {e}")
                    embedding = None
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:2]])
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            has_embedding = 1 if (embedding and sum(embedding) != 0) else 0
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (a:Action {{
                id: '{action_id}',
                type: '{action_type}',
                description: '{desc_escaped}',
                actor: '{actor}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {has_embedding},
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_ACTION]->(a)
            RETURN a.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["actions"] = self.stats.get("actions", 0) + 1
                
                if embedding and sum(embedding) != 0:
                    self._store_full_embedding(action_id, "Action", embedding)
                    logger.info(f"✓ Created action node with embedding: {action_id}")
                else:
                    logger.info(f"✓ Created action node (no embedding): {action_id}")
            
        except Exception as e:
            logger.error(f"Error adding action {action_id}: {e}")
    
    def add_evidence(self, evidence_id: str, evidence_type: str, 
                    description: str, perspective: str, req_id: str,
                    citations: List[str], embedding: Optional[List[float]] = None):
        """Add evidence node with vector embedding"""
        if not self.graph:
            self.stats["evidence"] = self.stats.get("evidence", 0) + 1
            return
        
        try:
            if embedding is None and self.embedding_service and description:
                try:
                    embedding = self.embedding_service.embed_text(description)
                    if not embedding or sum(embedding) == 0:
                        embedding = None
                except Exception as e:
                    logger.error(f"Error generating embedding for evidence {evidence_id}: {e}")
                    embedding = None
            
            desc_escaped = self._escape_text(description)
            citations_str = "|".join([self._escape_text(c) for c in citations[:2]])
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            has_embedding = 1 if (embedding and sum(embedding) != 0) else 0
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (e:Evidence {{
                id: '{evidence_id}',
                type: '{evidence_type}',
                description: '{desc_escaped}',
                perspective: '{perspective}',
                citations: '{citations_str}',
                embedding: '{embedding_str}',
                has_embedding: {has_embedding},
                timestamp: timestamp()
            }})
            CREATE (r)-[:REQUIRES_EVIDENCE]->(e)
            RETURN e.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["evidence"] = self.stats.get("evidence", 0) + 1
                
                if embedding and sum(embedding) != 0:
                    self._store_full_embedding(evidence_id, "Evidence", embedding)
                    logger.info(f"✓ Created evidence node with embedding: {evidence_id}")
                else:
                    logger.info(f"✓ Created evidence node (no embedding): {evidence_id}")
            
        except Exception as e:
            logger.error(f"Error adding evidence {evidence_id}: {e}")
    
    def add_constraint(self, constraint_id: str, constraint_type: str,
                      description: str, operator: str, value: Any, req_id: str,
                      embedding: Optional[List[float]] = None):
        """Add constraint node with vector embedding"""
        if not self.graph:
            self.stats["constraints"] = self.stats.get("constraints", 0) + 1
            return
        
        try:
            if embedding is None and self.embedding_service and description:
                try:
                    embedding = self.embedding_service.embed_text(description)
                    if not embedding or sum(embedding) == 0:
                        embedding = None
                except Exception as e:
                    logger.error(f"Error generating embedding for constraint {constraint_id}: {e}")
                    embedding = None
            
            desc_escaped = self._escape_text(description)
            value_str = self._escape_text(str(value))
            embedding_str = self._serialize_embedding(embedding) if embedding else ""
            has_embedding = 1 if (embedding and sum(embedding) != 0) else 0
            
            query = f"""
            MATCH (r:Requirement {{id: '{req_id}'}})
            CREATE (c:Constraint {{
                id: '{constraint_id}',
                type: '{constraint_type}',
                description: '{desc_escaped}',
                operator: '{operator}',
                value: '{value_str}',
                embedding: '{embedding_str}',
                has_embedding: {has_embedding},
                timestamp: timestamp()
            }})
            CREATE (r)-[:HAS_CONSTRAINT]->(c)
            RETURN c.id AS created_id
            """
            
            result = self.graph.query(query)
            if result:
                self.stats["constraints"] = self.stats.get("constraints", 0) + 1
                
                if embedding and sum(embedding) != 0:
                    self._store_full_embedding(constraint_id, "Constraint", embedding)
                    logger.info(f"✓ Created constraint node with embedding: {constraint_id}")
                else:
                    logger.info(f"✓ Created constraint node (no embedding): {constraint_id}")
            
        except Exception as e:
            logger.error(f"Error adding constraint {constraint_id}: {e}")
    
    def _store_full_embedding(self, node_id: str, node_type: str, embedding: List[float]):
        """Store full embedding vector in separate embedding cache (in-memory for now)"""
        if not hasattr(self, '_embedding_cache'):
            self._embedding_cache = {}
        
        cache_key = f"{node_type}:{node_id}"
        self._embedding_cache[cache_key] = embedding
        logger.debug(f"Cached full embedding for {cache_key} ({len(embedding)}D)")
    
    def _get_full_embedding(self, node_id: str, node_type: str) -> Optional[List[float]]:
        """Retrieve full embedding vector"""
        if not hasattr(self, '_embedding_cache'):
            return None
        
        cache_key = f"{node_type}:{node_id}"
        return self._embedding_cache.get(cache_key)
    
    def batch_generate_embeddings(self, node_type: str = None) -> Dict[str, int]:
        """Batch generate embeddings for nodes that don't have them yet"""
        if not self.graph or not self.embedding_service:
            return {"error": "Graph or embedding service not available"}
        
        try:
            node_types = [node_type] if node_type else ["Requirement", "Action", "Evidence", "Constraint"]
            results = {"generated": 0, "skipped": 0, "errors": 0}
            
            for ntype in node_types:
                # Get nodes without embeddings
                query = f"""
                MATCH (n:{ntype})
                WHERE n.has_embedding = 0 OR n.has_embedding IS NULL
                RETURN n.id AS id, n.description AS description
                LIMIT 50
                """
                
                nodes = self.graph.query(query)
                
                if not nodes:
                    continue
                
                print(f"\nGenerating embeddings for {len(nodes)} {ntype} nodes...")
                
                for node_data in nodes:
                    try:
                        # Handle different result formats
                        if isinstance(node_data, dict):
                            node_id = node_data.get('id', '')
                            description = node_data.get('description', '')
                        elif isinstance(node_data, (list, tuple)) and len(node_data) >= 2:
                            node_id = node_data[0]
                            description = node_data[1]
                        else:
                            results["skipped"] += 1
                            continue
                        
                        if not description:
                            results["skipped"] += 1
                            continue
                        
                        # Generate embedding
                        embedding = self.embedding_service.embed_text(description)
                        
                        if embedding and sum(embedding) != 0:
                            # Update node with embedding
                            embedding_str = self._serialize_embedding(embedding)
                            
                            update_query = f"""
                            MATCH (n:{ntype} {{id: '{node_id}'}})
                            SET n.embedding = '{embedding_str}',
                                n.has_embedding = 1
                            RETURN n.id AS updated_id
                            """
                            
                            update_result = self.graph.query(update_query)
                            
                            if update_result:
                                # Store full embedding in cache
                                self._store_full_embedding(node_id, ntype, embedding)
                                results["generated"] += 1
                                print(f"  ✓ Generated embedding for {node_id}")
                            else:
                                results["errors"] += 1
                        else:
                            results["errors"] += 1
                            
                    except Exception as e:
                        logger.error(f"Error generating embedding for node: {e}")
                        results["errors"] += 1
            
            print(f"\nBatch embedding generation complete:")
            print(f"  Generated: {results['generated']}")
            print(f"  Skipped: {results['skipped']}")
            print(f"  Errors: {results['errors']}")
            
            return results
            
        except Exception as e:
            logger.error(f"Batch embedding generation error: {e}")
            return {"error": str(e)}
    
    def semantic_search(self, query_text: str, node_types: List[str] = None, 
                       top_k: int = 5) -> List[Dict[str, Any]]:
        """Semantic search using vector embeddings"""
        if not self.embedding_service or not self.graph:
            return []
        
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.embed_text(query_text)
            
            if node_types is None:
                node_types = ["Requirement", "Action", "Evidence", "Constraint"]
            
            results = []
            
            for node_type in node_types:
                # Get all nodes with embeddings
                query = f"""
                MATCH (n:{node_type})
                WHERE n.has_embedding = 1
                RETURN n.id AS id, n.description AS description, 
                       n.embedding AS embedding_sample
                LIMIT 100
                """
                
                nodes = self.graph.query(query)
                
                if not nodes:
                    continue
                
                # Calculate similarity scores
                for node_data in nodes:
                    # Handle different result formats
                    if isinstance(node_data, dict):
                        node_id = node_data.get('id', '')
                        description = node_data.get('description', '')
                    elif isinstance(node_data, (list, tuple)) and len(node_data) >= 2:
                        node_id = node_data[0]
                        description = node_data[1]
                    else:
                        continue
                    
                    # Get full embedding from cache
                    node_embedding = self._get_full_embedding(node_id, node_type)
                    
                    if node_embedding:
                        similarity = self._cosine_similarity(query_embedding, node_embedding)
                        
                        results.append({
                            "node_id": node_id,
                            "node_type": node_type,
                            "description": description,
                            "similarity_score": similarity
                        })
            
            # Sort by similarity and return top_k
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Semantic search error: {e}")
            return []
    
    def full_text_search(self, query_text: str, node_types: List[str] = None,
                        top_k: int = 5) -> List[Dict[str, Any]]:
        """Full-text search on node descriptions"""
        if not self.graph:
            return []
        
        try:
            if node_types is None:
                node_types = ["Requirement", "Action", "Evidence"]
            
            results = []
            
            for node_type in node_types:
                # Simple text matching (FalkorDB may not support full FULLTEXT yet)
                # Using CONTAINS for text matching
                query_escaped = self._escape_text(query_text)
                
                query = f"""
                MATCH (n:{node_type})
                WHERE n.description CONTAINS '{query_escaped}'
                RETURN n.id AS id, n.description AS description,
                       n.level AS level, n.type AS type
                LIMIT {top_k}
                """
                
                nodes = self.graph.query(query)
                
                if not nodes:
                    continue
                
                for node_data in nodes:
                    if isinstance(node_data, dict):
                        results.append({
                            "node_id": node_data.get('id', ''),
                            "node_type": node_type,
                            "description": node_data.get('description', ''),
                            "level": node_data.get('level'),
                            "search_method": "full_text"
                        })
                    elif isinstance(node_data, (list, tuple)) and len(node_data) >= 2:
                        results.append({
                            "node_id": node_data[0],
                            "node_type": node_type,
                            "description": node_data[1],
                            "level": node_data[2] if len(node_data) > 2 else None,
                            "search_method": "full_text"
                        })
            
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Full-text search error: {e}")
            return []
    
    def graph_rag_search(self, query_text: str, top_k: int = 5,
                        include_related: bool = True) -> Dict[str, Any]:
        """Graph RAG: Combines semantic search + graph traversal + full-text search"""
        if not self.graph:
            return {"results": [], "related_nodes": [], "graph_paths": []}
        
        try:
            # 1. Semantic search for most relevant nodes
            semantic_results = self.semantic_search(query_text, top_k=top_k)
            
            # 2. Full-text search for keyword matches
            fulltext_results = self.full_text_search(query_text, top_k=top_k)
            
            # 3. Combine and deduplicate
            combined_results = {}
            
            for result in semantic_results:
                node_id = result['node_id']
                combined_results[node_id] = {
                    **result,
                    "search_method": "semantic",
                    "score": result['similarity_score']
                }
            
            for result in fulltext_results:
                node_id = result['node_id']
                if node_id not in combined_results:
                    combined_results[node_id] = {
                        **result,
                        "search_method": "full_text",
                        "score": 0.5  # Default score for text match
                    }
            
            # 4. Graph traversal to find related nodes
            related_nodes = []
            graph_paths = []
            
            if include_related:
                for node_id in list(combined_results.keys())[:3]:  # Top 3 results
                    # Find connected nodes
                    related = self._find_related_nodes(node_id, max_depth=2)
                    related_nodes.extend(related)
                    
                    # Find graph paths
                    paths = self._find_citation_paths(node_id)
                    graph_paths.extend(paths)
            
            # 5. Sort final results by score
            final_results = sorted(
                combined_results.values(),
                key=lambda x: x['score'],
                reverse=True
            )
            
            return {
                "results": final_results[:top_k],
                "related_nodes": related_nodes[:10],
                "graph_paths": graph_paths[:5],
                "search_methods": ["semantic", "full_text", "graph_traversal"]
            }
            
        except Exception as e:
            logger.error(f"Graph RAG search error: {e}")
            return {"results": [], "related_nodes": [], "graph_paths": []}
    
    def _find_related_nodes(self, node_id: str, max_depth: int = 2) -> List[Dict[str, Any]]:
        """Find nodes connected to given node through graph traversal"""
        if not self.graph:
            return []
        
        try:
            query = f"""
            MATCH path = (n {{id: '{node_id}'}})-[*1..{max_depth}]-(related)
            RETURN DISTINCT related.id AS id, 
                   labels(related)[0] AS type,
                   related.description AS description,
                   length(path) AS distance
            LIMIT 10
            """
            
            result = self.graph.query(query)
            
            related = []
            if result:
                for row in result:
                    if isinstance(row, dict):
                        related.append({
                            "node_id": row.get('id', ''),
                            "node_type": row.get('type', ''),
                            "description": row.get('description', ''),
                            "distance": row.get('distance', 0)
                        })
                    elif isinstance(row, (list, tuple)) and len(row) >= 3:
                        related.append({
                            "node_id": row[0],
                            "node_type": row[1],
                            "description": row[2],
                            "distance": row[3] if len(row) > 3 else 0
                        })
            
            return related
            
        except Exception as e:
            logger.error(f"Error finding related nodes: {e}")
            return []
    
    def _find_citation_paths(self, node_id: str) -> List[Dict[str, Any]]:
        """Find citation paths through the graph"""
        if not self.graph:
            return []
        
        try:
            query = f"""
            MATCH path = (n {{id: '{node_id}'}})-[:REQUIRES_ACTION|REQUIRES_EVIDENCE|HAS_CONSTRAINT*1..3]-(related)
            RETURN [node IN nodes(path) | {{id: node.id, type: labels(node)[0]}}] AS path_nodes,
                   [rel IN relationships(path) | type(rel)] AS path_rels
            LIMIT 5
            """
            
            result = self.graph.query(query)
            
            paths = []
            if result:
                for row in result:
                    if isinstance(row, dict):
                        paths.append({
                            "nodes": row.get('path_nodes', []),
                            "relationships": row.get('path_rels', [])
                        })
                    elif isinstance(row, (list, tuple)) and len(row) >= 2:
                        paths.append({
                            "nodes": row[0],
                            "relationships": row[1]
                        })
            
            return paths
            
        except Exception as e:
            logger.error(f"Error finding citation paths: {e}")
            return []
    
    def find_supporting_citations(self, claim: str, node_id: str = None) -> List[Dict[str, Any]]:
        """Find citations that support a specific claim using Graph RAG"""
        if not self.graph:
            return []
        
        try:
            # Use Graph RAG to find relevant nodes
            rag_results = self.graph_rag_search(claim, top_k=5, include_related=True)
            
            citations = []
            
            # Extract citations from results
            for result in rag_results.get('results', []):
                node_id_result = result.get('node_id', '')
                
                # Get the node's citations
                query = f"""
                MATCH (n {{id: '{node_id_result}'}})
                RETURN n.citations AS citations, n.description AS description
                """
                
                node_data = self.graph.query(query)
                
                if node_data:
                    row = node_data[0]
                    if isinstance(row, dict):
                        citation_str = row.get('citations', '')
                        description = row.get('description', '')
                    elif isinstance(row, (list, tuple)) and len(row) >= 2:
                        citation_str = row[0]
                        description = row[1]
                    else:
                        continue
                    
                    if citation_str:
                        # Parse citations
                        citation_texts = citation_str.split('|')
                        for cite_text in citation_texts:
                            if cite_text:
                                citations.append({
                                    "text": cite_text,
                                    "supporting_node": node_id_result,
                                    "node_description": description,
                                    "relevance_score": result.get('score', 0)
                                })
            
            # Sort by relevance
            citations.sort(key=lambda x: x['relevance_score'], reverse=True)
            return citations[:10]
            
        except Exception as e:
            logger.error(f"Error finding supporting citations: {e}")
            return []
    
    def validate_citation_consistency(self, node_id: str) -> Dict[str, Any]:
        """Validate that citations are consistent across related nodes"""
        if not self.graph:
            return {"consistent": True, "conflicts": []}
        
        try:
            # Get node and its related nodes
            related = self._find_related_nodes(node_id, max_depth=1)
            
            # Get citations from all nodes
            all_citations = {}
            
            query = f"""
            MATCH (n {{id: '{node_id}'}})
            RETURN n.citations AS citations, n.description AS description
            """
            
            result = self.graph.query(query)
            
            if result:
                row = result[0]
                if isinstance(row, dict):
                    all_citations[node_id] = row.get('citations', '')
                elif isinstance(row, (list, tuple)):
                    all_citations[node_id] = row[0]
            
            # Compare citations
            conflicts = []
            # Simple validation - check if related nodes have contradicting info
            
            return {
                "consistent": len(conflicts) == 0,
                "conflicts": conflicts,
                "related_nodes_checked": len(related)
            }
            
        except Exception as e:
            logger.error(f"Error validating citations: {e}")
            return {"consistent": True, "conflicts": [], "error": str(e)}
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get graph statistics - FIXED for FalkorDB result structure"""
        if not self.graph:
            return self.stats
        
        try:
            stats = {}
            
            # Count each node type
            for node_type in ["Requirement", "Action", "Evidence", "Constraint"]:
                try:
                    query = f"MATCH (n:{node_type}) RETURN count(n) AS count"
                    result = self.graph.query(query)
                    
                    # FalkorDB returns list of lists or list of Row objects
                    if result and len(result) > 0:
                        first_row = result[0]
                        
                        # Handle different result types
                        if isinstance(first_row, dict):
                            count_value = first_row.get('count', 0)
                        elif isinstance(first_row, (list, tuple)):
                            count_value = first_row[0] if len(first_row) > 0 else 0
                        elif hasattr(first_row, 'count'):
                            count_value = first_row.count
                        else:
                            # Try to access as attribute or index
                            try:
                                count_value = first_row[0]
                            except:
                                count_value = 0
                        
                        stats[node_type.lower()] = int(count_value) if count_value else 0
                    else:
                        stats[node_type.lower()] = 0
                        
                except Exception as e:
                    logger.warning(f"Error counting {node_type}: {e}")
                    stats[node_type.lower()] = 0
            
            # Count relationships
            try:
                query = "MATCH ()-[r]->() RETURN count(r) AS count"
                result = self.graph.query(query)
                
                if result and len(result) > 0:
                    first_row = result[0]
                    
                    # Handle different result types
                    if isinstance(first_row, dict):
                        count_value = first_row.get('count', 0)
                    elif isinstance(first_row, (list, tuple)):
                        count_value = first_row[0] if len(first_row) > 0 else 0
                    elif hasattr(first_row, 'count'):
                        count_value = first_row.count
                    else:
                        try:
                            count_value = first_row[0]
                        except:
                            count_value = 0
                    
                    stats['relationships'] = int(count_value) if count_value else 0
                else:
                    stats['relationships'] = 0
                    
            except Exception as e:
                logger.warning(f"Error counting relationships: {e}")
                stats['relationships'] = 0
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return self.stats


# ============================================================================
# STATE DEFINITIONS FOR LANGGRAPH
# ============================================================================

class AgentState(TypedDict):
    """State for LangGraph agent"""
    messages: Annotated[List[BaseMessage], operator.add]
    chunk_text: str
    chunk_id: int
    level: int
    rule_name: str
    jurisdiction: str
    enterprise_context: Optional[Dict[str, Any]]
    
    description: str
    original_terminology: List[str]
    citations: List[Dict[str, Any]]
    data_actions: List[Dict[str, Any]]
    user_evidence: List[Dict[str, Any]]
    system_evidence: List[Dict[str, Any]]
    constraints: List[Dict[str, Any]]
    enterprise_policies: List[Dict[str, Any]]
    classification: str
    classification_reasoning: str
    
    chain_of_thought: List[str]
    expert_opinions: Dict[str, Any]
    thought_tree: Dict[str, Any]
    
    kg_nodes: List[Dict[str, Any]]
    kg_edges: List[Dict[str, Any]]
    
    next_step: str
    iteration: int
    max_iterations: int
    
    # Machine-readable outputs
    machine_readable_rules: List[Dict[str, Any]]
    potential_duplicates: List[Dict[str, Any]]


# ============================================================================
# LANGGRAPH AGENT NODES - COMPLETE VERSION WITH ALL METHODS
# ============================================================================

class LegalAnalysisAgent:
    """LangGraph agent with advanced reasoning - COMPLETE - Preserves document terminology"""
    
    def __init__(self, llm: ChatOpenAI, kg: FalkorDBKnowledgeGraph):
        self.llm = llm
        self.kg = kg
    
    def chain_of_thought_reasoning(self, state: AgentState) -> AgentState:
        """Chain of Thought reasoning node - PRESERVES EXACT TERMINOLOGY"""
        level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
        level_name = level_names.get(state['level'], "Document")
        
        prompt = f"""Analyze this legal text from {level_name} using systematic step-by-step reasoning.

CRITICAL INSTRUCTION: You MUST preserve the EXACT terminology from the source text. If the text says "mandatory", do NOT change it to "required" or "must". If it says "shall", do NOT change it to "must". Preserve every key term EXACTLY as written.

TEXT:
{state['chunk_text'][:2500]}

Rule: {state['rule_name']}
Jurisdiction: {state['jurisdiction']}
Document Level: Level {state['level']} - {level_name}

IMPORTANT NOTES:
- "DataVisa" is an internal data governance tool
- Ignore classification markers like "INTERNAL" at document end
- PRESERVE EXACT TERMINOLOGY from the source text

Think through systematically:
1. What is the core legal requirement stated in this text? Use EXACT wording from source.
2. Is this a PERMISSION (allows something under conditions) or PROHIBITION (forbids something)?
3. What specific data operations are mentioned using THIS TAXONOMY ONLY:
   - sharing (transferring data to third parties)
   - hosting (storing data on infrastructure)
   - usage (processing or using data)
   - processing (transforming or analyzing data)
   - deletion (removing or destroying data)
   - collection (gathering data from sources)
   - access (viewing or retrieving data)
4. What conditions or constraints apply? Use EXACT wording.
5. What are the duties of the user? What are the duties of the system?

For each point, provide exact citations from the text (max 150 characters each) with source reference.

Return valid JSON:
{{
    "thought_steps": ["clear step 1", "clear step 2", "clear step 3", "clear step 4", "clear step 5"],
    "main_requirement": "Complete sentence using EXACT terminology from source",
    "original_key_terms": ["list", "of", "exact", "key", "terms", "from", "source"],
    "permission_or_prohibition": "permission" or "prohibition",
    "data_operations": [
        {{
            "action": "sharing|hosting|usage|processing|deletion|collection|access",
            "exact_terminology": "exact phrase from source describing this",
            "actor": "user" or "system"
        }}
    ],
    "constraints": [
        {{
            "type": "temporal|spatial|technical|procedural|purpose",
            "exact_terminology": "exact constraint wording from source"
        }}
    ],
    "user_duties": ["exact duty 1 from source", "exact duty 2 from source"],
    "system_duties": ["exact duty 1 from source", "exact duty 2 from source"],
    "citations": [
        {{
            "text": "exact quote from text",
            "reasoning": "why this supports the requirement",
            "source_reference": "Level {state['level']} - {level_name}, Section/Article reference if visible"
        }}
    ]
}}"""
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        try:
            result = self._extract_json(response.content)
            state['chain_of_thought'] = result.get('thought_steps', [])
            
            if result.get('main_requirement'):
                state['description'] += result['main_requirement'] + " "
            
            # Store original terminology
            original_terms = result.get('original_key_terms', [])
            if 'original_terminology' not in state:
                state['original_terminology'] = []
            state['original_terminology'].extend(original_terms)
            
            for cite in result.get('citations', []):
                if cite.get('text') and len(cite['text']) > 20:
                    state['citations'].append({
                        "text": cite['text'],
                        "reasoning": cite.get('reasoning', ''),
                        "source_reference": cite.get('source_reference', f"Level {state['level']} - {level_name}"),
                        "chunk_id": state['chunk_id'],
                        "level": state['level'],
                        "level_name": level_name
                    })
            
            # Set classification based on permission/prohibition
            perm_or_prohib = result.get('permission_or_prohibition', '')
            if perm_or_prohib and not state.get('classification'):
                state['classification'] = perm_or_prohib
            
            # Store data operations with exact terminology
            for op in result.get('data_operations', []):
                state['data_actions'].append({
                    "type": op.get('action', 'usage'),
                    "description": op.get('exact_terminology', ''),
                    "actor": op.get('actor', 'user'),
                    "citations": []
                })
            
            # Store constraints with exact terminology
            for constraint in result.get('constraints', []):
                state['constraints'].append({
                    "type": constraint.get('type', 'general'),
                    "description": constraint.get('exact_terminology', ''),
                    "citations": []
                })
            
            # Store duties
            for duty in result.get('user_duties', []):
                state['user_evidence'].append({
                    "description": duty,
                    "perspective": "user",
                    "citations": []
                })
            
            for duty in result.get('system_duties', []):
                state['system_evidence'].append({
                    "description": duty,
                    "perspective": "system",
                    "citations": []
                })
            
        except Exception as e:
            logger.error(f"Chain of thought error: {e}")
            state['chain_of_thought'].append(f"Error in reasoning: {str(e)}")
        
        state['next_step'] = 'mixture_of_experts'
        return state
    
    def mixture_of_experts(self, state: AgentState) -> AgentState:
        """Mixture of Experts reasoning - PRESERVES TERMINOLOGY"""
        level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
        level_name = level_names.get(state['level'], "Document")
        
        experts = {
            "legal_expert": "Extract legal obligations preserving exact terminology",
            "data_privacy_specialist": "Identify data handling operations using taxonomy: sharing, hosting, usage, processing, deletion, collection, access",
            "technical_architect": "Determine system duties preserving exact wording",
            "compliance_officer": "Identify user duties preserving exact wording"
        }
        
        expert_results = {}
        
        for expert_role, expert_task in experts.items():
            prompt = f"""You are a {expert_role.replace('_', ' ').title()}.

CRITICAL: PRESERVE EXACT TERMINOLOGY from source text. Do NOT paraphrase key legal terms.

Task: {expert_task}

TEXT (from {level_name}):
{state['chunk_text'][:2000]}

Rule: {state['rule_name']}

Use ONLY these action types: sharing, hosting, usage, processing, deletion, collection, access

Provide JSON:
{{
    "key_findings": ["finding 1 with exact terminology", "finding 2 with exact terminology"],
    "data_actions": [
        {{
            "type": "sharing|hosting|usage|processing|deletion|collection|access",
            "description": "EXACT wording from source",
            "actor": "user" or "system",
            "exact_terminology": "preserve key terms exactly",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ],
    "user_duties": [
        {{
            "description": "EXACT wording from source",
            "perspective": "user",
            "exact_terminology": "preserve key terms",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ],
    "system_duties": [
        {{
            "description": "EXACT wording from source",
            "perspective": "system",
            "exact_terminology": "preserve key terms",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ],
    "constraints": [
        {{
            "type": "temporal|spatial|technical|procedural|purpose",
            "description": "EXACT wording from source",
            "exact_terminology": "preserve constraint terms",
            "citations": [{{"text": "exact quote", "source_reference": "Level {state['level']} - {level_name}"}}]
        }}
    ]
}}"""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                expert_results[expert_role] = self._extract_json(response.content)
            except Exception as e:
                logger.error(f"Expert {expert_role} error: {e}")
                expert_results[expert_role] = {"error": str(e)}
        
        state['expert_opinions'] = expert_results
        self._consolidate_expert_findings(state, expert_results, level_name)
        
        state['next_step'] = 'tree_of_thought'
        return state
    
    def tree_of_thought(self, state: AgentState) -> AgentState:
        """Tree of Thought exploration"""
        prompt = f"""Explore alternative interpretations of this rule while PRESERVING exact terminology.

TEXT:
{state['chunk_text'][:2000]}

Current understanding:
- Description: {state['description'][:300]}
- Actions: {len(state['data_actions'])}
- Evidence: {len(state['user_evidence']) + len(state['system_evidence'])}

Generate 3 interpretation paths using EXACT terminology from source:

Path 1 - Conservative: Strictest reading
Path 2 - Balanced: Reasonable middle-ground
Path 3 - Expansive: Broadest reasonable interpretation

Return JSON:
{{
    "paths": [
        {{
            "name": "conservative",
            "interpretation": "Complete explanation with exact terms",
            "implications": ["implication 1", "implication 2"],
            "required_actions": ["action 1 with exact terminology", "action 2 with exact terminology"],
            "confidence": "high" or "medium" or "low"
        }}
    ],
    "recommended_path": "conservative" or "balanced" or "expansive",
    "reasoning": "Complete explanation"
}}"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = self._extract_json(response.content)
            state['thought_tree'] = result
            
            state['chain_of_thought'].append(
                f"Explored {len(result.get('paths', []))} interpretation paths. "
                f"Recommended: {result.get('recommended_path', 'balanced')}"
            )
        except Exception as e:
            logger.error(f"Tree of thought error: {e}")
            state['thought_tree'] = {"error": str(e)}
        
        state['next_step'] = 'knowledge_graph'
        return state
    
    def knowledge_graph_integration(self, state: AgentState) -> AgentState:
        """Integrate into FalkorDB with vector embeddings and validation"""
        chunk_id = state['chunk_id']
        level = state['level']
        
        req_id = f"req_{state['rule_name']}_{level}_{chunk_id}".replace(" ", "_").replace("/", "_")
        
        # Generate embedding for requirement
        req_embedding = None
        if self.kg.embedding_service and state['description']:
            req_embedding = self.kg.embedding_service.embed_text(state['description'])
        
        # Add requirement node with embedding
        self.kg.add_requirement(
            req_id,
            state['description'],
            level,
            state['classification'],
            [c['text'] for c in state['citations'][:5]],
            embedding=req_embedding
        )
        
        # Add action nodes with embeddings
        for i, action in enumerate(state['data_actions']):
            action_id = f"{req_id}_action_{i}"
            action_desc = action.get('description', '')
            
            # Generate embedding for action
            action_embedding = None
            if self.kg.embedding_service and action_desc:
                action_embedding = self.kg.embedding_service.embed_text(action_desc)
            
            self.kg.add_action(
                action_id,
                action.get('type', 'usage'),
                action_desc,
                action.get('actor', 'unknown'),
                req_id,
                [c.get('text', '') for c in action.get('citations', [])],
                embedding=action_embedding
            )
        
        # Add user evidence nodes with embeddings
        for i, evidence in enumerate(state['user_evidence']):
            evidence_id = f"{req_id}_evidence_user_{i}"
            evidence_desc = evidence.get('description', '')
            
            evidence_embedding = None
            if self.kg.embedding_service and evidence_desc:
                evidence_embedding = self.kg.embedding_service.embed_text(evidence_desc)
            
            self.kg.add_evidence(
                evidence_id,
                'user_requirement',
                evidence_desc,
                'user',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])],
                embedding=evidence_embedding
            )
        
        # Add system evidence nodes with embeddings
        for i, evidence in enumerate(state['system_evidence']):
            evidence_id = f"{req_id}_evidence_system_{i}"
            evidence_desc = evidence.get('description', '')
            
            evidence_embedding = None
            if self.kg.embedding_service and evidence_desc:
                evidence_embedding = self.kg.embedding_service.embed_text(evidence_desc)
            
            self.kg.add_evidence(
                evidence_id,
                'system_requirement',
                evidence_desc,
                'system',
                req_id,
                [c.get('text', '') for c in evidence.get('citations', [])],
                embedding=evidence_embedding
            )
        
        # Add constraint nodes with embeddings
        for i, constraint in enumerate(state['constraints']):
            constraint_id = f"{req_id}_constraint_{i}"
            constraint_desc = constraint.get('description', '')
            
            constraint_embedding = None
            if self.kg.embedding_service and constraint_desc:
                constraint_embedding = self.kg.embedding_service.embed_text(constraint_desc)
            
            self.kg.add_constraint(
                constraint_id,
                constraint.get('type', 'general'),
                constraint_desc,
                constraint.get('operator', 'eq'),
                constraint.get('right_operand', ''),
                req_id,
                embedding=constraint_embedding
            )
        
        # Get statistics
        stats = self.kg.get_statistics()
        state['kg_nodes'] = [{"type": k, "count": v} for k, v in stats.items()]
        state['chain_of_thought'].append(f"Graph integration with embeddings: {stats}")
        
        state['next_step'] = 'duplicate_detection'
        return state
    
    def duplicate_detection(self, state: AgentState) -> AgentState:
        """Detect and flag potential duplicates using semantic similarity"""
        
        if not state['description']:
            state['next_step'] = 'validate'
            return state
        
        # Use semantic search to find similar requirements
        similar_requirements = self.kg.semantic_search(
            state['description'],
            node_types=["Requirement"],
            top_k=5
        )
        
        # Check for high similarity (> 0.90 = likely duplicate)
        potential_duplicates = []
        for sim_req in similar_requirements:
            similarity = sim_req.get('similarity_score', 0)
            if similarity > 0.90:
                potential_duplicates.append({
                    "node_id": sim_req['node_id'],
                    "description": sim_req['description'],
                    "similarity_score": similarity
                })
        
        if potential_duplicates:
            state['chain_of_thought'].append(
                f"⚠ Found {len(potential_duplicates)} potential duplicates with similarity > 0.90"
            )
            
            # Store in state
            if 'potential_duplicates' not in state:
                state['potential_duplicates'] = []
            state['potential_duplicates'].extend(potential_duplicates)
            
            logger.warning(f"Potential duplicates detected for chunk {state['chunk_id']}")
        
        state['next_step'] = 'validate'
        return state
    
    def validate_and_refine(self, state: AgentState) -> AgentState:
        """Validation and refinement"""
        issues = []
        
        if len(state['description']) < 100:
            issues.append("Description too short")
        
        if len(state['citations']) == 0:
            issues.append("No citations")
        
        if len(state['data_actions']) == 0:
            issues.append("No data actions")
        
        if len(state['user_evidence']) == 0 and len(state['system_evidence']) == 0:
            issues.append("No evidence")
        
        if not state.get('classification'):
            issues.append("No classification")
        
        if issues and state['iteration'] < state['max_iterations']:
            level_names = {1: "Primary Legislation", 2: "Regulatory Guidance", 3: "Enterprise Policy"}
            level_name = level_names.get(state['level'], "Document")
            
            prompt = f"""Refine this analysis to address: {', '.join(issues)}

CRITICAL: PRESERVE EXACT TERMINOLOGY from source text.

Current state:
- Description: {state['description'][:400]}
- Citations: {len(state['citations'])}
- Actions: {len(state['data_actions'])}

Original text (from {level_name}):
{state['chunk_text'][:2000]}

Extract what was missed using EXACT terminology from source. Include source references for all citations (Level {state['level']} - {level_name}). 

Use ONLY these action types: sharing, hosting, usage, processing, deletion, collection, access

Return JSON."""
            
            try:
                response = self.llm.invoke([HumanMessage(content=prompt)])
                refinements = self._extract_json(response.content)
                
                if refinements.get('description'):
                    state['description'] += " " + refinements['description']
                
                for cite in refinements.get('citations', []):
                    if cite.get('text'):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                        state['citations'].append(cite)
                
                for action in refinements.get('data_actions', []):
                    state['data_actions'].append(action)
                
                for evidence in refinements.get('user_evidence', []):
                    state['user_evidence'].append(evidence)
                
                for evidence in refinements.get('system_evidence', []):
                    state['system_evidence'].append(evidence)
                
                if refinements.get('classification') and not state.get('classification'):
                    state['classification'] = refinements['classification']
                
                state['iteration'] += 1
                state['next_step'] = 'validate'
            except Exception as e:
                logger.error(f"Refinement error: {e}")
                state['next_step'] = 'machine_readable_generation'
        else:
            state['next_step'] = 'machine_readable_generation'
        
        return state
    
    def machine_readable_generation(self, state: AgentState) -> AgentState:
        """Generate machine-readable structured output"""
        
        # Generate machine-readable rules
        machine_readable = []
        
        for action in state['data_actions']:
            rule = {
                "rule_id": f"{state['rule_name']}_{state['chunk_id']}_{len(machine_readable)}",
                "rule_name": state['rule_name'],
                "level": state['level'],
                "permission_or_prohibition": state.get('classification', 'permission'),
                "action_type": action.get('type', 'usage'),
                "action_description": action.get('description', ''),
                "actor": action.get('actor', 'user'),
                "constraints": [],
                "user_duties": [],
                "system_duties": [],
                "citations": action.get('citations', [])
            }
            
            # Add constraints
            for constraint in state['constraints']:
                rule['constraints'].append({
                    "type": constraint.get('type', 'general'),
                    "description": constraint.get('description', ''),
                    "exact_terminology": constraint.get('description', '')
                })
            
            # Add user duties
            for duty in state['user_evidence']:
                rule['user_duties'].append({
                    "description": duty.get('description', ''),
                    "exact_terminology": duty.get('description', '')
                })
            
            # Add system duties
            for duty in state['system_evidence']:
                rule['system_duties'].append({
                    "description": duty.get('description', ''),
                    "exact_terminology": duty.get('description', '')
                })
            
            machine_readable.append(rule)
        
        state['machine_readable_rules'] = machine_readable
        state['chain_of_thought'].append(f"Generated {len(machine_readable)} machine-readable rules")
        
        state['next_step'] = 'end'
        return state
    
    def _extract_json(self, text: str) -> Dict[str, Any]:
        """Extract JSON from response"""
        try:
            return json.loads(text)
        except:
            pass
        
        match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
        
        pattern = r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}'
        matches = re.findall(pattern, text, re.DOTALL)
        for match in sorted(matches, key=len, reverse=True):
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and len(parsed) > 2:
                    return parsed
            except:
                continue
        
        return {}
    
    def _consolidate_expert_findings(self, state: AgentState, expert_results: Dict[str, Any], level_name: str):
        """Consolidate expert findings"""
        for expert_role, results in expert_results.items():
            if 'error' in results:
                continue
            
            for action in results.get('data_actions', []):
                if action.get('description'):
                    # Ensure source reference
                    for cite in action.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['data_actions'].append(action)
            
            for duty in results.get('user_duties', []):
                if duty.get('description'):
                    for cite in duty.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['user_evidence'].append(duty)
            
            for duty in results.get('system_duties', []):
                if duty.get('description'):
                    for cite in duty.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['system_evidence'].append(duty)
            
            for constraint in results.get('constraints', []):
                if constraint.get('description'):
                    # Ensure source reference
                    for cite in constraint.get('citations', []):
                        if 'source_reference' not in cite:
                            cite['source_reference'] = f"Level {state['level']} - {level_name}"
                    state['constraints'].append(constraint)


# ============================================================================
# ENHANCED ANALYZER - COMPLETE VERSION
# ============================================================================

class EnhancedLegalDocumentAnalyzer:
    """
    Enhanced analyzer with graph database, embeddings, and HSBC authentication
    COMPLETE VERSION - ALL METHODS INCLUDED
    UPDATED: Preserves exact terminology, generates machine-readable output, detects duplicates
    """
    
    """
Enhanced Legal Document Analyzer - Initialization Section
FIXED: Proper LangChain client initialization with HSBC auth
This is just the __init__ method - paste this into your analyzer
"""

    def __init__(self, config: Config = None):
        """
        Initialize Enhanced Legal Document Analyzer
        FIXED: Proper LangChain integration using http_client
        """
        self.config = config or Config()
        
        print(f"\n{'='*60}")
        print(f"INITIALIZING ENHANCED LEGAL DOCUMENT ANALYZER")
        print(f"{'='*60}")
        
        # Display configuration
        chat_model = self.config.CHAT_MODEL
        embedding_model = self.config.EMBEDDING_MODEL
        use_hsbc_auth = self.config.USE_HSBC_AUTH
        base_url = self.config.BASE_URL
        
        print(f"\nConfiguration:")
        print(f"  Chat Model: {chat_model}")
        print(f"  Embedding Model: {embedding_model}")
        print(f"  Base URL: {base_url}")
        print(f"  HSBC Auth: {use_hsbc_auth}")
        if use_hsbc_auth:
            print(f"  HSBC User: {self.config.HSBC_USER_ID}")
            print(f"  HSBC Username: {self.config.HSBC_USERNAME}")
        
        # Initialize LLM with HSBC auth support
        print(f"\n✓ Initializing LLM...")
        try:
            if use_hsbc_auth:
                # CRITICAL FIX: Use create_langchain_client which properly uses http_client
                print(f"  Using HSBC authentication with automatic token refresh")
                from src.services.hsbc_openai_client import create_langchain_client
                self.llm = create_langchain_client()
                print(f"  ✓ LLM initialized with HSBC authentication")
                print(f"  ✓ Token will auto-refresh on expiration")
            else:
                # Use standard ChatOpenAI
                from langchain_openai import ChatOpenAI
                self.llm = ChatOpenAI(
                    model=chat_model,
                    openai_api_key=self.config.API_KEY,
                    openai_api_base=base_url
                )
                print(f"  ✓ LLM initialized: {chat_model}")
        except Exception as e:
            print(f"  ✗ LLM initialization error: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # Initialize document chunker
        print(f"\n✓ Initializing document chunker...")
        from src.utils.document_chunker import DocumentChunker
        self.chunker = DocumentChunker(
            chunk_size=getattr(self.config, 'CHUNK_SIZE', 4000),
            chunk_overlap=getattr(self.config, 'OVERLAP_SIZE', 300),
            respect_boundaries=True
        )
        print(f"  ✓ Chunker initialized")
        
        # Initialize embedding service with full config
        print(f"\n✓ Initializing embedding service...")
        print(f"  Model: {embedding_model}")
        print(f"  Base URL: {base_url}")
        print(f"  HSBC Auth: {use_hsbc_auth}")
        
        try:
            # EmbeddingService is defined in this same file
            self.embedding_service = EmbeddingService(config=self.config)
            print(f"  ✓ Embedding service initialized")
        except Exception as e:
            print(f"  ✗ Embedding service error: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # Test embedding generation
        print(f"\n✓ Testing embedding generation...")
        try:
            test_text = "Test connection for legal analysis with HSBC authentication"
            test_embedding = self.embedding_service.embed_text(test_text)
            if test_embedding and sum(test_embedding) != 0:
                print(f"  ✓ Embedding test successful: {len(test_embedding)} dimensions")
                print(f"  ✓ Sample values: {test_embedding[:3]}")
            else:
                print(f"  ⚠ Warning: Embedding test returned zero vector")
        except Exception as e:
            print(f"  ✗ Embedding test error: {e}")
            import traceback
            traceback.print_exc()
        
        # Initialize FalkorDB
        falkordb_host = getattr(self.config, 'FALKORDB_HOST', 'localhost')
        falkordb_port = getattr(self.config, 'FALKORDB_PORT', 6379)
        
        print(f"\n✓ Connecting to FalkorDB...")
        print(f"  Host: {falkordb_host}")
        print(f"  Port: {falkordb_port}")
        
        try:
            # FalkorDBKnowledgeGraph is defined in this same file
            self.kg = FalkorDBKnowledgeGraph(
                host=falkordb_host,
                port=falkordb_port,
                embedding_service=self.embedding_service
            )
            print(f"  ✓ FalkorDB connected")
        except Exception as e:
            print(f"  ✗ FalkorDB connection error: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # Initialize LangGraph workflow
        print(f"\n✓ Initializing LangGraph workflow...")
        try:
            # LegalAnalysisAgent is defined in this same file
            self.agent = LegalAnalysisAgent(self.llm, self.kg)
            self.workflow = self._build_workflow()
            print(f"  ✓ Workflow initialized with duplicate detection")
            print(f"  ✓ All reasoning nodes will use HSBC-authenticated LLM")
        except Exception as e:
            print(f"  ✗ Workflow error: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print(f"\n{'='*60}")
        print(f"✓ INITIALIZATION COMPLETE")
        print(f"{'='*60}")
        print(f"  Embeddings: {'HSBC auth' if use_hsbc_auth else 'Standard'} with auto token refresh")
        print(f"  Chat/Reasoning: {'HSBC auth' if use_hsbc_auth else 'Standard'} with auto token refresh")
        print(f"  All 401 errors will trigger automatic token refresh")
        print(f"{'='*60}\n")
    
    def _build_workflow(self) -> StateGraph:
        """Build LangGraph workflow with duplicate detection"""
        workflow = StateGraph(AgentState)
        
        # Add all nodes including new ones
        workflow.add_node("chain_of_thought", self.agent.chain_of_thought_reasoning)
        workflow.add_node("mixture_of_experts", self.agent.mixture_of_experts)
        workflow.add_node("tree_of_thought", self.agent.tree_of_thought)
        workflow.add_node("knowledge_graph", self.agent.knowledge_graph_integration)
        workflow.add_node("duplicate_detection", self.agent.duplicate_detection)
        workflow.add_node("validate", self.agent.validate_and_refine)
        workflow.add_node("machine_readable_generation", self.agent.machine_readable_generation)
        
        # Set entry point
        workflow.set_entry_point("chain_of_thought")
        
        # Add conditional edges with proper routing
        workflow.add_conditional_edges(
            "chain_of_thought",
            lambda state: state['next_step'],
            {
                "mixture_of_experts": "mixture_of_experts",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "mixture_of_experts",
            lambda state: state['next_step'],
            {
                "tree_of_thought": "tree_of_thought",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "tree_of_thought",
            lambda state: state['next_step'],
            {
                "knowledge_graph": "knowledge_graph",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "knowledge_graph",
            lambda state: state['next_step'],
            {
                "duplicate_detection": "duplicate_detection",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "duplicate_detection",
            lambda state: state['next_step'],
            {
                "validate": "validate",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "validate",
            lambda state: state['next_step'],
            {
                "validate": "validate",
                "machine_readable_generation": "machine_readable_generation",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "machine_readable_generation",
            lambda state: state['next_step'],
            {
                "end": END
            }
        )
        
        return workflow.compile()
    
    def analyze_chunk(self, chunk: Dict[str, Any], rule_name: str,
                     jurisdiction: str, level: int,
                     enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze single chunk using LangGraph workflow"""
        
        initial_state = AgentState(
            messages=[],
            chunk_text=chunk['text'],
            chunk_id=chunk['chunk_id'],
            level=level,
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            enterprise_context=enterprise_context,
            description="",
            original_terminology=[],
            citations=[],
            data_actions=[],
            user_evidence=[],
            system_evidence=[],
            constraints=[],
            enterprise_policies=[],
            classification="",
            classification_reasoning="",
            chain_of_thought=[],
            expert_opinions={},
            thought_tree={},
            kg_nodes=[],
            kg_edges=[],
            next_step="chain_of_thought",
            iteration=0,
            max_iterations=2,
            machine_readable_rules=[],
            potential_duplicates=[]
        )
        
        try:
            final_state = self.workflow.invoke(initial_state)
            
            return {
                "description": final_state['description'].strip(),
                "original_terminology": final_state.get('original_terminology', []),
                "citations": final_state['citations'],
                "data_actions": final_state['data_actions'],
                "user_evidence": final_state['user_evidence'],
                "system_evidence": final_state['system_evidence'],
                "constraints": final_state['constraints'],
                "enterprise_policies": final_state['enterprise_policies'],
                "classification": final_state['classification'] or "permission",
                "classification_reasoning": final_state['classification_reasoning'],
                "machine_readable_rules": final_state.get('machine_readable_rules', []),
                "potential_duplicates": final_state.get('potential_duplicates', []),
                "metadata": {
                    "chunk_id": chunk['chunk_id'],
                    "level": level,
                    "kg_integration": {
                        "nodes": final_state['kg_nodes']
                    }
                }
            }
        except Exception as e:
            logger.error(f"Workflow error for chunk {chunk['chunk_id']}: {e}")
            import traceback
            traceback.print_exc()
            return self._empty_analysis(chunk['chunk_id'], level)
    
    def analyze_document(self, rule_name: str, jurisdiction: str,
                        document_text: str, level: int,
                        enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze complete document at one level"""
        print(f"\n{'='*60}")
        print(f"Level {level}: {rule_name}")
        print(f"Document: {len(document_text)} characters")
        print(f"{'='*60}")
        
        # Chunk document
        chunks = self.chunker.chunk_document(
            text=document_text,
            metadata={"rule_name": rule_name, "jurisdiction": jurisdiction, "level": level}
        )
        
        print(f"Created {len(chunks)} chunks")
        
        # Analyze each chunk
        chunk_analyses = []
        for i, chunk in enumerate(chunks):
            print(f"\nChunk {i+1}/{len(chunks)}...")
            
            analysis = self.analyze_chunk(
                chunk, rule_name, jurisdiction, level, enterprise_context
            )
            
            if analysis:
                chunk_analyses.append(analysis)
                print(f"  ✓ Description: {len(analysis['description'])} chars")
                print(f"  ✓ Citations: {len(analysis['citations'])}")
                print(f"  ✓ Actions: {len(analysis['data_actions'])}")
                print(f"  ✓ User Duties: {len(analysis['user_evidence'])}")
                print(f"  ✓ System Duties: {len(analysis['system_evidence'])}")
                print(f"  ✓ Machine-readable rules: {len(analysis.get('machine_readable_rules', []))}")
                
                # Report duplicates
                potential_dupes = analysis.get('potential_duplicates', [])
                if potential_dupes:
                    print(f"  ⚠ Potential duplicates: {len(potential_dupes)}")
        
        # Merge chunk analyses
        print(f"\nMerging {len(chunk_analyses)} chunk analyses...")
        final = self._merge_analyses(chunk_analyses, rule_name, jurisdiction, level)
        
        print(f"✓ Level {level} complete:")
        print(f"  Final description: {len(final['description'])} chars")
        print(f"  Total citations: {len(final['citations'])}")
        print(f"  Total actions: {len(final['data_actions'])}")
        print(f"  Machine-readable rules: {len(final.get('machine_readable_rules', []))}")
        
        return final
    
    def _merge_analyses(self, analyses: List[Dict[str, Any]], rule_name: str,
                       jurisdiction: str, level: int) -> Dict[str, Any]:
        """Merge multiple chunk analyses into single document analysis"""
        if not analyses:
            return self._empty_analysis(0, level)
        
        # Merge descriptions
        descriptions = [a['description'] for a in analyses if a['description']]
        combined_desc = " ".join(descriptions).strip()
        if combined_desc and not combined_desc.endswith('.'):
            combined_desc += '.'
        
        # Collect all original terminology
        all_terminology = []
        for analysis in analyses:
            all_terminology.extend(analysis.get('original_terminology', []))
        # Deduplicate
        all_terminology = list(set(all_terminology))
        
        # Deduplicate citations
        all_citations = []
        seen_cites = set()
        for analysis in analyses:
            for cite in analysis.get('citations', []):
                cite_key = cite.get('text', '')[:50]
                if cite_key and cite_key not in seen_cites:
                    all_citations.append(cite)
                    seen_cites.add(cite_key)
        
        # Deduplicate data actions
        all_actions = []
        seen_actions = set()
        for analysis in analyses:
            for action in analysis.get('data_actions', []):
                action_key = (action.get('type', ''), action.get('description', '')[:50])
                if action_key[1] and action_key not in seen_actions:
                    all_actions.append(action)
                    seen_actions.add(action_key)
        
        # Deduplicate user evidence
        all_user_evidence = []
        seen_user = set()
        for analysis in analyses:
            for evidence in analysis.get('user_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_user:
                    all_user_evidence.append(evidence)
                    seen_user.add(evidence_key)
        
        # Deduplicate system evidence
        all_system_evidence = []
        seen_system = set()
        for analysis in analyses:
            for evidence in analysis.get('system_evidence', []):
                evidence_key = evidence.get('description', '')[:50]
                if evidence_key and evidence_key not in seen_system:
                    all_system_evidence.append(evidence)
                    seen_system.add(evidence_key)
        
        # Deduplicate constraints
        all_constraints = []
        seen_constraints = set()
        for analysis in analyses:
            for constraint in analysis.get('constraints', []):
                constraint_key = (constraint.get('type', ''), constraint.get('description', '')[:50])
                if constraint_key[1] and constraint_key not in seen_constraints:
                    all_constraints.append(constraint)
                    seen_constraints.add(constraint_key)
        
        # Collect all machine-readable rules
        all_machine_readable = []
        for analysis in analyses:
            all_machine_readable.extend(analysis.get('machine_readable_rules', []))
        
        # Collect all potential duplicates
        all_potential_duplicates = []
        for analysis in analyses:
            all_potential_duplicates.extend(analysis.get('potential_duplicates', []))
        
        # Determine final classification (prohibition takes precedence)
        classifications = [a.get('classification', '') for a in analyses if a.get('classification')]
        final_classification = "permission"
        if "prohibition" in classifications:
            final_classification = "prohibition"
        elif classifications:
            final_classification = classifications[0]
        
        # Merge reasoning
        reasonings = [a.get('classification_reasoning', '') for a in analyses if a.get('classification_reasoning')]
        combined_reasoning = " ".join(reasonings)
        
        return {
            "description": combined_desc,
            "original_terminology": all_terminology,
            "citations": all_citations,
            "data_actions": all_actions,
            "user_evidence": all_user_evidence,
            "system_evidence": all_system_evidence,
            "constraints": all_constraints,
            "enterprise_policies": [],
            "classification": final_classification,
            "classification_reasoning": combined_reasoning,
            "machine_readable_rules": all_machine_readable,
            "potential_duplicates": all_potential_duplicates,
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "level": level,
                "chunks_processed": len(analyses),
                "total_citations": len(all_citations),
                "total_actions": len(all_actions),
                "total_user_evidence": len(all_user_evidence),
                "total_system_evidence": len(all_system_evidence),
                "total_constraints": len(all_constraints),
                "total_machine_readable_rules": len(all_machine_readable),
                "total_potential_duplicates": len(all_potential_duplicates),
                "kg_stats": self.kg.get_statistics()
            }
        }
    
    def _empty_analysis(self, chunk_id: int, level: int) -> Dict[str, Any]:
        """Return empty analysis structure"""
        return {
            "description": "",
            "original_terminology": [],
            "citations": [],
            "data_actions": [],
            "user_evidence": [],
            "system_evidence": [],
            "constraints": [],
            "enterprise_policies": [],
            "classification": "permission",
            "classification_reasoning": "",
            "machine_readable_rules": [],
            "potential_duplicates": [],
            "metadata": {"chunk_id": chunk_id, "level": level}
        }
    
    def analyze_multi_level(self, rule_name: str, jurisdiction: str,
                           level_1_text: str, level_2_text: str, level_3_text: str,
                           enterprise_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Analyze documents across three levels with Graph RAG capabilities
        COMPLETE VERSION - ALL METHODS INCLUDED
        UPDATED: Preserves terminology, generates machine-readable output, detects duplicates
        """
        print(f"\n{'#'*60}")
        print(f"# MULTI-LEVEL ANALYSIS: {rule_name}")
        print(f"# Graph RAG with Vector Embeddings & Semantic Search")
        print(f"# Duplicate Detection & Machine-Readable Output")
        print(f"# Authentication: {'HSBC' if self.config.USE_HSBC_AUTH else 'Standard OpenAI'}")
        print(f"{'#'*60}")
        
        # Analyze Level 1: Primary Legislation
        print(f"\n>>> LEVEL 1: PRIMARY LEGISLATION ({len(level_1_text)} chars)")
        level_1 = self.analyze_document(rule_name, jurisdiction, level_1_text, 1, enterprise_context)
        
        # Analyze Level 2: Regulatory Guidance
        print(f"\n>>> LEVEL 2: REGULATORY GUIDANCE ({len(level_2_text)} chars)")
        level_2 = self.analyze_document(rule_name, jurisdiction, level_2_text, 2, enterprise_context)
        
        # Analyze Level 3: Enterprise Policy
        print(f"\n>>> LEVEL 3: ENTERPRISE POLICY ({len(level_3_text)} chars)")
        level_3 = self.analyze_document(rule_name, jurisdiction, level_3_text, 3, enterprise_context)
        
        # Combine all levels
        combined = {
            "description": " ".join([
                level_1['description'],
                level_2['description'],
                level_3['description']
            ]).strip(),
            "original_terminology": (
                level_1.get('original_terminology', []) +
                level_2.get('original_terminology', []) +
                level_3.get('original_terminology', [])
            ),
            "citations": level_1['citations'] + level_2['citations'] + level_3['citations'],
            "data_actions": level_1['data_actions'] + level_2['data_actions'] + level_3['data_actions'],
            "user_evidence": level_1['user_evidence'] + level_2['user_evidence'] + level_3['user_evidence'],
            "system_evidence": level_1['system_evidence'] + level_2['system_evidence'] + level_3['system_evidence'],
            "constraints": level_1['constraints'] + level_2['constraints'] + level_3['constraints'],
            "enterprise_policies": level_3.get('enterprise_policies', []),
            "classification": level_1['classification'],
            "classification_reasoning": " ".join([
                level_1.get('classification_reasoning', ''),
                level_2.get('classification_reasoning', ''),
                level_3.get('classification_reasoning', '')
            ]).strip(),
            "machine_readable_rules": (
                level_1.get('machine_readable_rules', []) +
                level_2.get('machine_readable_rules', []) +
                level_3.get('machine_readable_rules', [])
            ),
            "potential_duplicates": (
                level_1.get('potential_duplicates', []) +
                level_2.get('potential_duplicates', []) +
                level_3.get('potential_duplicates', [])
            ),
            "metadata": {
                "rule_name": rule_name,
                "jurisdiction": jurisdiction,
                "enterprise_context": enterprise_context,
                "authentication_mode": "HSBC" if self.config.USE_HSBC_AUTH else "Standard",
                "level_1_chunks": level_1['metadata']['chunks_processed'],
                "level_2_chunks": level_2['metadata']['chunks_processed'],
                "level_3_chunks": level_3['metadata']['chunks_processed'],
                "total_citations": len(level_1['citations']) + len(level_2['citations']) + len(level_3['citations']),
                "total_actions": len(level_1['data_actions']) + len(level_2['data_actions']) + len(level_3['data_actions']),
                "total_machine_readable_rules": (
                    len(level_1.get('machine_readable_rules', [])) +
                    len(level_2.get('machine_readable_rules', [])) +
                    len(level_3.get('machine_readable_rules', []))
                ),
                "total_potential_duplicates": (
                    len(level_1.get('potential_duplicates', [])) +
                    len(level_2.get('potential_duplicates', [])) +
                    len(level_3.get('potential_duplicates', []))
                ),
                "kg_stats": self.kg.get_statistics()
            }
        }
        
        print(f"\n{'#'*60}")
        print(f"# GRAPH RAG ANALYSIS COMPLETE")
        print(f"{'#'*60}")
        print(f"  Description: {len(combined['description'])} chars")
        print(f"  Original Terminology Preserved: {len(combined.get('original_terminology', []))} terms")
        print(f"  Citations: {len(combined['citations'])}")
        print(f"  Actions: {len(combined['data_actions'])}")
        print(f"  User Duties: {len(combined['user_evidence'])}")
        print(f"  System Duties: {len(combined['system_evidence'])}")
        print(f"  Constraints: {len(combined['constraints'])}")
        print(f"  Machine-Readable Rules: {len(combined.get('machine_readable_rules', []))}")
        print(f"  Potential Duplicates: {len(combined.get('potential_duplicates', []))}")
        
        # Display graph statistics
        kg_stats = combined['metadata']['kg_stats']
        if kg_stats:
            print(f"\n{'='*60}")
            print(f"GRAPH DATABASE STATISTICS:")
            print(f"{'='*60}")
            for key, value in kg_stats.items():
                print(f"  {key.title()}: {value}")
        
        return combined
