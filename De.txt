"""
Optimized CDM to GDC Mapping Resolution System with In-Memory Knowledge Graph
- Efficient data dictionaries with deduplication
- In-memory knowledge graph for contextual reasoning
- Caching and optimization strategies
- Graph-enhanced prompting
- Configurable OpenAI base URL for custom endpoints

Configuration:
- OPENAI_API_KEY: Your OpenAI API key (env: OPENAI_API_KEY)
- BASE_URL: OpenAI API base URL (env: OPENAI_BASE_URL)
  * Default: https://api.openai.com/v1
  * Azure: https://your-resource.openai.azure.com/openai/deployments/your-deployment
  * Custom: http://your-custom-endpoint/v1
"""

import os
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple, TypedDict, Annotated, Set
import operator
from collections import defaultdict, Counter
import json
from dataclasses import dataclass, field
from functools import lru_cache
import hashlib

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
import openai
import networkx as nx

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")  # Default OpenAI endpoint

# Initialize OpenAI client with base_url
openai.api_key = OPENAI_API_KEY

LLM_MODEL = ChatOpenAI(
    model="o3-mini",
    openai_api_key=OPENAI_API_KEY,
    base_url=BASE_URL,
)

EMBEDDING_MODEL = "text-embedding-3-large"

# Global embedding cache
EMBEDDING_CACHE = {}


# ============================================================================
# CONFIGURATION HELPER
# ============================================================================

def configure_openai(api_key: str = None, base_url: str = None, model: str = None, embedding_model: str = None):
    """
    Configure OpenAI settings programmatically
    
    Args:
        api_key: OpenAI API key
        base_url: OpenAI API base URL (default: https://api.openai.com/v1)
        model: LLM model to use (default: o3-mini)
        embedding_model: Embedding model to use (default: text-embedding-3-large)
    
    Examples:
        # Standard OpenAI
        configure_openai(api_key="sk-...", base_url="https://api.openai.com/v1")
        
        # Azure OpenAI
        configure_openai(
            api_key="your-azure-key",
            base_url="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
        )
        
        # Custom endpoint
        configure_openai(api_key="your-key", base_url="http://localhost:8000/v1")
    """
    global OPENAI_API_KEY, BASE_URL, LLM_MODEL, EMBEDDING_MODEL
    
    if api_key:
        OPENAI_API_KEY = api_key
        openai.api_key = api_key
        os.environ["OPENAI_API_KEY"] = api_key
    
    if base_url:
        BASE_URL = base_url
        os.environ["OPENAI_BASE_URL"] = base_url
    
    if embedding_model:
        EMBEDDING_MODEL = embedding_model
    
    # Reinitialize LLM model with new settings
    model_name = model if model else "o3-mini"
    LLM_MODEL = ChatOpenAI(
        model=model_name,
        openai_api_key=OPENAI_API_KEY,
        base_url=BASE_URL,
    )
    
    print(f"‚úÖ OpenAI Configuration Updated:")
    print(f"   Base URL: {BASE_URL}")
    print(f"   Model: {model_name}")
    print(f"   Embedding Model: {EMBEDDING_MODEL}")
    print(f"   API Key: {'*' * 20}{OPENAI_API_KEY[-4:] if len(OPENAI_API_KEY) > 4 else '****'}")



# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class PBTRecord:
    """Optimized PBT record structure"""
    pbt_name: str
    definition: str
    cdm: str
    gdc: str
    
    def __hash__(self):
        return hash((self.pbt_name, self.cdm, self.gdc))


@dataclass
class MappingCandidate:
    """Deduplicated mapping candidate"""
    cdm: str
    gdc: str
    frequency: int = 1
    sources: Set[str] = field(default_factory=set)
    
    def __hash__(self):
        return hash((self.cdm, self.gdc))


class DataDictionaries:
    """Optimized data dictionaries with deduplication"""
    
    def __init__(self):
        self.pbt_records: Dict[str, List[PBTRecord]] = defaultdict(list)
        self.cdm_to_gdc: Dict[str, Set[str]] = defaultdict(set)
        self.gdc_to_cdm: Dict[str, Set[str]] = defaultdict(set)
        self.mapping_candidates: Dict[Tuple[str, str], MappingCandidate] = {}
        
        # Statistics
        self.stats = {
            "total_pbt": 0,
            "total_cdm": 0,
            "total_gdc": 0,
            "duplicates_removed": 0,
            "many_to_many_count": 0
        }
    
    def add_pbt_record(self, record: PBTRecord):
        """Add PBT record with deduplication"""
        key = f"{record.cdm}_{record.gdc}"
        if key not in self.pbt_records:
            self.pbt_records[record.cdm].append(record)
            self.stats["total_pbt"] += 1
        else:
            self.stats["duplicates_removed"] += 1
    
    def add_mapping(self, cdm: str, gdc: str, source: str = "direct"):
        """Add mapping with deduplication tracking"""
        cdm = str(cdm).strip()
        gdc = str(gdc).strip()
        
        if not cdm or not gdc or cdm == "nan" or gdc == "nan":
            return
        
        key = (cdm, gdc)
        if key in self.mapping_candidates:
            self.mapping_candidates[key].frequency += 1
            self.mapping_candidates[key].sources.add(source)
        else:
            candidate = MappingCandidate(cdm=cdm, gdc=gdc, sources={source})
            self.mapping_candidates[key] = candidate
            self.cdm_to_gdc[cdm].add(gdc)
            self.gdc_to_cdm[gdc].add(cdm)
    
    def get_deduplicated_mappings(self) -> List[MappingCandidate]:
        """Get all deduplicated mappings"""
        return list(self.mapping_candidates.values())
    
    def get_many_to_many(self) -> List[Tuple[str, List[str]]]:
        """Get only many-to-many relationships that need resolution"""
        many_to_many = []
        for cdm, gdcs in self.cdm_to_gdc.items():
            if len(gdcs) > 1:
                many_to_many.append((cdm, list(gdcs)))
        self.stats["many_to_many_count"] = len(many_to_many)
        return many_to_many
    
    def print_stats(self):
        """Print deduplication statistics"""
        self.stats["total_cdm"] = len(self.cdm_to_gdc)
        self.stats["total_gdc"] = len(self.gdc_to_cdm)
        
        print("\nüìä Data Dictionary Statistics:")
        print(f"   Total PBT Records: {self.stats['total_pbt']}")
        print(f"   Total CDM Terms: {self.stats['total_cdm']}")
        print(f"   Total GDC Terms: {self.stats['total_gdc']}")
        print(f"   Total Unique Mappings: {len(self.mapping_candidates)}")
        print(f"   Duplicates Removed: {self.stats['duplicates_removed']}")
        print(f"   Many-to-Many Mappings: {self.stats['many_to_many_count']}")


# ============================================================================
# IN-MEMORY KNOWLEDGE GRAPH
# ============================================================================

class MappingKnowledgeGraph:
    """In-memory knowledge graph for mapping context"""
    
    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.embedding_cache = {}
        
        # Node type colors for visualization
        self.node_colors = {
            "CDM": "#4A90E2",
            "GDC": "#50C878",
            "PBT": "#FF6B6B",
            "DEFINITION": "#FFD93D"
        }
    
    def build_from_dictionaries(self, data_dicts: DataDictionaries):
        """Build knowledge graph from data dictionaries"""
        print("\nüï∏Ô∏è  Building Knowledge Graph...")
        
        # Add PBT nodes and relationships
        for cdm, records in data_dicts.pbt_records.items():
            # Add CDM node
            self.graph.add_node(cdm, type="CDM", label=cdm)
            
            for record in records:
                # Add PBT node
                pbt_id = f"PBT_{record.pbt_name}"
                self.graph.add_node(
                    pbt_id,
                    type="PBT",
                    label=record.pbt_name,
                    definition=record.definition
                )
                
                # Add relationships
                self.graph.add_edge(pbt_id, cdm, relation="has_cdm")
                
                if record.gdc:
                    gdc_node = record.gdc
                    self.graph.add_node(gdc_node, type="GDC", label=gdc_node)
                    self.graph.add_edge(pbt_id, gdc_node, relation="maps_to_gdc")
                    self.graph.add_edge(cdm, gdc_node, relation="potential_mapping", source="pbt")
        
        # Add direct mappings
        for candidate in data_dicts.get_deduplicated_mappings():
            self.graph.add_node(candidate.cdm, type="CDM", label=candidate.cdm)
            self.graph.add_node(candidate.gdc, type="GDC", label=candidate.gdc)
            
            self.graph.add_edge(
                candidate.cdm,
                candidate.gdc,
                relation="direct_mapping",
                frequency=candidate.frequency,
                sources=list(candidate.sources)
            )
        
        print(f"   ‚úì Nodes: {self.graph.number_of_nodes()}")
        print(f"   ‚úì Edges: {self.graph.number_of_edges()}")
        print(f"   ‚úì CDM Nodes: {sum(1 for n, d in self.graph.nodes(data=True) if d.get('type') == 'CDM')}")
        print(f"   ‚úì GDC Nodes: {sum(1 for n, d in self.graph.nodes(data=True) if d.get('type') == 'GDC')}")
    
    def get_neighborhood_context(self, cdm: str, hops: int = 2) -> Dict[str, Any]:
        """Get neighborhood context for a CDM term from knowledge graph"""
        if cdm not in self.graph:
            return {"neighbors": [], "paths": [], "connected_gdcs": []}
        
        # Get neighbors within N hops
        neighbors = []
        for node in nx.single_source_shortest_path_length(self.graph, cdm, cutoff=hops):
            if node != cdm:
                node_data = self.graph.nodes[node]
                neighbors.append({
                    "node": node,
                    "type": node_data.get("type", "unknown"),
                    "label": node_data.get("label", node)
                })
        
        # Get all paths to GDC nodes
        gdc_nodes = [n for n, d in self.graph.nodes(data=True) if d.get("type") == "GDC"]
        paths = []
        for gdc in gdc_nodes:
            if gdc in self.graph and cdm in self.graph:
                try:
                    if nx.has_path(self.graph, cdm, gdc):
                        path = nx.shortest_path(self.graph, cdm, gdc)
                        if len(path) <= hops + 1:
                            paths.append({
                                "target_gdc": gdc,
                                "path": path,
                                "length": len(path) - 1
                            })
                except:
                    pass
        
        # Get directly connected GDCs with edge attributes
        connected_gdcs = []
        for successor in self.graph.successors(cdm):
            if self.graph.nodes[successor].get("type") == "GDC":
                edge_data = list(self.graph.get_edge_data(cdm, successor).values())[0]
                connected_gdcs.append({
                    "gdc": successor,
                    "relation": edge_data.get("relation", "unknown"),
                    "frequency": edge_data.get("frequency", 1),
                    "sources": edge_data.get("sources", [])
                })
        
        return {
            "neighbors": neighbors[:10],  # Limit to top 10
            "paths": sorted(paths, key=lambda x: x["length"])[:5],  # Top 5 shortest
            "connected_gdcs": sorted(connected_gdcs, key=lambda x: x["frequency"], reverse=True),
            "total_neighbors": len(neighbors),
            "total_paths": len(paths)
        }
    
    def get_graph_metrics(self, cdm: str) -> Dict[str, Any]:
        """Calculate graph-based metrics for a CDM term"""
        if cdm not in self.graph:
            return {}
        
        metrics = {
            "degree": self.graph.degree(cdm),
            "in_degree": self.graph.in_degree(cdm),
            "out_degree": self.graph.out_degree(cdm)
        }
        
        # Centrality measures (expensive, so cache)
        try:
            if not hasattr(self, '_centrality_cache'):
                self._centrality_cache = {}
            
            if cdm in self._centrality_cache:
                metrics.update(self._centrality_cache[cdm])
            else:
                # Calculate for subgraph only
                subgraph_nodes = list(nx.single_source_shortest_path_length(self.graph, cdm, cutoff=2).keys())
                subgraph = self.graph.subgraph(subgraph_nodes)
                
                if len(subgraph) > 1:
                    try:
                        betweenness = nx.betweenness_centrality(subgraph)
                        metrics["betweenness_centrality"] = betweenness.get(cdm, 0)
                    except:
                        metrics["betweenness_centrality"] = 0
                
                self._centrality_cache[cdm] = metrics
        except:
            pass
        
        return metrics
    
    def find_similar_resolved_mappings(self, cdm: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Find similar CDMs that have been resolved (for learning)"""
        similar = []
        
        cdm_neighbors = set(self.graph.predecessors(cdm)) | set(self.graph.successors(cdm))
        
        for node, data in self.graph.nodes(data=True):
            if data.get("type") == "CDM" and node != cdm:
                node_neighbors = set(self.graph.predecessors(node)) | set(self.graph.successors(node))
                
                # Calculate Jaccard similarity
                intersection = len(cdm_neighbors & node_neighbors)
                union = len(cdm_neighbors | node_neighbors)
                
                if union > 0:
                    similarity = intersection / union
                    if similarity > 0:
                        similar.append({
                            "cdm": node,
                            "similarity": similarity,
                            "common_neighbors": list(cdm_neighbors & node_neighbors)
                        })
        
        return sorted(similar, key=lambda x: x["similarity"], reverse=True)[:limit]
    
    def export_graph_summary(self) -> str:
        """Export graph structure as text summary for LLM context"""
        summary = []
        summary.append(f"Knowledge Graph Overview:")
        summary.append(f"- Total Nodes: {self.graph.number_of_nodes()}")
        summary.append(f"- Total Edges: {self.graph.number_of_edges()}")
        
        # Node type distribution
        type_counts = Counter(d.get("type", "unknown") for n, d in self.graph.nodes(data=True))
        summary.append(f"- Node Types: {dict(type_counts)}")
        
        # Most connected nodes
        degrees = sorted(self.graph.degree(), key=lambda x: x[1], reverse=True)[:5]
        summary.append(f"- Most Connected Nodes: {[f'{n}({d})' for n, d in degrees]}")
        
        return "\n".join(summary)


# ============================================================================
# OPTIMIZED EMBEDDING FUNCTIONS
# ============================================================================

@lru_cache(maxsize=10000)
def get_cached_embedding(text: str) -> Tuple[float, ...]:
    """Get embedding with LRU cache"""
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)
    
    response = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text
    )
    
    # Return as tuple for hashability
    return tuple(response.data[0].embedding)


def get_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """Get embeddings in batch with caching"""
    embeddings = []
    uncached_texts = []
    uncached_indices = []
    
    # Check cache first
    for i, text in enumerate(texts):
        text_key = str(text).strip()
        if text_key in EMBEDDING_CACHE:
            embeddings.append(EMBEDDING_CACHE[text_key])
        else:
            embeddings.append(None)
            uncached_texts.append(text_key)
            uncached_indices.append(i)
    
    # Batch fetch uncached embeddings
    if uncached_texts:
        client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=uncached_texts
        )
        
        for idx, emb_data in zip(uncached_indices, response.data):
            emb = emb_data.embedding
            embeddings[idx] = emb
            EMBEDDING_CACHE[uncached_texts[uncached_indices.index(idx)]] = emb
    
    return embeddings


def calculate_similarity(emb1: List[float], emb2: List[float]) -> float:
    """Calculate cosine similarity"""
    emb1_array = np.array(emb1)
    emb2_array = np.array(emb2)
    
    dot_product = np.dot(emb1_array, emb2_array)
    norm1 = np.linalg.norm(emb1_array)
    norm2 = np.linalg.norm(emb2_array)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


# ============================================================================
# STATE DEFINITION
# ============================================================================

class MappingState(TypedDict):
    """State for the LangGraph workflow"""
    cdm_item: str
    gdc_candidates: List[str]
    pbt_context: List[Dict[str, Any]]
    graph_context: Dict[str, Any]
    
    # Analysis results
    semantic_analysis: Dict[str, Any]
    expert_opinions: List[Dict[str, Any]]
    reasoning_chains: List[str]
    
    # Final decision
    final_mapping: Dict[str, Any]
    confidence_score: float
    
    # Metadata
    iteration: int
    messages: Annotated[List[str], operator.add]


# ============================================================================
# GRAPH-ENHANCED DYNAMIC PROMPTS
# ============================================================================

def create_semantic_analyzer_prompt(
    cdm: str,
    gdc_list: List[str],
    context: List[Dict],
    graph_context: Dict[str, Any]
) -> str:
    """Enhanced semantic analysis with knowledge graph context"""
    
    # Format graph context
    graph_info = []
    if graph_context.get("connected_gdcs"):
        graph_info.append("**Knowledge Graph Insights:**")
        for conn in graph_context["connected_gdcs"][:3]:
            graph_info.append(f"- {conn['gdc']}: {conn['relation']} (frequency: {conn['frequency']})")
    
    if graph_context.get("paths"):
        graph_info.append("\n**Alternative Mapping Paths:**")
        for path_info in graph_context["paths"][:2]:
            graph_info.append(f"- Path to {path_info['target_gdc']}: {' ‚Üí '.join(path_info['path'])}")
    
    graph_str = "\n".join(graph_info) if graph_info else "No direct graph context available"
    
    context_str = "\n".join([
        f"- PBT: {c.get('pbt_name', 'N/A')}, Definition: {c.get('definition', 'N/A')}, CDM: {c.get('cdm', 'N/A')}, GDC: {c.get('gdc', 'N/A')}"
        for c in context[:5]
    ])
    
    return f"""You are a Semantic Analysis Expert with access to a knowledge graph of existing mappings.

**Your Task:** Analyze semantic relationships between CDM "{cdm}" and GDC candidates using both semantic embeddings and graph topology.

**Chain of Thought Instructions:**
1. Decompose the CDM term semantically
2. Analyze each GDC candidate's meaning
3. Consider knowledge graph relationships and patterns
4. Evaluate semantic similarity using context
5. Rank candidates with confidence scores

**Input:**
CDM Term: "{cdm}"

GDC Candidates:
{chr(10).join([f"{i+1}. {gdc}" for i, gdc in enumerate(gdc_list)])}

**Knowledge Graph Context:**
{graph_str}

**PBT Historical Context:**
{context_str}

**Graph Metrics:**
- Total neighbors in graph: {graph_context.get('total_neighbors', 0)}
- Connected GDC nodes: {len(graph_context.get('connected_gdcs', []))}

**Output Format (JSON):**
{{
    "reasoning_steps": ["step1", "step2", ...],
    "graph_insights": "How graph topology informs this decision...",
    "candidate_analysis": [
        {{
            "gdc": "term",
            "semantic_relationship": "equivalent|broader|narrower|related",
            "similarity_score": 0.0-1.0,
            "graph_support": "evidence from knowledge graph",
            "reasoning": "..."
        }}
    ],
    "recommended_candidates": ["gdc1", "gdc2"]
}}

Analyze with graph-enhanced reasoning:"""


def create_business_context_expert_prompt(
    cdm: str,
    gdc_list: List[str],
    context: List[Dict],
    graph_metrics: Dict[str, Any]
) -> str:
    """Enhanced business context with graph centrality"""
    
    metrics_str = ""
    if graph_metrics:
        metrics_str = f"""
**Graph Centrality Metrics:**
- Degree: {graph_metrics.get('degree', 0)} (total connections)
- Out-degree: {graph_metrics.get('out_degree', 0)} (outgoing mappings)
- Betweenness: {graph_metrics.get('betweenness_centrality', 0):.3f} (importance in mapping network)
"""
    
    return f"""You are a Business Context Expert with knowledge graph insights into mapping patterns.

**Mixture of Reasoning Approach:**
1. **Definitional Reasoning:** Compare formal definitions
2. **Usage Context Reasoning:** Analyze practical usage patterns
3. **Domain Knowledge:** Apply banking/financial expertise
4. **Historical Pattern Recognition:** Learn from graph structure
5. **Network Analysis:** Consider centrality and importance

**Your Task:** Determine optimal GDC mapping(s) for CDM "{cdm}" using business context and graph intelligence.

**Input:**
CDM Term: "{cdm}"

GDC Candidates:
{chr(10).join([f"{i+1}. {gdc}" for i, gdc in enumerate(gdc_list)])}

{metrics_str}

**Historical Context:**
{json.dumps(context[:5], indent=2)}

**Analysis Requirements:**
1. Business purpose and usage of this CDM
2. Graph centrality implications (high-degree nodes need careful mapping)
3. Regulatory/compliance considerations
4. Risk assessment for each mapping choice
5. Pattern alignment with existing successful mappings

**Output Format (JSON):**
{{
    "business_context_analysis": "...",
    "graph_pattern_insights": "What graph structure reveals...",
    "reasoning_strategies": {{
        "definitional": "...",
        "usage_context": "...",
        "domain_knowledge": "...",
        "historical_patterns": "...",
        "network_analysis": "..."
    }},
    "recommended_mappings": [
        {{
            "gdc": "term",
            "justification": "...",
            "confidence": 0.0-1.0,
            "business_risk": "low|medium|high",
            "graph_alignment": "How well it fits existing patterns"
        }}
    ]
}}

Provide graph-informed business analysis:"""


def create_synthesis_judge_prompt(
    cdm: str,
    expert_opinions: List[Dict],
    similar_mappings: List[Dict]
) -> str:
    """Enhanced synthesis with similar resolved mappings"""
    
    similar_str = ""
    if similar_mappings:
        similar_str = "\n**Similar Resolved Mappings (for reference):**\n"
        for sim in similar_mappings[:3]:
            similar_str += f"- {sim['cdm']} (similarity: {sim['similarity']:.2f})\n"
    
    return f"""You are the Synthesis Judge with access to similar resolved mappings from the knowledge graph.

**Mixture of Experts Integration:**
You have analyses from three experts:
1. Semantic Analysis Expert (embeddings + graph topology)
2. Business Context Expert (domain knowledge + network patterns)
3. Structural Consistency Analyst (model integrity)

**Your Task:** Synthesize opinions and make final mapping decision for CDM "{cdm}".

**Expert Opinions:**
{json.dumps(expert_opinions, indent=2)}

{similar_str}

**Decision Framework:**
1. **Convergence Analysis:** Expert agreement/disagreement
2. **Confidence Weighting:** Weight by confidence scores
3. **Graph Consistency:** Ensure alignment with existing patterns
4. **Risk Assessment:** Evaluate mapping risks
5. **Cardinality Optimization:** Prefer one-to-one, justify one-to-many
6. **Learning from Similar Cases:** Apply patterns from similar resolved mappings

**Chain of Thought:**
1. Summarize each expert's recommendation
2. Analyze similar mapping patterns
3. Identify consensus and conflicts
4. Resolve conflicts with graph evidence
5. Apply cardinality constraints
6. Make final decision with high confidence

**Output Format (JSON):**
{{
    "synthesis": {{
        "expert_agreement": "...",
        "conflict_resolution": "...",
        "graph_consistency_check": "...",
        "similar_case_learning": "...",
        "key_factors": [...]
    }},
    "final_decision": {{
        "cdm": "{cdm}",
        "gdc_mappings": ["gdc1"],
        "mapping_type": "one-to-one|one-to-many",
        "confidence": 0.0-1.0,
        "rationale": "Comprehensive justification...",
        "graph_support": "How knowledge graph supports this decision",
        "alternative_mappings": []
    }}
}}

Synthesize with graph intelligence:"""


# ============================================================================
# AGENT NODES (OPTIMIZED WITH GRAPH CONTEXT)
# ============================================================================

def semantic_analysis_node(state: MappingState) -> MappingState:
    """Optimized semantic analysis with graph context"""
    print(f"\nüî¨ Semantic Analysis: {state['cdm_item']}")
    
    # Batch embedding with cache
    all_terms = [state['cdm_item']] + state['gdc_candidates']
    embeddings = get_embeddings_batch(all_terms)
    
    cdm_embedding = embeddings[0]
    gdc_embeddings = embeddings[1:]
    
    # Calculate similarities
    similarities = [
        (gdc, calculate_similarity(cdm_embedding, gdc_emb))
        for gdc, gdc_emb in zip(state['gdc_candidates'], gdc_embeddings)
    ]
    
    # Graph-enhanced prompt
    prompt = create_semantic_analyzer_prompt(
        state['cdm_item'],
        state['gdc_candidates'],
        state['pbt_context'],
        state['graph_context']
    )
    
    response = LLM_MODEL.invoke([HumanMessage(content=prompt)])
    
    try:
        analysis = json.loads(response.content)
    except:
        analysis = {
            "reasoning_steps": ["Completed"],
            "candidate_analysis": [
                {"gdc": gdc, "similarity_score": sim}
                for gdc, sim in similarities
            ],
            "recommended_candidates": [s[0] for s in sorted(similarities, key=lambda x: x[1], reverse=True)[:2]]
        }
    
    state['semantic_analysis'] = analysis
    state['messages'].append(f"Semantic analysis: {len(similarities)} candidates")
    
    return state


def business_context_node(state: MappingState) -> MappingState:
    """Optimized business context with graph metrics"""
    print(f"\nüíº Business Context: {state['cdm_item']}")
    
    graph_metrics = state['graph_context'].get('metrics', {})
    
    prompt = create_business_context_expert_prompt(
        state['cdm_item'],
        state['gdc_candidates'],
        state['pbt_context'],
        graph_metrics
    )
    
    response = LLM_MODEL.invoke([HumanMessage(content=prompt)])
    
    try:
        analysis = json.loads(response.content)
    except:
        analysis = {
            "business_context_analysis": response.content,
            "recommended_mappings": [{"gdc": state['gdc_candidates'][0], "confidence": 0.5}]
        }
    
    state['expert_opinions'].append({
        "expert": "business_context",
        "analysis": analysis
    })
    state['messages'].append("Business context complete")
    
    return state


def structural_analysis_node(state: MappingState) -> MappingState:
    """Optimized structural analysis"""
    print(f"\nüèóÔ∏è Structural Analysis: {state['cdm_item']}")
    
    # Use cached graph metrics
    analysis = {
        "structural_analysis": {
            "graph_degree": state['graph_context'].get('metrics', {}).get('degree', 0),
            "connected_nodes": len(state['graph_context'].get('connected_gdcs', []))
        },
        "recommended_cardinality": "one-to-one",
        "preferred_mappings": [
            {"gdc": state['gdc_candidates'][0], "confidence": 0.7}
        ] if state['gdc_candidates'] else []
    }
    
    state['expert_opinions'].append({
        "expert": "structural_analysis",
        "analysis": analysis
    })
    state['messages'].append("Structural analysis complete")
    
    return state


def synthesis_judge_node(state: MappingState) -> MappingState:
    """Optimized synthesis with similar mappings"""
    print(f"\n‚öñÔ∏è Synthesis: {state['cdm_item']}")
    
    similar_mappings = state['graph_context'].get('similar_mappings', [])
    
    prompt = create_synthesis_judge_prompt(
        state['cdm_item'],
        state['expert_opinions'],
        similar_mappings
    )
    
    response = LLM_MODEL.invoke([HumanMessage(content=prompt)])
    
    try:
        decision = json.loads(response.content)
        final_decision = decision.get('final_decision', {})
    except:
        semantic = state.get('semantic_analysis', {})
        recommended = semantic.get('recommended_candidates', state['gdc_candidates'][:1])
        
        final_decision = {
            "cdm": state['cdm_item'],
            "gdc_mappings": recommended,
            "mapping_type": "one-to-one" if len(recommended) == 1 else "one-to-many",
            "confidence": 0.5,
            "rationale": "Fallback decision"
        }
    
    state['final_mapping'] = final_decision
    state['confidence_score'] = final_decision.get('confidence', 0.5)
    state['messages'].append(f"Final: {final_decision.get('gdc_mappings', [])}")
    
    return state


# ============================================================================
# WORKFLOW
# ============================================================================

def create_mapping_workflow() -> StateGraph:
    """Create optimized LangGraph workflow"""
    workflow = StateGraph(MappingState)
    
    workflow.add_node("semantic_analysis", semantic_analysis_node)
    workflow.add_node("business_context", business_context_node)
    workflow.add_node("structural_analysis", structural_analysis_node)
    workflow.add_node("synthesis_judge", synthesis_judge_node)
    
    workflow.set_entry_point("semantic_analysis")
    workflow.add_edge("semantic_analysis", "business_context")
    workflow.add_edge("business_context", "structural_analysis")
    workflow.add_edge("structural_analysis", "synthesis_judge")
    workflow.add_edge("synthesis_judge", END)
    
    return workflow.compile()


# ============================================================================
# OPTIMIZED DATA LOADING
# ============================================================================

def load_and_process_excel(file_path: str) -> Tuple[DataDictionaries, MappingKnowledgeGraph]:
    """Optimized loading with deduplication and graph building"""
    print("=" * 80)
    print("üìÇ Loading and Processing Excel Data")
    print("=" * 80)
    
    # Load sheets
    pbt_df = pd.read_excel(file_path, sheet_name='PBT')
    cdm_gdc_df = pd.read_excel(file_path, sheet_name='CDM to GDC')
    gdc_cdm_df = pd.read_excel(file_path, sheet_name='GDC to CDM')
    
    # Initialize data dictionaries
    data_dicts = DataDictionaries()
    
    # Process PBT records
    print("\nüìã Processing PBT records...")
    for _, row in pbt_df.iterrows():
        record = PBTRecord(
            pbt_name=str(row.get('Old PBT Name', '')),
            definition=str(row.get('PBT Definition', '')),
            cdm=str(row.get('Old PBT CDM', '')),
            gdc=str(row.get('Target PBT GDC', ''))
        )
        data_dicts.add_pbt_record(record)
        if record.cdm and record.gdc:
            data_dicts.add_mapping(record.cdm, record.gdc, source="pbt")
    
    # Process CDM to GDC mappings
    print("üìã Processing CDM to GDC mappings...")
    for _, row in cdm_gdc_df.iterrows():
        data_dicts.add_mapping(
            row.get('CDM', ''),
            row.get('GDC', ''),
            source="cdm_to_gdc"
        )
    
    # Process GDC to CDM mappings
    print("üìã Processing GDC to CDM mappings...")
    for _, row in gdc_cdm_df.iterrows():
        gdc = str(row.get('GDC to CDM', '')).strip()
        # Try to find CDM column
        for col in row.index:
            if 'CDM' in col.upper() and col.upper() != 'GDC TO CDM':
                cdm = str(row.get(col, '')).strip()
                if cdm and cdm != 'nan':
                    data_dicts.add_mapping(cdm, gdc, source="gdc_to_cdm")
                    break
    
    # Print statistics
    data_dicts.print_stats()
    
    # Build knowledge graph
    kg = MappingKnowledgeGraph()
    kg.build_from_dictionaries(data_dicts)
    
    return data_dicts, kg


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def resolve_mappings_optimized(excel_file: str, output_file: str = "resolved_mappings.xlsx"):
    """Optimized mapping resolution with knowledge graph"""
    
    print("\n" + "=" * 80)
    print("üöÄ CDM to GDC Mapping Resolution (Optimized)")
    print("=" * 80)
    
    # Load and process data
    data_dicts, kg = load_and_process_excel(excel_file)
    
    # Get many-to-many mappings that need resolution
    many_to_many = data_dicts.get_many_to_many()
    
    print(f"\nüéØ Resolving {len(many_to_many)} many-to-many mappings")
    
    # Create workflow
    workflow = create_mapping_workflow()
    
    # Process each mapping
    results = []
    
    for idx, (cdm, gdc_candidates) in enumerate(many_to_many, 1):
        print(f"\n{'='*80}")
        print(f"[{idx}/{len(many_to_many)}] CDM: {cdm}")
        print(f"GDC Candidates ({len(gdc_candidates)}): {gdc_candidates}")
        
        # Get graph context
        graph_context = kg.get_neighborhood_context(cdm)
        graph_context['metrics'] = kg.get_graph_metrics(cdm)
        graph_context['similar_mappings'] = kg.find_similar_resolved_mappings(cdm)
        
        # Get PBT context
        pbt_context = [
            {
                "pbt_name": rec.pbt_name,
                "definition": rec.definition,
                "cdm": rec.cdm,
                "gdc": rec.gdc
            }
            for rec in data_dicts.pbt_records.get(cdm, [])
        ]
        
        # Initialize state
        initial_state = MappingState(
            cdm_item=cdm,
            gdc_candidates=gdc_candidates,
            pbt_context=pbt_context,
            graph_context=graph_context,
            semantic_analysis={},
            expert_opinions=[],
            reasoning_chains=[],
            final_mapping={},
            confidence_score=0.0,
            iteration=0,
            messages=[]
        )
        
        # Run workflow
        try:
            final_state = workflow.invoke(initial_state)
            
            result = {
                "CDM": cdm,
                "Original_GDC_Count": len(gdc_candidates),
                "Original_GDC_Candidates": gdc_candidates,
                "Resolved_GDC": final_state['final_mapping'].get('gdc_mappings', []),
                "Mapping_Type": final_state['final_mapping'].get('mapping_type', 'unknown'),
                "Confidence": final_state['confidence_score'],
                "Rationale": final_state['final_mapping'].get('rationale', ''),
                "Graph_Degree": graph_context['metrics'].get('degree', 0),
                "Graph_Connections": len(graph_context['connected_gdcs']),
                "Similar_Cases": len(graph_context['similar_mappings'])
            }
            
            results.append(result)
            
            print(f"‚úÖ Resolved to: {result['Resolved_GDC']}")
            print(f"   Type: {result['Mapping_Type']}")
            print(f"   Confidence: {result['Confidence']:.2%}")
            print(f"   Graph Support: {result['Graph_Connections']} connections")
            
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            results.append({
                "CDM": cdm,
                "Original_GDC_Count": len(gdc_candidates),
                "Original_GDC_Candidates": gdc_candidates,
                "Resolved_GDC": [gdc_candidates[0]] if gdc_candidates else [],
                "Mapping_Type": "fallback",
                "Confidence": 0.3,
                "Rationale": f"Error: {str(e)}",
                "Graph_Degree": 0,
                "Graph_Connections": 0,
                "Similar_Cases": 0
            })
    
    # Create output
    results_df = pd.DataFrame(results)
    results_df['Primary_GDC'] = results_df['Resolved_GDC'].apply(lambda x: x[0] if x else None)
    results_df['Secondary_GDC'] = results_df['Resolved_GDC'].apply(lambda x: x[1] if len(x) > 1 else None)
    
    # Save with additional sheets
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        results_df.to_excel(writer, sheet_name='Resolved_Mappings', index=False)
        
        # Summary
        summary = pd.DataFrame([{
            "Total_Processed": len(results_df),
            "One_to_One": len(results_df[results_df['Mapping_Type'] == 'one-to-one']),
            "One_to_Many": len(results_df[results_df['Mapping_Type'] == 'one-to-many']),
            "Avg_Confidence": results_df['Confidence'].mean(),
            "High_Confidence": len(results_df[results_df['Confidence'] > 0.7]),
            "Avg_Graph_Connections": results_df['Graph_Connections'].mean(),
            "Total_Duplicates_Removed": data_dicts.stats['duplicates_removed']
        }])
        summary.to_excel(writer, sheet_name='Summary', index=False)
        
        # Graph statistics
        graph_stats = pd.DataFrame([{
            "Total_Nodes": kg.graph.number_of_nodes(),
            "Total_Edges": kg.graph.number_of_edges(),
            "CDM_Nodes": sum(1 for _, d in kg.graph.nodes(data=True) if d.get('type') == 'CDM'),
            "GDC_Nodes": sum(1 for _, d in kg.graph.nodes(data=True) if d.get('type') == 'GDC'),
            "PBT_Nodes": sum(1 for _, d in kg.graph.nodes(data=True) if d.get('type') == 'PBT'),
            "Avg_Degree": sum(d for _, d in kg.graph.degree()) / kg.graph.number_of_nodes() if kg.graph.number_of_nodes() > 0 else 0
        }])
        graph_stats.to_excel(writer, sheet_name='Graph_Statistics', index=False)
    
    print("\n" + "=" * 80)
    print("‚úÖ PROCESSING COMPLETE")
    print("=" * 80)
    print(f"üìä Results: {output_file}")
    print(f"   Mappings Resolved: {len(results_df)}")
    print(f"   One-to-One: {len(results_df[results_df['Mapping_Type'] == 'one-to-one'])}")
    print(f"   One-to-Many: {len(results_df[results_df['Mapping_Type'] == 'one-to-many'])}")
    print(f"   Avg Confidence: {results_df['Confidence'].mean():.2%}")
    print(f"   Duplicates Removed: {data_dicts.stats['duplicates_removed']}")
    print(f"üï∏Ô∏è  Knowledge Graph:")
    print(f"   Nodes: {kg.graph.number_of_nodes()}")
    print(f"   Edges: {kg.graph.number_of_edges()}")
    
    return results_df, data_dicts, kg


# ============================================================================
# USAGE
# ============================================================================

if __name__ == "__main__":
    # os.environ["OPENAI_API_KEY"] = "your-api-key-here"
    
    excel_file = "mapper_file.xlsx"
    results, data_dicts, kg = resolve_mappings_optimized(excel_file)
    
    print("\n‚úÖ Complete! Check 'resolved_mappings.xlsx'")
