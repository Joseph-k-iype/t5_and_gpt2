#!/usr/bin/env python3
"""
Enhanced Ontology Schema Extractor
Extracts the schema (classes, subclasses, properties) from a TTL file
and saves it as a new TTL file without instance data.
This version analyzes the ontology first to understand its vocabulary.
"""

import argparse
import sys
from pathlib import Path
from collections import defaultdict, Counter
from rdflib import Graph, Namespace, RDF, RDFS, OWL, URIRef, BNode, Literal
from rdflib.namespace import XSD, SKOS, DCTERMS, FOAF


def analyze_ontology_vocabulary(graph):
    """
    Analyze the ontology to understand what vocabulary it uses.
    Returns dictionaries of predicates and types found.
    """
    print("Analyzing ontology vocabulary...")
    
    # Count all predicates and objects in type statements
    predicates = Counter()
    rdf_types = Counter()
    
    for s, p, o in graph:
        predicates[p] += 1
        if p == RDF.type:
            rdf_types[o] += 1
    
    print(f"Found {len(predicates)} unique predicates")
    print(f"Found {len(rdf_types)} unique RDF types")
    
    # Show top predicates
    print("\nTop 20 predicates:")
    for pred, count in predicates.most_common(20):
        print(f"  {pred}: {count}")
    
    # Show all RDF types
    print(f"\nAll RDF types found ({len(rdf_types)}):")
    for rdf_type, count in rdf_types.most_common():
        print(f"  {rdf_type}: {count}")
    
    return predicates, rdf_types


def identify_schema_elements(graph, predicates, rdf_types):
    """
    Identify what constitutes schema elements in this specific ontology.
    """
    print("\nIdentifying schema patterns...")
    
    schema_predicates = set()
    schema_types = set()
    
    # Standard schema predicates - always include these
    standard_schema_predicates = {
        RDF.type, RDFS.subClassOf, RDFS.subPropertyOf, RDFS.domain, RDFS.range,
        RDFS.label, RDFS.comment, RDFS.seeAlso, RDFS.isDefinedBy,
        OWL.equivalentClass, OWL.equivalentProperty, OWL.sameAs,
        OWL.inverseOf, OWL.disjointWith, OWL.unionOf, OWL.intersectionOf,
        OWL.complementOf, OWL.oneOf, OWL.onProperty, OWL.someValuesFrom,
        OWL.allValuesFrom, OWL.hasValue, OWL.cardinality, OWL.minCardinality,
        OWL.maxCardinality, OWL.qualifiedCardinality, OWL.minQualifiedCardinality,
        OWL.maxQualifiedCardinality, OWL.onClass, OWL.onDataRange,
        OWL.versionInfo, OWL.imports, OWL.versionIRI, OWL.priorVersion,
        OWL.backwardCompatibleWith, OWL.incompatibleWith,
        SKOS.broader, SKOS.narrower, SKOS.related, SKOS.broaderTransitive,
        SKOS.narrowerTransitive, SKOS.prefLabel, SKOS.altLabel, SKOS.hiddenLabel,
        SKOS.definition, SKOS.note, SKOS.scopeNote, SKOS.example,
        DCTERMS.title, DCTERMS.description, DCTERMS.creator, DCTERMS.created,
        DCTERMS.modified, DCTERMS.identifier
    }
    
    schema_predicates.update(standard_schema_predicates)
    
    # Standard schema types - always include these
    standard_schema_types = {
        RDFS.Class, OWL.Class, RDF.Property, OWL.ObjectProperty, 
        OWL.DatatypeProperty, OWL.AnnotationProperty, OWL.FunctionalProperty,
        OWL.InverseFunctionalProperty, OWL.TransitiveProperty, OWL.SymmetricProperty,
        OWL.AsymmetricProperty, OWL.ReflexiveProperty, OWL.IrreflexiveProperty,
        OWL.Restriction, OWL.Ontology, SKOS.Concept, SKOS.ConceptScheme,
        SKOS.Collection, SKOS.OrderedCollection
    }
    
    schema_types.update(standard_schema_types)
    
    # Look for additional schema-like predicates and types
    # These might indicate vocabulary-specific schema elements
    potential_schema_predicates = set()
    potential_schema_types = set()
    
    for pred, count in predicates.items():
        if isinstance(pred, URIRef):
            pred_str = str(pred).lower()
            # Look for predicates that seem definitional/schema-like
            if any(keyword in pred_str for keyword in [
                'subclass', 'subproperty', 'domain', 'range', 'type',
                'equivalent', 'disjoint', 'inverse', 'definition',
                'broader', 'narrower', 'related', 'isa', 'has_parent'
            ]):
                potential_schema_predicates.add(pred)
    
    for rdf_type, count in rdf_types.items():
        if isinstance(rdf_type, URIRef):
            type_str = str(rdf_type).lower()
            # Look for types that seem class/property-like
            if any(keyword in type_str for keyword in [
                'class', 'property', 'concept', 'category', 'type',
                'ontology', 'vocabulary', 'schema'
            ]):
                potential_schema_types.add(rdf_type)
    
    schema_predicates.update(potential_schema_predicates)
    schema_types.update(potential_schema_types)
    
    print(f"Identified {len(schema_predicates)} schema predicates")
    print(f"Identified {len(schema_types)} schema types")
    
    return schema_predicates, schema_types


def extract_comprehensive_schema(input_file, output_file):
    """
    Extract ontology schema comprehensively from a TTL file.
    """
    try:
        # Load the input TTL file
        print(f"Loading ontology from: {input_file}")
        input_graph = Graph()
        input_graph.parse(input_file, format="turtle")
        print(f"Loaded {len(input_graph)} triples")
        
        # Analyze the vocabulary used
        predicates, rdf_types = analyze_ontology_vocabulary(input_graph)
        
        # Identify schema elements
        schema_predicates, schema_types = identify_schema_elements(input_graph, predicates, rdf_types)
        
        # Create a new graph for the schema
        schema_graph = Graph()
        
        # Copy namespace prefixes from input graph
        for prefix, namespace in input_graph.namespaces():
            schema_graph.bind(prefix, namespace)
        
        # Ensure common ontology namespaces are bound
        schema_graph.bind("rdf", RDF)
        schema_graph.bind("rdfs", RDFS)
        schema_graph.bind("owl", OWL)
        schema_graph.bind("xsd", XSD)
        schema_graph.bind("skos", SKOS)
        schema_graph.bind("dcterms", DCTERMS)
        schema_graph.bind("foaf", FOAF)
        
        print("\nExtracting schema elements...")
        
        # Strategy 1: Extract all triples that use schema predicates
        schema_triples_count = 0
        
        print("Phase 1: Extracting triples with schema predicates...")
        for s, p, o in input_graph:
            if p in schema_predicates:
                schema_graph.add((s, p, o))
                schema_triples_count += 1
        
        print(f"Extracted {schema_triples_count} triples using schema predicates")
        
        # Strategy 2: Extract all entities that have schema types
        print("Phase 2: Extracting entities with schema types...")
        schema_entities = set()
        
        for s, p, o in input_graph:
            if p == RDF.type and o in schema_types:
                schema_graph.add((s, p, o))
                schema_entities.add(s)
                schema_triples_count += 1
        
        print(f"Found {len(schema_entities)} schema entities")
        
        # Strategy 3: Extract all properties of schema entities
        print("Phase 3: Extracting properties of schema entities...")
        for entity in schema_entities:
            for s, p, o in input_graph.triples((entity, None, None)):
                if (s, p, o) not in schema_graph:
                    # Only add if it looks like a schema property, not instance data
                    if not self_is_likely_instance_data(p, o):
                        schema_graph.add((s, p, o))
                        schema_triples_count += 1
        
        # Strategy 4: Look for additional structural elements
        print("Phase 4: Finding additional structural elements...")
        
        # Find subjects that appear to be classes or properties based on usage patterns
        potential_classes = set()
        potential_properties = set()
        
        # Things used as objects of rdf:type might be classes
        for s, p, o in input_graph:
            if p == RDF.type and isinstance(o, URIRef) and o not in schema_types:
                # Check if this type is used for multiple instances
                instances = list(input_graph.triples((None, RDF.type, o)))
                if len(instances) > 1:  # Used as type for multiple things
                    potential_classes.add(o)
        
        # Things used as predicates might be properties
        for s, p, o in input_graph:
            if isinstance(p, URIRef) and p not in schema_predicates:
                # Check if this is used as predicate multiple times
                uses = list(input_graph.triples((None, p, None)))
                if len(uses) > 1:  # Used as predicate multiple times
                    potential_properties.add(p)
        
        print(f"Found {len(potential_classes)} potential additional classes")
        print(f"Found {len(potential_properties)} potential additional properties")
        
        # Add declarations for potential classes
        for cls in potential_classes:
            schema_graph.add((cls, RDF.type, RDFS.Class))
            schema_triples_count += 1
            
            # Add any labels, comments, etc. for these classes
            for s, p, o in input_graph.triples((cls, None, None)):
                if p in {RDFS.label, RDFS.comment, SKOS.prefLabel, DCTERMS.title, DCTERMS.description}:
                    if (s, p, o) not in schema_graph:
                        schema_graph.add((s, p, o))
                        schema_triples_count += 1
        
        # Add declarations for potential properties
        for prop in potential_properties:
            # Try to determine if it's object or data property
            sample_objects = []
            for s, p, o in input_graph.triples((None, prop, None)):
                sample_objects.append(o)
                if len(sample_objects) >= 5:  # Sample up to 5
                    break
            
            # Heuristic: if most objects are literals, it's probably a data property
            literal_count = sum(1 for obj in sample_objects if isinstance(obj, Literal))
            if literal_count > len(sample_objects) / 2:
                schema_graph.add((prop, RDF.type, OWL.DatatypeProperty))
            else:
                schema_graph.add((prop, RDF.type, OWL.ObjectProperty))
            schema_triples_count += 1
            
            # Add any labels, comments, etc. for these properties
            for s, p_inner, o in input_graph.triples((prop, None, None)):
                if p_inner in {RDFS.label, RDFS.comment, SKOS.prefLabel, DCTERMS.title, DCTERMS.description}:
                    if (s, p_inner, o) not in schema_graph:
                        schema_graph.add((s, p_inner, o))
                        schema_triples_count += 1
        
        # Final count
        final_count = len(schema_graph)
        
        # Save the schema graph
        print(f"\nSaving schema with {final_count} triples to: {output_file}")
        schema_graph.serialize(destination=output_file, format="turtle")
        
        # Print detailed summary
        print("\n" + "="*60)
        print("COMPREHENSIVE EXTRACTION SUMMARY")
        print("="*60)
        print(f"Input file: {input_file}")
        print(f"Output file: {output_file}")
        print(f"Original triples: {len(input_graph)}")
        print(f"Schema triples: {final_count}")
        print(f"Extraction ratio: {final_count/len(input_graph)*100:.1f}%")
        
        # Count specific elements in the extracted schema
        classes_rdfs = len(list(schema_graph.triples((None, RDF.type, RDFS.Class))))
        classes_owl = len(list(schema_graph.triples((None, RDF.type, OWL.Class))))
        total_classes = classes_rdfs + classes_owl
        
        obj_props = len(list(schema_graph.triples((None, RDF.type, OWL.ObjectProperty))))
        data_props = len(list(schema_graph.triples((None, RDF.type, OWL.DatatypeProperty))))
        rdf_props = len(list(schema_graph.triples((None, RDF.type, RDF.Property))))
        total_properties = obj_props + data_props + rdf_props
        
        subclass_relations = len(list(schema_graph.triples((None, RDFS.subClassOf, None))))
        subprop_relations = len(list(schema_graph.triples((None, RDFS.subPropertyOf, None))))
        
        skos_concepts = len(list(schema_graph.triples((None, RDF.type, SKOS.Concept))))
        
        print(f"\nExtracted Elements:")
        print(f"  Classes (total): {total_classes}")
        print(f"    - RDFS Classes: {classes_rdfs}")
        print(f"    - OWL Classes: {classes_owl}")
        print(f"  Properties (total): {total_properties}")
        print(f"    - Object Properties: {obj_props}")
        print(f"    - Data Properties: {data_props}")
        print(f"    - RDF Properties: {rdf_props}")
        print(f"  Subclass relationships: {subclass_relations}")
        print(f"  Subproperty relationships: {subprop_relations}")
        print(f"  SKOS Concepts: {skos_concepts}")
        
        print(f"\nNamespaces in schema:")
        for prefix, namespace in sorted(schema_graph.namespaces()):
            if prefix:  # Skip empty prefix
                print(f"  {prefix}: {namespace}")
        
        print("\nComprehensive schema extraction completed successfully!")
        
    except Exception as e:
        print(f"Error processing ontology: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def self_is_likely_instance_data(predicate, obj):
    """
    Heuristic to determine if a triple represents instance data rather than schema.
    """
    # These predicates typically indicate instance data
    instance_predicates = {
        # Common instance-indicating predicates - add more as needed
    }
    
    if predicate in instance_predicates:
        return True
    
    # If the object is a very specific literal value, it might be instance data
    if isinstance(obj, Literal):
        str_val = str(obj).lower()
        # Very specific values that look like instance data
        if any(pattern in str_val for pattern in ['@', '.com', '.org', 'http://', 'https://']):
            return True
    
    return False


def main():
    parser = argparse.ArgumentParser(description="Extract ontology schema from TTL file")
    parser.add_argument("input_file", help="Input TTL file path")
    parser.add_argument("-o", "--output", 
                       help="Output TTL file path (default: input_schema.ttl)")
    parser.add_argument("-v", "--verbose", action="store_true",
                       help="Enable verbose output")
    
    args = parser.parse_args()
    
    # Validate input file
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"Error: Input file '{args.input_file}' does not exist")
        sys.exit(1)
    
    # Set output file
    if args.output:
        output_file = args.output
    else:
        output_file = input_path.stem + "_schema.ttl"
    
    # Extract schema
    extract_comprehensive_schema(args.input_file, output_file)


if __name__ == "__main__":
    main()
