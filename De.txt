#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System (Rules-as-Code)
Features:
- ReAct agents with explicit reasoning and competency questions
- Rich ontology with DPV, PROV-O integration
- LangMem for long-term memory across sessions
- o3-mini-2025-01-31 with reasoning effort control
- Web interface for querying the knowledge graph
- SPARQL endpoint for semantic queries
- Comprehensive SHACL validation
- Multi-format exports (TTL, JSON-LD, XML, OWL)

Author: Claude (Anthropic) - Enhanced Rules-as-Code Implementation
Date: July 2025
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import ssl
import threading

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
from rdflib.plugins.stores.sparqlstore import SPARQLStore
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langmem import (
    create_manage_memory_tool, 
    create_search_memory_tool, 
    create_memory_manager,
    create_memory_store_manager
)

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logger.warning("Flask not available - web interface will be disabled")
    class Flask: pass
    class Response: pass
    def jsonify(*args, **kwargs): return {}
    def render_template_string(*args, **kwargs): return ""
    def CORS(*args, **kwargs): pass

# Token counting (offline)
import tiktoken
from tiktoken.load import load_tiktoken_bpe

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the Rules-as-Code system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_as_code_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    TIKTOKEN_MODELS_PATH = os.getenv("TIKTOKEN_MODELS_PATH", "./tiktoken_models")
    
    # o3-mini Model parameters
    REASONING_EFFORT = "high"  # low, medium, high for complex legal analysis
    MAX_COMPLETION_TOKENS = 8000
    
    # Processing parameters
    BATCH_SIZE = 5
    MAX_CONCURRENT = 3
    CHUNK_SIZE = 4000  # Larger chunks for legal documents
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if not cls.ELASTICSEARCH_PASSWORD:
            missing_vars.append("ELASTICSEARCH_PASSWORD")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )

# ====================================
# ENHANCED LEGAL ONTOLOGY NAMESPACES
# ====================================

class LegalRulesNamespaces:
    """Enhanced namespaces for Rules-as-Code with DPV and PROV-O integration"""
    
    # Core W3C vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal domain vocabularies
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Our enhanced Rules-as-Code namespace
    RAC = Namespace("https://rules-as-code.org/ontology#")
    
    # Data management domain-specific namespaces
    STORAGE = Namespace("https://rules-as-code.org/storage#")
    USAGE = Namespace("https://rules-as-code.org/usage#")
    MOVEMENT = Namespace("https://rules-as-code.org/movement#")
    PRIVACY = Namespace("https://rules-as-code.org/privacy#")
    SECURITY = Namespace("https://rules-as-code.org/security#")
    ACCESS = Namespace("https://rules-as-code.org/access#")
    ENTITLEMENTS = Namespace("https://rules-as-code.org/entitlements#")
    
    # Property and relation namespaces
    PROPERTIES = Namespace("https://rules-as-code.org/properties#")
    RELATIONS = Namespace("https://rules-as-code.org/relations#")
    
    # Competency question namespace
    CQ = Namespace("https://rules-as-code.org/competency-questions#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("rac", cls.RAC)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("entitlements", cls.ENTITLEMENTS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("cq", cls.CQ)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# TIKTOKEN MANAGER FOR o3-mini
# ====================================

class TiktokenManager:
    """Manage tiktoken encodings for o3-mini model"""
    
    def __init__(self, models_dir: str = "./tiktoken_models"):
        self.models_dir = Path(models_dir)
        self.encodings = {}
        self._load_encodings()
    
    def _load_encodings(self):
        """Load tiktoken encodings"""
        try:
            # o3-mini uses o200k_base encoding
            self.encoding = tiktoken.get_encoding("o200k_base")
            logger.info("Loaded o200k_base encoding for o3-mini")
        except Exception as e:
            logger.warning(f"Failed to load tiktoken encoding: {e}")
            # Fallback to cl100k_base
            try:
                self.encoding = tiktoken.get_encoding("cl100k_base")
                logger.info("Using cl100k_base as fallback encoding")
            except:
                self.encoding = None
                logger.error("No tiktoken encoding available")
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        if not self.encoding:
            return len(text) // 4  # Rough estimate
        return len(self.encoding.encode(text))
    
    def truncate_text(self, text: str, max_tokens: int) -> str:
        """Truncate text to fit within token limit"""
        if not self.encoding:
            # Character-based fallback
            return text[:max_tokens * 4]
        
        tokens = self.encoding.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        truncated_tokens = tokens[:max_tokens]
        return self.encoding.decode(truncated_tokens)

# ====================================
# OPENAI CLIENT FOR o3-mini
# ====================================

class OpenAIClient:
    """Enhanced OpenAI client for o3-mini with reasoning effort control"""
    
    def __init__(self):
        Config.validate_config()
        
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        self.tiktoken_manager = TiktokenManager(Config.TIKTOKEN_MODELS_PATH)
        
        # Test the client
        try:
            test_response = self.client.embeddings.create(
                input=["test"],
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            logger.info(f"OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConnectionError(f"OpenAI API connection failed: {e}")
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        logger.info(f"Generating embeddings for {len(texts)} text(s)...")
        
        try:
            # Truncate texts if too long
            truncated_texts = []
            for text in texts:
                token_count = self.tiktoken_manager.count_tokens(text)
                if token_count > 8000:  # Conservative limit for embeddings
                    truncated_text = self.tiktoken_manager.truncate_text(text, 8000)
                    truncated_texts.append(truncated_text)
                else:
                    truncated_texts.append(text)
            
            response = self.client.embeddings.create(
                input=truncated_texts,
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Generate chat completion with o3-mini and reasoning effort"""
        logger.info(f"Starting chat completion with model: {Config.OPENAI_MODEL}")
        
        try:
            # Prepare messages for o3-mini
            prepared_messages = self._prepare_messages_for_o3_mini(messages)
            
            # Set reasoning effort based on complexity
            reasoning_effort = kwargs.get('reasoning_effort', Config.REASONING_EFFORT)
            
            response = self.client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=prepared_messages,
                reasoning_effort=reasoning_effort,
                max_completion_tokens=kwargs.get('max_completion_tokens', Config.MAX_COMPLETION_TOKENS)
            )
            
            content = response.choices[0].message.content
            logger.info(f"Received response with {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            raise
    
    def _prepare_messages_for_o3_mini(self, messages: List[Dict]) -> List[Dict]:
        """Prepare messages for o3-mini model with token management"""
        
        # Convert system messages to developer messages for o3-mini
        prepared_messages = []
        for msg in messages:
            if msg.get('role') == 'system':
                prepared_messages.append({
                    'role': 'developer',  # o3-mini uses developer role instead of system
                    'content': msg.get('content', '')
                })
            else:
                prepared_messages.append(msg)
        
        # Check token count and truncate if necessary
        total_tokens = sum(self.tiktoken_manager.count_tokens(msg.get('content', '')) 
                          for msg in prepared_messages)
        
        if total_tokens > 150000:  # Conservative limit for o3-mini
            # Truncate the longest message
            longest_msg_idx = max(range(len(prepared_messages)), 
                                key=lambda i: len(prepared_messages[i].get('content', '')))
            
            original_content = prepared_messages[longest_msg_idx]['content']
            truncated_content = self.tiktoken_manager.truncate_text(original_content, 100000)
            prepared_messages[longest_msg_idx]['content'] = truncated_content + "\n\n[Content truncated due to length]"
            
            logger.warning("Messages truncated due to token limit")
        
        return prepared_messages

# ====================================
# LANGMEM MEMORY MANAGER
# ====================================

class LegalMemoryManager:
    """Enhanced memory manager for legal knowledge with LangMem"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.openai_client = openai_client
        self.store = None
        self.memory_manager = None
        self._initialize_store()
    
    def _initialize_store(self):
        """Initialize memory store (InMemoryStore for development and testing)"""
        try:
            # Use in-memory store with vector index for embeddings
            self.store = InMemoryStore(
                index={
                    "dims": 3072,  # text-embedding-3-large dimensions
                    "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
                }
            )
            logger.info("Using in-memory store for LangMem (suitable for development and testing)")
            
            # Create memory manager for legal concepts
            from pydantic import BaseModel, Field
            
            class LegalConcept(BaseModel):
                """Schema for legal concepts in memory"""
                concept_name: str = Field(..., description="Name of the legal concept")
                concept_type: str = Field(..., description="Type: subject|property|relationship|domain")
                jurisdiction: str = Field(..., description="Legal jurisdiction")
                domain: str = Field(..., description="Data management domain")
                description: str = Field(..., description="Description of the concept")
                relationships: List[str] = Field(default=[], description="Related concepts")
                has_definition: bool = Field(default=False, description="Whether concept has a definition")
                has_rules: bool = Field(default=False, description="Whether concept has associated rules")
            
            class LegalSubject(BaseModel):
                """Schema for legal subjects"""
                subject_name: str = Field(..., description="Name of the legal subject")
                definition: str = Field(..., description="Legal definition from text")
                domains: List[str] = Field(..., description="Data management domains")
                rule_count: int = Field(default=0, description="Number of associated rules")
                condition_count: int = Field(default=0, description="Number of conditions")
                jurisdiction: str = Field(..., description="Legal jurisdiction")
            
            self.memory_manager = create_memory_store_manager(
                f"openai:{Config.OPENAI_MODEL}",
                namespace=("legal_knowledge", "{jurisdiction}", "{organization}"),
                schemas=[LegalConcept, LegalSubject],
                instructions="""Extract and store important legal concepts and subjects from legal documents. 
                Focus on:
                1. Legal subjects with comprehensive definitions
                2. Legal rules and their associated subjects
                3. Data management domains (storage, usage, movement, privacy, security, access, entitlements)
                4. Relationships between legal concepts and subjects
                5. Cross-jurisdictional relationships and adequacy determinations
                """,
                store=self.store,
                enable_inserts=True,
                enable_updates=True,
                enable_deletes=False
            )
            
        except Exception as e:
            logger.error(f"Failed to initialize memory manager: {e}")
            # Use basic in-memory store as fallback
            self.store = InMemoryStore()
    
    def get_memory_tools(self, namespace_params: Dict[str, str]):
        """Get memory tools for agents"""
        namespace = ("legal_knowledge", namespace_params.get("jurisdiction", "default"), 
                    namespace_params.get("organization", "default"))
        
        manage_tool = create_manage_memory_tool(namespace=namespace)
        search_tool = create_search_memory_tool(namespace=namespace)
        
        return [manage_tool, search_tool]

# ====================================
# COMPETENCY QUESTION GENERATOR
# ====================================

class CompetencyQuestionAgent:
    """ReAct agent that uses competency questions for better reasoning during extraction"""
    
    def __init__(self, openai_client: OpenAIClient, memory_manager: LegalMemoryManager):
        self.openai_client = openai_client
        self.memory_manager = memory_manager
        self.reasoning_steps = []
        self.competency_questions = self._get_core_competency_questions()
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "CompetencyQuestionAgent"
        })
        logger.info(f"[CQ Agent] REASONING: {thought}")
    
    def _get_core_competency_questions(self) -> List[str]:
        """Get core competency questions to guide agent reasoning"""
        return [
            # Scoping questions for coverage
            "What legal entities are involved in data processing?",
            "What types of personal data are regulated?", 
            "What are the different legal bases for processing?",
            "What data management domains are covered?",
            
            # Foundational questions for core concepts
            "What constitutes personal data under this jurisdiction?",
            "Who can be a data controller vs processor?",
            "What are the essential elements of consent?",
            "What adequacy countries exist for data transfers?",
            
            # Relationship questions for connections
            "How are data controllers related to processors?",
            "What is the relationship between purpose and legal basis?",
            "How do technical measures relate to security requirements?",
            "What conditions apply to cross-border transfers?",
            
            # Validation questions for completeness
            "Can we determine if processing is lawful?",
            "Are all data subject rights identified?",
            "Are consent requirements properly defined?",
            "Are retention periods specified?",
            
            # Domain-specific questions
            "What storage requirements and retention periods apply?",
            "What usage limitations and purpose restrictions exist?",
            "What cross-border transfer rules and safeguards apply?",
            "What privacy rights and transparency requirements exist?",
            "What security measures and breach procedures are mandated?",
            "What access controls and authentication requirements apply?",
            "What role-based entitlements and permissions are defined?"
        ]
    
    async def enhance_extraction_with_competency_reasoning(self, text: str, extraction_result: Dict, context: Dict) -> Dict:
        """Use competency questions to enhance and validate extraction results"""
        
        self.log_reasoning("Using competency questions to enhance extraction reasoning")
        self.log_reasoning(f"Applying {len(self.competency_questions)} competency questions for validation")
        
        system_prompt = """You are an expert legal ontology engineer. Use the provided competency questions to reason about and enhance the legal rule extraction results.

Your task is to:
1. Review the extraction results against each competency question
2. Identify gaps or missing information
3. Extract additional concepts, definitions, rules, or relationships
4. Ensure completeness for each data management domain

COMPETENCY QUESTIONS TO CONSIDER:
{competency_questions}

For each subject/concept extracted, ensure you have:
- DEFINITION: Clear, precise definition from the legal text
- RULES: Associated legal rules (obligations, permissions, prohibitions) - optional
- CONDITIONS: Conditions under which rules apply - optional  
- DOMAIN: Data management domain(s) (storage, usage, movement, privacy, security, access, entitlements)
- RELATIONSHIPS: How this concept relates to other concepts

Focus on creating comprehensive semantic structures that can answer the competency questions through ontology reasoning.

CRITICAL: Respond ONLY with enhanced JSON extraction results.""".format(
            competency_questions="\n".join([f"- {q}" for q in self.competency_questions])
        )

        user_prompt = f"""
Review and enhance these extraction results using competency question reasoning:

ORIGINAL EXTRACTION:
{json.dumps(extraction_result, indent=2)}

LEGAL TEXT CONTEXT:
{text[:3000]}...

JURISDICTION CONTEXT:
- Country: {context.get('country', 'Unknown')}
- Jurisdiction: {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}

Enhance the extraction by ensuring each competency question can be answered through the ontology. Add missing definitions, rules, conditions, and relationships.
"""

        self.log_reasoning("Sending enhancement request to o3-mini with competency question guidance")
        response = await self.openai_client.chat_completion([
            {"role": "developer", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], reasoning_effort="high")
        
        # Parse enhanced results
        response_text = response.strip()
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        try:
            enhanced_result = json.loads(response_text)
            self.log_reasoning(f"Successfully enhanced extraction with competency question reasoning")
            return enhanced_result
        except json.JSONDecodeError as e:
            self.log_reasoning(f"Enhancement parsing failed: {e}. Returning original results.")
            return extraction_result

# ====================================
# ENHANCED RULE EXTRACTION AGENT
# ====================================

class EnhancedRuleExtractionAgent:
    """Enhanced ReAct agent for comprehensive legal rule extraction"""
    
    def __init__(self, openai_client: OpenAIClient, memory_manager: LegalMemoryManager):
        self.openai_client = openai_client
        self.memory_manager = memory_manager
        self.reasoning_steps = []
        self.action_steps = []
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "EnhancedRuleExtractor"
        })
        logger.info(f"[Enhanced RuleExtractor] REASONING: {thought}")
    
    def log_action(self, action: str, result: Any = None):
        """Log an action step"""
        self.action_steps.append({
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "result": str(result)[:200] if result else None,
            "agent": "EnhancedRuleExtractor"
        })
        logger.info(f"[Enhanced RuleExtractor] ACTION: {action}")
    
    async def extract_comprehensive_rules(self, text: str, context: Dict) -> Dict:
        """Extract comprehensive legal rules with DPV and domain alignment"""
        
        self.log_reasoning("Starting comprehensive Rules-as-Code extraction")
        self.log_reasoning(f"Target jurisdiction: {context.get('country', 'Unknown')}")
        self.log_reasoning("Will extract rules, map to DPV concepts, and identify data management domains")
        
        system_prompt = """You are a specialized Rules-as-Code expert converting legislation into machine-readable formats. Your task is to extract comprehensive legal knowledge and structure it with proper definitions, rules, conditions, and domain classifications.

EXTRACTION REQUIREMENTS:

1. SUBJECTS - Extract all legal subjects/entities/concepts with:
   - DEFINITION: Precise legal definition from the text (MANDATORY)
   - RULES: Associated legal rules - obligations, permissions, prohibitions (OPTIONAL)
   - CONDITIONS: Conditions under which rules/definitions apply (OPTIONAL)
   - DOMAIN: Data management domain(s) (MANDATORY)
   - RELATIONSHIPS: How this subject relates to other subjects

2. OBJECT PROPERTIES - Define relationships between subjects:
   - Property name and URI suffix
   - Domain (subject type that has this property)
   - Range (object type this property points to)
   - Definition and usage
   - Inverse property if applicable

3. DATA PROPERTIES - Define attributes with literal values:
   - Property name and URI suffix  
   - Domain (subject type that has this property)
   - Range (datatype: string, date, boolean, integer, etc.)
   - Definition and constraints

4. DATA MANAGEMENT DOMAINS:
   - STORAGE: retention, archiving, deletion, backup policies
   - USAGE: purpose limitation, data minimization, processing activities
   - MOVEMENT: transfers, cross-border, adequacy, safeguards
   - PRIVACY: consent, transparency, data subject rights
   - SECURITY: encryption, access controls, breach notification
   - ACCESS: authorization, authentication, audit trails
   - ENTITLEMENTS: roles, permissions, data access rights

5. ADEQUACY AND JURISDICTIONAL ANALYSIS:
   - Extract adequacy decisions and approved countries
   - Identify third country transfer requirements
   - Map international data transfer rules and safeguards

CRITICAL: Every subject MUST have a definition and domain. Focus on creating comprehensive semantic structures.

Output format:
{
  "subjects": [
    {
      "name": "Data Controller",
      "uri_suffix": "DataController",
      "definition": "Natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data",
      "rules": [
        {
          "type": "obligation",
          "description": "Must implement appropriate technical and organisational measures",
          "conditions": ["when processing personal data", "to ensure security"],
          "legal_basis": "Article 32 GDPR"
        }
      ],
      "conditions": [
        {
          "type": "applicability",
          "description": "Applies when determining purposes and means of processing",
          "trigger": "processing decision authority"
        }
      ],
      "domains": ["usage", "privacy", "security"],
      "relationships": [
        {
          "property": "hasDataProcessor",
          "target": "DataProcessor", 
          "description": "May engage data processors"
        }
      ],
      "dpv_mapping": "dpv:DataController"
    }
  ],
  "object_properties": [
    {
      "name": "hasLegalBasis",
      "uri_suffix": "hasLegalBasis",
      "domain": "ProcessingActivity",
      "range": "LegalBasis",
      "definition": "Links a processing activity to its legal justification",
      "inverse_property": "isLegalBasisFor"
    }
  ],
  "data_properties": [
    {
      "name": "hasRetentionPeriod", 
      "uri_suffix": "hasRetentionPeriod",
      "domain": "DataRetentionPolicy",
      "range": "xsd:duration",
      "definition": "Specifies how long data must be retained",
      "constraints": ["must be reasonable and proportionate"]
    }
  ],
  "adequacy_decisions": [
    {
      "country": "Canada",
      "status": "adequate",
      "decision_date": "2001-12-20",
      "scope": "Personal Information Protection and Electronic Documents Act",
      "restrictions": []
    }
  ],
  "extraction_metadata": {
    "total_subjects": 15,
    "subjects_with_rules": 8,
    "subjects_with_conditions": 5,
    "domains_covered": ["storage", "usage", "movement", "privacy", "security"],
    "adequacy_decisions_found": 2
  }
}"""

        user_prompt = f"""
Analyze the following legal text and extract comprehensive Rules-as-Code with proper semantic structure:

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}
- Legal System: {context.get('legal_system', 'Civil Law')}

LEGAL TEXT:
{text}

Extract all legal subjects/entities/concepts with their definitions (mandatory), associated rules (optional), conditions (optional), and domain classifications. Create proper object and data properties to define relationships and attributes. Focus on creating a comprehensive semantic structure suitable for automated reasoning and compliance checking.

For each subject ensure:
- Clear, precise definition from the legal text
- Associated legal rules if applicable (obligations, permissions, prohibitions)
- Conditions under which the subject/rules apply
- Proper domain classification (storage, usage, movement, privacy, security, access, entitlements)
- Relationships to other subjects
"""

        self.log_action("Sending comprehensive Rules-as-Code extraction request to o3-mini")
        response = await self.openai_client.chat_completion([
            {"role": "developer", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], reasoning_effort="high")
        
        self.log_action("Processing response and storing in long-term memory")
        
        # Parse JSON response
        response_text = response.strip()
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        try:
            result = json.loads(response_text)
            self.log_action(f"Successfully extracted {len(result.get('subjects', []))} subjects with definitions")
            
            # Store in long-term memory using LangMem
            await self._store_in_memory(result, context)
            
            return result
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Returning basic structure.")
            return {
                "subjects": [],
                "object_properties": [],
                "data_properties": [],
                "adequacy_decisions": [],
                "extraction_metadata": {
                    "total_subjects": 0,
                    "subjects_with_rules": 0,
                    "subjects_with_conditions": 0,
                    "domains_covered": [],
                    "adequacy_decisions_found": 0
                }
            }
    
    async def _store_in_memory(self, extraction_result: Dict, context: Dict):
        """Store extraction results in long-term memory"""
        try:
            namespace_params = {
                "jurisdiction": context.get('jurisdiction', 'default'),
                "organization": context.get('organization', 'default')
            }
            
            # Store key concepts and subjects in memory for future reference
            if self.memory_manager.memory_manager:
                subjects_count = len(extraction_result.get('subjects', []))
                properties_count = len(extraction_result.get('object_properties', [])) + len(extraction_result.get('data_properties', []))
                
                memory_data = {
                    "messages": [
                        {
                            "role": "user",
                            "content": f"Extracted {subjects_count} legal subjects and {properties_count} properties from {context.get('country', 'unknown')} legislation"
                        }
                    ]
                }
                
                config = {"configurable": namespace_params}
                await self.memory_manager.memory_manager.ainvoke(memory_data, config=config)
                
                self.log_action("Stored extraction results in long-term memory")
        except Exception as e:
            self.log_reasoning(f"Failed to store in memory: {e}")

# ====================================
# ENHANCED ONTOLOGY BUILDER
# ====================================

class EnhancedOntologyBuilder:
    """Enhanced ontology builder with DPV/PROV-O integration and competency questions"""
    
    def __init__(self):
        self.ns = LegalRulesNamespaces()
        self.reasoning_steps = []
    
    def log_reasoning(self, thought: str):
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "EnhancedOntologyBuilder"
        })
        logger.info(f"[Enhanced OntologyBuilder] REASONING: {thought}")
    
    def build_comprehensive_ontology(self, extraction_result: Dict, context: Dict) -> Tuple[Graph, Graph]:
        """Build comprehensive OWL ontology and TTL knowledge graph"""
        
        self.log_reasoning("Building comprehensive Rules-as-Code ontology with subject-based structure")
        
        # Create OWL ontology (structure/schema)
        owl_ontology = Graph()
        owl_ontology = self.ns.bind_to_graph(owl_ontology)
        
        # Create TTL knowledge graph (instances/data)
        ttl_graph = Graph()
        ttl_graph = self.ns.bind_to_graph(ttl_graph)
        
        # Build ontology structure
        self._build_ontology_structure(owl_ontology, context)
        self._integrate_dpv_concepts(owl_ontology)
        self._add_data_management_domains(owl_ontology)
        self._add_extracted_object_properties(owl_ontology, extraction_result.get('object_properties', []))
        self._add_extracted_data_properties(owl_ontology, extraction_result.get('data_properties', []))
        
        # Populate knowledge graph with instances
        self._populate_knowledge_graph(ttl_graph, extraction_result, context)
        
        self.log_reasoning(f"Built ontology with {len(owl_ontology)} triples and knowledge graph with {len(ttl_graph)} triples")
        
        return owl_ontology, ttl_graph
    
    def _build_ontology_structure(self, graph: Graph, context: Dict):
        """Build the core ontology structure"""
        
        # Ontology metadata
        ontology_uri = URIRef(f"{self.ns.RAC}ontology")
        graph.add((ontology_uri, RDF.type, OWL.Ontology))
        graph.add((ontology_uri, DCTERMS.title, 
                  Literal(f"Rules-as-Code Ontology for {context.get('country', 'Legal Domain')}")))
        graph.add((ontology_uri, DCTERMS.created, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        graph.add((ontology_uri, RDFS.comment, 
                  Literal("Machine-readable legal rules ontology with DPV integration")))
        
        # Import DPV and PROV-O
        graph.add((ontology_uri, OWL.imports, URIRef("https://w3id.org/dpv")))
        graph.add((ontology_uri, OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        
        # Core classes
        core_classes = [
            ("LegalSubject", "A legal entity, concept, or subject with defined characteristics"),
            ("LegalRule", "A specific legal obligation, permission, or prohibition"),
            ("LegalCondition", "A condition under which legal rules or subjects apply"),
            ("LegalDefinition", "A formal legal definition of a concept or subject"),
            ("DataManagementDomain", "A domain of data management activity"),
            ("AdequacyDecision", "A decision on adequacy for data transfers"),
            ("Country", "A nation state with legal jurisdiction"),
            ("Organization", "A legal organization or entity"),
            ("Jurisdiction", "A legal administrative area"),
            ("ObjectProperty", "A relationship between legal subjects"),
            ("DataProperty", "An attribute or characteristic of a legal subject")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.RAC}{class_name}")
            graph.add((class_uri, RDF.type, OWL.Class))
            graph.add((class_uri, RDFS.label, Literal(class_name)))
            graph.add((class_uri, RDFS.comment, Literal(definition)))
    
    def _integrate_dpv_concepts(self, graph: Graph):
        """Integrate DPV concepts into the ontology"""
        
        # Map our concepts to DPV
        dpv_mappings = [
            ("LegalEntity", "dpv:LegalEntity"),
            ("PersonalDataProcessing", "dpv:PersonalDataProcessing"),
            ("DataController", "dpv:DataController"),
            ("DataProcessor", "dpv:DataProcessor"),
            ("DataSubject", "dpv:DataSubject"),
            ("LegalBasis", "dpv:LegalBasis"),
            ("Consent", "dpv:Consent"),
            ("LegitimateInterest", "dpv:LegitimateInterest"),
            ("TechnicalMeasure", "dpv:TechnicalMeasure"),
            ("OrganisationalMeasure", "dpv:OrganisationalMeasure")
        ]
        
        for local_concept, dpv_concept in dpv_mappings:
            local_uri = URIRef(f"{self.ns.RAC}{local_concept}")
            dpv_uri = URIRef(f"{self.ns.DPV}{dpv_concept.split(':')[1]}")
            
            graph.add((local_uri, RDF.type, OWL.Class))
            graph.add((local_uri, RDFS.subClassOf, dpv_uri))
            graph.add((local_uri, SKOS.exactMatch, dpv_uri))
    
    def _add_data_management_domains(self, graph: Graph):
        """Add data management domain concepts"""
        
        domains = [
            ("StorageDomain", "Data storage, retention, and archiving"),
            ("UsageDomain", "Data usage, purpose limitation, and processing"),
            ("MovementDomain", "Data transfers, cross-border movement, and adequacy"),
            ("PrivacyDomain", "Privacy rights, consent, and data subject rights"),
            ("SecurityDomain", "Data security, encryption, and access controls"),
            ("AccessDomain", "Data access, authorization, and authentication"),
            ("EntitlementsDomain", "Data entitlements, roles, and permissions")
        ]
        
        for domain_name, definition in domains:
            domain_uri = URIRef(f"{self.ns.RAC}{domain_name}")
            graph.add((domain_uri, RDF.type, OWL.Class))
            graph.add((domain_uri, RDFS.subClassOf, URIRef(f"{self.ns.RAC}DataManagementDomain")))
            graph.add((domain_uri, RDFS.label, Literal(domain_name)))
            graph.add((domain_uri, RDFS.comment, Literal(definition)))
    
    def _add_extracted_object_properties(self, graph: Graph, object_properties: List[Dict]):
        """Add extracted object properties to the ontology"""
        
        self.log_reasoning(f"Adding {len(object_properties)} extracted object properties")
        
        for prop in object_properties:
            prop_name = prop.get('name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            graph.add((prop_uri, RDF.type, OWL.ObjectProperty))
            graph.add((prop_uri, RDFS.label, Literal(prop_name)))
            graph.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain and range
            if prop.get('domain'):
                domain_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['domain'])}")
                graph.add((prop_uri, RDFS.domain, domain_uri))
            
            if prop.get('range'):
                range_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['range'])}")
                graph.add((prop_uri, RDFS.range, range_uri))
            
            # Add inverse property
            if prop.get('inverse_property'):
                inverse_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(prop['inverse_property'])}")
                graph.add((prop_uri, OWL.inverseOf, inverse_uri))
    
    def _add_extracted_data_properties(self, graph: Graph, data_properties: List[Dict]):
        """Add extracted data properties to the ontology"""
        
        self.log_reasoning(f"Adding {len(data_properties)} extracted data properties")
        
        for prop in data_properties:
            prop_name = prop.get('name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            graph.add((prop_uri, RDFS.label, Literal(prop_name)))
            graph.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain
            if prop.get('domain'):
                domain_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['domain'])}")
                graph.add((prop_uri, RDFS.domain, domain_uri))
            
            # Add range (datatype)
            if prop.get('range'):
                range_type = prop['range']
                if range_type.startswith('xsd:'):
                    range_uri = getattr(XSD, range_type.split(':')[1])
                    graph.add((prop_uri, RDFS.range, range_uri))
                else:
                    # Default to string if not specified as XSD type
                    graph.add((prop_uri, RDFS.range, XSD.string))
    
    def _populate_knowledge_graph(self, graph: Graph, extraction_result: Dict, context: Dict):
        """Populate the knowledge graph with extracted subjects and instances"""
        
        # Add country, jurisdiction, organization as individuals
        country_uri = URIRef(f"{self.ns.RAC}Country_{self._safe_uri_encode(context['country'])}")
        graph.add((country_uri, RDF.type, URIRef(f"{self.ns.RAC}Country")))
        graph.add((country_uri, RDFS.label, Literal(context['country'])))
        
        jurisdiction_uri = URIRef(f"{self.ns.RAC}Jurisdiction_{self._safe_uri_encode(context['jurisdiction'])}")
        graph.add((jurisdiction_uri, RDF.type, URIRef(f"{self.ns.RAC}Jurisdiction")))
        graph.add((jurisdiction_uri, RDFS.label, Literal(context['jurisdiction'])))
        
        org_uri = URIRef(f"{self.ns.RAC}Organization_{self._safe_uri_encode(context['organization'])}")
        graph.add((org_uri, RDF.type, URIRef(f"{self.ns.RAC}Organization")))
        graph.add((org_uri, RDFS.label, Literal(context['organization'])))
        
        # Add extracted subjects as classes and individuals
        for subject in extraction_result.get('subjects', []):
            self._add_subject_to_graph(graph, subject, country_uri)
        
        # Add adequacy decisions
        for adequacy in extraction_result.get('adequacy_decisions', []):
            self._add_adequacy_decision(graph, adequacy, country_uri)
    
    def _add_subject_to_graph(self, graph: Graph, subject: Dict, country_uri: URIRef):
        """Add a legal subject as both class and individual"""
        
        subject_name = subject.get('name', '')
        uri_suffix = subject.get('uri_suffix', self._safe_uri_encode(subject_name))
        
        # Add as class
        class_uri = URIRef(f"{self.ns.RAC}{uri_suffix}")
        graph.add((class_uri, RDF.type, OWL.Class))
        graph.add((class_uri, RDFS.label, Literal(subject_name)))
        
        # Add definition (mandatory)
        if subject.get('definition'):
            graph.add((class_uri, RDFS.comment, Literal(subject['definition'])))
            graph.add((class_uri, SKOS.definition, Literal(subject['definition'])))
        
        # Add as individual instance
        individual_uri = URIRef(f"{self.ns.RAC}{uri_suffix}_Instance")
        graph.add((individual_uri, RDF.type, class_uri))
        graph.add((individual_uri, RDFS.label, Literal(f"{subject_name} Instance")))
        
        # Link to country
        graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}appliesInCountry"), country_uri))
        
        # Add domain classifications
        for domain in subject.get('domains', []):
            domain_uri = URIRef(f"{self.ns.RAC}{domain.title()}Domain")
            graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}belongsToDomain"), domain_uri))
        
        # Add rules if present
        for i, rule in enumerate(subject.get('rules', [])):
            rule_uri = URIRef(f"{self.ns.RAC}Rule_{uri_suffix}_{i}")
            graph.add((rule_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalRule")))
            graph.add((rule_uri, RDFS.label, Literal(f"Rule for {subject_name} #{i+1}")))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasRuleType"), Literal(rule.get('type', 'obligation'))))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasRuleDescription"), Literal(rule.get('description', ''))))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}appliesTo"), individual_uri))
            
            # Add rule conditions
            for j, condition in enumerate(rule.get('conditions', [])):
                condition_uri = URIRef(f"{self.ns.RAC}Condition_{uri_suffix}_{i}_{j}")
                graph.add((condition_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalCondition")))
                graph.add((condition_uri, RDFS.label, Literal(f"Condition {j+1}")))
                graph.add((condition_uri, URIRef(f"{self.ns.PROPERTIES}hasConditionDescription"), Literal(condition)))
                graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasCondition"), condition_uri))
        
        # Add general conditions if present
        for i, condition in enumerate(subject.get('conditions', [])):
            condition_uri = URIRef(f"{self.ns.RAC}Condition_{uri_suffix}_{i}")
            graph.add((condition_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalCondition")))
            graph.add((condition_uri, RDFS.label, Literal(f"Condition for {subject_name} #{i+1}")))
            graph.add((condition_uri, URIRef(f"{self.ns.PROPERTIES}hasConditionType"), Literal(condition.get('type', 'applicability'))))
            graph.add((condition_uri, URIRef(f"{self.ns.PROPERTIES}hasConditionDescription"), Literal(condition.get('description', ''))))
            graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}hasCondition"), condition_uri))
        
        # Add relationships
        for relationship in subject.get('relationships', []):
            if relationship.get('property') and relationship.get('target'):
                prop_uri = URIRef(f"{self.ns.PROPERTIES}{relationship['property']}")
                target_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(relationship['target'])}_Instance")
                graph.add((individual_uri, prop_uri, target_uri))
        
        # Add DPV mapping if present
        if subject.get('dpv_mapping'):
            dpv_concept = subject['dpv_mapping']
            if dpv_concept.startswith('dpv:'):
                dpv_uri = URIRef(f"{self.ns.DPV}{dpv_concept.split(':')[1]}")
                graph.add((class_uri, SKOS.exactMatch, dpv_uri))
        
        # Add provenance
        graph.add((individual_uri, self.ns.PROV.wasGeneratedBy, URIRef(f"{self.ns.RAC}RulesAsCodeExtraction")))
        graph.add((individual_uri, self.ns.PROV.generatedAtTime, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_adequacy_decision(self, graph: Graph, adequacy: Dict, country_uri: URIRef):
        """Add adequacy decision as individual"""
        
        country_name = adequacy.get('country', '').replace(' ', '_')
        adequacy_uri = URIRef(f"{self.ns.RAC}AdequacyDecision_{country_name}")
        
        graph.add((adequacy_uri, RDF.type, URIRef(f"{self.ns.RAC}AdequacyDecision")))
        graph.add((adequacy_uri, RDFS.label, Literal(f"Adequacy Decision for {adequacy.get('country', 'Unknown')}")))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasAdequacyStatus"), Literal(adequacy.get('status', 'unknown'))))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}forCountry"), Literal(adequacy.get('country', 'Unknown'))))
        
        if adequacy.get('decision_date'):
            graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasDecisionDate"), 
                      Literal(adequacy['decision_date'], datatype=XSD.date)))
        
        if adequacy.get('scope'):
            graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasScope"), Literal(adequacy['scope'])))
        
        # Link to source country
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}decidedBy"), country_uri))
    
    def _safe_uri_encode(self, text: str) -> str:
        """Safely encode text for use in URIs"""
        import urllib.parse
        safe_text = text.replace(' ', '_').replace('/', '_').replace('\\', '_')
        return urllib.parse.quote(safe_text, safe='')

# ====================================
# MAIN ORCHESTRATOR
# ====================================

class RulesAsCodeOrchestrator:
    """Main orchestrator for the Rules-as-Code system"""
    
    def __init__(self):
        # Initialize core components
        self.openai_client = self._initialize_openai_client()
        self.es_client = self._initialize_elasticsearch_client()
        self.memory_manager = LegalMemoryManager(self.openai_client)
        
        # Initialize agents
        self.rule_agent = EnhancedRuleExtractionAgent(self.openai_client, self.memory_manager)
        self.cq_agent = CompetencyQuestionAgent(self.openai_client, self.memory_manager)
        self.ontology_builder = EnhancedOntologyBuilder()
        
        # Initialize document processor
        self.doc_processor = DocumentProcessor()
        
        # Initialize SHACL validator
        self.shacl_validator = SHACLValidator()
        
        # Initialize query interface
        self.query_interface = None
        if FLASK_AVAILABLE:
            self.query_interface = LegalKnowledgeQueryInterface(self)
    
    def _initialize_openai_client(self):
        """Initialize OpenAI client"""
        try:
            return OpenAIClient()
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _initialize_elasticsearch_client(self):
        """Initialize Elasticsearch client"""
        try:
            client = ElasticsearchClient()
            client.create_index()
            return client
        except Exception as e:
            logger.error(f"Failed to initialize Elasticsearch client: {e}")
            raise
    
    async def process_legal_document(self, document_path: str, metadata: Dict) -> Dict:
        """Process a legal document into Rules-as-Code"""
        
        logger.info(f"Processing legal document: {document_path}")
        logger.info(f"Jurisdiction: {metadata.get('country', 'Unknown')}/{metadata.get('jurisdiction', 'Unknown')}")
        
        try:
            # Extract text from PDF
            text_content = self.doc_processor.extract_text_from_pdf(document_path)
            logger.info(f"Extracted {len(text_content)} characters from document")
            
            # Enhanced rule extraction with subject-based structure
            extraction_result = await self.rule_agent.extract_comprehensive_rules(text_content, metadata)
            
            # Enhance extraction using competency question reasoning
            enhanced_result = await self.cq_agent.enhance_extraction_with_competency_reasoning(
                text_content, extraction_result, metadata
            )
            
            # Build ontology and knowledge graph
            owl_ontology, ttl_graph = self.ontology_builder.build_comprehensive_ontology(
                enhanced_result, metadata
            )
            
            # Validate ontology with SHACL
            validation_results = self._validate_ontology(owl_ontology)
            
            # Store in Elasticsearch for search
            await self._store_in_elasticsearch(enhanced_result, metadata, ttl_graph)
            
            # Export in multiple formats
            exports = self._export_ontologies(owl_ontology, ttl_graph, metadata)
            
            # Setup query interface
            if self.query_interface:
                self.query_interface.load_ontology(ttl_graph)
            
            return {
                "success": True,
                "extraction_stats": {
                    "subjects_extracted": len(enhanced_result.get("subjects", [])),
                    "subjects_with_definitions": len([s for s in enhanced_result.get("subjects", []) if s.get("definition")]),
                    "subjects_with_rules": len([s for s in enhanced_result.get("subjects", []) if s.get("rules")]),
                    "subjects_with_conditions": len([s for s in enhanced_result.get("subjects", []) if s.get("conditions")]),
                    "object_properties": len(enhanced_result.get("object_properties", [])),
                    "data_properties": len(enhanced_result.get("data_properties", [])),
                    "adequacy_decisions": len(enhanced_result.get("adequacy_decisions", [])),
                    "domains_covered": list(set([d for s in enhanced_result.get("subjects", []) for d in s.get("domains", [])]))
                },
                "ontology_stats": {
                    "owl_triples": len(owl_ontology),
                    "ttl_triples": len(ttl_graph),
                    "classes_defined": len(list(owl_ontology.subjects(RDF.type, OWL.Class))),
                    "object_properties_defined": len(list(owl_ontology.subjects(RDF.type, OWL.ObjectProperty))),
                    "data_properties_defined": len(list(owl_ontology.subjects(RDF.type, OWL.DatatypeProperty)))
                },
                "validation_results": validation_results,
                "exports": exports,
                "query_interface_ready": self.query_interface is not None
            }
            
        except Exception as e:
            logger.error(f"Failed to process document: {e}")
            return {"success": False, "error": str(e)}
    
    def _validate_ontology(self, ontology: Graph) -> Dict:
        """Validate ontology using SHACL"""
        try:
            conforms, results_graph, results_text = pyshacl.validate(
                data_graph=ontology,
                inference='rdfs',
                serialize_report_graph=True
            )
            
            return {
                "conforms": conforms,
                "validation_performed": True,
                "results_text": results_text if not conforms else "Validation passed",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"SHACL validation failed: {e}")
            return {
                "conforms": False,
                "validation_performed": False,
                "error": str(e)
            }
    
    async def _store_in_elasticsearch(self, extraction_result: Dict, metadata: Dict, knowledge_graph: Graph):
        """Store results in Elasticsearch for search and retrieval"""
        
        # Generate embeddings for search
        subjects_text = " ".join([
            f"{subject.get('name', '')} {subject.get('definition', '')}" 
            for subject in extraction_result.get('subjects', [])
        ])
        
        if subjects_text:
            embeddings = await self.openai_client.generate_embeddings([subjects_text])
            content_vector = embeddings[0] if embeddings else []
        else:
            content_vector = []
        
        # Prepare document for Elasticsearch
        doc = {
            "document_id": str(uuid.uuid4()),
            "country": metadata.get("country", "Unknown"),
            "jurisdiction": metadata.get("jurisdiction", "Unknown"),
            "organization": metadata.get("organization", "Unknown"),
            "title": f"Rules-as-Code for {metadata.get('country', 'Unknown')}",
            
            # Subject-based structure
            "subjects": extraction_result.get("subjects", []),
            "object_properties": extraction_result.get("object_properties", []),
            "data_properties": extraction_result.get("data_properties", []),
            "adequacy_decisions": extraction_result.get("adequacy_decisions", []),
            
            # Search fields
            "subject_names": [s.get('name', '') for s in extraction_result.get('subjects', [])],
            "domains": list(set([d for s in extraction_result.get('subjects', []) for d in s.get('domains', [])])),
            "has_definitions": len([s for s in extraction_result.get('subjects', []) if s.get('definition')]),
            "has_rules": len([s for s in extraction_result.get('subjects', []) if s.get('rules')]),
            
            # Vector for semantic search
            "content_vector": content_vector,
            
            # Metadata
            "processing_timestamp": datetime.now().isoformat(),
            "ontology_triples_count": len(knowledge_graph),
            "system_version": "rules-as-code-v2.1"
        }
        
        # Store in Elasticsearch
        try:
            self.es_client.client.index(
                index=Config.ELASTICSEARCH_INDEX,
                body=doc
            )
            logger.info("Successfully stored in Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to store in Elasticsearch: {e}")
    
    def _extract_concepts_for_search(self, extraction_result: Dict) -> List[str]:
        """Extract concepts for search indexing"""
        concepts = []
        
        # From subjects
        for subject in extraction_result.get('subjects', []):
            if subject.get('name'):
                concepts.append(subject['name'])
        
        # From object properties
        for prop in extraction_result.get('object_properties', []):
            if prop.get('name'):
                concepts.append(prop['name'])
        
        # From data properties
        for prop in extraction_result.get('data_properties', []):
            if prop.get('name'):
                concepts.append(prop['name'])
        
        return [c for c in concepts if c and isinstance(c, str)]
    
    def _export_ontologies(self, owl_ontology: Graph, ttl_graph: Graph, metadata: Dict) -> Dict:
        """Export ontologies in multiple formats"""
        
        safe_country = metadata['country'].replace(' ', '_').replace('/', '_')
        output_dir = Path(Config.OUTPUT_PATH) / safe_country
        output_dir.mkdir(parents=True, exist_ok=True)
        
        exports = {}
        
        # Export OWL ontology (schema/structure)
        owl_formats = {
            'owl': 'xml',
            'ttl': 'turtle',
            'jsonld': 'json-ld'
        }
        
        for ext, format_name in owl_formats.items():
            filename = output_dir / f"rules_as_code_ontology_{safe_country}.{ext}"
            try:
                owl_ontology.serialize(destination=str(filename), format=format_name)
                exports[f"ontology_{ext}"] = str(filename)
                logger.info(f"Exported OWL ontology to {filename}")
            except Exception as e:
                logger.error(f"Failed to export ontology in {format_name}: {e}")
        
        # Export TTL knowledge graph (instances/data)
        kg_formats = {
            'ttl': 'turtle',
            'jsonld': 'json-ld',
            'xml': 'xml'
        }
        
        for ext, format_name in kg_formats.items():
            filename = output_dir / f"rules_as_code_knowledge_graph_{safe_country}.{ext}"
            try:
                ttl_graph.serialize(destination=str(filename), format=format_name)
                exports[f"knowledge_graph_{ext}"] = str(filename)
                logger.info(f"Exported knowledge graph to {filename}")
            except Exception as e:
                logger.error(f"Failed to export knowledge graph in {format_name}: {e}")
        
        return exports
    
    def start_query_interface(self, host='localhost', port=5000):
        """Start the web-based query interface"""
        if not FLASK_AVAILABLE:
            logger.warning("Flask not available - query interface disabled")
            return None
        
        if not self.query_interface:
            logger.warning("Query interface not initialized")
            return None
        
        def run_server():
            self.query_interface.start_server(host, port)
        
        server_thread = threading.Thread(target=run_server, daemon=True)
        server_thread.start()
        logger.info(f"Query interface started at http://{host}:{port}")
        return server_thread

# ====================================
# SUPPORTING CLASSES
# ====================================

class DocumentProcessor:
    """Document processor for PDF files"""
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = pymupdf.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text.strip()
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            raise

class ElasticsearchClient:
    """Elasticsearch client for storing and searching rules"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize Elasticsearch client"""
        ssl_context = ssl.create_default_context()
        if os.path.exists(Config.ELASTICSEARCH_CERT_PATH):
            ssl_context.load_verify_locations(Config.ELASTICSEARCH_CERT_PATH)
        else:
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
        
        self.client = Elasticsearch(
            [Config.ELASTICSEARCH_URL],
            basic_auth=(Config.ELASTICSEARCH_USERNAME, Config.ELASTICSEARCH_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True if os.path.exists(Config.ELASTICSEARCH_CERT_PATH) else False
        )
        
        if self.client.ping():
            logger.info("Successfully connected to Elasticsearch")
        else:
            raise ConnectionError("Failed to connect to Elasticsearch")
    
    def create_index(self):
        """Create the legal rules index"""
        mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "country": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "organization": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "rules": {"type": "nested"},
                    "adequacy_decisions": {"type": "nested"},
                    "competency_questions": {"type": "nested"},
                    "concepts": {"type": "keyword"},
                    "domains": {"type": "keyword"},
                    "rule_types": {"type": "keyword"},
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "processing_timestamp": {"type": "date"},
                    "ontology_triples_count": {"type": "integer"}
                }
            }
        }
        
        if not self.client.indices.exists(index=Config.ELASTICSEARCH_INDEX):
            self.client.indices.create(index=Config.ELASTICSEARCH_INDEX, body=mapping)
            logger.info(f"Created index: {Config.ELASTICSEARCH_INDEX}")

class SHACLValidator:
    """SHACL validator for ontology validation"""
    
    def __init__(self):
        pass  # Basic validator - can be extended with custom shapes

class LegalKnowledgeQueryInterface:
    """Web interface for querying the Rules-as-Code knowledge graph"""
    
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.knowledge_graph = Graph()
        
        if FLASK_AVAILABLE:
            self.app = Flask(__name__)
            CORS(self.app)
            self._setup_routes()
        else:
            self.app = None
    
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/')
        def index():
            return render_template_string(self._get_index_template())
        
        @self.app.route('/api/sparql', methods=['POST'])
        def sparql_query():
            try:
                query = request.json.get('query', '')
                results = self.execute_sparql_query(query)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/triples')
        def get_triples():
            try:
                # Get all triples from knowledge graph
                triples = []
                for subj, pred, obj in self.knowledge_graph:
                    triples.append({
                        "subject": str(subj),
                        "predicate": str(pred),
                        "object": str(obj)
                    })
                
                return jsonify({
                    "success": True, 
                    "triples": triples[:1000],  # Limit to first 1000 for performance
                    "total_count": len(triples)
                })
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/classes')
        def get_classes():
            try:
                # Get all classes from knowledge graph
                classes_query = """
                PREFIX rac: <https://rules-as-code.org/ontology#>
                PREFIX owl: <http://www.w3.org/2002/07/owl#>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
                
                SELECT ?class ?label ?definition ?superClass WHERE {
                    ?class a owl:Class .
                    OPTIONAL { ?class rdfs:label ?label }
                    OPTIONAL { ?class skos:definition ?definition }
                    OPTIONAL { ?class rdfs:subClassOf ?superClass }
                }
                """
                results = self.execute_sparql_query(classes_query)
                return jsonify({"success": True, "classes": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/properties')
        def get_properties():
            try:
                # Get all properties from knowledge graph
                properties_query = """
                PREFIX rac: <https://rules-as-code.org/ontology#>
                PREFIX owl: <http://www.w3.org/2002/07/owl#>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                
                SELECT ?property ?type ?label ?domain ?range WHERE {
                    ?property a ?type .
                    FILTER(?type = owl:ObjectProperty || ?type = owl:DatatypeProperty)
                    OPTIONAL { ?property rdfs:label ?label }
                    OPTIONAL { ?property rdfs:domain ?domain }
                    OPTIONAL { ?property rdfs:range ?range }
                }
                """
                results = self.execute_sparql_query(properties_query)
                return jsonify({"success": True, "properties": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/subjects')
        def get_subjects():
            try:
                # Get all subjects with their definitions and rules
                subjects_query = """
                PREFIX rac: <https://rules-as-code.org/ontology#>
                PREFIX properties: <https://rules-as-code.org/properties#>
                PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                
                SELECT ?subject ?label ?definition ?domain WHERE {
                    ?subject a owl:Class .
                    ?instance a ?subject .
                    OPTIONAL { ?subject rdfs:label ?label }
                    OPTIONAL { ?subject skos:definition ?definition }
                    OPTIONAL { ?instance properties:belongsToDomain ?domain }
                }
                """
                results = self.execute_sparql_query(subjects_query)
                return jsonify({"success": True, "subjects": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
    
    def execute_sparql_query(self, query: str) -> List[Dict]:
        """Execute SPARQL query against the knowledge graph"""
        try:
            results = self.knowledge_graph.query(query)
            result_list = []
            
            for row in results:
                row_dict = {}
                for var in results.vars:
                    value = row[var]
                    if value:
                        row_dict[str(var)] = str(value)
                result_list.append(row_dict)
            
            return result_list
        except Exception as e:
            raise Exception(f"SPARQL query failed: {e}")
    
    def load_ontology(self, knowledge_graph: Graph):
        """Load knowledge graph for querying"""
        self.knowledge_graph = knowledge_graph
        logger.info(f"Loaded knowledge graph with {len(knowledge_graph)} triples")
    
    def _get_index_template(self) -> str:
        """Return enhanced HTML template for Rules-as-Code interface"""
        return """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rules-as-Code Query Interface</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; }
        .container { max-width: 1400px; margin: 0 auto; padding: 20px; }
        .header { text-align: center; color: white; margin-bottom: 30px; }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3); }
        .header p { font-size: 1.2em; opacity: 0.9; }
        .main-panel { background: white; border-radius: 15px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); overflow: hidden; }
        .tabs { display: flex; background: #f8f9fa; border-bottom: 1px solid #dee2e6; }
        .tab { flex: 1; padding: 15px 20px; background: none; border: none; cursor: pointer; font-weight: 500; transition: all 0.3s; }
        .tab.active { background: #007bff; color: white; }
        .tab:hover:not(.active) { background: #e9ecef; }
        .tab-content { display: none; padding: 30px; }
        .tab-content.active { display: block; }
        .query-section { margin-bottom: 20px; }
        .query-area { width: 100%; height: 200px; font-family: 'Courier New', monospace; border: 2px solid #e9ecef; border-radius: 8px; padding: 15px; font-size: 14px; resize: vertical; }
        .query-area:focus { border-color: #007bff; outline: none; box-shadow: 0 0 0 3px rgba(0,123,255,0.25); }
        .button-group { display: flex; gap: 10px; margin: 20px 0; }
        .button { background: linear-gradient(45deg, #007bff, #0056b3); color: white; padding: 12px 24px; border: none; border-radius: 8px; cursor: pointer; font-weight: 500; transition: all 0.3s; }
        .button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,123,255,0.3); }
        .button.secondary { background: linear-gradient(45deg, #6c757d, #545b62); }
        .results { border: 2px solid #e9ecef; border-radius: 8px; padding: 20px; margin-top: 20px; max-height: 400px; overflow-y: auto; background: #f8f9fa; }
        .results table { width: 100%; border-collapse: collapse; background: white; border-radius: 8px; overflow: hidden; }
        .results th { background: #007bff; color: white; padding: 12px; text-align: left; }
        .results td { padding: 12px; border-bottom: 1px solid #e9ecef; }
        .results tr:hover { background: #f1f3f4; }
        .sample-queries { background: #e7f3ff; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .sample-query { background: white; border-radius: 6px; padding: 15px; margin: 10px 0; cursor: pointer; transition: all 0.3s; border-left: 4px solid #007bff; }
        .sample-query:hover { box-shadow: 0 2px 8px rgba(0,0,0,0.1); transform: translateX(5px); }
        .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
        .card { background: white; border-radius: 8px; padding: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); transition: all 0.3s; }
        .card:hover { transform: translateY(-2px); box-shadow: 0 4px 16px rgba(0,0,0,0.15); }
        .card-title { font-weight: bold; margin-bottom: 10px; color: #007bff; }
        .card-content { font-size: 0.9em; color: #666; }
        .stats { display: flex; justify-content: space-around; background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0; }
        .stat { text-align: center; }
        .stat-number { font-size: 2em; font-weight: bold; color: #007bff; }
        .stat-label { color: #666; margin-top: 5px; }
        .triple-item { background: white; border-radius: 6px; padding: 10px; margin: 5px 0; border-left: 4px solid #28a745; }
        .triple-subject { font-weight: bold; color: #007bff; }
        .triple-predicate { color: #6c757d; margin: 0 10px; }
        .triple-object { color: #28a745; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1> Rules-as-Code Query Interface</h1>
            <p>Explore machine-readable legal rules with semantic queries and ontology structure</p>
        </div>
        
        <div class="main-panel">
            <div class="tabs">
                <button class="tab active" onclick="showTab('sparql')">SPARQL Queries</button>
                <button class="tab" onclick="showTab('triples')">Triples</button>
                <button class="tab" onclick="showTab('classes')">Classes</button>
                <button class="tab" onclick="showTab('properties')">Properties</button>
                <button class="tab" onclick="showTab('subjects')">Subjects</button>
            </div>
            
            <div id="sparql" class="tab-content active">
                <h3> SPARQL Query Interface</h3>
                <p>Query the Rules-as-Code knowledge graph using SPARQL. The system includes comprehensive legal subject definitions and semantic relationships.</p>
                
                <div class="query-section">
                    <textarea id="sparqlQuery" class="query-area" placeholder="Enter your SPARQL query here...

Example - Find all legal subjects with definitions:
PREFIX rac: <https://rules-as-code.org/ontology#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>

SELECT ?subject ?label ?definition WHERE {
  ?subject a owl:Class ;
           rdfs:label ?label ;
           skos:definition ?definition .
} LIMIT 10"></textarea>
                    
                    <div class="button-group">
                        <button class="button" onclick="executeSparqlQuery()"> Execute Query</button>
                        <button class="button secondary" onclick="clearResults()"> Clear Results</button>
                        <button class="button secondary" onclick="loadSampleQuery()"> Load Sample</button>
                    </div>
                </div>
            </div>
            
            <div id="triples" class="tab-content">
                <h3> Knowledge Graph Triples</h3>
                <p>Browse the RDF triples that make up the Rules-as-Code knowledge graph.</p>
                
                <div class="button-group">
                    <button class="button" onclick="loadTriples()"> Load Triples</button>
                    <button class="button secondary" onclick="exportTriples()"> Export</button>
                </div>
                
                <div id="triplesContainer">
                    <!-- Triples will be loaded here -->
                </div>
            </div>
            
            <div id="classes" class="tab-content">
                <h3> Ontology Classes</h3>
                <p>Explore the classes defined in the Rules-as-Code ontology with their definitions and relationships.</p>
                
                <div class="button-group">
                    <button class="button" onclick="loadClasses()"> Load Classes</button>
                </div>
                
                <div id="classesContainer" class="grid">
                    <!-- Classes will be loaded here -->
                </div>
            </div>
            
            <div id="properties" class="tab-content">
                <h3> Object & Data Properties</h3>
                <p>View the properties that define relationships and attributes in the legal domain.</p>
                
                <div class="button-group">
                    <button class="button" onclick="loadProperties()"> Load Properties</button>
                </div>
                
                <div id="propertiesContainer" class="grid">
                    <!-- Properties will be loaded here -->
                </div>
            </div>
            
            <div id="subjects" class="tab-content">
                <h3> Legal Subjects</h3>
                <p>Explore legal subjects with their definitions, rules, conditions, and domain classifications.</p>
                
                <div class="button-group">
                    <button class="button" onclick="loadSubjects()"> Load Subjects</button>
                </div>
                
                <div id="subjectsContainer" class="grid">
                    <!-- Subjects will be loaded here -->
                </div>
            </div>
            
            <div id="results" class="results" style="display: none;">
                <h3> Query Results</h3>
                <div id="resultContent"></div>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            document.querySelectorAll('.tab-content').forEach(tab => tab.classList.remove('active'));
            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
            
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
        
        async function executeSparqlQuery() {
            const query = document.getElementById('sparqlQuery').value;
            if (!query.trim()) {
                alert('Please enter a SPARQL query');
                return;
            }
            
            try {
                const response = await fetch('/api/sparql', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'table');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function loadTriples() {
            try {
                const response = await fetch('/api/triples');
                const data = await response.json();
                
                if (data.success) {
                    displayTriples(data.triples, data.total_count);
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Failed to load triples: ' + error.message);
            }
        }
        
        async function loadClasses() {
            try {
                const response = await fetch('/api/classes');
                const data = await response.json();
                
                if (data.success) {
                    displayClasses(data.classes);
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Failed to load classes: ' + error.message);
            }
        }
        
        async function loadProperties() {
            try {
                const response = await fetch('/api/properties');
                const data = await response.json();
                
                if (data.success) {
                    displayProperties(data.properties);
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Failed to load properties: ' + error.message);
            }
        }
        
        async function loadSubjects() {
            try {
                const response = await fetch('/api/subjects');
                const data = await response.json();
                
                if (data.success) {
                    displaySubjects(data.subjects);
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Failed to load subjects: ' + error.message);
            }
        }
        
        function displayTriples(triples, totalCount) {
            const container = document.getElementById('triplesContainer');
            let html = `<div class="stats">
                <div class="stat">
                    <div class="stat-number">${totalCount}</div>
                    <div class="stat-label">Total Triples</div>
                </div>
                <div class="stat">
                    <div class="stat-number">${triples.length}</div>
                    <div class="stat-label">Displayed</div>
                </div>
            </div>`;
            
            triples.forEach(triple => {
                html += `
                    <div class="triple-item">
                        <span class="triple-subject">${triple.subject}</span>
                        <span class="triple-predicate">${triple.predicate}</span>
                        <span class="triple-object">${triple.object}</span>
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }
        
        function displayClasses(classes) {
            const container = document.getElementById('classesContainer');
            let html = '';
            
            classes.forEach(cls => {
                html += `
                    <div class="card">
                        <div class="card-title">${cls.label || cls.class}</div>
                        <div class="card-content">
                            ${cls.definition ? `<p><strong>Definition:</strong> ${cls.definition}</p>` : ''}
                            ${cls.superClass ? `<p><strong>Super Class:</strong> ${cls.superClass}</p>` : ''}
                            <p><strong>URI:</strong> ${cls.class}</p>
                        </div>
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }
        
        function displayProperties(properties) {
            const container = document.getElementById('propertiesContainer');
            let html = '';
            
            properties.forEach(prop => {
                const isObjectProperty = prop.type.includes('ObjectProperty');
                const cardColor = isObjectProperty ? '#007bff' : '#28a745';
                
                html += `
                    <div class="card">
                        <div class="card-title" style="color: ${cardColor};">
                            ${prop.label || prop.property}
                            <span style="font-size: 0.8em; color: #666;">
                                (${isObjectProperty ? 'Object Property' : 'Data Property'})
                            </span>
                        </div>
                        <div class="card-content">
                            ${prop.domain ? `<p><strong>Domain:</strong> ${prop.domain}</p>` : ''}
                            ${prop.range ? `<p><strong>Range:</strong> ${prop.range}</p>` : ''}
                            <p><strong>URI:</strong> ${prop.property}</p>
                        </div>
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }
        
        function displaySubjects(subjects) {
            const container = document.getElementById('subjectsContainer');
            let html = '';
            
            subjects.forEach(subject => {
                html += `
                    <div class="card">
                        <div class="card-title">${subject.label || subject.subject}</div>
                        <div class="card-content">
                            ${subject.definition ? `<p><strong>Definition:</strong> ${subject.definition}</p>` : ''}
                            ${subject.domain ? `<p><strong>Domain:</strong> ${subject.domain}</p>` : ''}
                            <p><strong>URI:</strong> ${subject.subject}</p>
                        </div>
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }
        
        function displayResults(results, format) {
            const resultsDiv = document.getElementById('results');
            const contentDiv = document.getElementById('resultContent');
            
            if (results.length === 0) {
                contentDiv.innerHTML = '<p>No results found.</p>';
            } else {
                let html = '<table><thead><tr>';
                
                // Get column headers
                if (results.length > 0) {
                    Object.keys(results[0]).forEach(key => {
                        html += `<th>${key}</th>`;
                    });
                    html += '</tr></thead><tbody>';
                    
                    results.forEach(row => {
                        html += '<tr>';
                        Object.values(row).forEach(value => {
                            html += `<td>${value}</td>`;
                        });
                        html += '</tr>';
                    });
                    html += '</tbody></table>';
                }
                
                contentDiv.innerHTML = html;
            }
            
            resultsDiv.style.display = 'block';
        }
        
        function loadSampleQuery() {
            const sampleQuery = `PREFIX rac: <https://rules-as-code.org/ontology#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>

SELECT ?subject ?label ?definition WHERE {
  ?subject a owl:Class ;
           rdfs:label ?label ;
           skos:definition ?definition .
} LIMIT 5`;
            
            document.getElementById('sparqlQuery').value = sampleQuery;
        }
        
        function showError(message) {
            const contentDiv = document.getElementById('resultContent');
            contentDiv.innerHTML = `<div style="color: #dc3545; background: #f8d7da; padding: 15px; border-radius: 8px; border: 1px solid #f5c6cb;">
                <strong>Error:</strong> ${message}
            </div>`;
            document.getElementById('results').style.display = 'block';
        }
        
        function clearResults() {
            document.getElementById('results').style.display = 'none';
            document.getElementById('resultContent').innerHTML = '';
        }
        
        function exportTriples() {
            // This would trigger a download of the triples in a suitable format
            alert('Export functionality would be implemented here');
        }
    </script>
</body>
</html>
        """
    
    def start_server(self, host='localhost', port=5000):
        """Start the web server"""
        if not FLASK_AVAILABLE or not self.app:
            logger.error("Flask not available - cannot start web server")
            return
        
        logger.info(f"Starting Rules-as-Code query interface on {host}:{port}")
        self.app.run(host=host, port=port, debug=False)

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file for Rules-as-Code"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission",
                    "legal_system": "Civil Law",
                    "pdf_document": "./documents/gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access"]
                },
                {
                    "country": "United States",
                    "jurisdiction": "Federal",
                    "organization": "Federal Trade Commission",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/ccpa.pdf",
                    "adequacy_focus": False,
                    "data_management_domains": ["privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United Kingdom",
                    "jurisdiction": "UK",
                    "organization": "Information Commissioner's Office",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/uk_gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                }
            ],
            "processing_options": {
                "enable_query_interface": True,
                "interface_host": "localhost",
                "interface_port": 5000,
                "reasoning_effort": "high",
                "generate_competency_questions": True,
                "dpv_integration": True,
                "export_formats": ["ttl", "jsonld", "xml", "owl"]
            },
            "adequacy_countries": [
                "Andorra", "Argentina", "Canada", "Faroe Islands", "Guernsey", "Israel", 
                "Isle of Man", "Japan", "Jersey", "New Zealand", "South Korea", 
                "Switzerland", "United Kingdom", "Uruguay"
            ]
        }

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Enhanced main application for Rules-as-Code"""
    
    load_dotenv()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info(" Starting Enhanced Rules-as-Code System")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info(" Configuration validation passed")
    except ValueError as e:
        logger.error(f" Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info(" Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json with your documents and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize Rules-as-Code orchestrator
    logger.info(" Initializing Rules-as-Code orchestrator...")
    orchestrator = RulesAsCodeOrchestrator()
    
    # Start query interface if enabled
    interface_enabled = config.get('processing_options', {}).get('enable_query_interface', True) and FLASK_AVAILABLE
    if interface_enabled:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        server_thread = orchestrator.start_query_interface(interface_host, interface_port)
    else:
        logger.info(" Query interface disabled or Flask not available")
        server_thread = None
    
    # Process legal documents
    results = []
    total_docs = len(config['documents'])
    
    for i, doc_config in enumerate(config['documents'], 1):
        logger.info(f" Processing document {i}/{total_docs}: {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f" Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_legal_document(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "jurisdiction": doc_config['jurisdiction'],
                "result": result
            })
            
            if result['success']:
                logger.info(f" Successfully processed {doc_config['country']}")
                logger.info(f"    Rules extracted: {result['extraction_stats']['rules_extracted']}")
                logger.info(f"    Competency questions: {result['extraction_stats']['competency_questions_generated']}")
                logger.info(f"    Domain coverage: {', '.join(result['extraction_stats']['domain_coverage'])}")
            else:
                logger.error(f" Failed to process {doc_config['country']}: {result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f" Processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Generate summary
    successful = sum(1 for r in results if r['result']['success'])
    total_subjects = sum(r['result'].get('extraction_stats', {}).get('subjects_extracted', 0) 
                        for r in results if r['result']['success'])
    total_definitions = sum(r['result'].get('extraction_stats', {}).get('subjects_with_definitions', 0) 
                           for r in results if r['result']['success'])
    total_properties = sum(
        r['result'].get('extraction_stats', {}).get('object_properties', 0) + 
        r['result'].get('extraction_stats', {}).get('data_properties', 0)
        for r in results if r['result']['success']
    )
    
    logger.info(" Rules-as-Code processing complete!")
    logger.info(f"    Documents processed: {successful}/{total_docs}")
    logger.info(f"    Total subjects extracted: {total_subjects}")
    logger.info(f"    Total definitions: {total_definitions}")
    logger.info(f"    Total properties: {total_properties}")
    
    # Save comprehensive results
    summary_path = Path(Config.OUTPUT_PATH) / "rules_as_code_summary.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary = {
        "processing_summary": {
            "timestamp": datetime.now().isoformat(),
            "documents_processed": successful,
            "total_documents": total_docs,
            "total_subjects_extracted": total_subjects,
            "total_definitions": total_definitions,
            "total_properties": total_properties,
            "system_version": "rules-as-code-v2.1-subject-based"
        },
        "results": results
    }
    
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    logger.info(f" Summary saved to: {summary_path}")
    
    if interface_enabled and server_thread:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        logger.info(f" Query interface available at http://{interface_host}:{interface_port}")
        logger.info("Press Ctrl+C to stop the system")
        
        try:
            # Keep the main thread alive
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info(" Shutting down Rules-as-Code system...")
    else:
        logger.info(" Rules-as-Code processing completed. Query interface not started.")

if __name__ == "__main__":
    asyncio.run(main())
