"""
Tagging Evaluation Agent - Agent for evaluating the quality of tagging results.

This module provides the AITaggingEvaluationAgent for evaluating the quality of
matches between data elements and business terms.
"""

import logging
import json
import re
from typing import List, Dict, Any, Optional, Tuple
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.core.models import TaggingResult
from app.config.settings import get_llm

logger = logging.getLogger(__name__)

class AITaggingEvaluationAgent:
    """
    Agent for evaluating the quality of tagging results using LLM reasoning.
    """
    
    def __init__(self):
        """Initialize the tagging evaluation agent."""
        self.llm = get_llm()
        self._setup_evaluation_chain()
        logger.info("AI tagging evaluation agent initialized")
    
    def _setup_evaluation_chain(self):
        """Set up the evaluation chain."""
        tagging_evaluation_template = """
        You are an expert in data governance and business terminology with deep domain knowledge in finance, 
        healthcare, and enterprise data standards. Your task is to evaluate the appropriateness of tagging 
        a data element with business terms.
        
        Data Element:
        - ID: {element_id}
        - Name: {element_name}
        - Description: {element_description}
        
        Matched Business Terms:
        {matched_terms}
        
        Instructions:
        1. Analyze the semantic match between the data element and each business term
        2. Consider conceptual alignment, completeness of coverage, and appropriate specificity
        3. Determine if ANY of the business terms are appropriate for this data element
        4. If none are appropriate, explain why new term modeling is needed
        
        Evaluation steps:
        1. CAREFULLY examine the data element name and description
        2. CAREFULLY examine each business term name and description
        3. For each term, assess if it accurately represents the data element's semantics
        4. Provide detailed reasoning about the semantic appropriateness of each match
        5. Give an overall assessment of whether any terms are good matches
        
        Provide your evaluation in this format:
        1. Overall valid match? (true/false): [true if ANY term is a good match, false otherwise]
        2. Overall confidence score (0.0-1.0): [provide a confidence score]
        3. Detailed justification for the evaluation:
           [provide your reasoning and analysis for each term]
        4. Final recommendation: [indicate whether modeling is needed or which term is best]
        """
        
        self.evaluation_prompt = PromptTemplate(
            input_variables=["element_id", "element_name", "element_description", "matched_terms"],
            template=tagging_evaluation_template
        )
        
        self.evaluation_chain = self.evaluation_prompt | self.llm | StrOutputParser()
    
    async def evaluate_tagging_result(self, tagging_result: TaggingResult) -> Tuple[bool, float, str, str]:
        """
        Evaluate the quality of a tagging result.
        
        Args:
            tagging_result: The tagging result to evaluate
            
        Returns:
            Tuple of (is_valid, confidence_score, reasoning, recommendation)
        """
        try:
            # Skip evaluation if modeling is required
            if tagging_result.modeling_required and not tagging_result.matching_terms:
                return False, 0.0, "Modeling is required as no suitable matches were found.", "Create a new business term"
            
            if not tagging_result.matching_terms:
                return False, 0.0, "No matching terms were found to evaluate.", "Create a new business term"
            
            # Format matched terms
            matched_terms_str = ""
            for i, term in enumerate(tagging_result.matching_terms):
                score = tagging_result.confidence_scores[i] if i < len(tagging_result.confidence_scores) else 0.0
                matched_terms_str += f"{i+1}. Term: {term.get('name', '')}\n"
                matched_terms_str += f"   Description: {term.get('description', '')}\n"
                matched_terms_str += f"   Similarity Score: {score:.2f}\n\n"
            
            # Invoke the evaluation chain
            result = await self.evaluation_chain.ainvoke({
                "element_id": tagging_result.element_id,
                "element_name": tagging_result.element_name,
                "element_description": tagging_result.element_description,
                "matched_terms": matched_terms_str
            })
            
            # Parse the result
            is_valid = self._extract_validity(result)
            confidence_score = self._extract_confidence(result)
            reasoning = self._extract_reasoning(result)
            recommendation = self._extract_recommendation(result)
            
            return is_valid, confidence_score, reasoning, recommendation
        
        except Exception as e:
            logger.error(f"Error in evaluate_tagging_result: {e}")
            return False, 0.0, f"Error during evaluation: {str(e)}", "Could not evaluate"
    
    def _extract_validity(self, result: str) -> bool:
        """Extract the validity from the evaluation result."""
        for line in result.split("\n"):
            if "valid match" in line.lower():
                return "true" in line.lower()
        return False
    
    def _extract_confidence(self, result: str) -> float:
        """Extract the confidence score from the evaluation result."""
        for line in result.split("\n"):
            if "confidence score" in line.lower():
                # Extract the confidence score
                match = re.search(r"(\d+\.\d+)", line)
                if match:
                    try:
                        return float(match.group(1))
                    except ValueError:
                        pass
        return 0.5  # Default confidence
    
    def _extract_reasoning(self, result: str) -> str:
        """Extract the reasoning from the evaluation result."""
        parts = result.split("3. Detailed justification")
        if len(parts) > 1:
            reasoning_part = parts[1].split("4. Final recommendation")[0]
            return reasoning_part.strip()
        return "No detailed justification provided."
    
    def _extract_recommendation(self, result: str) -> str:
        """Extract the recommendation from the evaluation result."""
        parts = result.split("4. Final recommendation")
        if len(parts) > 1:
            return parts[1].strip()
        return "No recommendation provided."
