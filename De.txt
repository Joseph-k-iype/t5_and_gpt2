def process_pdf(file_path):
    """Extract text from a PDF file, one document per page."""
    documents = []
    try:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    documents.append(text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing PDF file: {str(e)}")
        return []

def process_txt(file_path):
    """Read the entire text from a TXT file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        return [text.strip()]
    except Exception as e:
        logger.error(f"Error processing TXT file: {str(e)}")
        return []

def process_csv(file_path):
    """
    Read a CSV file, ask the user for the main column and supporting columns,
    then construct a document per row by combining those columns.
    """
    documents = []
    try:
        with open(file_path, "r", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            fieldnames = reader.fieldnames
            if not fieldnames:
                logger.error("CSV file has no header")
                return []
            print(f"CSV Columns: {fieldnames}")
            main_col = input("Enter the name of the main column: ").strip()
            support_cols = input("Enter supporting column names (comma-separated), or leave blank: ").split(",")
            support_cols = [col.strip() for col in support_cols if col.strip()]
            for row in reader:
                parts = []
                if main_col in row:
                    parts.append(row[main_col])
                for col in support_cols:
                    if col in row:
                        parts.append(row[col])
                doc_text = " ".join(parts)
                documents.append(doc_text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing CSV file: {str(e)}")
        return []

# --- ChromaDB Integration ---
# Instantiate a ChromaDB client with telemetry disabled.
chroma_settings = Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db",
    anonymized_telemetry=False  # Ensure no telemetry is sent
)
chroma_client = chromadb.Client(chroma_settings)
collection = chroma_client.get_or_create_collection("documents_collection")

def store_documents_in_vector_db(documents, source_filename=""):
    """
    Given a list of document texts, compute embeddings and add them to ChromaDB.
    """
    if not documents:
        print("No documents to store.")
        return
    embeddings = get_embeddings(documents, embeddings_endpoint, deployment_name="text-embedding-3-large")
    ids = [str(uuid.uuid4()) for _ in documents]
    metadatas = [{"source": source_filename, "doc_index": i} for i in range(len(documents))]
    collection.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)
    print(f"Stored {len(documents)} documents in the vector database.")

# --- Querying the Vector DB and Answering Questions ---
def answer_question(query: str) -> str:
    """
    Given a query, compute its embedding, retrieve the top documents from ChromaDB,
    then construct a prompt with the retrieved context and ask the LLM agent.
    """
    try:
        query_embedding = get_embeddings([query], embeddings_endpoint, deployment_name="text-embedding-3-large")[0]
        results = collection.query(query_embedding, n_results=5)
        context = "\n\n".join(results.get("documents", []))
        combined_prompt = f"Use the following context to answer the question:\n\nContext:\n{context}\n\nQuestion: {query}"
        answer = run_llm_agent(combined_prompt)
        return answer
    except Exception as e:
        logger.error(f"Error in answering question: {str(e)}")
        return "Error generating answer."

# --- Main Execution ---
if __name__ == "__main__":
    # Test embeddings connection.
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-azure-endpoint.openai.azure.com")
    if azure_endpoint.startswith("https:") and not azure_endpoint.startswith("https://"):
        azure_endpoint = azure_endpoint.replace("https:", "https://", 1)
    
    if test_connection(azure_endpoint):
        print("Embeddings connection successful")
    else:
        print("Embeddings connection failed")
    
    # File upload section.
    current_file = input("Enter the path of the file to upload (pdf, txt, csv) or press Enter to skip: ").strip()
    docs = []
    if current_file:
        if current_file.lower().endswith(".pdf"):
            docs = process_pdf(current_file)
        elif current_file.lower().endswith(".txt"):
            docs = process_txt(current_file)
        elif current_file.lower().endswith(".csv"):
            docs = process_csv(current_file)
        else:
            print("Unsupported file type.")
        if docs:
            store_documents_in_vector_db(docs, source_filename=os.path.basename(current_file))
    
    # Query loop.
    print("\nNow you can ask questions based on the stored data. Type 'exit' to quit.")
    while True:
        user_query = input("Your question: ").strip()
        if user_query.lower() in ["exit", "quit"]:
            break
        answer = answer_question(user_query)
        print("Answer:", answer)
