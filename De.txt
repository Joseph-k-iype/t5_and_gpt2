#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System
Features:
- ReAct agents with explicit reasoning and memory
- LangMem integration for long-term memory
- Rich ontology with comprehensive definitions
- Enhanced prompts ensuring complete coverage
- Web interface for querying the knowledge graph
- SPARQL endpoint for semantic queries

Author: Claude (Anthropic)
Date: July 2025
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import uuid

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import ssl

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    import threading
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logger.warning("Flask not available - web interface will be disabled")
    # Create dummy classes to prevent import errors
    class Flask: pass
    class Response: pass
    def jsonify(*args, **kwargs): return {}
    def render_template_string(*args, **kwargs): return ""
    def CORS(*args, **kwargs): pass
    import threading

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
from rdflib.plugins.stores.sparqlstore import SPARQLStore
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent, ToolNode
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langmem import create_manage_memory_tool, create_search_memory_tool, create_memory_manager

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# Tiktoken for token counting (offline)
import tiktoken
from tiktoken.load import load_tiktoken_bpe

# Message types for ReAct agents
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# TIKTOKEN OFFLINE MANAGER
# ====================================

class OfflineTiktokenManager:
    """Manage tiktoken encodings with offline model files"""
    
    def __init__(self, models_dir: str = "./tiktoken_models"):
        self.models_dir = Path(models_dir)
        self.encodings = {}
        self._load_offline_encodings()
    
    def _load_offline_encodings(self):
        """Load all available offline tiktoken encodings"""
        
        model_files = {
            "cl100k_base": "cl100k_base.tiktoken",
            "o200k_base": "o200k_base.tiktoken", 
            "p50k_base": "p50k_base.tiktoken",
            "r50k_base": "r50k_base.tiktoken"
        }
        
        for encoding_name, filename in model_files.items():
            file_path = self.models_dir / filename
            if file_path.exists():
                try:
                    # Load BPE ranks from file
                    bpe = load_tiktoken_bpe(str(file_path))
                    
                    # Create encoding based on known parameters
                    if encoding_name == "cl100k_base":
                        special_tokens = {
                            "<|endoftext|>": 100257,
                            "<|fim_prefix|>": 100258,
                            "<|fim_middle|>": 100259,
                            "<|fim_suffix|>": 100260,
                            "<|endofprompt|>": 100276
                        }
                        pat_str = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
                    elif encoding_name == "o200k_base":
                        special_tokens = {
                            "<|endoftext|>": 199999,
                            "<|endofprompt|>": 200018
                        }
                        pat_str = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
                    else:
                        special_tokens = {"<|endoftext|>": 50256}
                        pat_str = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
                    
                    encoding = tiktoken.Encoding(
                        name=encoding_name,
                        pat_str=pat_str,
                        mergeable_ranks=bpe,
                        special_tokens=special_tokens
                    )
                    
                    self.encodings[encoding_name] = encoding
                    logger.info(f"Loaded offline tiktoken encoding: {encoding_name}")
                    
                except Exception as e:
                    logger.error(f"Failed to load {encoding_name}: {e}")
            else:
                logger.warning(f"Tiktoken model file not found: {file_path}")
    
    def get_encoding(self, model_name: str = "o3-mini-2025-01-31") -> tiktoken.Encoding:
        """Get appropriate encoding for a model"""
        
        # Map models to encodings
        model_to_encoding = {
            "o3-mini-2025-01-31": "o200k_base",  # o3-mini likely uses o200k_base
            "gpt-4": "cl100k_base",
            "gpt-3.5-turbo": "cl100k_base",
            "text-embedding-3-large": "cl100k_base",
            "text-embedding-ada-002": "cl100k_base"
        }
        
        encoding_name = model_to_encoding.get(model_name, "cl100k_base")
        
        if encoding_name in self.encodings:
            return self.encodings[encoding_name]
        else:
            # Fallback to cl100k_base if available
            if "cl100k_base" in self.encodings:
                logger.warning(f"Using cl100k_base fallback for {model_name}")
                return self.encodings["cl100k_base"]
            else:
                raise ValueError(f"No suitable encoding available for {model_name}")
    
    def count_tokens(self, text: str, model_name: str = "o3-mini-2025-01-31") -> int:
        """Count tokens in text for given model"""
        encoding = self.get_encoding(model_name)
        return len(encoding.encode(text))
    
    def truncate_text(self, text: str, max_tokens: int, model_name: str = "o3-mini-2025-01-31") -> str:
        """Truncate text to fit within token limit"""
        encoding = self.get_encoding(model_name)
        tokens = encoding.encode(text)
        
        if len(tokens) <= max_tokens:
            return text
        
        # Truncate tokens and decode back to text
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens)
    
    def count_messages_tokens(self, messages: List[Dict], model_name: str = "o3-mini-2025-01-31") -> int:
        """Count tokens in a list of messages (OpenAI format)"""
        encoding = self.get_encoding(model_name)
        
        total_tokens = 0
        
        for message in messages:
            # Each message has some overhead tokens
            total_tokens += 4  # Basic message overhead
            
            for key, value in message.items():
                if isinstance(value, str):
                    total_tokens += len(encoding.encode(value))
                    if key == "role":
                        total_tokens += 1  # Role has extra token
        
        total_tokens += 2  # Conversation overhead
        return total_tokens

# ====================================
# TOKEN-AWARE PROMPT MANAGER
# ====================================

class TokenAwarePromptManager:
    """Manage prompts with token counting and truncation"""
    
    def __init__(self, tiktoken_manager: OfflineTiktokenManager):
        self.tiktoken_manager = tiktoken_manager
        self.model_limits = {
            "o3-mini-2025-01-31": 8192,
            "gpt-4": 8192,
            "gpt-3.5-turbo": 4096
        }
    
    def prepare_messages(self, system_prompt: str, user_prompt: str, model_name: str = "o3-mini-2025-01-31") -> List[Dict]:
        """Prepare messages with token limit enforcement"""
        
        max_tokens = self.model_limits.get(model_name, 8192)
        # Reserve tokens for response and overhead
        available_tokens = max_tokens - 1000  # Reserve 1000 tokens for response
        
        # Create initial messages
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # Count current tokens
        current_tokens = self.tiktoken_manager.count_messages_tokens(messages, model_name)
        
        if current_tokens <= available_tokens:
            logger.info(f"Messages fit within limit: {current_tokens}/{available_tokens} tokens")
            return messages
        
        logger.warning(f"Messages exceed limit: {current_tokens}/{available_tokens} tokens. Truncating...")
        
        # Calculate how much to truncate
        excess_tokens = current_tokens - available_tokens
        
        # Try to preserve system prompt, truncate user prompt
        system_tokens = self.tiktoken_manager.count_tokens(system_prompt, model_name)
        user_tokens = self.tiktoken_manager.count_tokens(user_prompt, model_name)
        
        if user_tokens > excess_tokens + 100:  # Keep some user content
            # Truncate user prompt
            target_user_tokens = user_tokens - excess_tokens - 100
            truncated_user_prompt = self.tiktoken_manager.truncate_text(
                user_prompt, target_user_tokens, model_name
            )
            messages[1]["content"] = truncated_user_prompt + "\n\n[Content truncated due to length]"
            
        else:
            # If user prompt is too small to truncate meaningfully, truncate both
            target_system_tokens = min(system_tokens, available_tokens // 3)
            target_user_tokens = available_tokens - target_system_tokens - 100
            
            truncated_system = self.tiktoken_manager.truncate_text(
                system_prompt, target_system_tokens, model_name
            )
            truncated_user = self.tiktoken_manager.truncate_text(
                user_prompt, target_user_tokens, model_name
            )
            
            messages = [
                {"role": "system", "content": truncated_system + "\n\n[System prompt truncated]"},
                {"role": "user", "content": truncated_user + "\n\n[Content truncated due to length]"}
            ]
        
        # Verify final token count
        final_tokens = self.tiktoken_manager.count_messages_tokens(messages, model_name)
        logger.info(f"Truncated messages: {final_tokens}/{available_tokens} tokens")
        
        return messages
    
    def smart_truncate_legal_text(self, text: str, max_tokens: int, model_name: str = "o3-mini-2025-01-31") -> str:
        """Smart truncation that preserves important legal content"""
        
        current_tokens = self.tiktoken_manager.count_tokens(text, model_name)
        
        if current_tokens <= max_tokens:
            return text
        
        # Try to preserve key legal sections
        lines = text.split('\n')
        important_lines = []
        regular_lines = []
        
        for line in lines:
            line_lower = line.lower().strip()
            # Identify important legal content
            if any(keyword in line_lower for keyword in [
                'article', 'section', 'paragraph', 'shall', 'must', 'obligation',
                'right', 'duty', 'liability', 'consent', 'processing', 'data',
                'personal', 'controller', 'processor', 'lawful', 'legitimate'
            ]):
                important_lines.append(line)
            else:
                regular_lines.append(line)
        
        # Start with important lines
        result_lines = important_lines[:]
        current_text = '\n'.join(result_lines)
        current_tokens = self.tiktoken_manager.count_tokens(current_text, model_name)
        
        # Add regular lines until we hit the limit
        for line in regular_lines:
            test_text = current_text + '\n' + line
            test_tokens = self.tiktoken_manager.count_tokens(test_text, model_name)
            
            if test_tokens <= max_tokens:
                current_text = test_text
                current_tokens = test_tokens
            else:
                break
        
        return current_text

# ====================================
# ELASTICSEARCH CLIENT
# ====================================

class ElasticsearchClient:
    """Enhanced Elasticsearch client with SSL and authentication"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize Elasticsearch client with proper configuration"""
        # SSL context
        ssl_context = ssl.create_default_context()
        if os.path.exists(Config.ELASTICSEARCH_CERT_PATH):
            ssl_context.load_verify_locations(Config.ELASTICSEARCH_CERT_PATH)
        else:
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            logger.warning("SSL certificate not found, using insecure connection")
        
        self.client = Elasticsearch(
            [Config.ELASTICSEARCH_URL],
            basic_auth=(Config.ELASTICSEARCH_USERNAME, Config.ELASTICSEARCH_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True if os.path.exists(Config.ELASTICSEARCH_CERT_PATH) else False
        )
        
        # Test connection
        if self.client.ping():
            logger.info("Successfully connected to Elasticsearch")
        else:
            raise ConnectionError("Failed to connect to Elasticsearch")
    
    def create_index(self):
        """Create the legal rules index with proper mappings"""
        mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "country": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "organization": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "rules": {"type": "nested"},
                    "definitions": {"type": "nested"},
                    "object_properties": {"type": "nested"},
                    "data_properties": {"type": "nested"},
                    "concept_schemes": {"type": "nested"},
                    "compliance_frameworks": {"type": "nested"},
                    "concepts": {"type": "keyword"},
                    "actors": {"type": "keyword"},
                    "objects": {"type": "keyword"},
                    "data_domains": {"type": "keyword"},
                    "extraction_stats": {"type": "object"},
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 3072,  # text-embedding-3-large dimensions
                        "index": True,
                        "similarity": "cosine"
                    },
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            },
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "index": {
                    "max_result_window": 50000
                }
            }
        }
        
        if not self.client.indices.exists(index=Config.ELASTICSEARCH_INDEX):
            self.client.indices.create(index=Config.ELASTICSEARCH_INDEX, body=mapping)
            logger.info(f"Created index: {Config.ELASTICSEARCH_INDEX}")
        else:
            logger.info(f"Index {Config.ELASTICSEARCH_INDEX} already exists")

# ====================================
# OPENAI CLIENT
# ====================================

class OpenAIClient:
    """Enhanced OpenAI client with token management for o3-mini"""
    
    def __init__(self):
        # Validate configuration first
        Config.validate_config()
        
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Initialize token management
        try:
            self.tiktoken_manager = OfflineTiktokenManager(Config.TIKTOKEN_MODELS_PATH)
            self.prompt_manager = TokenAwarePromptManager(self.tiktoken_manager)
            logger.info("Initialized token management with offline tiktoken models")
        except Exception as e:
            logger.error(f"Failed to initialize token management: {e}")
            # Create a dummy manager that doesn't do truncation
            self.tiktoken_manager = None
            self.prompt_manager = None
        
        # Test the client with a simple API call
        try:
            # Test with a minimal embedding request
            test_response = self.client.embeddings.create(
                input=["test"],
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            logger.info(f"OpenAI client initialized successfully with model {Config.OPENAI_EMBEDDING_MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConnectionError(f"OpenAI API connection failed: {e}")
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        logger.info(f"Generating embeddings for {len(texts)} text(s)...")
        
        try:
            # Check token limits for embeddings if tiktoken is available
            if self.tiktoken_manager:
                for i, text in enumerate(texts):
                    token_count = self.tiktoken_manager.count_tokens(text, Config.OPENAI_EMBEDDING_MODEL)
                    if token_count > 8000:  # Conservative limit for embeddings
                        logger.warning(f"Text {i} has {token_count} tokens, truncating...")
                        texts[i] = self.tiktoken_manager.truncate_text(text, 8000, Config.OPENAI_EMBEDDING_MODEL)
            
            response = self.client.embeddings.create(
                input=texts,
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Generate chat completion with token-aware o3-mini"""
        logger.info(f"Starting chat completion with model: {Config.OPENAI_MODEL}")
        
        try:
            # Use token-aware prompt management if available
            if self.prompt_manager and len(messages) >= 2:
                # Extract system and user messages
                system_msg = messages[0].get('content', '') if messages[0].get('role') == 'system' else ''
                user_msg = messages[-1].get('content', '') if messages[-1].get('role') == 'user' else ''
                
                # Get token-managed messages
                managed_messages = self.prompt_manager.prepare_messages(
                    system_msg, user_msg, Config.OPENAI_MODEL
                )
                
                # Add any intermediate messages that fit
                if len(messages) > 2:
                    current_tokens = self.tiktoken_manager.count_messages_tokens(managed_messages, Config.OPENAI_MODEL)
                    available_tokens = 7000 - current_tokens  # Leave room for response
                    
                    for msg in messages[1:-1]:  # Skip first and last
                        msg_tokens = self.tiktoken_manager.count_tokens(msg.get('content', ''), Config.OPENAI_MODEL)
                        if msg_tokens <= available_tokens:
                            managed_messages.insert(-1, msg)  # Insert before last message
                            available_tokens -= msg_tokens
                        else:
                            break
                
                final_messages = managed_messages
            else:
                final_messages = messages
            
            # Log token usage
            if self.tiktoken_manager:
                total_tokens = self.tiktoken_manager.count_messages_tokens(final_messages, Config.OPENAI_MODEL)
                logger.info(f"Sending {total_tokens} tokens to {Config.OPENAI_MODEL}")
            
            response = self.client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=final_messages,
                reasoning_effort=kwargs.get('reasoning_effort', Config.REASONING_EFFORT)
            )
            
            content = response.choices[0].message.content
            logger.info(f"Received response with {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            # Check if it's a token limit error
            if "maximum context length" in str(e).lower():
                logger.error("Token limit exceeded even after truncation. Consider reducing input further.")
            raise

# ====================================
# DOCUMENT PROCESSOR
# ====================================

class DocumentProcessor:
    """Process PDF documents with token-aware chunking"""
    
    def __init__(self, tiktoken_manager: Optional[OfflineTiktokenManager] = None):
        self.tiktoken_manager = tiktoken_manager
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = pymupdf.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text.strip()
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            raise
    
    def chunk_text_by_tokens(self, text: str, max_tokens: int = 3000, model_name: str = "o3-mini-2025-01-31") -> List[str]:
        """Split text into chunks based on token count rather than character count"""
        
        if not self.tiktoken_manager:
            # Fallback to character-based chunking
            logger.warning("No tiktoken manager available, using character-based chunking")
            return self.chunk_text(text, max_tokens * 4)  # Rough estimate: 4 chars per token
        
        # Split text into sentences first for better boundary detection
        sentences = self._split_into_sentences(text)
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self.tiktoken_manager.count_tokens(sentence, model_name)
            
            # If single sentence exceeds limit, split it further
            if sentence_tokens > max_tokens:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = []
                    current_tokens = 0
                
                # Split long sentence by words
                words = sentence.split()
                temp_sentence = []
                temp_tokens = 0
                
                for word in words:
                    word_tokens = self.tiktoken_manager.count_tokens(word, model_name)
                    if temp_tokens + word_tokens <= max_tokens:
                        temp_sentence.append(word)
                        temp_tokens += word_tokens
                    else:
                        if temp_sentence:
                            chunks.append(' '.join(temp_sentence))
                        temp_sentence = [word]
                        temp_tokens = word_tokens
                
                if temp_sentence:
                    chunks.append(' '.join(temp_sentence))
                
            elif current_tokens + sentence_tokens <= max_tokens:
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
            else:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        logger.info(f"Split text into {len(chunks)} token-aware chunks")
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences using simple rules"""
        import re
        
        # Simple sentence splitting - could be enhanced with more sophisticated rules
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def chunk_text(self, text: str, chunk_size: int = Config.CHUNK_SIZE) -> List[str]:
        """Split text into manageable chunks with overlap for better context (fallback method)"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        overlap_size = 200  # Words to overlap between chunks
        
        for word in words:
            if current_length + len(word) > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                # Create overlap for context preservation
                overlap_start = max(0, len(current_chunk) - overlap_size)
                current_chunk = current_chunk[overlap_start:] + [word]
                current_length = sum(len(w) + 1 for w in current_chunk)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1  # +1 for space
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        logger.info(f"Split text into {len(chunks)} character-based chunks")
        return chunks
    
    def smart_extract_for_analysis(self, text: str, max_analysis_tokens: int = 5000, model_name: str = "o3-mini-2025-01-31") -> str:
        """Extract and prioritize the most important content for analysis"""
        
        if not self.tiktoken_manager:
            # Simple truncation fallback
            return text[:max_analysis_tokens * 4]
        
        current_tokens = self.tiktoken_manager.count_tokens(text, model_name)
        
        if current_tokens <= max_analysis_tokens:
            return text
        
        logger.info(f"Text has {current_tokens} tokens, extracting most important content...")
        
        # Split into paragraphs
        paragraphs = text.split('\n\n')
        
        # Score paragraphs by importance
        scored_paragraphs = []
        for para in paragraphs:
            if not para.strip():
                continue
                
            score = self._score_legal_paragraph(para)
            tokens = self.tiktoken_manager.count_tokens(para, model_name)
            scored_paragraphs.append((score, tokens, para))
        
        # Sort by score (descending)
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # Select paragraphs until we hit the token limit
        selected_paragraphs = []
        total_tokens = 0
        
        for score, tokens, para in scored_paragraphs:
            if total_tokens + tokens <= max_analysis_tokens:
                selected_paragraphs.append(para)
                total_tokens += tokens
            else:
                break
        
        result = '\n\n'.join(selected_paragraphs)
        
        final_tokens = self.tiktoken_manager.count_tokens(result, model_name)
        logger.info(f"Extracted {len(selected_paragraphs)} paragraphs ({final_tokens} tokens) for analysis")
        
        return result
    
    def _score_legal_paragraph(self, paragraph: str) -> float:
        """Score a paragraph based on legal importance"""
        
        text_lower = paragraph.lower()
        score = 0.0
        
        # High-value legal keywords
        high_value_keywords = [
            'shall', 'must', 'required', 'obligation', 'prohibited', 'forbidden',
            'right', 'duty', 'liability', 'consent', 'lawful', 'unlawful',
            'processing', 'personal data', 'data subject', 'controller', 'processor',
            'article', 'section', 'paragraph', 'regulation', 'directive'
        ]
        
        # Medium-value keywords
        medium_value_keywords = [
            'may', 'should', 'could', 'appropriate', 'necessary', 'reasonable',
            'ensure', 'implement', 'maintain', 'provide', 'collect', 'use',
            'store', 'transfer', 'delete', 'access', 'security', 'protection'
        ]
        
        # Count keyword occurrences
        for keyword in high_value_keywords:
            score += text_lower.count(keyword) * 3.0
        
        for keyword in medium_value_keywords:
            score += text_lower.count(keyword) * 1.0
        
        # Bonus for containing numbers (often legal references)
        import re
        if re.search(r'\b\d+\b', paragraph):
            score += 1.0
        
        # Bonus for containing dates
        if re.search(r'\b\d{4}\b', paragraph):
            score += 0.5
        
        # Penalty for very short paragraphs
        if len(paragraph.split()) < 10:
            score *= 0.5
        
        return score

# ====================================
# SHACL VALIDATION
# ====================================

class SHACLValidator:
    """SHACL validation for ontologies"""
    
    def __init__(self):
        self.shapes_graph = self._create_validation_shapes()
    
    def _create_validation_shapes(self) -> Graph:
        """Create SHACL shapes for validation"""
        
        shapes = Graph()
        ns = EnhancedOntologyNamespaces()
        shapes = ns.bind_to_graph(shapes)
        
        # Add SHACL namespace
        SH = Namespace("http://www.w3.org/ns/shacl#")
        shapes.bind("sh", SH)
        
        # Rule shape
        rule_shape = URIRef(f"{ns.LEGAL}RuleShape")
        shapes.add((rule_shape, RDF.type, SH.NodeShape))
        shapes.add((rule_shape, SH.targetClass, URIRef(f"{ns.LEGAL}LegalRule")))
        
        # Required properties for rules
        subject_prop = BNode()
        shapes.add((rule_shape, SH.property, subject_prop))
        shapes.add((subject_prop, SH.path, URIRef(f"{ns.PROPERTIES}hasSubject")))
        shapes.add((subject_prop, SH.minCount, Literal(1)))
        shapes.add((subject_prop, SH.datatype, XSD.string))
        
        predicate_prop = BNode()
        shapes.add((rule_shape, SH.property, predicate_prop))
        shapes.add((predicate_prop, SH.path, URIRef(f"{ns.PROPERTIES}hasPredicate")))
        shapes.add((predicate_prop, SH.minCount, Literal(1)))
        shapes.add((predicate_prop, SH.datatype, XSD.string))
        
        # Legal Entity shape
        entity_shape = URIRef(f"{ns.LEGAL}LegalEntityShape")
        shapes.add((entity_shape, RDF.type, SH.NodeShape))
        shapes.add((entity_shape, SH.targetClass, URIRef(f"{ns.LEGAL}LegalEntity")))
        
        # Required label for entities
        label_prop = BNode()
        shapes.add((entity_shape, SH.property, label_prop))
        shapes.add((label_prop, SH.path, RDFS.label))
        shapes.add((label_prop, SH.minCount, Literal(1)))
        shapes.add((label_prop, SH.datatype, XSD.string))
        
        return shapes
    
    def validate_ontology(self, ontology_graph: Graph) -> Tuple[bool, Graph]:
        """Validate ontology against SHACL shapes"""
        
        try:
            conforms, results_graph, results_text = pyshacl.validate(
                data_graph=ontology_graph,
                shacl_graph=self.shapes_graph,
                inference='rdfs',
                serialize_report_graph=True
            )
            
            if not conforms:
                logger.warning(f"SHACL validation failed: {results_text}")
            else:
                logger.info("SHACL validation passed")
            
            return conforms, results_graph
            
        except Exception as e:
            logger.error(f"SHACL validation error: {e}")
            return False, Graph()

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            # Validate configuration structure
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file with supporting documents"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission",
                    "pdf_document": "./documents/gdpr.pdf",
                    "supporting_documents": [
                        "./documents/gdpr_guidance.pdf",
                        "./documents/gdpr_implementation.pdf"
                    ]
                },
                {
                    "country": "United States",
                    "jurisdiction": "Federal",
                    "organization": "Federal Trade Commission",
                    "pdf_document": "./documents/ccpa.pdf",
                    "supporting_documents": [
                        "./documents/ccpa_regulations.pdf"
                    ]
                }
            ],
            "processing_options": {
                "enable_query_interface": True,
                "interface_host": "localhost",
                "interface_port": 5000,
                "batch_size": 5,
                "max_concurrent": 3,
                "reasoning_effort": "high",
                "export_formats": ["ttl", "jsonld", "xml"]
            }
        }

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    NLP_MODEL_PATH = os.getenv("NLP_MODEL_PATH", "./nlp_model/en_core_web_sm-3.8.0.tar.gz")
    TIKTOKEN_MODELS_PATH = os.getenv("TIKTOKEN_MODELS_PATH", "./tiktoken_models")
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if not cls.ELASTICSEARCH_PASSWORD:
            missing_vars.append("ELASTICSEARCH_PASSWORD")
        
        # Check if tiktoken models directory exists
        if not os.path.exists(cls.TIKTOKEN_MODELS_PATH):
            logger.warning(f"Tiktoken models directory not found: {cls.TIKTOKEN_MODELS_PATH}")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )
    
    # Model parameters
    REASONING_EFFORT = "high"  # Use high reasoning for detailed extraction
    
    # Processing parameters
    BATCH_SIZE = 10
    MAX_CONCURRENT = 5
    CHUNK_SIZE = 2000

# ====================================
# ENHANCED ONTOLOGY NAMESPACES
# ====================================

class EnhancedOntologyNamespaces:
    """Enhanced ontology namespaces with detailed property definitions"""
    
    # Core vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal and jurisdictional
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Custom namespace for our legal rules
    LEGAL = Namespace("https://legal-rules.org/ontology#")
    
    # Data management domains
    STORAGE = Namespace("https://legal-rules.org/storage#")
    USAGE = Namespace("https://legal-rules.org/usage#")
    MOVEMENT = Namespace("https://legal-rules.org/movement#")
    PRIVACY = Namespace("https://legal-rules.org/privacy#")
    SECURITY = Namespace("https://legal-rules.org/security#")
    ACCESS = Namespace("https://legal-rules.org/access#")
    
    # Property namespaces
    PROPERTIES = Namespace("https://legal-rules.org/properties#")
    RELATIONS = Namespace("https://legal-rules.org/relations#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("legal", cls.LEGAL)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# REACT AGENTS WITH LANGMEM
# ====================================

class ReActAgent:
    """Base ReAct agent with explicit reasoning and action steps using LangMem"""
    
    def __init__(self, openai_client, name: str, memory_store: InMemoryStore):
        self.openai_client = openai_client
        self.name = name
        self.memory_store = memory_store
        self.reasoning_steps = []
        self.action_steps = []
        
        # Initialize LangMem components
        self.memory_manager = create_memory_manager(
            Config.OPENAI_MODEL,
            instructions=f"Store and manage long-term memory for {name} agent processing legal documents",
            enable_inserts=True,
            enable_updates=True,
            enable_deletes=False
        )
        
        # Create memory tools with namespace
        memory_namespace = (f"{name.lower()}_memories", "{user_id}")
        self.manage_memory_tool = create_manage_memory_tool(namespace=memory_namespace)
        self.search_memory_tool = create_search_memory_tool(namespace=memory_namespace)
        
        # Setup ReAct agent
        self._setup_react_agent()
    
    def _setup_react_agent(self):
        """Setup the ReAct agent with tools"""
        from langgraph.prebuilt import create_react_agent
        from langchain_openai import ChatOpenAI
        
        # Initialize the model
        llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Create tools list including memory tools
        tools = [self.manage_memory_tool, self.search_memory_tool]
        
        # Add agent-specific tools
        agent_tools = self._get_agent_specific_tools()
        if agent_tools:
            tools.extend(agent_tools)
        
        # Create ReAct agent with memory
        checkpointer = MemorySaver()
        self.react_agent = create_react_agent(
            llm,
            tools=tools,
            checkpointer=checkpointer,
            store=self.memory_store
        )
    
    def _get_agent_specific_tools(self):
        """Override in subclasses to add agent-specific tools"""
        return []
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": self.name
        })
        logger.info(f"[{self.name}] REASONING: {thought}")
    
    def log_action(self, action: str, result: Any = None):
        """Log an action step"""
        self.action_steps.append({
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "result": str(result)[:200] if result else None,
            "agent": self.name
        })
        logger.info(f"[{self.name}] ACTION: {action}")
    
    async def invoke_agent(self, messages: List[Dict], thread_id: str = None) -> Dict:
        """Invoke the ReAct agent with memory"""
        if not thread_id:
            thread_id = str(uuid.uuid4())
        
        config = {"configurable": {"thread_id": thread_id, "user_id": "legal_analyst"}}
        
        try:
            result = self.react_agent.invoke(
                {"messages": messages},
                config=config
            )
            return result
        except Exception as e:
            logger.error(f"ReAct agent invocation failed: {e}")
            raise

class EnhancedRuleExtractionAgent(ReActAgent):
    """Enhanced ReAct agent for rule extraction with detailed semantics and memory"""
    
    def __init__(self, openai_client, memory_store: InMemoryStore):
        super().__init__(openai_client, "RuleExtractor", memory_store)
    
    def _get_agent_specific_tools(self):
        """Add rule extraction specific tools"""
        
        @tool
        def extract_legal_definitions(text: str) -> str:
            """Extract legal definitions and terms from text"""
            # This would contain extraction logic
            return f"Extracted definitions from: {text[:100]}..."
        
        @tool 
        def identify_rule_components(text: str) -> str:
            """Identify subjects, predicates, objects in legal rules"""
            return f"Identified rule components in: {text[:100]}..."
        
        return [extract_legal_definitions, identify_rule_components]
    
    async def extract_rules_and_definitions(self, text: str, context: Dict) -> Dict:
        """Extract legal rules with comprehensive definitions and properties using ReAct pattern"""
        
        self.log_reasoning("Starting comprehensive rule extraction from legal document")
        self.log_reasoning(f"Context: {context['country']}/{context['jurisdiction']}")
        self.log_reasoning("Will extract rules, definitions, object properties, and data properties")
        
        # ENHANCED PROMPT - Ensures every subject has definition and comprehensive properties
        system_prompt = """You are a specialized legal semantic analysis expert using the ReAct (Reasoning and Acting) methodology. Your task is to extract comprehensive machine-readable rules from legal documents with rich semantic information.

CRITICAL REQUIREMENTS - MUST BE FOLLOWED:

1. EVERY SUBJECT MUST HAVE A COMPLETE DEFINITION - No subject can be extracted without a formal definition
2. COMPREHENSIVE PROPERTY COVERAGE - Must include both object properties AND data properties for every concept
3. MANDATORY VALIDATION - Ensure every extracted element has complete semantic information

EXTRACTION REQUIREMENTS:

1. RULES - Extract with detailed semantics (EVERY subject MUST have definition):
   - Type: obligation|permission|prohibition|condition|exception
   - Subject (who/what entity) - MUST include complete definition
   - Predicate (action/relationship) - MUST include complete definition  
   - Object (target/recipient) - MUST include complete definition
   - Modality (must|may|shall|shall not|should|could|etc.)
   - Conditions (when/where/if/unless clauses)
   - Temporal aspects (before/after/during/within timeframes)
   - Jurisdiction scope (local/national/international)
   - Data domain classification

2. DEFINITIONS - Extract comprehensive definitions (MANDATORY for every concept):
   - Concept name and type (class/individual/property)
   - Formal definition from the text (REQUIRED)
   - Informal description in plain language (REQUIRED)
   - Synonyms and alternative terms
   - Related concepts and hierarchical relationships
   - Domain and range for properties
   - Cardinality constraints
   - Legal source reference

3. OBJECT PROPERTIES - Relationships between entities (REQUIRED):
   - Property name and URI suffix
   - Domain (subject type) with definition
   - Range (object type) with definition
   - Characteristics (transitive/symmetric/functional/etc.)
   - Sub-property relationships
   - Inverse properties
   - Complete definition of the property relationship

4. DATA PROPERTIES - Attributes with literal values (REQUIRED):
   - Property name and URI suffix
   - Domain (subject type) with definition
   - Range (datatype: string/date/boolean/number/etc.)
   - Cardinality (min/max/exact)
   - Default values
   - Validation constraints
   - Complete definition of the data attribute

5. SEMANTIC RELATIONSHIPS:
   - Hierarchical (broader/narrower/subClassOf)
   - Associative (related/sameAs/differentFrom)
   - Dependency (requires/implies/conflicts)
   - Temporal (before/after/during)

REASONING METHODOLOGY:
Think step by step using ReAct pattern:
1. REASON about what legal concepts are present
2. ACT by extracting each concept with full definition
3. OBSERVE the extracted content for completeness
4. REASON about relationships between concepts
5. ACT by defining properties and relationships
6. OBSERVE for any missing definitions or properties

CRITICAL VALIDATION CHECKS:
- Every subject entity has a formal definition
- Every object entity has a formal definition
- Every predicate has a clear definition
- At least 3 object properties are defined per document section
- At least 3 data properties are defined per document section
- All property domains and ranges are defined
- No undefined references in the semantic network

CRITICAL: Respond ONLY with valid JSON. Include comprehensive semantic information.

Output format:
{
  "rules": [
    {
      "id": "unique_rule_id",
      "type": "obligation|permission|prohibition|condition|exception",
      "subject": {
        "entity": "entity name",
        "type": "DataController|DataProcessor|Individual|Organization|System",
        "scope": "specific|general|conditional",
        "definition": "REQUIRED: Complete formal definition of the subject entity",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "predicate": {
        "action": "action name",
        "category": "processing|transfer|storage|access|notification",
        "semantic_type": "activity|state|relationship",
        "definition": "REQUIRED: Complete formal definition of the predicate",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "object": {
        "entity": "target entity",
        "type": "PersonalData|NonPersonalData|System|Process|Document",
        "characteristics": ["sensitive|public|confidential|etc"],
        "definition": "REQUIRED: Complete formal definition of the object entity",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "modality": {
        "strength": "must|may|shall|should|could",
        "polarity": "positive|negative",
        "certainty": "certain|probable|possible"
      },
      "conditions": [
        {
          "type": "temporal|spatial|logical|circumstantial",
          "description": "condition description",
          "trigger": "event or state that activates the rule"
        }
      ],
      "temporal_aspects": {
        "start_condition": "when rule becomes active",
        "end_condition": "when rule expires",
        "duration": "how long rule applies",
        "frequency": "how often rule applies"
      },
      "jurisdiction_scope": {
        "geographic": "local|regional|national|international",
        "legal_system": "common_law|civil_law|religious_law|etc",
        "enforcement_authority": "regulatory body or court"
      },
      "data_domain": "storage|usage|movement|privacy|security|access|entitlements",
      "original_text": "exact text from document",
      "confidence": 0.0-1.0,
      "legal_basis": "constitutional|statutory|regulatory|case_law|contract"
    }
  ],
  "definitions": [
    {
      "concept": "concept name",
      "uri_suffix": "CamelCaseURISuffix",
      "type": "class|individual|object_property|data_property",
      "formal_definition": "REQUIRED: precise legal definition from text",
      "informal_description": "REQUIRED: plain language explanation",
      "synonyms": ["alternative term 1", "alternative term 2"],
      "broader_concepts": ["parent concept 1", "parent concept 2"],
      "narrower_concepts": ["child concept 1", "child concept 2"],
      "related_concepts": ["related concept 1", "related concept 2"],
      "domain_range": {
        "domain": "applicable entity types with definitions",
        "range": "applicable value types or target entities with definitions"
      },
      "characteristics": ["transitive", "symmetric", "functional", "etc"],
      "examples": ["example 1", "example 2"],
      "legal_source": "section or article reference",
      "cardinality_constraints": "min/max/exact constraints if applicable"
    }
  ],
  "object_properties": [
    {
      "property_name": "hasLegalBasis",
      "uri_suffix": "hasLegalBasis",
      "domain": ["LegalRule", "ProcessingActivity"],
      "domain_definitions": {
        "LegalRule": "REQUIRED: Complete definition of LegalRule domain",
        "ProcessingActivity": "REQUIRED: Complete definition of ProcessingActivity domain"
      },
      "range": ["LegalBasis", "LegalProvision"],
      "range_definitions": {
        "LegalBasis": "REQUIRED: Complete definition of LegalBasis range",
        "LegalProvision": "REQUIRED: Complete definition of LegalProvision range"
      },
      "characteristics": ["functional"],
      "sub_property_of": ["hasJustification"],
      "inverse_property": "isLegalBasisFor",
      "definition": "REQUIRED: Complete definition of this property relationship",
      "informal_description": "REQUIRED: Plain language explanation of the property",
      "cardinality": "exactly_one",
      "legal_source": "source section or article"
    }
  ],
  "data_properties": [
    {
      "property_name": "hasEffectiveDate",
      "uri_suffix": "hasEffectiveDate",
      "domain": ["LegalRule", "Regulation"],
      "domain_definitions": {
        "LegalRule": "REQUIRED: Complete definition of LegalRule domain",
        "Regulation": "REQUIRED: Complete definition of Regulation domain"
      },
      "range": "xsd:date",
      "range_definition": "REQUIRED: Definition of what this date represents",
      "cardinality": "max_one",
      "definition": "REQUIRED: Complete definition of this data property",
      "informal_description": "REQUIRED: Plain language explanation",
      "validation_constraints": ["must be a valid date", "cannot be in the past for new rules"],
      "default_value": "none",
      "legal_source": "source section or article"
    }
  ],
  "semantic_relationships": [
    {
      "source": "concept1",
      "source_definition": "REQUIRED: Complete definition of source concept",
      "relationship_type": "subClassOf|broader|requires|implies|conflicts",
      "target": "concept2", 
      "target_definition": "REQUIRED: Complete definition of target concept",
      "strength": "strong|moderate|weak",
      "bidirectional": true|false,
      "explanation": "REQUIRED: Explanation of this semantic relationship"
    }
  ],
  "validation_summary": {
    "all_subjects_defined": true,
    "all_objects_defined": true,
    "all_predicates_defined": true,
    "object_properties_count": "minimum 3",
    "data_properties_count": "minimum 3",
    "undefined_references": []
  }
}"""

        user_prompt = f"""
ANALYZE THE FOLLOWING LEGAL TEXT using ReAct methodology:

REASONING STEP 1: What are the main legal concepts in this text?
ACTION STEP 1: Identify all subjects, predicates, and objects
OBSERVATION STEP 1: List what I found

REASONING STEP 2: Which concepts need definitions?
ACTION STEP 2: Extract formal definitions for every concept
OBSERVATION STEP 2: Ensure no concept lacks definition

REASONING STEP 3: What relationships exist between concepts?
ACTION STEP 3: Define object properties and data properties
OBSERVATION STEP 3: Verify comprehensive property coverage

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}
- Legal System: {context.get('legal_system', 'Unknown')}

LEGAL TEXT:
{text}

CRITICAL VALIDATION REQUIREMENTS:
✓ Every subject entity MUST have a complete formal definition
✓ Every object entity MUST have a complete formal definition  
✓ Every predicate MUST have a clear definition
✓ Minimum 3 object properties with full definitions
✓ Minimum 3 data properties with full definitions
✓ All property domains and ranges must be defined
✓ No undefined references allowed

Extract all rules, definitions, object properties, data properties, and semantic relationships according to the specified JSON format. Focus on creating a comprehensive semantic network suitable for automated reasoning and compliance checking.

RESPOND WITH COMPLETE JSON ONLY - NO ADDITIONAL TEXT.
"""

        # Use ReAct agent for extraction
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        self.log_action("Invoking ReAct agent for comprehensive extraction")
        
        try:
            result = await self.invoke_agent(messages, f"extraction_{uuid.uuid4()}")
            
            # Extract the JSON from the agent response
            response_content = result['messages'][-1].content if result.get('messages') else ""
            
            self.log_action("Received response from ReAct agent, parsing JSON")
            
            # Parse JSON response
            response_text = response_content.strip()
            
            # Handle markdown-wrapped JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            parsed_result = json.loads(response_text)
            
            # Validate the result has required elements
            validation_issues = self._validate_extraction_result(parsed_result)
            if validation_issues:
                self.log_reasoning(f"Validation issues found: {validation_issues}")
                # Store issues in memory for future improvement
                asyncio.create_task(self._store_validation_issues(validation_issues))
            
            self.log_action(f"Successfully parsed {len(parsed_result.get('rules', []))} rules, {len(parsed_result.get('definitions', []))} definitions")
            
            # Store successful extraction pattern in memory
            asyncio.create_task(self._store_extraction_success(parsed_result, context))
            
            return parsed_result
            
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Attempting to clean and retry.")
            # Store failure pattern in memory
            asyncio.create_task(self._store_extraction_failure(str(e), context))
            
            # Return basic structure
            return self._get_empty_extraction_result()
        except Exception as e:
            self.log_action(f"ReAct agent extraction failed: {e}")
            asyncio.create_task(self._store_extraction_failure(str(e), context))
            return self._get_empty_extraction_result()
    
    def _validate_extraction_result(self, result: Dict) -> List[str]:
        """Validate that extraction result meets requirements"""
        issues = []
        
        # Check that every rule has defined subjects/objects/predicates
        for i, rule in enumerate(result.get('rules', [])):
            if not rule.get('subject', {}).get('definition'):
                issues.append(f"Rule {i}: Subject lacks definition")
            if not rule.get('object', {}).get('definition'):
                issues.append(f"Rule {i}: Object lacks definition")
            if not rule.get('predicate', {}).get('definition'):
                issues.append(f"Rule {i}: Predicate lacks definition")
        
        # Check minimum property counts
        if len(result.get('object_properties', [])) < 3:
            issues.append(f"Only {len(result.get('object_properties', []))} object properties found, minimum 3 required")
        
        if len(result.get('data_properties', [])) < 3:
            issues.append(f"Only {len(result.get('data_properties', []))} data properties found, minimum 3 required")
        
        # Check property definitions
        for i, prop in enumerate(result.get('object_properties', [])):
            if not prop.get('definition'):
                issues.append(f"Object property {i}: Lacks definition")
            if not prop.get('domain_definitions'):
                issues.append(f"Object property {i}: Lacks domain definitions")
            if not prop.get('range_definitions'):
                issues.append(f"Object property {i}: Lacks range definitions")
        
        for i, prop in enumerate(result.get('data_properties', [])):
            if not prop.get('definition'):
                issues.append(f"Data property {i}: Lacks definition")
            if not prop.get('domain_definitions'):
                issues.append(f"Data property {i}: Lacks domain definitions")
        
        return issues
    
    async def _store_validation_issues(self, issues: List[str]):
        """Store validation issues in memory for learning"""
        memory_content = {
            "type": "validation_issues",
            "timestamp": datetime.now().isoformat(),
            "issues": issues,
            "agent": self.name
        }
        
        # Store using memory manager (stateless)
        try:
            conversation = [
                {"role": "system", "content": f"Store validation issues for {self.name} agent"},
                {"role": "user", "content": f"Remember these validation issues: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store validation issues in memory: {e}")
    
    async def _store_extraction_success(self, result: Dict, context: Dict):
        """Store successful extraction patterns in memory"""
        memory_content = {
            "type": "extraction_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "rules_count": len(result.get('rules', [])),
            "definitions_count": len(result.get('definitions', [])),
            "object_properties_count": len(result.get('object_properties', [])),
            "data_properties_count": len(result.get('data_properties', [])),
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store successful extraction pattern for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful extraction: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store extraction success in memory: {e}")
    
    async def _store_extraction_failure(self, error: str, context: Dict):
        """Store extraction failures in memory for learning"""
        memory_content = {
            "type": "extraction_failure", 
            "timestamp": datetime.now().isoformat(),
            "error": error,
            "context": context,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store extraction failure for {self.name} agent"},
                {"role": "user", "content": f"Remember this failure to learn from: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store extraction failure in memory: {e}")
    
    def _get_empty_extraction_result(self) -> Dict:
        """Return empty but valid extraction result structure"""
        return {
            "rules": [],
            "definitions": [],
            "object_properties": [],
            "data_properties": [],
            "semantic_relationships": [],
            "validation_summary": {
                "all_subjects_defined": False,
                "all_objects_defined": False,
                "all_predicates_defined": False,
                "object_properties_count": 0,
                "data_properties_count": 0,
                "undefined_references": ["parsing_failed"]
            }
        }

class EnhancedConceptAnalysisAgent(ReActAgent):
    """Enhanced ReAct agent for concept analysis with taxonomic reasoning and memory"""
    
    def __init__(self, openai_client, memory_store: InMemoryStore):
        super().__init__(openai_client, "ConceptAnalyzer", memory_store)
    
    def _get_agent_specific_tools(self):
        """Add concept analysis specific tools"""
        
        @tool
        def build_taxonomy_hierarchy(concepts: str) -> str:
            """Build hierarchical taxonomy from concepts"""
            return f"Built taxonomy for concepts: {concepts[:100]}..."
        
        @tool
        def identify_concept_relationships(concepts: str) -> str:
            """Identify relationships between legal concepts"""
            return f"Identified relationships in: {concepts[:100]}..."
        
        return [build_taxonomy_hierarchy, identify_concept_relationships]
    
    async def analyze_concepts_and_taxonomy(self, extraction_result: Dict, context: Dict) -> Dict:
        """Analyze extracted concepts and build comprehensive taxonomies using ReAct pattern"""
        
        self.log_reasoning("Starting comprehensive concept analysis and taxonomy building")
        self.log_reasoning(f"Processing {len(extraction_result.get('rules', []))} rules and {len(extraction_result.get('definitions', []))} definitions")
        
        # ENHANCED PROMPT for concept analysis
        system_prompt = """You are a legal ontology expert specializing in creating comprehensive taxonomic structures for legal knowledge representation using ReAct methodology.

REASONING APPROACH:
1. REASON about the extracted concepts and their inherent relationships
2. ACT by creating hierarchical structures and concept schemes
3. OBSERVE the created structures for completeness and accuracy
4. REASON about compliance frameworks and cross-references
5. ACT by building logical axioms and constraints
6. OBSERVE final structure for comprehensive coverage

Your task is to analyze extracted legal rules and definitions to create:

1. HIERARCHICAL TAXONOMIES using SKOS relationships (EVERY concept must be placed in hierarchy)
2. CONCEPT SCHEMES for different legal domains (minimum 5 schemes required)
3. COMPLIANCE FRAMEWORKS linking rules to requirements (minimum 3 frameworks)
4. CROSS-REFERENCES between related concepts (extensive linking required)
5. AXIOMS and LOGICAL CONSTRAINTS (comprehensive rule coverage)

Build using established vocabularies:
- Data Privacy Vocabulary (DPV) concepts and relationships
- PROV-O provenance and activity concepts
- SKOS hierarchical and associative relationships
- ELI (European Legislation Identifier) for legal document structure

Focus on creating interconnected concept networks for:
- Legal Entities (controllers, processors, subjects, authorities)
- Data Categories (personal, sensitive, public, confidential)
- Processing Activities (collection, analysis, sharing, storage)
- Legal Bases (consent, contract, legal obligation, vital interests)
- Rights and Obligations (access, rectification, erasure, portability)
- Technical and Organizational Measures (encryption, access controls, audit)

CRITICAL REQUIREMENTS:
✓ Every concept from extraction MUST appear in at least one taxonomy
✓ Minimum 5 comprehensive concept schemes
✓ Minimum 3 compliance frameworks with detailed requirements
✓ Extensive cross-referencing between concepts
✓ Complete logical axioms for rule validation
✓ No orphaned concepts allowed

CRITICAL: Respond ONLY with valid JSON. Create comprehensive taxonomic structures.

Output format:
{
  "concept_schemes": [
    {
      "scheme_id": "legal_entities_scheme",
      "title": "Legal Entities in Data Protection",
      "description": "Classification of legal entities and their roles",
      "top_concepts": ["LegalEntity"],
      "concepts": [
        {
          "concept_id": "DataController",
          "pref_label": "Data Controller",
          "alt_labels": ["Controller", "Data Responsible"],
          "definition": "REQUIRED: Complete definition from extraction or legal sources",
          "broader": ["LegalEntity"],
          "narrower": ["PublicDataController", "PrivateDataController"],
          "related": ["DataProcessor", "DataSubject"],
          "dpv_mapping": "dpv:DataController",
          "examples": ["Company collecting customer data", "Government agency processing citizen data"],
          "legal_obligations": ["Must have lawful basis", "Must ensure data security"],
          "compliance_requirements": ["GDPR Article 5", "GDPR Article 24"]
        }
      ]
    }
  ],
  "taxonomic_hierarchies": {
    "data_categories": {
      "root": "Data",
      "hierarchy": {
        "PersonalData": {
          "definition": "Information relating to an identified or identifiable natural person",
          "legal_source": "GDPR Article 4(1)",
          "children": {
            "SensitivePersonalData": {
              "definition": "Personal data revealing sensitive attributes",
              "legal_source": "GDPR Article 9",
              "children": {
                "BiometricData": {
                  "definition": "Data from biometric measurements",
                  "examples": ["Fingerprints", "Facial recognition data"],
                  "special_protections": ["Explicit consent required", "Limited processing purposes"]
                },
                "HealthData": {
                  "definition": "Data concerning health status",
                  "examples": ["Medical records", "Fitness tracker data"],
                  "special_protections": ["Medical professional privilege", "Strict consent requirements"]
                }
              }
            },
            "ContactData": {
              "definition": "Data for contacting individuals",
              "children": {
                "EmailAddress": {"definition": "Electronic mail address"},
                "PhoneNumber": {"definition": "Telephone contact number"}
              }
            }
          }
        }
      }
    }
  },
  "compliance_frameworks": [
    {
      "framework_id": "gdpr_compliance",
      "title": "GDPR Compliance Framework",
      "description": "Requirements and obligations under GDPR",
      "legal_source": "Regulation (EU) 2016/679",
      "scope": "Personal data processing in EU",
      "requirements": [
        {
          "requirement_id": "lawful_basis_requirement",
          "title": "Lawful Basis for Processing",
          "description": "Processing must have a lawful basis under Article 6",
          "legal_reference": "GDPR Article 6",
          "applicable_to": ["DataController"],
          "mandatory": true,
          "related_rules": ["rule_id_1", "rule_id_2"],
          "verification_criteria": ["Legal basis documented", "Basis communicated to data subjects"],
          "penalties": "Up to 4% of annual turnover or €20 million",
          "implementation_guidance": ["Document legal basis", "Review basis regularly", "Communicate to data subjects"]
        }
      ],
      "enforcement_authority": "Data Protection Authorities",
      "territorial_scope": "European Union and EEA"
    }
  ],
  "cross_references": [
    {
      "source_concept": "DataController",
      "target_concept": "LawfulBasis",
      "relationship": "must_have",
      "description": "Every data controller must have a lawful basis for processing",
      "legal_source": "GDPR Article 6",
      "cardinality": "one_to_many",
      "examples": ["Controller processes based on consent", "Controller processes for contract performance"]
    }
  ],
  "logical_axioms": [
    {
      "axiom_type": "universal_restriction",
      "subject": "ProcessingActivity",
      "property": "hasLawfulBasis",
      "constraint": "exactly_one",
      "description": "Every processing activity must have exactly one primary lawful basis",
      "formal_logic": "∀x (ProcessingActivity(x) → ∃!y (LawfulBasis(y) ∧ hasLawfulBasis(x,y)))",
      "enforcement_rule": "System must validate lawful basis before allowing processing"
    }
  ],
  "semantic_mappings": {
    "dpv_alignments": [
      {
        "local_concept": "DataController",
        "dpv_concept": "dpv:DataController",
        "alignment_type": "exact_match",
        "confidence": 1.0
      }
    ],
    "prov_alignments": [
      {
        "local_concept": "ProcessingActivity", 
        "prov_concept": "prov:Activity",
        "alignment_type": "sub_class",
        "confidence": 0.9
      }
    ]
  },
  "data_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"],
  "quality_metrics": {
    "total_concepts": 150,
    "max_hierarchy_depth": 5,
    "cross_references_count": 45,
    "axioms_count": 23,
    "concept_schemes_count": 5,
    "compliance_frameworks_count": 3,
    "orphaned_concepts": []
  }
}"""

        user_prompt = f"""
ANALYZE EXTRACTION RESULTS using ReAct methodology:

REASONING STEP 1: What concepts were extracted and how should they be organized?
ACTION STEP 1: Create hierarchical taxonomies for all concepts
OBSERVATION STEP 1: Verify every concept is placed in a hierarchy

REASONING STEP 2: What concept schemes are needed for this legal domain?
ACTION STEP 2: Build comprehensive concept schemes (minimum 5)
OBSERVATION STEP 2: Ensure complete domain coverage

REASONING STEP 3: What compliance frameworks apply to these concepts?
ACTION STEP 3: Define compliance frameworks with detailed requirements
OBSERVATION STEP 3: Validate framework completeness

REASONING STEP 4: How do concepts relate to each other?
ACTION STEP 4: Create extensive cross-references and relationships
OBSERVATION STEP 4: Check for comprehensive interconnection

REASONING STEP 5: What logical constraints govern these concepts?
ACTION STEP 5: Build logical axioms and enforcement rules
OBSERVATION STEP 5: Ensure logical consistency

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}

EXTRACTION RESULTS:
{json.dumps(extraction_result, indent=2)}

CRITICAL REQUIREMENTS:
✓ Every extracted concept must appear in taxonomies
✓ Minimum 5 comprehensive concept schemes required
✓ Minimum 3 detailed compliance frameworks required
✓ Extensive cross-referencing between all concepts
✓ Complete logical axioms for rule validation
✓ No orphaned or undefined concepts allowed

Create hierarchical taxonomies, concept schemes, compliance frameworks, and logical constraints that represent the legal knowledge in a machine-readable format suitable for automated reasoning and compliance checking.

RESPOND WITH COMPLETE JSON ONLY - NO ADDITIONAL TEXT.
"""

        # Use ReAct agent for analysis
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        self.log_action("Invoking ReAct agent for taxonomic analysis")
        
        try:
            result = await self.invoke_agent(messages, f"analysis_{uuid.uuid4()}")
            
            # Extract the JSON from the agent response
            response_content = result['messages'][-1].content if result.get('messages') else ""
            
            self.log_action("Processing taxonomic analysis response")
            
            response_text = response_content.strip()
            
            # Handle markdown-wrapped JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            parsed_result = json.loads(response_text)
            
            # Validate the analysis result
            validation_issues = self._validate_analysis_result(parsed_result, extraction_result)
            if validation_issues:
                self.log_reasoning(f"Analysis validation issues: {validation_issues}")
                asyncio.create_task(self._store_analysis_issues(validation_issues))
            
            self.log_action(f"Successfully created {len(parsed_result.get('concept_schemes', []))} concept schemes")
            
            # Store successful analysis in memory
            asyncio.create_task(self._store_analysis_success(parsed_result, context))
            
            return parsed_result
            
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Returning basic structure.")
            asyncio.create_task(self._store_analysis_failure(str(e), context))
            return self._get_empty_analysis_result()
        except Exception as e:
            self.log_action(f"ReAct agent analysis failed: {e}")
            asyncio.create_task(self._store_analysis_failure(str(e), context))
            return self._get_empty_analysis_result()
    
    def _validate_analysis_result(self, result: Dict, extraction_result: Dict) -> List[str]:
        """Validate that analysis result meets requirements"""
        issues = []
        
        # Check minimum counts
        if len(result.get('concept_schemes', [])) < 5:
            issues.append(f"Only {len(result.get('concept_schemes', []))} concept schemes, minimum 5 required")
        
        if len(result.get('compliance_frameworks', [])) < 3:
            issues.append(f"Only {len(result.get('compliance_frameworks', []))} compliance frameworks, minimum 3 required")
        
        # Check that extracted concepts are covered
        extracted_concepts = set()
        for definition in extraction_result.get('definitions', []):
            extracted_concepts.add(definition.get('concept', ''))
        
        covered_concepts = set()
        for scheme in result.get('concept_schemes', []):
            for concept in scheme.get('concepts', []):
                covered_concepts.add(concept.get('concept_id', ''))
        
        uncovered = extracted_concepts - covered_concepts
        if uncovered:
            issues.append(f"Concepts not covered in taxonomies: {list(uncovered)}")
        
        return issues
    
    async def _store_analysis_issues(self, issues: List[str]):
        """Store analysis validation issues in memory"""
        memory_content = {
            "type": "analysis_issues",
            "timestamp": datetime.now().isoformat(),
            "issues": issues,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store analysis issues for {self.name} agent"},
                {"role": "user", "content": f"Remember these analysis issues: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis issues in memory: {e}")
    
    async def _store_analysis_success(self, result: Dict, context: Dict):
        """Store successful analysis patterns in memory"""
        memory_content = {
            "type": "analysis_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "concept_schemes_count": len(result.get('concept_schemes', [])),
            "compliance_frameworks_count": len(result.get('compliance_frameworks', [])),
            "cross_references_count": len(result.get('cross_references', [])),
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store successful analysis pattern for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful analysis: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis success in memory: {e}")
    
    async def _store_analysis_failure(self, error: str, context: Dict):
        """Store analysis failures in memory"""
        memory_content = {
            "type": "analysis_failure",
            "timestamp": datetime.now().isoformat(),
            "error": error,
            "context": context,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store analysis failure for {self.name} agent"},
                {"role": "user", "content": f"Remember this failure to learn from: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis failure in memory: {e}")
    
    def _get_empty_analysis_result(self) -> Dict:
        """Return empty but valid analysis result structure"""
        return {
            "concept_schemes": [],
            "taxonomic_hierarchies": {},
            "compliance_frameworks": [],
            "cross_references": [],
            "logical_axioms": [],
            "semantic_mappings": {"dpv_alignments": [], "prov_alignments": []},
            "data_domains": ["storage", "usage", "movement", "privacy", "security", "access"],
            "quality_metrics": {
                "total_concepts": 0,
                "max_hierarchy_depth": 0,
                "cross_references_count": 0,
                "axioms_count": 0,
                "concept_schemes_count": 0,
                "compliance_frameworks_count": 0,
                "orphaned_concepts": ["analysis_failed"]
            }
        }

class EnhancedOntologyBuilderAgent(ReActAgent):
    """Enhanced ReAct agent for building comprehensive OWL ontologies with memory"""
    
    def __init__(self, memory_store: InMemoryStore):
        super().__init__(None, "OntologyBuilder", memory_store)
        self.ns = EnhancedOntologyNamespaces()
    
    def build_comprehensive_ontology(self, extraction_result: Dict, taxonomy_result: Dict, context: Dict) -> Graph:
        """Build a comprehensive OWL ontology with detailed properties and constraints using ReAct pattern"""
        
        self.log_reasoning("Starting comprehensive ontology construction using ReAct methodology")
        self.log_reasoning("REASONING: Will create classes, properties, individuals, and constraints")
        
        g = Graph()
        g = self.ns.bind_to_graph(g)
        
        # Safely encode context
        safe_country = self._safe_uri_encode(context['country'])
        safe_jurisdiction = self._safe_uri_encode(context['jurisdiction'])
        safe_organization = self._safe_uri_encode(context['organization'])
        
        self.log_action("ACTION: Creating ontology metadata and structure")
        
        # Create comprehensive ontology metadata
        ontology_uri = URIRef(f"{self.ns.LEGAL}ontology_{safe_country}_{datetime.now().strftime('%Y%m%d')}")
        g.add((ontology_uri, RDF.type, OWL.Ontology))
        g.add((ontology_uri, DCTERMS.title, Literal(f"Comprehensive Legal Rules Ontology for {context['country']}")))
        g.add((ontology_uri, DCTERMS.created, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        g.add((ontology_uri, DCTERMS.description, Literal(f"Machine-readable legal knowledge base extracted from {context['country']} legislation with comprehensive semantic properties")))
        g.add((ontology_uri, DCTERMS.creator, Literal("Legal Document Analysis System")))
        g.add((ontology_uri, OWL.versionInfo, Literal("1.0")))
        
        self.log_reasoning("REASONING: Now processing extracted content to build ontology structure")
        
        # Add core classes and properties
        self.log_action("ACTION: Adding core ontology structure")
        self._add_core_ontology_structure(g, context)
        
        # Add extracted definitions as classes and properties
        self.log_action("ACTION: Adding definitions as ontology elements")
        self._add_definitions_as_ontology_elements(g, extraction_result.get('definitions', []))
        
        # Add object properties with detailed constraints
        self.log_action("ACTION: Adding object properties with constraints")
        self._add_object_properties(g, extraction_result.get('object_properties', []))
        
        # Add data properties with constraints
        self.log_action("ACTION: Adding data properties with constraints")
        self._add_data_properties(g, extraction_result.get('data_properties', []))
        
        # Add taxonomic hierarchies
        self.log_action("ACTION: Adding taxonomic hierarchies")
        self._add_taxonomic_hierarchies(g, taxonomy_result.get('taxonomic_hierarchies', {}))
        
        # Add concept schemes
        self.log_action("ACTION: Adding concept schemes")
        self._add_concept_schemes(g, taxonomy_result.get('concept_schemes', []))
        
        # Add rules as individuals with detailed properties
        self.log_action("ACTION: Adding rules as individuals")
        self._add_rules_as_individuals(g, extraction_result.get('rules', []), context)
        
        # Add logical axioms and constraints
        self.log_action("ACTION: Adding logical axioms and constraints")
        self._add_logical_axioms(g, taxonomy_result.get('logical_axioms', []))
        
        # Add compliance frameworks
        self.log_action("ACTION: Adding compliance frameworks")
        self._add_compliance_frameworks(g, taxonomy_result.get('compliance_frameworks', []))
        
        # Add cross-references
        self.log_action("ACTION: Adding cross-references")
        self._add_cross_references(g, taxonomy_result.get('cross_references', []))
        
        # Add vocabulary mappings (DPV, PROV-O)
        self.log_action("ACTION: Adding vocabulary mappings")
        self._add_vocabulary_mappings(g)
        
        self.log_reasoning("OBSERVATION: Ontology construction complete, validating structure")
        self.log_action(f"Ontology construction complete with {len(g)} triples")
        
        # Store ontology building patterns in memory
        asyncio.create_task(self._store_ontology_success(len(g), context))
        
        return g
    
    def _safe_uri_encode(self, text: str) -> str:
        """Safely encode text for use in URIs"""
        import urllib.parse
        safe_text = text.replace(' ', '_').replace('/', '_').replace('\\', '_')
        return urllib.parse.quote(safe_text, safe='')
    
    def _add_core_ontology_structure(self, g: Graph, context: Dict):
        """Add core ontology classes and structure"""
        
        self.log_action("Adding core ontology structure")
        
        # Core top-level classes
        core_classes = [
            ("LegalEntity", "An entity with legal standing or recognition"),
            ("LegalRule", "A rule, regulation, or legal provision"),
            ("ProcessingActivity", "An activity involving data processing"),
            ("DataCategory", "A classification of data"),
            ("LegalBasis", "Legal justification for an action"),
            ("Right", "A legal right or entitlement"),
            ("Obligation", "A legal duty or obligation"),
            ("TechnicalMeasure", "Technical safeguard or control"),
            ("OrganizationalMeasure", "Organizational safeguard or procedure"),
            ("JurisdictionEntity", "Entity representing legal jurisdiction"),
            ("ComplianceRequirement", "Requirement for legal compliance")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.LEGAL}{class_name}")
            g.add((class_uri, RDF.type, OWL.Class))
            g.add((class_uri, RDF.type, SKOS.Concept))
            g.add((class_uri, RDFS.label, Literal(class_name)))
            g.add((class_uri, RDFS.comment, Literal(definition)))
            g.add((class_uri, SKOS.definition, Literal(definition)))
        
        # Country/Jurisdiction/Organization as individuals
        safe_country = self._safe_uri_encode(context['country'])
        safe_jurisdiction = self._safe_uri_encode(context['jurisdiction'])
        safe_organization = self._safe_uri_encode(context['organization'])
        
        country_uri = URIRef(f"{self.ns.LEGAL}Country_{safe_country}")
        jurisdiction_uri = URIRef(f"{self.ns.LEGAL}Jurisdiction_{safe_jurisdiction}")
        org_uri = URIRef(f"{self.ns.LEGAL}Organization_{safe_organization}")
        
        g.add((country_uri, RDF.type, URIRef(f"{self.ns.LEGAL}JurisdictionEntity")))
        g.add((country_uri, RDFS.label, Literal(context['country'])))
        g.add((country_uri, SKOS.prefLabel, Literal(context['country'])))
        
        g.add((jurisdiction_uri, RDF.type, URIRef(f"{self.ns.LEGAL}JurisdictionEntity")))
        g.add((jurisdiction_uri, RDFS.label, Literal(context['jurisdiction'])))
        g.add((jurisdiction_uri, URIRef(f"{self.ns.LEGAL}isPartOf"), country_uri))
        
        g.add((org_uri, RDF.type, URIRef(f"{self.ns.LEGAL}LegalEntity")))
        g.add((org_uri, RDFS.label, Literal(context['organization'])))
    
    def _add_definitions_as_ontology_elements(self, g: Graph, definitions: List[Dict]):
        """Add extracted definitions as ontology classes, properties, or individuals"""
        
        self.log_action(f"Adding {len(definitions)} definitions to ontology")
        
        for definition in definitions:
            concept_name = definition.get('concept', '')
            uri_suffix = definition.get('uri_suffix', self._safe_uri_encode(concept_name))
            concept_type = definition.get('type', 'class')
            
            if concept_type == 'class':
                concept_uri = URIRef(f"{self.ns.LEGAL}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.Class))
                g.add((concept_uri, RDF.type, SKOS.Concept))
            elif concept_type == 'object_property':
                concept_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.ObjectProperty))
            elif concept_type == 'data_property':
                concept_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.DatatypeProperty))
            else:  # individual
                concept_uri = URIRef(f"{self.ns.LEGAL}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.NamedIndividual))
            
            # Add labels and definitions
            g.add((concept_uri, RDFS.label, Literal(concept_name)))
            if definition.get('formal_definition'):
                g.add((concept_uri, SKOS.definition, Literal(definition['formal_definition'])))
            if definition.get('informal_description'):
                g.add((concept_uri, RDFS.comment, Literal(definition['informal_description'])))
            
            # Add alternative labels
            for alt_label in definition.get('synonyms', []):
                g.add((concept_uri, SKOS.altLabel, Literal(alt_label)))
            
            # Add hierarchical relationships
            for broader in definition.get('broader_concepts', []):
                broader_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(broader)}")
                if concept_type == 'class':
                    g.add((concept_uri, RDFS.subClassOf, broader_uri))
                g.add((concept_uri, SKOS.broader, broader_uri))
            
            # Add examples
            for example in definition.get('examples', []):
                g.add((concept_uri, SKOS.example, Literal(example)))
    
    def _add_object_properties(self, g: Graph, object_properties: List[Dict]):
        """Add object properties with detailed constraints"""
        
        self.log_action(f"Adding {len(object_properties)} object properties")
        
        for prop in object_properties:
            prop_name = prop.get('property_name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            g.add((prop_uri, RDF.type, OWL.ObjectProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain and range
            for domain_class in prop.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(domain_class)}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            for range_class in prop.get('range', []):
                range_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(range_class)}")
                g.add((prop_uri, RDFS.range, range_uri))
            
            # Add characteristics
            characteristics = prop.get('characteristics', [])
            if 'functional' in characteristics:
                g.add((prop_uri, RDF.type, OWL.FunctionalProperty))
            if 'transitive' in characteristics:
                g.add((prop_uri, RDF.type, OWL.TransitiveProperty))
            if 'symmetric' in characteristics:
                g.add((prop_uri, RDF.type, OWL.SymmetricProperty))
            
            # Add sub-property relationships
            for super_prop in prop.get('sub_property_of', []):
                super_prop_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(super_prop)}")
                g.add((prop_uri, RDFS.subPropertyOf, super_prop_uri))
            
            # Add inverse property
            if prop.get('inverse_property'):
                inverse_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(prop['inverse_property'])}")
                g.add((prop_uri, OWL.inverseOf, inverse_uri))
    
    def _add_data_properties(self, g: Graph, data_properties: List[Dict]):
        """Add data properties with constraints"""
        
        self.log_action(f"Adding {len(data_properties)} data properties")
        
        for prop in data_properties:
            prop_name = prop.get('property_name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            g.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain
            for domain_class in prop.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(domain_class)}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            # Add range (datatype)
            range_type = prop.get('range', 'xsd:string')
            if range_type.startswith('xsd:'):
                range_uri = getattr(XSD, range_type.split(':')[1])
                g.add((prop_uri, RDFS.range, range_uri))
    
    def _add_taxonomic_hierarchies(self, g: Graph, hierarchies: Dict):
        """Add taxonomic hierarchies using SKOS"""
        
        self.log_action(f"Adding {len(hierarchies)} taxonomic hierarchies")
        
        for hierarchy_name, hierarchy_data in hierarchies.items():
            root_concept = hierarchy_data.get('root', hierarchy_name)
            self._add_hierarchy_recursive(g, hierarchy_data.get('hierarchy', {}), None)
    
    def _add_hierarchy_recursive(self, g: Graph, hierarchy: Dict, parent_uri: Optional[URIRef]):
        """Recursively add hierarchical concepts"""
        
        for concept_name, concept_data in hierarchy.items():
            safe_name = self._safe_uri_encode(concept_name)
            concept_uri = URIRef(f"{self.ns.LEGAL}{safe_name}")
            
            g.add((concept_uri, RDF.type, OWL.Class))
            g.add((concept_uri, RDF.type, SKOS.Concept))
            g.add((concept_uri, RDFS.label, Literal(concept_name)))
            
            if isinstance(concept_data, dict):
                if concept_data.get('definition'):
                    g.add((concept_uri, SKOS.definition, Literal(concept_data['definition'])))
                
                if parent_uri:
                    g.add((concept_uri, RDFS.subClassOf, parent_uri))
                    g.add((concept_uri, SKOS.broader, parent_uri))
                
                # Recursively add children
                children = concept_data.get('children', {})
                if children:
                    self._add_hierarchy_recursive(g, children, concept_uri)
    
    def _add_concept_schemes(self, g: Graph, concept_schemes: List[Dict]):
        """Add SKOS concept schemes"""
        
        self.log_action(f"Adding {len(concept_schemes)} concept schemes")
        
        for scheme in concept_schemes:
            scheme_id = scheme.get('scheme_id', '')
            scheme_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(scheme_id)}")
            
            g.add((scheme_uri, RDF.type, SKOS.ConceptScheme))
            g.add((scheme_uri, RDFS.label, Literal(scheme.get('title', scheme_id))))
            g.add((scheme_uri, RDFS.comment, Literal(scheme.get('description', ''))))
            
            # Add top concepts
            for top_concept in scheme.get('top_concepts', []):
                top_concept_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(top_concept)}")
                g.add((scheme_uri, SKOS.hasTopConcept, top_concept_uri))
    
    def _add_rules_as_individuals(self, g: Graph, rules: List[Dict], context: Dict):
        """Add rules as named individuals with detailed properties"""
        
        self.log_action(f"Adding {len(rules)} rules as individuals")
        
        for rule in rules:
            rule_id = rule.get('id', str(uuid.uuid4()))
            safe_rule_id = self._safe_uri_encode(rule_id)
            rule_uri = URIRef(f"{self.ns.LEGAL}Rule_{safe_rule_id}")
            
            # Basic rule properties
            g.add((rule_uri, RDF.type, OWL.NamedIndividual))
            g.add((rule_uri, RDF.type, URIRef(f"{self.ns.LEGAL}LegalRule")))
            
            if rule.get('type'):
                rule_type_uri = URIRef(f"{self.ns.LEGAL}{rule['type'].title()}Rule")
                g.add((rule_uri, RDF.type, rule_type_uri))
            
            # Add detailed rule components
            subject_info = rule.get('subject', {})
            if isinstance(subject_info, dict):
                if subject_info.get('entity'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasSubjectEntity"), Literal(subject_info['entity'])))
                if subject_info.get('type'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasSubjectType"), Literal(subject_info['type'])))
            
            # Add temporal aspects
            temporal_aspects = rule.get('temporal_aspects', {})
            if temporal_aspects:
                if temporal_aspects.get('start_condition'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasStartCondition"), Literal(temporal_aspects['start_condition'])))
                if temporal_aspects.get('duration'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasDuration"), Literal(temporal_aspects['duration'])))
            
            # Add jurisdiction scope
            jurisdiction_scope = rule.get('jurisdiction_scope', {})
            if jurisdiction_scope:
                if jurisdiction_scope.get('geographic'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasGeographicScope"), Literal(jurisdiction_scope['geographic'])))
                if jurisdiction_scope.get('enforcement_authority'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasEnforcementAuthority"), Literal(jurisdiction_scope['enforcement_authority'])))
            
            # Add provenance
            g.add((rule_uri, self.ns.PROV.wasDerivedFrom, Literal(rule.get('original_text', ''))))
            g.add((rule_uri, self.ns.PROV.wasGeneratedBy, URIRef(f"{self.ns.LEGAL}ExtractionProcess")))
            g.add((rule_uri, self.ns.PROV.generatedAtTime, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_logical_axioms(self, g: Graph, axioms: List[Dict]):
        """Add logical axioms and constraints"""
        
        self.log_action(f"Adding {len(axioms)} logical axioms")
        
        for axiom in axioms:
            axiom_type = axiom.get('axiom_type', '')
            subject_class = axiom.get('subject', '')
            property_name = axiom.get('property', '')
            constraint = axiom.get('constraint', '')
            
            if axiom_type == 'universal_restriction' and constraint == 'exactly_one':
                subject_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(subject_class)}")
                property_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(property_name)}")
                
                # Create cardinality restriction
                restriction = BNode()
                g.add((restriction, RDF.type, OWL.Restriction))
                g.add((restriction, OWL.onProperty, property_uri))
                g.add((restriction, OWL.cardinality, Literal(1)))
                g.add((subject_uri, RDFS.subClassOf, restriction))
    
    def _add_compliance_frameworks(self, g: Graph, frameworks: List[Dict]):
        """Add compliance frameworks"""
        
        self.log_action(f"Adding {len(frameworks)} compliance frameworks")
        
        for framework in frameworks:
            framework_id = framework.get('framework_id', '')
            framework_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(framework_id)}")
            
            g.add((framework_uri, RDF.type, URIRef(f"{self.ns.LEGAL}ComplianceFramework")))
            g.add((framework_uri, RDFS.label, Literal(framework.get('title', framework_id))))
            g.add((framework_uri, RDFS.comment, Literal(framework.get('description', ''))))
    
    def _add_cross_references(self, g: Graph, cross_refs: List[Dict]):
        """Add cross-references between concepts"""
        
        self.log_action(f"Adding {len(cross_refs)} cross-references")
        
        for ref in cross_refs:
            source_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(ref.get('source_concept', ''))}")
            target_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(ref.get('target_concept', ''))}")
            relationship = ref.get('relationship', 'related')
            
            if relationship == 'must_have':
                property_uri = URIRef(f"{self.ns.RELATIONS}mustHave")
            else:
                property_uri = SKOS.related
            
            g.add((source_uri, property_uri, target_uri))
    
    def _add_vocabulary_mappings(self, g: Graph):
        """Add mappings to external vocabularies (DPV, PROV-O)"""
        
        self.log_action("Adding vocabulary mappings to DPV and PROV-O")
        
        # Add imports
        g.add((URIRef(f"{self.ns.LEGAL}"), OWL.imports, URIRef("https://w3id.org/dpv")))
        g.add((URIRef(f"{self.ns.LEGAL}"), OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        
        # Map core concepts to DPV
        dpv_mappings = {
            "PersonalData": "dpv:PersonalData",
            "DataController": "dpv:DataController",
            "DataProcessor": "dpv:DataProcessor",
            "DataSubject": "dpv:DataSubject",
            "ProcessingActivity": "dpv:Processing",
            "LegalBasis": "dpv:LegalBasis",
            "Consent": "dpv:Consent"
        }
        
        for local_concept, dpv_concept in dpv_mappings.items():
            local_uri = URIRef(f"{self.ns.LEGAL}{local_concept}")
            dpv_uri = URIRef(f"{self.ns.DPV}{dpv_concept.split(':')[1]}")
            g.add((local_uri, SKOS.exactMatch, dpv_uri))
    
    async def _store_ontology_success(self, triple_count: int, context: Dict):
        """Store ontology building success in memory"""
        memory_content = {
            "type": "ontology_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "triple_count": triple_count,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store ontology building success for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful ontology construction: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store ontology success in memory: {e}")

# ====================================
# WEB INTERFACE FOR QUERYING
# ====================================

class LegalKnowledgeQueryInterface:
    """Web interface for querying the legal knowledge graph"""
    
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.knowledge_graph = Graph()
        
        if FLASK_AVAILABLE:
            self.app = Flask(__name__)
            CORS(self.app)
            self._setup_routes()
        else:
            self.app = None
            logger.warning("Flask not available - query interface disabled")
    
    def _setup_routes(self):
        """Setup Flask routes for the web interface"""
        
        @self.app.route('/')
        def index():
            return render_template_string(self._get_index_template())
        
        @self.app.route('/api/sparql', methods=['POST'])
        def sparql_query():
            try:
                query = request.json.get('query', '')
                results = self.execute_sparql_query(query)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/search', methods=['POST'])
        def semantic_search():
            try:
                query_text = request.json.get('query', '')
                country = request.json.get('country', '')
                results = self.execute_semantic_search(query_text, country)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/ontology/stats')
        def ontology_stats():
            try:
                stats = self.get_ontology_statistics()
                return jsonify(stats)
            except Exception as e:
                return jsonify({"error": str(e)})
        
        @self.app.route('/api/concept/<concept_id>')
        def get_concept_details(concept_id):
            try:
                details = self.get_concept_details(concept_id)
                return jsonify(details)
            except Exception as e:
                return jsonify({"error": str(e)})
    
    def execute_sparql_query(self, query: str) -> List[Dict]:
        """Execute SPARQL query against the knowledge graph"""
        try:
            results = self.knowledge_graph.query(query)
            result_list = []
            
            for row in results:
                row_dict = {}
                for var in results.vars:
                    value = row[var]
                    if value:
                        row_dict[str(var)] = str(value)
                result_list.append(row_dict)
            
            return result_list
        except Exception as e:
            raise Exception(f"SPARQL query failed: {e}")
    
    def execute_semantic_search(self, query_text: str, country: str = "") -> List[Dict]:
        """Execute semantic search using Elasticsearch"""
        try:
            # Build Elasticsearch query
            es_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "multi_match": {
                                    "query": query_text,
                                    "fields": ["content", "concepts", "actors", "objects"]
                                }
                            }
                        ]
                    }
                },
                "size": 20
            }
            
            if country:
                es_query["query"]["bool"]["must"].append({
                    "term": {"country.keyword": country}
                })
            
            # Execute search
            response = self.orchestrator.es_client.client.search(
                index=Config.ELASTICSEARCH_INDEX,
                body=es_query
            )
            
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                results.append({
                    "document_id": source.get('document_id'),
                    "country": source.get('country'),
                    "jurisdiction": source.get('jurisdiction'),
                    "organization": source.get('organization'),
                    "rules_count": len(source.get('rules', [])),
                    "concepts": source.get('concepts', [])[:10],  # Limit for display
                    "score": hit['_score']
                })
            
            return results
        except Exception as e:
            raise Exception(f"Semantic search failed: {e}")
    
    def get_ontology_statistics(self) -> Dict:
        """Get statistics about the ontology"""
        try:
            stats = {
                "total_triples": len(self.knowledge_graph),
                "total_classes": len(list(self.knowledge_graph.subjects(RDF.type, OWL.Class))),
                "total_properties": len(list(self.knowledge_graph.subjects(RDF.type, OWL.ObjectProperty))) + 
                                  len(list(self.knowledge_graph.subjects(RDF.type, OWL.DatatypeProperty))),
                "total_individuals": len(list(self.knowledge_graph.subjects(RDF.type, OWL.NamedIndividual))),
                "namespaces": [str(ns) for prefix, ns in self.knowledge_graph.namespaces()]
            }
            return stats
        except Exception as e:
            return {"error": str(e)}
    
    def get_concept_details(self, concept_id: str) -> Dict:
        """Get detailed information about a specific concept"""
        try:
            # Safely encode concept ID
            safe_concept_id = concept_id.replace(' ', '_').replace('/', '_')
            concept_uri = URIRef(f"https://legal-rules.org/ontology#{safe_concept_id}")
            
            details = {
                "concept_id": concept_id,
                "labels": [],
                "definitions": [],
                "broader_concepts": [],
                "narrower_concepts": [],
                "related_concepts": [],
                "properties": []
            }
            
            # Get labels
            for label in self.knowledge_graph.objects(concept_uri, RDFS.label):
                details["labels"].append(str(label))
            
            # Get definitions
            for definition in self.knowledge_graph.objects(concept_uri, SKOS.definition):
                details["definitions"].append(str(definition))
            
            # Get broader concepts
            for broader in self.knowledge_graph.objects(concept_uri, SKOS.broader):
                details["broader_concepts"].append(str(broader))
            
            # Get narrower concepts
            for narrower in self.knowledge_graph.subjects(SKOS.broader, concept_uri):
                details["narrower_concepts"].append(str(narrower))
            
            return details
        except Exception as e:
            return {"error": str(e)}
    
    def load_ontology(self, ontology_graph: Graph):
        """Load an ontology graph for querying"""
        self.knowledge_graph = ontology_graph
        logger.info(f"Loaded ontology with {len(ontology_graph)} triples")
    
    def _get_index_template(self) -> str:
        """Return HTML template for the web interface"""
        return """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Legal Knowledge Graph Query Interface</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 30px; color: #333; }
        .query-section { margin-bottom: 30px; }
        .query-area { width: 100%; height: 150px; font-family: monospace; border: 1px solid #ddd; border-radius: 4px; padding: 10px; }
        .search-input { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 4px; margin-bottom: 10px; }
        .button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; margin-right: 10px; }
        .button:hover { background: #0056b3; }
        .results { border: 1px solid #ddd; border-radius: 4px; padding: 15px; margin-top: 20px; max-height: 400px; overflow-y: auto; }
        .result-item { border-bottom: 1px solid #eee; padding: 10px 0; }
        .error { color: #dc3545; background: #f8d7da; padding: 10px; border-radius: 4px; margin: 10px 0; }
        .success { color: #155724; background: #d4edda; padding: 10px; border-radius: 4px; margin: 10px 0; }
        .tab { display: inline-block; padding: 10px 20px; background: #e9ecef; margin-right: 5px; cursor: pointer; border-radius: 4px 4px 0 0; }
        .tab.active { background: #007bff; color: white; }
        .tab-content { display: none; }
        .tab-content.active { display: block; }
        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
        .stat-card { background: #f8f9fa; padding: 15px; border-radius: 4px; text-align: center; }
        .stat-number { font-size: 24px; font-weight: bold; color: #007bff; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Enhanced Legal Knowledge Graph Query Interface</h1>
            <p>Query and explore machine-readable legal rules and concepts with ReAct agents and LangMem memory</p>
        </div>
        
        <div class="tabs">
            <div class="tab active" onclick="showTab('sparql')">SPARQL Query</div>
            <div class="tab" onclick="showTab('search')">Semantic Search</div>
            <div class="tab" onclick="showTab('stats')">Ontology Statistics</div>
        </div>
        
        <div id="sparql" class="tab-content active">
            <div class="query-section">
                <h3>SPARQL Query</h3>
                <textarea id="sparqlQuery" class="query-area" placeholder="Enter your SPARQL query here...
Example:
PREFIX legal: <https://legal-rules.org/ontology#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>

SELECT ?concept ?label ?definition WHERE {
  ?concept a legal:LegalRule ;
           rdfs:label ?label ;
           skos:definition ?definition .
} LIMIT 10"></textarea>
                <button class="button" onclick="executeSparqlQuery()">Execute Query</button>
                <button class="button" onclick="clearResults()">Clear Results</button>
            </div>
        </div>
        
        <div id="search" class="tab-content">
            <div class="query-section">
                <h3>Semantic Search</h3>
                <input type="text" id="searchQuery" class="search-input" placeholder="Enter search terms (e.g., 'data protection', 'consent', 'GDPR')">
                <input type="text" id="countryFilter" class="search-input" placeholder="Filter by country (optional)">
                <button class="button" onclick="executeSemanticSearch()">Search</button>
                <button class="button" onclick="clearResults()">Clear Results</button>
            </div>
        </div>
        
        <div id="stats" class="tab-content">
            <div class="query-section">
                <h3>Ontology Statistics</h3>
                <button class="button" onclick="loadStatistics()">Load Statistics</button>
                <div id="statsDisplay"></div>
            </div>
        </div>
        
        <div id="results" class="results" style="display: none;">
            <h3>Results</h3>
            <div id="resultContent"></div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            document.querySelectorAll('.tab-content').forEach(tab => tab.classList.remove('active'));
            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
        
        async function executeSparqlQuery() {
            const query = document.getElementById('sparqlQuery').value;
            if (!query.trim()) {
                showError('Please enter a SPARQL query');
                return;
            }
            
            try {
                const response = await fetch('/api/sparql', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'sparql');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function executeSemanticSearch() {
            const query = document.getElementById('searchQuery').value;
            const country = document.getElementById('countryFilter').value;
            
            if (!query.trim()) {
                showError('Please enter search terms');
                return;
            }
            
            try {
                const response = await fetch('/api/search', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query, country })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'search');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function loadStatistics() {
            try {
                const response = await fetch('/api/ontology/stats');
                const data = await response.json();
                
                if (data.error) {
                    showError(data.error);
                } else {
                    displayStatistics(data);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        function displayResults(results, type) {
            const resultsDiv = document.getElementById('results');
            const contentDiv = document.getElementById('resultContent');
            
            if (results.length === 0) {
                contentDiv.innerHTML = '<p>No results found.</p>';
            } else if (type === 'sparql') {
                let html = '<table border="1" style="width: 100%; border-collapse: collapse;"><thead><tr>';
                
                // Get column headers from first result
                if (results.length > 0) {
                    Object.keys(results[0]).forEach(key => {
                        html += `<th style="padding: 8px; background: #f8f9fa;">${key}</th>`;
                    });
                    html += '</tr></thead><tbody>';
                    
                    results.forEach(row => {
                        html += '<tr>';
                        Object.values(row).forEach(value => {
                            html += `<td style="padding: 8px; border: 1px solid #ddd;">${value}</td>`;
                        });
                        html += '</tr>';
                    });
                    html += '</tbody></table>';
                }
                
                contentDiv.innerHTML = html;
            } else if (type === 'search') {
                let html = '<div>';
                results.forEach(result => {
                    html += `
                        <div class="result-item">
                            <h4>${result.country} - ${result.jurisdiction}</h4>
                            <p><strong>Organization:</strong> ${result.organization}</p>
                            <p><strong>Rules:</strong> ${result.rules_count}</p>
                            <p><strong>Key Concepts:</strong> ${result.concepts.join(', ')}</p>
                            <p><strong>Relevance Score:</strong> ${result.score.toFixed(2)}</p>
                        </div>
                    `;
                });
                html += '</div>';
                contentDiv.innerHTML = html;
            }
            
            resultsDiv.style.display = 'block';
        }
        
        function displayStatistics(stats) {
            const statsDiv = document.getElementById('statsDisplay');
            const html = `
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_triples || 0}</div>
                        <div>Total Triples</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_classes || 0}</div>
                        <div>Classes</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_properties || 0}</div>
                        <div>Properties</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_individuals || 0}</div>
                        <div>Individuals</div>
                    </div>
                </div>
                <h4>Namespaces:</h4>
                <ul>
                    ${(stats.namespaces || []).map(ns => `<li>${ns}</li>`).join('')}
                </ul>
            `;
            statsDiv.innerHTML = html;
        }
        
        function showError(message) {
            const resultContent = document.getElementById('resultContent');
            resultContent.innerHTML = `<div class="error">Error: ${message}</div>`;
            document.getElementById('results').style.display = 'block';
        }
        
        function clearResults() {
            document.getElementById('results').style.display = 'none';
            document.getElementById('resultContent').innerHTML = '';
        }
    </script>
</body>
</html>
        """
    
    def start_server(self, host='localhost', port=5000):
        """Start the web server"""
        if not FLASK_AVAILABLE:
            logger.error("Flask not available - cannot start web server")
            return
            
        if not self.app:
            logger.error("Flask app not initialized - cannot start web server")
            return
            
        logger.info(f"Starting query interface server on {host}:{port}")
        self.app.run(host=host, port=port, debug=False)

# ====================================
# ENHANCED LEGAL ANALYSIS STATE
# ====================================

class EnhancedLegalAnalysisState(TypedDict):
    """Enhanced state for legal analysis workflow"""
    messages: Annotated[list, add_messages]
    document_content: str
    country: str
    jurisdiction: str
    organization: str
    extraction_result: Dict
    taxonomy_result: Dict
    ontology_graph: Optional[Graph]
    validation_info: Optional[Dict]
    reasoning_steps: List[Dict]
    action_steps: List[Dict]
    analysis_complete: bool
    memory_namespace: str

# ====================================
# ENHANCED MULTI-AGENT ORCHESTRATOR
# ====================================

class EnhancedLegalAnalysisOrchestrator:
    """Enhanced orchestrator with ReAct agents, LangMem, and query interface"""
    
    def __init__(self):
        # Initialize memory store for long-term memory across sessions
        self.memory_store = InMemoryStore(
            index={
                "dims": 3072,  # text-embedding-3-large dimensions
                "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
            }
        )
        
        # Initialize clients and agents
        self.openai_client = self._initialize_openai_client()
        self.es_client = self._initialize_elasticsearch_client()
        
        # Initialize document processor with tiktoken manager
        tiktoken_manager = getattr(self.openai_client, 'tiktoken_manager', None)
        self.doc_processor = DocumentProcessor(tiktoken_manager)
        
        # Initialize enhanced ReAct agents with memory
        self.rule_agent = EnhancedRuleExtractionAgent(self.openai_client, self.memory_store)
        self.concept_agent = EnhancedConceptAnalysisAgent(self.openai_client, self.memory_store)
        self.ontology_builder = EnhancedOntologyBuilderAgent(self.memory_store)
        self.shacl_validator = SHACLValidator()
        
        # Initialize query interface
        self.query_interface = LegalKnowledgeQueryInterface(self)
        
        # Build the enhanced workflow
        self.workflow = self._build_enhanced_workflow()
    
    def _initialize_openai_client(self):
        """Initialize OpenAI client with validation"""
        try:
            Config.validate_config()
            return OpenAIClient()
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _initialize_elasticsearch_client(self):
        """Initialize Elasticsearch client"""
        try:
            client = ElasticsearchClient()
            client.create_index()
            return client
        except Exception as e:
            logger.error(f"Failed to initialize Elasticsearch client: {e}")
            raise
    
    def _build_enhanced_workflow(self) -> StateGraph:
        """Build enhanced LangGraph workflow with ReAct agents and memory"""
        
        workflow = StateGraph(EnhancedLegalAnalysisState)
        
        # Add enhanced processing nodes
        workflow.add_node("document_processing", self._enhanced_document_processing_node)
        workflow.add_node("comprehensive_extraction", self._comprehensive_extraction_node)
        workflow.add_node("taxonomic_analysis", self._taxonomic_analysis_node)
        workflow.add_node("ontology_construction", self._ontology_construction_node)
        workflow.add_node("ontology_validation", self._ontology_validation_node)
        workflow.add_node("knowledge_storage", self._knowledge_storage_node)
        workflow.add_node("interface_setup", self._interface_setup_node)
        
        # Add edges
        workflow.add_edge(START, "document_processing")
        workflow.add_edge("document_processing", "comprehensive_extraction")
        workflow.add_edge("comprehensive_extraction", "taxonomic_analysis")
        workflow.add_edge("taxonomic_analysis", "ontology_construction")
        workflow.add_edge("ontology_construction", "ontology_validation")
        workflow.add_edge("ontology_validation", "knowledge_storage")
        workflow.add_edge("knowledge_storage", "interface_setup")
        workflow.add_edge("interface_setup", END)
        
        return workflow.compile(checkpointer=MemorySaver(), store=self.memory_store)
    
    def _enhanced_document_processing_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Enhanced document processing with better chunking and memory"""
        logger.info("Enhanced document processing with memory...")
        
        if not state.get("document_content"):
            logger.error("No document content provided")
            return state
        
        # Set up memory namespace for this processing session
        state["memory_namespace"] = f"session_{uuid.uuid4()}"
        
        # Enhanced chunking with overlap for better context preservation
        chunks = self.doc_processor.chunk_text(state["document_content"], chunk_size=3000)
        # Use more chunks for comprehensive analysis
        state["document_content"] = " ".join(chunks[:15])
        
        return state
    
    def _comprehensive_extraction_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Comprehensive rule and definition extraction using ReAct agent with memory"""
        logger.info("Comprehensive extraction with ReAct agent and memory...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown"),
            "legal_system": "Mixed"  # Could be enhanced based on country
        }
        
        async def extract_async():
            return await self.rule_agent.extract_rules_and_definitions(state["document_content"], context)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, extract_async())
                    result = future.result()
            else:
                result = asyncio.run(extract_async())
        except Exception as e:
            logger.error(f"Comprehensive extraction failed: {e}")
            result = {"rules": [], "definitions": [], "object_properties": [], "data_properties": [], "semantic_relationships": []}
        
        state["extraction_result"] = result
        state["reasoning_steps"] = self.rule_agent.reasoning_steps
        state["action_steps"] = self.rule_agent.action_steps
        
        return state
    
    def _taxonomic_analysis_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Taxonomic analysis using ReAct agent with memory"""
        logger.info("Taxonomic analysis with ReAct agent and memory...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown")
        }
        
        async def analyze_async():
            return await self.concept_agent.analyze_concepts_and_taxonomy(state["extraction_result"], context)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, analyze_async())
                    result = future.result()
            else:
                result = asyncio.run(analyze_async())
        except Exception as e:
            logger.error(f"Taxonomic analysis failed: {e}")
            result = {"concept_schemes": [], "taxonomic_hierarchies": {}, "compliance_frameworks": []}
        
        state["taxonomy_result"] = result
        
        # Combine reasoning steps from both agents
        state["reasoning_steps"].extend(self.concept_agent.reasoning_steps)
        state["action_steps"].extend(self.concept_agent.action_steps)
        
        return state
    
    def _ontology_construction_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Comprehensive ontology construction with memory"""
        logger.info("Building comprehensive ontology with memory...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown")
        }
        
        ontology = self.ontology_builder.build_comprehensive_ontology(
            state["extraction_result"],
            state["taxonomy_result"], 
            context
        )
        
        state["ontology_graph"] = ontology
        
        # Add ontology builder reasoning steps
        state["reasoning_steps"].extend(self.ontology_builder.reasoning_steps)
        state["action_steps"].extend(self.ontology_builder.action_steps)
        
        return state
    
    def _ontology_validation_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Validate ontology using SHACL shapes"""
        logger.info("Validating ontology with SHACL...")
        
        if state.get("ontology_graph"):
            try:
                conforms, validation_results = self.shacl_validator.validate_ontology(state["ontology_graph"])
                
                validation_info = {
                    "conforms": conforms,
                    "validation_performed": True,
                    "timestamp": datetime.now().isoformat()
                }
                
                if not conforms:
                    logger.warning("Ontology validation failed - see validation report")
                    validation_info["status"] = "failed"
                    state["validation_results"] = validation_results
                else:
                    logger.info("Ontology validation passed successfully")
                    validation_info["status"] = "passed"
                
                state["validation_info"] = validation_info
                
            except Exception as e:
                logger.error(f"Ontology validation error: {e}")
                state["validation_info"] = {
                    "conforms": False,
                    "validation_performed": False,
                    "error": str(e),
                    "status": "error"
                }
        else:
            logger.warning("No ontology graph available for validation")
            state["validation_info"] = {
                "conforms": False,
                "validation_performed": False,
                "status": "no_ontology"
            }
        
        return state
    
    def _knowledge_storage_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Enhanced knowledge storage in Elasticsearch with memory integration"""
        logger.info("Storing knowledge in Elasticsearch with memory integration...")
        
        # Generate embeddings for enhanced search
        async def generate_embeddings_async():
            return await self.openai_client.generate_embeddings([state["document_content"]])
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, generate_embeddings_async())
                    embeddings = future.result()
            else:
                embeddings = asyncio.run(generate_embeddings_async())
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            embeddings = []
        
        # Enhanced document structure for storage
        doc = {
            "document_id": str(uuid.uuid4()),
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown"),
            "title": f"{state.get('country', 'Unknown')} Legal Knowledge Base",
            "content": state["document_content"],
            
            # Enhanced rule storage
            "rules": state["extraction_result"].get("rules", []),
            "definitions": state["extraction_result"].get("definitions", []),
            "object_properties": state["extraction_result"].get("object_properties", []),
            "data_properties": state["extraction_result"].get("data_properties", []),
            
            # Taxonomic information
            "concept_schemes": state["taxonomy_result"].get("concept_schemes", []),
            "compliance_frameworks": state["taxonomy_result"].get("compliance_frameworks", []),
            
            # Simple keyword fields for search
            "concepts": self._extract_simple_concepts(state),
            "actors": self._extract_actors(state),
            "objects": self._extract_objects(state),
            "data_domains": state["taxonomy_result"].get("data_domains", []),
            
            # Enhanced metadata with memory tracking
            "extraction_stats": {
                "rules_count": len(state["extraction_result"].get("rules", [])),
                "definitions_count": len(state["extraction_result"].get("definitions", [])),
                "properties_count": len(state["extraction_result"].get("object_properties", [])) + len(state["extraction_result"].get("data_properties", [])),
                "reasoning_steps": len(state.get("reasoning_steps", [])),
                "action_steps": len(state.get("action_steps", [])),
                "memory_namespace": state.get("memory_namespace", "")
            },
            
            # Vector for semantic search
            "content_vector": embeddings[0] if embeddings else [],
            
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # Store in Elasticsearch
        try:
            self.es_client.client.index(
                index=Config.ELASTICSEARCH_INDEX,
                body=doc
            )
            logger.info("Knowledge stored successfully in Elasticsearch with memory integration")
        except Exception as e:
            logger.error(f"Failed to store knowledge: {e}")
        
        return state
    
    def _interface_setup_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Setup query interface with loaded ontology"""
        logger.info("Setting up query interface...")
        
        if state.get("ontology_graph"):
            self.query_interface.load_ontology(state["ontology_graph"])
        
        state["analysis_complete"] = True
        return state
    
    def _extract_simple_concepts(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract simple concept keywords for Elasticsearch"""
        concepts = []
        
        # From definitions
        for definition in state["extraction_result"].get("definitions", []):
            concepts.append(definition.get("concept", ""))
            concepts.extend(definition.get("synonyms", []))
        
        # From taxonomic hierarchies
        for hierarchy_name, hierarchy_data in state["taxonomy_result"].get("taxonomic_hierarchies", {}).items():
            concepts.append(hierarchy_name)
            concepts.extend(self._extract_hierarchy_concepts(hierarchy_data.get("hierarchy", {})))
        
        return [c for c in concepts if c and isinstance(c, str)]
    
    def _extract_hierarchy_concepts(self, hierarchy: Dict) -> List[str]:
        """Recursively extract concepts from hierarchy"""
        concepts = []
        for concept_name, concept_data in hierarchy.items():
            concepts.append(concept_name)
            if isinstance(concept_data, dict) and "children" in concept_data:
                concepts.extend(self._extract_hierarchy_concepts(concept_data["children"]))
        return concepts
    
    def _extract_actors(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract actor entities for search"""
        actors = []
        for rule in state["extraction_result"].get("rules", []):
            subject_info = rule.get("subject", {})
            if isinstance(subject_info, dict) and subject_info.get("entity"):
                actors.append(subject_info["entity"])
        return list(set([a for a in actors if a and isinstance(a, str)]))
    
    def _extract_objects(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract object entities for search"""
        objects = []
        for rule in state["extraction_result"].get("rules", []):
            object_info = rule.get("object", {})
            if isinstance(object_info, dict) and object_info.get("entity"):
                objects.append(object_info["entity"])
        return list(set([o for o in objects if o and isinstance(o, str)]))
    
    async def process_document_enhanced(self, document_path: str, metadata: Dict) -> Dict:
        """Enhanced document processing with comprehensive analysis, ReAct agents, and memory"""
        
        try:
            # Extract text from document
            text_content = self.doc_processor.extract_text_from_pdf(document_path)
            
            # Initialize enhanced state
            initial_state: EnhancedLegalAnalysisState = {
                "messages": [],
                "document_content": text_content,
                "country": metadata.get('country', 'Unknown'),
                "jurisdiction": metadata.get('jurisdiction', 'Unknown'),
                "organization": metadata.get('organization', 'Unknown'),
                "extraction_result": {},
                "taxonomy_result": {},
                "ontology_graph": None,
                "validation_info": None,
                "reasoning_steps": [],
                "action_steps": [],
                "analysis_complete": False,
                "memory_namespace": ""
            }
            
            logger.info(f"Enhanced processing with memory for {initial_state['country']}/{initial_state['jurisdiction']}")
            
            # Run enhanced workflow with memory
            config = {"configurable": {"thread_id": str(uuid.uuid4())}}
            final_state = self.workflow.invoke(initial_state, config=config)
            
            # Export ontology in multiple formats
            ontology_exports = {}
            if final_state.get("ontology_graph"):
                ontology_exports = self._export_ontology_enhanced(
                    final_state["ontology_graph"],
                    metadata['country']
                )
            
            return {
                "success": True,
                "extraction_stats": {
                    "rules_extracted": len(final_state["extraction_result"].get("rules", [])),
                    "definitions_extracted": len(final_state["extraction_result"].get("definitions", [])),
                    "object_properties": len(final_state["extraction_result"].get("object_properties", [])),
                    "data_properties": len(final_state["extraction_result"].get("data_properties", [])),
                    "concept_schemes": len(final_state["taxonomy_result"].get("concept_schemes", [])),
                    "compliance_frameworks": len(final_state["taxonomy_result"].get("compliance_frameworks", []))
                },
                "reasoning_summary": {
                    "reasoning_steps": len(final_state.get("reasoning_steps", [])),
                    "action_steps": len(final_state.get("action_steps", [])),
                    "agents_involved": ["RuleExtractor", "ConceptAnalyzer", "OntologyBuilder"],
                    "memory_namespace": final_state.get("memory_namespace", "")
                },
                "validation_summary": final_state.get("validation_info", {"status": "not_performed"}),
                "ontology_exports": ontology_exports,
                "analysis_complete": final_state.get("analysis_complete", False),
                "query_interface_ready": True,
                "memory_integration": {
                    "long_term_memory_enabled": True,
                    "memory_store_active": True,
                    "session_memory_namespace": final_state.get("memory_namespace", "")
                }
            }
            
        except Exception as e:
            logger.error(f"Enhanced processing failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _export_ontology_enhanced(self, graph: Graph, country: str) -> Dict[str, str]:
        """Enhanced ontology export with validation"""
        
        safe_country = country.replace(' ', '_').replace('/', '_').replace('\\', '_')
        output_dir = Path(Config.OUTPUT_PATH) / safe_country
        output_dir.mkdir(parents=True, exist_ok=True)
        
        exports = {}
        formats = {
            'ttl': 'turtle',
            'jsonld': 'json-ld',
            'xml': 'xml'
        }
        
        for ext, format_name in formats.items():
            filename = output_dir / f"enhanced_ontology_{safe_country}.{ext}"
            try:
                graph.serialize(destination=str(filename), format=format_name)
                exports[ext] = str(filename)
                logger.info(f"Enhanced ontology exported to {filename}")
                
                # Validate the exported file
                if ext == 'ttl':
                    # Test parsing the turtle file
                    test_graph = Graph()
                    test_graph.parse(str(filename), format='turtle')
                    logger.info(f"Validated {ext} export: {len(test_graph)} triples")
                    
            except Exception as e:
                logger.error(f"Failed to export ontology in {format_name}: {e}")
        
        return exports
    
    def start_query_interface(self, host='localhost', port=5000):
        """Start the web-based query interface"""
        if not FLASK_AVAILABLE:
            logger.warning("Flask not available - query interface disabled")
            return None
            
        def run_server():
            self.query_interface.start_server(host, port)
        
        # Start server in a separate thread
        server_thread = threading.Thread(target=run_server, daemon=True)
        server_thread.start()
        logger.info(f"Query interface started at http://{host}:{port}")
        return server_thread

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Enhanced main application with ReAct agents and LangMem memory"""
    
    load_dotenv()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("Starting Enhanced Legal Document Analysis System with ReAct Agents and LangMem")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info("Configuration validation passed")
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info("Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize enhanced orchestrator with memory
    orchestrator = EnhancedLegalAnalysisOrchestrator()
    
    # Start query interface if enabled
    interface_enabled = config.get('processing_options', {}).get('enable_query_interface', True) and FLASK_AVAILABLE
    if interface_enabled:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        server_thread = orchestrator.start_query_interface(interface_host, interface_port)
    else:
        if not FLASK_AVAILABLE:
            logger.info("Query interface disabled - Flask not available")
        else:
            logger.info("Query interface disabled in configuration")
        server_thread = None
    
    # Process documents with enhanced ReAct agents and memory
    results = []
    for doc_config in config['documents']:
        logger.info(f"Enhanced processing with memory for {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f"Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_document_enhanced(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "result": result
            })
            
            logger.info(f"Enhanced processing with memory completed for {doc_config['country']}")
            
        except Exception as e:
            logger.error(f"Enhanced processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Summary
    successful = sum(1 for r in results if r['result']['success'])
    logger.info(f"Enhanced processing with memory complete: {successful}/{len(results)} documents processed")
    
    # Save enhanced results
    summary_path = Path(Config.OUTPUT_PATH) / "enhanced_processing_summary_with_memory.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    if interface_enabled and server_thread:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        logger.info(f"Query interface with memory features available at http://{interface_host}:{interface_port}")
        logger.info("Press Ctrl+C to stop the system")
        
        try:
            # Keep the main thread alive
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("Shutting down...")
    else:
        logger.info("System processing completed. Query interface not started.")

if __name__ == "__main__":
    asyncio.run(main())
