#!/usr/bin/env python3
"""
Enhanced Multi-Agent Legal Document Rule Extraction System using LangGraph
NO TRUNCATION - Processes complete PDF content with knowledge graph construction
Dedicated agents with Chain of Thought and ReAct prompting + Knowledge Graphs
Uses geography.json for all country data - no hardcoding
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Literal, TypedDict
from datetime import datetime
import re
import uuid
import math
from dataclasses import dataclass

# Core libraries
import pandas as pd
import PyPDF2
from pydantic import BaseModel, Field, ValidationError, model_validator
from pydantic_core import from_json

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

# OpenAI for embeddings (if needed)
import openai

# Global Configuration
API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
BASE_URL = "https://api.openai.com/v1"
MODEL_NAME = "gpt-4o-mini"  # Using the model from existing code
EMBEDDING_MODEL = "text-embedding-3-large"
CHUNK_SIZE = 8000  # Increased chunk size for better processing
OVERLAP_SIZE = 1000  # Increased overlap to prevent information loss
MAX_CHUNKS = 50  # Process up to 50 chunks to handle large documents

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Pydantic v2 Models with exact required structure
class RuleCondition(BaseModel):
    """Rule condition with required structure and role assignment"""
    condition_id: str = Field(..., description="Unique condition identifier")
    condition_definition: str = Field(..., description="Clear condition definition in simple English")
    fact: str = Field(..., description="The fact to evaluate")
    operator: str = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    role: str = Field(..., description="Role: controller, processor, joint_controller, or data_subject")

class ExtractedRule(BaseModel):
    """Extracted rule with exact required structure"""
    rule_id: str = Field(..., description="Unique rule identifier")
    rule_definition: str = Field(..., description="Complete rule definition in simple English")
    applicable_countries: List[str] = Field(..., description="Country ISO codes from metadata config")
    adequacy_countries: List[str] = Field(default_factory=list, description="Actual countries with adequacy decisions from PDF")
    conditions: List[RuleCondition] = Field(..., description="List of rule conditions with roles")
    aggregated_roles: List[str] = Field(default_factory=list, description="All roles involved in this rule")
    data_category: str = Field(..., description="Primary data category")
    domain: str = Field(default="access_and_entitlements", description="Domain focus")
    action: str = Field(..., description="Required action in simple English")
    reference: str = Field(..., description="Level and article/text reference")

class MetadataConfig(BaseModel):
    """Configuration model"""
    pdf_path: str = Field(..., description="Path to PDF file")
    applicable_countries: List[str] = Field(..., description="ISO country codes where rules apply")
    document_type: str = Field(default="regulation", description="Type of document")

# Document Chunk Management
@dataclass
class DocumentChunk:
    """Document chunk with metadata"""
    content: str
    chunk_id: str
    start_page: int
    end_page: int
    section_type: str
    overlap_content: str = ""

# Multi-Agent State Management (Enhanced)
class MultiAgentState(TypedDict):
    """Enhanced shared state between all agents"""
    # Input data
    document_text: str
    document_chunks: List[Dict[str, Any]]
    metadata_config: dict
    geography_data: dict
    
    # Processing stages
    parsed_sections: dict
    knowledge_graph: dict  # New: Knowledge graph representation
    identified_countries: dict
    extracted_rules: list
    validated_rules: list
    
    # Agent reasoning traces with knowledge graphs
    agent1_reasoning: list
    agent1_knowledge_graph: dict
    agent2_reasoning: list  
    agent2_knowledge_graph: dict
    agent3_reasoning: list
    agent3_knowledge_graph: dict
    agent4_reasoning: list
    agent4_knowledge_graph: dict
    
    # Processing control
    current_agent: str
    processing_complete: bool
    chunks_processed: int
    total_chunks: int

# Enhanced Geography Handler - NO HARDCODING
class GeographyHandler:
    """Enhanced geography handler with better country verification"""
    
    def __init__(self, geography_file: str):
        self.geography_data = self._load_geography_data(geography_file)
        self.all_countries = self._extract_all_countries()
        self.country_variations = self._build_country_variations()
        logger.info(f"🌍 Geography data loaded: {len(self.all_countries)} countries")
    
    def _load_geography_data(self, file_path: str) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load geography data: {e}")
            return {}
    
    def _extract_all_countries(self) -> Dict[str, str]:
        """Extract all countries from geography.json - NO HARDCODING"""
        countries = {}
        
        # Process all regions in geography data
        for region_key, region_data in self.geography_data.items():
            if isinstance(region_data, dict):
                # Direct countries list
                if 'countries' in region_data:
                    for country in region_data['countries']:
                        countries[country['iso2']] = country['name']
                
                # Nested continent structure
                if region_key == 'By_Continent':
                    for continent, continent_data in region_data.items():
                        if isinstance(continent_data, dict) and 'countries' in continent_data:
                            for country in continent_data['countries']:
                                countries[country['iso2']] = country['name']
        
        return countries
    
    def _build_country_variations(self) -> Dict[str, str]:
        """Build country name variations for better matching"""
        variations = {}
        for iso, name in self.all_countries.items():
            variations[name.lower()] = iso
            variations[iso.lower()] = iso
            # Add common variations
            if "," in name:
                short_name = name.split(",")[0].strip()
                variations[short_name.lower()] = iso
        return variations
    
    def get_country_name(self, iso_code: str) -> Optional[str]:
        """Get country name from ISO code"""
        return self.all_countries.get(iso_code.upper())
    
    def get_country_iso(self, country_name: str) -> Optional[str]:
        """Get ISO code from country name with variations"""
        return self.country_variations.get(country_name.lower())
    
    def is_valid_country(self, iso_code: str) -> bool:
        """Check if ISO code is valid country"""
        return iso_code.upper() in self.all_countries
    
    def find_countries_in_text(self, text: str) -> List[str]:
        """Enhanced country finding with better matching"""
        found_countries = set()
        text_lower = text.lower()
        
        # Search for country names and ISO codes
        for variation, iso in self.country_variations.items():
            if len(variation) > 2:  # Avoid matching very short strings
                if re.search(r'\b' + re.escape(variation) + r'\b', text_lower):
                    found_countries.add(iso.upper())
        
        return list(found_countries)

# Enhanced PDF Processing with NO TRUNCATION
class PDFProcessor:
    """Enhanced PDF processor with complete content extraction"""
    
    @staticmethod
    def extract_complete_text_from_pdf(pdf_path: str) -> Dict[str, Any]:
        """Extract complete text from PDF with page metadata"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                complete_text = ""
                page_contents = []
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    page_contents.append({
                        'page_number': page_num + 1,
                        'content': page_text,
                        'length': len(page_text)
                    })
                    complete_text += f"\n[PAGE {page_num + 1}]\n{page_text}\n"
                
                return {
                    'complete_text': complete_text,
                    'page_contents': page_contents,
                    'total_pages': len(pdf_reader.pages),
                    'total_length': len(complete_text)
                }
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            return {'complete_text': "", 'page_contents': [], 'total_pages': 0, 'total_length': 0}
    
    @staticmethod
    def create_overlapping_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP_SIZE) -> List[DocumentChunk]:
        """Create overlapping chunks from text for complete coverage"""
        chunks = []
        text_length = len(text)
        
        if text_length <= chunk_size:
            # Single chunk if text is small
            chunks.append(DocumentChunk(
                content=text,
                chunk_id=f"chunk_1",
                start_page=1,
                end_page=1,
                section_type="complete_document"
            ))
            return chunks
        
        start = 0
        chunk_num = 1
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            
            # Extract chunk content
            chunk_content = text[start:end]
            
            # Add overlap from next chunk if available
            overlap_content = ""
            if end < text_length:
                overlap_end = min(end + overlap, text_length)
                overlap_content = text[end:overlap_end]
            
            # Determine page numbers (approximate based on [PAGE X] markers)
            start_page = len(re.findall(r'\[PAGE (\d+)\]', text[:start])) + 1
            end_page = len(re.findall(r'\[PAGE (\d+)\]', text[:end]))
            if end_page == 0:
                end_page = start_page
            
            chunk = DocumentChunk(
                content=chunk_content,
                chunk_id=f"chunk_{chunk_num}",
                start_page=start_page,
                end_page=end_page,
                section_type="document_section",
                overlap_content=overlap_content
            )
            
            chunks.append(chunk)
            start += chunk_size - overlap  # Move start position with overlap
            chunk_num += 1
            
            # Safety limit
            if chunk_num > MAX_CHUNKS:
                logger.warning(f"Reached maximum chunk limit ({MAX_CHUNKS})")
                break
        
        logger.info(f"📄 Created {len(chunks)} overlapping chunks from {text_length} characters")
        return chunks

# AGENT 1: Enhanced Document Parser Agent with Knowledge Graphs
class DocumentParserAgent:
    """Enhanced Agent 1: Document parsing with knowledge graphs and NO truncation"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "DocumentParserAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process document with enhanced ReAct prompting and knowledge graphs"""
        
        logger.info(f"🤖 {self.name}: Starting complete document parsing (NO TRUNCATION)")
        
        # Create document chunks for complete processing
        pdf_processor = PDFProcessor()
        chunks = pdf_processor.create_overlapping_chunks(state['document_text'])
        
        system_prompt = """You are a specialized Document Parser Agent using ReAct prompting with Knowledge Graph construction.

Your task: Parse legal documents completely and create structured knowledge representations.

Knowledge Graph Construction Process:
1. Build internal knowledge graph of document structure
2. Identify entities: Sections, Articles, Regulations, References
3. Map relationships: Contains, References, Applies-to, Requires
4. Create hierarchical structure: Level-1, Level-2, Level-3

ReAct Framework Enhanced:
1. REASONING: Analyze document structure and build knowledge map
2. ACTION: Extract sections and populate knowledge graph
3. OBSERVATION: Validate completeness and structure integrity
4. KNOWLEDGE GRAPH: Update internal graph representation
5. CONCLUSION: Provide complete parsed sections

Chain of Thought with Knowledge Graphs:
1. Construct document knowledge graph internally
2. Identify Level-1-Regulation-*, Level-2-Regulator-Guidance, Level-3-Supporting-Information
3. Map cross-references and dependencies
4. Extract complete content without truncation
5. Validate against knowledge graph structure

CRITICAL: Process ENTIRE document content - NO truncation allowed.
Build comprehensive knowledge graph for document structure understanding."""

        # Process all chunks to ensure complete coverage
        all_sections = {}
        reasoning_trace = []
        knowledge_graph = {"entities": [], "relationships": [], "sections": {}}
        
        for i, chunk in enumerate(chunks):
            logger.info(f"📄 Processing chunk {i+1}/{len(chunks)} (pages {chunk.start_page}-{chunk.end_page})")
            
            user_prompt = f"""Parse this document chunk using ReAct methodology with Knowledge Graph construction:

CHUNK {i+1}/{len(chunks)} - Pages {chunk.start_page} to {chunk.end_page}:
{chunk.content}

Previous knowledge from other chunks: {json.dumps(all_sections, indent=2)[:500]}...

Use ReAct with Knowledge Graphs:
1. REASONING: Analyze chunk for legal structure and entities
2. ACTION: Extract sections and build knowledge graph representation
3. OBSERVATION: Evaluate extraction completeness 
4. KNOWLEDGE GRAPH: Create/update internal knowledge representation
5. CONCLUSION: Provide structured sections

Build Knowledge Graph internally:
- Entities: Regulations, Articles, Sections, References, Requirements
- Relationships: Contains, References, Requires, Applies-to
- Hierarchy: Level-1 → Level-2 → Level-3

Extract ALL Level-1-Regulation-*, Level-2-Regulator-Guidance, Level-3-Supporting-Information sections.
Include complete content - NO truncation. Update knowledge graph with new entities and relationships.

Return sections in JSON format with knowledge graph annotations."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            try:
                response = self.llm.invoke(messages)
                response_text = response.content
                
                # Extract reasoning and knowledge graph
                chunk_reasoning = self._extract_reasoning_trace(response_text)
                reasoning_trace.extend(chunk_reasoning)
                
                # Extract sections from this chunk
                chunk_sections = self._extract_sections_from_response(response_text, chunk.content)
                
                # Merge sections (combine if same section found across chunks)
                for section_name, content in chunk_sections.items():
                    if section_name in all_sections:
                        # Combine content from multiple chunks
                        all_sections[section_name] += f"\n\n[CONTINUED FROM CHUNK {i+1}]\n{content}"
                    else:
                        all_sections[section_name] = content
                
                # Update knowledge graph
                chunk_kg = self._extract_knowledge_graph(response_text)
                self._merge_knowledge_graphs(knowledge_graph, chunk_kg)
                
            except Exception as e:
                logger.error(f"❌ Chunk {i+1} processing failed: {e}")
                reasoning_trace.append(f"ERROR Chunk {i+1}: {e}")
        
        # Update state
        state['parsed_sections'] = all_sections
        state['agent1_reasoning'] = reasoning_trace
        state['agent1_knowledge_graph'] = knowledge_graph
        state['document_chunks'] = [chunk.__dict__ for chunk in chunks]
        state['chunks_processed'] = len(chunks)
        state['total_chunks'] = len(chunks)
        state['current_agent'] = 'GeographyAgent'
        
        logger.info(f"✅ {self.name}: Successfully parsed {len(all_sections)} sections from {len(chunks)} chunks")
        logger.info(f"📊 Knowledge Graph: {len(knowledge_graph['entities'])} entities, {len(knowledge_graph['relationships'])} relationships")
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced ReAct reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_sections_from_response(self, response_text: str, original_content: str) -> Dict[str, str]:
        """Extract sections with enhanced pattern matching"""
        sections = {}
        
        # Try JSON extraction first
        json_data = self._safe_json_extract(response_text)
        if json_data and isinstance(json_data, dict):
            return json_data
        
        # Enhanced pattern matching for legal document structure
        level_patterns = {
            "Level-1": r"Level-1-Regulation-([^:]+):(.*?)(?=Level-[123]|$)",
            "Level-2": r"Level-2-Regulator-Guidance:(.*?)(?=Level-[123]|$)",
            "Level-3": r"Level-3-Supporting-Information:(.*?)(?=Level-[123]|$)"
        }
        
        # Search in both response and original content
        search_texts = [response_text, original_content]
        
        for text in search_texts:
            for level, pattern in level_patterns.items():
                matches = re.finditer(pattern, text, re.DOTALL | re.IGNORECASE)
                for i, match in enumerate(matches):
                    if level == "Level-1":
                        reg_name = match.group(1).strip()
                        content = match.group(2).strip()
                        section_key = f"{level}-Regulation-{reg_name}"
                        sections[section_key] = content
                    else:
                        content = match.group(1).strip()
                        if content and len(content) > 50:  # Only add substantial content
                            sections[level] = content
        
        # If no sections found, create general sections from content blocks
        if not sections and original_content:
            sections["General-Content"] = original_content[:2000] + "..." if len(original_content) > 2000 else original_content
        
        return sections
    
    def _extract_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract knowledge graph from response"""
        kg = {"entities": [], "relationships": []}
        
        # Look for knowledge graph patterns in response
        kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
        kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        if kg_match:
            kg_text = kg_match.group(1)
            
            # Extract entities
            entity_pattern = r'Entity:\s*([^,\n]+)'
            entities = re.findall(entity_pattern, kg_text, re.IGNORECASE)
            kg["entities"] = entities
            
            # Extract relationships
            rel_pattern = r'Relationship:\s*([^,\n]+)\s*→\s*([^,\n]+)'
            relationships = re.findall(rel_pattern, kg_text, re.IGNORECASE)
            kg["relationships"] = [{"from": rel[0].strip(), "to": rel[1].strip()} for rel in relationships]
        
        return kg
    
    def _merge_knowledge_graphs(self, main_kg: Dict, chunk_kg: Dict):
        """Merge knowledge graphs from different chunks"""
        # Merge entities
        for entity in chunk_kg.get("entities", []):
            if entity not in main_kg["entities"]:
                main_kg["entities"].append(entity)
        
        # Merge relationships
        existing_rels = set((rel["from"], rel["to"]) for rel in main_kg["relationships"])
        for rel in chunk_kg.get("relationships", []):
            rel_tuple = (rel["from"], rel["to"])
            if rel_tuple not in existing_rels:
                main_kg["relationships"].append(rel)
                existing_rels.add(rel_tuple)
    
    def _safe_json_extract(self, text: str) -> Optional[Dict]:
        """Safely extract JSON from text"""
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            r'\{.*?\}',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

# AGENT 2: Enhanced Geography Agent with Knowledge Graphs
class GeographyAgent:
    """Enhanced Agent 2: Country identification with knowledge graphs"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "GeographyAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process geography with enhanced knowledge graphs"""
        
        logger.info(f"🌍 {self.name}: Starting country identification with knowledge graphs")
        
        # Prepare comprehensive country data for LLM
        country_data = self._prepare_country_knowledge_graph()
        
        system_prompt = f"""You are a specialized Geography Agent using ReAct prompting with Knowledge Graph construction.

Your task: Identify countries in legal documents using geography data and build location knowledge graphs.

Available Geography Knowledge Graph:
Countries: {len(self.geography_handler.all_countries)} total
Regions: EU, EEA, MENAT, By_Continent
{json.dumps(dict(list(self.geography_handler.all_countries.items())[:20]), indent=2)}...

Knowledge Graph Construction:
1. Build location entity graph: Countries → Regions → Legal_Frameworks
2. Map relationships: Member-of, Has-adequacy, Applies-to, References
3. Track adequacy decisions and legal contexts
4. Connect countries to legal obligations

ReAct Framework Enhanced:
1. REASONING: Analyze document for location references and adequacy context
2. ACTION: Identify countries and build location knowledge graph
3. OBSERVATION: Validate against geography data and adequacy patterns
4. KNOWLEDGE GRAPH: Create location-legal framework mapping
5. CONCLUSION: Provide verified country identification

Chain of Thought with Location Knowledge:
1. Scan all document sections for country mentions
2. Build knowledge graph of Country → Legal_Status → Adequacy
3. Verify all countries exist in geography data
4. Identify adequacy contexts: "adequacy decision", "adequate protection"
5. NO HARDCODING - use only provided geography data

CRITICAL: Verify ALL countries against geography data. Build comprehensive location knowledge graph."""

        # Combine all document content for comprehensive analysis
        all_content = ""
        for section_name, content in state['parsed_sections'].items():
            all_content += f"\n[SECTION: {section_name}]\n{content}\n"
        
        user_prompt = f"""Identify countries using ReAct methodology with Knowledge Graph construction:

COMPLETE DOCUMENT CONTENT (ALL SECTIONS):
{all_content[:8000]}{"..." if len(all_content) > 8000 else ""}

Applicable Countries (from config): {state['metadata_config']['applicable_countries']}

Use ReAct with Location Knowledge Graphs:
1. REASONING: Analyze all sections for country references and adequacy contexts
2. ACTION: Build location knowledge graph with countries and legal statuses
3. OBSERVATION: Verify against geography data and adequacy patterns
4. KNOWLEDGE GRAPH: Map Countries → Regions → Adequacy_Status → Legal_Framework
5. CONCLUSION: Provide complete verified country identification

Knowledge Graph Structure:
- Entities: Countries, Regions, Adequacy_Decisions, Legal_Frameworks
- Relationships: Member-of, Has-adequacy, References, Applies-to

Identify patterns:
- Countries with "adequacy decision" or "adequate protection" context
- Regional references (EU, EEA) mapped to specific countries
- Legal framework applications by country

Return comprehensive country analysis with knowledge graph mapping.
Verify ALL countries against geography data - NO HARDCODING."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract knowledge graph
            knowledge_graph = self._extract_location_knowledge_graph(response_text)
            
            # Extract and verify country results
            country_results = self._extract_and_verify_countries(response_text, all_content)
            
            # Update state
            state['identified_countries'] = country_results
            state['agent2_reasoning'] = reasoning_trace
            state['agent2_knowledge_graph'] = knowledge_graph
            state['current_agent'] = 'RuleExtractionAgent'
            
            logger.info(f"✅ {self.name}: Identified {len(country_results.get('mentioned_countries', []))} countries")
            logger.info(f"   📍 Adequacy countries: {country_results.get('adequacy_countries', [])}")
            logger.info(f"📊 Location KG: {len(knowledge_graph.get('entities', []))} entities")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent2_reasoning'] = [f"ERROR: {e}"]
            state['agent2_knowledge_graph'] = {"entities": [], "relationships": []}
            state['identified_countries'] = {
                "mentioned_countries": [],
                "adequacy_countries": [],
                "verification_status": "failed"
            }
        
        return state
    
    def _prepare_country_knowledge_graph(self) -> Dict[str, Any]:
        """Prepare country knowledge graph for LLM context"""
        kg = {
            "regions": {},
            "countries_by_region": {},
            "total_countries": len(self.geography_handler.all_countries)
        }
        
        # Process geography data into knowledge graph format
        for region_name, region_data in self.geography_handler.geography_data.items():
            if isinstance(region_data, dict) and 'countries' in region_data:
                kg["regions"][region_name] = [country['iso2'] for country in region_data['countries'][:10]]  # Sample
        
        return kg
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_location_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract location knowledge graph"""
        kg = {"entities": [], "relationships": [], "adequacy_mapping": {}}
        
        # Extract from knowledge graph section
        kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
        kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        if kg_match:
            kg_text = kg_match.group(1)
            
            # Extract location entities
            entity_patterns = [
                r'Country:\s*([A-Z]{2})\s*-\s*([^,\n]+)',
                r'Region:\s*([^,\n]+)',
                r'Adequacy:\s*([^,\n]+)'
            ]
            
            for pattern in entity_patterns:
                matches = re.findall(pattern, kg_text, re.IGNORECASE)
                kg["entities"].extend(matches)
        
        return kg
    
    def _extract_and_verify_countries(self, response_text: str, document_text: str) -> Dict[str, Any]:
        """Extract and verify countries against geography data"""
        # Try JSON extraction first
        json_data = self._safe_json_extract(response_text)
        if json_data and isinstance(json_data, dict):
            # Verify countries
            mentioned = json_data.get('mentioned_countries', [])
            adequacy = json_data.get('adequacy_countries', [])
            
            verified_mentioned = [c.upper() for c in mentioned if self.geography_handler.is_valid_country(c)]
            verified_adequacy = [c.upper() for c in adequacy if self.geography_handler.is_valid_country(c)]
            
            return {
                "mentioned_countries": verified_mentioned,
                "adequacy_countries": verified_adequacy,
                "verification_status": "verified against geography data"
            }
        
        # Fallback: comprehensive text analysis
        mentioned_countries = self.geography_handler.find_countries_in_text(document_text)
        
        # Enhanced adequacy country detection
        adequacy_countries = []
        text_lower = document_text.lower()
        
        adequacy_patterns = [
            r'adequacy decision[^.]*?([A-Z]{2})',
            r'adequate protection[^.]*?([A-Z]{2})',
            r'([A-Z]{2})[^.]*?adequacy decision',
            r'([A-Z]{2})[^.]*?adequate protection'
        ]
        
        for pattern in adequacy_patterns:
            matches = re.findall(pattern, document_text, re.IGNORECASE)
            for match in matches:
                if self.geography_handler.is_valid_country(match):
                    adequacy_countries.append(match.upper())
        
        # Also check for country names in adequacy contexts
        for iso in mentioned_countries:
            country_name = self.geography_handler.get_country_name(iso)
            if country_name:
                name_lower = country_name.lower()
                adequacy_context_patterns = [
                    rf'{name_lower}[^.]*?adequacy',
                    rf'adequacy[^.]*?{name_lower}',
                ]
                
                for pattern in adequacy_context_patterns:
                    if re.search(pattern, text_lower):
                        adequacy_countries.append(iso)
                        break
        
        return {
            "mentioned_countries": list(set(mentioned_countries)),
            "adequacy_countries": list(set(adequacy_countries)),
            "verification_status": "verified against geography data"
        }
    
    def _safe_json_extract(self, text: str) -> Optional[Dict]:
        """Safely extract JSON from text"""
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            r'\{.*?\}'
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

# AGENT 3: Enhanced Rule Extraction Agent with Knowledge Graphs
class RuleExtractionAgent:
    """Enhanced Agent 3: Rule extraction with knowledge graphs and role assignment"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "RuleExtractionAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Extract rules with enhanced knowledge graphs and role assignment"""
        
        logger.info(f"⚖️ {self.name}: Starting rule extraction with knowledge graphs")
        
        system_prompt = """You are a specialized Rule Extraction Agent using ReAct prompting with Legal Knowledge Graph construction.

Your task: Extract structured rules for access and entitlements with role assignments and comprehensive legal relationships.

Legal Knowledge Graph Construction:
1. Build legal entity graph: Rules → Conditions → Actions → Roles → Data_Categories
2. Map relationships: Requires, Triggers, Applies-to, Governs, References
3. Assign roles: controller, processor, joint_controller, data_subject
4. Track data categories and access entitlements

Required Rule Structure with Roles:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule in simple English",
  "conditions": [
    {
      "condition_id": "unique_condition_id",
      "condition_definition": "condition in simple English", 
      "fact": "data.category or user.role etc",
      "operator": "equal/in/greaterThan etc",
      "value": "comparison_value",
      "role": "controller/processor/joint_controller/data_subject"
    }
  ],
  "aggregated_roles": ["all_roles_in_this_rule"],
  "data_category": "Personal Data/Special Category Data etc",
  "domain": "access_and_entitlements",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework Enhanced:
1. REASONING: Analyze sections for legal obligations and build legal knowledge graph
2. ACTION: Extract rules with conditions and assign appropriate roles
3. OBSERVATION: Validate rule completeness and role assignments
4. KNOWLEDGE GRAPH: Update legal relationship mapping
5. CONCLUSION: Provide structured rules with roles

Chain of Thought with Legal Knowledge:
1. Build knowledge graph: Legal_Obligation → Condition → Role → Action
2. Identify access and entitlement patterns
3. Extract conditions with proper role assignments
4. Map data categories to processing activities
5. Create comprehensive rule definitions

Focus Areas:
- Access rights and data subject requests
- Controller and processor obligations
- Consent and data processing requirements  
- Compliance timeframes and actions
- Role-specific requirements

CRITICAL: Assign appropriate roles to each condition. Aggregate all roles at rule level."""

        # Process all sections comprehensively
        all_sections_text = ""
        for section_name, content in state['parsed_sections'].items():
            all_sections_text += f"\n\n[SECTION: {section_name}]\n{content}\n"
        
        user_prompt = f"""Extract structured rules using ReAct methodology with Legal Knowledge Graphs:

COMPLETE DOCUMENT CONTENT (ALL SECTIONS):
{all_sections_text[:10000]}{"..." if len(all_sections_text) > 10000 else ""}

Available Context:
- Applicable Countries: {state['metadata_config']['applicable_countries']}
- Adequacy Countries: {state['identified_countries'].get('adequacy_countries', [])}

Use ReAct with Legal Knowledge Graphs:
1. REASONING: Build legal knowledge graph of obligations, conditions, roles, and actions
2. ACTION: Extract rules focusing on access and entitlements with proper role assignments
3. OBSERVATION: Validate rule structure and role completeness
4. KNOWLEDGE GRAPH: Map Legal_Rules → Conditions → Roles → Data_Categories → Actions
5. CONCLUSION: Provide comprehensive rules with role assignments

Knowledge Graph Structure:
- Entities: Rules, Conditions, Roles, Data_Categories, Actions, References
- Relationships: Has-condition, Requires-role, Governs-data, Triggers-action

Role Assignment Guidelines:
- controller: Data processing decisions and purposes
- processor: Data processing on behalf of controller  
- joint_controller: Shared processing decisions
- data_subject: Individual rights and requests

Extract rules for:
- Data subject access requests and rights
- Controller obligations for data processing
- Processor requirements and limitations
- Joint controller responsibilities
- Consent management and withdrawal
- Data retention and deletion requirements
- Cross-border transfer requirements

Return comprehensive JSON array of rules with complete structure and role assignments."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract knowledge graph
            knowledge_graph = self._extract_legal_knowledge_graph(response_text)
            
            # Extract rules with role assignments
            extracted_rules = self._extract_rules_with_roles(response_text, state)
            
            # Update state
            state['extracted_rules'] = extracted_rules
            state['agent3_reasoning'] = reasoning_trace
            state['agent3_knowledge_graph'] = knowledge_graph
            state['current_agent'] = 'ValidationAgent'
            
            logger.info(f"✅ {self.name}: Extracted {len(extracted_rules)} rules")
            logger.info(f"📊 Legal KG: {len(knowledge_graph.get('entities', []))} entities")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent3_reasoning'] = [f"ERROR: {e}"]
            state['agent3_knowledge_graph'] = {"entities": [], "relationships": []}
            state['extracted_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_legal_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract legal knowledge graph"""
        kg = {"entities": [], "relationships": [], "rule_mappings": {}}
        
        # Extract legal entities and relationships
        kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
        kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        if kg_match:
            kg_text = kg_match.group(1)
            
            # Extract legal entities
            entity_patterns = [
                r'Rule:\s*([^,\n]+)',
                r'Condition:\s*([^,\n]+)',
                r'Role:\s*(controller|processor|joint_controller|data_subject)',
                r'Data_Category:\s*([^,\n]+)',
                r'Action:\s*([^,\n]+)'
            ]
            
            for pattern in entity_patterns:
                matches = re.findall(pattern, kg_text, re.IGNORECASE)
                kg["entities"].extend(matches)
            
            # Extract relationships
            rel_pattern = r'(\w+)\s*→\s*(\w+):\s*([^,\n]+)'
            relationships = re.findall(rel_pattern, kg_text, re.IGNORECASE)
            kg["relationships"] = [{"from": rel[0], "to": rel[1], "type": rel[2]} for rel in relationships]
        
        return kg
    
    def _extract_rules_with_roles(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Extract rules with proper role assignments"""
        rules = []
        
        # Try JSON extraction first
        json_data = self._safe_json_extract(response_text)
        if json_data:
            if isinstance(json_data, list):
                rules = json_data
            elif isinstance(json_data, dict):
                rules = [json_data]
        
        # If no JSON rules found, create from text analysis
        if not rules:
            rules = self._create_rules_from_comprehensive_analysis(state)
        
        # Enhance rules with role assignments
        enhanced_rules = []
        for rule in rules:
            enhanced_rule = self._enhance_rule_with_roles(rule, state)
            enhanced_rules.append(enhanced_rule)
        
        return enhanced_rules
    
    def _enhance_rule_with_roles(self, rule: Dict, state: MultiAgentState) -> Dict:
        """Enhance rule with proper role assignments"""
        # Ensure conditions have roles
        conditions = rule.get('conditions', [])
        enhanced_conditions = []
        
        for condition in conditions:
            if isinstance(condition, dict):
                # Assign role if not present
                if 'role' not in condition:
                    condition['role'] = self._assign_role_from_context(condition.get('condition_definition', ''))
                
                # Ensure all required fields
                enhanced_condition = {
                    "condition_id": condition.get("condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    "condition_definition": condition.get("condition_definition", "Data processing condition"),
                    "fact": condition.get("fact", "data.category"),
                    "operator": condition.get("operator", "equal"),
                    "value": condition.get("value", "Personal Data"),
                    "role": condition.get("role", "controller")
                }
                enhanced_conditions.append(enhanced_condition)
        
        # Aggregate roles from conditions
        aggregated_roles = list(set(cond.get('role', 'controller') for cond in enhanced_conditions))
        
        # Enhanced rule structure
        enhanced_rule = {
            "rule_id": rule.get("rule_id", f"rule_{uuid.uuid4().hex[:8]}"),
            "rule_definition": rule.get("rule_definition", "Legal compliance requirement"),
            "conditions": enhanced_conditions,
            "aggregated_roles": aggregated_roles,
            "data_category": rule.get("data_category", "Personal Data"),
            "domain": "access_and_entitlements",
            "action": rule.get("action", "Must ensure compliance"),
            "reference": rule.get("reference", "Document reference")
        }
        
        return enhanced_rule
    
    def _assign_role_from_context(self, condition_text: str) -> str:
        """Assign role based on condition context"""
        text_lower = condition_text.lower()
        
        # Role assignment patterns
        if any(pattern in text_lower for pattern in ['controller', 'determine purpose', 'processing decisions']):
            return 'controller'
        elif any(pattern in text_lower for pattern in ['processor', 'on behalf', 'process data for']):
            return 'processor'
        elif any(pattern in text_lower for pattern in ['joint controller', 'shared decision', 'jointly determine']):
            return 'joint_controller'
        elif any(pattern in text_lower for pattern in ['data subject', 'individual', 'person', 'user right']):
            return 'data_subject'
        else:
            return 'controller'  # Default
    
    def _create_rules_from_comprehensive_analysis(self, state: MultiAgentState) -> List[Dict]:
        """Create rules from comprehensive text analysis"""
        rules = []
        
        # Analyze all sections for legal obligations
        for section_name, content in state['parsed_sections'].items():
            sentences = re.split(r'[.!?]+', content)
            
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if len(sentence) < 30:  # Skip very short sentences
                    continue
                
                # Look for obligation patterns
                obligation_patterns = [
                    r'must\s+([^.]+)',
                    r'shall\s+([^.]+)',
                    r'required\s+to\s+([^.]+)',
                    r'obligation\s+to\s+([^.]+)',
                    r'right\s+to\s+([^.]+)'
                ]
                
                for pattern in obligation_patterns:
                    match = re.search(pattern, sentence.lower())
                    if match:
                        action = match.group(1).strip()
                        
                        # Create rule with role assignment
                        rule = {
                            "rule_id": f"rule_{section_name.lower().replace('-', '_').replace(' ', '_')}_{i}_{uuid.uuid4().hex[:8]}",
                            "rule_definition": sentence,
                            "conditions": [{
                                "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                                "condition_definition": f"When processing data in {section_name}",
                                "fact": "data.category",
                                "operator": "equal",
                                "value": "Personal Data",
                                "role": self._assign_role_from_context(sentence)
                            }],
                            "aggregated_roles": [self._assign_role_from_context(sentence)],
                            "data_category": "Personal Data",
                            "domain": "access_and_entitlements",
                            "action": action,
                            "reference": f"{section_name} - Article/Text reference"
                        }
                        
                        rules.append(rule)
                        
                        # Limit to avoid too many rules
                        if len(rules) >= 20:
                            break
                
                if len(rules) >= 20:
                    break
            
            if len(rules) >= 20:
                break
        
        return rules
    
    def _safe_json_extract(self, text: str) -> Optional[Union[Dict, List]]:
        """Safely extract JSON from text"""
        # Look for JSON arrays and objects
        json_patterns = [
            r'\[.*?\]',  # Array pattern
            r'\{.*?\}'   # Object pattern
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

# AGENT 4: Enhanced Validation Agent with Knowledge Graphs
class ValidationAgent:
    """Enhanced Agent 4: Rule validation with knowledge graphs"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "ValidationAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Validate and structure rules with enhanced knowledge graphs"""
        
        logger.info(f"✅ {self.name}: Starting rule validation with knowledge graphs")
        
        system_prompt = """You are a specialized Validation Agent using ReAct prompting with Validation Knowledge Graph construction.

Your task: Validate and structure extracted rules with comprehensive verification and knowledge relationships.

Validation Knowledge Graph Construction:
1. Build validation entity graph: Rules → Validation_Checks → Compliance_Status → Issues
2. Map relationships: Validates, Complies-with, Has-issue, References, Verifies
3. Track validation results and compliance status
4. Connect rules to legal requirements and standards

EXACT Required Format with Roles:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule definition in simple English",
  "applicable_countries": ["ISO codes from metadata config"],
  "adequacy_countries": ["verified ISO codes from document"],
  "conditions": [
    {
      "condition_id": "unique_condition_id",
      "condition_definition": "clear condition in simple English",
      "fact": "property to evaluate", 
      "operator": "comparison operator",
      "value": "value to compare",
      "role": "controller/processor/joint_controller/data_subject"
    }
  ],
  "aggregated_roles": ["all_roles_from_conditions"],
  "data_category": "primary data category",
  "domain": "access_and_entitlements",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework Enhanced:
1. REASONING: Analyze rule structure and build validation knowledge graph
2. ACTION: Validate rules against format requirements and verify data
3. OBSERVATION: Check validation results and identify compliance issues
4. KNOWLEDGE GRAPH: Update validation status and issue tracking
5. CONCLUSION: Provide validated rules with compliance verification

Chain of Thought with Validation Knowledge:
1. Build validation graph: Rule → Validation_Check → Result → Issue/Compliance
2. Validate rule structure completeness and format compliance
3. Verify country codes against geography data
4. Validate role assignments and aggregations
5. Check reference accuracy and completeness
6. Apply comprehensive format validation

Validation Checks:
- Structure completeness (all required fields)
- Country code verification against geography data
- Role assignment accuracy and aggregation
- Data category consistency
- Reference format and accuracy
- Action clarity and compliance

CRITICAL: All countries must be verified against geography data. Roles must be properly assigned and aggregated."""

        user_prompt = f"""Validate and structure rules using ReAct methodology with Validation Knowledge Graphs:

EXTRACTED RULES FOR VALIDATION:
{json.dumps(state['extracted_rules'][:5], indent=2)}
... (total: {len(state['extracted_rules'])} rules)

Validation Context:
- Applicable Countries (config): {state['metadata_config']['applicable_countries']}
- Adequacy Countries (verified): {state['identified_countries'].get('adequacy_countries', [])}
- Geography Data Available: {len(self.geography_handler.all_countries)} countries

Use ReAct with Validation Knowledge Graphs:
1. REASONING: Build validation knowledge graph for rule compliance checking
2. ACTION: Validate each rule against exact format and verify all data
3. OBSERVATION: Check validation results and compliance status
4. KNOWLEDGE GRAPH: Map Validation_Results → Compliance_Status → Issues → Corrections
5. CONCLUSION: Provide fully validated rules with compliance verification

Knowledge Graph Structure:
- Entities: Rules, Validation_Checks, Countries, Roles, Compliance_Status
- Relationships: Validates, Verifies, Complies-with, Has-issue, Requires-correction

Validation Requirements:
- applicable_countries: MUST use metadata config countries
- adequacy_countries: MUST be verified actual countries (not regions like "EU")
- conditions: Each MUST have proper role assignment
- aggregated_roles: MUST include all roles from conditions
- references: MUST include Level and article/text information

Return complete validated rules array in exact required format.
Fix any validation issues and ensure full compliance."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract validation knowledge graph
            knowledge_graph = self._extract_validation_knowledge_graph(response_text)
            
            # Validate and structure rules
            validated_rules = self._comprehensive_rule_validation(response_text, state)
            
            # Update state
            state['validated_rules'] = validated_rules
            state['agent4_reasoning'] = reasoning_trace
            state['agent4_knowledge_graph'] = knowledge_graph
            state['processing_complete'] = True
            
            logger.info(f"✅ {self.name}: Validated {len(validated_rules)} rules")
            logger.info(f"📊 Validation KG: {len(knowledge_graph.get('entities', []))} entities")
            
        except Exception as e:
            logger.error(f"❌ {self.name}: Processing failed: {e}")
            state['agent4_reasoning'] = [f"ERROR: {e}"]
            state['agent4_knowledge_graph'] = {"entities": [], "relationships": []}
            state['validated_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        patterns = [
            (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
            (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
            (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
            (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
            (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
        ]
        
        for pattern, label in patterns:
            matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                trace.append(f"{label}: {match.strip()}")
        
        return trace
    
    def _extract_validation_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract validation knowledge graph"""
        kg = {"entities": [], "relationships": [], "validation_results": {}}
        
        kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
        kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
        
        if kg_match:
            kg_text = kg_match.group(1)
            
            # Extract validation entities
            entity_patterns = [
                r'Validation_Check:\s*([^,\n]+)',
                r'Compliance_Status:\s*([^,\n]+)',
                r'Issue:\s*([^,\n]+)',
                r'Rule:\s*([^,\n]+)'
            ]
            
            for pattern in entity_patterns:
                matches = re.findall(pattern, kg_text, re.IGNORECASE)
                kg["entities"].extend(matches)
        
        return kg
    
    def _comprehensive_rule_validation(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Comprehensive rule validation with Pydantic v2"""
        validated_rules = []
        
        # Try to extract validated rules from response
        json_data = self._safe_json_extract(response_text)
        if json_data:
            if isinstance(json_data, list):
                rules_data = json_data
            elif isinstance(json_data, dict):
                rules_data = [json_data]
            else:
                rules_data = state['extracted_rules']
        else:
            rules_data = state['extracted_rules']
        
        # Validate each rule comprehensively
        for rule_data in rules_data:
            try:
                # Ensure complete rule structure
                validated_rule_data = {
                    "rule_id": rule_data.get("rule_id", f"rule_{uuid.uuid4().hex[:8]}"),
                    "rule_definition": rule_data.get("rule_definition", "Legal compliance requirement"),
                    "applicable_countries": state['metadata_config']['applicable_countries'],
                    "adequacy_countries": self._verify_adequacy_countries(state['identified_countries'].get('adequacy_countries', [])),
                    "conditions": self._validate_conditions_with_roles(rule_data.get("conditions", [])),
                    "aggregated_roles": [],  # Will be populated from conditions
                    "data_category": rule_data.get("data_category", "Personal Data"),
                    "domain": "access_and_entitlements",
                    "action": rule_data.get("action", "Must ensure compliance with requirements"),
                    "reference": rule_data.get("reference", "Document - Level reference")
                }
                
                # Aggregate roles from conditions
                validated_rule_data["aggregated_roles"] = list(set(
                    cond.get('role', 'controller') for cond in validated_rule_data['conditions']
                ))
                
                # Validate with Pydantic v2
                extracted_rule = ExtractedRule.model_validate(validated_rule_data)
                validated_rules.append(extracted_rule.model_dump())
                
            except ValidationError as e:
                logger.warning(f"Rule validation failed: {e}")
                # Create fallback rule
                fallback_rule = self._create_fallback_rule(rule_data, state)
                validated_rules.append(fallback_rule)
            except Exception as e:
                logger.error(f"Unexpected error in rule validation: {e}")
                continue
        
        return validated_rules
    
    def _verify_adequacy_countries(self, adequacy_countries: List[str]) -> List[str]:
        """Verify adequacy countries are actual countries (not regions)"""
        verified = []
        for country in adequacy_countries:
            # Skip regions like "EU", "EEA"
            if len(country) == 2 and self.geography_handler.is_valid_country(country):
                verified.append(country.upper())
            elif len(country) > 2:
                # Try to convert country name to ISO
                iso = self.geography_handler.get_country_iso(country)
                if iso:
                    verified.append(iso.upper())
        return list(set(verified))
    
    def _validate_conditions_with_roles(self, conditions: List) -> List[Dict]:
        """Validate conditions with proper role assignments"""
        validated_conditions = []
        
        valid_roles = ['controller', 'processor', 'joint_controller', 'data_subject']
        
        for condition in conditions:
            if isinstance(condition, dict):
                # Ensure role is valid
                role = condition.get('role', 'controller')
                if role not in valid_roles:
                    role = 'controller'  # Default
                
                validated_condition = {
                    "condition_id": condition.get("condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    "condition_definition": condition.get("condition_definition", "Data processing condition"),
                    "fact": condition.get("fact", "data.category"),
                    "operator": condition.get("operator", "equal"),
                    "value": condition.get("value", "Personal Data"),
                    "role": role
                }
                validated_conditions.append(validated_condition)
        
        # Ensure at least one condition
        if not validated_conditions:
            validated_conditions.append({
                "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                "condition_definition": "When processing personal data",
                "fact": "data.category",
                "operator": "equal",
                "value": "Personal Data",
                "role": "controller"
            })
        
        return validated_conditions
    
    def _create_fallback_rule(self, rule_data: Dict, state: MultiAgentState) -> Dict:
        """Create fallback rule when validation fails"""
        return {
            "rule_id": f"rule_fallback_{uuid.uuid4().hex[:8]}",
            "rule_definition": str(rule_data.get("rule_definition", "Legal compliance requirement")),
            "applicable_countries": state['metadata_config']['applicable_countries'],
            "adequacy_countries": [],
            "conditions": [{
                "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                "condition_definition": "When processing personal data",
                "fact": "data.category",
                "operator": "equal",
                "value": "Personal Data",
                "role": "controller"
            }],
            "aggregated_roles": ["controller"],
            "data_category": "Personal Data",
            "domain": "access_and_entitlements",
            "action": "Must ensure compliance with data protection requirements",
            "reference": "Document - General requirement"
        }
    
    def _safe_json_extract(self, text: str) -> Optional[Union[Dict, List]]:
        """Safely extract JSON from text"""
        json_patterns = [
            r'\[.*?\]',  # Array pattern
            r'\{.*?\}'   # Object pattern
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

# Enhanced Multi-Agent Orchestrator
class MultiAgentLegalProcessor:
    """Enhanced multi-agent orchestrator with complete processing"""
    
    def __init__(self, geography_file: str):
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=MODEL_NAME,
            openai_api_key=API_KEY,
            openai_api_base=BASE_URL
        )
        
        # Initialize geography handler
        self.geography_handler = GeographyHandler(geography_file)
        
        # Initialize enhanced agents
        self.doc_parser_agent = DocumentParserAgent(self.llm)
        self.geography_agent = GeographyAgent(self.llm, self.geography_handler)
        self.rule_extraction_agent = RuleExtractionAgent(self.llm)
        self.validation_agent = ValidationAgent(self.llm, self.geography_handler)
        
        # Create enhanced workflow
        self.workflow = self._create_enhanced_workflow()
        
        logger.info("🤖 Enhanced Multi-Agent Legal Processor initialized")
        logger.info(f"📍 Geography data: {len(self.geography_handler.all_countries)} countries")
        logger.info(f"⚙️ Configuration: {MODEL_NAME}, chunks={CHUNK_SIZE}, overlap={OVERLAP_SIZE}")
    
    def _create_enhanced_workflow(self) -> StateGraph:
        """Create enhanced workflow with LangGraph"""
        
        workflow = StateGraph(MultiAgentState)
        
        # Add enhanced agent nodes
        workflow.add_node("document_parser", self.doc_parser_agent.process)
        workflow.add_node("geography_agent", self.geography_agent.process)
        workflow.add_node("rule_extraction", self.rule_extraction_agent.process)
        workflow.add_node("validation_agent", self.validation_agent.process)
        
        # Set entry point
        workflow.set_entry_point("document_parser")
        
        # Sequential processing
        workflow.add_edge("document_parser", "geography_agent")
        workflow.add_edge("geography_agent", "rule_extraction")
        workflow.add_edge("rule_extraction", "validation_agent")
        workflow.add_edge("validation_agent", END)
        
        return workflow.compile(checkpointer=MemorySaver())
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process document with enhanced complete processing"""
        
        logger.info(f"📄 Starting enhanced processing: {metadata_config.pdf_path}")
        
        # Extract complete PDF content
        pdf_processor = PDFProcessor()
        pdf_data = pdf_processor.extract_complete_text_from_pdf(metadata_config.pdf_path)
        
        if not pdf_data['complete_text']:
            raise ValueError("No text extracted from PDF")
        
        logger.info(f"📊 PDF extracted: {pdf_data['total_pages']} pages, {pdf_data['total_length']} characters")
        
        # Initialize enhanced state
        initial_state = MultiAgentState(
            document_text=pdf_data['complete_text'],
            document_chunks=[],
            metadata_config=metadata_config.model_dump(),
            geography_data=self.geography_handler.geography_data,
            parsed_sections={},
            knowledge_graph={"entities": [], "relationships": []},
            identified_countries={},
            extracted_rules=[],
            validated_rules=[],
            agent1_reasoning=[],
            agent1_knowledge_graph={},
            agent2_reasoning=[],
            agent2_knowledge_graph={},
            agent3_reasoning=[],
            agent3_knowledge_graph={},
            agent4_reasoning=[],
            agent4_knowledge_graph={},
            current_agent="DocumentParserAgent",
            processing_complete=False,
            chunks_processed=0,
            total_chunks=0
        )
        
        # Run enhanced workflow
        config = {"configurable": {"thread_id": f"enhanced_{uuid.uuid4().hex[:8]}"}}
        
        try:
            final_state = self.workflow.invoke(initial_state, config)
            
            # Convert to Pydantic models
            validated_rules = []
            for rule_data in final_state['validated_rules']:
                try:
                    rule = ExtractedRule.model_validate(rule_data)
                    validated_rules.append(rule)
                except ValidationError as e:
                    logger.warning(f"Final rule validation failed: {e}")
                    continue
            
            # Log comprehensive summary
            self._log_enhanced_summary(final_state, validated_rules, pdf_data)
            
            return validated_rules
            
        except Exception as e:
            logger.error(f"Enhanced processing failed: {e}")
            raise
    
    def _log_enhanced_summary(self, final_state: MultiAgentState, rules: List[ExtractedRule], pdf_data: Dict):
        """Log comprehensive processing summary"""
        
        logger.info("🎯 Enhanced Multi-Agent Processing Complete!")
        logger.info(f"📄 PDF: {pdf_data['total_pages']} pages, {pdf_data['total_length']} characters")
        logger.info(f"🧩 Chunks processed: {final_state.get('chunks_processed', 0)}")
        logger.info(f"📂 Sections parsed: {len(final_state['parsed_sections'])}")
        logger.info(f"🌍 Countries identified: {len(final_state['identified_countries'].get('mentioned_countries', []))}")
        logger.info(f"⚖️ Rules extracted: {len(final_state['extracted_rules'])}")
        logger.info(f"✅ Rules validated: {len(rules)}")
        
        # Log knowledge graphs
        for i, agent_name in enumerate(['DocumentParserAgent', 'GeographyAgent', 'RuleExtractionAgent', 'ValidationAgent'], 1):
            kg = final_state.get(f'agent{i}_knowledge_graph', {})
            entities = len(kg.get('entities', []))
            relationships = len(kg.get('relationships', []))
            logger.info(f"🧠 {agent_name} KG: {entities} entities, {relationships} relationships")
        
        # Log role distribution
        if rules:
            all_roles = set()
            for rule in rules:
                all_roles.update(rule.aggregated_roles)
            logger.info(f"👥 Roles identified: {', '.join(all_roles)}")
            
            # Sample rule
            sample_rule = rules[0]
            logger.info("📋 Sample validated rule:")
            logger.info(f"   🆔 ID: {sample_rule.rule_id}")
            logger.info(f"   📝 Definition: {sample_rule.rule_definition[:100]}...")
            logger.info(f"   🌍 Applicable: {sample_rule.applicable_countries}")
            logger.info(f"   ✅ Adequacy: {sample_rule.adequacy_countries}")
            logger.info(f"   👥 Roles: {sample_rule.aggregated_roles}")
            logger.info(f"   🏷️ Domain: {sample_rule.domain}")

# Enhanced Pipeline
class LegalRuleExtractionPipeline:
    """Enhanced pipeline with complete processing"""
    
    def __init__(self, geography_file: str):
        self.processor = MultiAgentLegalProcessor(geography_file)
        logger.info("🚀 Enhanced Legal Rule Extraction Pipeline initialized")
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process single document with complete coverage"""
        return await self.processor.process_document(metadata_config)
    
    async def process_multiple_documents(self, config_file: str) -> List[ExtractedRule]:
        """Process multiple documents"""
        logger.info(f"📁 Processing multiple documents from: {config_file}")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                configs_data = json.load(f)
        except Exception as e:
            logger.error(f"Config loading failed: {e}")
            raise
        
        all_rules = []
        
        for config_data in configs_data:
            try:
                metadata_config = MetadataConfig.model_validate(config_data)
                rules = await self.process_document(metadata_config)
                all_rules.extend(rules)
                
            except ValidationError as e:
                logger.error(f"Invalid config: {e}")
                continue
            except Exception as e:
                logger.error(f"Processing failed for {config_data.get('pdf_path', 'unknown')}: {e}")
                continue
        
        return all_rules
    
    def save_results(self, rules: List[ExtractedRule], output_dir: str):
        """Save enhanced results with knowledge graphs"""
        logger.info(f"💾 Saving enhanced results to {output_dir}")
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Convert to dictionaries
        rules_dicts = [rule.model_dump() for rule in rules]
        
        # Save JSON
        json_file = Path(output_dir) / "extracted_rules.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(rules_dicts, f, indent=2, ensure_ascii=False)
        
        # Save enhanced CSV with role analysis
        csv_file = Path(output_dir) / "extracted_rules.csv"
        if rules_dicts:
            flattened_rules = []
            for rule in rules_dicts:
                flat_rule = {
                    "rule_id": rule["rule_id"],
                    "rule_definition": rule["rule_definition"],
                    "applicable_countries": json.dumps(rule["applicable_countries"]),
                    "adequacy_countries": json.dumps(rule["adequacy_countries"]),
                    "aggregated_roles": json.dumps(rule["aggregated_roles"]),
                    "data_category": rule["data_category"],
                    "domain": rule["domain"],
                    "action": rule["action"],
                    "reference": rule["reference"],
                    "conditions_count": len(rule["conditions"]),
                    "conditions_details": json.dumps(rule["conditions"])
                }
                flattened_rules.append(flat_rule)
            
            df = pd.DataFrame(flattened_rules)
            df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # Save analysis summary
        summary_file = Path(output_dir) / "processing_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                "processing_summary": f"Enhanced processing completed: {len(rules)} rules",
                "agents_used": [
                    "DocumentParserAgent (with knowledge graphs)",
                    "GeographyAgent (with location knowledge)",
                    "RuleExtractionAgent (with legal knowledge)",
                    "ValidationAgent (with compliance knowledge)"
                ],
                "methodology": "Enhanced Chain of Thought with ReAct prompting and Knowledge Graph construction",
                "features": [
                    "NO truncation - complete document processing",
                    "Overlapping chunks for full coverage",
                    "Knowledge graph construction in all agents",
                    "Role assignment for all conditions",
                    "Geography data verification",
                    "Enhanced ReAct prompting"
                ],
                "configuration": {
                    "model": MODEL_NAME,
                    "chunk_size": CHUNK_SIZE,
                    "overlap_size": OVERLAP_SIZE,
                    "max_chunks": MAX_CHUNKS
                }
            }, f, indent=2)
        
        logger.info(f"✅ Enhanced results saved: {json_file}, {csv_file}, {summary_file}")

# CLI Interface
async def main():
    """Enhanced CLI interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced Multi-Agent Legal Rule Extraction")
    parser.add_argument("--config", required=True, help="Metadata configuration JSON file")
    parser.add_argument("--geography", required=True, help="Geography JSON file")
    parser.add_argument("--output", default="./output", help="Output directory")
    
    args = parser.parse_args()
    
    try:
        # Initialize enhanced pipeline
        pipeline = LegalRuleExtractionPipeline(args.geography)
        
        # Process documents
        rules = await pipeline.process_multiple_documents(args.config)
        
        # Save results
        pipeline.save_results(rules, args.output)
        
        logger.info(f"🎉 Enhanced processing complete! Generated {len(rules)} rules")
        
        # Display enhanced sample
        if rules:
            sample_rule = rules[0]
            logger.info("📋 Sample enhanced rule:")
            logger.info(f"   🆔 Rule ID: {sample_rule.rule_id}")
            logger.info(f"   📝 Definition: {sample_rule.rule_definition[:100]}...")
            logger.info(f"   🌍 Applicable Countries: {sample_rule.applicable_countries}")
            logger.info(f"   ✅ Adequacy Countries: {sample_rule.adequacy_countries}")
            logger.info(f"   👥 Aggregated Roles: {sample_rule.aggregated_roles}")
            logger.info(f"   🏷️ Data Category: {sample_rule.data_category}")
            logger.info(f"   🎯 Domain: {sample_rule.domain}")
            logger.info(f"   ⚡ Action: {sample_rule.action[:100]}...")
            logger.info(f"   📍 Reference: {sample_rule.reference}")
            logger.info(f"   🧩 Conditions: {len(sample_rule.conditions)} with roles")
        
        return 0
        
    except Exception as e:
        logger.error(f"Enhanced pipeline failed: {e}")
        return 1

if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
