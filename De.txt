import os
import pandas as pd
from typing import List, Dict
import logging
from pathlib import Path
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.embeddings import OpenAIEmbedding
from llama_index.schema import TextNode
from llama_index.vector_stores import SimpleVectorStore
from llama_index.storage.storage_context import StorageContext

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GuaranteedPBTPredictor:
    def __init__(self):
        self.embed_model = OpenAIEmbedding(
            model="text-embedding-3-large",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.service_context = ServiceContext.from_defaults(
            embed_model=self.embed_model
        )
        self.min_confidence = 0.01  # Minimum confidence score for fallback matches

    def _ensure_predictions(self, matches: List[Dict], target_metadata: Dict) -> List[Dict]:
        """Ensure exactly 4 predictions using fallback strategies"""
        # If not enough matches, create low-confidence fallbacks
        while len(matches) < 4:
            fallback = {
                "pbt_name": "No confident match",
                "pbt_definition": "Insufficient data for confident match",
                "similarity_score": max(self.min_confidence, 0.01 * len(matches))
            }
            matches.append(fallback)
        
        return matches[:4]

    def build_robust_index(self, df: pd.DataFrame, is_source: bool) -> VectorStoreIndex:
        """Build index with error handling"""
        try:
            nodes = [
                TextNode(
                    text=f"Title: {row['name']}\nDetails: {row['description']}" if is_source 
                         else f"Term: {row['pbt-name']}\nDefinition: {row['pbt-definition']}",
                    metadata=row.to_dict()
                )
                for _, row in df.iterrows()
            ]
            
            vector_store = SimpleVectorStore()
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            return VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                service_context=self.service_context
            )
        except Exception as e:
            logger.error(f"Index build failed: {str(e)}")
            raise

    def get_guaranteed_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame) -> pd.DataFrame:
        """Guarantee 4 matches per entry with fallback mechanism"""
        # Build indices
        target_index = self.build_robust_index(target_df, is_source=False)
        
        results = []
        for _, source_row in source_df.iterrows():
            try:
                query_text = f"Title: {source_row['name']}\nDetails: {source_row['description']}"
                query_embedding = self.embed_model.get_text_embedding(query_text)
                
                # Get matches with error handling
                matches = target_index.vector_store.similarity_search(
                    query_embedding, 
                    top_k=6  # Get extra matches for redundancy
                )
                
                # Convert to usable format
                processed_matches = [
                    {
                        "pbt_name": m.metadata.get("pbt-name", "Unknown"),
                        "pbt_definition": m.metadata.get("pbt-definition", "Not available"),
                        "similarity_score": m.score
                    }
                    for m in matches
                ]
                
                # Apply confidence threshold and ensure 4 predictions
                valid_matches = [m for m in processed_matches if m["similarity_score"] > self.min_confidence]
                final_matches = self._ensure_predictions(valid_matches, target_df.to_dict('records'))
                
            except Exception as e:
                logger.warning(f"Failed processing row {source_row['name']}: {str(e)}")
                final_matches = self._ensure_predictions([], [])

            results.append({
                "source_name": source_row["name"],
                "source_description": source_row["description"],
                "matches": final_matches[:4]  # Ensure exactly 4 predictions
            })
        
        return self._format_output(results)

    def _format_output(self, results: List[Dict]) -> pd.DataFrame:
        """Create structured output with guaranteed 4 columns"""
        formatted = []
        for entry in results:
            for rank, match in enumerate(entry["matches"][:4], 1):  # Force 4 entries
                formatted.append({
                    "source_name": entry["source_name"],
                    "source_description": entry["source_description"],
                    f"match_{rank}_pbt_name": match["pbt_name"],
                    f"match_{rank}_score": match["similarity_score"],
                    f"match_{rank}_definition": match["pbt_definition"]
                })
        
        # Pivot to wide format
        df = pd.DataFrame(formatted)
        return df.groupby(["source_name", "source_description"]).first().reset_index()

    def save_guaranteed_results(self, df: pd.DataFrame, output_path: str) -> None:
        """Save results with guaranteed columns"""
        required_columns = {
            f"match_{i}_{field}" 
            for i in range(1,5) 
            for field in ["pbt_name", "score", "definition"]
        }
        
        # Ensure all columns exist
        for col in required_columns:
            if col not in df.columns:
                df[col] = "Not available"
        
        df.to_csv(output_path, index=False)
        logger.info(f"Guaranteed results saved to {output_path}")

# Usage
if __name__ == "__main__":
    try:
        # Load data
        data_dir = Path("data")
        source_df = pd.read_csv(data_dir/"source.csv").fillna("")
        target_df = pd.read_csv(data_dir/"target.csv").fillna("")
        
        # Initialize predictor
        predictor = GuaranteedPBTPredictor()
        
        # Get matches
        result_df = predictor.get_guaranteed_matches(source_df, target_df)
        
        # Save results
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        predictor.save_guaranteed_results(result_df, output_dir/"guaranteed_pbt_matches.csv")
        
    except Exception as e:
        logger.error(f"Critical failure: {str(e)}")
