# backend/app/core/config.py (Fixed for Pydantic v2)
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field
from typing import List
import os
from functools import lru_cache

class Settings(BaseSettings):
    """Application settings with environment variable support"""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        case_sensitive=True,
        extra="ignore"
    )
    
    # API Configuration
    API_V1_STR: str = "/api/v1"
    PROJECT_NAME: str = "Deep Research Chatbot"
    
    # CORS settings
    ALLOWED_ORIGINS: List[str] = Field(
        default=["http://localhost:3000", "http://127.0.0.1:3000"],
        description="Allowed CORS origins"
    )
    
    # Session settings
    SESSION_TIMEOUT_MINUTES: int = Field(default=60, description="Session timeout in minutes")
    MAX_CONCURRENT_SESSIONS: int = Field(default=100, description="Maximum concurrent sessions")
    
    # Research engine settings
    MAX_RESEARCH_TIME_MINUTES: int = Field(default=10, description="Maximum research time")
    ENABLE_KNOWLEDGE_GRAPH: bool = Field(default=True, description="Enable knowledge graph generation")
    
    # Logging
    LOG_LEVEL: str = Field(default="INFO", description="Logging level")
    
    # WebSocket settings
    WEBSOCKET_HEARTBEAT_INTERVAL: int = Field(default=30, description="WebSocket heartbeat interval")
    
    # Knowledge graph settings
    MAX_GRAPH_NODES: int = Field(default=50, description="Maximum nodes in knowledge graph")
    MAX_GRAPH_EDGES: int = Field(default=100, description="Maximum edges in knowledge graph")

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance"""
    return Settings()

# backend/app/core/session_manager.py (Fixed async issues)
import uuid
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Optional, Any
import logging
from dataclasses import dataclass, field
from .config import get_settings

logger = logging.getLogger(__name__)

@dataclass
class SessionData:
    """Session data container"""
    session_id: str
    user_id: str
    created_at: datetime
    last_activity: datetime
    namespace: str
    conversation_history: list = field(default_factory=list)
    research_cache: dict = field(default_factory=dict)
    metadata: dict = field(default_factory=dict)
    
    def update_activity(self):
        """Update last activity timestamp"""
        self.last_activity = datetime.utcnow()
    
    def is_expired(self, timeout_minutes: int) -> bool:
        """Check if session has expired"""
        return datetime.utcnow() - self.last_activity > timedelta(minutes=timeout_minutes)
    
    def to_dict(self) -> dict:
        """Convert to dictionary for serialization"""
        return {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "created_at": self.created_at.isoformat(),
            "last_activity": self.last_activity.isoformat(),
            "namespace": self.namespace,
            "conversation_count": len(self.conversation_history),
            "research_cache_size": len(self.research_cache),
            "metadata": self.metadata
        }

class SessionManager:
    """Manages user sessions with automatic cleanup and dynamic namespaces"""
    
    def __init__(self):
        self.sessions: Dict[str, SessionData] = {}
        self.settings = get_settings()
        self._cleanup_task = None
        self._should_stop_cleanup = False
    
    def start_cleanup_task(self):
        """Start background task for session cleanup"""
        if not self._cleanup_task or self._cleanup_task.done():
            self._should_stop_cleanup = False
            loop = asyncio.get_event_loop()
            self._cleanup_task = loop.create_task(self._cleanup_expired_sessions())
    
    async def _cleanup_expired_sessions(self):
        """Background task to clean up expired sessions"""
        while not self._should_stop_cleanup:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes
                await self._remove_expired_sessions()
            except asyncio.CancelledError:
                logger.info("Session cleanup task cancelled")
                break
            except Exception as e:
                logger.error(f"Error in session cleanup: {e}")
                await asyncio.sleep(60)  # Wait a minute before retrying
    
    async def _remove_expired_sessions(self):
        """Remove expired sessions"""
        expired_sessions = []
        timeout_minutes = self.settings.SESSION_TIMEOUT_MINUTES
        
        for session_id, session in self.sessions.items():
            if session.is_expired(timeout_minutes):
                expired_sessions.append(session_id)
        
        for session_id in expired_sessions:
            logger.info(f"Removing expired session: {session_id}")
            del self.sessions[session_id]
    
    def _generate_namespace(self, user_id: str, session_id: str) -> str:
        """Generate dynamic namespace based on user and session"""
        # Create a unique namespace that's deterministic but private
        timestamp = datetime.utcnow().strftime("%Y%m%d")
        short_session = session_id[:8]
        return f"research_{user_id}_{timestamp}_{short_session}"
    
    async def create_session(self, user_id: Optional[str] = None) -> SessionData:
        """Create a new session with dynamic namespace"""
        # Check session limits
        if len(self.sessions) >= self.settings.MAX_CONCURRENT_SESSIONS:
            # Remove oldest session
            oldest_session_id = min(
                self.sessions.keys(),
                key=lambda x: self.sessions[x].last_activity
            )
            logger.warning(f"Session limit reached, removing oldest: {oldest_session_id}")
            del self.sessions[oldest_session_id]
        
        # Generate session ID and user ID
        session_id = str(uuid.uuid4())
        user_id = user_id or f"user_{uuid.uuid4().hex[:8]}"
        
        # Create session with dynamic namespace
        now = datetime.utcnow()
        session = SessionData(
            session_id=session_id,
            user_id=user_id,
            created_at=now,
            last_activity=now,
            namespace=self._generate_namespace(user_id, session_id)
        )
        
        self.sessions[session_id] = session
        logger.info(f"Created session {session_id} for user {user_id} with namespace {session.namespace}")
        
        # Start cleanup task if not running
        self.start_cleanup_task()
        
        return session
    
    async def get_session(self, session_id: str) -> Optional[SessionData]:
        """Get session by ID"""
        session = self.sessions.get(session_id)
        if session:
            session.update_activity()
            return session
        return None
    
    async def update_session_data(self, session_id: str, data: dict) -> bool:
        """Update session data"""
        session = await self.get_session(session_id)
        if session:
            session.metadata.update(data)
            session.update_activity()
            return True
        return False
    
    async def add_conversation_message(self, session_id: str, message: dict) -> bool:
        """Add message to conversation history"""
        session = await self.get_session(session_id)
        if session:
            session.conversation_history.append({
                **message,
                "timestamp": datetime.utcnow().isoformat()
            })
            session.update_activity()
            
            # Limit conversation history size
            if len(session.conversation_history) > 100:
                session.conversation_history = session.conversation_history[-50:]
            
            return True
        return False
    
    async def cache_research_result(self, session_id: str, query: str, result: dict) -> bool:
        """Cache research result for the session"""
        session = await self.get_session(session_id)
        if session:
            # Create a cache key from the query
            cache_key = str(hash(query.lower().strip()))
            session.research_cache[cache_key] = {
                "query": query,
                "result": result,
                "timestamp": datetime.utcnow().isoformat()
            }
            session.update_activity()
            
            # Limit cache size
            if len(session.research_cache) > 10:
                # Remove oldest cached result
                oldest_key = min(
                    session.research_cache.keys(),
                    key=lambda x: session.research_cache[x]["timestamp"]
                )
                del session.research_cache[oldest_key]
            
            return True
        return False
    
    async def get_cached_research(self, session_id: str, query: str) -> Optional[dict]:
        """Get cached research result"""
        session = await self.get_session(session_id)
        if session:
            cache_key = str(hash(query.lower().strip()))
            cached = session.research_cache.get(cache_key)
            if cached:
                # Check if cache is not too old (1 hour)
                cached_time = datetime.fromisoformat(cached["timestamp"])
                if datetime.utcnow() - cached_time < timedelta(hours=1):
                    session.update_activity()
                    return cached["result"]
        return None
    
    async def delete_session(self, session_id: str) -> bool:
        """Delete a session"""
        if session_id in self.sessions:
            logger.info(f"Deleting session: {session_id}")
            del self.sessions[session_id]
            return True
        return False
    
    async def get_session_stats(self) -> dict:
        """Get session statistics"""
        total_sessions = len(self.sessions)
        active_sessions = sum(
            1 for session in self.sessions.values()
            if not session.is_expired(self.settings.SESSION_TIMEOUT_MINUTES)
        )
        
        return {
            "total_sessions": total_sessions,
            "active_sessions": active_sessions,
            "expired_sessions": total_sessions - active_sessions,
            "max_sessions": self.settings.MAX_CONCURRENT_SESSIONS
        }
    
    async def cleanup(self):
        """Cleanup resources"""
        self._should_stop_cleanup = True
        
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        self.sessions.clear()
        logger.info("Session manager cleanup complete")

# backend/app/main.py (Fixed dependency injection and startup)
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
import sys
import os
from typing import Dict

# Import our API routers
from app.api import chat, research, knowledge_graph
from app.core.config import get_settings
from app.core.session_manager import SessionManager
from app.core.research_engine import ResearchEngineWrapper

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global instances
session_manager = SessionManager()
research_engine_wrapper = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan events"""
    # Startup
    logger.info("üöÄ Initializing Deep Research Chatbot API")
    
    global research_engine_wrapper
    try:
        # Initialize the research engine wrapper
        research_engine_wrapper = ResearchEngineWrapper()
        await research_engine_wrapper.initialize()
        logger.info("‚úÖ Research engine initialized successfully")
        
        # Start session manager cleanup task
        session_manager.start_cleanup_task()
        logger.info("‚úÖ Session manager started")
        
        # Set global reference for dependency injection
        app.state.research_engine = research_engine_wrapper
        app.state.session_manager = session_manager
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize application: {e}")
        sys.exit(1)
    
    yield
    
    # Shutdown
    logger.info("üîÑ Shutting down Deep Research Chatbot API")
    if research_engine_wrapper:
        await research_engine_wrapper.cleanup()
    
    await session_manager.cleanup()
    logger.info("‚úÖ Shutdown complete")

# Create FastAPI app with lifespan management
app = FastAPI(
    title="Deep Research Chatbot API",
    description="Advanced AI-powered research assistant with knowledge graph generation",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configure CORS
settings = get_settings()
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Dependency injection functions
async def get_session_manager(request: Request):
    """Dependency to get session manager"""
    return request.app.state.session_manager

async def get_research_engine(request: Request):
    """Dependency to get research engine"""
    return request.app.state.research_engine

# Update router dependencies by patching the dependency functions
chat.get_session_manager = get_session_manager
chat.get_research_engine = get_research_engine
research.get_session_manager = get_session_manager
research.get_research_engine = get_research_engine
knowledge_graph.get_session_manager = get_session_manager

# Include API routers
app.include_router(chat.router, prefix="/api/v1/chat", tags=["chat"])
app.include_router(research.router, prefix="/api/v1/research", tags=["research"])
app.include_router(knowledge_graph.router, prefix="/api/v1/knowledge-graph", tags=["knowledge-graph"])

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, session_id: str):
        await websocket.accept()
        self.active_connections[session_id] = websocket
        logger.info(f"WebSocket connected: {session_id}")
    
    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]
            logger.info(f"WebSocket disconnected: {session_id}")
    
    async def send_personal_message(self, message: dict, session_id: str):
        if session_id in self.active_connections:
            try:
                await self.active_connections[session_id].send_json(message)
            except Exception as e:
                logger.error(f"Error sending message to {session_id}: {e}")
                self.disconnect(session_id)

connection_manager = ConnectionManager()

# WebSocket endpoint for real-time communication
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await connection_manager.connect(websocket, session_id)
    try:
        while True:
            # Keep connection alive and handle any incoming messages
            data = await websocket.receive_text()
            # Echo back for now - can be extended for real-time features
            await connection_manager.send_personal_message(
                {"type": "echo", "data": data}, session_id
            )
    except WebSocketDisconnect:
        connection_manager.disconnect(session_id)
    except Exception as e:
        logger.error(f"WebSocket error for {session_id}: {e}")
        connection_manager.disconnect(session_id)

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Check if research engine is available
        engine_status = "healthy" if research_engine_wrapper else "unavailable"
        
        return {
            "status": "healthy",
            "research_engine": engine_status,
            "active_sessions": len(session_manager.sessions),
            "active_websockets": len(connection_manager.active_connections)
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service unavailable")

# Root endpoint
@app.get("/")
async def root():
    """Root endpoint with API information"""
    return {
        "message": "Deep Research Chatbot API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "features": [
            "Quick Chat with AI Assistant",
            "Deep Multi-Agent Research",
            "Real-time Knowledge Graph Generation",
            "Session Management",
            "WebSocket Support"
        ]
    }

# Error handlers
@app.exception_handler(404)
async def not_found_handler(request, exc):
    return {"error": "Endpoint not found", "detail": str(exc)}

@app.exception_handler(500)
async def internal_error_handler(request, exc):
    logger.error(f"Internal server error: {exc}")
    return {"error": "Internal server error", "detail": "Please try again later"}

if __name__ == "__main__":
    import uvicorn
    
    # Development server
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
