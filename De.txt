"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Completely error-free implementation with o3-mini and proper tool calling
Enhanced with Chain of Thought, Mixture of Thought, and Mixture of Reasoning
Focused on Data Governance Rules (Usage, Transfer, Storage, Access)
"""

import json
import re
import time
import os
from typing import List, Dict, Any, Optional, Annotated, Sequence, TypedDict, Literal, Union
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# ========================= Global Configuration =========================

# Global model configuration for o3-mini
OPENAI_MODEL = "o3-mini"
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
REASONING_EFFORT = "high"  # high, medium, or low

# Global model instance
def get_model():
    """Get configured o3-mini model instance"""
    return ChatOpenAI(
        model=OPENAI_MODEL,
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
        model_kwargs={
            "reasoning_effort": REASONING_EFFORT,
            "max_completion_tokens": 4000
        }
    )


# ========================= Utility Functions =========================

def safe_str(obj: Any) -> str:
    """Safely convert any object to string"""
    if obj is None:
        return ""
    if isinstance(obj, str):
        return obj
    if isinstance(obj, (list, tuple)):
        return " ".join(str(item) for item in obj)
    if isinstance(obj, dict):
        return json.dumps(obj, default=str)
    return str(obj)


def safe_json_parse(content: str) -> Union[Dict, List, None]:
    """Safely parse JSON content with error handling"""
    if not content or not isinstance(content, str):
        return None
    
    try:
        content = content.strip()
        
        # Remove markdown formatting
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        # Try to parse JSON
        if content.startswith('{') or content.startswith('['):
            return json.loads(content)
    except (json.JSONDecodeError, AttributeError, IndexError):
        pass
    
    return None


def extract_content_safely(message: Any) -> str:
    """Safely extract content from any message type"""
    if not hasattr(message, 'content'):
        return ""
    
    content = message.content
    return safe_str(content).strip()


# ========================= State Management =========================

class AgentState(TypedDict):
    """State for the legislation processing agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str
    current_phase: str
    analysis_count: int
    extraction_count: int
    extracted_rules: Dict[str, List[Dict[str, Any]]]
    json_rules: List[Dict[str, Any]]
    validation_results: Dict[str, Any]
    reasoning_pathways: List[Dict[str, Any]]
    reasoning_steps: List[str]
    error_log: List[str]


# ========================= Pydantic Models for Tools =========================

class AnalyzeWithReasoningInput(BaseModel):
    legislation_text: str = Field(..., description="Legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Reasoning pathway: structural/semantic/logical/contextual/compliance")
    reasoning_mode: str = Field(..., description="Reasoning mode: deductive/inductive/abductive/analogical/causal")
    focus_domain: str = Field(..., description="Focus domain: data_usage/data_transfer/data_storage/data_access")


class ExtractDataRulesInput(BaseModel):
    legislation_text: str = Field(..., description="Legislation text to extract rules from")
    data_domain: str = Field(..., description="Domain: data_usage/data_transfer/data_storage/data_access")


class SynthesizeRulesInput(BaseModel):
    all_extracted_rules: str = Field(..., description="JSON string of all extracted rules by domain")


class ConvertToJsonRulesInput(BaseModel):
    synthesized_rules: str = Field(..., description="JSON string of synthesized rules to convert")


class ValidateJsonRulesInput(BaseModel):
    json_rules: str = Field(..., description="JSON string of rules to validate")


# ========================= Tool Definitions =========================

@tool(args_schema=AnalyzeWithReasoningInput)
def analyze_with_reasoning(
    legislation_text: str,
    reasoning_pathway: str,
    reasoning_mode: str,
    focus_domain: str
) -> str:
    """
    Analyze legislation using Chain of Thought, Mixture of Thought, and Mixture of Reasoning.
    
    This implements:
    - Mixture of Thought: Different analytical pathways (structural, semantic, logical, contextual, compliance)
    - Mixture of Reasoning: Different reasoning modes (deductive, inductive, abductive, analogical, causal)
    - Chain of Thought: Step-by-step analysis process
    """
    
    # Mixture of Thought - Different analytical pathways
    pathway_prompts = {
        "structural": f"""
        STRUCTURAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify main sections containing {focus_domain} provisions
        2. Map hierarchical organization of {focus_domain} rules
        3. Find cross-references between {focus_domain} sections
        4. Analyze document structure patterns for {focus_domain}
        5. Extract structural dependencies in {focus_domain} rules
        """,
        
        "semantic": f"""
        SEMANTIC ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify key definitions related to {focus_domain}
        2. Extract explicit meanings in {focus_domain} provisions
        3. Infer implicit meanings and legislative intent for {focus_domain}
        4. Analyze stakeholder roles and obligations in {focus_domain}
        5. Map semantic relationships between {focus_domain} concepts
        """,
        
        "logical": f"""
        LOGICAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Identify IF-THEN conditional statements for {focus_domain}
        2. Extract MUST/SHALL requirements for {focus_domain}
        3. Find MUST NOT/SHALL NOT prohibitions for {focus_domain}
        4. Map logical operators (AND, OR, NOT) in {focus_domain} rules
        5. Analyze cause-effect relationships in {focus_domain}
        """,
        
        "contextual": f"""
        CONTEXTUAL ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Understand regulatory environment affecting {focus_domain}
        2. Identify industry-specific requirements for {focus_domain}
        3. Extract temporal conditions and deadlines for {focus_domain}
        4. Find exceptions and special cases in {focus_domain}
        5. Analyze contextual triggers and circumstances for {focus_domain}
        """,
        
        "compliance": f"""
        COMPLIANCE ANALYSIS for {focus_domain}:
        
        Chain of Thought Steps:
        1. Extract mandatory obligations for {focus_domain}
        2. Identify penalties for non-compliance with {focus_domain}
        3. Find audit and reporting requirements for {focus_domain}
        4. Analyze enforcement mechanisms for {focus_domain}
        5. Map compliance verification processes for {focus_domain}
        """
    }
    
    # Mixture of Reasoning - Different reasoning modes
    mode_prompts = {
        "deductive": "Apply DEDUCTIVE reasoning: Start from general data governance principles and derive specific rules",
        "inductive": "Apply INDUCTIVE reasoning: Examine specific examples and patterns to infer general rules",
        "abductive": "Apply ABDUCTIVE reasoning: Find the best explanation for why these provisions exist",
        "analogical": "Apply ANALOGICAL reasoning: Compare with known data governance frameworks",
        "causal": "Apply CAUSAL reasoning: Identify cause-effect chains and triggers"
    }
    
    analysis_prompt = f"""
    You are an expert legal analyst using advanced reasoning techniques.
    
    REASONING FRAMEWORK:
    PATHWAY: {reasoning_pathway.upper()}
    {pathway_prompts.get(reasoning_pathway, pathway_prompts["structural"])}
    
    REASONING MODE: {reasoning_mode.upper()}  
    {mode_prompts.get(reasoning_mode, mode_prompts["deductive"])}
    
    LEGISLATION TEXT:
    {legislation_text}
    
    Provide comprehensive analysis focusing on {focus_domain} with clear findings and insights.
    """
    
    try:
        model = get_model()
        response = model.invoke(analysis_prompt)
        content = extract_content_safely(response)
        
        return f"ANALYSIS COMPLETE - {reasoning_pathway}+{reasoning_mode} for {focus_domain}: {content}"
        
    except Exception as e:
        return f"ANALYSIS ERROR: {safe_str(e)}"


@tool(args_schema=ExtractDataRulesInput)
def extract_data_rules(legislation_text: str, data_domain: str) -> str:
    """Extract specific data governance rules using Chain of Thought methodology"""
    
    domain_instructions = {
        "data_usage": """
        DATA USAGE RULES EXTRACTION - Chain of Thought Process:
        1. IDENTIFY: Scan for data usage, processing, utilization mentions
        2. CATEGORIZE: Group by consent-based, legitimate interest, etc.
        3. EXTRACT CONDITIONS: What circumstances permit/prohibit usage?
        4. EXTRACT REQUIREMENTS: What must be done before/during usage?
        5. EXTRACT PROHIBITIONS: What uses are forbidden?
        6. EXTRACT CONSEQUENCES: What happens when violated?
        """,
        
        "data_transfer": """
        DATA TRANSFER RULES EXTRACTION - Chain of Thought Process:
        1. IDENTIFY: Find transfer, sharing, transmission provisions
        2. CATEGORIZE: Internal vs external vs cross-border
        3. EXTRACT CONDITIONS: When are transfers permitted?
        4. EXTRACT REQUIREMENTS: What safeguards, agreements needed?
        5. EXTRACT PROHIBITIONS: What transfers are forbidden?
        6. EXTRACT CONSEQUENCES: Penalties for unauthorized transfers
        """,
        
        "data_storage": """
        DATA STORAGE RULES EXTRACTION - Chain of Thought Process:
        1. IDENTIFY: Locate storage, retention, deletion provisions
        2. CATEGORIZE: By data type, duration, security level
        3. EXTRACT CONDITIONS: When must data be stored/deleted?
        4. EXTRACT REQUIREMENTS: Security, encryption, backup needs
        5. EXTRACT PROHIBITIONS: Storage restrictions
        6. EXTRACT CONSEQUENCES: Penalties for storage violations
        """,
        
        "data_access": """
        DATA ACCESS RULES EXTRACTION - Chain of Thought Process:
        1. IDENTIFY: Find access rights, permissions provisions
        2. CATEGORIZE: By user type, data type, access level
        3. EXTRACT CONDITIONS: Who can access what when?
        4. EXTRACT REQUIREMENTS: Authentication, authorization needs
        5. EXTRACT PROHIBITIONS: Access restrictions
        6. EXTRACT CONSEQUENCES: Penalties for unauthorized access
        """
    }
    
    extraction_prompt = f"""
    Extract {data_domain} rules from this legislation:
    
    {domain_instructions.get(data_domain, "Extract relevant rules")}
    
    LEGISLATION:
    {legislation_text}
    
    Return ONLY a JSON array with this structure:
    [
        {{
            "rule_id": "unique_id",
            "domain": "{data_domain}",
            "description": "rule description",
            "conditions": ["condition1", "condition2"],
            "requirements": ["requirement1", "requirement2"],
            "prohibitions": ["prohibition1", "prohibition2"],
            "consequences": ["consequence1", "consequence2"],
            "confidence": 0.9
        }}
    ]
    
    Return ONLY the JSON array, no other text.
    """
    
    try:
        model = get_model()
        response = model.invoke(extraction_prompt)
        content = extract_content_safely(response)
        
        # Ensure we return valid JSON or a fallback
        parsed = safe_json_parse(content)
        if parsed and isinstance(parsed, list):
            return json.dumps(parsed)
        else:
            # Fallback rule if parsing fails
            fallback_rule = [{
                "rule_id": f"{data_domain}_fallback",
                "domain": data_domain,
                "description": f"Fallback rule for {data_domain}",
                "conditions": [],
                "requirements": [],
                "prohibitions": [],
                "consequences": [],
                "confidence": 0.5
            }]
            return json.dumps(fallback_rule)
            
    except Exception as e:
        # Return error as JSON
        error_rule = [{
            "rule_id": f"{data_domain}_error",
            "domain": data_domain,
            "description": f"Error extracting {data_domain} rules: {safe_str(e)}",
            "conditions": [],
            "requirements": [],
            "prohibitions": [],
            "consequences": [],
            "confidence": 0.0
        }]
        return json.dumps(error_rule)


@tool(args_schema=SynthesizeRulesInput)
def synthesize_rules(all_extracted_rules: str) -> str:
    """Synthesize rules using convergent reasoning from divergent analysis"""
    
    synthesis_prompt = f"""
    Synthesize these extracted rules into a coherent set:
    
    EXTRACTED RULES:
    {all_extracted_rules}
    
    SYNTHESIS REQUIREMENTS:
    1. Remove duplicates
    2. Merge related rules
    3. Resolve conflicts
    4. Assign priorities (1-100)
    
    Return ONLY a JSON array of synthesized rules with this structure:
    [
        {{
            "rule_id": "synth_rule_id",
            "domain": "domain_name",
            "description": "clear description",
            "conditions": ["condition1"],
            "requirements": ["requirement1"],
            "prohibitions": ["prohibition1"],
            "consequences": ["consequence1"],
            "priority": 75,
            "confidence": 0.85
        }}
    ]
    
    Return ONLY the JSON array.
    """
    
    try:
        model = get_model()
        response = model.invoke(synthesis_prompt)
        content = extract_content_safely(response)
        
        # Ensure valid JSON response
        parsed = safe_json_parse(content)
        if parsed and isinstance(parsed, list):
            return json.dumps(parsed)
        else:
            # Try to parse the input rules as fallback
            input_parsed = safe_json_parse(all_extracted_rules)
            if input_parsed:
                return json.dumps(input_parsed)
            else:
                return json.dumps([])
                
    except Exception as e:
        return json.dumps([{"error": f"Synthesis failed: {safe_str(e)}"}])


@tool(args_schema=ConvertToJsonRulesInput)
def convert_to_json_rules(synthesized_rules: str) -> str:
    """Convert to json-rules-engine format"""
    
    conversion_prompt = f"""
    Convert these rules to json-rules-engine format:
    
    INPUT RULES:
    {synthesized_rules}
    
    Use this json-rules-engine structure:
    [
        {{
            "name": "rule_name",
            "conditions": {{
                "all": [
                    {{
                        "fact": "dataOperation",
                        "operator": "equal",
                        "value": "usage"
                    }},
                    {{
                        "fact": "userConsent",
                        "operator": "equal",
                        "value": true
                    }}
                ]
            }},
            "event": {{
                "type": "data_governance_rule",
                "params": {{
                    "ruleId": "rule_id",
                    "domain": "data_usage",
                    "action": "enforce_consent",
                    "message": "Consent required"
                }}
            }},
            "priority": 80
        }}
    ]
    
    Return ONLY the JSON array.
    """
    
    try:
        model = get_model()
        response = model.invoke(conversion_prompt)
        content = extract_content_safely(response)
        
        # Ensure valid JSON response
        parsed = safe_json_parse(content)
        if parsed and isinstance(parsed, list):
            return json.dumps(parsed)
        else:
            # Create fallback json-rules-engine rule
            fallback_rules = [{
                "name": "fallback_rule",
                "conditions": {
                    "all": [
                        {"fact": "ruleApplicable", "operator": "equal", "value": True}
                    ]
                },
                "event": {
                    "type": "governance_rule",
                    "params": {
                        "message": "Fallback governance rule"
                    }
                },
                "priority": 50
            }]
            return json.dumps(fallback_rules)
            
    except Exception as e:
        error_rule = [{
            "name": "conversion_error",
            "conditions": {"all": [{"fact": "error", "operator": "equal", "value": True}]},
            "event": {"type": "error", "params": {"message": safe_str(e)}},
            "priority": 1
        }]
        return json.dumps(error_rule)


@tool(args_schema=ValidateJsonRulesInput)
def validate_json_rules(json_rules: str) -> str:
    """Validate JSON rules for json-rules-engine compatibility"""
    
    validation_prompt = f"""
    Validate these JSON rules for json-rules-engine compatibility:
    
    RULES:
    {json_rules}
    
    Check:
    1. Required fields: name, conditions, event
    2. Valid operators and structure
    3. Logical consistency
    
    Return validation report as JSON:
    {{
        "valid": true,
        "total_rules": 5,
        "valid_rules": 5,
        "invalid_rules": 0,
        "errors": [],
        "warnings": [],
        "quality_score": 95,
        "json_rules_engine_compatible": true
    }}
    
    Return ONLY the JSON validation report.
    """
    
    try:
        model = get_model()
        response = model.invoke(validation_prompt)
        content = extract_content_safely(response)
        
        # Ensure valid JSON response
        parsed = safe_json_parse(content)
        if parsed and isinstance(parsed, dict):
            return json.dumps(parsed)
        else:
            # Simple validation fallback
            rules_parsed = safe_json_parse(json_rules)
            rule_count = len(rules_parsed) if isinstance(rules_parsed, list) else 0
            
            validation_result = {
                "valid": rule_count > 0,
                "total_rules": rule_count,
                "valid_rules": rule_count,
                "invalid_rules": 0,
                "errors": [],
                "warnings": [],
                "quality_score": 80 if rule_count > 0 else 0,
                "json_rules_engine_compatible": rule_count > 0
            }
            return json.dumps(validation_result)
            
    except Exception as e:
        error_validation = {
            "valid": False,
            "total_rules": 0,
            "valid_rules": 0,
            "invalid_rules": 0,
            "errors": [safe_str(e)],
            "warnings": [],
            "quality_score": 0,
            "json_rules_engine_compatible": False
        }
        return json.dumps(error_validation)


# ========================= Agent Nodes =========================

def agent_node(state: AgentState) -> Dict[str, Any]:
    """Main reasoning agent that orchestrates processing"""
    
    messages = state["messages"]
    current_phase = state.get("current_phase", "start")
    legislation_text = state.get("legislation_text", "")
    analysis_count = state.get("analysis_count", 0)
    extraction_count = state.get("extraction_count", 0)
    
    model = get_model()
    tools = [analyze_with_reasoning, extract_data_rules, synthesize_rules, convert_to_json_rules, validate_json_rules]
    model_with_tools = model.bind_tools(tools)
    
    try:
        # Phase-specific prompts
        if current_phase == "start":
            system_msg = f"""You are an expert legal analyst. Process legislation systematically:

PHASE 1: Call analyze_with_reasoning 4 times:
1. analyze_with_reasoning(legislation_text="{legislation_text[:200]}...", reasoning_pathway="structural", reasoning_mode="deductive", focus_domain="data_usage")
2. analyze_with_reasoning(legislation_text="{legislation_text[:200]}...", reasoning_pathway="semantic", reasoning_mode="inductive", focus_domain="data_transfer")
3. analyze_with_reasoning(legislation_text="{legislation_text[:200]}...", reasoning_pathway="logical", reasoning_mode="abductive", focus_domain="data_storage")
4. analyze_with_reasoning(legislation_text="{legislation_text[:200]}...", reasoning_pathway="contextual", reasoning_mode="analogical", focus_domain="data_access")

Start with the first analysis call."""
            
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "analysis" and analysis_count < 4:
            system_msg = f"""Continue PHASE 1: Analysis {analysis_count + 1}/4. Call the next analyze_with_reasoning with different parameters."""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "analysis" and analysis_count >= 4:
            system_msg = f"""PHASE 2: Extract rules. Call extract_data_rules 4 times:
1. extract_data_rules(legislation_text="{legislation_text[:200]}...", data_domain="data_usage")
2. extract_data_rules(legislation_text="{legislation_text[:200]}...", data_domain="data_transfer")
3. extract_data_rules(legislation_text="{legislation_text[:200]}...", data_domain="data_storage")
4. extract_data_rules(legislation_text="{legislation_text[:200]}...", data_domain="data_access")

Start with the first extraction."""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "extraction" and extraction_count < 4:
            system_msg = f"""Continue PHASE 2: Extraction {extraction_count + 1}/4. Call the next extract_data_rules."""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "extraction" and extraction_count >= 4:
            extracted_rules = state.get("extracted_rules", {})
            rules_json = json.dumps(extracted_rules)
            system_msg = f"""PHASE 3: Call synthesize_rules(all_extracted_rules='{rules_json}')"""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "synthesis":
            system_msg = """PHASE 4: Call convert_to_json_rules with the synthesized rules"""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        elif current_phase == "conversion":
            system_msg = """PHASE 5: Call validate_json_rules with the converted rules"""
            new_messages = [SystemMessage(content=system_msg)] + list(messages)
            
        else:
            new_messages = list(messages)
        
        response = model_with_tools.invoke(new_messages)
        
        return {
            "messages": [response],
            "current_phase": "processing"
        }
        
    except Exception as e:
        error_msg = f"Agent error: {safe_str(e)}"
        error_response = AIMessage(content=error_msg)
        
        return {
            "messages": [error_response],
            "current_phase": "error",
            "error_log": state.get("error_log", []) + [error_msg]
        }


def tool_node(state: AgentState) -> Dict[str, Any]:
    """Execute tools with comprehensive error handling"""
    
    tools = [analyze_with_reasoning, extract_data_rules, synthesize_rules, convert_to_json_rules, validate_json_rules]
    tool_node_instance = ToolNode(tools)
    
    try:
        result = tool_node_instance.invoke(state)
        
        # Safe state updates
        updates = {"messages": result.get("messages", [])}
        extracted_rules = state.get("extracted_rules", {}).copy()
        analysis_count = state.get("analysis_count", 0)
        extraction_count = state.get("extraction_count", 0)
        reasoning_steps = state.get("reasoning_steps", []).copy()
        error_log = state.get("error_log", []).copy()
        
        # Process messages safely
        messages = result.get("messages", [])
        
        for message in messages:
            if isinstance(message, ToolMessage):
                try:
                    # Safely extract content
                    content = extract_content_safely(message)
                    tool_name = getattr(message, 'name', 'unknown_tool')
                    
                    reasoning_steps.append(f"Executed {tool_name}")
                    
                    # Handle specific tools
                    if tool_name == "analyze_with_reasoning":
                        analysis_count += 1
                        
                    elif tool_name == "extract_data_rules":
                        extraction_count += 1
                        # Parse extracted rules
                        parsed_rules = safe_json_parse(content)
                        if parsed_rules and isinstance(parsed_rules, list):
                            for rule in parsed_rules:
                                if isinstance(rule, dict):
                                    domain = rule.get("domain", "unknown")
                                    if domain not in extracted_rules:
                                        extracted_rules[domain] = []
                                    extracted_rules[domain].append(rule)
                    
                    elif tool_name == "convert_to_json_rules":
                        parsed_rules = safe_json_parse(content)
                        if parsed_rules and isinstance(parsed_rules, list):
                            updates["json_rules"] = parsed_rules
                    
                    elif tool_name == "validate_json_rules":
                        parsed_validation = safe_json_parse(content)
                        if parsed_validation and isinstance(parsed_validation, dict):
                            updates["validation_results"] = parsed_validation
                
                except Exception as e:
                    error_msg = f"Error processing {getattr(message, 'name', 'tool')}: {safe_str(e)}"
                    error_log.append(error_msg)
        
        # Update state
        updates.update({
            "extracted_rules": extracted_rules,
            "analysis_count": analysis_count,
            "extraction_count": extraction_count,
            "reasoning_steps": reasoning_steps,
            "error_log": error_log
        })
        
        return updates
        
    except Exception as e:
        error_msg = f"Tool node error: {safe_str(e)}"
        return {
            "messages": state.get("messages", []),
            "error_log": state.get("error_log", []) + [error_msg]
        }


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """Determine whether to continue or end"""
    try:
        messages = state.get("messages", [])
        
        if not messages:
            return "end"
        
        last_message = messages[-1]
        
        # Check for tool calls
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        
        # Check completion status
        current_phase = state.get("current_phase", "start")
        if current_phase in ["complete", "error"]:
            return "end"
        
        # Check if we have errors
        error_log = state.get("error_log", [])
        if len(error_log) > 5:  # Too many errors
            return "end"
        
        return "end"
        
    except Exception:
        return "end"


# ========================= Create Graph =========================

def create_legislation_processing_graph():
    """Create the LangGraph workflow with error handling"""
    
    try:
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("agent", agent_node)
        workflow.add_node("tools", tool_node)
        
        # Set entry point
        workflow.set_entry_point("agent")
        
        # Add edges
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                "end": END
            }
        )
        
        workflow.add_edge("tools", "agent")
        
        # Compile with memory
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
        
    except Exception as e:
        print(f"‚ùå Error creating graph: {safe_str(e)}")
        return None


# ========================= Main Processing Function =========================

def process_legislation(legislation_text: str) -> Dict[str, Any]:
    """Process legislation with comprehensive error handling"""
    
    print("üöÄ Processing legislation with o3-mini + advanced reasoning...")
    
    try:
        # Create the processing graph
        graph = create_legislation_processing_graph()
        
        if not graph:
            return {"status": "error", "error": "Failed to create processing graph"}
        
        # Initial state
        initial_state = {
            "messages": [HumanMessage(content=f"Process this legislation:\n\n{legislation_text}")],
            "legislation_text": legislation_text,
            "current_phase": "start",
            "analysis_count": 0,
            "extraction_count": 0,
            "extracted_rules": {},
            "json_rules": [],
            "validation_results": {},
            "reasoning_pathways": [],
            "reasoning_steps": [],
            "error_log": []
        }
        
        # Configuration
        config = {"configurable": {"thread_id": "legislation_processing"}}
        
        # Run the graph
        final_state = graph.invoke(initial_state, config)
        
        return {
            "status": "completed",
            "extracted_rules": final_state.get("extracted_rules", {}),
            "json_rules": final_state.get("json_rules", []),
            "validation_results": final_state.get("validation_results", {}),
            "reasoning_steps": final_state.get("reasoning_steps", []),
            "analysis_count": final_state.get("analysis_count", 0),
            "extraction_count": final_state.get("extraction_count", 0),
            "error_log": final_state.get("error_log", []),
            "messages": final_state.get("messages", [])
        }
        
    except Exception as e:
        error_msg = f"Processing failed: {safe_str(e)}"
        print(f"‚ùå {error_msg}")
        return {"status": "error", "error": error_msg}


# ========================= Helper Functions =========================

def save_rules_to_file(rules: List[Dict[str, Any]], filename: str = "data_governance_rules.json"):
    """Save JSON rules to file"""
    try:
        output = {
            "rules": rules,
            "metadata": {
                "created_by": "legislation_converter_o3mini",
                "model": OPENAI_MODEL,
                "reasoning_effort": REASONING_EFFORT,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "rule_count": len(rules)
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Saved {len(rules)} rules to {filename}")
        
    except Exception as e:
        print(f"‚ùå Error saving rules: {safe_str(e)}")


def print_results_summary(results: Dict[str, Any]):
    """Print processing results summary"""
    print("\n" + "="*80)
    print("üìä PROCESSING RESULTS SUMMARY")
    print("="*80)
    
    try:
        print(f"üß† REASONING FRAMEWORK:")
        print(f"   ‚Ä¢ Analysis phases: {results.get('analysis_count', 0)}/4")
        print(f"   ‚Ä¢ Extraction phases: {results.get('extraction_count', 0)}/4")
        print(f"   ‚Ä¢ Total steps: {len(results.get('reasoning_steps', []))}")
        
        extracted = results.get("extracted_rules", {})
        total_extracted = sum(len(rules) for rules in extracted.values())
        print(f"\nüìã EXTRACTION RESULTS:")
        print(f"   ‚Ä¢ Total rules: {total_extracted}")
        for domain, rules in extracted.items():
            print(f"   ‚Ä¢ {domain}: {len(rules)} rules")
        
        json_rules = results.get("json_rules", [])
        print(f"\n‚öôÔ∏è JSON RULES: {len(json_rules)}")
        
        validation = results.get("validation_results", {})
        if validation:
            print(f"\n‚úÖ VALIDATION:")
            print(f"   ‚Ä¢ Status: {'PASS' if validation.get('valid', False) else 'FAIL'}")
            print(f"   ‚Ä¢ Quality: {validation.get('quality_score', 'N/A')}/100")
        
        error_log = results.get("error_log", [])
        if error_log:
            print(f"\n‚ö†Ô∏è ERRORS: {len(error_log)} encountered")
        
    except Exception as e:
        print(f"‚ùå Error printing summary: {safe_str(e)}")


# ========================= Main Execution =========================

if __name__ == "__main__":
    # Check API key
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-openai-api-key'")
        exit(1)
    
    # Sample legislation
    SAMPLE_LEGISLATION = """
    DATA PROTECTION AND PRIVACY ACT
    
    PART II - DATA USAGE REQUIREMENTS
    
    Section 2. Lawful Basis for Data Usage
    2.1 Personal data shall only be used when there is a lawful basis, including:
        (a) The data subject has given explicit consent
        (b) Processing is necessary for contract performance
        (c) Processing is required for legal compliance
    
    2.2 Data controllers must not use personal data for purposes incompatible with those for which it was originally collected.
    
    PART III - DATA TRANSFER REGULATIONS
    
    Section 5. Third-Party Data Transfers
    5.1 Data controllers shall not transfer personal data to third parties unless:
        (a) A data processing agreement is in place
        (b) The third party provides appropriate security guarantees
        (c) The data subject has been informed of the transfer
    
    PART IV - DATA STORAGE REQUIREMENTS
    
    Section 7. Storage Duration and Retention
    7.1 Personal data shall not be stored longer than necessary for the purposes for which it was collected.
    7.2 Data retention periods must be defined and documented for each category of personal data.
    
    PART V - DATA ACCESS CONTROLS
    
    Section 10. Access Rights and Permissions
    10.1 Data controllers must implement role-based access controls ensuring:
        (a) Access is granted on a need-to-know basis
        (b) Privileged access is monitored and reviewed quarterly
    """
    
    print("üöÄ Advanced Data Governance Rules Extractor")
    print(f"ü§ñ Model: {OPENAI_MODEL} (reasoning effort: {REASONING_EFFORT})")
    print("üìä Framework: Chain of Thought + Mixture of Thought + Mixture of Reasoning")
    print("=" * 80)
    
    # Process the legislation
    results = process_legislation(SAMPLE_LEGISLATION)
    
    if results["status"] == "completed":
        print_results_summary(results)
        
        json_rules = results.get("json_rules", [])
        if json_rules:
            save_rules_to_file(json_rules)
            print(f"\n‚ú® Generated {len(json_rules)} data governance rules!")
        else:
            print("\n‚ö†Ô∏è No JSON rules generated")
    else:
        print(f"\n‚ùå Processing failed: {results.get('error', 'Unknown error')}")
