import os
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import logging
from dotenv import load_dotenv
from pathlib import Path
from azure.identity import ClientSecretCredential
from tqdm import tqdm
import json
import faiss
from openai import AzureOpenAI
from sklearn.preprocessing import normalize
import requests
from sentence_transformers import CrossEncoder

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Original OSEnv class remains unchanged
class OSEnv:
    # [Keep the exact same OSEnv implementation from previous code]
    # ... (preserve all original methods without changes)

class EnhancedSemanticMatcher:
    def __init__(self, env_setup: OSEnv):
        """Initialize with enhanced matching capabilities."""
        self.env = env_setup
        self.batch_size = 8
        self.main_dimension = 3072  # text-embedding-3-large
        self.aux_dimension = 1024   # text-embedding-3-small
        self._setup_openai_client()
        self._setup_cross_encoder()
        self._setup_templates()

    def _setup_openai_client(self):
        """Configure Azure OpenAI client."""
        self.client = AzureOpenAI(
            api_key=self.env.token,
            api_version=self.env.get("API_VERSION", "2024-02-01"),
            azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT")
        )

    def _setup_cross_encoder(self):
        """Initialize cross-encoder for precision re-ranking."""
        self.cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2")
        
    def _setup_templates(self):
        """Configure context-aware templates."""
        self.source_template = (
            "Technical Specification Analysis:\n"
            "Component: {title}\n"
            "Details: {content}\n"
            "Key Aspects: Functionality, technical parameters, "
            "system integration, and operational constraints."
        )
        
        self.target_template = (
            "Technical Definition Context:\n"
            "Term: {title}\n"
            "Full Specification: {content}\n"
            "Includes: Implementation guidelines, dependencies, "
            "performance characteristics, and use case scenarios."
        )

    def _prepare_text(self, row: pd.Series, is_source: bool) -> str:
        """Generate context-rich text representations."""
        if is_source:
            return self.source_template.format(
                title=row['name'].strip(),
                content=row['description'].strip()
            )
        return self.target_template.format(
            title=row['pbt-name'].strip(),
            content=row['pbt-definition'].strip()
        )

    def _expand_query(self, text: str) -> str:
        """Enhance query context using LLM expansion."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": "Expand this technical description with 2-3 relevant terms. Keep output concise."
                }, {
                    "role": "user",
                    "content": text
                }],
                temperature=0.3,
                max_tokens=100
            )
            return f"{text}\nExpanded Context: {response.choices[0].message.content}"
        except Exception as e:
            logger.warning(f"Query expansion failed: {str(e)}")
            return text

    def _generate_embeddings(self, texts: List[str], model: str, dim: int) -> List[List[float]]:
        """Generate embeddings with error handling and retries."""
        try:
            response = self.client.embeddings.create(
                model=model,
                input=texts,
                dimensions=dim
            )
            return [e.embedding for e in response.data]
        except Exception as e:
            logger.error(f"Embedding generation failed: {str(e)}")
            raise

    def process_dataset(self, df: pd.DataFrame, is_source: bool) -> Dict[str, np.ndarray]:
        """Process dataset with hybrid embeddings."""
        embeddings = {
            'main': [],
            'aux': [],
            'texts': [],
            'metadata': []
        }

        for i in tqdm(range(0, len(df), self.batch_size), 
                    desc=f"Processing {'source' if is_source else 'target'}"):
            batch = df.iloc[i:i+self.batch_size]
            batch_texts = []
            
            for _, row in batch.iterrows():
                text = self._prepare_text(row, is_source)
                if is_source:
                    text = self._expand_query(text)
                batch_texts.append(text)

            # Generate dual embeddings
            main_embs = self._generate_embeddings(batch_texts, 
                                                 "text-embedding-3-large", 
                                                 self.main_dimension)
            aux_embs = self._generate_embeddings(batch_texts,
                                                "text-embedding-3-small",
                                                self.aux_dimension)

            embeddings['main'].extend(main_embs)
            embeddings['aux'].extend(aux_embs)
            embeddings['texts'].extend(batch_texts)
            embeddings['metadata'].extend(batch.to_dict('records'))

        # Normalize embeddings
        embeddings['main'] = normalize(np.array(embeddings['main'], dtype='float32'))
        embeddings['aux'] = normalize(np.array(embeddings['aux'], dtype='float32'))
        
        return embeddings

    def _hybrid_search(self, source_emb: Dict[str, np.ndarray], 
                      target_embs: Dict[str, np.ndarray], 
                      top_k: int = 20) -> List[Tuple[int, float]]:
        """Perform hybrid semantic search with re-ranking."""
        # First-stage retrieval
        main_scores = np.dot(target_embs['main'], source_emb['main'])
        aux_scores = np.dot(target_embs['aux'], source_emb['aux'])
        combined_scores = 0.6 * main_scores + 0.4 * aux_scores
        
        # Get initial candidates
        candidate_indices = np.argsort(-combined_scores)[:top_k]
        
        # Cross-encoder re-ranking
        pairs = [(source_emb['text'], target_embs['texts'][i]) 
                for i in candidate_indices]
        cross_scores = self.cross_encoder.predict(pairs)
        
        # Combine scores
        final_scores = 0.7 * combined_scores[candidate_indices] + 0.3 * cross_scores
        sorted_indices = np.argsort(-final_scores)
        
        return [(candidate_indices[i], final_scores[i]) for i in sorted_indices]

    def find_matches(self, source_df: pd.DataFrame, 
                    target_df: pd.DataFrame, 
                    k: int = 4) -> pd.DataFrame:
        """Execute enhanced matching pipeline."""
        logger.info("Processing source data...")
        source_data = self.process_dataset(source_df, is_source=True)
        
        logger.info("Processing target data...")
        target_data = self.process_dataset(target_df, is_source=False)

        # Build FAISS indices
        main_index = faiss.IndexFlatIP(self.main_dimension)
        main_index.add(target_data['main'])
        
        aux_index = faiss.IndexFlatIP(self.aux_dimension)
        aux_index.add(target_data['aux'])

        results = []
        for src_idx in tqdm(range(len(source_data['main'])), desc="Matching entries"):
            source_emb = {
                'main': source_data['main'][src_idx],
                'aux': source_data['aux'][src_idx],
                'text': source_data['texts'][src_idx]
            }
            
            # Perform hybrid search
            matches = self._hybrid_search(source_emb, target_data, top_k=k*3)
            
            # Process top k matches
            top_matches = []
            for match_idx, score in matches[:k]:
                target_meta = target_data['metadata'][match_idx]
                top_matches.append({
                    'pbt_name': target_meta['pbt-name'],
                    'definition': target_meta['pbt-definition'],
                    'score': float(score)
                })
            
            results.append({
                'name': source_data['metadata'][src_idx]['name'],
                'description': source_data['metadata'][src_idx]['description'],
                'matches': top_matches
            })
        
        return self._format_results(results)

    def _format_results(self, results: List[Dict]) -> pd.DataFrame:
        """Format results into structured DataFrame."""
        formatted = []
        for entry in results:
            for rank, match in enumerate(entry['matches'], 1):
                formatted.append({
                    'name': entry['name'],
                    'description': entry['description'],
                    'rank': rank,
                    'pbt_name': match['pbt_name'],
                    'pbt_definition': match['definition'],
                    'confidence': match['score']
                })
        
        df = pd.DataFrame(formatted)
        return df.pivot_table(
            index=['name', 'description'],
            columns='rank',
            aggfunc='first',
            values=['pbt_name', 'pbt_definition', 'confidence']
        ).sort_values(by=('confidence', 1), ascending=False)

    def save_results(self, df: pd.DataFrame, output_path: str) -> None:
        """Save results with enhanced formatting."""
        try:
            # Flatten multi-index columns
            df.columns = [f'{col[0]}_{col[1]}' for col in df.columns]
            df.reset_index(inplace=True)
            
            # Save CSV
            df.to_csv(output_path, index=False)
            logger.info(f"Results saved to CSV: {output_path}")
            
            # Save JSON
            json_path = output_path.replace('.csv', '.json')
            with open(json_path, 'w') as f:
                json.dump(df.to_dict('records'), f, indent=2)
            logger.info(f"Results saved to JSON: {json_path}")
            
        except Exception as e:
            logger.error(f"Error saving results: {str(e)}")
            raise

def main():
    """Enhanced main function with improved error handling."""
    try:
        # Initialize environment
        base_dir = Path(__file__).parent.parent
        env_setup = OSEnv(
            config_file=str(base_dir / 'env/config.env'),
            creds_file=str(base_dir / 'env/credentials.env'),
            certificate_path=str(base_dir / 'env/cacert.pem')
        )

        # Load data
        data_dir = base_dir / 'data'
        source_df = pd.read_csv(data_dir / 'source.csv').fillna('').astype(str)
        target_df = pd.read_csv(data_dir / 'target.csv').fillna('').astype(str)

        # Validate schema
        required_source = {'name', 'description'}
        required_target = {'pbt-name', 'pbt-definition'}
        if not required_source.issubset(source_df.columns):
            raise ValueError("Source CSV missing required columns")
        if not required_target.issubset(target_df.columns):
            raise ValueError("Target CSV missing required columns")

        # Process matches
        matcher = EnhancedSemanticMatcher(env_setup)
        result_df = matcher.find_matches(source_df, target_df, k=4)
        
        # Save results
        output_dir = base_dir / 'output'
        output_dir.mkdir(exist_ok=True)
        matcher.save_results(result_df, str(output_dir / 'enhanced_matches.csv'))

        # Generate report
        logger.info("\nMatching Analysis:")
        logger.info(f"Total Matches Processed: {len(result_df)}")
        logger.info(f"Average Confidence: {result_df['confidence_1'].mean():.2f}")
        logger.info(f"High Confidence Matches (>0.8): {len(result_df[result_df['confidence_1'] > 0.8])}")

    except Exception as e:
        logger.error(f"Critical error in main process: {str(e)}")
        raise

if __name__ == "__main__":
    main()
