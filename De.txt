import os
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import logging
from dotenv import load_dotenv
from pathlib import Path
from azure.identity import ClientSecretCredential
from tqdm import tqdm
import json
import faiss
from openai import AzureOpenAI
from sklearn.preprocessing import normalize
import requests

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert string to boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

class OSEnv:
    """Environment and certificate management class."""
    
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        """Initialize with configuration files and certificate path."""
        self.var_list = []
        
        # Load main configuration
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        
        # Load credentials
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        
        # Set up certificates
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")
        
        # Configure proxy if enabled
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")
        
        # Set up Azure token if secure endpoints enabled
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        """Set up the certificate path for SSL verification."""
        try:
            if is_file_readable(certificate_path):
                cert_path = str(Path(certificate_path))
                self.set("REQUESTS_CA_BUNDLE", cert_path)
                self.set("SSL_CERT_FILE", cert_path)
                self.set("CURL_CA_BUNDLE", cert_path)
                logger.info(f"Certificate path set to: {cert_path}")
        except Exception as e:
            logger.error(f"Certificate configuration failed: {str(e)}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        """Read and set environment variables from a dotenv file."""
        try:
            if is_file_readable(dotenvfile):
                logger.info(f"Loading environment variables from {dotenvfile}")
                with open(dotenvfile) as f:
                    lines = f.readlines()
                
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip("'").strip('"')
                            self.set(key, value, print_val)
                        except ValueError:
                            continue
                            
                logger.info(f"Successfully loaded variables from {dotenvfile}")
                
        except Exception as e:
            logger.error(f"Failed to load environment file {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        """Set an environment variable."""
        try:
            os.environ[var_name] = val
            if var_name not in self.var_list:
                self.var_list.append(var_name)
            if print_val and var_name not in ['AZURE_CLIENT_SECRET', 'AD_USER_PW']:
                logger.info(f"Set {var_name}={val}")
        except Exception as e:
            logger.error(f"Failed to set environment variable {var_name}: {str(e)}")
            raise

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        """Get an environment variable value."""
        return os.getenv(var_name, default)

    def set_proxy(self) -> None:
        """Set up proxy configuration with authentication."""
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Missing proxy credentials")
            
            proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains))
            
            logger.info("Proxy configuration completed")
            
        except Exception as e:
            logger.error(f"Proxy configuration failed: {str(e)}")
            raise

    def get_azure_token(self) -> str:
        """Get Azure authentication token."""
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token acquired successfully")
            return token.token
        except Exception as e:
            logger.error(f"Failed to get Azure token: {str(e)}")
            raise

class SemanticMatcherTopK:
    def __init__(self, env_setup: OSEnv):
        """Initialize with environment setup."""
        self.env = env_setup
        self.batch_size = 16
        self.dimension = 3072  # dimension for text-embedding-3-large
        self._setup_openai_client()
        
    def _setup_openai_client(self):
        """Configure OpenAI client with Azure settings."""
        self.client = AzureOpenAI(
            api_key=self.env.token,
            api_version=self.env.get("API_VERSION", "2024-02-01"),
            azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT")
        )

    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings using text-embedding-3-large model."""
        try:
            response = self.client.embeddings.create(
                model="text-embedding-3-large",
                input=texts,
                dimensions=self.dimension
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            logger.error(f"Failed to get embeddings batch: {str(e)}")
            raise

    def prepare_text(self, row: pd.Series, is_source: bool = True) -> str:
        """Combine name and description for semantic matching with improved context."""
        if is_source:
            return f"Title: {row['name'].strip()}\nDescription: {row['description'].strip()}"
        else:
            return f"Title: {row['pbt-name'].strip()}\nDefinition: {row['pbt-definition'].strip()}"

    def build_index(self, embeddings: np.ndarray) -> faiss.IndexFlatIP:
        """Build FAISS index for efficient similarity search."""
        index = faiss.IndexFlatIP(self.dimension)
        normalized_embeddings = normalize(embeddings)
        index.add(normalized_embeddings)
        return index

    def process_embeddings(self, df: pd.DataFrame, is_source: bool = True, desc: str = "") -> Tuple[np.ndarray, List[str]]:
        """Process dataframe and return embeddings and texts."""
        texts = []
        all_embeddings = []
        
        # Process in batches
        for i in tqdm(range(0, len(df), self.batch_size), desc=desc):
            batch_df = df.iloc[i:i + self.batch_size]
            batch_texts = [self.prepare_text(row, is_source) for _, row in batch_df.iterrows()]
            batch_embeddings = self.get_embeddings_batch(batch_texts)
            
            texts.extend(batch_texts)
            all_embeddings.extend(batch_embeddings)
        
        return np.array(all_embeddings).astype('float32'), texts

    def find_top_k_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame, k: int = 4) -> pd.DataFrame:
        """Find top k semantic matches for each source entry using FAISS."""
        logger.info("Processing semantic matches...")
        
        # Process target entries
        logger.info("Processing target entries...")
        target_embeddings, _ = self.process_embeddings(
            target_df, 
            is_source=False, 
            desc="Processing target entries"
        )
        
        # Build FAISS index
        logger.info("Building FAISS index...")
        index = self.build_index(target_embeddings)
        
        # Process source entries and find matches
        all_matches = []
        batch_size = min(self.batch_size, len(source_df))
        
        for i in tqdm(range(0, len(source_df), batch_size), desc="Finding matches"):
            # Process batch of source entries
            batch_df = source_df.iloc[i:i + batch_size]
            source_embeddings, _ = self.process_embeddings(
                batch_df,
                is_source=True,
                desc=f"Processing batch {i//batch_size + 1}"
            )
            
            # Normalize source embeddings
            source_embeddings = normalize(source_embeddings)
            
            # Search using FAISS
            similarities, indices = index.search(source_embeddings, k)
            
            # Record matches for the batch
            for idx, (batch_similarities, batch_indices) in enumerate(zip(similarities, indices)):
                source_row = batch_df.iloc[idx]
                for rank, (similarity, match_idx) in enumerate(zip(batch_similarities, batch_indices), 1):
                    all_matches.append({
                        'source_name': source_row['name'],
                        'source_description': source_row['description'],
                        'matched_pbt_name': target_df.iloc[match_idx]['pbt-name'],
                        'matched_pbt_definition': target_df.iloc[match_idx]['pbt-definition'],
                        'similarity_score': float(similarity),
                        'rank': rank
                    })
        
        # Convert to DataFrame
        matches_df = pd.DataFrame(all_matches)
        
        # Create result DataFrame with one row per source entry
        result_df = pd.DataFrame()
        result_df['name'] = source_df['name']
        result_df['description'] = source_df['description']
        
        # Add matches and scores
        for i in range(k):
            rank = i + 1
            rank_matches = matches_df[matches_df['rank'] == rank]
            result_df[f'match_{rank}_pbt_name'] = rank_matches.set_index('source_name')['matched_pbt_name']
            result_df[f'match_{rank}_similarity'] = rank_matches.set_index('source_name')['similarity_score']
            result_df[f'match_{rank}_definition'] = rank_matches.set_index('source_name')['matched_pbt_definition']

        # Sort matches by highest similarity score
        result_df.sort_values(by='match_1_similarity', ascending=False, inplace=True)
        
        # Add summary statistics
        result_df['avg_similarity'] = result_df[[f'match_{i}_similarity' for i in range(1, k+1)]].mean(axis=1)
        result_df['max_similarity'] = result_df[[f'match_{i}_similarity' for i in range(1, k+1)]].max(axis=1)
        result_df['min_similarity'] = result_df[[f'match_{i}_similarity' for i in range(1, k+1)]].min(axis=1)
        
        return result_df

    def save_results(self, matches_df: pd.DataFrame, output_path: str) -> None:
        """Save matches to CSV and JSON formats."""
        try:
            # Save CSV
            matches_df.to_csv(output_path, index=False)
            logger.info(f"Results saved to CSV: {output_path}")
            
            # Save JSON
            json_path = output_path.rsplit('.', 1)[0] + '.json'
            matches_dict = matches_df.to_dict(orient='records')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(matches_dict, f, indent=2, ensure_ascii=False)
            logger.info(f"Results saved to JSON: {json_path}")
            
        except Exception as e:
            logger.error(f"Error saving results: {str(e)}")
            raise

def main():
    """Main function to run the semantic matching process."""
    try:
        # Setup paths
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / 'env'
        data_dir = base_dir / 'data'
        output_dir = base_dir / 'output'
        
        # Create directories if they don't exist
        for directory in [data_dir, output_dir]:
            directory.mkdir(exist_ok=True)
        
        # Initialize environment
        env_setup = OSEnv(
            config_file=str(env_dir / 'config.env'),
            creds_file=str(env_dir / 'credentials.env'),
            certificate_path=str(env_dir / 'cacert.pem')
        )
        
        # Load source and target CSVs
        source_csv = data_dir / 'source.csv'
        target_csv = data_dir / 'target.csv'
        output_file = output_dir / 'semantic_matches.csv'
        
        logger.info("Loading CSV files...")
        
        # Read CSVs with proper encoding handling
        def read_csv_with_encoding(file_path: Path) -> pd.DataFrame:
            encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    return df
                except UnicodeDecodeError:
                    continue
            raise ValueError(f"Could not read {file_path} with any supported encoding")
        
        source_df = read_csv_with_encoding(source_csv)
        target_df = read_csv_with_encoding(target_csv)
        
        # Validate required columns
        source_columns = {'name', 'description'}
        target_columns = {'pbt-name', 'pbt-definition'}
        
        if not source_columns.issubset(source_df.columns):
            raise ValueError(f"Source CSV missing required columns. Expected {source_columns}")
        if not target_columns.issubset(target_df.columns):
            raise ValueError(f"Target CSV missing required columns. Expected {target_columns}")
        
        # Clean data
        for df in [source_df, target_df]:
            for col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].fillna('')
                    df[col] = df[col].astype(str).str.strip()
        
        logger.info(f"Loaded {len(source_df)} source entries and {len(target_df)} target entries")
        
        # Initialize matcher
        matcher = SemanticMatcherTopK(env_setup)
        
        # Find top 4 matches
        logger.info("Finding semantic matches...")
        result_df = matcher.find_top_k_matches(source_df, target_df, k=4)
        
        # Save results
        matcher.save_results(result_df, str(output_file))
        
        # Print summary statistics
        print("\nMatching Summary:")
        print(f"Total source entries processed: {len(source_df)}")
        print(f"Total target entries processed: {len(target_df)}")
        print(f"\nSimilarity Score Statistics:")
        print(f"Average similarity across all matches: {result_df['avg_similarity'].mean():.4f}")
        print(f"Average top match similarity: {result_df['match_1_similarity'].mean():.4f}")
        print(f"Highest similarity score: {result_df['max_similarity'].max():.4f}")
        print(f"90th percentile similarity: {result_df['match_1_similarity'].quantile(0.9):.4f}")
        
        # Generate quality metrics
        high_quality_matches = result_df[result_df['match_1_similarity'] >= 0.8]
        medium_quality_matches = result_df[(result_df['match_1_similarity'] >= 0.6) & (result_df['match_1_similarity'] < 0.8)]
        
        print(f"\nMatch Quality Distribution:")
        print(f"High-quality matches (≥0.8): {len(high_quality_matches)} ({len(high_quality_matches)/len(result_df)*100:.1f}%)")
        print(f"Medium-quality matches (0.6-0.8): {len(medium_quality_matches)} ({len(medium_quality_matches)/len(result_df)*100:.1f}%)")
        
        # Save detailed analysis report
        report_file = output_dir / 'matching_analysis_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Semantic Matching Analysis Report\n")
            f.write("================================\n\n")
            f.write(f"Source Dataset: {source_csv.name}\n")
            f.write(f"Target Dataset: {target_csv.name}\n")
            f.write(f"Total source entries: {len(source_df)}\n")
            f.write(f"Total target entries: {len(target_df)}\n\n")
            
            f.write("Similarity Score Statistics\n")
            f.write("--------------------------\n")
            f.write(f"Mean similarity (all matches): {result_df['avg_similarity'].mean():.4f}\n")
            f.write(f"Mean top match similarity: {result_df['match_1_similarity'].mean():.4f}\n")
            f.write(f"Highest similarity: {result_df['max_similarity'].max():.4f}\n")
            f.write(f"Lowest similarity: {result_df['min_similarity'].min():.4f}\n")
            f.write(f"90th percentile: {result_df['match_1_similarity'].quantile(0.9):.4f}\n")
            f.write(f"Median similarity: {result_df['match_1_similarity'].median():.4f}\n\n")
            
            f.write("Match Quality Distribution\n")
            f.write("-------------------------\n")
            f.write(f"High-quality matches (≥0.8): {len(high_quality_matches)}\n")
            f.write(f"Medium-quality matches (0.6-0.8): {len(medium_quality_matches)}\n")
            f.write(f"Low-quality matches (<0.6): {len(result_df) - len(high_quality_matches) - len(medium_quality_matches)}\n")
        
        logger.info(f"Analysis report saved to {report_file}")
        logger.info("Process completed successfully!")
        
    except FileNotFoundError as e:
        logger.error(f"File Error: {str(e)}")
        print(f"\nFile Error: {str(e)}")
        print("Please check your file paths and try again.")
        
    except ValueError as e:
        logger.error(f"Validation Error: {str(e)}")
        print(f"\nValidation Error: {str(e)}")
        print("Please check your CSV files and try again.")
        
    except Exception as e:
        logger.error(f"Unexpected Error: {str(e)}")
        print(f"\nUnexpected Error: {str(e)}")
        print("Please check the logs for more details.")
        logger.exception("Unexpected error occurred")

if __name__ == "__main__":
    main()
