import os
import glob
import pickle
import json
import numpy as np
from typing import List, Dict, Any, Tuple
import asyncio
from pathlib import Path
import time

# Core libraries
import PyPDF2
from langdetect import detect
import chromadb
from chromadb.config import Settings

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document, BaseRetriever
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.vectorstores.base import VectorStore

# OpenAI for embeddings and translation
import openai
from openai import OpenAI

# Global Configuration
class Config:
    """Global configuration for OpenAI API and other settings"""
    OPENAI_API_KEY = "your-openai-api-key-here"  # Replace with your actual API key
    OPENAI_BASE_URL = "https://api.openai.com/v1"  # OpenAI base URL
    EMBEDDING_MODEL = "text-embedding-3-large"
    CHAT_MODEL = "gpt-4"
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    PDF_DIRECTORY = "./pdfs"  # Directory containing PDF files
    CHROMADB_PATH = "./chromadb"
    MEMORY_PATH = "./memory"
    EMBEDDINGS_CACHE_PATH = "./embeddings_cache"
    
    @classmethod
    def setup_openai(cls):
        """Setup OpenAI client with global configuration"""
        openai.api_key = cls.OPENAI_API_KEY
        openai.base_url = cls.OPENAI_BASE_URL
        return OpenAI(api_key=cls.OPENAI_API_KEY, base_url=cls.OPENAI_BASE_URL)

# Initialize OpenAI client globally
openai_client = Config.setup_openai()

class OpenAIEmbeddingsManager:
    """Handles OpenAI API embeddings creation and management"""
    
    def __init__(self):
        self.client = openai_client
        self.model = Config.EMBEDDING_MODEL
        self.cache_dir = Config.EMBEDDINGS_CACHE_PATH
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """Create embeddings using OpenAI API with batching"""
        all_embeddings = []
        
        print(f"Creating embeddings for {len(texts)} text chunks using {self.model}...")
        
        # Process in batches to handle API limits
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")
            
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch_texts,
                    encoding_format="float"
                )
                
                batch_embeddings = [embedding.embedding for embedding in response.data]
                all_embeddings.extend(batch_embeddings)
                
                # Small delay to respect rate limits
                time.sleep(0.1)
                
            except Exception as e:
                print(f"Error creating embeddings for batch {i//batch_size + 1}: {str(e)}")
                # Create zero embeddings as fallback
                embedding_dim = 3072  # text-embedding-3-large dimension
                batch_embeddings = [[0.0] * embedding_dim for _ in batch_texts]
                all_embeddings.extend(batch_embeddings)
        
        print(f"Created {len(all_embeddings)} embeddings successfully!")
        return all_embeddings
    
    def save_embeddings_cache(self, texts: List[str], embeddings: List[List[float]], metadata: List[Dict]):
        """Save embeddings and metadata to cache"""
        cache_data = {
            "texts": texts,
            "embeddings": embeddings,
            "metadata": metadata,
            "model": self.model,
            "timestamp": time.time()
        }
        
        cache_file = os.path.join(self.cache_dir, "embeddings_cache.json")
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"Embeddings cache saved to {cache_file}")
    
    def load_embeddings_cache(self) -> Tuple[List[str], List[List[float]], List[Dict]]:
        """Load embeddings from cache if available"""
        cache_file = os.path.join(self.cache_dir, "embeddings_cache.json")
        
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                if cache_data.get("model") == self.model:
                    print(f"Loaded {len(cache_data['embeddings'])} embeddings from cache")
                    return cache_data["texts"], cache_data["embeddings"], cache_data["metadata"]
                else:
                    print("Cache model mismatch, will recreate embeddings")
            except Exception as e:
                print(f"Error loading cache: {str(e)}")
        
        return [], [], []

class PDFProcessor:
    """Handles PDF reading and text extraction"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
        )
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from a PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text.strip()
        except Exception as e:
            print(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""
    
    def detect_language(self, text: str) -> str:
        """Detect the language of the text"""
        try:
            # Take a sample of the text for language detection
            sample = text[:1000] if len(text) > 1000 else text
            return detect(sample)
        except:
            return "unknown"
    
    def translate_to_english(self, text: str) -> str:
        """Translate German text to English using OpenAI"""
        try:
            response = openai_client.chat.completions.create(
                model=Config.CHAT_MODEL,
                messages=[
                    {"role": "system", "content": "You are a professional translator. Translate the following German text to English. Maintain the original meaning and structure as much as possible. If the text is already in English, return it unchanged."},
                    {"role": "user", "content": text}
                ],
                max_tokens=4000,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error translating text: {str(e)}")
            return text  # Return original text if translation fails
    
    def process_pdfs(self, directory: str) -> Tuple[List[str], List[Dict]]:
        """Process all PDFs in the directory and return texts and metadata"""
        texts = []
        metadata_list = []
        pdf_files = glob.glob(os.path.join(directory, "*.pdf"))
        
        if not pdf_files:
            print(f"No PDF files found in {directory}")
            return texts, metadata_list
        
        print(f"Found {len(pdf_files)} PDF files to process...")
        
        for pdf_path in pdf_files:
            print(f"Processing: {os.path.basename(pdf_path)}")
            
            # Extract text
            text = self.extract_text_from_pdf(pdf_path)
            if not text:
                continue
            
            # Detect language
            language = self.detect_language(text)
            print(f"Detected language: {language}")
            
            # Translate if German
            original_text = text
            if language == 'de':
                print("Translating German text to English...")
                text = self.translate_to_english(text)
            
            # Create document with metadata
            doc = Document(
                page_content=text,
                metadata={
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "original_language": language,
                    "translated": language == 'de'
                }
            )
            
            # Split into chunks
            chunks = self.text_splitter.split_documents([doc])
            
            # Extract texts and metadata
            for i, chunk in enumerate(chunks):
                texts.append(chunk.page_content)
                chunk_metadata = chunk.metadata.copy()
                chunk_metadata["chunk_id"] = i
                chunk_metadata["total_chunks"] = len(chunks)
                metadata_list.append(chunk_metadata)
            
            print(f"Created {len(chunks)} chunks from {os.path.basename(pdf_path)}")
        
        return texts, metadata_list

class ChromaDBManager:
    """Manages ChromaDB operations with direct OpenAI embeddings"""
    
    def __init__(self, persist_directory: str = Config.CHROMADB_PATH):
        self.persist_directory = persist_directory
        self.collection_name = "pdf_documents"
        
        # Ensure directory exists
        os.makedirs(persist_directory, exist_ok=True)
        
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # Get or create collection
        try:
            self.collection = self.client.get_collection(name=self.collection_name)
            print(f"Loaded existing ChromaDB collection: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            print(f"Created new ChromaDB collection: {self.collection_name}")
    
    def add_embeddings(self, texts: List[str], embeddings: List[List[float]], metadata_list: List[Dict]):
        """Add texts and embeddings to ChromaDB"""
        print(f"Adding {len(texts)} documents to ChromaDB...")
        
        # Generate IDs
        ids = [f"doc_{i}" for i in range(len(texts))]
        
        # Add to collection
        self.collection.add(
            embeddings=embeddings,
            documents=texts,
            metadatas=metadata_list,
            ids=ids
        )
        
        print("Documents added to ChromaDB successfully!")
    
    def search_similar(self, query_embedding: List[float], k: int = 4) -> Dict[str, Any]:
        """Search for similar documents"""
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        return {
            "documents": results["documents"][0],
            "metadatas": results["metadatas"][0],
            "distances": results["distances"][0]
        }
    
    def get_collection_info(self) -> Dict:
        """Get information about the collection"""
        return {
            "count": self.collection.count(),
            "name": self.collection_name
        }

class CustomRetriever(BaseRetriever):
    """Custom retriever that uses ChromaDB with OpenAI embeddings"""
    
    def __init__(self, chroma_manager: ChromaDBManager, embeddings_manager: OpenAIEmbeddingsManager):
        super().__init__()
        self.chroma_manager = chroma_manager
        self.embeddings_manager = embeddings_manager
    
    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:
        """Retrieve relevant documents for a query"""
        # Create embedding for the query
        query_embeddings = self.embeddings_manager.create_embeddings([query])
        query_embedding = query_embeddings[0]
        
        # Search for similar documents
        results = self.chroma_manager.search_similar(query_embedding)
        
        # Convert to Document objects
        documents = []
        for i, (doc_text, metadata) in enumerate(zip(results["documents"], results["metadatas"])):
            documents.append(Document(
                page_content=doc_text,
                metadata=metadata
            ))
        
        return documents

class RAGChatbot:
    """RAG Chatbot with memory checkpoints"""
    
    def __init__(self, retriever: CustomRetriever):
        self.retriever = retriever
        self.llm = ChatOpenAI(
            model=Config.CHAT_MODEL,
            openai_api_key=Config.OPENAI_API_KEY,
            openai_api_base=Config.OPENAI_BASE_URL,
            temperature=0.7
        )
        
        # Setup memory with file persistence
        os.makedirs(Config.MEMORY_PATH, exist_ok=True)
        self.memory_file = os.path.join(Config.MEMORY_PATH, "chat_history.json")
        
        # Initialize memory with file-based chat history
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            chat_memory=FileChatMessageHistory(self.memory_file),
            output_key="answer",
            return_messages=True
        )
        
        # Create conversational retrieval chain
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.retriever,
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
    
    def chat(self, question: str) -> Dict[str, Any]:
        """Process a chat question and return response with sources"""
        try:
            # Add instruction to ensure English output
            enhanced_question = f"""Please answer the following question in English only, even if the source documents contain German text: {question}"""
            
            result = self.qa_chain({"question": enhanced_question})
            
            return {
                "answer": result["answer"],
                "source_documents": result.get("source_documents", []),
                "sources": list(set([doc.metadata.get("filename", "Unknown") 
                                   for doc in result.get("source_documents", [])]))
            }
        except Exception as e:
            return {
                "answer": f"I encountered an error while processing your question: {str(e)}",
                "source_documents": [],
                "sources": []
            }
    
    def save_memory_checkpoint(self, checkpoint_name: str = "default"):
        """Save current memory state to a checkpoint"""
        checkpoint_path = os.path.join(Config.MEMORY_PATH, f"checkpoint_{checkpoint_name}.pkl")
        try:
            with open(checkpoint_path, 'wb') as f:
                pickle.dump(self.memory.chat_memory.messages, f)
            print(f"Memory checkpoint saved: {checkpoint_name}")
        except Exception as e:
            print(f"Error saving checkpoint: {str(e)}")
    
    def load_memory_checkpoint(self, checkpoint_name: str = "default"):
        """Load memory state from a checkpoint"""
        checkpoint_path = os.path.join(Config.MEMORY_PATH, f"checkpoint_{checkpoint_name}.pkl")
        try:
            if os.path.exists(checkpoint_path):
                with open(checkpoint_path, 'rb') as f:
                    messages = pickle.load(f)
                self.memory.chat_memory.messages = messages
                print(f"Memory checkpoint loaded: {checkpoint_name}")
            else:
                print(f"Checkpoint not found: {checkpoint_name}")
        except Exception as e:
            print(f"Error loading checkpoint: {str(e)}")
    
    def clear_memory(self):
        """Clear conversation memory"""
        self.memory.clear()
        print("Memory cleared!")

def main():
    """Main function to run the RAG system"""
    print("=== RAG Chatbot with German Translation (Direct OpenAI Embeddings) ===")
    print(f"PDF Directory: {Config.PDF_DIRECTORY}")
    print(f"ChromaDB Path: {Config.CHROMADB_PATH}")
    
    # Ensure PDF directory exists
    os.makedirs(Config.PDF_DIRECTORY, exist_ok=True)
    
    # Initialize components
    pdf_processor = PDFProcessor()
    embeddings_manager = OpenAIEmbeddingsManager()
    chroma_manager = ChromaDBManager()
    
    # Check ChromaDB status
    collection_info = chroma_manager.get_collection_info()
    print(f"ChromaDB collection has {collection_info['count']} documents")
    
    # Process PDFs if collection is empty or forced reprocessing
    if collection_info['count'] == 0:
        print("\nProcessing PDFs and creating embeddings...")
        
        # Process PDFs
        texts, metadata_list = pdf_processor.process_pdfs(Config.PDF_DIRECTORY)
        
        if not texts:
            print("No documents found to process. Please add PDF files to the directory.")
            return
        
        print(f"Processed {len(texts)} text chunks from PDFs")
        
        # Check cache first
        cached_texts, cached_embeddings, cached_metadata = embeddings_manager.load_embeddings_cache()
        
        if len(cached_texts) == len(texts) and cached_texts == texts:
            print("Using cached embeddings...")
            embeddings = cached_embeddings
        else:
            print("Creating fresh embeddings...")
            # Create embeddings using OpenAI API directly
            embeddings = embeddings_manager.create_embeddings(texts)
            
            # Save to cache
            embeddings_manager.save_embeddings_cache(texts, embeddings, metadata_list)
        
        # Add to ChromaDB
        chroma_manager.add_embeddings(texts, embeddings, metadata_list)
        
        print("✅ Documents processed and embeddings saved successfully!")
    else:
        print("✅ Using existing ChromaDB collection")
    
    # Initialize retriever and chatbot
    retriever = CustomRetriever(chroma_manager, embeddings_manager)
    chatbot = RAGChatbot(retriever)
    
    print(f"\n=== Chatbot Ready! ({collection_info['count']} documents loaded) ===")
    print("Commands:")
    print("- Type your questions normally")
    print("- 'save_checkpoint <n>' - Save memory checkpoint")
    print("- 'load_checkpoint <n>' - Load memory checkpoint") 
    print("- 'clear_memory' - Clear conversation history")
    print("- 'info' - Show database info")
    print("- 'quit' - Exit the chatbot")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nYou: ").strip()
            
            if user_input.lower() == 'quit':
                print("Goodbye!")
                break
            
            elif user_input.lower() == 'clear_memory':
                chatbot.clear_memory()
                continue
            
            elif user_input.lower() == 'info':
                info = chroma_manager.get_collection_info()
                print(f"Database info: {info['count']} documents in collection '{info['name']}'")
                continue
            
            elif user_input.startswith('save_checkpoint'):
                parts = user_input.split(' ', 1)
                checkpoint_name = parts[1] if len(parts) > 1 else "default"
                chatbot.save_memory_checkpoint(checkpoint_name)
                continue
            
            elif user_input.startswith('load_checkpoint'):
                parts = user_input.split(' ', 1)
                checkpoint_name = parts[1] if len(parts) > 1 else "default"
                chatbot.load_memory_checkpoint(checkpoint_name)
                continue
            
            elif not user_input:
                continue
            
            # Process question
            print("Assistant: Processing your question...")
            response = chatbot.chat(user_input)
            
            print(f"\nAssistant: {response['answer']}")
            
            if response['sources']:
                print(f"\nSources: {', '.join(response['sources'])}")
        
        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
        except Exception as e:
            print(f"Error: {str(e)}")

if __name__ == "__main__":
    # Check if required packages are installed
    required_packages = [
        "PyPDF2", "langdetect", "chromadb", "langchain", "langchain-openai", 
        "langchain-community", "openai", "numpy"
    ]
    
    print("Required packages:", ", ".join(required_packages))
    print("Install with: pip install " + " ".join(required_packages))
    print()
    
    main()
