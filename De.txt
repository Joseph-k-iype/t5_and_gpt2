"""
GDC Record Class Mapping System using OpenAI o3-mini with LangGraph ReAct Agents
Supports ONE-TO-MANY mappings with RAG using text-embedding-3-large
Uses OpenAI API directly for embeddings (no tiktoken)
Implements mixture of experts pattern with dynamic chain of thought reasoning
Uses Pydantic v2 for validation and latest LangChain/LangGraph
Uses InMemoryVectorStore for semantic search
"""

import json
import os
from typing import List, Dict, Optional, Any
import pandas as pd
from pydantic import BaseModel, Field, field_validator
from langchain_openai import ChatOpenAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
import re
from openai import OpenAI
import time

# ==================== GLOBAL CONFIGURATION ====================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_MODEL = "o3-mini"
REASONING_EFFORT = "high"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Validate API key before initializing clients
if not OPENAI_API_KEY:
    print("WARNING: OPENAI_API_KEY is not set. Please set it before running.")

# Initialize OpenAI client for embeddings with proper configuration
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    timeout=60.0,  # 60 second timeout
    max_retries=3   # Retry up to 3 times
)

# Initialize global LLM instance with reasoning_effort as direct parameter
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    reasoning_effort=REASONING_EFFORT
)

# ==================== CUSTOM EMBEDDINGS CLASS ====================

# ==================== CUSTOM EMBEDDINGS CLASS ====================

class OpenAIDirectEmbeddings(Embeddings):
    """Custom embeddings class using OpenAI API directly without tiktoken"""
    
    def __init__(self, model: str = EMBEDDING_MODEL, dimensions: int = EMBEDDING_DIMENSIONS):
        self.model = model
        self.dimensions = dimensions
        self.client = openai_client
        self.batch_size = 50  # Reduced batch size for stability
        self.max_chars = 30000  # Approximate character limit (8192 tokens ≈ 30k chars)
        self.chunk_overlap = 200  # Character overlap between chunks
    
    def chunk_text(self, text: str) -> List[str]:
        """Dynamically chunk text if it exceeds character limit"""
        if len(text) <= self.max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            # Calculate end position
            end = start + self.max_chars
            
            # If not at the end, try to break at sentence boundary
            if end < len(text):
                # Look for sentence endings near the chunk boundary
                search_start = max(start, end - 500)
                sentence_ends = ['.', '!', '?', '\n\n']
                
                best_break = None
                for delimiter in sentence_ends:
                    pos = text.rfind(delimiter, search_start, end)
                    if pos != -1:
                        best_break = pos + 1
                        break
                
                # If no sentence boundary found, break at word boundary
                if best_break is None:
                    pos = text.rfind(' ', search_start, end)
                    if pos != -1:
                        best_break = pos
                    else:
                        best_break = end
                
                end = best_break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # Move start position with overlap
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def embed_single_text(self, text: str) -> List[float]:
        """Embed a single text with automatic chunking if needed"""
        chunks = self.chunk_text(text)
        
        if len(chunks) == 1:
            # Single chunk - embed directly
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[text],
                    dimensions=self.dimensions
                )
                return response.data[0].embedding
            except Exception as e:
                error_msg = str(e).lower()
                if "maximum" in error_msg and "token" in error_msg:
                    # Even our estimate was wrong, force chunk more aggressively
                    print(f"  ⚠ Text still too long, forcing more aggressive chunking...")
                    smaller_chunks = self.chunk_text_aggressive(text)
                    return self.average_embeddings(smaller_chunks)
                raise
        else:
            # Multiple chunks - embed each and average
            print(f"  📄 Text chunked into {len(chunks)} parts for embedding")
            return self.average_embeddings(chunks)
    
    def chunk_text_aggressive(self, text: str, max_chars: int = 15000) -> List[str]:
        """More aggressive chunking for very long texts"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + max_chars, len(text))
            
            # Try to break at word boundary
            if end < len(text):
                space_pos = text.rfind(' ', start, end)
                if space_pos > start:
                    end = space_pos
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def average_embeddings(self, chunks: List[str]) -> List[float]:
        """Create embeddings for chunks and return averaged embedding"""
        chunk_embeddings = []
        
        for i, chunk in enumerate(chunks):
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[chunk],
                    dimensions=self.dimensions
                )
                chunk_embeddings.append(response.data[0].embedding)
                time.sleep(0.2)  # Small delay between chunks
            except Exception as e:
                print(f"  ✗ Failed to embed chunk {i+1}/{len(chunks)}: {e}")
                # Skip failed chunks
                continue
        
        if not chunk_embeddings:
            # All chunks failed, return zero vector
            return [0.0] * self.dimensions
        
        # Average the embeddings
        avg_embedding = [
            sum(emb[i] for emb in chunk_embeddings) / len(chunk_embeddings)
            for i in range(self.dimensions)
        ]
        
        return avg_embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using OpenAI API directly with batch processing and chunking"""
        if not texts:
            return []
        
        print(f"Embedding {len(texts)} documents with dynamic chunking...")
        all_embeddings = []
        
        try:
            # Process documents individually to handle chunking properly
            for i, text in enumerate(texts):
                if (i + 1) % 10 == 0:
                    print(f"  Progress: {i+1}/{len(texts)} documents...")
                
                try:
                    # Check if text needs chunking
                    if len(text) > self.max_chars:
                        print(f"  📏 Document {i+1} is long ({len(text)} chars), chunking...")
                    
                    embedding = self.embed_single_text(text)
                    all_embeddings.append(embedding)
                    
                    # Rate limiting
                    if (i + 1) % self.batch_size == 0:
                        time.sleep(1.0)  # Longer delay after batch
                    else:
                        time.sleep(0.1)  # Short delay between documents
                    
                except Exception as doc_error:
                    print(f"  ✗ Failed to embed document {i+1}: {doc_error}")
                    print(f"    Document length: {len(text)} chars")
                    
                    # Add zero vector as fallback
                    all_embeddings.append([0.0] * self.dimensions)
            
            print(f"✓ Successfully embedded {len(all_embeddings)}/{len(texts)} documents")
            return all_embeddings
            
        except Exception as e:
            print(f"✗ CRITICAL ERROR embedding documents: {e}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error details: {str(e)}")
            print(f"  Model: {self.model}")
            print(f"  Dimensions: {self.dimensions}")
            
            # Return zero vectors as fallback
            print(f"  Returning zero vectors as fallback")
            return [[0.0] * self.dimensions for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using OpenAI API directly with chunking support"""
        try:
            print(f"Embedding query ({len(text)} chars): {text[:50]}...")
            
            # Use chunking if needed
            embedding = self.embed_single_text(text)
            
            print(f"✓ Query embedded successfully")
            return embedding
            
        except Exception as e:
            print(f"✗ Error embedding query: {e}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error details: {str(e)}")
            print(f"  Query length: {len(text)} chars")
            
            # Return zero vector as fallback
            print(f"  Returning zero vector as fallback")
            return [0.0] * self.dimensions

# Initialize global custom embeddings instance
embeddings = OpenAIDirectEmbeddings(model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSIONS)

# Global vector stores
gdc_master_vectorstore: Optional[InMemoryVectorStore] = None
gdc_context_vectorstore: Optional[InMemoryVectorStore] = None

# ==================== PYDANTIC V2 MODELS ====================

class GDCMaster(BaseModel):
    """Pydantic model for GDC Master data"""
    data_domain: str = Field(alias="Data Domain", default="")
    gdc_name: str = Field(alias="GDC Name")
    definition: str = Field(alias="Definition", default="")
    
    class Config:
        populate_by_name = True

class ProcessInfo(BaseModel):
    """Pydantic model for Process information"""
    process_name: str = Field(alias="Process Name", default="")
    process_description: str = Field(alias="Process Description", default="")
    
    class Config:
        populate_by_name = True

class AppInfo(BaseModel):
    """Pydantic model for Application information"""
    app_id: str = Field(alias="App ID", default="")
    app_name: str = Field(alias="App Name", default="")
    app_description: str = Field(alias="App Description", default="")
    processes: List[ProcessInfo] = Field(default_factory=list, alias="Processes")
    
    class Config:
        populate_by_name = True

class PBTInfo(BaseModel):
    """Pydantic model for PBT information"""
    pbt_id: str = Field(alias="PBT ID", default="")
    pbt_name: str = Field(alias="PBT Name", default="")
    pbt_desc: str = Field(alias="PBT Desc", default="")
    apps: List[AppInfo] = Field(default_factory=list, alias="Apps")
    
    class Config:
        populate_by_name = True

class GDCWithContext(BaseModel):
    """Pydantic model for GDC with Context data"""
    gdc_id: str = Field(alias="GDC ID", default="")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    pbts: List[PBTInfo] = Field(default_factory=list, alias="PBTs")
    
    class Config:
        populate_by_name = True

class ValidationEntry(BaseModel):
    """Pydantic model for Validation data"""
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class RecordClass(BaseModel):
    """Pydantic model for Record Class data"""
    guid: str = Field(alias="Guid", default="")
    code: str = Field(alias="Code", default="")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description", default="")
    
    class Config:
        populate_by_name = True

class SemanticMatch(BaseModel):
    """Pydantic model for Semantic Match result"""
    gdc_name: str = Field(description="Name of the matched GDC")
    gdc_description: str = Field(description="Description of the matched GDC", default="")
    similarity_score: float = Field(ge=0, le=100, description="Similarity score between 0-100")
    reasoning: str = Field(description="Detailed reasoning for the match")
    
    @field_validator('similarity_score')
    @classmethod
    def validate_score(cls, v: float) -> float:
        if not 0 <= v <= 100:
            raise ValueError('Similarity score must be between 0 and 100')
        return v

class SemanticMatchResponse(BaseModel):
    """Pydantic model for Semantic Matching Expert response"""
    matches: List[SemanticMatch] = Field(description="List of all relevant semantic matches")
    multiple_matches_rationale: str = Field(description="Explanation of why multiple GDCs are relevant")

class ContextEvidence(BaseModel):
    """Pydantic model for Context Evidence"""
    gdc_name: str = Field(description="Name of the GDC")
    context_evidence: List[str] = Field(description="List of contextual evidence", default_factory=list)
    alignment_score: float = Field(ge=0, le=100, description="Alignment score")
    relevance_justification: str = Field(description="Justification for why this GDC is relevant")
    reasoning: str = Field(description="Detailed reasoning for context analysis")

class ContextAnalysisResponse(BaseModel):
    """Pydantic model for Context Analysis Expert response"""
    context_analysis: List[ContextEvidence] = Field(description="List of context analyses for all relevant GDCs")
    all_relevant_gdcs: List[str] = Field(description="List of all GDC names that are relevant")

class ValidationMatchEntry(BaseModel):
    """Pydantic model for a single validation match entry"""
    gdc_name: str = Field(description="GDC name from validation", default="")
    ilm_category_name: str = Field(description="ILM category name from validation", default="")

class ValidationResultItem(BaseModel):
    """Pydantic model for a single validation result"""
    gdc_name: str = Field(description="GDC being validated")
    validation_found: bool = Field(description="Whether validation entry was found")
    matching_entry: Optional[ValidationMatchEntry] = Field(None, description="Matching entry if found")
    validation_status: str = Field(description="Status: confirmed/conflicted/not_found")
    validation_reasoning: str = Field(description="Reasoning for this validation")

class ValidationResponse(BaseModel):
    """Pydantic model for Validation Expert response"""
    validation_results: List[ValidationResultItem] = Field(description="Validation results for each proposed GDC")
    overall_validation_reasoning: str = Field(description="Overall validation reasoning")

class SingleGDCMapping(BaseModel):
    """Pydantic model for a single GDC mapping"""
    gdc_name: str = Field(description="GDC name")
    gdc_description: str = Field(description="GDC description")
    mapping_rank: int = Field(ge=1, description="Rank of this mapping")
    reasoning: str = Field(description="Comprehensive reasoning for this mapping")
    evidence_summary: List[str] = Field(description="Summary of evidence", default_factory=list)

class FinalMappingDecision(BaseModel):
    """Pydantic model for Final Mapping Decision"""
    gdc_mappings: List[SingleGDCMapping] = Field(description="All relevant GDC mappings")
    overall_reasoning: str = Field(description="Overall reasoning for the mapping decisions")

class MappingResult(BaseModel):
    """Pydantic model for final mapping result"""
    guid: str = Field(alias="GUID")
    code: str = Field(alias="Code")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description")
    mapping_rank: int = Field(alias="Mapping Rank")
    reasoning: str = Field(alias="Reasoning")
    
    class Config:
        populate_by_name = True

# ==================== TEXT PREPROCESSING ====================

def to_lowercase(text: str) -> str:
    """Convert text to lowercase for consistent processing"""
    return text.lower() if text else ""

def preprocess_text(text: str) -> str:
    """Preprocess text: lowercase and clean"""
    text = to_lowercase(text)
    text = " ".join(text.split())
    return text

# ==================== CONTEXT ENGINEERING ====================

def create_enriched_gdc_master_document(gdc: GDCMaster) -> Document:
    """Create context-enriched document for GDC Master with metadata"""
    gdc_name_lower = preprocess_text(gdc.gdc_name)
    definition_lower = preprocess_text(gdc.definition)
    domain_lower = preprocess_text(gdc.data_domain)
    
    enriched_content = f"""gdc category name: {gdc_name_lower}
data domain: {domain_lower}
definition and description: {definition_lower}
this is a group data category for classification purposes
keywords: {gdc_name_lower} {domain_lower}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "data_domain": domain_lower,
        "definition": definition_lower,
        "type": "gdc_master"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

def create_enriched_gdc_context_document(gdc_ctx: GDCWithContext) -> Document:
    """Create context-enriched document for GDC with hierarchical context"""
    gdc_name_lower = preprocess_text(gdc_ctx.gdc_name)
    gdc_desc_lower = preprocess_text(gdc_ctx.gdc_description)
    
    pbt_names = []
    app_names = []
    process_names = []
    
    for pbt in gdc_ctx.pbts:
        pbt_name_lower = preprocess_text(pbt.pbt_name)
        pbt_desc_lower = preprocess_text(pbt.pbt_desc)
        pbt_names.append(f"{pbt_name_lower} ({pbt_desc_lower})")
        
        for app in pbt.apps:
            app_name_lower = preprocess_text(app.app_name)
            app_desc_lower = preprocess_text(app.app_description)
            app_names.append(f"{app_name_lower} - {app_desc_lower}")
            
            for proc in app.processes:
                proc_name_lower = preprocess_text(proc.process_name)
                proc_desc_lower = preprocess_text(proc.process_description)
                process_names.append(f"{proc_name_lower}: {proc_desc_lower}")
    
    enriched_content = f"""gdc category: {gdc_name_lower}
gdc_description: {gdc_desc_lower}

primary business types (pbts):
{chr(10).join(f"- {pbt}" for pbt in pbt_names) if pbt_names else "- none"}

applications using this gdc:
{chr(10).join(f"- {app}" for app in app_names) if app_names else "- none"}

related processes:
{chr(10).join(f"- {proc}" for proc in process_names) if process_names else "- none"}

contextual keywords: {gdc_name_lower} {' '.join(pbt_names)} {' '.join(app_names)}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "gdc_description": gdc_desc_lower,
        "pbt_count": len(pbt_names),
        "app_count": len(app_names),
        "process_count": len(process_names),
        "type": "gdc_context"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

# ==================== UTILITY FUNCTIONS ====================

def extract_json_from_text(text: str) -> str:
    """Extract JSON from text that might contain markdown code blocks"""
    text = re.sub(r'```(?:json)?\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    
    json_match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
    if json_match:
        return json_match.group(1)
    
    return text

def load_json_file(filepath: str, model_class: type[BaseModel]) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except Exception as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== RAG VECTOR STORE SETUP ====================

def build_gdc_master_vectorstore(gdc_master_list: List[GDCMaster]) -> InMemoryVectorStore:
    """Build vector store for GDC Master data using OpenAI API directly"""
    print(f"🔧 Building GDC Master vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_master_document(gdc) for gdc in gdc_master_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Master vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def build_gdc_context_vectorstore(gdc_context_list: List[GDCWithContext]) -> InMemoryVectorStore:
    """Build vector store for GDC Context data using OpenAI API directly"""
    print(f"🔧 Building GDC Context vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_context_document(gdc_ctx) for gdc_ctx in gdc_context_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Context vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def rag_retrieve_relevant_gdcs(query: str, k: int = 10) -> str:
    """RAG: Retrieve relevant GDCs using semantic search"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    query_lower = preprocess_text(query)
    
    master_results = gdc_master_vectorstore.similarity_search(query_lower, k=k)
    context_results = gdc_context_vectorstore.similarity_search(query_lower, k=k)
    
    retrieved_info = {
        "master_matches": [],
        "context_matches": []
    }
    
    for doc in master_results:
        retrieved_info["master_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "definition": doc.metadata.get("definition", ""),
            "data_domain": doc.metadata.get("data_domain", "")
        })
    
    for doc in context_results:
        retrieved_info["context_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "gdc_description": doc.metadata.get("gdc_description", ""),
            "pbt_count": doc.metadata.get("pbt_count", 0),
            "app_count": doc.metadata.get("app_count", 0),
            "process_count": doc.metadata.get("process_count", 0),
            "content_preview": doc.page_content[:300]
        })
    
    return json.dumps(retrieved_info, indent=2)

# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str) -> str:
    """
    Expert for comprehensive semantic similarity analysis using RAG with OpenAI embeddings API.
    
    This expert performs deep semantic analysis to identify ALL potentially relevant GDCs 
    that could apply to the given Record Class. The analysis considers:
    
    1. Direct semantic similarity between record and GDC names/descriptions
    2. Conceptual relationships and domain alignment
    3. Functional equivalence and purpose matching
    4. Terminology variations and synonyms
    
    The expert uses RAG (Retrieval Augmented Generation) with vector embeddings to find
    semantically similar GDCs from the entire knowledge base, ensuring comprehensive coverage.
    
    Args:
        record_name: The name of the Record Class to analyze
        record_desc: The detailed description of the Record Class
    
    Returns:
        JSON string containing all semantically matched GDCs with scores and detailed reasoning
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    query = f"{record_name_lower} {record_desc_lower}"
    retrieved_gdcs = rag_retrieve_relevant_gdcs(query, k=15)
    
    prompt = f"""You are a semantic similarity expert specializing in Group Data Category (GDC) classification with access to RAG-retrieved knowledge.

CRITICAL CONTEXT: A single Record Class can and often should map to MULTIPLE GDCs because:
- Record classes may contain data from multiple functional domains
- Different aspects of a record may align with different GDC categories
- Business requirements may necessitate multiple classification paths
- Comprehensive data governance requires multi-dimensional categorization

YOUR TASK: Perform comprehensive semantic analysis to identify ALL potentially relevant GDCs.

RECORD CLASS TO ANALYZE:
Name: {record_name_lower}
Description: {record_desc_lower}

RAG-RETRIEVED RELEVANT GDCs (via OpenAI embeddings API with semantic vector search):
{retrieved_gdcs}

ANALYSIS METHODOLOGY:
1. SEMANTIC MATCHING: Analyze linguistic and conceptual similarity between the record and each GDC
   - Compare names, keywords, and terminology
   - Identify synonyms and related concepts
   - Consider domain-specific language patterns

2. FUNCTIONAL ALIGNMENT: Assess how the record's purpose aligns with each GDC's scope
   - What data does the record contain?
   - What business functions does it support?
   - What processes use this record?

3. COMPREHENSIVE SCORING: Rate each potential match from 0-100 based on:
   - Name similarity (0-30 points)
   - Description/definition alignment (0-40 points)
   - Domain and context relevance (0-30 points)
   
4. MULTIPLE MATCH IDENTIFICATION: Explicitly identify when multiple GDCs are relevant
   - Don't just find the "best" match - find ALL valid matches
   - Consider different aspects of the record that may map to different GDCs
   - Include GDCs with scores above 60 as potentially relevant

5. DETAILED REASONING: For each match, explain:
   - Why this GDC is semantically related to the record
   - What specific aspects create the connection
   - How strong the relationship is and why

OUTPUT FORMAT (must be valid JSON only, no markdown, no explanation text):
{{
  "matches": [
    {{
      "gdc_name": "exact gdc name from rag results",
      "gdc_description": "exact definition from rag results",
      "similarity_score": 88.5,
      "reasoning": "Comprehensive explanation: This GDC is highly relevant because [specific semantic connections]. The record name '{record_name_lower}' directly relates to [GDC aspects]. The description indicates [functional alignment]. Key semantic markers include [specific terms/concepts]. Score breakdown: Name similarity (25/30), Description alignment (35/40), Domain relevance (28.5/30)."
    }},
    {{
      "gdc_name": "another relevant gdc",
      "gdc_description": "its definition",
      "similarity_score": 76.0,
      "reasoning": "Detailed explanation of why this is also relevant..."
    }}
  ],
  "multiple_matches_rationale": "Comprehensive explanation: Multiple GDCs are relevant because this record class encompasses [multiple aspects/domains/functions]. Specifically: [GDC 1] addresses [aspect 1], while [GDC 2] covers [aspect 2]. This multi-dimensional mapping ensures complete data governance coverage and aligns with [business requirements/regulatory needs/operational practices]."
}}

CRITICAL REQUIREMENTS:
- Return ALL semantically relevant GDCs, not just the top match
- Include detailed, evidence-based reasoning for each match
- Consider scores above 60 as potentially relevant
- Explain multi-dimensional aspects that justify multiple mappings
- Output ONLY valid JSON with no additional text, markdown, or explanations
- Use exact GDC names and definitions from the RAG-retrieved results"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = SemanticMatchResponse(
            matches=[],
            multiple_matches_rationale=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, semantic_matches: str) -> str:
    """
    Expert for comprehensive context analysis using RAG retrieval with OpenAI embeddings API.
    
    This expert performs deep contextual analysis by examining the hierarchical relationships
    and operational context of GDCs. The analysis includes:
    
    1. Primary Business Type (PBT) alignment - which business areas use this GDC
    2. Application context - which systems and applications utilize this GDC
    3. Process relationships - which business processes involve this GDC
    4. Organizational and functional hierarchies
    
    The expert validates semantic matches by checking if the operational context supports
    the mapping, ensuring that GDCs are not only semantically similar but also contextually
    appropriate for the record class.
    
    Args:
        record_name: The name of the Record Class
        record_desc: The description of the Record Class
        semantic_matches: JSON string of semantic matches from the semantic expert
    
    Returns:
        JSON string containing context analysis for all relevant GDCs with evidence
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    try:
        matches_data = json.loads(extract_json_from_text(semantic_matches))
        gdc_names = [m.get("gdc_name", "").lower() for m in matches_data.get("matches", [])]
    except:
        gdc_names = []
    
    context_query = f"{record_name_lower} {record_desc_lower} {' '.join(gdc_names)}"
    retrieved_context = rag_retrieve_relevant_gdcs(context_query, k=10)
    
    prompt = f"""You are a context analysis expert specializing in validating GDC mappings through operational and hierarchical context using RAG-retrieved information.

RECORD CLASS TO ANALYZE:
Name: {record_name_lower}
Description: {record_desc_lower}

SEMANTIC MATCHES TO VALIDATE:
{semantic_matches}

RAG-RETRIEVED CONTEXTUAL INFORMATION (via OpenAI embeddings API):
{retrieved_context}

YOUR MISSION: Validate and strengthen each semantic match by analyzing the operational context and organizational hierarchy. Context includes:
- Primary Business Types (PBTs) - business areas that use the GDC
- Applications - systems that manage data classified under the GDC  
- Processes - business workflows involving the GDC
- Organizational structures and functional domains

ANALYSIS METHODOLOGY:

1. CONTEXT EXTRACTION: For each semantically matched GDC, extract:
   - All associated PBTs and their descriptions
   - All applications using this GDC
   - All processes that involve this GDC
   - Hierarchical relationships (PBT → App → Process)

2. ALIGNMENT ASSESSMENT: Evaluate alignment between record class and GDC context:
   - Does the record's purpose align with the PBTs using this GDC?
   - Would applications using this GDC logically create/manage this record?
   - Do the processes match the record's business function?

3. EVIDENCE GATHERING: Build concrete evidence list for each GDC:
   - "PBT: [name] - [description] - relevant because [reason]"
   - "Application: [name] - [description] - relevant because [reason]"
   - "Process: [name] - [description] - relevant because [reason]"

4. ALIGNMENT SCORING: Rate contextual alignment from 0-100:
   - PBT alignment (0-40 points)
   - Application relevance (0-30 points)
   - Process matching (0-30 points)

5. KEEP ALL VALID GDCS: Do not eliminate GDCs with supporting context
   - Even if context is limited, if semantic match was strong, keep the GDC
   - Explain what context is available and what it means
   - Only remove GDCs with contradictory or conflicting context

6. RELEVANCE JUSTIFICATION: For each GDC, explain:
   - How the operational context supports this mapping
   - What organizational structures align with this classification
   - Why this GDC makes sense given the hierarchical relationships

OUTPUT FORMAT (must be valid JSON only, no markdown, no explanation text):
{{
  "context_analysis": [
    {{
      "gdc_name": "exact gdc name",
      "context_evidence": [
        "PBT: [pbt name] ([pbt description]) - This PBT is relevant because [specific alignment with record class purpose/function]",
        "Application: [app name] - [app description] - This application is relevant because [how it would use/create this record]",
        "Process: [process name]: [process description] - This process is relevant because [connection to record's business function]"
      ],
      "alignment_score": 85.5,
      "relevance_justification": "Comprehensive explanation: This GDC's operational context strongly supports the mapping because [PBT analysis], [Application analysis], [Process analysis]. The hierarchical structure shows [organizational alignment]. Score breakdown: PBT alignment (35/40), Application relevance (25/30), Process matching (25.5/30).",
      "reasoning": "Detailed contextual reasoning: The RAG-retrieved context reveals [specific findings]. The PBT '[name]' directly aligns with [record aspect]. Applications like '[app name]' would logically [create/manage/process] records of this type because [reason]. The processes '[process names]' involve [activities] that match the record's [purpose/function]. Even though [limitation if any], the overall context strongly validates this mapping."
    }}
  ],
  "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
}}

CRITICAL REQUIREMENTS:
- Preserve ALL semantically matched GDCs unless context explicitly contradicts them
- Extract detailed, specific evidence from PBTs, Applications, and Processes
- Provide comprehensive justification for why context supports each mapping
- Include ALL GDC names in the all_relevant_gdcs list
- Output ONLY valid JSON with no additional text, markdown, or explanations
- Be thorough - more context evidence is better than less"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            all_relevant_gdcs=[]
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdcs: str, validation_data: str) -> str:
    """
    Expert for comprehensive validation of proposed GDC mappings against historical data.
    
    This expert cross-references proposed mappings with a validation dataset containing
    historical GDC-to-ILM mappings. The validation process:
    
    1. Searches for each proposed GDC in the validation set
    2. Identifies confirmations (validation data supports the mapping)
    3. Identifies conflicts (validation data suggests different mapping)
    4. Documents cases where no validation data exists (not_found)
    
    Validation adds confidence to mappings by showing historical precedent or
    highlighting potential issues that need review.
    
    Args:
        record_name: The name of the Record Class being validated
        proposed_gdcs: JSON string of proposed GDCs from previous experts
        validation_data: JSON string containing validation dataset
    
    Returns:
        JSON string containing validation results for each proposed GDC
    """
    record_name_lower = preprocess_text(record_name)
    
    try:
        val_data = json.loads(validation_data)
        val_data_lower = []
        for entry in val_data:
            val_data_lower.append({
                "gdc_name": to_lowercase(entry.get("GDC Name", "")),
                "gdc_description": to_lowercase(entry.get("GDC Description", "")),
                "ilm_category_name": to_lowercase(entry.get("ILM Category Name", ""))
            })
        validation_data_lower = json.dumps(val_data_lower, indent=2)
    except:
        validation_data_lower = validation_data
    
    prompt = f"""You are a validation expert specializing in cross-referencing proposed GDC mappings against historical validation data.

RECORD CLASS NAME: {record_name_lower}

PROPOSED GDC MAPPINGS TO VALIDATE:
{proposed_gdcs}

VALIDATION DATASET (Historical GDC-ILM Mappings):
{validation_data_lower}

YOUR TASK: Validate each proposed GDC mapping by searching the validation dataset for supporting or conflicting evidence.

VALIDATION METHODOLOGY:

1. SYSTEMATIC SEARCH: For each proposed GDC:
   - Search validation dataset for exact GDC name matches
   - Search for partial matches or similar names
   - Identify any related entries

2. VALIDATION STATUS DETERMINATION:
   - "confirmed": GDC found in validation set, supports this mapping
   - "conflicted": GDC found but suggests different classification
   - "not_found": No validation data available for this GDC

3. EVIDENCE DOCUMENTATION: For each validation result:
   - If FOUND: Document the matching entry details (GDC name, ILM category)
   - Explain how validation data supports or conflicts with the mapping
   - Note any additional context from the validation entry

4. COMPREHENSIVE REASONING: Provide detailed explanation:
   - What was found in the validation dataset
   - How it relates to the proposed mapping
   - Whether it increases or decreases confidence in the mapping
   - What implications the validation has for the final decision

5. OVERALL ASSESSMENT: Synthesize findings across all proposed GDCs:
   - How many mappings are confirmed vs conflicted vs unvalidated
   - What patterns emerge from the validation
   - What the validation tells us about mapping confidence

OUTPUT FORMAT (must be valid JSON only, no markdown, no explanation text):
{{
  "validation_results": [
    {{
      "gdc_name": "exact proposed gdc name",
      "validation_found": true,
      "matching_entry": {{
        "gdc_name": "exact match from validation set",
        "ilm_category_name": "associated ilm category"
      }},
      "validation_status": "confirmed",
      "validation_reasoning": "Comprehensive explanation: The validation dataset contains an entry for '{record_name_lower}' mapped to GDC '[gdc name]' with ILM category '[category]'. This CONFIRMS the proposed mapping because [detailed explanation of why validation supports it]. The ILM category '[category]' aligns with [record characteristics]. Historical precedent: [what this tells us about prior classification decisions]. This validation [increases/maintains] confidence in this mapping by [amount/degree] because [reasons]."
    }},
    {{
      "gdc_name": "another proposed gdc",
      "validation_found": false,
      "matching_entry": null,
      "validation_status": "not_found",
      "validation_reasoning": "Comprehensive explanation: No validation entry found for GDC '[gdc name]' in the dataset. This does NOT invalidate the mapping - it simply means there's no historical precedent available. The mapping is based on [semantic similarity/contextual analysis]. Lack of validation suggests [this might be a new use case / emerging classification / edge case]. We should [proceed with mapping based on analytical evidence / flag for manual review / etc.]."
    }}
  ],
  "overall_validation_reasoning": "Comprehensive validation summary: Out of [N] proposed GDC mappings, [X] were confirmed by validation data, [Y] were not found in validation, and [Z] showed conflicts. The confirmed mappings ([list]) have strong historical support, increasing confidence in these classifications. The unvalidated mappings ([list]) lack precedent but are supported by [semantic/contextual analysis]. Overall, the validation [strongly supports / moderately supports / provides limited support for] the proposed mappings because [synthesis of all validation findings]. Confidence level: [high/medium/low] based on [validation coverage and confirmation rate]."
}}

CRITICAL REQUIREMENTS:
- Validate EVERY proposed GDC systematically
- Provide detailed reasoning for each validation result
- Explain the implications of validation findings
- Distinguish between "no match found" and "conflicting match"
- Output ONLY valid JSON with no additional text, markdown, or explanations
- Be thorough in explaining how validation affects mapping confidence"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ValidationResponse(
            validation_results=[],
            overall_validation_reasoning=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    Expert for synthesizing all analyses and making final mapping decisions.
    
    This expert is the final arbiter that:
    
    1. Reviews all evidence from semantic, contextual, and validation analyses
    2. Synthesizes findings into coherent, ranked mapping decisions
    3. Resolves any conflicts between different types of analysis
    4. Provides comprehensive justification for each final mapping
    5. Ranks multiple mappings by relevance and confidence
    
    The final decision is evidence-based, considering all analytical dimensions
    and providing transparent reasoning for the classifications.
    
    Args:
        record_info: JSON string containing record class information
        all_analyses: JSON string containing all expert analyses
    
    Returns:
        JSON string containing final ranked GDC mappings with comprehensive reasoning
    """
    prompt = f"""You are the final decision expert responsible for synthesizing all analytical evidence and making definitive GDC mapping decisions.

RECORD INFORMATION:
{record_info}

COMPREHENSIVE ANALYTICAL EVIDENCE:
{all_analyses}

YOUR MISSION: Make final, evidence-based decisions on which GDCs this record class should map to, with complete justification.

DECISION-MAKING METHODOLOGY:

1. EVIDENCE SYNTHESIS: Review all available evidence:
   - Semantic similarity scores and reasoning
   - Contextual alignment (PBTs, Applications, Processes)
   - Validation results (confirmed/conflicted/not_found)
   - Convergence vs divergence across analytical dimensions

2. CONFLICT RESOLUTION: When analyses disagree:
   - Weight evidence by reliability (validation > context > semantic)
   - Consider strength of supporting evidence
   - Look for explanatory factors (edge cases, emerging uses, etc.)
   - Make principled decisions with transparent reasoning

3. RELEVANCE RANKING: Order GDCs by overall relevance:
   - Rank 1: Strongest, most confident mapping
   - Rank 2, 3, etc.: Additional relevant GDCs in decreasing confidence
   - Base ranking on: (semantic score * 0.3) + (context score * 0.4) + (validation weight * 0.3)

4. COMPREHENSIVE REASONING: For EACH mapping, provide:
   - Summary of why this GDC is relevant
   - Key evidence supporting the mapping
   - How different analyses converge on this conclusion
   - Confidence level and any caveats
   - Practical implications of this classification

5. EVIDENCE SUMMARY: Compile bullet-point evidence for each GDC:
   - "Semantic: [key semantic match finding]"
   - "Context: [key contextual alignment finding]"
   - "Validation: [key validation finding]"

6. OVERALL RATIONALE: Explain the complete mapping decision:
   - Why this set of GDCs (singular or multiple)
   - How the GDCs relate to each other (if multiple)
   - Why this is the optimal classification
   - What business/governance value it provides

OUTPUT FORMAT (must be valid JSON only, no markdown, no explanation text):
{{
  "gdc_mappings": [
    {{
      "gdc_name": "primary gdc name",
      "gdc_description": "full gdc description/definition",
      "mapping_rank": 1,
      "reasoning": "Comprehensive mapping rationale: This is the PRIMARY GDC mapping because [synthesis of all evidence]. Semantic analysis shows [key findings with score]. Contextual analysis reveals [PBT/App/Process alignments]. Validation [confirms/supports/doesn't contradict] this mapping because [validation findings]. The convergence of evidence across all dimensions strongly supports this classification. Confidence: [HIGH/MEDIUM/LOW] based on [evidence strength]. This mapping means the record will be classified under [domain/category] for purposes of [governance/compliance/management].",
      "evidence_summary": [
        "Semantic: Strong name similarity (score: 88) and description alignment indicating [specific connection]",
        "Context: Aligned with PBT '[name]', used by application '[name]', involved in process '[name]'",
        "Validation: Confirmed by validation dataset with ILM category '[category]', supporting historical precedent"
      ]
    }},
    {{
      "gdc_name": "secondary gdc name",
      "gdc_description": "full description",
      "mapping_rank": 2,
      "reasoning": "Comprehensive rationale for secondary mapping: This ADDITIONAL GDC mapping is relevant because [specific aspects of record]. While [primary GDC] addresses [aspect 1], this GDC covers [aspect 2]. Evidence: [semantic findings], [contextual findings], [validation findings]. Confidence: [level] based on [evidence]. This secondary mapping provides [additional classification value].",
      "evidence_summary": [
        "Semantic: [finding]",
        "Context: [finding]",
        "Validation: [finding]"
      ]
    }}
  ],
  "overall_reasoning": "Comprehensive decision rationale: This record class maps to [N] GDC(s) because [fundamental explanation]. [If multiple] Multiple GDCs are necessary because this record encompasses [multiple aspects/domains/functions]: [GDC 1] addresses [aspect 1], [GDC 2] addresses [aspect 2], etc. [If single] A single GDC mapping is appropriate because [focused scope explanation]. Evidence synthesis: All analytical dimensions [converge/largely agree] on [these mappings], with [semantic analysis showing X], [contextual analysis revealing Y], and [validation confirming/supporting Z]. Confidence assessment: [Overall confidence level - HIGH/MEDIUM/LOW] based on [evidence quality, convergence, validation support]. Business implications: These mappings enable [governance outcomes], support [compliance requirements], and facilitate [data management practices]. The classification scheme [achieves intended objectives]."
}}

CRITICAL REQUIREMENTS:
- Include ALL relevant GDCs with proper ranking
- Provide comprehensive, evidence-based reasoning for EACH mapping
- Synthesize semantic + contextual + validation evidence
- Explain multi-dimensional aspects if multiple GDCs
- Include detailed evidence summary for each GDC
- Provide thorough overall reasoning explaining the complete decision
- Output ONLY valid JSON with no additional text, markdown, or explanations
- Reasoning should be substantial - at least 3-5 detailed sentences per mapping"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = FinalMappingDecision(
            gdc_mappings=[],
            overall_reasoning=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an expert GDC mapping coordinator using RAG (Retrieval Augmented Generation) and a mixture of experts approach.

CRITICAL UNDERSTANDING: Record Classes can and often should map to MULTIPLE GDCs. This is not an error - it's an expected outcome when a record class encompasses multiple functional domains or data categories.

YOUR WORKFLOW - Execute these steps in order:

1. SEMANTIC MATCHING (Required First Step)
   - Call semantic_similarity_expert with record name and description
   - This expert uses RAG with OpenAI embeddings API for vector similarity search
   - Identifies ALL semantically relevant GDCs, not just the top match
   - Returns comprehensive matches with scores and reasoning

2. CONTEXT ANALYSIS (Required Second Step)
   - Call context_analysis_expert with record info and semantic matches
   - This expert uses RAG to retrieve operational context (PBTs, Apps, Processes)
   - Validates semantic matches through contextual evidence
   - Keeps all GDCs with supporting context

3. VALIDATION (Required Third Step)
   - Call validation_expert with record name, proposed GDCs, and validation dataset
   - Cross-references against historical mappings
   - Identifies confirmed vs conflicted vs not_found status
   - Provides validation reasoning for each GDC

4. FINAL DECISION (Required Last Step)
   - Call final_decision_expert with all gathered analyses
   - Synthesizes evidence from all experts
   - Ranks GDCs by relevance and confidence
   - Provides comprehensive reasoning for each mapping
   - Explains overall rationale for the mapping decisions

IMPORTANT NOTES:
- All text processing uses lowercase for consistency
- RAG retrieval uses OpenAI embeddings API (not tiktoken)
- Focus on evidence quality, not just quantity
- Multiple GDCs are expected and appropriate when evidence supports them
- Each expert builds on the previous expert's findings
- The final output should be comprehensive and well-justified"""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    validation_set: List[ValidationEntry]
) -> List[MappingResult]:
    """Process a single record through the ReAct agent"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    query = f"""Map this Record Class to ALL relevant GDCs using the RAG-based mixture of experts workflow:

RECORD CLASS TO MAP:
- GUID: {record.guid}
- Code: {record.code}
- Name: {record.name}
- Description: {record.description}

VALIDATION DATASET:
{validation_json}

REQUIRED WORKFLOW STEPS:
1. Call semantic_similarity_expert(record_name="{record.name}", record_desc="{record.description}")
   - This uses RAG with OpenAI embeddings API for semantic vector search
   - Identifies ALL relevant GDCs with comprehensive reasoning

2. Call context_analysis_expert(record_name="{record.name}", record_desc="{record.description}", semantic_matches="<results from step 1>")
   - This uses RAG to retrieve operational context
   - Validates through PBTs, Applications, and Processes

3. Call validation_expert(record_name="{record.name}", proposed_gdcs="<results from step 2>", validation_data=<validation dataset>)
   - Cross-references against historical mappings
   - Provides validation status for each GDC

4. Call final_decision_expert(record_info="<record details>", all_analyses="<results from steps 1-3>")
   - Synthesizes all evidence
   - Makes ranked mapping decisions
   - Provides comprehensive reasoning

Execute all four steps in order and provide the final decision. The goal is to identify ALL appropriate GDC mappings with strong justification."""
    
    try:
        result = agent.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        print("\n🔍 Analyzing agent messages...")
        messages = result.get("messages", [])
        
        # Extract final decision from tool messages
        final_decision_data = None
        
        # Look through all messages for final_decision_expert tool result
        for msg in messages:
            if isinstance(msg, ToolMessage):
                # Check if this is from final_decision_expert
                if "final_decision_expert" in msg.name:
                    print(f"✓ Found final_decision_expert tool result")
                    try:
                        json_str = extract_json_from_text(msg.content)
                        decision_dict = json.loads(json_str)
                        final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
                        final_decision_data = final_decision_obj.model_dump()
                        print(f"✓ Successfully parsed final decision")
                        break
                    except Exception as parse_error:
                        print(f"⚠ Error parsing final decision from tool message: {parse_error}")
                        continue
        
        # Fallback: check AI messages if tool message not found
        if not final_decision_data:
            print("⚠ No tool message found, checking AI messages...")
            for msg in reversed(messages):
                if isinstance(msg, AIMessage) and msg.content:
                    try:
                        json_str = extract_json_from_text(msg.content)
                        if "gdc_mappings" in json_str:
                            decision_dict = json.loads(json_str)
                            final_decision_obj = FinalMappingDecision.model_validate(decision_dict)
                            final_decision_data = final_decision_obj.model_dump()
                            print(f"✓ Successfully parsed final decision from AI message")
                            break
                    except:
                        continue
        
        if not final_decision_data:
            raise ValueError("Could not extract final decision from agent response")
        
        # Convert to mapping results
        mapping_results = []
        gdc_mappings = final_decision_data.get("gdc_mappings", [])
        overall_reasoning = final_decision_data.get("overall_reasoning", "")
        
        if not gdc_mappings:
            print("⚠ No GDC mappings found in final decision")
            mapping_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="unknown",
                gdc_description="",
                mapping_rank=1,
                reasoning="no valid gdc mappings found in agent response"
            ))
        else:
            print(f"✓ Found {len(gdc_mappings)} GDC mapping(s)")
            for mapping in gdc_mappings:
                gdc_name = mapping.get("gdc_name", "unknown")
                gdc_desc = mapping.get("gdc_description", "")
                rank = mapping.get("mapping_rank", 1)
                print(f"  - Rank {rank}: {gdc_name}")
                
                mapping_results.append(MappingResult(
                    guid=record.guid,
                    code=record.code,
                    name=record.name,
                    description=record.description,
                    gdc_name=gdc_name,
                    gdc_description=gdc_desc,
                    mapping_rank=rank,
                    reasoning=format_mapping_reasoning(mapping, overall_reasoning)
                ))
        
        return mapping_results
        
    except Exception as e:
        print(f"✗ Error processing record: {e}")
        import traceback
        traceback.print_exc()
        return [MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="error",
            gdc_description="",
            mapping_rank=1,
            reasoning=f"error during processing: {str(e)}"
        )]

def extract_final_decision(response_text: str) -> Dict[str, Any]:
    """Extract final decision from agent response"""
    try:
        json_str = extract_json_from_text(response_text)
        decision_data = json.loads(json_str)
        final_decision = FinalMappingDecision.model_validate(decision_data)
        return final_decision.model_dump()
    except Exception as e:
        print(f"⚠ Error extracting final decision: {e}")
        return {
            "gdc_mappings": [],
            "overall_reasoning": f"unable to extract: {str(e)}"
        }

def format_mapping_reasoning(mapping: Dict[str, Any], overall_reasoning: str) -> str:
    """Format reasoning for output"""
    parts = []
    
    reasoning = mapping.get("reasoning", "")
    if reasoning:
        parts.append(f"mapping reasoning:\n{reasoning}")
    
    evidence = mapping.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nevidence summary:\n" + "\n".join(f"• {e}" for e in evidence))
    
    if overall_reasoning and mapping.get("mapping_rank", 1) == 1:
        parts.append(f"\n\noverall context:\n{overall_reasoning}")
    
    return "\n".join(parts)

# ==================== MAIN EXECUTION ====================

def test_openai_connection():
    """Test OpenAI API connection and embedding capability"""
    print("\n🔍 Testing OpenAI API connection...")
    
    if not OPENAI_API_KEY:
        print("❌ OPENAI_API_KEY is not set!")
        return False
    
    print(f"✓ API Key is set (length: {len(OPENAI_API_KEY)})")
    print(f"✓ Base URL: {OPENAI_BASE_URL}")
    print(f"✓ Embedding Model: {EMBEDDING_MODEL}")
    print(f"✓ Embedding Dimensions: {EMBEDDING_DIMENSIONS}")
    
    try:
        print("\n🧪 Testing embedding with a sample text...")
        test_response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=["test connection"],
            dimensions=EMBEDDING_DIMENSIONS
        )
        
        if test_response.data and len(test_response.data[0].embedding) == EMBEDDING_DIMENSIONS:
            print(f"✅ Successfully created test embedding!")
            print(f"✓ Embedding dimension: {len(test_response.data[0].embedding)}")
            return True
        else:
            print(f"❌ Test embedding failed - unexpected response format")
            return False
            
    except Exception as e:
        print(f"❌ Connection test failed!")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")
        
        # Check for common errors
        if "api_key" in str(e).lower() or "authentication" in str(e).lower():
            print("\n💡 Troubleshooting: API Key issue")
            print("   - Verify your API key is correct")
            print("   - Check if the key has necessary permissions")
            print("   - Ensure the key is not expired")
        elif "rate" in str(e).lower() or "limit" in str(e).lower():
            print("\n💡 Troubleshooting: Rate limit issue")
            print("   - Wait a few moments and try again")
            print("   - Check your OpenAI usage limits")
        elif "network" in str(e).lower() or "connection" in str(e).lower():
            print("\n💡 Troubleshooting: Network issue")
            print("   - Check your internet connection")
            print("   - Verify you can access api.openai.com")
            print("   - Check if you're behind a proxy/firewall")
        
        return False

def main():
    """Main execution function"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM")
    print("RAG with OpenAI Embeddings API (Direct) | InMemoryVectorStore")
    print("One-to-Many Mappings | LangGraph ReAct Agents")
    print("No tiktoken - Using OpenAI API directly for embeddings")
    print("=" * 80)
    
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY environment variable not set")
        print("\nPlease set it using:")
        print("  export OPENAI_API_KEY='your-api-key'")
        print("\nOr in Python:")
        print("  os.environ['OPENAI_API_KEY'] = 'your-api-key'")
        return
    
    # Test OpenAI connection before proceeding
    if not test_openai_connection():
        print("\n❌ OpenAI connection test failed. Please fix the issues above before continuing.")
        return
    
    print("\n📁 Loading data...")
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded {len(gdc_master)} gdc master entries")
    print(f"✓ Loaded {len(gdc_context)} gdc context entries")
    print(f"✓ Loaded {len(validation_set)} validation entries")
    print(f"✓ Loaded {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load data")
        return
    
    print("\n🔍 Building RAG vector stores (using OpenAI API directly)...")
    gdc_master_vectorstore = build_gdc_master_vectorstore(gdc_master)
    gdc_context_vectorstore = build_gdc_context_vectorstore(gdc_context)
    
    print("\n🤖 Creating ReAct agent...")
    agent = create_react_agent_workflow()
    print(f"✓ Model: {OPENAI_MODEL}")
    print(f"✓ Reasoning: {REASONING_EFFORT}")
    print(f"✓ Embeddings: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)")
    print(f"✓ Embedding method: OpenAI API Direct (no tiktoken)")
    print(f"✓ Mode: one-to-many with rag")
    
    print("\n🚀 Starting mapping...\n")
    all_results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\nrecord {i}/{len(record_classes)}")
        
        try:
            record_mappings = process_single_record(
                agent=agent,
                record=record,
                validation_set=validation_set
            )
            
            all_results.extend(record_mappings)
            gdc_names = [m.gdc_name for m in record_mappings]
            print(f"✓ mapped to {len(record_mappings)} gdc(s): {', '.join(gdc_names)}")
            
        except Exception as e:
            print(f"✗ error: {e}")
            all_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="error",
                gdc_description="",
                mapping_rank=1,
                reasoning=f"error: {str(e)}"
            ))
    
    print("\n" + "="*80)
    print("💾 Saving...")
    results_dict = [r.model_dump(by_alias=True) for r in all_results]
    df = pd.DataFrame(results_dict)
    df.to_csv("GDC_Mapping_Results.csv", index=False, encoding='utf-8-sig')
    print("✓ Results saved to GDC_Mapping_Results.csv")
    
    print("\n📊 SUMMARY")
    print("="*80)
    print(f"records: {len(record_classes)}")
    print(f"mappings: {len(all_results)}")
    if len(record_classes) > 0:
        print(f"avg mappings/record: {len(all_results)/len(record_classes):.2f}")
    print("="*80)

if __name__ == "__main__":
    main()
