"""
Advanced ELDM Field Mapping System with Full Hierarchical Graph RAG
Hierarchy: Name (Attribute) -> Entity -> Conceptual Entity (optional)
Creates embeddings for ALL levels to fully leverage knowledge graph power
"""

import os
import json
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, TypedDict, Annotated, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from scipy.stats import zscore
import operator

# OpenAI and LangChain imports
import openai
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# FalkorDB
from falkordb import FalkorDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI API Configuration
BASE_URL = "https://your-openai-gateway.company.com/v1"
API_KEY = "your-api-key-here"
REASONING_MODEL = "o3-mini"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# FalkorDB Configuration
FALKORDB_HOST = "localhost"
FALKORDB_PORT = 6379
GRAPH_NAME = "ELDM_MAPPING"

# File Paths
TRANSACTION_FILE = "transaction_fields.xlsx"
ELDM_FILE = "eldm_attributes.xlsx"
OUTPUT_FILE = "eldm_mapping_results.xlsx"

# Search Configuration
HYBRID_SEARCH_ALPHA = 0.6  # Weight for vector vs full-text

# Confidence Thresholds
HIGH_CONFIDENCE_THRESHOLD = 0.85
MEDIUM_CONFIDENCE_THRESHOLD = 0.70
LOW_CONFIDENCE_THRESHOLD = 0.50

# Processing Configuration
BATCH_SIZE = 50
TOP_K_CANDIDATES = 5


# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class TransactionField:
    """Represents a field from the transaction file"""
    tt_file_name: str
    tt_column_name: str
    standardised_name: str
    is_primary_key: bool
    comment: str
    data_type: str
    creation_sql: str
    transformation_sql: str
    
    def get_context(self) -> str:
        """Get rich context for this field"""
        context = f"""
Field: {self.tt_column_name}
Standardised Name: {self.standardised_name}
File: {self.tt_file_name}
Data Type: {self.data_type}
Primary Key: {self.is_primary_key}
Description: {self.comment}

SQL Context:
Creation: {self.creation_sql}
Transformation: {self.transformation_sql}
"""
        return context.strip()
    
    def get_searchable_text(self) -> str:
        """Get text for embedding and full-text search"""
        parts = [
            self.tt_column_name,
            self.standardised_name,
            self.comment,
            self.data_type,
            f"primary_key_{self.is_primary_key}"
        ]
        return " ".join([p for p in parts if p and str(p).strip()])


@dataclass
class ELDMAttribute:
    """Represents an ELDM Attribute (Name)"""
    name: str
    description: str
    entity: Optional[str]
    conceptual_entity: Optional[str]
    full_path: str
    embedding: Optional[np.ndarray] = None
    
    def get_searchable_text(self) -> str:
        """Get text representation for embedding"""
        parts = [self.name, self.description, self.entity or '', self.conceptual_entity or '']
        return " ".join([p for p in parts if p and str(p).strip()])


@dataclass
class ELDMEntity:
    """Represents an ELDM Entity with all its attributes"""
    name: str
    conceptual_entity: Optional[str]
    attributes: List[str]  # List of attribute names belonging to this entity
    embedding: Optional[np.ndarray] = None
    
    def get_searchable_text(self) -> str:
        """Get text representation for embedding"""
        # Include entity name, its attributes, and conceptual entity for rich context
        parts = [
            self.name,
            self.conceptual_entity or '',
            ' '.join(self.attributes)
        ]
        return " ".join([p for p in parts if p and str(p).strip()])


@dataclass
class ELDMConceptualEntity:
    """Represents an ELDM Conceptual Entity with all its entities"""
    name: str
    entities: List[str]  # List of entity names belonging to this conceptual entity
    embedding: Optional[np.ndarray] = None
    
    def get_searchable_text(self) -> str:
        """Get text representation for embedding"""
        parts = [self.name] + self.entities
        return " ".join([p for p in parts if p and str(p).strip()])


@dataclass
class MappingResult:
    """Represents a mapping result with confidence scores"""
    transaction_field: str
    mapped_node_name: str  # The actual node name mapped to
    mapped_node_type: str  # ATTRIBUTE, ENTITY, or CONCEPTUAL_ENTITY
    eldm_attribute_name: str  # Most granular attribute (if applicable)
    eldm_entity: str
    eldm_conceptual_entity: str
    eldm_description: str
    confidence_score: float
    vector_similarity: float
    fulltext_score: float
    hybrid_score: float
    graph_rag_score: float
    reasoning_confidence: float
    validation_score: float
    reasoning_explanation: str
    validation_notes: str
    statistical_metrics: Dict[str, float]
    confidence_level: str
    retrieval_method: str
    hierarchy_path: str
    timestamp: str


class GraphState(TypedDict):
    """State for the LangGraph workflow"""
    transaction_field: TransactionField
    vector_candidates: List[Dict[str, Any]]
    fulltext_candidates: List[Dict[str, Any]]
    hybrid_candidates: List[Dict[str, Any]]
    graph_rag_candidates: List[Dict[str, Any]]
    final_candidates: List[Dict[str, Any]]
    mapping_proposals: List[Dict[str, Any]]
    validation_results: List[Dict[str, Any]]
    final_mapping: Optional[MappingResult]
    messages: Annotated[List, operator.add]
    iteration_count: int
    memory_context: Dict[str, Any]


# ============================================================================
# EMBEDDING SERVICE
# ============================================================================

class EmbeddingService:
    """Direct OpenAI API embedding service"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            base_url=BASE_URL,
            api_key=API_KEY
        )
        logger.info(f"Initialized EmbeddingService with model: {EMBEDDING_MODEL}")
    
    def create_embedding(self, text: str) -> np.ndarray:
        """Create embedding for a single text"""
        try:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text,
                dimensions=EMBEDDING_DIMENSIONS
            )
            embedding = np.array(response.data[0].embedding)
            return embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    def create_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """Create embeddings for multiple texts"""
        try:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=texts,
                dimensions=EMBEDDING_DIMENSIONS
            )
            embeddings = [np.array(item.embedding) for item in response.data]
            logger.info(f"Created {len(embeddings)} embeddings in batch")
            return embeddings
        except Exception as e:
            logger.error(f"Error creating batch embeddings: {e}")
            raise


# ============================================================================
# FALKORDB FULL HIERARCHICAL GRAPH RAG
# ============================================================================

class ELDMFullHierarchicalGraphRAG:
    """
    FalkorDB-based Graph RAG with embeddings at ALL hierarchy levels
    Hierarchy: Attribute -> Entity -> Conceptual Entity
    All nodes have vector embeddings for maximum knowledge graph power
    """
    
    def __init__(self):
        self.db = FalkorDB(
            host=FALKORDB_HOST,
            port=FALKORDB_PORT
        )
        self.graph = self.db.select_graph(GRAPH_NAME)
        logger.info(f"Connected to FalkorDB graph: {GRAPH_NAME}")
    
    def clear_graph(self):
        """Clear all data from the graph"""
        self.graph.query("MATCH (n) DETACH DELETE n")
        logger.info("Cleared FalkorDB graph")
    
    def create_indexes(self):
        """Create vector and full-text indexes for ALL node types"""
        try:
            # Vector index for Attributes
            attr_vector_query = f"""
            CREATE VECTOR INDEX FOR (a:Attribute) ON (a.embedding) 
            OPTIONS {{
                dimension: {EMBEDDING_DIMENSIONS}, 
                similarityFunction: 'cosine',
                M: 32,
                efConstruction: 200,
                efRuntime: 10
            }}
            """
            self.graph.query(attr_vector_query)
            logger.info("Created vector index for Attributes")
        except Exception as e:
            logger.warning(f"Attribute vector index note: {e}")
            
        try:
            # Vector index for Entities
            entity_vector_query = f"""
            CREATE VECTOR INDEX FOR (e:Entity) ON (e.embedding) 
            OPTIONS {{
                dimension: {EMBEDDING_DIMENSIONS}, 
                similarityFunction: 'cosine',
                M: 32,
                efConstruction: 200,
                efRuntime: 10
            }}
            """
            self.graph.query(entity_vector_query)
            logger.info("Created vector index for Entities")
        except Exception as e:
            logger.warning(f"Entity vector index note: {e}")
            
        try:
            # Vector index for Conceptual Entities
            conceptual_vector_query = f"""
            CREATE VECTOR INDEX FOR (c:ConceptualEntity) ON (c.embedding) 
            OPTIONS {{
                dimension: {EMBEDDING_DIMENSIONS}, 
                similarityFunction: 'cosine',
                M: 32,
                efConstruction: 200,
                efRuntime: 10
            }}
            """
            self.graph.query(conceptual_vector_query)
            logger.info("Created vector index for Conceptual Entities")
        except Exception as e:
            logger.warning(f"Conceptual Entity vector index note: {e}")
            
        # Full-text indexes - create separately with error handling
        try:
            attr_ft_query = "CALL db.idx.fulltext.createNodeIndex('Attribute', 'name', 'description', 'searchable_text')"
            self.graph.query(attr_ft_query)
            logger.info("Created full-text index for Attributes")
        except Exception as e:
            logger.warning(f"Attribute full-text index note: {e}")
            
        try:
            entity_ft_query = "CALL db.idx.fulltext.createNodeIndex('Entity', 'name', 'searchable_text')"
            self.graph.query(entity_ft_query)
            logger.info("Created full-text index for Entities")
        except Exception as e:
            logger.warning(f"Entity full-text index note: {e}")
            
        try:
            conceptual_ft_query = "CALL db.idx.fulltext.createNodeIndex('ConceptualEntity', 'name', 'searchable_text')"
            self.graph.query(conceptual_ft_query)
            logger.info("Created full-text index for Conceptual Entities")
        except Exception as e:
            logger.warning(f"Conceptual Entity full-text index note: {e}")
            
        # Property indexes
        try:
            self.graph.query("CREATE INDEX FOR (a:Attribute) ON (a.name)")
            self.graph.query("CREATE INDEX FOR (e:Entity) ON (e.name)")
            self.graph.query("CREATE INDEX FOR (c:ConceptualEntity) ON (c.name)")
            self.graph.query("CREATE INDEX FOR (t:TransactionField) ON (t.name)")
            self.graph.query("CREATE INDEX FOR (t:TransactionField) ON (t.data_type)")
            logger.info("Created property indexes")
        except Exception as e:
            logger.warning(f"Property index note: {e}")
        
        logger.info("Index creation completed for full hierarchy")
    
    def store_full_hierarchy(self, attributes: List[ELDMAttribute], 
                            entities: List[ELDMEntity], 
                            conceptuals: List[ELDMConceptualEntity]):
        """
        Store complete ELDM hierarchy with embeddings at ALL levels
        """
        # Store Conceptual Entities with embeddings
        for conceptual in conceptuals:
            embedding_list = conceptual.embedding.tolist()
            query = """
            CREATE (c:ConceptualEntity {
                name: $name,
                searchable_text: $searchable_text,
                embedding: vecf32($embedding),
                created_at: $created_at
            })
            """
            self.graph.query(query, {
                'name': conceptual.name,
                'searchable_text': conceptual.get_searchable_text(),
                'embedding': embedding_list,
                'created_at': datetime.now().isoformat()
            })
        
        logger.info(f"Created {len(conceptuals)} Conceptual Entity nodes with embeddings")
        
        # Store Entities with embeddings
        for entity in entities:
            embedding_list = entity.embedding.tolist()
            query = """
            CREATE (e:Entity {
                name: $name,
                conceptual_entity_name: $conceptual_entity_name,
                searchable_text: $searchable_text,
                embedding: vecf32($embedding),
                created_at: $created_at
            })
            """
            self.graph.query(query, {
                'name': entity.name,
                'conceptual_entity_name': entity.conceptual_entity or '',
                'searchable_text': entity.get_searchable_text(),
                'embedding': embedding_list,
                'created_at': datetime.now().isoformat()
            })
        
        logger.info(f"Created {len(entities)} Entity nodes with embeddings")
        
        # Store Attributes with embeddings
        for attr in attributes:
            embedding_list = attr.embedding.tolist()
            query = """
            CREATE (a:Attribute {
                name: $name,
                description: $description,
                entity_name: $entity_name,
                conceptual_entity_name: $conceptual_entity_name,
                full_path: $full_path,
                searchable_text: $searchable_text,
                embedding: vecf32($embedding),
                created_at: $created_at
            })
            """
            self.graph.query(query, {
                'name': attr.name,
                'description': attr.description,
                'entity_name': attr.entity or '',
                'conceptual_entity_name': attr.conceptual_entity or '',
                'full_path': attr.full_path,
                'searchable_text': attr.get_searchable_text(),
                'embedding': embedding_list,
                'created_at': datetime.now().isoformat()
            })
        
        logger.info(f"Created {len(attributes)} Attribute nodes with embeddings")
        
        # Create relationships: Attribute -> Entity
        entity_rel_query = """
        MATCH (a:Attribute)
        WHERE a.entity_name <> ''
        MATCH (e:Entity {name: a.entity_name})
        MERGE (a)-[:BELONGS_TO]->(e)
        MERGE (e)-[:HAS_ATTRIBUTE]->(a)
        """
        self.graph.query(entity_rel_query)
        logger.info("Created Attribute -> Entity relationships")
        
        # Create relationships: Entity -> Conceptual Entity
        conceptual_rel_query = """
        MATCH (e:Entity)
        WHERE e.conceptual_entity_name <> ''
        MATCH (c:ConceptualEntity {name: e.conceptual_entity_name})
        MERGE (e)-[:BELONGS_TO]->(c)
        MERGE (c)-[:HAS_ENTITY]->(e)
        """
        self.graph.query(conceptual_rel_query)
        logger.info("Created Entity -> Conceptual Entity relationships")
        
        logger.info(f"Stored complete hierarchy with embeddings at ALL levels: "
                   f"{len(attributes)} Attributes, {len(entities)} Entities, "
                   f"{len(conceptuals)} Conceptual Entities")
    
    def multi_level_vector_search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Perform vector search across ALL hierarchy levels
        Returns candidates from Attributes, Entities, and Conceptual Entities
        """
        candidates = []
        
        # Search Attributes (most granular - highest priority)
        attr_query = f"""
        CALL db.idx.vector.queryNodes('Attribute', 'embedding', {top_k}, vecf32($embedding))
        YIELD node, score
        OPTIONAL MATCH (node)-[:BELONGS_TO]->(e:Entity)
        OPTIONAL MATCH (e)-[:BELONGS_TO]->(c:ConceptualEntity)
        RETURN 'ATTRIBUTE' as node_type,
               node.name as name,
               node.description as description,
               node.full_path as full_path,
               e.name as entity,
               c.name as conceptual_entity,
               score
        ORDER BY score DESC
        """
        
        result = self.graph.query(attr_query, {'embedding': query_embedding.tolist()})
        for record in result.result_set:
            candidates.append({
                'node_type': record[0],
                'name': record[1],
                'description': record[2] or '',
                'full_path': record[3],
                'entity': record[4] or '',
                'conceptual_entity': record[5] or '',
                'similarity': record[6],
                'method': 'VECTOR',
                'level_boost': 1.0  # Highest priority for attributes
            })
        
        # Search Entities (middle level)
        entity_query = f"""
        CALL db.idx.vector.queryNodes('Entity', 'embedding', {top_k}, vecf32($embedding))
        YIELD node, score
        OPTIONAL MATCH (node)-[:BELONGS_TO]->(c:ConceptualEntity)
        OPTIONAL MATCH (node)-[:HAS_ATTRIBUTE]->(a:Attribute)
        WITH node, score, c, COLLECT(a.name) as attributes
        RETURN 'ENTITY' as node_type,
               node.name as name,
               attributes as description,
               node.name as full_path,
               node.name as entity,
               c.name as conceptual_entity,
               score
        ORDER BY score DESC
        """
        
        result = self.graph.query(entity_query, {'embedding': query_embedding.tolist()})
        for record in result.result_set:
            attr_list = record[2] if record[2] else []
            candidates.append({
                'node_type': record[0],
                'name': record[1],
                'description': f"Entity with attributes: {', '.join(attr_list[:5])}",
                'full_path': record[3],
                'entity': record[4] or '',
                'conceptual_entity': record[5] or '',
                'similarity': record[6],
                'method': 'VECTOR',
                'level_boost': 0.8  # Medium priority for entities
            })
        
        # Search Conceptual Entities (highest level)
        conceptual_query = f"""
        CALL db.idx.vector.queryNodes('ConceptualEntity', 'embedding', {top_k}, vecf32($embedding))
        YIELD node, score
        OPTIONAL MATCH (node)-[:HAS_ENTITY]->(e:Entity)
        WITH node, score, COLLECT(e.name) as entities
        RETURN 'CONCEPTUAL_ENTITY' as node_type,
               node.name as name,
               entities as description,
               node.name as full_path,
               '' as entity,
               node.name as conceptual_entity,
               score
        ORDER BY score DESC
        """
        
        result = self.graph.query(conceptual_query, {'embedding': query_embedding.tolist()})
        for record in result.result_set:
            entity_list = record[2] if record[2] else []
            candidates.append({
                'node_type': record[0],
                'name': record[1],
                'description': f"Conceptual entity with entities: {', '.join(entity_list[:5])}",
                'full_path': record[3],
                'entity': record[4] or '',
                'conceptual_entity': record[5] or '',
                'similarity': record[6],
                'method': 'VECTOR',
                'level_boost': 0.6  # Lower priority for conceptual entities
            })
        
        # Apply level boost and re-sort
        for candidate in candidates:
            candidate['similarity'] = candidate['similarity'] * candidate['level_boost']
        
        candidates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Multi-level vector search returned {len(candidates)} candidates from all hierarchy levels")
        return candidates[:top_k * 2]  # Return more for further filtering
    
    def multi_level_fulltext_search(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform full-text search across ALL hierarchy levels"""
        candidates = []
        
        # Search Attributes
        try:
            attr_query = f"""
            CALL db.idx.fulltext.queryNodes('Attribute', $query_text)
            YIELD node, score
            OPTIONAL MATCH (node)-[:BELONGS_TO]->(e:Entity)
            OPTIONAL MATCH (e)-[:BELONGS_TO]->(c:ConceptualEntity)
            RETURN 'ATTRIBUTE' as node_type,
                   node.name as name,
                   node.description as description,
                   node.full_path as full_path,
                   e.name as entity,
                   c.name as conceptual_entity,
                   score
            ORDER BY score DESC
            LIMIT {top_k}
            """
            
            result = self.graph.query(attr_query, {'query_text': query_text})
            for record in result.result_set:
                candidates.append({
                    'node_type': record[0],
                    'name': record[1],
                    'description': record[2] or '',
                    'full_path': record[3],
                    'entity': record[4] or '',
                    'conceptual_entity': record[5] or '',
                    'similarity': record[6],
                    'method': 'FULLTEXT',
                    'level_boost': 1.0
                })
        except Exception as e:
            logger.warning(f"Attribute full-text search warning: {e}")
        
        # Search Entities
        try:
            entity_query = f"""
            CALL db.idx.fulltext.queryNodes('Entity', $query_text)
            YIELD node, score
            OPTIONAL MATCH (node)-[:BELONGS_TO]->(c:ConceptualEntity)
            RETURN 'ENTITY' as node_type,
                   node.name as name,
                   node.searchable_text as description,
                   node.name as full_path,
                   node.name as entity,
                   c.name as conceptual_entity,
                   score
            ORDER BY score DESC
            LIMIT {top_k}
            """
            
            result = self.graph.query(entity_query, {'query_text': query_text})
            for record in result.result_set:
                candidates.append({
                    'node_type': record[0],
                    'name': record[1],
                    'description': record[2] or '',
                    'full_path': record[3],
                    'entity': record[4] or '',
                    'conceptual_entity': record[5] or '',
                    'similarity': record[6],
                    'method': 'FULLTEXT',
                    'level_boost': 0.8
                })
        except Exception as e:
            logger.warning(f"Entity full-text search warning: {e}")
        
        # Search Conceptual Entities
        try:
            conceptual_query = f"""
            CALL db.idx.fulltext.queryNodes('ConceptualEntity', $query_text)
            YIELD node, score
            RETURN 'CONCEPTUAL_ENTITY' as node_type,
                   node.name as name,
                   node.searchable_text as description,
                   node.name as full_path,
                   '' as entity,
                   node.name as conceptual_entity,
                   score
            ORDER BY score DESC
            LIMIT {top_k}
            """
            
            result = self.graph.query(conceptual_query, {'query_text': query_text})
            for record in result.result_set:
                candidates.append({
                    'node_type': record[0],
                    'name': record[1],
                    'description': record[2] or '',
                    'full_path': record[3],
                    'entity': record[4] or '',
                    'conceptual_entity': record[5] or '',
                    'similarity': record[6],
                    'method': 'FULLTEXT',
                    'level_boost': 0.6
                })
        except Exception as e:
            logger.warning(f"Conceptual Entity full-text search warning: {e}")
        
        # If no full-text results, return empty list (vector search will still work)
        if not candidates:
            logger.info("No full-text search results found, relying on vector search")
            return []
        
        # Apply level boost
        for candidate in candidates:
            candidate['similarity'] = candidate['similarity'] * candidate['level_boost']
        
        candidates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Multi-level full-text search returned {len(candidates)} candidates")
        return candidates[:top_k * 2]
    
    def hybrid_search(self, query_embedding: np.ndarray, query_text: str, 
                     top_k: int = 5, alpha: float = HYBRID_SEARCH_ALPHA) -> List[Dict[str, Any]]:
        """Hybrid search across all hierarchy levels"""
        vector_results = self.multi_level_vector_search(query_embedding, top_k * 2)
        fulltext_results = self.multi_level_fulltext_search(query_text, top_k * 2)
        
        # If full-text search failed or returned nothing, fall back to vector search
        if not fulltext_results:
            logger.info("Full-text search empty, using vector search only")
            return vector_results[:top_k]
        
        # Combine and normalize
        combined_scores = {}
        metadata = {}
        
        vector_scores = {r['name']: r['similarity'] for r in vector_results}
        max_vector = max(vector_scores.values()) if vector_scores else 1.0
        
        fulltext_scores = {r['name']: r['similarity'] for r in fulltext_results}
        max_fulltext = max(fulltext_scores.values()) if fulltext_scores else 1.0
        
        for r in vector_results + fulltext_results:
            if r['name'] not in metadata:
                metadata[r['name']] = r
        
        for name in metadata.keys():
            v_score = vector_scores.get(name, 0.0) / max_vector if max_vector > 0 else 0.0
            f_score = fulltext_scores.get(name, 0.0) / max_fulltext if max_fulltext > 0 else 0.0
            combined_scores[name] = alpha * v_score + (1 - alpha) * f_score
        
        sorted_names = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        candidates = []
        for name, hybrid_score in sorted_names:
            record = metadata[name]
            candidates.append({
                'node_type': record['node_type'],
                'name': record['name'],
                'description': record['description'],
                'full_path': record['full_path'],
                'entity': record['entity'],
                'conceptual_entity': record['conceptual_entity'],
                'similarity': hybrid_score,
                'vector_score': vector_scores.get(name, 0.0),
                'fulltext_score': fulltext_scores.get(name, 0.0),
                'method': 'HYBRID'
            })
        
        logger.info(f"Hybrid search returned {len(candidates)} candidates from all levels")
        return candidates
    
    def graph_rag_search(self, query_embedding: np.ndarray, query_text: str, 
                        transaction_field: TransactionField, top_k: int = 5) -> List[Dict[str, Any]]:
        """Graph RAG search with multi-level historical patterns"""
        candidates = self.hybrid_search(query_embedding, query_text, top_k * 2)
        
        # Check historical mappings at all levels
        similar_mappings_query = """
        MATCH (t:TransactionField)-[r:MAPPED_TO]->(n)
        WHERE t.data_type = $data_type 
           OR t.file_name = $file_name
           OR t.standardised_name CONTAINS $partial_name
        WITH n, labels(n) as node_labels, COUNT(r) as mapping_frequency, AVG(r.confidence_score) as avg_confidence
        OPTIONAL MATCH (n)-[:BELONGS_TO*]->(parent)
        RETURN n.name as name,
               node_labels[0] as node_type,
               mapping_frequency,
               avg_confidence,
               COLLECT(DISTINCT parent.name) as ancestors
        ORDER BY mapping_frequency DESC, avg_confidence DESC
        LIMIT $top_k
        """
        
        params = {
            'data_type': transaction_field.data_type,
            'file_name': transaction_field.tt_file_name,
            'partial_name': transaction_field.standardised_name[:10] if transaction_field.standardised_name else "",
            'top_k': top_k * 2
        }
        
        result = self.graph.query(similar_mappings_query, params)
        
        historical_boost = {}
        for record in result.result_set:
            name = record[0]
            mapping_freq = record[2]
            avg_conf = record[3]
            historical_boost[name] = {
                'boost': mapping_freq * avg_conf * 0.2,
                'frequency': mapping_freq,
                'avg_confidence': avg_conf
            }
        
        enhanced_candidates = []
        for candidate in candidates:
            name = candidate['name']
            base_score = candidate['similarity']
            
            hist_boost = 0.0
            mapping_freq = 0
            if name in historical_boost:
                hist_boost = historical_boost[name]['boost']
                mapping_freq = historical_boost[name]['frequency']
            
            enhanced_score = min(base_score + hist_boost, 1.0)
            
            candidate['similarity'] = enhanced_score
            candidate['graph_rag_score'] = hist_boost
            candidate['mapping_frequency'] = mapping_freq
            candidate['method'] = 'GRAPH_RAG'
            
            enhanced_candidates.append(candidate)
        
        enhanced_candidates.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"Graph RAG search returned {len(enhanced_candidates[:top_k])} candidates with multi-level context")
        return enhanced_candidates[:top_k]
    
    def store_mapping_result(self, result: MappingResult, transaction_field: TransactionField):
        """Store mapping result - can be to any hierarchy level"""
        query = """
        MATCH (n {name: $node_name})
        WHERE n:Attribute OR n:Entity OR n:ConceptualEntity
        MERGE (t:TransactionField {
            name: $transaction_field,
            file_name: $file_name,
            data_type: $data_type,
            standardised_name: $standardised_name
        })
        MERGE (t)-[r:MAPPED_TO]->(n)
        SET r.confidence_score = $confidence_score,
            r.vector_similarity = $vector_similarity,
            r.hybrid_score = $hybrid_score,
            r.mapped_node_type = $mapped_node_type,
            r.reasoning_confidence = $reasoning_confidence,
            r.validation_score = $validation_score,
            r.confidence_level = $confidence_level,
            r.retrieval_method = $retrieval_method,
            r.reasoning_explanation = $reasoning_explanation,
            r.timestamp = $timestamp
        """
        
        params = {
            'node_name': result.mapped_node_name,
            'transaction_field': result.transaction_field,
            'file_name': transaction_field.tt_file_name,
            'data_type': transaction_field.data_type,
            'standardised_name': transaction_field.standardised_name,
            'confidence_score': result.confidence_score,
            'vector_similarity': result.vector_similarity,
            'hybrid_score': result.hybrid_score,
            'mapped_node_type': result.mapped_node_type,
            'reasoning_confidence': result.reasoning_confidence,
            'validation_score': result.validation_score,
            'confidence_level': result.confidence_level,
            'retrieval_method': result.retrieval_method,
            'reasoning_explanation': result.reasoning_explanation,
            'timestamp': result.timestamp
        }
        self.graph.query(query, params)
        logger.info(f"Stored mapping to {result.mapped_node_type} level")


# ============================================================================
# AGENT PROMPTS (UPDATED FOR MULTI-LEVEL)
# ============================================================================

MAPPING_AGENT_SYSTEM_PROMPT = """You are an expert data architect and semantic mapping specialist.

ELDM HIERARCHY:
Name (Attribute) - MOST GRANULAR (preferred)
  ↓ belongs to
Entity - Parent grouping
  ↓ belongs to (optional)
Conceptual Entity - High-level grouping

MAPPING STRATEGY:
1. PREFER Attribute (Name) level - most granular and specific
2. Use Entity level if no suitable Attribute exists but entity matches well
3. Use Conceptual Entity only as last resort for very broad matches

You receive candidates from ALL hierarchy levels. Each candidate shows:
- Node Type (ATTRIBUTE, ENTITY, or CONCEPTUAL_ENTITY)
- Name and description
- Hierarchy context

Select the best match considering:
- Semantic alignment (most important)
- Granularity (prefer more specific)
- Historical patterns (Graph RAG provides this)

Response must be JSON (use exact field names shown):
{{
    "selected_node_name": "exact name",
    "selected_node_type": "ATTRIBUTE or ENTITY or CONCEPTUAL_ENTITY",
    "confidence_score": 0.0-1.0,
    "reasoning": "detailed reasoning including why this level",
    "key_factors": ["factor1", "factor2"],
    "concerns": ["concerns"],
    "granularity_justification": "why this level is appropriate"
}}
"""

VALIDATION_AGENT_SYSTEM_PROMPT = """You are a senior data governance specialist.

Validate that the proposed mapping:
1. Is semantically correct
2. Is at the appropriate hierarchy level (prefer granular when possible)
3. Has supporting evidence from context and history

Response must be JSON (use exact field names shown):
{{
    "validation_decision": "APPROVE or NEEDS_REVIEW or REJECT",
    "validation_score": 0.0-1.0,
    "validation_reasoning": "detailed analysis",
    "identified_issues": ["issue1"],
    "strengths": ["strength1"],
    "level_appropriateness": "assessment of hierarchy level choice",
    "recommendations": "improvements"
}}
"""

SUPERVISOR_AGENT_SYSTEM_PROMPT = """You are the chief data architect.

Make final decisions considering:
1. Semantic correctness
2. Appropriate hierarchy level
3. Overall confidence from all agents
4. Historical patterns

Confidence levels:
- HIGH: ≥ 0.85
- MEDIUM: 0.70-0.84
- LOW: 0.50-0.69
- REJECTED: < 0.50

Response must be JSON (use exact field names shown):
{{
    "final_decision": "APPROVE or REJECT",
    "confidence_level": "HIGH or MEDIUM or LOW or REJECTED",
    "overall_confidence_score": 0.0-1.0,
    "reasoning": "comprehensive reasoning",
    "action_required": "action"
}}
"""


# ============================================================================
# MULTI-AGENT SYSTEM (UPDATED)
# ============================================================================

class MappingAgent:
    """Agent for proposing mappings at any hierarchy level"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model=REASONING_MODEL, base_url=BASE_URL, api_key=API_KEY)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", MAPPING_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
    
    def propose_mapping(self, state: GraphState) -> GraphState:
        field = state['transaction_field']
        candidates = state.get('graph_rag_candidates') or state.get('hybrid_candidates') or state.get('vector_candidates', [])
        
        input_text = f"""
Transaction Field:
{field.get_context()}

Candidates (from all hierarchy levels):
"""
        for i, candidate in enumerate(candidates, 1):
            input_text += f"""
{i}. Type: {candidate.get('node_type', 'UNKNOWN')}
   Name: {candidate['name']}
   Description: {candidate.get('description', 'N/A')}
   Hierarchy: {candidate.get('full_path', 'N/A')}
   Similarity: {candidate['similarity']:.4f}
   Method: {candidate.get('method', 'UNKNOWN')}
"""
            if candidate.get('mapping_frequency', 0) > 0:
                input_text += f"   Historical: {candidate['mapping_frequency']} similar mappings\n"
        
        input_text += "\nSelect the best match (prefer ATTRIBUTE level when suitable).\n"
        
        try:
            messages = state.get('messages', [])
            response = (self.prompt | self.llm).invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            proposal = json.loads(content)
            state['mapping_proposals'].append(proposal)
            state['messages'].extend([HumanMessage(content=input_text), AIMessage(content=response.content)])
            
            logger.info(f"Proposed: {proposal['selected_node_name']} ({proposal['selected_node_type']}) - confidence: {proposal['confidence_score']:.3f}")
            
        except Exception as e:
            logger.error(f"Error in propose_mapping: {e}")
            proposal = {
                'selected_node_name': candidates[0]['name'],
                'selected_node_type': candidates[0].get('node_type', 'ATTRIBUTE'),
                'confidence_score': candidates[0]['similarity'],
                'reasoning': f"Fallback: {str(e)}",
                'key_factors': ['semantic_similarity'],
                'concerns': ['Error in reasoning'],
                'granularity_justification': 'Top candidate'
            }
            state['mapping_proposals'].append(proposal)
        
        return state


class ValidationAgent:
    """Agent for validating mappings"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model=REASONING_MODEL, base_url=BASE_URL, api_key=API_KEY)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", VALIDATION_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
    
    def validate_mapping(self, state: GraphState) -> GraphState:
        field = state['transaction_field']
        proposals = state['mapping_proposals']
        
        if not proposals:
            return state
        
        latest_proposal = proposals[-1]
        all_candidates = (state.get('graph_rag_candidates', []) + state.get('hybrid_candidates', []) + state.get('vector_candidates', []))
        selected_candidate = next((c for c in all_candidates if c['name'] == latest_proposal['selected_node_name']), all_candidates[0] if all_candidates else None)
        
        if not selected_candidate:
            return state
        
        input_text = f"""
Transaction Field:
{field.get_context()}

Proposed Mapping:
- Node: {latest_proposal['selected_node_name']}
- Type: {latest_proposal['selected_node_type']}
- Description: {selected_candidate.get('description', 'N/A')}
- Hierarchy: {selected_candidate.get('full_path', 'N/A')}

Agent Reasoning:
{latest_proposal['reasoning']}

Confidence: {latest_proposal['confidence_score']:.3f}

Validate this mapping.
"""
        
        try:
            messages = state.get('messages', [])
            response = (self.prompt | self.llm).invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            validation = json.loads(content)
            state['validation_results'].append(validation)
            state['messages'].extend([HumanMessage(content=input_text), AIMessage(content=response.content)])
            
            logger.info(f"Validation: {validation['validation_decision']} (score: {validation['validation_score']:.3f})")
            
        except Exception as e:
            logger.error(f"Error in validate_mapping: {e}")
            validation = {'validation_decision': 'NEEDS_REVIEW', 'validation_score': 0.6, 'validation_reasoning': f"Error: {str(e)}", 'identified_issues': ['Validation error'], 'strengths': [], 'level_appropriateness': 'Unable to assess', 'recommendations': 'Manual review'}
            state['validation_results'].append(validation)
        
        return state


class SupervisorAgent:
    """Agent for final decisions"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model=REASONING_MODEL, base_url=BASE_URL, api_key=API_KEY)
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SUPERVISOR_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
    
    def make_final_decision(self, state: GraphState) -> GraphState:
        field = state['transaction_field']
        proposals = state['mapping_proposals']
        validations = state['validation_results']
        
        if not proposals or not validations:
            return state
        
        latest_proposal = proposals[-1]
        latest_validation = validations[-1]
        all_candidates = (state.get('graph_rag_candidates', []) + state.get('hybrid_candidates', []) + state.get('vector_candidates', []))
        selected_candidate = next((c for c in all_candidates if c['name'] == latest_proposal['selected_node_name']), all_candidates[0] if all_candidates else None)
        
        if not selected_candidate:
            return state
        
        all_similarities = [c['similarity'] for c in all_candidates]
        z_scores = zscore(all_similarities) if len(all_similarities) > 1 else [0.0]
        top_similarity = selected_candidate['similarity']
        top_z_score = float(z_scores[0]) if all_similarities and all_similarities[0] == top_similarity else 0.0
        gap_to_second = top_similarity - all_similarities[1] if len(all_similarities) > 1 else top_similarity
        
        statistical_metrics = {'semantic_similarity': top_similarity, 'z_score': top_z_score, 'gap_to_second_best': gap_to_second}
        
        input_text = f"""
Transaction Field: {field.tt_column_name}

Proposal:
- Node: {latest_proposal['selected_node_name']}
- Type: {latest_proposal['selected_node_type']}
- Confidence: {latest_proposal['confidence_score']:.3f}

Validation:
- Decision: {latest_validation['validation_decision']}
- Score: {latest_validation['validation_score']:.3f}

Metrics:
- Similarity: {statistical_metrics['semantic_similarity']:.4f}
- Z-Score: {statistical_metrics['z_score']:.4f}

Make final decision.
"""
        
        try:
            messages = state.get('messages', [])
            response = (self.prompt | self.llm).invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            decision = json.loads(content)
            
            if decision['final_decision'] == 'APPROVE':
                # Determine hierarchy details based on node type
                node_type = latest_proposal['selected_node_type']
                if node_type == 'ATTRIBUTE':
                    attr_name = selected_candidate['name']
                    entity_name = selected_candidate.get('entity', '')
                    conceptual_name = selected_candidate.get('conceptual_entity', '')
                elif node_type == 'ENTITY':
                    attr_name = ''
                    entity_name = selected_candidate['name']
                    conceptual_name = selected_candidate.get('conceptual_entity', '')
                else:  # CONCEPTUAL_ENTITY
                    attr_name = ''
                    entity_name = ''
                    conceptual_name = selected_candidate['name']
                
                result = MappingResult(
                    transaction_field=field.tt_column_name,
                    mapped_node_name=latest_proposal['selected_node_name'],
                    mapped_node_type=node_type,
                    eldm_attribute_name=attr_name,
                    eldm_entity=entity_name,
                    eldm_conceptual_entity=conceptual_name,
                    eldm_description=selected_candidate.get('description', ''),
                    confidence_score=decision['overall_confidence_score'],
                    vector_similarity=selected_candidate.get('vector_score', selected_candidate['similarity']),
                    fulltext_score=selected_candidate.get('fulltext_score', 0.0),
                    hybrid_score=selected_candidate.get('similarity', 0.0),
                    graph_rag_score=selected_candidate.get('graph_rag_score', 0.0),
                    reasoning_confidence=latest_proposal['confidence_score'],
                    validation_score=latest_validation['validation_score'],
                    reasoning_explanation=latest_proposal['reasoning'],
                    validation_notes=latest_validation['validation_reasoning'],
                    statistical_metrics=statistical_metrics,
                    confidence_level=decision['confidence_level'],
                    retrieval_method=selected_candidate.get('method', 'UNKNOWN'),
                    hierarchy_path=selected_candidate.get('full_path', selected_candidate['name']),
                    timestamp=datetime.now().isoformat()
                )
                state['final_mapping'] = result
                logger.info(f"APPROVED: {result.mapped_node_name} ({result.mapped_node_type})")
            else:
                logger.info(f"REJECTED: {latest_proposal['selected_node_name']}")
            
            state['messages'].extend([HumanMessage(content=input_text), AIMessage(content=response.content)])
            
        except Exception as e:
            logger.error(f"Error in final_decision: {e}")
        
        return state


# ============================================================================
# WORKFLOW & MAIN SYSTEM
# ============================================================================

def create_mapping_workflow() -> StateGraph:
    workflow = StateGraph(GraphState)
    workflow.add_node("propose_mapping", MappingAgent().propose_mapping)
    workflow.add_node("validate_mapping", ValidationAgent().validate_mapping)
    workflow.add_node("final_decision", SupervisorAgent().make_final_decision)
    workflow.set_entry_point("propose_mapping")
    workflow.add_edge("propose_mapping", "validate_mapping")
    workflow.add_edge("validate_mapping", "final_decision")
    workflow.add_edge("final_decision", END)
    return workflow.compile(checkpointer=MemorySaver())


class ELDMMappingSystem:
    """Complete ELDM mapping system with embeddings at ALL hierarchy levels"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.graph_rag = ELDMFullHierarchicalGraphRAG()
        self.workflow = create_mapping_workflow()
        logger.info("Initialized Full Hierarchical ELDM Mapping System")
    
    def load_transaction_data(self) -> List[TransactionField]:
        df = pd.read_excel(TRANSACTION_FILE)
        fields = [TransactionField(
            tt_file_name=str(row['TT File name']),
            tt_column_name=str(row['TT COLUMN/FIELD Name']),
            standardised_name=str(row.get('Standardised Name', '')),
            is_primary_key=bool(row.get('PK', False)),
            comment=str(row.get('Comment', '')),
            data_type=str(row.get('DataType', '')),
            creation_sql=str(row.get('Creation', '')),
            transformation_sql=str(row.get('Transformation', ''))
        ) for _, row in df.iterrows()]
        logger.info(f"Loaded {len(fields)} transaction fields")
        return fields
    
    def load_eldm_full_hierarchy(self) -> Tuple[List[ELDMAttribute], List[ELDMEntity], List[ELDMConceptualEntity]]:
        """Load and create embeddings for ALL hierarchy levels"""
        df = pd.read_excel(ELDM_FILE)
        
        # Collect data
        attributes = []
        entities_data = {}
        conceptuals_data = {}
        
        for _, row in df.iterrows():
            name = str(row['Name'])
            description = str(row.get('Description', ''))
            entity = str(row.get('Entity', '')) if pd.notna(row.get('Entity')) else None
            conceptual = str(row.get('Conceptual Entity', '')) if pd.notna(row.get('Conceptual Entity')) else None
            
            path_parts = [p for p in [conceptual, entity, name] if p]
            full_path = ' > '.join(path_parts)
            
            attr = ELDMAttribute(name=name, description=description, entity=entity, conceptual_entity=conceptual, full_path=full_path)
            attributes.append(attr)
            
            if entity:
                if entity not in entities_data:
                    entities_data[entity] = {'conceptual': conceptual, 'attributes': []}
                entities_data[entity]['attributes'].append(name)
            
            if conceptual:
                if conceptual not in conceptuals_data:
                    conceptuals_data[conceptual] = {'entities': set()}
                if entity:
                    conceptuals_data[conceptual]['entities'].add(entity)
        
        # Create entity and conceptual objects
        entities = [ELDMEntity(name=name, conceptual_entity=data['conceptual'], attributes=data['attributes']) 
                   for name, data in entities_data.items()]
        conceptuals = [ELDMConceptualEntity(name=name, entities=list(data['entities'])) 
                      for name, data in conceptuals_data.items()]
        
        logger.info(f"Loaded {len(attributes)} Attributes, {len(entities)} Entities, {len(conceptuals)} Conceptual Entities")
        
        # Create embeddings for ALL levels
        logger.info("Creating embeddings for Attributes...")
        attr_texts = [attr.get_searchable_text() for attr in attributes]
        for i in range(0, len(attr_texts), BATCH_SIZE):
            batch_embeddings = self.embedding_service.create_embeddings_batch(attr_texts[i:i+BATCH_SIZE])
            for j, emb in enumerate(batch_embeddings):
                attributes[i+j].embedding = emb
        
        logger.info("Creating embeddings for Entities...")
        entity_texts = [ent.get_searchable_text() for ent in entities]
        for i in range(0, len(entity_texts), BATCH_SIZE):
            batch_embeddings = self.embedding_service.create_embeddings_batch(entity_texts[i:i+BATCH_SIZE])
            for j, emb in enumerate(batch_embeddings):
                entities[i+j].embedding = emb
        
        logger.info("Creating embeddings for Conceptual Entities...")
        conceptual_texts = [con.get_searchable_text() for con in conceptuals]
        for i in range(0, len(conceptual_texts), BATCH_SIZE):
            batch_embeddings = self.embedding_service.create_embeddings_batch(conceptual_texts[i:i+BATCH_SIZE])
            for j, emb in enumerate(batch_embeddings):
                conceptuals[i+j].embedding = emb
        
        logger.info("✅ Created embeddings for ALL hierarchy levels")
        return attributes, entities, conceptuals
    
    def initialize_knowledge_graph(self):
        logger.info("Initializing full hierarchical knowledge graph...")
        self.graph_rag.clear_graph()
        attributes, entities, conceptuals = self.load_eldm_full_hierarchy()
        self.graph_rag.store_full_hierarchy(attributes, entities, conceptuals)
        self.graph_rag.create_indexes()
        logger.info("✅ Full hierarchical knowledge graph initialized with embeddings at ALL levels")
    
    def map_single_field(self, field: TransactionField) -> Optional[MappingResult]:
        field_text = field.get_searchable_text()
        field_embedding = self.embedding_service.create_embedding(field_text)
        
        vector_candidates = self.graph_rag.multi_level_vector_search(field_embedding, TOP_K_CANDIDATES)
        fulltext_candidates = self.graph_rag.multi_level_fulltext_search(field_text, TOP_K_CANDIDATES)
        hybrid_candidates = self.graph_rag.hybrid_search(field_embedding, field_text, TOP_K_CANDIDATES, HYBRID_SEARCH_ALPHA)
        graph_rag_candidates = self.graph_rag.graph_rag_search(field_embedding, field_text, field, TOP_K_CANDIDATES)
        
        initial_state = {
            'transaction_field': field,
            'vector_candidates': vector_candidates,
            'fulltext_candidates': fulltext_candidates,
            'hybrid_candidates': hybrid_candidates,
            'graph_rag_candidates': graph_rag_candidates,
            'final_candidates': graph_rag_candidates,
            'mapping_proposals': [],
            'validation_results': [],
            'final_mapping': None,
            'messages': [],
            'iteration_count': 0,
            'memory_context': {'sql_context': {'creation': field.creation_sql, 'transformation': field.transformation_sql}, 'lineage': {'file': field.tt_file_name, 'standardised_name': field.standardised_name}}
        }
        
        try:
            result = self.workflow.invoke(initial_state, {"configurable": {"thread_id": field.tt_column_name}})
            final_mapping = result.get('final_mapping')
            
            if final_mapping:
                self.graph_rag.store_mapping_result(final_mapping, field)
                logger.info(f"Mapped: {field.tt_column_name} -> {final_mapping.mapped_node_name} ({final_mapping.mapped_node_type})")
                return final_mapping
            
        except Exception as e:
            logger.error(f"Error mapping {field.tt_column_name}: {e}")
        
        return None
    
    def map_all_fields(self) -> List[MappingResult]:
        fields = self.load_transaction_data()
        results = []
        
        for i, field in enumerate(fields, 1):
            logger.info(f"Processing {i}/{len(fields)}: {field.tt_column_name}")
            result = self.map_single_field(field)
            if result:
                results.append(result)
            if i % 10 == 0:
                logger.info(f"Progress: {i}/{len(fields)} processed, {len(results)} mapped")
        
        logger.info(f"Complete: {len(results)}/{len(fields)} successful mappings")
        return results
    
    def export_results(self, results: List[MappingResult]):
        data = [{
            'Transaction Field': r.transaction_field,
            'Mapped To': r.mapped_node_name,
            'Mapping Level': r.mapped_node_type,
            'ELDM Attribute': r.eldm_attribute_name,
            'ELDM Entity': r.eldm_entity,
            'ELDM Conceptual Entity': r.eldm_conceptual_entity,
            'Description': r.eldm_description,
            'Hierarchy Path': r.hierarchy_path,
            'Confidence Score': r.confidence_score,
            'Confidence Level': r.confidence_level,
            'Retrieval Method': r.retrieval_method,
            'Vector Similarity': r.vector_similarity,
            'Graph RAG Score': r.graph_rag_score,
            'Reasoning': r.reasoning_explanation,
            'Validation': r.validation_notes,
            'Timestamp': r.timestamp
        } for r in results]
        
        df = pd.DataFrame(data)
        
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Mapping Results', index=False)
            
            summary = {
                'Total Mappings': len(results),
                'ATTRIBUTE Level': len([r for r in results if r.mapped_node_type == 'ATTRIBUTE']),
                'ENTITY Level': len([r for r in results if r.mapped_node_type == 'ENTITY']),
                'CONCEPTUAL_ENTITY Level': len([r for r in results if r.mapped_node_type == 'CONCEPTUAL_ENTITY']),
                'HIGH Confidence': len([r for r in results if r.confidence_level == 'HIGH']),
                'MEDIUM Confidence': len([r for r in results if r.confidence_level == 'MEDIUM']),
                'LOW Confidence': len([r for r in results if r.confidence_level == 'LOW']),
                'Graph RAG Method': len([r for r in results if r.retrieval_method == 'GRAPH_RAG']),
                'Avg Confidence': np.mean([r.confidence_score for r in results])
            }
            pd.DataFrame([summary]).to_excel(writer, sheet_name='Summary', index=False)
        
        logger.info(f"Results exported to {OUTPUT_FILE}")
    
    def run(self):
        logger.info("=" * 80)
        logger.info("FULL HIERARCHICAL ELDM Mapping System - Starting")
        logger.info("Embeddings at ALL levels: Attribute, Entity, Conceptual Entity")
        logger.info("=" * 80)
        
        try:
            self.initialize_knowledge_graph()
            results = self.map_all_fields()
            if results:
                self.export_results(results)
            logger.info("=" * 80)
            logger.info("✅ Completed Successfully")
            logger.info("=" * 80)
            return results
        except Exception as e:
            logger.error(f"Error: {e}")
            raise


if __name__ == "__main__":
    system = ELDMMappingSystem()
    results = system.run()
    
    if results:
        print(f"\n✅ Mapping completed: {len(results)} fields mapped")
        print(f"📊 Results: {OUTPUT_FILE}")
        
        by_level = {}
        for r in results:
            by_level[r.mapped_node_type] = by_level.get(r.mapped_node_type, 0) + 1
        
        print("\n🎯 Mapping Level Distribution:")
        for level, count in by_level.items():
            print(f"   {level}: {count} ({count/len(results)*100:.1f}%)")
