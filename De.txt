"""
Result preparation node for the Agentic RAG system.

This module implements the result preparation node for the Agentic RAG system,
which finalizes the results and formats them for return to the client.
"""

import logging
from typing import Dict, Any, List, Tuple

logger = logging.getLogger(__name__)

def result_preparation(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Prepare the final results to be returned to the calling function.
    
    Args:
        state: Current graph state
        
    Returns:
        Updated state with finalized results
    """
    logger.info("Preparing final results")
    
    try:
        # Extract top candidates
        top_candidates = state.get("top_candidates", [])
        
        # Determine if modeling is required based on:
        # 1. Having no candidates
        # 2. Having candidates but all with low scores
        has_candidates = len(top_candidates) > 0
        has_good_match = any((c.get("final_score") or 0) >= 0.7 for c in top_candidates)
        
        modeling_required = not has_candidates or not has_good_match
        
        # Format results for return
        matching_terms = []
        confidence_scores = []
        
        for candidate in top_candidates:
            # Format the term in the expected format
            term = {
                "id": candidate.get("id", ""),
                "name": candidate.get("name", ""),
                "description": candidate.get("description", ""),
                "metadata": candidate.get("metadata", {}).copy(),
                "similarity": candidate.get("final_score", 0.0)
            }
            
            # Add reasoning to metadata
            if candidate.get("reasoning"):
                term["metadata"]["reasoning"] = candidate.get("reasoning", "")
            
            # Add vector and semantic scores
            term["metadata"]["vector_score"] = candidate.get("vector_score", 0.0)
            term["metadata"]["semantic_score"] = candidate.get("semantic_score", 0.0)
            term["metadata"]["keyword_score"] = candidate.get("keyword_score", 0.0)
            
            matching_terms.append(term)
            confidence_scores.append(candidate.get("final_score", 0.0))
        
        # Prepare message
        if modeling_required:
            if not has_candidates:
                message = "No matching terms found. Consider modeling a new term."
            else:
                message = "Found potential matches, but confidence is low. Consider modeling a new term."
        else:
            message = f"Found {len(matching_terms)} relevant business terms with high confidence."
        
        # Prepare return state
        state["result"] = {
            "matching_terms": matching_terms,
            "confidence_scores": confidence_scores,
            "modeling_required": modeling_required,
            "message": message
        }
        
        logger.info(f"Result preparation complete: {message}")
        return state
        
    except Exception as e:
        logger.error(f"Error in result preparation: {e}", exc_info=True)
        state["error"] = f"Result preparation failed: {str(e)}"
        
        # Create minimal result to avoid downstream errors
        state["result"] = {
            "matching_terms": [],
            "confidence_scores": [],
            "modeling_required": True,
            "message": f"Error preparing results: {str(e)}"
        }
        
        return state

def format_term_for_output(term: Dict[str, Any]) -> Dict[str, Any]:
    """
    Format a term for output to the client.
    
    Args:
        term: Internal term representation
        
    Returns:
        Client-ready term format
    """
    # Create a clean copy
    output_term = {
        "id": term.get("id", ""),
        "name": term.get("name", ""),
        "description": term.get("description", ""),
        "similarity": term.get("final_score", 0.0),
        "metadata": {}
    }
    
    # Copy metadata
    if term.get("metadata"):
        output_term["metadata"] = term.get("metadata", {}).copy()
    
    # Add reasoning to metadata
    if term.get("reasoning"):
        output_term["metadata"]["reasoning"] = term.get("reasoning", "")
    
    # Add component scores to metadata
    output_term["metadata"]["vector_score"] = term.get("vector_score", 0.0)
    output_term["metadata"]["semantic_score"] = term.get("semantic_score", 0.0)
    output_term["metadata"]["keyword_score"] = term.get("keyword_score", 0.0)
    
    return output_term
