#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System (Rules-as-Code)
Multi-Agent Architecture with LangGraph, ReAct Agents, and LangMem

Features:
- True ReAct agents with LangGraph orchestration
- Multi-agent chain of experts architecture
- LangMem for long-term memory across sessions
- o3-mini-2025-01-31 with reasoning effort control
- Web search and document processing tools
- SPARQL query interface and semantic outputs
- Comprehensive ontology generation with validation

Author: Enhanced Rules-as-Code Implementation
Date: July 2025
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import ssl
import threading

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig

# LangMem imports
from langmem import (
    create_memory_manager,
    create_manage_memory_tool,
    create_search_memory_tool
)

# LangChain imports for tools and models
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.tools import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logging.warning("Flask not available - web interface will be disabled")

# Token counting
import tiktoken

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the Rules-as-Code system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Tavily Configuration for web search
    TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_as_code_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    
    # o3-mini Model parameters
    REASONING_EFFORT = "high"  # low, medium, high for complex legal analysis
    MAX_COMPLETION_TOKENS = 8000
    
    # Processing parameters
    BATCH_SIZE = 5
    MAX_CONCURRENT = 3
    CHUNK_SIZE = 4000
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if not cls.TAVILY_API_KEY:
            missing_vars.append("TAVILY_API_KEY")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )

# ====================================
# ENHANCED LEGAL ONTOLOGY NAMESPACES
# ====================================

class LegalRulesNamespaces:
    """Enhanced namespaces for Rules-as-Code with DPV and PROV-O integration"""
    
    # Core W3C vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal domain vocabularies
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Our enhanced Rules-as-Code namespace
    RAC = Namespace("https://rules-as-code.org/ontology#")
    
    # Data management domain-specific namespaces
    STORAGE = Namespace("https://rules-as-code.org/storage#")
    USAGE = Namespace("https://rules-as-code.org/usage#")
    MOVEMENT = Namespace("https://rules-as-code.org/movement#")
    PRIVACY = Namespace("https://rules-as-code.org/privacy#")
    SECURITY = Namespace("https://rules-as-code.org/security#")
    ACCESS = Namespace("https://rules-as-code.org/access#")
    ENTITLEMENTS = Namespace("https://rules-as-code.org/entitlements#")
    
    # Property and relation namespaces
    PROPERTIES = Namespace("https://rules-as-code.org/properties#")
    RELATIONS = Namespace("https://rules-as-code.org/relations#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("rac", cls.RAC)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("entitlements", cls.ENTITLEMENTS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# STATE DEFINITIONS FOR LANGGRAPH
# ====================================

class DocumentProcessingState(TypedDict):
    """State for document processing workflow"""
    messages: Annotated[List, add_messages]
    document_path: str
    document_metadata: Dict[str, Any]
    extracted_text: Optional[str]
    processing_status: str
    current_agent: str
    extraction_results: Optional[Dict[str, Any]]
    ontology_graph: Optional[str]  # Serialized graph
    validation_results: Optional[Dict[str, Any]]
    memory_stored: bool
    exports: Optional[Dict[str, str]]
    reasoning_log: List[Dict[str, Any]]

class AgentState(TypedDict):
    """Base state for individual agents"""
    messages: Annotated[List, add_messages]
    current_task: str
    context: Dict[str, Any]
    results: Dict[str, Any]
    reasoning_steps: List[str]
    tool_calls: List[Dict[str, Any]]

# ====================================
# TOOLS FOR REACT AGENTS
# ====================================

# Document processing tools
@tool
def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text content from a PDF document"""
    try:
        doc = pymupdf.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text.strip()
    except Exception as e:
        return f"Error extracting text from PDF: {str(e)}"

@tool
def analyze_document_structure(text: str) -> Dict[str, Any]:
    """Analyze the structure of a legal document"""
    try:
        lines = text.split('\n')
        structure = {
            "total_lines": len(lines),
            "estimated_pages": len(text) // 3000,
            "has_articles": any("Article" in line for line in lines[:100]),
            "has_sections": any("Section" in line for line in lines[:100]),
            "has_definitions": any("definition" in line.lower() for line in lines[:200]),
            "language_indicators": {
                "gdpr_terms": any(term in text.lower() for term in ["personal data", "data controller", "data processor"]),
                "privacy_terms": any(term in text.lower() for term in ["privacy", "consent", "rights"]),
                "data_management": any(term in text.lower() for term in ["storage", "retention", "transfer"])
            }
        }
        return structure
    except Exception as e:
        return {"error": f"Error analyzing structure: {str(e)}"}

# Web search tools
@tool 
def search_legal_concepts(query: str, max_results: int = 5) -> List[Dict[str, str]]:
    """Search for legal concepts and definitions online"""
    try:
        search_tool = TavilySearchResults(
            max_results=max_results,
            api_wrapper_kwargs={"api_key": Config.TAVILY_API_KEY}
        )
        results = search_tool.invoke(query)
        return results if isinstance(results, list) else [results]
    except Exception as e:
        return [{"error": f"Search failed: {str(e)}"}]

@tool
def search_adequacy_decisions(country: str) -> List[Dict[str, str]]:
    """Search for adequacy decisions related to a specific country"""
    query = f"adequacy decision {country} data protection GDPR"
    return search_legal_concepts(query, max_results=3)

# Validation tools
@tool
def validate_extraction_completeness(extraction_data: Dict[str, Any]) -> Dict[str, Any]:
    """Validate the completeness of extraction results"""
    try:
        validation = {
            "subjects_found": len(extraction_data.get("subjects", [])),
            "definitions_found": len([s for s in extraction_data.get("subjects", []) if s.get("definition")]),
            "rules_found": len([s for s in extraction_data.get("subjects", []) if s.get("rules")]),
            "domains_covered": list(set([d for s in extraction_data.get("subjects", []) for d in s.get("domains", [])])),
            "completeness_score": 0.0
        }
        
        # Calculate completeness score
        if validation["subjects_found"] > 0:
            definition_ratio = validation["definitions_found"] / validation["subjects_found"]
            domain_coverage = len(validation["domains_covered"]) / 7  # 7 domains max
            validation["completeness_score"] = (definition_ratio * 0.6) + (domain_coverage * 0.4)
        
        validation["is_complete"] = validation["completeness_score"] > 0.7
        return validation
    except Exception as e:
        return {"error": f"Validation failed: {str(e)}"}

# ====================================
# MEMORY SETUP WITH LANGMEM
# ====================================

def setup_memory_store():
    """Setup LangMem memory store for long-term memory"""
    try:
        # Create memory store with vector indexing
        store = InMemoryStore(
            index={
                "dims": 3072,  # text-embedding-3-large dimensions
                "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
            }
        )
        logger.info("Memory store initialized with vector indexing")
        return store
    except Exception as e:
        logger.error(f"Failed to setup memory store: {e}")
        return InMemoryStore()

def create_memory_tools(namespace_params: Dict[str, str]):
    """Create memory management tools for agents"""
    try:
        namespace = ("legal_rules", namespace_params.get("jurisdiction", "default"), 
                    namespace_params.get("organization", "default"))
        
        manage_tool = create_manage_memory_tool(namespace=namespace)
        search_tool = create_search_memory_tool(namespace=namespace)
        
        return [manage_tool, search_tool]
    except Exception as e:
        logger.error(f"Failed to create memory tools: {e}")
        return []

# ====================================
# REACT AGENTS WITH LANGGRAPH
# ====================================

class DocumentProcessorAgent:
    """ReAct agent for document processing and text extraction"""
    
    def __init__(self, memory_store):
        self.memory_store = memory_store
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=0,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Define tools for this agent
        self.tools = [
            extract_text_from_pdf,
            analyze_document_structure,
            search_legal_concepts
        ]
        
        # Create ReAct agent with LangGraph
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt="You are a legal document processing expert. Your role is to extract and analyze text from legal documents, identify their structure, and prepare them for rule extraction. Always be thorough and precise in your analysis."
        )
        
        logger.info("DocumentProcessorAgent initialized with ReAct architecture")
    
    async def process_document(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Process a legal document and extract structured information"""
        
        # Create message for the agent
        message = HumanMessage(content=f"""
        Process this legal document for rules extraction:
        
        Document Path: {state['document_path']}
        Jurisdiction: {state['document_metadata'].get('jurisdiction', 'Unknown')}
        Country: {state['document_metadata'].get('country', 'Unknown')}
        Organization: {state['document_metadata'].get('organization', 'Unknown')}
        
        Tasks:
        1. Extract text from the PDF document
        2. Analyze the document structure
        3. Identify key legal concepts and terms
        4. Prepare a summary for rule extraction
        
        Return a comprehensive analysis that will help the next agent extract legal rules.
        """)
        
        # Invoke the ReAct agent
        result = await self.agent.ainvoke({"messages": [message]}, config)
        
        # Extract the response
        assistant_message = result["messages"][-1]
        
        # Update state
        state["messages"].extend(result["messages"])
        state["processing_status"] = "text_extracted"
        state["current_agent"] = "DocumentProcessor"
        
        # Try to extract actual text if agent provided instructions
        if state["document_path"] and os.path.exists(state["document_path"]):
            try:
                extracted_text = extract_text_from_pdf(state["document_path"])
                state["extracted_text"] = extracted_text
                logger.info(f"Extracted {len(extracted_text)} characters from document")
            except Exception as e:
                logger.error(f"Text extraction failed: {e}")
                state["extracted_text"] = "Text extraction failed"
        
        return state

class RuleExtractionAgent:
    """ReAct agent for extracting legal rules from text"""
    
    def __init__(self, memory_store):
        self.memory_store = memory_store
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=0,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL,
            model_kwargs={
                "reasoning_effort": Config.REASONING_EFFORT,
                "max_completion_tokens": Config.MAX_COMPLETION_TOKENS
            }
        )
        
        # Define tools for this agent
        self.tools = [
            search_legal_concepts,
            search_adequacy_decisions,
            validate_extraction_completeness
        ]
        
        # Create ReAct agent
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt="""You are a legal rules extraction expert specializing in converting legislation into machine-readable formats. 

Your task is to extract comprehensive legal knowledge from legal texts and structure it as Rules-as-Code with:

1. SUBJECTS - Legal entities/concepts with:
   - DEFINITION: Precise legal definition (MANDATORY)
   - RULES: Legal obligations, permissions, prohibitions (OPTIONAL)
   - CONDITIONS: Applicability conditions (OPTIONAL)
   - DOMAINS: Data management domains (MANDATORY)
   - RELATIONSHIPS: Connections to other subjects

2. OBJECT PROPERTIES - Relationships between subjects
3. DATA PROPERTIES - Attributes with literal values  
4. ADEQUACY DECISIONS - Cross-border transfer rules

Focus on these data management domains:
- STORAGE: retention, archiving, deletion
- USAGE: purpose limitation, processing activities  
- MOVEMENT: transfers, cross-border, adequacy
- PRIVACY: consent, transparency, rights
- SECURITY: encryption, access controls, breach notification
- ACCESS: authorization, authentication, audit
- ENTITLEMENTS: roles, permissions, data access rights

Use your tools to search for additional context and validate your extractions."""
        )
        
        logger.info("RuleExtractionAgent initialized with ReAct architecture")
    
    async def extract_rules(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Extract legal rules from the processed text"""
        
        if not state.get("extracted_text"):
            state["extraction_results"] = {"error": "No extracted text available"}
            return state
        
        # Add memory tools to the agent for this specific jurisdiction
        memory_tools = create_memory_tools({
            "jurisdiction": state["document_metadata"].get("jurisdiction", "default"),
            "organization": state["document_metadata"].get("organization", "default")
        })
        
        # Create message for rule extraction
        text_preview = state["extracted_text"][:5000] + "..." if len(state["extracted_text"]) > 5000 else state["extracted_text"]
        
        message = HumanMessage(content=f"""
        Extract comprehensive legal rules from this legal document:
        
        CONTEXT:
        - Country: {state['document_metadata'].get('country', 'Unknown')}
        - Jurisdiction: {state['document_metadata'].get('jurisdiction', 'Unknown')}
        - Organization: {state['document_metadata'].get('organization', 'Unknown')}
        
        LEGAL TEXT (first 5000 characters):
        {text_preview}
        
        Extract all legal subjects with their definitions, rules, conditions, and domain classifications.
        Use your search tools to validate definitions and find additional context.
        
        Return structured JSON with subjects, object_properties, data_properties, and adequacy_decisions.
        """)
        
        # Invoke the ReAct agent
        result = await self.agent.ainvoke({"messages": [message]}, config)
        
        # Extract and parse the results
        assistant_message = result["messages"][-1].content
        
        # Try to parse JSON from the response
        try:
            # Look for JSON in the response
            json_start = assistant_message.find('{')
            json_end = assistant_message.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = assistant_message[json_start:json_end]
                extraction_results = json.loads(json_str)
            else:
                # Fallback: create basic structure
                extraction_results = {
                    "subjects": [],
                    "object_properties": [],
                    "data_properties": [],
                    "adequacy_decisions": [],
                    "raw_response": assistant_message
                }
        except json.JSONDecodeError:
            extraction_results = {
                "subjects": [],
                "object_properties": [],
                "data_properties": [],
                "adequacy_decisions": [],
                "raw_response": assistant_message,
                "parse_error": "Failed to parse JSON from agent response"
            }
        
        # Update state
        state["messages"].extend(result["messages"])
        state["extraction_results"] = extraction_results
        state["processing_status"] = "rules_extracted"
        state["current_agent"] = "RuleExtractor"
        
        logger.info(f"Extracted {len(extraction_results.get('subjects', []))} subjects")
        
        return state

class OntologyBuilderAgent:
    """ReAct agent for building ontologies from extracted rules"""
    
    def __init__(self, memory_store):
        self.memory_store = memory_store
        self.ns = LegalRulesNamespaces()
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=0,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Create ReAct agent for ontology building
        self.agent = create_react_agent(
            model=self.llm,
            tools=[validate_extraction_completeness],
            prompt="You are an ontology engineering expert. Your role is to build comprehensive OWL ontologies and RDF knowledge graphs from extracted legal rules. Focus on semantic accuracy, proper relationships, and DPV/PROV-O integration."
        )
        
        logger.info("OntologyBuilderAgent initialized")
    
    async def build_ontology(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Build ontology from extraction results"""
        
        if not state.get("extraction_results"):
            state["ontology_graph"] = None
            return state
        
        # Create RDF graphs
        owl_ontology = Graph()
        ttl_graph = Graph()
        
        # Bind namespaces
        owl_ontology = self.ns.bind_to_graph(owl_ontology)
        ttl_graph = self.ns.bind_to_graph(ttl_graph)
        
        # Build ontology structure
        self._build_ontology_structure(owl_ontology, state["document_metadata"])
        self._populate_knowledge_graph(ttl_graph, state["extraction_results"], state["document_metadata"])
        
        # Serialize graphs
        ontology_ttl = owl_ontology.serialize(format="turtle")
        knowledge_graph_ttl = ttl_graph.serialize(format="turtle")
        
        # Store serialized graphs in state
        state["ontology_graph"] = {
            "owl_ontology": ontology_ttl,
            "knowledge_graph": knowledge_graph_ttl,
            "owl_triples": len(owl_ontology),
            "kg_triples": len(ttl_graph)
        }
        
        state["processing_status"] = "ontology_built"
        state["current_agent"] = "OntologyBuilder"
        
        logger.info(f"Built ontology with {len(owl_ontology)} OWL triples and {len(ttl_graph)} KG triples")
        
        return state
    
    def _build_ontology_structure(self, graph: Graph, metadata: Dict):
        """Build the core ontology structure"""
        
        # Ontology metadata
        ontology_uri = URIRef(f"{self.ns.RAC}ontology")
        graph.add((ontology_uri, RDF.type, OWL.Ontology))
        graph.add((ontology_uri, DCTERMS.title, 
                  Literal(f"Rules-as-Code Ontology for {metadata.get('country', 'Legal Domain')}")))
        graph.add((ontology_uri, DCTERMS.created, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        
        # Core classes
        core_classes = [
            ("LegalSubject", "A legal entity, concept, or subject"),
            ("LegalRule", "A legal obligation, permission, or prohibition"),
            ("LegalDefinition", "A formal legal definition"),
            ("DataManagementDomain", "A domain of data management"),
            ("AdequacyDecision", "An adequacy decision for data transfers")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.RAC}{class_name}")
            graph.add((class_uri, RDF.type, OWL.Class))
            graph.add((class_uri, RDFS.label, Literal(class_name)))
            graph.add((class_uri, RDFS.comment, Literal(definition)))
    
    def _populate_knowledge_graph(self, graph: Graph, extraction_results: Dict, metadata: Dict):
        """Populate knowledge graph with extracted data"""
        
        # Add subjects as individuals
        for subject in extraction_results.get("subjects", []):
            self._add_subject_individual(graph, subject, metadata)
        
        # Add adequacy decisions
        for adequacy in extraction_results.get("adequacy_decisions", []):
            self._add_adequacy_decision(graph, adequacy, metadata)
    
    def _add_subject_individual(self, graph: Graph, subject: Dict, metadata: Dict):
        """Add a legal subject as an individual"""
        
        subject_name = subject.get("name", "")
        if not subject_name:
            return
        
        # Create individual URI
        uri_suffix = self._safe_uri_encode(subject_name)
        individual_uri = URIRef(f"{self.ns.RAC}{uri_suffix}_Individual")
        
        # Add basic properties
        graph.add((individual_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalSubject")))
        graph.add((individual_uri, RDFS.label, Literal(subject_name)))
        
        # Add definition
        if subject.get("definition"):
            graph.add((individual_uri, SKOS.definition, Literal(subject["definition"])))
        
        # Add domain classifications
        for domain in subject.get("domains", []):
            domain_uri = URIRef(f"{self.ns.RAC}{domain.title()}Domain")
            graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}belongsToDomain"), domain_uri))
        
        # Add provenance
        graph.add((individual_uri, self.ns.PROV.wasGeneratedBy, URIRef(f"{self.ns.RAC}RulesAsCodeExtraction")))
        graph.add((individual_uri, self.ns.PROV.generatedAtTime, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_adequacy_decision(self, graph: Graph, adequacy: Dict, metadata: Dict):
        """Add adequacy decision as individual"""
        
        country_name = adequacy.get("country", "").replace(" ", "_")
        adequacy_uri = URIRef(f"{self.ns.RAC}AdequacyDecision_{country_name}")
        
        graph.add((adequacy_uri, RDF.type, URIRef(f"{self.ns.RAC}AdequacyDecision")))
        graph.add((adequacy_uri, RDFS.label, Literal(f"Adequacy Decision for {adequacy.get('country', 'Unknown')}")))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasAdequacyStatus"), 
                  Literal(adequacy.get("status", "unknown"))))
    
    def _safe_uri_encode(self, text: str) -> str:
        """Safely encode text for URIs"""
        import urllib.parse
        safe_text = text.replace(' ', '_').replace('/', '_').replace('\\', '_')
        return urllib.parse.quote(safe_text, safe='')

class MemoryManagerAgent:
    """ReAct agent for managing long-term memory with LangMem"""
    
    def __init__(self, memory_store):
        self.memory_store = memory_store
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=0,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        logger.info("MemoryManagerAgent initialized")
    
    async def store_memories(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Store extraction results in long-term memory"""
        
        try:
            # Get memory tools for this jurisdiction
            jurisdiction = state["document_metadata"].get("jurisdiction", "default")
            organization = state["document_metadata"].get("organization", "default")
            
            memory_tools = create_memory_tools({
                "jurisdiction": jurisdiction,
                "organization": organization
            })
            
            if memory_tools and state.get("extraction_results"):
                # Create memory manager for this specific case
                memory_manager = create_memory_manager(
                    f"openai:{Config.OPENAI_MODEL}",
                    namespace=("legal_rules", jurisdiction, organization),
                    instructions=f"""Store important legal concepts and rules from {jurisdiction} legislation.
                    Focus on subjects with definitions, legal rules, and cross-jurisdictional relationships.""",
                    store=self.memory_store,
                    enable_inserts=True,
                    enable_updates=True
                )
                
                # Prepare memory data
                extraction_summary = {
                    "subjects_count": len(state["extraction_results"].get("subjects", [])),
                    "adequacy_decisions": state["extraction_results"].get("adequacy_decisions", []),
                    "jurisdiction": jurisdiction,
                    "country": state["document_metadata"].get("country", ""),
                    "processing_date": datetime.now().isoformat()
                }
                
                # Store in memory
                memory_input = {
                    "messages": [
                        HumanMessage(content=f"Store legal knowledge from {jurisdiction}: {json.dumps(extraction_summary)}")
                    ]
                }
                
                config_with_namespace = {
                    "configurable": {
                        "jurisdiction": jurisdiction,
                        "organization": organization
                    }
                }
                
                await memory_manager.ainvoke(memory_input, config=config_with_namespace)
                
                state["memory_stored"] = True
                logger.info(f"Stored memories for {jurisdiction}")
            
        except Exception as e:
            logger.error(f"Failed to store memories: {e}")
            state["memory_stored"] = False
        
        state["processing_status"] = "memory_stored"
        state["current_agent"] = "MemoryManager"
        
        return state

# ====================================
# MULTI-AGENT ORCHESTRATOR WITH LANGGRAPH
# ====================================

class RulesAsCodeOrchestrator:
    """Main orchestrator using LangGraph for multi-agent coordination"""
    
    def __init__(self):
        # Validate configuration
        Config.validate_config()
        
        # Setup memory store
        self.memory_store = setup_memory_store()
        
        # Initialize checkpointer for conversation memory
        self.checkpointer = MemorySaver()
        
        # Initialize agents
        self.doc_processor = DocumentProcessorAgent(self.memory_store)
        self.rule_extractor = RuleExtractionAgent(self.memory_store)
        self.ontology_builder = OntologyBuilderAgent(self.memory_store)
        self.memory_manager = MemoryManagerAgent(self.memory_store)
        
        # Build the agent graph
        self.graph = self._build_agent_graph()
        
        logger.info("RulesAsCodeOrchestrator initialized with multi-agent architecture")
    
    def _build_agent_graph(self) -> StateGraph:
        """Build the LangGraph state graph for agent coordination"""
        
        # Create the state graph
        workflow = StateGraph(DocumentProcessingState)
        
        # Add agent nodes
        workflow.add_node("document_processor", self._document_processor_node)
        workflow.add_node("rule_extractor", self._rule_extractor_node)
        workflow.add_node("ontology_builder", self._ontology_builder_node)
        workflow.add_node("memory_manager", self._memory_manager_node)
        workflow.add_node("validator", self._validator_node)
        workflow.add_node("exporter", self._exporter_node)
        
        # Define the workflow edges
        workflow.add_edge(START, "document_processor")
        workflow.add_edge("document_processor", "rule_extractor")
        workflow.add_edge("rule_extractor", "ontology_builder")
        workflow.add_edge("ontology_builder", "memory_manager")
        workflow.add_edge("memory_manager", "validator")
        workflow.add_edge("validator", "exporter")
        workflow.add_edge("exporter", END)
        
        # Compile the graph with checkpointer
        compiled_graph = workflow.compile(checkpointer=self.checkpointer)
        
        logger.info("Agent workflow graph compiled successfully")
        return compiled_graph
    
    async def _document_processor_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Document processor node"""
        logger.info("Executing document processor agent")
        return await self.doc_processor.process_document(state, config)
    
    async def _rule_extractor_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Rule extractor node"""
        logger.info("Executing rule extraction agent")
        return await self.rule_extractor.extract_rules(state, config)
    
    async def _ontology_builder_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Ontology builder node"""
        logger.info("Executing ontology builder agent")
        return await self.ontology_builder.build_ontology(state, config)
    
    async def _memory_manager_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Memory manager node"""
        logger.info("Executing memory manager agent")
        return await self.memory_manager.store_memories(state, config)
    
    async def _validator_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Validation node"""
        logger.info("Executing validation")
        
        if state.get("extraction_results"):
            validation = validate_extraction_completeness(state["extraction_results"])
            state["validation_results"] = validation
            
            # Add validation message
            validation_msg = AIMessage(content=f"Validation completed. Completeness score: {validation.get('completeness_score', 0):.2f}")
            state["messages"].append(validation_msg)
        
        state["processing_status"] = "validated"
        return state
    
    async def _exporter_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Export node"""
        logger.info("Executing export")
        
        exports = {}
        
        if state.get("ontology_graph"):
            # Create output directory
            country = state["document_metadata"].get("country", "Unknown").replace(" ", "_")
            output_dir = Path(Config.OUTPUT_PATH) / country
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Export ontology
            owl_path = output_dir / f"ontology_{country}.ttl"
            with open(owl_path, "w") as f:
                f.write(state["ontology_graph"]["owl_ontology"])
            exports["owl_ontology"] = str(owl_path)
            
            # Export knowledge graph
            kg_path = output_dir / f"knowledge_graph_{country}.ttl"
            with open(kg_path, "w") as f:
                f.write(state["ontology_graph"]["knowledge_graph"])
            exports["knowledge_graph"] = str(kg_path)
            
            # Export extraction results as JSON
            json_path = output_dir / f"extraction_results_{country}.json"
            with open(json_path, "w") as f:
                json.dump(state["extraction_results"], f, indent=2)
            exports["extraction_json"] = str(json_path)
        
        state["exports"] = exports
        state["processing_status"] = "completed"
        
        # Add completion message
        completion_msg = AIMessage(content=f"Processing completed. Exported {len(exports)} files.")
        state["messages"].append(completion_msg)
        
        return state
    
    async def process_document(self, document_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process a legal document through the agent workflow"""
        
        logger.info(f"Starting document processing: {document_path}")
        
        # Initialize state
        initial_state = DocumentProcessingState(
            messages=[],
            document_path=document_path,
            document_metadata=metadata,
            extracted_text=None,
            processing_status="initialized",
            current_agent="",
            extraction_results=None,
            ontology_graph=None,
            validation_results=None,
            memory_stored=False,
            exports=None,
            reasoning_log=[]
        )
        
        # Create config with thread ID for conversation memory
        config = RunnableConfig(configurable={"thread_id": str(uuid.uuid4())})
        
        try:
            # Execute the workflow
            final_state = await self.graph.ainvoke(initial_state, config)
            
            # Prepare results summary
            results = {
                "success": True,
                "processing_status": final_state.get("processing_status", "unknown"),
                "extraction_stats": self._calculate_extraction_stats(final_state),
                "ontology_stats": self._calculate_ontology_stats(final_state),
                "validation_results": final_state.get("validation_results", {}),
                "exports": final_state.get("exports", {}),
                "memory_stored": final_state.get("memory_stored", False),
                "message_count": len(final_state.get("messages", [])),
                "reasoning_log": final_state.get("reasoning_log", [])
            }
            
            logger.info(f"Document processing completed successfully")
            return results
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_status": "failed"
            }
    
    def _calculate_extraction_stats(self, state: DocumentProcessingState) -> Dict[str, Any]:
        """Calculate extraction statistics"""
        
        extraction_results = state.get("extraction_results", {})
        subjects = extraction_results.get("subjects", [])
        
        return {
            "subjects_extracted": len(subjects),
            "subjects_with_definitions": len([s for s in subjects if s.get("definition")]),
            "subjects_with_rules": len([s for s in subjects if s.get("rules")]),
            "object_properties": len(extraction_results.get("object_properties", [])),
            "data_properties": len(extraction_results.get("data_properties", [])),
            "adequacy_decisions": len(extraction_results.get("adequacy_decisions", [])),
            "domains_covered": list(set([d for s in subjects for d in s.get("domains", [])]))
        }
    
    def _calculate_ontology_stats(self, state: DocumentProcessingState) -> Dict[str, Any]:
        """Calculate ontology statistics"""
        
        ontology_graph = state.get("ontology_graph", {})
        
        return {
            "owl_triples": ontology_graph.get("owl_triples", 0),
            "kg_triples": ontology_graph.get("kg_triples", 0),
            "ontology_generated": ontology_graph is not None
        }

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission", 
                    "legal_system": "Civil Law",
                    "pdf_document": "./documents/gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United States",
                    "jurisdiction": "California",
                    "organization": "California Consumer Privacy Act",
                    "legal_system": "Common Law", 
                    "pdf_document": "./documents/ccpa.pdf",
                    "adequacy_focus": False,
                    "data_management_domains": ["privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United Kingdom",
                    "jurisdiction": "UK",
                    "organization": "Information Commissioner's Office",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/uk_gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                }
            ],
            "processing_options": {
                "enable_memory": True,
                "enable_web_search": True,
                "reasoning_effort": "high",
                "generate_competency_questions": True,
                "export_formats": ["ttl", "jsonld", "xml"]
            },
            "adequacy_countries": [
                "Andorra", "Argentina", "Canada", "Faroe Islands", "Guernsey", "Israel",
                "Isle of Man", "Japan", "Jersey", "New Zealand", "South Korea", 
                "Switzerland", "United Kingdom", "Uruguay"
            ]
        }

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Main application for Rules-as-Code with LangGraph agents"""
    
    load_dotenv()
    
    logger.info("🏛️ Starting Enhanced Rules-as-Code System with LangGraph Agents")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info("✅ Configuration validation passed")
    except ValueError as e:
        logger.error(f"❌ Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info("📝 Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json with your documents and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize orchestrator
    logger.info("🔧 Initializing LangGraph orchestrator with ReAct agents...")
    orchestrator = RulesAsCodeOrchestrator()
    
    # Process documents
    results = []
    total_docs = len(config['documents'])
    
    for i, doc_config in enumerate(config['documents'], 1):
        logger.info(f"📖 Processing document {i}/{total_docs}: {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f"⚠️ Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_document(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "jurisdiction": doc_config['jurisdiction'],
                "result": result
            })
            
            if result['success']:
                logger.info(f"✅ Successfully processed {doc_config['country']}")
                logger.info(f"   📊 Subjects extracted: {result['extraction_stats']['subjects_extracted']}")
                logger.info(f"   🏷️ Domains covered: {', '.join(result['extraction_stats']['domains_covered'])}")
                logger.info(f"   💾 Memory stored: {result['memory_stored']}")
            else:
                logger.error(f"❌ Failed to process {doc_config['country']}: {result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f"💥 Processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Generate summary
    successful = sum(1 for r in results if r['result']['success'])
    total_subjects = sum(r['result'].get('extraction_stats', {}).get('subjects_extracted', 0) 
                        for r in results if r['result']['success'])
    
    logger.info("🎉 LangGraph Rules-as-Code processing complete!")
    logger.info(f"   📈 Documents processed: {successful}/{total_docs}")
    logger.info(f"   ⚖️ Total subjects extracted: {total_subjects}")
    
    # Save results
    summary_path = Path(Config.OUTPUT_PATH) / "langgraph_rules_as_code_summary.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary = {
        "processing_summary": {
            "timestamp": datetime.now().isoformat(),
            "documents_processed": successful,
            "total_documents": total_docs,
            "total_subjects_extracted": total_subjects,
            "system_version": "rules-as-code-langgraph-v1.0",
            "agents_used": ["DocumentProcessor", "RuleExtractor", "OntologyBuilder", "MemoryManager"]
        },
        "results": results
    }
    
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    logger.info(f"📄 Summary saved to: {summary_path}")
    logger.info("✨ LangGraph Rules-as-Code processing completed!")

if __name__ == "__main__":
    asyncio.run(main())
