"""
GDC Record Class Mapping System using OpenAI o3-mini with LangGraph ReAct Agents
Supports ONE-TO-MANY mappings with RAG using text-embedding-3-large
Uses OpenAI API directly for embeddings (no tiktoken)
Implements mixture of experts pattern with dynamic chain of thought reasoning
Uses Pydantic v2 for validation and latest LangChain/LangGraph
Uses InMemoryVectorStore for semantic search
"""

import json
import os
from typing import List, Dict, Optional, Any
import pandas as pd
from pydantic import BaseModel, Field, field_validator
from langchain_openai import ChatOpenAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import re
from openai import OpenAI

# ==================== GLOBAL CONFIGURATION ====================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_MODEL = "o3-mini"
REASONING_EFFORT = "high"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Initialize OpenAI client for embeddings
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Initialize global LLM instance with reasoning_effort as direct parameter
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    reasoning_effort=REASONING_EFFORT
)

# ==================== CUSTOM EMBEDDINGS CLASS ====================

class OpenAIDirectEmbeddings(Embeddings):
    """Custom embeddings class using OpenAI API directly without tiktoken"""
    
    def __init__(self, model: str = EMBEDDING_MODEL, dimensions: int = EMBEDDING_DIMENSIONS):
        self.model = model
        self.dimensions = dimensions
        self.client = openai_client
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using OpenAI API directly"""
        try:
            # OpenAI API allows batch embedding
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions
            )
            
            # Extract embeddings from response
            embeddings = [item.embedding for item in response.data]
            return embeddings
        except Exception as e:
            print(f"Error embedding documents: {e}")
            # Return zero vectors as fallback
            return [[0.0] * self.dimensions for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using OpenAI API directly"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions
            )
            
            return response.data[0].embedding
        except Exception as e:
            print(f"Error embedding query: {e}")
            # Return zero vector as fallback
            return [0.0] * self.dimensions

# Initialize global custom embeddings instance
embeddings = OpenAIDirectEmbeddings(model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSIONS)

# Global vector stores
gdc_master_vectorstore: Optional[InMemoryVectorStore] = None
gdc_context_vectorstore: Optional[InMemoryVectorStore] = None

# ==================== PYDANTIC V2 MODELS ====================

class GDCMaster(BaseModel):
    """Pydantic model for GDC Master data"""
    data_domain: str = Field(alias="Data Domain", default="")
    gdc_name: str = Field(alias="GDC Name")
    definition: str = Field(alias="Definition", default="")
    
    class Config:
        populate_by_name = True

class ProcessInfo(BaseModel):
    """Pydantic model for Process information"""
    process_name: str = Field(alias="Process Name", default="")
    process_description: str = Field(alias="Process Description", default="")
    
    class Config:
        populate_by_name = True

class AppInfo(BaseModel):
    """Pydantic model for Application information"""
    app_id: str = Field(alias="App ID", default="")
    app_name: str = Field(alias="App Name", default="")
    app_description: str = Field(alias="App Description", default="")
    processes: List[ProcessInfo] = Field(default_factory=list, alias="Processes")
    
    class Config:
        populate_by_name = True

class PBTInfo(BaseModel):
    """Pydantic model for PBT information"""
    pbt_id: str = Field(alias="PBT ID", default="")
    pbt_name: str = Field(alias="PBT Name", default="")
    pbt_desc: str = Field(alias="PBT Desc", default="")
    apps: List[AppInfo] = Field(default_factory=list, alias="Apps")
    
    class Config:
        populate_by_name = True

class GDCWithContext(BaseModel):
    """Pydantic model for GDC with Context data"""
    gdc_id: str = Field(alias="GDC ID", default="")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    pbts: List[PBTInfo] = Field(default_factory=list, alias="PBTs")
    
    class Config:
        populate_by_name = True

class ValidationEntry(BaseModel):
    """Pydantic model for Validation data"""
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class RecordClass(BaseModel):
    """Pydantic model for Record Class data"""
    guid: str = Field(alias="Guid", default="")
    code: str = Field(alias="Code", default="")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description", default="")
    
    class Config:
        populate_by_name = True

class SemanticMatch(BaseModel):
    """Pydantic model for Semantic Match result"""
    gdc_name: str = Field(description="Name of the matched GDC")
    gdc_description: str = Field(description="Description of the matched GDC", default="")
    similarity_score: float = Field(ge=0, le=100, description="Similarity score between 0-100")
    reasoning: str = Field(description="Detailed reasoning for the match")
    
    @field_validator('similarity_score')
    @classmethod
    def validate_score(cls, v: float) -> float:
        if not 0 <= v <= 100:
            raise ValueError('Similarity score must be between 0 and 100')
        return v

class SemanticMatchResponse(BaseModel):
    """Pydantic model for Semantic Matching Expert response"""
    matches: List[SemanticMatch] = Field(description="List of all relevant semantic matches")
    multiple_matches_rationale: str = Field(description="Explanation of why multiple GDCs are relevant")

class ContextEvidence(BaseModel):
    """Pydantic model for Context Evidence"""
    gdc_name: str = Field(description="Name of the GDC")
    context_evidence: List[str] = Field(description="List of contextual evidence", default_factory=list)
    alignment_score: float = Field(ge=0, le=100, description="Alignment score")
    relevance_justification: str = Field(description="Justification for why this GDC is relevant")
    reasoning: str = Field(description="Detailed reasoning for context analysis")

class ContextAnalysisResponse(BaseModel):
    """Pydantic model for Context Analysis Expert response"""
    context_analysis: List[ContextEvidence] = Field(description="List of context analyses for all relevant GDCs")
    all_relevant_gdcs: List[str] = Field(description="List of all GDC names that are relevant")

class ValidationMatchEntry(BaseModel):
    """Pydantic model for a single validation match entry"""
    gdc_name: str = Field(description="GDC name from validation", default="")
    ilm_category_name: str = Field(description="ILM category name from validation", default="")

class ValidationResultItem(BaseModel):
    """Pydantic model for a single validation result"""
    gdc_name: str = Field(description="GDC being validated")
    validation_found: bool = Field(description="Whether validation entry was found")
    matching_entry: Optional[ValidationMatchEntry] = Field(None, description="Matching entry if found")
    validation_status: str = Field(description="Status: confirmed/conflicted/not_found")
    validation_reasoning: str = Field(description="Reasoning for this validation")

class ValidationResponse(BaseModel):
    """Pydantic model for Validation Expert response"""
    validation_results: List[ValidationResultItem] = Field(description="Validation results for each proposed GDC")
    overall_validation_reasoning: str = Field(description="Overall validation reasoning")

class SingleGDCMapping(BaseModel):
    """Pydantic model for a single GDC mapping"""
    gdc_name: str = Field(description="GDC name")
    gdc_description: str = Field(description="GDC description")
    mapping_rank: int = Field(ge=1, description="Rank of this mapping")
    reasoning: str = Field(description="Comprehensive reasoning for this mapping")
    evidence_summary: List[str] = Field(description="Summary of evidence", default_factory=list)

class FinalMappingDecision(BaseModel):
    """Pydantic model for Final Mapping Decision"""
    gdc_mappings: List[SingleGDCMapping] = Field(description="All relevant GDC mappings")
    overall_reasoning: str = Field(description="Overall reasoning for the mapping decisions")

class MappingResult(BaseModel):
    """Pydantic model for final mapping result"""
    guid: str = Field(alias="GUID")
    code: str = Field(alias="Code")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description")
    mapping_rank: int = Field(alias="Mapping Rank")
    reasoning: str = Field(alias="Reasoning")
    
    class Config:
        populate_by_name = True

# ==================== TEXT PREPROCESSING ====================

def to_lowercase(text: str) -> str:
    """Convert text to lowercase for consistent processing"""
    return text.lower() if text else ""

def preprocess_text(text: str) -> str:
    """Preprocess text: lowercase and clean"""
    text = to_lowercase(text)
    text = " ".join(text.split())
    return text

# ==================== CONTEXT ENGINEERING ====================

def create_enriched_gdc_master_document(gdc: GDCMaster) -> Document:
    """Create context-enriched document for GDC Master with metadata"""
    gdc_name_lower = preprocess_text(gdc.gdc_name)
    definition_lower = preprocess_text(gdc.definition)
    domain_lower = preprocess_text(gdc.data_domain)
    
    enriched_content = f"""gdc category name: {gdc_name_lower}
data domain: {domain_lower}
definition and description: {definition_lower}
this is a group data category for classification purposes
keywords: {gdc_name_lower} {domain_lower}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "data_domain": domain_lower,
        "definition": definition_lower,
        "type": "gdc_master"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

def create_enriched_gdc_context_document(gdc_ctx: GDCWithContext) -> Document:
    """Create context-enriched document for GDC with hierarchical context"""
    gdc_name_lower = preprocess_text(gdc_ctx.gdc_name)
    gdc_desc_lower = preprocess_text(gdc_ctx.gdc_description)
    
    pbt_names = []
    app_names = []
    process_names = []
    
    for pbt in gdc_ctx.pbts:
        pbt_name_lower = preprocess_text(pbt.pbt_name)
        pbt_desc_lower = preprocess_text(pbt.pbt_desc)
        pbt_names.append(f"{pbt_name_lower} ({pbt_desc_lower})")
        
        for app in pbt.apps:
            app_name_lower = preprocess_text(app.app_name)
            app_desc_lower = preprocess_text(app.app_description)
            app_names.append(f"{app_name_lower} - {app_desc_lower}")
            
            for proc in app.processes:
                proc_name_lower = preprocess_text(proc.process_name)
                proc_desc_lower = preprocess_text(proc.process_description)
                process_names.append(f"{proc_name_lower}: {proc_desc_lower}")
    
    enriched_content = f"""gdc category: {gdc_name_lower}
gdc_description: {gdc_desc_lower}

primary business types (pbts):
{chr(10).join(f"- {pbt}" for pbt in pbt_names) if pbt_names else "- none"}

applications using this gdc:
{chr(10).join(f"- {app}" for app in app_names) if app_names else "- none"}

related processes:
{chr(10).join(f"- {proc}" for proc in process_names) if process_names else "- none"}

contextual keywords: {gdc_name_lower} {' '.join(pbt_names)} {' '.join(app_names)}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "gdc_description": gdc_desc_lower,
        "pbt_count": len(pbt_names),
        "app_count": len(app_names),
        "process_count": len(process_names),
        "type": "gdc_context"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

# ==================== UTILITY FUNCTIONS ====================

def extract_json_from_text(text: str) -> str:
    """Extract JSON from text that might contain markdown code blocks"""
    text = re.sub(r'```(?:json)?\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    
    json_match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
    if json_match:
        return json_match.group(1)
    
    return text

def load_json_file(filepath: str, model_class: type[BaseModel]) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except Exception as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== RAG VECTOR STORE SETUP ====================

def build_gdc_master_vectorstore(gdc_master_list: List[GDCMaster]) -> InMemoryVectorStore:
    """Build vector store for GDC Master data using OpenAI API directly"""
    print(f"🔧 Building GDC Master vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_master_document(gdc) for gdc in gdc_master_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Master vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def build_gdc_context_vectorstore(gdc_context_list: List[GDCWithContext]) -> InMemoryVectorStore:
    """Build vector store for GDC Context data using OpenAI API directly"""
    print(f"🔧 Building GDC Context vector store with {EMBEDDING_MODEL} (OpenAI API)...")
    
    documents = [create_enriched_gdc_context_document(gdc_ctx) for gdc_ctx in gdc_context_list]
    
    # Create vector store with custom embeddings
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Context vector store created with {len(documents)} documents")
    print(f"✓ Using OpenAI API directly (no tiktoken)")
    return vectorstore

def rag_retrieve_relevant_gdcs(query: str, k: int = 10) -> str:
    """RAG: Retrieve relevant GDCs using semantic search"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    query_lower = preprocess_text(query)
    
    master_results = gdc_master_vectorstore.similarity_search(query_lower, k=k)
    context_results = gdc_context_vectorstore.similarity_search(query_lower, k=k)
    
    retrieved_info = {
        "master_matches": [],
        "context_matches": []
    }
    
    for doc in master_results:
        retrieved_info["master_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "definition": doc.metadata.get("definition", ""),
            "data_domain": doc.metadata.get("data_domain", "")
        })
    
    for doc in context_results:
        retrieved_info["context_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "gdc_description": doc.metadata.get("gdc_description", ""),
            "pbt_count": doc.metadata.get("pbt_count", 0),
            "app_count": doc.metadata.get("app_count", 0),
            "process_count": doc.metadata.get("process_count", 0),
            "content_preview": doc.page_content[:300]
        })
    
    return json.dumps(retrieved_info, indent=2)

# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str) -> str:
    """
    Expert for semantic similarity analysis using RAG with OpenAI embeddings API.
    Identifies ALL relevant GDCs using vector similarity search.
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    query = f"{record_name_lower} {record_desc_lower}"
    retrieved_gdcs = rag_retrieve_relevant_gdcs(query, k=15)
    
    prompt = f"""You are a semantic similarity expert with RAG-retrieved GDCs.

IMPORTANT: A Record Class can map to MULTIPLE GDCs.

RECORD CLASS:
Name: {record_name_lower}
Description: {record_desc_lower}

RAG-RETRIEVED RELEVANT GDCs (via OpenAI embeddings API):
{retrieved_gdcs}

INSTRUCTIONS:
1. Review the RAG-retrieved GDCs
2. Identify ALL relevant GDCs (not just top match)
3. Score each from 0-100 (internal ranking)
4. Provide detailed reasoning

OUTPUT FORMAT (valid JSON only):
{{
  "matches": [
    {{
      "gdc_name": "gdc name",
      "gdc_description": "definition",
      "similarity_score": 88.5,
      "reasoning": "detailed explanation"
    }}
  ],
  "multiple_matches_rationale": "why multiple gdcs needed"
}}

CRITICAL: Return ALL relevant GDCs, only valid JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = SemanticMatchResponse(
            matches=[],
            multiple_matches_rationale=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, semantic_matches: str) -> str:
    """
    Expert for analyzing GDC context using RAG retrieval with OpenAI embeddings API.
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    try:
        matches_data = json.loads(extract_json_from_text(semantic_matches))
        gdc_names = [m.get("gdc_name", "").lower() for m in matches_data.get("matches", [])]
    except:
        gdc_names = []
    
    context_query = f"{record_name_lower} {record_desc_lower} {' '.join(gdc_names)}"
    retrieved_context = rag_retrieve_relevant_gdcs(context_query, k=10)
    
    prompt = f"""You are a context analysis expert with RAG context.

RECORD CLASS:
Name: {record_name_lower}
Description: {record_desc_lower}

SEMANTIC MATCHES:
{semantic_matches}

RAG-RETRIEVED CONTEXT (via OpenAI embeddings API):
{retrieved_context}

INSTRUCTIONS:
1. Analyze RAG-retrieved context for each match
2. Identify PBTs, Apps, Processes
3. Keep ALL GDCs with valid support
4. Provide evidence-based justification

OUTPUT FORMAT (valid JSON only):
{{
  "context_analysis": [
    {{
      "gdc_name": "name",
      "context_evidence": ["pbt: ...", "app: ...", "process: ..."],
      "alignment_score": 90.0,
      "relevance_justification": "why relevant",
      "reasoning": "detailed explanation"
    }}
  ],
  "all_relevant_gdcs": ["gdc1", "gdc2"]
}}

CRITICAL: Only valid JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            all_relevant_gdcs=[]
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdcs: str, validation_data: str) -> str:
    """
    Expert for validating ALL proposed GDC mappings.
    """
    record_name_lower = preprocess_text(record_name)
    
    try:
        val_data = json.loads(validation_data)
        val_data_lower = []
        for entry in val_data:
            val_data_lower.append({
                "gdc_name": to_lowercase(entry.get("GDC Name", "")),
                "gdc_description": to_lowercase(entry.get("GDC Description", "")),
                "ilm_category_name": to_lowercase(entry.get("ILM Category Name", ""))
            })
        validation_data_lower = json.dumps(val_data_lower, indent=2)
    except:
        validation_data_lower = validation_data
    
    prompt = f"""You are a validation expert.

RECORD CLASS NAME: {record_name_lower}
PROPOSED GDCs: {proposed_gdcs}

VALIDATION DATASET:
{validation_data_lower}

INSTRUCTIONS:
1. For each proposed GDC, search validation set
2. Identify confirmations or conflicts
3. Provide reasoning

OUTPUT FORMAT (valid JSON only):
{{
  "validation_results": [
    {{
      "gdc_name": "name",
      "validation_found": true,
      "matching_entry": {{
        "gdc_name": "...",
        "ilm_category_name": "..."
      }},
      "validation_status": "confirmed",
      "validation_reasoning": "explanation"
    }}
  ],
  "overall_validation_reasoning": "summary"
}}

CRITICAL: Only valid JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ValidationResponse(
            validation_results=[],
            overall_validation_reasoning=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    Expert for making final mapping decisions for ALL relevant GDCs.
    """
    prompt = f"""You are a final decision expert.

RECORD INFORMATION:
{record_info}

ALL ANALYSES:
{all_analyses}

INSTRUCTIONS:
1. Synthesize all analyses
2. Rank GDCs by relevance
3. Provide reasoning for EACH mapping
4. Explain why multiple GDCs needed

OUTPUT FORMAT (valid JSON only):
{{
  "gdc_mappings": [
    {{
      "gdc_name": "name",
      "gdc_description": "definition",
      "mapping_rank": 1,
      "reasoning": "comprehensive reasoning",
      "evidence_summary": ["semantic: ...", "context: ...", "validation: ..."]
    }}
  ],
  "overall_reasoning": "rationale"
}}

CRITICAL: Only valid JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = FinalMappingDecision(
            gdc_mappings=[],
            overall_reasoning=f"error: {str(e)}"
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an expert GDC mapping coordinator using RAG and mixture of experts.

CRITICAL: Record Classes can map to MULTIPLE GDCs. Use RAG-retrieved context.

Workflow:
1. SEMANTIC MATCHING: semantic_similarity_expert (with RAG via OpenAI API)
2. CONTEXT ANALYSIS: context_analysis_expert (with RAG via OpenAI API)
3. VALIDATION: validation_expert
4. FINAL DECISION: final_decision_expert

All text is lowercase. Focus on evidence quality."""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    validation_set: List[ValidationEntry]
) -> List[MappingResult]:
    """Process a single record through the ReAct agent"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    query = f"""Map this Record Class to ALL relevant GDCs using RAG:

RECORD CLASS:
- GUID: {record.guid}
- Code: {record.code}
- Name: {record.name}
- Description: {record.description}

Steps:
1. semantic_similarity_expert (uses RAG with OpenAI embeddings API)
2. context_analysis_expert (uses RAG with OpenAI embeddings API)
3. validation_expert
4. final_decision_expert

Provide comprehensive reasoning."""
    
    try:
        result = agent.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        messages = result.get("messages", [])
        final_message = messages[-1] if messages else None
        
        if not final_message:
            raise ValueError("No response from agent")
        
        response_content = final_message.content
        final_decision = extract_final_decision(response_content)
        
        mapping_results = []
        gdc_mappings = final_decision.get("gdc_mappings", [])
        overall_reasoning = final_decision.get("overall_reasoning", "")
        
        if not gdc_mappings:
            mapping_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="unknown",
                gdc_description="",
                mapping_rank=1,
                reasoning="no valid gdc mappings"
            ))
        else:
            for mapping in gdc_mappings:
                mapping_results.append(MappingResult(
                    guid=record.guid,
                    code=record.code,
                    name=record.name,
                    description=record.description,
                    gdc_name=mapping.get("gdc_name", "unknown"),
                    gdc_description=mapping.get("gdc_description", ""),
                    mapping_rank=mapping.get("mapping_rank", 1),
                    reasoning=format_mapping_reasoning(mapping, overall_reasoning)
                ))
        
        return mapping_results
        
    except Exception as e:
        print(f"Error: {e}")
        return [MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="error",
            gdc_description="",
            mapping_rank=1,
            reasoning=f"error: {str(e)}"
        )]

def extract_final_decision(response_text: str) -> Dict[str, Any]:
    """Extract final decision from agent response"""
    try:
        json_str = extract_json_from_text(response_text)
        decision_data = json.loads(json_str)
        final_decision = FinalMappingDecision.model_validate(decision_data)
        return final_decision.model_dump()
    except:
        return {
            "gdc_mappings": [],
            "overall_reasoning": "unable to extract"
        }

def format_mapping_reasoning(mapping: Dict[str, Any], overall_reasoning: str) -> str:
    """Format reasoning for output"""
    parts = []
    
    reasoning = mapping.get("reasoning", "")
    if reasoning:
        parts.append(f"mapping reasoning:\n{reasoning}")
    
    evidence = mapping.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nevidence summary:\n" + "\n".join(f"• {e}" for e in evidence))
    
    if overall_reasoning and mapping.get("mapping_rank", 1) == 1:
        parts.append(f"\n\noverall context:\n{overall_reasoning}")
    
    return "\n".join(parts)

# ==================== MAIN EXECUTION ====================

def main():
    """Main execution function"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM")
    print("RAG with OpenAI Embeddings API (Direct) | InMemoryVectorStore")
    print("One-to-Many Mappings | LangGraph ReAct Agents")
    print("No tiktoken - Using OpenAI API directly for embeddings")
    print("=" * 80)
    
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY not set")
        return
    
    print("\n📁 Loading data...")
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded {len(gdc_master)} gdc master entries")
    print(f"✓ Loaded {len(gdc_context)} gdc context entries")
    print(f"✓ Loaded {len(validation_set)} validation entries")
    print(f"✓ Loaded {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load data")
        return
    
    print("\n🔍 Building RAG vector stores (using OpenAI API directly)...")
    gdc_master_vectorstore = build_gdc_master_vectorstore(gdc_master)
    gdc_context_vectorstore = build_gdc_context_vectorstore(gdc_context)
    
    print("\n🤖 Creating ReAct agent...")
    agent = create_react_agent_workflow()
    print(f"✓ Model: {OPENAI_MODEL}")
    print(f"✓ Reasoning: {REASONING_EFFORT}")
    print(f"✓ Embeddings: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)")
    print(f"✓ Embedding method: OpenAI API Direct (no tiktoken)")
    print(f"✓ Mode: one-to-many with rag")
    
    print("\n🚀 Starting mapping...\n")
    all_results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\nrecord {i}/{len(record_classes)}")
        
        try:
            record_mappings = process_single_record(
                agent=agent,
                record=record,
                validation_set=validation_set
            )
            
            all_results.extend(record_mappings)
            gdc_names = [m.gdc_name for m in record_mappings]
            print(f"✓ mapped to {len(record_mappings)} gdc(s): {', '.join(gdc_names)}")
            
        except Exception as e:
            print(f"✗ error: {e}")
            all_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="error",
                gdc_description="",
                mapping_rank=1,
                reasoning=f"error: {str(e)}"
            ))
    
    print("\n" + "="*80)
    print("💾 Saving...")
    results_dict = [r.model_dump(by_alias=True) for r in all_results]
    df = pd.DataFrame(results_dict)
    df.to_csv("GDC_Mapping_Results.csv", index=False, encoding='utf-8-sig')
    print("✓ Results saved to GDC_Mapping_Results.csv")
    
    print("\n📊 SUMMARY")
    print("="*80)
    print(f"records: {len(record_classes)}")
    print(f"mappings: {len(all_results)}")
    if len(record_classes) > 0:
        print(f"avg mappings/record: {len(all_results)/len(record_classes):.2f}")
    print("="*80)

if __name__ == "__main__":
    main()
