"""
Performance Monitoring System for API endpoints and LLM operations.

This module provides functionality to track and analyze performance metrics
for API calls, token acquisitions, and LLM operations.
"""

import time
import logging
import threading
import statistics
import traceback
import json
from typing import Dict, List, Optional, Any, Callable, TypeVar, Union
from functools import wraps
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# Type variable for generic function
F = TypeVar('F', bound=Callable[..., Any])

class PerformanceMetrics:
    """Thread-safe class to collect and analyze performance metrics."""
    
    _instance = None
    _lock = threading.RLock()
    
    def __new__(cls):
        """Implement singleton pattern."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(PerformanceMetrics, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize the performance metrics collection."""
        if self._initialized:
            return
            
        with self._lock:
            if not self._initialized:
                # Category -> operation -> list of durations (ms)
                self._metrics: Dict[str, Dict[str, List[float]]] = {}
                
                # Store recent failures
                self._failures: List[Dict[str, Any]] = []
                
                # Store timing data by timestamp for time-series analysis
                self._timeseries: Dict[str, List[Dict[str, Any]]] = {}
                
                # Timestamp of initialization
                self._start_time = datetime.now()
                
                # Maximum number of samples to keep
                self._max_samples = 1000
                
                # Maximum number of failures to keep
                self._max_failures = 100
                
                # Automatic cleanup interval (seconds)
                self._cleanup_interval = 3600  # 1 hour
                
                # Start cleanup thread
                self._cleanup_thread = threading.Thread(
                    target=self._cleanup_worker,
                    daemon=True,
                    name="PerformanceMetricsCleanup"
                )
                self._cleanup_thread.start()
                
                self._initialized = True
                logger.info("Performance metrics collection initialized")
    
    def record(self, category: str, operation: str, duration_ms: float, metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Record a performance metric.
        
        Args:
            category: Category of the operation (e.g., 'token', 'llm', 'api')
            operation: Name of the operation
            duration_ms: Duration in milliseconds
            metadata: Additional metadata for the operation
        """
        with self._lock:
            # Initialize category if needed
            if category not in self._metrics:
                self._metrics[category] = {}
            
            # Initialize operation if needed
            if operation not in self._metrics[category]:
                self._metrics[category][operation] = []
            
            # Append duration
            self._metrics[category][operation].append(duration_ms)
            
            # Limit list size to avoid memory issues
            if len(self._metrics[category][operation]) > self._max_samples:
                self._metrics[category][operation] = self._metrics[category][operation][-self._max_samples:]
            
            # Record in timeseries for time-based analysis
            timestamp = datetime.now().isoformat()
            if category not in self._timeseries:
                self._timeseries[category] = []
            
            # Record time series data
            self._timeseries[category].append({
                'timestamp': timestamp,
                'operation': operation,
                'duration_ms': duration_ms,
                'metadata': metadata or {}
            })
            
            # Limit timeseries size
            if len(self._timeseries[category]) > self._max_samples * 5:  # Allow more timeseries data
                self._timeseries[category] = self._timeseries[category][-self._max_samples * 5:]
    
    def record_failure(self, category: str, operation: str, error: Exception, metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Record an operation failure.
        
        Args:
            category: Category of the operation
            operation: Name of the operation
            error: The exception that occurred
            metadata: Additional metadata
        """
        with self._lock:
            # Record failure
            failure = {
                'timestamp': datetime.now().isoformat(),
                'category': category,
                'operation': operation,
                'error_type': type(error).__name__,
                'error_message': str(error),
                'traceback': traceback.format_exc(),
                'metadata': metadata or {}
            }
            
            self._failures.append(failure)
            
            # Limit failures list size
            if len(self._failures) > self._max_failures:
                self._failures = self._failures[-self._max_failures:]
    
    def get_stats(self, category: Optional[str] = None, operation: Optional[str] = None) -> Dict[str, Any]:
        """
        Get statistics for a category or operation.
        
        Args:
            category: Category to get stats for (all if None)
            operation: Operation to get stats for (all in category if None)
            
        Returns:
            Dictionary with statistics
        """
        with self._lock:
            if category and operation:
                # Get stats for specific operation
                if category in self._metrics and operation in self._metrics[category]:
                    durations = self._metrics[category][operation]
                    return self._calculate_stats(durations)
                return {}
            
            elif category:
                # Get stats for all operations in category
                if category in self._metrics:
                    result = {}
                    for op_name, durations in self._metrics[category].items():
                        result[op_name] = self._calculate_stats(durations)
                    return result
                return {}
            
            else:
                # Get stats for all categories
                result = {}
                for cat_name, operations in self._metrics.items():
                    cat_result = {}
                    for op_name, durations in operations.items():
                        cat_result[op_name] = self._calculate_stats(durations)
                    result[cat_name] = cat_result
                return result
    
    def get_failures(self, category: Optional[str] = None, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Get recent failures.
        
        Args:
            category: Category to filter by (optional)
            limit: Maximum number of failures to return
            
        Returns:
            List of failures
        """
        with self._lock:
            if category:
                # Filter by category
                filtered = [f for f in self._failures if f['category'] == category]
                return filtered[-limit:]
            else:
                # Return all failures (up to limit)
                return self._failures[-limit:]
    
    def get_timeseries(self, category: str, operation: Optional[str] = None, 
                      hours: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Get time series data.
        
        Args:
            category: Category to get data for
            operation: Operation to filter by (optional)
            hours: Number of hours to look back (optional)
            
        Returns:
            List of time series data points
        """
        with self._lock:
            if category not in self._timeseries:
                return []
            
            # Filter by category
            timeseries = self._timeseries[category]
            
            # Filter by operation if specified
            if operation:
                timeseries = [t for t in timeseries if t['operation'] == operation]
            
            # Filter by time if specified
            if hours:
                cutoff = (datetime.now() - timedelta(hours=hours)).isoformat()
                timeseries = [t for t in timeseries if t['timestamp'] >= cutoff]
            
            return timeseries
    
    def get_summary_report(self) -> Dict[str, Any]:
        """
        Get a summary report of all metrics.
        
        Returns:
            Dictionary with summary statistics
        """
        with self._lock:
            report = {
                'system': {
                    'uptime_seconds': (datetime.now() - self._start_time).total_seconds(),
                    'metrics_collected': sum(
                        len(durations) for operations in self._metrics.values() 
                        for durations in operations.values()
                    ),
                    'failures_recorded': len(self._failures),
                    'categories_tracked': list(self._metrics.keys())
                },
                'categories': {}
            }
            
            # Calculate aggregate stats for each category
            for category, operations in self._metrics.items():
                total_ops = sum(len(durations) for durations in operations.values())
                
                if total_ops == 0:
                    continue
                
                all_durations = []
                for durations in operations.values():
                    all_durations.extend(durations)
                
                # Only calculate percentiles if we have enough data
                p90 = None
                p95 = None
                p99 = None
                
                if len(all_durations) >= 10:
                    p90 = statistics.quantiles(all_durations, n=10)[8]
                if len(all_durations) >= 20:
                    p95 = statistics.quantiles(all_durations, n=20)[18]
                if len(all_durations) >= 100:
                    p99 = statistics.quantiles(all_durations, n=100)[98]
                
                report['categories'][category] = {
                    'operations_count': len(operations),
                    'total_calls': total_ops,
                    'min_duration_ms': min(all_durations) if all_durations else 0,
                    'max_duration_ms': max(all_durations) if all_durations else 0,
                    'avg_duration_ms': statistics.mean(all_durations) if all_durations else 0,
                    'p90_duration_ms': p90,
                    'p95_duration_ms': p95,
                    'p99_duration_ms': p99,
                    'failure_count': len([f for f in self._failures if f['category'] == category])
                }
            
            return report
    
    def reset(self) -> None:
        """Reset all metrics."""
        with self._lock:
            self._metrics = {}
            self._failures = []
            self._timeseries = {}
            self._start_time = datetime.now()
            logger.info("Performance metrics reset")
    
    def _calculate_stats(self, durations: List[float]) -> Dict[str, Any]:
        """
        Calculate statistics for a list of durations.
        
        Args:
            durations: List of duration values in milliseconds
            
        Returns:
            Dictionary with statistics
        """
        if not durations:
            return {
                'count': 0,
                'min_ms': None,
                'max_ms': None,
                'avg_ms': None,
                'median_ms': None,
                'p90_ms': None,
                'p95_ms': None,
                'p99_ms': None,
                'std_dev_ms': None
            }
        
        result = {
            'count': len(durations),
            'min_ms': min(durations),
            'max_ms': max(durations),
            'avg_ms': statistics.mean(durations)
        }
        
        # Calculate median and percentiles
        if len(durations) >= 1:
            result['median_ms'] = statistics.median(durations)
        
        if len(durations) >= 10:
            try:
                percentiles = statistics.quantiles(durations, n=100)
                result['p90_ms'] = percentiles[89]
                result['p95_ms'] = percentiles[94]
                result['p99_ms'] = percentiles[98]
            except Exception as e:
                logger.warning(f"Error calculating percentiles: {e}")
        
        # Calculate standard deviation
        if len(durations) >= 2:
            try:
                result['std_dev_ms'] = statistics.stdev(durations)
            except Exception as e:
                logger.warning(f"Error calculating standard deviation: {e}")
        
        return result
    
    def _cleanup_worker(self) -> None:
        """Background worker to perform periodic cleanup of metrics."""
        while True:
            try:
                # Sleep first, then cleanup
                time.sleep(self._cleanup_interval)
                
                # Remove old timeseries data (older than 24 hours)
                cutoff = (datetime.now() - timedelta(hours=24)).isoformat()
                
                with self._lock:
                    for category in self._timeseries:
                        self._timeseries[category] = [
                            t for t in self._timeseries[category]
                            if t['timestamp'] >= cutoff
                        ]
                
                logger.debug("Cleaned up old performance metrics data")
                
            except Exception as e:
                logger.error(f"Error in performance metrics cleanup: {e}")


# Create global instance
performance_metrics = PerformanceMetrics()

def timed(category: str, operation: Optional[str] = None) -> Callable[[F], F]:
    """
    Decorator to time a function and record its performance.
    
    Args:
        category: Category of the operation
        operation: Name of the operation (defaults to function name)
        
    Returns:
        Decorated function
    """
    def decorator(func: F) -> F:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            # Get operation name
            op_name = operation or func.__name__
            
            # Record start time
            start_time = time.time()
            
            try:
                # Call the function
                result = func(*args, **kwargs)
                
                # Calculate duration
                duration_ms = (time.time() - start_time) * 1000.0
                
                # Record metric
                performance_metrics.record(category, op_name, duration_ms)
                
                # Log for slow operations
                if duration_ms > 1000:  # Log operations taking more than 1 second
                    logger.warning(f"Slow operation: {category}.{op_name} took {duration_ms:.2f}ms")
                
                return result
                
            except Exception as e:
                # Calculate duration even for failures
                duration_ms = (time.time() - start_time) * 1000.0
                
                # Record metric with negative duration to indicate failure
                performance_metrics.record(category, op_name, -duration_ms)
                
                # Record failure
                performance_metrics.record_failure(category, op_name, e)
                
                # Re-raise the exception
                raise
        
        return wrapper  # type: ignore
    
    return decorator

def timed_async(category: str, operation: Optional[str] = None):
    """
    Decorator to time an async function and record its performance.
    
    Args:
        category: Category of the operation
        operation: Name of the operation (defaults to function name)
        
    Returns:
        Decorated async function
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Get operation name
            op_name = operation or func.__name__
            
            # Record start time
            start_time = time.time()
            
            try:
                # Call the function
                result = await func(*args, **kwargs)
                
                # Calculate duration
                duration_ms = (time.time() - start_time) * 1000.0
                
                # Record metric
                performance_metrics.record(category, op_name, duration_ms)
                
                # Log for slow operations
                if duration_ms > 1000:  # Log operations taking more than 1 second
                    logger.warning(f"Slow operation: {category}.{op_name} took {duration_ms:.2f}ms")
                
                return result
                
            except Exception as e:
                # Calculate duration even for failures
                duration_ms = (time.time() - start_time) * 1000.0
                
                # Record metric with negative duration to indicate failure
                performance_metrics.record(category, op_name, -duration_ms)
                
                # Record failure
                performance_metrics.record_failure(category, op_name, e)
                
                # Re-raise the exception
                raise
        
        return wrapper
    
    return decorator

# Context manager for timing code blocks
class TimedContext:
    """
    Context manager for timing code blocks.
    
    Example:
        with TimedContext('api', 'process_request') as timer:
            # Do some work
            result = process_data()
            timer.add_metadata('items_processed', len(result))
    """
    
    def __init__(self, category: str, operation: str):
        """
        Initialize the context manager.
        
        Args:
            category: Category of the operation
            operation: Name of the operation
        """
        self.category = category
        self.operation = operation
        self.start_time = 0.0
        self.metadata: Dict[str, Any] = {}
    
    def __enter__(self) -> 'TimedContext':
        """Start timing."""
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """End timing and record metric."""
        duration_ms = (time.time() - self.start_time) * 1000.0
        
        if exc_type is not None:
            # Record failure
            performance_metrics.record_failure(
                self.category, 
                self.operation, 
                exc_val or Exception("Unknown error"),
                self.metadata
            )
            
            # Record negative duration to indicate failure
            performance_metrics.record(
                self.category, 
                self.operation, 
                -duration_ms,
                self.metadata
            )
        else:
            # Record successful operation
            performance_metrics.record(
                self.category, 
                self.operation, 
                duration_ms,
                self.metadata
            )
    
    def add_metadata(self, key: str, value: Any) -> None:
        """
        Add metadata to the operation.
        
        Args:
            key: Metadata key
            value: Metadata value
        """
        self.metadata[key] = value

class AsyncTimedContext:
    """
    Async context manager for timing code blocks.
    
    Example:
        async with AsyncTimedContext('api', 'process_request') as timer:
            # Do some async work
            result = await process_data()
            timer.add_metadata('items_processed', len(result))
    """
    
    def __init__(self, category: str, operation: str):
        """
        Initialize the context manager.
        
        Args:
            category: Category of the operation
            operation: Name of the operation
        """
        self.category = category
        self.operation = operation
        self.start_time = 0.0
        self.metadata: Dict[str, Any] = {}
    
    async def __aenter__(self) -> 'AsyncTimedContext':
        """Start timing."""
        self.start_time = time.time()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """End timing and record metric."""
        duration_ms = (time.time() - self.start_time) * 1000.0
        
        if exc_type is not None:
            # Record failure
            performance_metrics.record_failure(
                self.category, 
                self.operation, 
                exc_val or Exception("Unknown error"),
                self.metadata
            )
            
            # Record negative duration to indicate failure
            performance_metrics.record(
                self.category, 
                self.operation, 
                -duration_ms,
                self.metadata
            )
        else:
            # Record successful operation
            performance_metrics.record(
                self.category, 
                self.operation, 
                duration_ms,
                self.metadata
            )
    
    def add_metadata(self, key: str, value: Any) -> None:
        """
        Add metadata to the operation.
        
        Args:
            key: Metadata key
            value: Metadata value
        """
        self.metadata[key] = value

def add_performance_endpoints(app):
    """
    Add performance metrics endpoints to a FastAPI app.
    
    Args:
        app: FastAPI application instance
    """
    @app.get("/api/v1/metrics", tags=["Metrics"])
    async def get_metrics_summary():
        """Get a summary of all performance metrics."""
        return performance_metrics.get_summary_report()
    
    @app.get("/api/v1/metrics/{category}", tags=["Metrics"])
    async def get_category_metrics(category: str):
        """Get metrics for a specific category."""
        return performance_metrics.get_stats(category)
    
    @app.get("/api/v1/metrics/{category}/{operation}", tags=["Metrics"])
    async def get_operation_metrics(category: str, operation: str):
        """Get metrics for a specific operation."""
        return performance_metrics.get_stats(category, operation)
    
    @app.get("/api/v1/metrics/failures", tags=["Metrics"])
    async def get_failures(category: Optional[str] = None, limit: int = 20):
        """Get recent operation failures."""
        return performance_metrics.get_failures(category, limit)
    
    @app.get("/api/v1/metrics/timeseries/{category}", tags=["Metrics"])
    async def get_timeseries(
        category: str, 
        operation: Optional[str] = None,
        hours: Optional[int] = None
    ):
        """Get time series data for metrics."""
        return performance_metrics.get_timeseries(category, operation, hours)
    
    @app.post("/api/v1/metrics/reset", tags=["Metrics"])
    async def reset_metrics():
        """Reset all metrics."""
        performance_metrics.reset()
        return {"status": "Metrics reset successfully"}
    
    logger.info("Performance metrics endpoints added to API")


# Utility functions for timing specific operations

def time_token_acquisition(operation: str = "azure_token"):
    """
    Time the acquisition of an Azure token.
    
    Args:
        operation: Specific token operation name
        
    Returns:
        Timing decorator
    """
    return timed("token", operation)

def time_llm_operation(operation: str = "llm_invoke"):
    """
    Time an LLM operation.
    
    Args:
        operation: Specific LLM operation name
        
    Returns:
        Timing decorator
    """
    return timed("llm", operation)

def time_embedding_operation(operation: str = "generate_embedding"):
    """
    Time an embedding operation.
    
    Args:
        operation: Specific embedding operation name
        
    Returns:
        Timing decorator
    """
    return timed("embedding", operation)

def time_api_endpoint(operation: str = "api_call"):
    """
    Time an API endpoint.
    
    Args:
        operation: Specific API operation name
        
    Returns:
        Timing decorator
    """
    return timed("api", operation)

async def time_async_llm_operation(operation: str = "llm_invoke_async"):
    """
    Time an async LLM operation.
    
    Args:
        operation: Specific LLM operation name
        
    Returns:
        Async timing decorator
    """
    return timed_async("llm", operation)

async def time_async_api_endpoint(operation: str = "api_call_async"):
    """
    Time an async API endpoint.
    
    Args:
        operation: Specific API operation name
        
    Returns:
        Async timing decorator
    """
    return timed_async("api", operation)

# Apply performance monitoring to token acquisition
def apply_token_monitoring():
    """
    Apply performance monitoring to token acquisition functions.
    This patches the auth_helper.py functions to add timing.
    """
    try:
        import utils.auth_helper as auth_helper
        
        # Save original functions
        original_get_token = auth_helper.get_azure_token_cached
        
        # Replace with timed versions
        @timed("token", "get_azure_token_cached")
        def timed_get_token(*args, **kwargs):
            return original_get_token(*args, **kwargs)
        
        # Patch the module
        auth_helper.get_azure_token_cached = timed_get_token
        
        logger.info("Token acquisition functions patched with performance monitoring")
    except ImportError:
        logger.warning("Could not apply token monitoring - auth_helper module not found")
    except Exception as e:
        logger.error(f"Error applying token monitoring: {e}")

# Apply performance monitoring to LLM operations
def apply_llm_monitoring():
    """
    Apply performance monitoring to LLM operations.
    This patches the settings.py functions to add timing.
    """
    try:
        from app.config.settings import get_llm as original_get_llm
        import app.config.settings
        
        # Create a timed version
        @timed("llm", "get_llm")
        def timed_get_llm(*args, **kwargs):
            return original_get_llm(*args, **kwargs)
        
        # Patch the module
        app.config.settings.get_llm = timed_get_llm
        
        logger.info("LLM functions patched with performance monitoring")
    except ImportError:
        logger.warning("Could not apply LLM monitoring - settings module not found")
    except Exception as e:
        logger.error(f"Error applying LLM monitoring: {e}")

# Initialize performance monitoring
def initialize_performance_monitoring(app = None):
    """
    Initialize the performance monitoring system and optionally add endpoints.
    
    Args:
        app: FastAPI application instance (optional)
    """
    # Ensure metrics singleton is initialized
    _ = PerformanceMetrics()
    
    # Apply monitoring to token and LLM operations
    apply_token_monitoring()
    apply_llm_monitoring()
    
    # Add endpoints if app is provided
    if app is not None:
        add_performance_endpoints(app)
    
    logger.info("Performance monitoring system initialized")
