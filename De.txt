import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union, Tuple, Callable, Sequence, Annotated, TypedDict, cast
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel, Field, field_validator  # Updated for Pydantic v2
from langchain_openai import AzureChatOpenAI  # Updated import path
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, ToolMessage, AIMessage
from langchain_core.tools import tool, BaseTool
from langchain_core.documents import Document  # Updated import path
from langchain_core.runnables import RunnableConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
import json
from langgraph.graph import StateGraph, END  # For LangGraph
from langgraph.graph.message import add_messages  # For LangGraph message handling
from langgraph.prebuilt import create_react_agent  # For creating the React agent
from langgraph.checkpoint.memory import MemorySaver  # For persistent memory
from sklearn.metrics.pairwise import cosine_similarity

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## Helper Functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

def serialize_message(message):
    """Serialize a LangChain message object to a JSON-serializable dict."""
    if isinstance(message, BaseMessage):
        return {
            "type": message.type,
            "content": message.content,
            "additional_kwargs": message.additional_kwargs
        }
    elif isinstance(message, list):
        return [serialize_message(m) for m in message]
    elif isinstance(message, dict):
        return {k: serialize_message(v) for k, v in message.items()}
    return message

## Agent State for LangGraph
class AgentState(TypedDict):
    """The state of the agent."""
    messages: List[BaseMessage]
    context: Dict[str, Any]

## OSEnv class
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        self.credential = self._get_credential()
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(tenant_id=self.get("AZURE_TENANT_ID"), client_id=self.get("AZURE_CLIENT_ID"), client_secret=self.get("AZURE_CLIENT_SECRET"))
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                raise FileNotFoundError(f"The file '{dotenvfile}' does not exist or is not readable")
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## embedding class + Document class
class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class EmbeddingClient:
    def __init__(self, env: OSEnv, azure_api_version: str = "2024-05-01-preview", embeddings_model: str = "text-embedding-3-large"):
        self.env = env
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
        # Deployment name may be different from model name
        self.deployment_name = self.env.get("EMBEDDING_DEPLOYMENT_NAME", embeddings_model)
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(
            azure_endpoint=self.env.get("AZURE_ENDPOINT", ""),
            api_version=self.azure_api_version,
            azure_ad_token_provider=token_provider
        )
    
    def generate_embeddings(self, doc: MyDocument)->MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.deployment_name,  # Use deployment name instead of model name
                input=doc.text,
                encoding_format="float"  # Explicitly specify the format
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc
    
    def batch_generate_embeddings(self, docs: List[MyDocument]) -> List[MyDocument]:
        """Generate embeddings for a batch of documents."""
        try:
            # Process in batches of 16 to avoid limits
            batch_size = 16
            batched_docs = []
            
            for i in range(0, len(docs), batch_size):
                batch = docs[i:i + batch_size]
                texts = [doc.text for doc in batch]
                
                response = self.direct_azure_client.embeddings.create(
                    model=self.deployment_name,  # Use deployment name instead of model name
                    input=texts,
                    encoding_format="float"  # Explicitly specify the format
                )
                
                for j, embedding_data in enumerate(response.data):
                    batch[j].embedding = embedding_data.embedding
                
                batched_docs.extend(batch)
                logger.info(f"Generated embeddings for batch {i//batch_size + 1} of {(len(docs) - 1)//batch_size + 1}")
            
            return batched_docs
        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            raise

## AzureChatbot components
class AzureChatbot:
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
    
    def _setup_chat_model(self):
        try:
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            model_name = self.env.get("MODEL_NAME", "gpt-4o")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2024-05-01-preview")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider
            )
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise
    
    def classify_with_llm(self, user_input: str, pbt_options: List[Dict]) -> Dict:
        """Use the LLM to classify the user input against the PBT options."""
        try:
            # Create a prompt for the classification
            prompt_template = """
            You are an expert in data classification. You need to map a user input to the most relevant category in a database.

            User Input: {user_input}

            Available Categories:
            {pbt_options}

            Please analyze the user input and determine which category is the most contextually relevant match.
            Return only the ID of the best matching category. Do not include any explanations or additional text.
            """
            
            # Format the PBT options
            formatted_options = "\n".join([
                f"ID: {option['id']}, Name: {option['name']}, Definition: {option['definition']}" 
                for option in pbt_options
            ])
            
            # Create a prompt message
            prompt = prompt_template.format(
                user_input=user_input,
                pbt_options=formatted_options
            )
            
            # Get the classification from the LLM
            response = self.llm.invoke(prompt)
            
            # Extract the ID from the response
            result_id = response.content.strip()
            
            # Find the matching option
            for option in pbt_options:
                if str(option['id']) == result_id:
                    return option
            
            # If no exact match is found
            logger.warning(f"No exact match found for ID: {result_id}. Returning best guess.")
            return pbt_options[0]  # Return the first option as a fallback
            
        except Exception as e:
            logger.error(f"Error in LLM classification: {e}")
            return None

## PBT Data Manager Class
class PBTDataManager:
    def __init__(self, embedding_client: EmbeddingClient):
        self.embedding_client = embedding_client
        self.pbt_data = []
        self.pbt_embeddings = []
        self.concept_hierarchy = {}  # Maps specific terms to their broader categories
    
    def load_csv(self, csv_path: str) -> None:
        """Load PBT data from CSV file."""
        try:
            df = pd.read_csv(csv_path)
            if not all(col in df.columns for col in ['id', 'PBT_NAME', 'PBT_DEFINITION']):
                raise ValueError("CSV file must contain columns: id, PBT_NAME, PBT_DEFINITION")
            
            # Convert DataFrame to list of dictionaries
            self.pbt_data = df.to_dict('records')
            
            # Create documents for embedding
            docs = []
            for item in self.pbt_data:
                combined_text = f"{item['PBT_NAME']} - {item['PBT_DEFINITION']}"
                doc = MyDocument(
                    id=str(item['id']),
                    text=combined_text,
                    metadata={
                        'name': item['PBT_NAME'],
                        'definition': item['PBT_DEFINITION']
                    }
                )
                docs.append(doc)
            
            # Generate embeddings for all PBT items
            embedded_docs = self.embedding_client.batch_generate_embeddings(docs)
            
            # Store the embeddings
            self.pbt_embeddings = [doc.embedding for doc in embedded_docs]
            
            # Build concept hierarchy using embeddings-based clustering
            self._build_concept_hierarchy()
            
            logger.info(f"Loaded {len(self.pbt_data)} PBT records from CSV")
        except Exception as e:
            logger.error(f"Error loading CSV: {e}")
            raise
    
    def _build_concept_hierarchy(self):
        """Build a concept hierarchy based on the similarity between terms."""
        try:
            # Calculate similarity matrix between all terms
            similarity_matrix = np.zeros((len(self.pbt_embeddings), len(self.pbt_embeddings)))
            
            for i in range(len(self.pbt_embeddings)):
                for j in range(len(self.pbt_embeddings)):
                    if i != j:  # Skip self-comparison
                        similarity_matrix[i, j] = cosine_similarity(
                            [self.pbt_embeddings[i]], 
                            [self.pbt_embeddings[j]]
                        )[0][0]
            
            # Identify broader terms (terms that are similar to many other terms)
            avg_similarities = np.mean(similarity_matrix, axis=1)
            
            # Calculate term specificity (lower means more specific)
            term_generality = {}
            term_lengths = []
            
            for i, item in enumerate(self.pbt_data):
                # Calculate term specificity based on:
                # 1. Average similarity to other terms
                # 2. Term name length (shorter terms tend to be more general)
                # 3. Definition complexity
                name_length = len(item['PBT_NAME'].split())
                term_lengths.append(name_length)
                
                term_generality[str(item['id'])] = {
                    'avg_similarity': float(avg_similarities[i]),
                    'name_length': name_length,
                    'id': item['id'],
                    'name': item['PBT_NAME']
                }
            
            # Normalize term lengths
            max_length = max(term_lengths) if term_lengths else 1
            for term_id, data in term_generality.items():
                # Score where higher means more general
                data['generality_score'] = data['avg_similarity'] * (1 - (data['name_length'] / max_length))
            
            # Sort terms by generality score (higher is more general)
            sorted_terms = sorted(
                term_generality.items(), 
                key=lambda x: x[1]['generality_score'], 
                reverse=True
            )
            
            # Take the top 20% as broader terms
            broader_terms_count = max(1, int(len(sorted_terms) * 0.2))
            broader_terms = [term[0] for term in sorted_terms[:broader_terms_count]]
            
            # Store concept hierarchy
            self.concept_hierarchy = {
                'broader_terms': [self.pbt_data[i] for i in range(len(self.pbt_data)) 
                                if str(self.pbt_data[i]['id']) in broader_terms],
                'term_generality': term_generality
            }
            
            logger.info(f"Built concept hierarchy with {len(broader_terms)} broader terms")
        except Exception as e:
            logger.error(f"Error building concept hierarchy: {e}")
            self.concept_hierarchy = {'broader_terms': [], 'term_generality': {}}
    
    def find_similar_items(self, query_text: str, top_n: int = 5, include_broader_terms: bool = True) -> List[Dict]:
        """Find the most similar PBT items to the query text, including broader terms."""
        try:
            # Create a document for the query
            query_doc = MyDocument(
                id="query",
                text=query_text
            )
            
            # Generate embedding for the query
            query_doc = self.embedding_client.generate_embeddings(query_doc)
            
            # Calculate similarity scores for all terms
            similarity_scores = []
            for i, embedding in enumerate(self.pbt_embeddings):
                # Calculate cosine similarity
                similarity = cosine_similarity(
                    [query_doc.embedding], 
                    [embedding]
                )[0][0]
                
                similarity_scores.append((i, similarity))
            
            # Sort by similarity (descending)
            similarity_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Get the top N results
            specific_results = []
            for idx, score in similarity_scores[:top_n]:
                result = self.pbt_data[idx].copy()
                result['similarity_score'] = float(score)  # Convert numpy.float to Python float
                result['match_type'] = 'specific'
                specific_results.append(result)
            
            final_results = specific_results
            
            # Include broader terms if requested
            if include_broader_terms and self.concept_hierarchy.get('broader_terms'):
                # Get broader terms that might be applicable
                broader_matches = []
                
                # Calculate similarity to broader terms
                for broader_term in self.concept_hierarchy.get('broader_terms', []):
                    # Skip terms already in the specific results
                    if any(r['id'] == broader_term['id'] for r in specific_results):
                        continue
                    
                    # Find the broader term's embedding
                    broader_idx = next((i for i, item in enumerate(self.pbt_data) 
                                      if item['id'] == broader_term['id']), None)
                    
                    if broader_idx is not None:
                        similarity = cosine_similarity(
                            [query_doc.embedding], 
                            [self.pbt_embeddings[broader_idx]]
                        )[0][0]
                        
                        # Only include broader terms with reasonable similarity
                        if similarity > 0.5:  # Threshold can be adjusted
                            result = broader_term.copy()
                            result['similarity_score'] = float(similarity)
                            result['match_type'] = 'broader'
                            broader_matches.append(result)
                
                # Sort broader matches by similarity
                broader_matches.sort(key=lambda x: x['similarity_score'], reverse=True)
                
                # Take up to 3 broader matches
                broader_matches = broader_matches[:3]
                
                # Add broader matches to results
                final_results = specific_results + broader_matches
            
            return final_results
        except Exception as e:
            logger.error(f"Error finding similar items: {e}")
            return []

## LangGraph React Agent Tool
class PBTClassifierTool(BaseTool):
    name: str = Field("pbt_classifier")
    description: str = Field("Classifies user input against PBT (Preferred Business Terms) database entries")
    pbt_manager: PBTDataManager = Field(exclude=True)
    
    def __init__(self, pbt_manager: PBTDataManager):
        # Initialize with proper field values
        super().__init__(name="pbt_classifier", 
                        description="Classifies user input against PBT (Preferred Business Terms) database entries. Include parameter 'return_broader_terms=True' to get both specific and broader business terms.",
                        pbt_manager=pbt_manager)
        
    def _run(self, name: str, description: str, return_broader_terms: bool = True) -> dict:
        """Classify the input against the PBT data."""
        combined_input = f"{name} - {description}"
        similar_items = self.pbt_manager.find_similar_items(
            combined_input, 
            top_n=5, 
            include_broader_terms=return_broader_terms
        )
        
        if not similar_items:
            return {"status": "error", "message": "No similar items found"}
        
        # Group results by match type
        specific_matches = [item for item in similar_items if item.get('match_type') == 'specific']
        broader_matches = [item for item in similar_items if item.get('match_type') == 'broader']
        
        # If no specific grouping, treat all as specific
        if not specific_matches and not broader_matches:
            specific_matches = similar_items
        
        return {
            "status": "success",
            "best_match": similar_items[0] if similar_items else None,
            "specific_matches": specific_matches,
            "broader_matches": broader_matches,
            "similar_items": similar_items
        }

## LangGraph React Agent Class
class ReactAgent:
    def __init__(self, env: OSEnv, pbt_manager: PBTDataManager):
        self.env = env
        self.pbt_manager = pbt_manager
        self.memory_saver = MemorySaver()
        self.model = self._setup_llm()
        self.tools = self._setup_tools()
        self.agent = self._create_agent()
    
    def _setup_llm(self):
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        model_name = self.env.get("MODEL_NAME", "gpt-4o")
        temperature = float(self.env.get("TEMPERATURE", "0.3"))
        api_version = self.env.get("API_VERSION", "2024-05-01-preview")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        return AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token_provider=token_provider
        )
    
    def _setup_tools(self):
        return [PBTClassifierTool(pbt_manager=self.pbt_manager)]
    
    def _create_agent(self):
        # Define custom system message
        system_message = """
        You are an expert business terminology standardization system. Your task is to map user-provided terms 
        and descriptions to the organization's Preferred Business Terms (PBT).

        When a user provides a name and description, use the pbt_classifier tool to find the most appropriate 
        standard business terms from the database. 
        
        The tool will return both specific matches and broader category matches. Consider both types when making 
        your recommendation. For example, "drawdown client account number" might match to both specific terms 
        like "Account Number" and broader categories like "Account Identifier" or "Customer Account".
        
        Explain why the matches are appropriate, focusing on conceptual alignment rather than just keyword matching.
        Always mention both specific and broader matches in your response when available.
        """
        
        # Create the React agent
        return create_react_agent(
            self.model,
            self.tools,
            prompt=system_message,
            checkpointer=self.memory_saver
        )
    
    def classify(self, name: str, description: str) -> Dict:
        """Use the agent to classify the input."""
        try:
            # Generate a unique ID for this classification session
            session_id = str(uuid.uuid4())
            
            input_message = f"""
            Please map the following to standard Preferred Business Terms, including both specific and broader category matches:
            
            Name: {name}
            Description: {description}
            
            Provide both specific matches and broader category matches when available.
            """
            
            # Create a config with thread_id for the checkpointer
            config = {"configurable": {"thread_id": session_id}}
            
            result = self.agent.invoke(
                {"messages": [HumanMessage(content=input_message)]},
                config=config
            )
            
            # The last message contains the final response
            final_message = result.get("messages", [])[-1] if result.get("messages") else None
            final_content = ""
            
            if hasattr(final_message, 'content'):
                final_content = final_message.content
            elif isinstance(final_message, dict) and 'content' in final_message:
                final_content = final_message['content']
            elif final_message is not None:
                final_content = str(final_message)
            
            # Manually serialize messages to avoid JSON issues
            serialized_result = {"messages": []}
            
            # Process each message
            for msg in result.get("messages", []):
                serialized_msg = {}
                
                # Handle different message types and formats
                if hasattr(msg, 'content'):
                    serialized_msg["content"] = msg.content
                    serialized_msg["type"] = getattr(msg, "type", "unknown")
                    
                    # Handle tool calls
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        serialized_msg["tool_calls"] = []
                        for tool_call in msg.tool_calls:
                            if isinstance(tool_call, dict):
                                serialized_msg["tool_calls"].append(tool_call)
                    
                    # Handle additional kwargs
                    if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
                        serialized_msg["additional_kwargs"] = msg.additional_kwargs
                        
                    # Handle name (for tool messages)
                    if hasattr(msg, 'name'):
                        serialized_msg["name"] = msg.name
                
                elif isinstance(msg, dict):
                    # If message is already a dict, just copy the relevant fields
                    for key in ['content', 'type', 'tool_calls', 'additional_kwargs', 'name']:
                        if key in msg:
                            serialized_msg[key] = msg[key]
                
                # Only add non-empty messages
                if serialized_msg:
                    serialized_result["messages"].append(serialized_msg)
            
            return {
                "status": "success",
                "agent_response": final_content,
                "full_trace": serialized_result
            }
        except Exception as e:
            logger.error(f"Error in agent classification: {e}")
            return {"status": "error", "message": str(e)}

## Confidence Evaluator Agent Class
class ConfidenceEvaluatorAgent:
    def __init__(self, env: OSEnv):
        self.env = env
        self.model = self._setup_llm()
        self.chain = self._create_confidence_chain()
    
    def _setup_llm(self):
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        model_name = self.env.get("MODEL_NAME", "gpt-4o")
        temperature = float(self.env.get("TEMPERATURE", "0.2"))  # Lower temperature for more deterministic confidence scores
        api_version = self.env.get("API_VERSION", "2024-05-01-preview")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        return AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token_provider=token_provider
        )
    
    def _create_confidence_chain(self):
        # Define prompt for confidence evaluation
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
            You are an expert system that evaluates the confidence of matches between user-provided terms and standard
            Preferred Business Terms (PBT). Analyze the semantic similarity, contextual relevance, and overall 
            appropriateness of the match.
            
            Provide a confidence score between 0 and 100, where:
            - 0-20: Very low confidence. The match seems arbitrary or incorrect.
            - 21-40: Low confidence. There's a vague relationship but likely not the best match.
            - 41-60: Moderate confidence. There's a reasonable connection but potentially better alternatives.
            - 61-80: High confidence. The match is strong and likely appropriate.
            - 81-100: Very high confidence. The match is excellent and almost certainly correct.
            
            Return your evaluation as a JSON object with the following fields:
            - confidence_score: (number between 0-100)
            - explanation: (string explaining your reasoning)
            """),
            HumanMessage(content="""
            User Input: {user_input}
            
            Matched PBT: 
            ID: {pbt_id}
            Name: {pbt_name}
            Definition: {pbt_definition}
            
            Evaluate the confidence of this match.
            """)
        ])
        
        # Create the chain
        return prompt | self.model
    
    def evaluate_confidence(self, user_input: str, pbt_match: Dict) -> Dict:
        """Evaluate the confidence of a match between user input and a PBT."""
        try:
            # Call the LLM
            llm_response = self.chain.invoke({
                "user_input": user_input,
                "pbt_id": pbt_match.get("id", ""),
                "pbt_name": pbt_match.get("PBT_NAME", ""),
                "pbt_definition": pbt_match.get("PBT_DEFINITION", "")
            })
            
            # Extract JSON data from the response
            content = llm_response.content
            
            # Handle possible JSON formatting issues
            try:
                # Try direct JSON parsing first
                result = json.loads(content)
            except json.JSONDecodeError:
                # If that fails, try to extract JSON using regex
                import re
                json_pattern = r'\{.*\}'
                match = re.search(json_pattern, content, re.DOTALL)
                
                if match:
                    try:
                        result = json.loads(match.group(0))
                    except json.JSONDecodeError:
                        # If regex extraction fails, create a default result with explanation
                        result = {
                            "confidence_score": 50,
                            "explanation": "Could not parse confidence score from LLM output. Using default medium confidence. Original response: " + content[:100] + "..."
                        }
                else:
                    # If no JSON-like structure found, create a default result with explanation
                    result = {
                        "confidence_score": 50,
                        "explanation": "Could not parse confidence score from LLM output. Using default medium confidence. Original response: " + content[:100] + "..."
                    }
            
            # Ensure the result has the expected fields
            if "confidence_score" not in result:
                result["confidence_score"] = 50
            if "explanation" not in result:
                result["explanation"] = "No explanation provided"
            
            # Convert confidence score to integer if it's a string
            if isinstance(result["confidence_score"], str):
                try:
                    result["confidence_score"] = int(result["confidence_score"])
                except ValueError:
                    result["confidence_score"] = 50
            
            return result
            
        except Exception as e:
            logger.error(f"Error in confidence evaluation: {e}")
            return {
                "confidence_score": 50,
                "explanation": f"Error evaluating confidence: {str(e)}"
            }

## Data Classifier Class
class DataClassifier:
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.embedding_client = EmbeddingClient(self.env)
        self.pbt_manager = PBTDataManager(self.embedding_client)
        self.chatbot = AzureChatbot(config_file, creds_file, cert_file)
        self.react_agent = None  # Will be initialized after loading data
        self.confidence_agent = ConfidenceEvaluatorAgent(self.env)
    
    def load_data(self, csv_path: str) -> None:
        """Load the PBT data from CSV."""
        self.pbt_manager.load_csv(csv_path)
        # Initialize the React agent after data is loaded
        self.react_agent = ReactAgent(self.env, self.pbt_manager)
    
    def classify_with_embeddings(self, name: str, description: str, top_n: int = 5) -> Dict:
        """Classify using embedding similarity."""
        combined_input = f"{name} - {description}"
        similar_items = self.pbt_manager.find_similar_items(combined_input, top_n=top_n)
        
        if not similar_items:
            logger.warning("No similar items found")
            return {"status": "error", "message": "No similar items found"}
        
        # Evaluate confidence
        confidence_result = self.confidence_agent.evaluate_confidence(
            combined_input, 
            similar_items[0]
        )
        
        return {
            "status": "success",
            "best_match": similar_items[0],
            "similar_items": similar_items,
            "confidence": confidence_result
        }
    
    def classify_with_llm(self, name: str, description: str, top_n: int = 5) -> Dict:
        """Classify using LLM."""
        # First get similar items with embeddings
        combined_input = f"{name} - {description}"
        similar_items = self.pbt_manager.find_similar_items(combined_input, top_n=top_n)
        
        if not similar_items:
            logger.warning("No similar items found")
            return {"status": "error", "message": "No similar items found"}
        
        # Use LLM for final classification among top candidates
        best_match = self.chatbot.classify_with_llm(combined_input, [
            {
                'id': item['id'],
                'name': item['PBT_NAME'],
                'definition': item['PBT_DEFINITION']
            } for item in similar_items
        ])
        
        if best_match:
            # Evaluate confidence
            confidence_result = self.confidence_agent.evaluate_confidence(
                combined_input, 
                best_match
            )
            
            return {
                "status": "success",
                "best_match": best_match,
                "similar_items": similar_items,
                "confidence": confidence_result
            }
        
        # Fallback to embedding similarity
        confidence_result = self.confidence_agent.evaluate_confidence(
            combined_input, 
            similar_items[0]
        )
        
        return {
            "status": "success",
            "best_match": similar_items[0],
            "similar_items": similar_items,
            "confidence": confidence_result
        }
    
    def classify_with_agent(self, name: str, description: str) -> Dict:
        """Classify using the LangGraph React agent."""
        if not self.react_agent:
            logger.error("React agent not initialized. Call load_data() first.")
            return {"status": "error", "message": "React agent not initialized"}
        
        agent_result = self.react_agent.classify(name, description)
        
        if agent_result.get("status") == "error":
            return agent_result
        
        # Extract the best match from the agent's result
        # This assumes the agent's tool returns a result with similar format to other methods
        # We'll get this from the "full_trace" which contains all messages including tool outputs
        full_trace = agent_result.get("full_trace", {})
        messages = full_trace.get("messages", [])
        
        # Find the tool message with the classification result
        best_match = None
        specific_matches = []
        broader_matches = []
        
        for message in messages:
            if message.get("type") == "tool" and message.get("name", message.get("additional_kwargs", {}).get("name")) == "pbt_classifier":
                try:
                    # Try to parse the content
                    if isinstance(message.get("content"), str):
                        tool_content = json.loads(message.get("content", "{}"))
                        
                        if tool_content.get("status") == "success":
                            best_match = tool_content.get("best_match")
                            specific_matches = tool_content.get("specific_matches", [])
                            broader_matches = tool_content.get("broader_matches", [])
                            break
                except json.JSONDecodeError:
                    logger.warning("Failed to parse tool content JSON")
                except Exception as e:
                    logger.warning(f"Error processing tool message: {e}")
        
        # If we couldn't find a best match in the tool results, fall back to embedding similarity
        if best_match is None:
            logger.warning("Could not extract best match from agent result, falling back to embedding similarity")
            combined_input = f"{name} - {description}"
            similar_items = self.pbt_manager.find_similar_items(combined_input, top_n=5, include_broader_terms=True)
            
            if not similar_items:
                return {"status": "error", "message": "No similar items found"}
            
            best_match = similar_items[0]
            specific_matches = [item for item in similar_items if item.get('match_type') == 'specific']
            broader_matches = [item for item in similar_items if item.get('match_type') == 'broader']
            
            # If no match types specified, treat all as specific
            if not specific_matches and not broader_matches:
                specific_matches = similar_items
        
        # Evaluate confidence for the best match
        combined_input = f"{name} - {description}"
        confidence_result = self.confidence_agent.evaluate_confidence(
            combined_input, 
            best_match
        )
        
        # Compile final result
        result = {
            "status": "success",
            "agent_response": agent_result.get("agent_response", ""),
            "best_match": best_match,
            "specific_matches": specific_matches,
            "broader_matches": broader_matches,
            "confidence": confidence_result
        }
        
        return result
    
    def classify(self, name: str, description: str, method: str = "agent") -> Dict:
        """Classify using the specified method."""
        if method == "embeddings":
            return self.classify_with_embeddings(name, description)
        elif method == "llm":
            return self.classify_with_llm(name, description)
        elif method == "agent":
            return self.classify_with_agent(name, description)
        else:
            logger.error(f"Unknown classification method: {method}")
            return {"status": "error", "message": f"Unknown classification method: {method}"}

## Main Application Class
class AIClassificationApp:
    def __init__(self, csv_path: str, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        self.classifier = DataClassifier(config_file, creds_file, cert_file)
        self.classifier.load_data(csv_path)
        logger.info("AI Classification App initialized")
    
    def classify_input(self, name: str, description: str, method: str = "agent") -> Dict:
        """Classify user input against PBT data using the specified method."""
        result = self.classifier.classify(name, description, method)
        
        # Ensure result is JSON serializable (no Message objects)
        if isinstance(result, dict) and "full_trace" in result and isinstance(result["full_trace"], dict):
            # Already serialized
            pass
        else:
            # Attempt to serialize any message objects
            result = json.loads(json.dumps(result, default=lambda o: o.__dict__ if hasattr(o, "__dict__") else str(o)))
        
        return result
    
    def batch_classify(self, inputs: List[Dict], method: str = "agent") -> List[Dict]:
        """Batch classify multiple inputs."""
        results = []
        for input_item in inputs:
            result = self.classify_input(
                input_item.get('name', ''),
                input_item.get('description', ''),
                method
            )
            results.append({
                "input": input_item,
                "classification": result
            })
        return results

# Example usage
def main():
    # Example configuration
    csv_path = "path/to/your/pbt_data.csv"
    
    # Initialize the app
    app = AIClassificationApp(csv_path)
    
    # Example classification using React agent
    result = app.classify_input(
        "Machine Learning Algorithm", 
        "A computational method that uses statistical techniques to enable machines to improve through experience"
    )
    
    print(json.dumps(result, indent=2))
    
    # Example batch classification
    batch_results = app.batch_classify([
        {
            "name": "Neural Network",
            "description": "A computing system inspired by biological neural networks that can learn from examples"
        },
        {
            "name": "Decision Tree",
            "description": "A tree-like model of decisions where each internal node represents a test on an attribute"
        }
    ])
    
    print(json.dumps(batch_results, indent=2))

if __name__ == "__main__":
    main()
