#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System
Features:
- ReAct agents with explicit reasoning and memory
- LangMem integration for long-term memory
- Rich ontology with comprehensive definitions
- Enhanced prompts ensuring complete coverage
- Web interface for querying the knowledge graph
- SPARQL endpoint for semantic queries

Author: Claude (Anthropic)
Date: July 2025
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import uuid

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import ssl

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    import threading
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Flask not available - web interface will be disabled")
    # Create dummy classes to prevent import errors
    class Flask: pass
    class Response: pass
    def jsonify(*args, **kwargs): return {}
    def render_template_string(*args, **kwargs): return ""
    def CORS(*args, **kwargs): pass
    import threading

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
from rdflib.plugins.stores.sparqlstore import SPARQLStore
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent, ToolNode
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langmem import create_manage_memory_tool, create_search_memory_tool, create_memory_manager

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# Tiktoken for token counting (offline)
import tiktoken
from tiktoken.load import load_tiktoken_bpe

# Message types for ReAct agents
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# TIKTOKEN OFFLINE MANAGER
# ====================================

class OfflineTiktokenManager:
    """Manage tiktoken encodings with offline model files"""
    
    def __init__(self, models_dir: str = "./tiktoken_models"):
        self.models_dir = Path(models_dir)
        self.encodings = {}
        self._load_offline_encodings()
    
    def _load_offline_encodings(self):
        """Load all available offline tiktoken encodings"""
        
        model_files = {
            "cl100k_base": "cl100k_base.tiktoken",
            "o200k_base": "o200k_base.tiktoken", 
            "p50k_base": "p50k_base.tiktoken",
            "r50k_base": "r50k_base.tiktoken"
        }
        
        for encoding_name, filename in model_files.items():
            file_path = self.models_dir / filename
            if file_path.exists():
                try:
                    # Load BPE ranks from file
                    bpe = load_tiktoken_bpe(str(file_path))
                    
                    # Create encoding based on known parameters
                    if encoding_name == "cl100k_base":
                        special_tokens = {
                            "<|endoftext|>": 100257,
                            "<|fim_prefix|>": 100258,
                            "<|fim_middle|>": 100259,
                            "<|fim_suffix|>": 100260,
                            "<|endofprompt|>": 100276
                        }
                        pat_str = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
                    elif encoding_name == "o200k_base":
                        special_tokens = {
                            "<|endoftext|>": 199999,
                            "<|endofprompt|>": 200018
                        }
                        pat_str = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
                    else:
                        special_tokens = {"<|endoftext|>": 50256}
                        pat_str = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
                    
                    encoding = tiktoken.Encoding(
                        name=encoding_name,
                        pat_str=pat_str,
                        mergeable_ranks=bpe,
                        special_tokens=special_tokens
                    )
                    
                    self.encodings[encoding_name] = encoding
                    logger.info(f"Loaded offline tiktoken encoding: {encoding_name}")
                    
                except Exception as e:
                    logger.error(f"Failed to load {encoding_name}: {e}")
            else:
                logger.warning(f"Tiktoken model file not found: {file_path}")
    
    def get_encoding(self, model_name: str = "o3-mini-2025-01-31") -> tiktoken.Encoding:
        """Get appropriate encoding for a model"""
        
        # Map models to encodings
        model_to_encoding = {
            "o3-mini-2025-01-31": "o200k_base",  # o3-mini likely uses o200k_base
            "gpt-4": "cl100k_base",
            "gpt-3.5-turbo": "cl100k_base",
            "text-embedding-3-large": "cl100k_base",
            "text-embedding-ada-002": "cl100k_base"
        }
        
        encoding_name = model_to_encoding.get(model_name, "cl100k_base")
        
        if encoding_name in self.encodings:
            return self.encodings[encoding_name]
        else:
            # Fallback to cl100k_base if available
            if "cl100k_base" in self.encodings:
                logger.warning(f"Using cl100k_base fallback for {model_name}")
                return self.encodings["cl100k_base"]
            else:
                raise ValueError(f"No suitable encoding available for {model_name}")
    
    def count_tokens(self, text: str, model_name: str = "o3-mini-2025-01-31") -> int:
        """Count tokens in text for given model"""
        encoding = self.get_encoding(model_name)
        return len(encoding.encode(text))
    
    def truncate_text(self, text: str, max_tokens: int, model_name: str = "o3-mini-2025-01-31") -> str:
        """Truncate text to fit within token limit"""
        encoding = self.get_encoding(model_name)
        tokens = encoding.encode(text)
        
        if len(tokens) <= max_tokens:
            return text
        
        # Truncate tokens and decode back to text
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens)
    
    def count_messages_tokens(self, messages: List[Dict], model_name: str = "o3-mini-2025-01-31") -> int:
        """Count tokens in a list of messages (OpenAI format)"""
        encoding = self.get_encoding(model_name)
        
        total_tokens = 0
        
        for message in messages:
            # Each message has some overhead tokens
            total_tokens += 4  # Basic message overhead
            
            for key, value in message.items():
                if isinstance(value, str):
                    total_tokens += len(encoding.encode(value))
                    if key == "role":
                        total_tokens += 1  # Role has extra token
        
        total_tokens += 2  # Conversation overhead
        return total_tokens

# ====================================
# TOKEN-AWARE PROMPT MANAGER
# ====================================

class TokenAwarePromptManager:
    """Manage prompts with token counting and truncation"""
    
    def __init__(self, tiktoken_manager: OfflineTiktokenManager):
        self.tiktoken_manager = tiktoken_manager
        self.model_limits = {
            "o3-mini-2025-01-31": 8192,
            "gpt-4": 8192,
            "gpt-3.5-turbo": 4096
        }
    
    def prepare_messages(self, system_prompt: str, user_prompt: str, model_name: str = "o3-mini-2025-01-31") -> List[Dict]:
        """Prepare messages with token limit enforcement"""
        
        max_tokens = self.model_limits.get(model_name, 8192)
        # Reserve tokens for response and overhead
        available_tokens = max_tokens - 1000  # Reserve 1000 tokens for response
        
        # Create initial messages
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # Count current tokens
        current_tokens = self.tiktoken_manager.count_messages_tokens(messages, model_name)
        
        if current_tokens <= available_tokens:
            logger.info(f"Messages fit within limit: {current_tokens}/{available_tokens} tokens")
            return messages
        
        logger.warning(f"Messages exceed limit: {current_tokens}/{available_tokens} tokens. Truncating...")
        
        # Calculate how much to truncate
        excess_tokens = current_tokens - available_tokens
        
        # Try to preserve system prompt, truncate user prompt
        system_tokens = self.tiktoken_manager.count_tokens(system_prompt, model_name)
        user_tokens = self.tiktoken_manager.count_tokens(user_prompt, model_name)
        
        if user_tokens > excess_tokens + 100:  # Keep some user content
            # Truncate user prompt
            target_user_tokens = user_tokens - excess_tokens - 100
            truncated_user_prompt = self.tiktoken_manager.truncate_text(
                user_prompt, target_user_tokens, model_name
            )
            messages[1]["content"] = truncated_user_prompt + "\n\n[Content truncated due to length]"
            
        else:
            # If user prompt is too small to truncate meaningfully, truncate both
            target_system_tokens = min(system_tokens, available_tokens // 3)
            target_user_tokens = available_tokens - target_system_tokens - 100
            
            truncated_system = self.tiktoken_manager.truncate_text(
                system_prompt, target_system_tokens, model_name
            )
            truncated_user = self.tiktoken_manager.truncate_text(
                user_prompt, target_user_tokens, model_name
            )
            
            messages = [
                {"role": "system", "content": truncated_system + "\n\n[System prompt truncated]"},
                {"role": "user", "content": truncated_user + "\n\n[Content truncated due to length]"}
            ]
        
        # Verify final token count
        final_tokens = self.tiktoken_manager.count_messages_tokens(messages, model_name)
        logger.info(f"Truncated messages: {final_tokens}/{available_tokens} tokens")
        
        return messages
    
    def smart_truncate_legal_text(self, text: str, max_tokens: int, model_name: str = "o3-mini-2025-01-31") -> str:
        """Smart truncation that preserves important legal content"""
        
        current_tokens = self.tiktoken_manager.count_tokens(text, model_name)
        
        if current_tokens <= max_tokens:
            return text
        
        # Try to preserve key legal sections
        lines = text.split('\n')
        important_lines = []
        regular_lines = []
        
        for line in lines:
            line_lower = line.lower().strip()
            # Identify important legal content
            if any(keyword in line_lower for keyword in [
                'article', 'section', 'paragraph', 'shall', 'must', 'obligation',
                'right', 'duty', 'liability', 'consent', 'processing', 'data',
                'personal', 'controller', 'processor', 'lawful', 'legitimate'
            ]):
                important_lines.append(line)
            else:
                regular_lines.append(line)
        
        # Start with important lines
        result_lines = important_lines[:]
        current_text = '\n'.join(result_lines)
        current_tokens = self.tiktoken_manager.count_tokens(current_text, model_name)
        
        # Add regular lines until we hit the limit
        for line in regular_lines:
            test_text = current_text + '\n' + line
            test_tokens = self.tiktoken_manager.count_tokens(test_text, model_name)
            
            if test_tokens <= max_tokens:
                current_text = test_text
                current_tokens = test_tokens
            else:
                break
        
        return current_text

# ====================================
# ELASTICSEARCH CLIENT
# ====================================

class ElasticsearchClient:
    """Enhanced Elasticsearch client with SSL and authentication"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize Elasticsearch client with proper configuration"""
        # SSL context
        ssl_context = ssl.create_default_context()
        if os.path.exists(Config.ELASTICSEARCH_CERT_PATH):
            ssl_context.load_verify_locations(Config.ELASTICSEARCH_CERT_PATH)
        else:
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            logger.warning("SSL certificate not found, using insecure connection")
        
        self.client = Elasticsearch(
            [Config.ELASTICSEARCH_URL],
            basic_auth=(Config.ELASTICSEARCH_USERNAME, Config.ELASTICSEARCH_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True if os.path.exists(Config.ELASTICSEARCH_CERT_PATH) else False
        )
        
        # Test connection
        if self.client.ping():
            logger.info("Successfully connected to Elasticsearch")
        else:
            raise ConnectionError("Failed to connect to Elasticsearch")
    
    def create_index(self):
        """Create the legal rules index with proper mappings"""
        mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "country": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "organization": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "rules": {"type": "nested"},
                    "definitions": {"type": "nested"},
                    "object_properties": {"type": "nested"},
                    "data_properties": {"type": "nested"},
                    "concept_schemes": {"type": "nested"},
                    "compliance_frameworks": {"type": "nested"},
                    "concepts": {"type": "keyword"},
                    "actors": {"type": "keyword"},
                    "objects": {"type": "keyword"},
                    "data_domains": {"type": "keyword"},
                    "extraction_stats": {"type": "object"},
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 3072,  # text-embedding-3-large dimensions
                        "index": True,
                        "similarity": "cosine"
                    },
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            },
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "index": {
                    "max_result_window": 50000
                }
            }
        }
        
        if not self.client.indices.exists(index=Config.ELASTICSEARCH_INDEX):
            self.client.indices.create(index=Config.ELASTICSEARCH_INDEX, body=mapping)
            logger.info(f"Created index: {Config.ELASTICSEARCH_INDEX}")
        else:
            logger.info(f"Index {Config.ELASTICSEARCH_INDEX} already exists")

# ====================================
# OPENAI CLIENT
# ====================================

class OpenAIClient:
    """Enhanced OpenAI client with token management for o3-mini"""
    
    def __init__(self):
        # Validate configuration first
        Config.validate_config()
        
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Initialize token management
        try:
            self.tiktoken_manager = OfflineTiktokenManager(Config.TIKTOKEN_MODELS_PATH)
            self.prompt_manager = TokenAwarePromptManager(self.tiktoken_manager)
            logger.info("Initialized token management with offline tiktoken models")
        except Exception as e:
            logger.error(f"Failed to initialize token management: {e}")
            # Create a dummy manager that doesn't do truncation
            self.tiktoken_manager = None
            self.prompt_manager = None
        
        # Test the client with a simple API call
        try:
            # Test with a minimal embedding request
            test_response = self.client.embeddings.create(
                input=["test"],
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            logger.info(f"OpenAI client initialized successfully with model {Config.OPENAI_EMBEDDING_MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConnectionError(f"OpenAI API connection failed: {e}")
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        logger.info(f"Generating embeddings for {len(texts)} text(s)...")
        
        try:
            # Check token limits for embeddings if tiktoken is available
            if self.tiktoken_manager:
                for i, text in enumerate(texts):
                    token_count = self.tiktoken_manager.count_tokens(text, Config.OPENAI_EMBEDDING_MODEL)
                    if token_count > 8000:  # Conservative limit for embeddings
                        logger.warning(f"Text {i} has {token_count} tokens, truncating...")
                        texts[i] = self.tiktoken_manager.truncate_text(text, 8000, Config.OPENAI_EMBEDDING_MODEL)
            
            response = self.client.embeddings.create(
                input=texts,
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Generate chat completion with token-aware o3-mini"""
        logger.info(f"Starting chat completion with model: {Config.OPENAI_MODEL}")
        
        try:
            # Use token-aware prompt management if available
            if self.prompt_manager and len(messages) >= 2:
                # Extract system and user messages
                system_msg = messages[0].get('content', '') if messages[0].get('role') == 'system' else ''
                user_msg = messages[-1].get('content', '') if messages[-1].get('role') == 'user' else ''
                
                # Get token-managed messages
                managed_messages = self.prompt_manager.prepare_messages(
                    system_msg, user_msg, Config.OPENAI_MODEL
                )
                
                # Add any intermediate messages that fit
                if len(messages) > 2:
                    current_tokens = self.tiktoken_manager.count_messages_tokens(managed_messages, Config.OPENAI_MODEL)
                    available_tokens = 7000 - current_tokens  # Leave room for response
                    
                    for msg in messages[1:-1]:  # Skip first and last
                        msg_tokens = self.tiktoken_manager.count_tokens(msg.get('content', ''), Config.OPENAI_MODEL)
                        if msg_tokens <= available_tokens:
                            managed_messages.insert(-1, msg)  # Insert before last message
                            available_tokens -= msg_tokens
                        else:
                            break
                
                final_messages = managed_messages
            else:
                final_messages = messages
            
            # Log token usage
            if self.tiktoken_manager:
                total_tokens = self.tiktoken_manager.count_messages_tokens(final_messages, Config.OPENAI_MODEL)
                logger.info(f"Sending {total_tokens} tokens to {Config.OPENAI_MODEL}")
            
            response = self.client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=final_messages,
                reasoning_effort=kwargs.get('reasoning_effort', Config.REASONING_EFFORT)
            )
            
            content = response.choices[0].message.content
            logger.info(f"Received response with {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            # Check if it's a token limit error
            if "maximum context length" in str(e).lower():
                logger.error("Token limit exceeded even after truncation. Consider reducing input further.")
            raise

# ====================================
# DOCUMENT PROCESSOR
# ====================================

class DocumentProcessor:
    """Process PDF documents with comprehensive legal-aware chunking"""
    
    def __init__(self, tiktoken_manager: Optional[OfflineTiktokenManager] = None):
        self.tiktoken_manager = tiktoken_manager
        
        # Legal document structure patterns
        self.legal_patterns = {
            'article': r'(?i)(?:article\s+\d+|art\.?\s*\d+)',
            'section': r'(?i)(?:section\s+\d+|sec\.?\s*\d+|ยง\s*\d+)',
            'paragraph': r'(?i)(?:\(\d+\)|\d+\.|\(\w\))',
            'chapter': r'(?i)(?:chapter\s+\d+|chap\.?\s*\d+)',
            'subsection': r'(?i)(?:\(\d+\)\(\w\)|\d+\.\d+)',
            'clause': r'(?i)(?:clause\s+\d+)',
            'regulation': r'(?i)(?:regulation\s+\d+|reg\.?\s*\d+)'
        }
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF with page tracking"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            page_metadata = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                
                # Track page boundaries for context
                page_metadata.append({
                    'page_number': page_num + 1,
                    'text_start': len(full_text),
                    'text_end': len(full_text) + len(page_text),
                    'char_count': len(page_text)
                })
                
                full_text += f"\n--- PAGE {page_num + 1} ---\n" + page_text
            
            doc.close()
            
            # Store page metadata for later use
            self.page_metadata = page_metadata
            
            logger.info(f"Extracted {len(full_text)} characters from {len(page_metadata)} pages")
            return full_text.strip()
            
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            raise
    
    def create_comprehensive_legal_chunks(self, text: str, max_tokens_per_chunk: int = 2500, model_name: str = "o3-mini-2025-01-31") -> List[Dict]:
        """Create comprehensive chunks for legal documents with NO information loss"""
        
        logger.info("Creating comprehensive legal document chunks with structure awareness...")
        
        # First, try structure-aware chunking based on legal document patterns
        structural_chunks = self._chunk_by_legal_structure(text)
        
        if structural_chunks:
            logger.info(f"Found {len(structural_chunks)} structural chunks (articles, sections, etc.)")
            # Further subdivide large structural chunks if needed
            final_chunks = []
            for chunk_info in structural_chunks:
                if self.tiktoken_manager:
                    chunk_tokens = self.tiktoken_manager.count_tokens(chunk_info['text'], model_name)
                    if chunk_tokens <= max_tokens_per_chunk:
                        final_chunks.append(chunk_info)
                    else:
                        # Split large structural chunks while preserving boundaries
                        sub_chunks = self._split_large_structural_chunk(chunk_info, max_tokens_per_chunk, model_name)
                        final_chunks.extend(sub_chunks)
                else:
                    # Fallback to character-based estimation
                    if len(chunk_info['text']) <= max_tokens_per_chunk * 4:
                        final_chunks.append(chunk_info)
                    else:
                        sub_chunks = self._split_large_structural_chunk(chunk_info, max_tokens_per_chunk, model_name)
                        final_chunks.extend(sub_chunks)
            
            return final_chunks
        else:
            # Fallback to semantic/paragraph-based chunking
            logger.info("No clear legal structure found, using semantic chunking...")
            return self._chunk_by_semantic_boundaries(text, max_tokens_per_chunk, model_name)
    
    def _chunk_by_legal_structure(self, text: str) -> List[Dict]:
        """Chunk document based on legal structure (articles, sections, etc.)"""
        import re
        
        chunks = []
        current_chunk = ""
        current_structure = None
        current_start = 0
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Check if this line starts a new legal section
            new_structure = None
            for structure_type, pattern in self.legal_patterns.items():
                if re.match(pattern, line_stripped):
                    new_structure = {
                        'type': structure_type,
                        'title': line_stripped[:100],  # First 100 chars as title
                        'line_number': i
                    }
                    break
            
            if new_structure and current_chunk:
                # Save current chunk
                chunks.append({
                    'text': current_chunk.strip(),
                    'structure': current_structure,
                    'chunk_id': len(chunks),
                    'start_line': current_start,
                    'end_line': i - 1,
                    'type': 'structural'
                })
                
                # Start new chunk
                current_chunk = line + '\n'
                current_structure = new_structure
                current_start = i
            else:
                current_chunk += line + '\n'
                if current_structure is None:
                    current_structure = {'type': 'preamble', 'title': 'Document Preamble', 'line_number': 0}
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'structure': current_structure,
                'chunk_id': len(chunks),
                'start_line': current_start,
                'end_line': len(lines) - 1,
                'type': 'structural'
            })
        
        # Filter out very small chunks (likely just headers)
        meaningful_chunks = [chunk for chunk in chunks if len(chunk['text'].split()) > 10]
        
        logger.info(f"Created {len(meaningful_chunks)} structural chunks")
        return meaningful_chunks
    
    def _split_large_structural_chunk(self, chunk_info: Dict, max_tokens: int, model_name: str) -> List[Dict]:
        """Split large structural chunks while preserving legal context"""
        
        text = chunk_info['text']
        structure = chunk_info['structure']
        
        # Split by paragraphs first
        paragraphs = text.split('\n\n')
        sub_chunks = []
        current_sub_chunk = ""
        current_tokens = 0
        
        overlap_text = ""  # For context preservation
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            para_tokens = self.tiktoken_manager.count_tokens(paragraph, model_name) if self.tiktoken_manager else len(paragraph) // 4
            
            if current_tokens + para_tokens <= max_tokens:
                current_sub_chunk += paragraph + '\n\n'
                current_tokens += para_tokens
            else:
                if current_sub_chunk:
                    # Add overlap from previous chunk for context
                    full_text = overlap_text + current_sub_chunk
                    
                    sub_chunks.append({
                        'text': full_text.strip(),
                        'structure': structure,
                        'chunk_id': f"{chunk_info['chunk_id']}.{len(sub_chunks)}",
                        'parent_chunk': chunk_info['chunk_id'],
                        'sub_chunk_index': len(sub_chunks),
                        'type': 'structural_subdivision',
                        'context_preserved': True
                    })
                    
                    # Keep last paragraph as overlap for next chunk
                    last_para = current_sub_chunk.split('\n\n')[-1]
                    overlap_text = f"[CONTEXT FROM PREVIOUS SECTION: {last_para[:200]}...]\n\n"
                
                current_sub_chunk = paragraph + '\n\n'
                current_tokens = para_tokens
        
        # Add final sub-chunk
        if current_sub_chunk:
            full_text = overlap_text + current_sub_chunk
            sub_chunks.append({
                'text': full_text.strip(),
                'structure': structure,
                'chunk_id': f"{chunk_info['chunk_id']}.{len(sub_chunks)}",
                'parent_chunk': chunk_info['chunk_id'],
                'sub_chunk_index': len(sub_chunks),
                'type': 'structural_subdivision',
                'context_preserved': True
            })
        
        logger.info(f"Split large chunk into {len(sub_chunks)} sub-chunks with context preservation")
        return sub_chunks
    
    def _chunk_by_semantic_boundaries(self, text: str, max_tokens: int, model_name: str) -> List[Dict]:
        """Chunk by semantic boundaries (paragraphs) with overlap for context"""
        
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        # Overlap settings for context preservation
        overlap_size = max_tokens // 10  # 10% overlap
        overlap_text = ""
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            para_tokens = self.tiktoken_manager.count_tokens(paragraph, model_name) if self.tiktoken_manager else len(paragraph) // 4
            
            if current_tokens + para_tokens <= max_tokens:
                current_chunk += paragraph + '\n\n'
                current_tokens += para_tokens
            else:
                if current_chunk:
                    # Add overlap from previous chunk
                    full_text = overlap_text + current_chunk
                    
                    chunks.append({
                        'text': full_text.strip(),
                        'structure': {'type': 'semantic', 'title': f'Semantic Chunk {len(chunks) + 1}'},
                        'chunk_id': len(chunks),
                        'type': 'semantic',
                        'context_preserved': bool(overlap_text),
                        'paragraph_range': f"{max(0, i-len(current_chunk.split('\\n\\n')))}-{i-1}"
                    })
                    
                    # Create overlap for next chunk
                    chunk_paragraphs = current_chunk.strip().split('\n\n')
                    if len(chunk_paragraphs) > 0:
                        overlap_text = f"[CONTEXT: {chunk_paragraphs[-1][:300]}...]\n\n"
                        
                        # Ensure overlap doesn't exceed limit
                        overlap_tokens = self.tiktoken_manager.count_tokens(overlap_text, model_name) if self.tiktoken_manager else len(overlap_text) // 4
                        if overlap_tokens > overlap_size:
                            overlap_text = f"[CONTEXT: {chunk_paragraphs[-1][:150]}...]\n\n"
                
                current_chunk = paragraph + '\n\n'
                current_tokens = para_tokens
        
        # Add final chunk
        if current_chunk:
            full_text = overlap_text + current_chunk
            chunks.append({
                'text': full_text.strip(),
                'structure': {'type': 'semantic', 'title': f'Semantic Chunk {len(chunks) + 1}'},
                'chunk_id': len(chunks),
                'type': 'semantic',
                'context_preserved': bool(overlap_text),
                'is_final': True
            })
        
        logger.info(f"Created {len(chunks)} semantic chunks with context preservation")
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences using simple rules"""
        import re
        
        # Simple sentence splitting - could be enhanced with more sophisticated rules
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def chunk_text(self, text: str, chunk_size: int = Config.CHUNK_SIZE) -> List[str]:
        """Split text into manageable chunks with overlap for better context (fallback method)"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        overlap_size = 200  # Words to overlap between chunks
        
        for word in words:
            if current_length + len(word) > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                # Create overlap for context preservation
                overlap_start = max(0, len(current_chunk) - overlap_size)
                current_chunk = current_chunk[overlap_start:] + [word]
                current_length = sum(len(w) + 1 for w in current_chunk)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1  # +1 for space
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        logger.info(f"Split text into {len(chunks)} character-based chunks")
        return chunks
    
    def smart_extract_for_analysis(self, text: str, max_analysis_tokens: int = 5000, model_name: str = "o3-mini-2025-01-31") -> str:
        """Extract and prioritize the most important content for analysis"""
        
        if not self.tiktoken_manager:
            # Simple truncation fallback
            return text[:max_analysis_tokens * 4]
        
        current_tokens = self.tiktoken_manager.count_tokens(text, model_name)
        
        if current_tokens <= max_analysis_tokens:
            return text
        
        logger.info(f"Text has {current_tokens} tokens, extracting most important content...")
        
        # Split into paragraphs
        paragraphs = text.split('\n\n')
        
        # Score paragraphs by importance
        scored_paragraphs = []
        for para in paragraphs:
            if not para.strip():
                continue
                
            score = self._score_legal_paragraph(para)
            tokens = self.tiktoken_manager.count_tokens(para, model_name)
            scored_paragraphs.append((score, tokens, para))
        
        # Sort by score (descending)
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # Select paragraphs until we hit the token limit
        selected_paragraphs = []
        total_tokens = 0
        
        for score, tokens, para in scored_paragraphs:
            if total_tokens + tokens <= max_analysis_tokens:
                selected_paragraphs.append(para)
                total_tokens += tokens
            else:
                break
        
        result = '\n\n'.join(selected_paragraphs)
        
        final_tokens = self.tiktoken_manager.count_tokens(result, model_name)
        logger.info(f"Extracted {len(selected_paragraphs)} paragraphs ({final_tokens} tokens) for analysis")
        
        return result
    
    def _score_legal_paragraph(self, paragraph: str) -> float:
        """Score a paragraph based on legal importance"""
        
        text_lower = paragraph.lower()
        score = 0.0
        
        # High-value legal keywords
        high_value_keywords = [
            'shall', 'must', 'required', 'obligation', 'prohibited', 'forbidden',
            'right', 'duty', 'liability', 'consent', 'lawful', 'unlawful',
            'processing', 'personal data', 'data subject', 'controller', 'processor',
            'article', 'section', 'paragraph', 'regulation', 'directive'
        ]
        
        # Medium-value keywords
        medium_value_keywords = [
            'may', 'should', 'could', 'appropriate', 'necessary', 'reasonable',
            'ensure', 'implement', 'maintain', 'provide', 'collect', 'use',
            'store', 'transfer', 'delete', 'access', 'security', 'protection'
        ]
        
        # Count keyword occurrences
        for keyword in high_value_keywords:
            score += text_lower.count(keyword) * 3.0
        
        for keyword in medium_value_keywords:
            score += text_lower.count(keyword) * 1.0
        
        # Bonus for containing numbers (often legal references)
        import re
        if re.search(r'\b\d+\b', paragraph):
            score += 1.0
        
        # Bonus for containing dates
        if re.search(r'\b\d{4}\b', paragraph):
            score += 0.5
        
        # Penalty for very short paragraphs
        if len(paragraph.split()) < 10:
            score *= 0.5
        
        return score

# ====================================
# SHACL VALIDATION
# ====================================

class SHACLValidator:
    """SHACL validation for ontologies"""
    
    def __init__(self):
        self.shapes_graph = self._create_validation_shapes()
    
    def _create_validation_shapes(self) -> Graph:
        """Create SHACL shapes for validation"""
        
        shapes = Graph()
        ns = EnhancedOntologyNamespaces()
        shapes = ns.bind_to_graph(shapes)
        
        # Add SHACL namespace
        SH = Namespace("http://www.w3.org/ns/shacl#")
        shapes.bind("sh", SH)
        
        # Create more comprehensive and correct SHACL shapes
        
        # 1. Shape for OWL Classes - ensure they have labels
        class_shape = URIRef(f"{ns.LEGAL}ClassShape")
        shapes.add((class_shape, RDF.type, SH.NodeShape))
        shapes.add((class_shape, SH.targetClass, OWL.Class))
        
        # Require rdfs:label for all classes
        class_label_prop = BNode()
        shapes.add((class_shape, SH.property, class_label_prop))
        shapes.add((class_label_prop, SH.path, RDFS.label))
        shapes.add((class_label_prop, SH.minCount, Literal(1)))
        shapes.add((class_label_prop, SH.datatype, XSD.string))
        
        # 2. Shape for Object Properties
        obj_prop_shape = URIRef(f"{ns.LEGAL}ObjectPropertyShape")
        shapes.add((obj_prop_shape, RDF.type, SH.NodeShape))
        shapes.add((obj_prop_shape, SH.targetClass, OWL.ObjectProperty))
        
        # Require label for object properties
        obj_prop_label = BNode()
        shapes.add((obj_prop_shape, SH.property, obj_prop_label))
        shapes.add((obj_prop_label, SH.path, RDFS.label))
        shapes.add((obj_prop_label, SH.minCount, Literal(1)))
        shapes.add((obj_prop_label, SH.datatype, XSD.string))
        
        # 3. Shape for Data Properties
        data_prop_shape = URIRef(f"{ns.LEGAL}DataPropertyShape")
        shapes.add((data_prop_shape, RDF.type, SH.NodeShape))
        shapes.add((data_prop_shape, SH.targetClass, OWL.DatatypeProperty))
        
        # Require label for data properties
        data_prop_label = BNode()
        shapes.add((data_prop_shape, SH.property, data_prop_label))
        shapes.add((data_prop_label, SH.path, RDFS.label))
        shapes.add((data_prop_label, SH.minCount, Literal(1)))
        shapes.add((data_prop_label, SH.datatype, XSD.string))
        
        # 4. Shape for SKOS Concepts - ensure they have preferred labels
        concept_shape = URIRef(f"{ns.LEGAL}ConceptShape")
        shapes.add((concept_shape, RDF.type, SH.NodeShape))
        shapes.add((concept_shape, SH.targetClass, SKOS.Concept))
        
        # Prefer SKOS prefLabel for concepts
        concept_pref_label = BNode()
        shapes.add((concept_shape, SH.property, concept_pref_label))
        shapes.add((concept_pref_label, SH.path, SKOS.prefLabel))
        shapes.add((concept_pref_label, SH.minCount, Literal(0)))  # Optional but recommended
        shapes.add((concept_pref_label, SH.datatype, XSD.string))
        
        # 5. Shape for Named Individuals
        individual_shape = URIRef(f"{ns.LEGAL}IndividualShape")
        shapes.add((individual_shape, RDF.type, SH.NodeShape))
        shapes.add((individual_shape, SH.targetClass, OWL.NamedIndividual))
        
        # Require label for individuals
        individual_label = BNode()
        shapes.add((individual_shape, SH.property, individual_label))
        shapes.add((individual_label, SH.path, RDFS.label))
        shapes.add((individual_label, SH.minCount, Literal(1)))
        shapes.add((individual_label, SH.datatype, XSD.string))
        
        return shapes
    
    def validate_ontology(self, ontology_graph: Graph) -> Tuple[bool, Graph]:
        """Validate ontology against SHACL shapes"""
        
        try:
            conforms, results_graph, results_text = pyshacl.validate(
                data_graph=ontology_graph,
                shacl_graph=self.shapes_graph,
                inference='rdfs',
                serialize_report_graph=True
            )
            
            if not conforms:
                logger.warning(f"SHACL validation failed: {results_text}")
            else:
                logger.info("SHACL validation passed")
            
            return conforms, results_graph
            
        except Exception as e:
            logger.error(f"SHACL validation error: {e}")
            return False, Graph()

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            # Validate configuration structure
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file with supporting documents"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission",
                    "pdf_document": "./documents/gdpr.pdf",
                    "supporting_documents": [
                        "./documents/gdpr_guidance.pdf",
                        "./documents/gdpr_implementation.pdf"
                    ]
                },
                {
                    "country": "United States",
                    "jurisdiction": "Federal",
                    "organization": "Federal Trade Commission",
                    "pdf_document": "./documents/ccpa.pdf",
                    "supporting_documents": [
                        "./documents/ccpa_regulations.pdf"
                    ]
                }
            ],
            "processing_options": {
                "enable_query_interface": True,
                "interface_host": "localhost",
                "interface_port": 5000,
                "batch_size": 5,
                "max_concurrent": 3,
                "reasoning_effort": "high",
                "export_formats": ["ttl", "jsonld", "xml"]
            }
        }

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    NLP_MODEL_PATH = os.getenv("NLP_MODEL_PATH", "./nlp_model/en_core_web_sm-3.8.0.tar.gz")
    TIKTOKEN_MODELS_PATH = os.getenv("TIKTOKEN_MODELS_PATH", "./tiktoken_models")
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if not cls.ELASTICSEARCH_PASSWORD:
            missing_vars.append("ELASTICSEARCH_PASSWORD")
        
        # Check if tiktoken models directory exists
        if not os.path.exists(cls.TIKTOKEN_MODELS_PATH):
            logger.warning(f"Tiktoken models directory not found: {cls.TIKTOKEN_MODELS_PATH}")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )
    
    # Model parameters
    REASONING_EFFORT = "high"  # Use high reasoning for detailed extraction
    
    # Processing parameters
    BATCH_SIZE = 10
    MAX_CONCURRENT = 5
    CHUNK_SIZE = 2000

# ====================================
# ENHANCED ONTOLOGY NAMESPACES
# ====================================

class EnhancedOntologyNamespaces:
    """Enhanced ontology namespaces with detailed property definitions"""
    
    # Core vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal and jurisdictional
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Custom namespace for our legal rules
    LEGAL = Namespace("https://legal-rules.org/ontology#")
    
    # Data management domains
    STORAGE = Namespace("https://legal-rules.org/storage#")
    USAGE = Namespace("https://legal-rules.org/usage#")
    MOVEMENT = Namespace("https://legal-rules.org/movement#")
    PRIVACY = Namespace("https://legal-rules.org/privacy#")
    SECURITY = Namespace("https://legal-rules.org/security#")
    ACCESS = Namespace("https://legal-rules.org/access#")
    
    # Property namespaces
    PROPERTIES = Namespace("https://legal-rules.org/properties#")
    RELATIONS = Namespace("https://legal-rules.org/relations#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("legal", cls.LEGAL)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# REACT AGENTS WITH LANGMEM
# ====================================

class ReActAgent:
    """Base ReAct agent with explicit reasoning and action steps using LangMem"""
    
    def __init__(self, openai_client, name: str, memory_store: InMemoryStore):
        self.openai_client = openai_client
        self.name = name
        self.memory_store = memory_store
        self.reasoning_steps = []
        self.action_steps = []
        
        # Initialize LangMem components
        self.memory_manager = create_memory_manager(
            Config.OPENAI_MODEL,
            instructions=f"Store and manage long-term memory for {name} agent processing legal documents",
            enable_inserts=True,
            enable_updates=True,
            enable_deletes=False
        )
        
        # Create memory tools with namespace
        memory_namespace = (f"{name.lower()}_memories", "{user_id}")
        self.manage_memory_tool = create_manage_memory_tool(namespace=memory_namespace)
        self.search_memory_tool = create_search_memory_tool(namespace=memory_namespace)
        
        # Setup ReAct agent
        self._setup_react_agent()
    
    def _setup_react_agent(self):
        """Setup the ReAct agent with tools"""
        from langgraph.prebuilt import create_react_agent
        from langchain_openai import ChatOpenAI
        
        # Initialize the model
        llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Create tools list including memory tools
        tools = [self.manage_memory_tool, self.search_memory_tool]
        
        # Add agent-specific tools
        agent_tools = self._get_agent_specific_tools()
        if agent_tools:
            tools.extend(agent_tools)
        
        # Create ReAct agent with memory
        checkpointer = MemorySaver()
        self.react_agent = create_react_agent(
            llm,
            tools=tools,
            checkpointer=checkpointer,
            store=self.memory_store
        )
    
    def _get_agent_specific_tools(self):
        """Override in subclasses to add agent-specific tools"""
        return []
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": self.name
        })
        logger.info(f"[{self.name}] REASONING: {thought}")
    
    def log_action(self, action: str, result: Any = None):
        """Log an action step"""
        self.action_steps.append({
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "result": str(result)[:200] if result else None,
            "agent": self.name
        })
        logger.info(f"[{self.name}] ACTION: {action}")
    
    async def invoke_agent(self, messages: List[Dict], thread_id: str = None) -> Dict:
        """Invoke the ReAct agent with memory"""
        if not thread_id:
            thread_id = str(uuid.uuid4())
        
        config = {"configurable": {"thread_id": thread_id, "user_id": "legal_analyst"}}
        
        try:
            result = self.react_agent.invoke(
                {"messages": messages},
                config=config
            )
            return result
        except Exception as e:
            logger.error(f"ReAct agent invocation failed: {e}")
            raise

class EnhancedRuleExtractionAgent(ReActAgent):
    """Enhanced ReAct agent for rule extraction with detailed semantics and memory"""
    
    def __init__(self, openai_client, memory_store: InMemoryStore):
        super().__init__(openai_client, "RuleExtractor", memory_store)
    
    def _get_agent_specific_tools(self):
        """Add rule extraction specific tools"""
        
        @tool
        def extract_legal_definitions(text: str) -> str:
            """Extract legal definitions and terms from text"""
            # This would contain extraction logic
            return f"Extracted definitions from: {text[:100]}..."
        
        @tool 
        def identify_rule_components(text: str) -> str:
            """Identify subjects, predicates, objects in legal rules"""
            return f"Identified rule components in: {text[:100]}..."
        
        return [extract_legal_definitions, identify_rule_components]
    
    async def extract_rules_and_definitions(self, text: str, context: Dict) -> Dict:
        """Extract legal rules with comprehensive definitions and properties using ReAct pattern"""
        
        self.log_reasoning("Starting comprehensive rule extraction from legal document")
        self.log_reasoning(f"Context: {context['country']}/{context['jurisdiction']}")
        self.log_reasoning("Will extract rules, definitions, object properties, and data properties")
        
        # ENHANCED PROMPT - Ensures every subject has definition and comprehensive properties
        system_prompt = """You are a specialized legal semantic analysis expert using the ReAct (Reasoning and Acting) methodology. Your task is to extract comprehensive machine-readable rules from legal documents with rich semantic information.

CRITICAL REQUIREMENTS - MUST BE FOLLOWED:

1. EVERY SUBJECT MUST HAVE A COMPLETE DEFINITION - No subject can be extracted without a formal definition
2. COMPREHENSIVE PROPERTY COVERAGE - Must include both object properties AND data properties for every concept
3. MANDATORY VALIDATION - Ensure every extracted element has complete semantic information

EXTRACTION REQUIREMENTS:

1. RULES - Extract with detailed semantics (EVERY subject MUST have definition):
   - Type: obligation|permission|prohibition|condition|exception
   - Subject (who/what entity) - MUST include complete definition
   - Predicate (action/relationship) - MUST include complete definition  
   - Object (target/recipient) - MUST include complete definition
   - Modality (must|may|shall|shall not|should|could|etc.)
   - Conditions (when/where/if/unless clauses)
   - Temporal aspects (before/after/during/within timeframes)
   - Jurisdiction scope (local/national/international)
   - Data domain classification

2. DEFINITIONS - Extract comprehensive definitions (MANDATORY for every concept):
   - Concept name and type (class/individual/property)
   - Formal definition from the text (REQUIRED)
   - Informal description in plain language (REQUIRED)
   - Synonyms and alternative terms
   - Related concepts and hierarchical relationships
   - Domain and range for properties
   - Cardinality constraints
   - Legal source reference

3. OBJECT PROPERTIES - Relationships between entities (REQUIRED):
   - Property name and URI suffix
   - Domain (subject type) with definition
   - Range (object type) with definition
   - Characteristics (transitive/symmetric/functional/etc.)
   - Sub-property relationships
   - Inverse properties
   - Complete definition of the property relationship

4. DATA PROPERTIES - Attributes with literal values (REQUIRED):
   - Property name and URI suffix
   - Domain (subject type) with definition
   - Range (datatype: string/date/boolean/number/etc.)
   - Cardinality (min/max/exact)
   - Default values
   - Validation constraints
   - Complete definition of the data attribute

5. SEMANTIC RELATIONSHIPS:
   - Hierarchical (broader/narrower/subClassOf)
   - Associative (related/sameAs/differentFrom)
   - Dependency (requires/implies/conflicts)
   - Temporal (before/after/during)

REASONING METHODOLOGY:
Think step by step using ReAct pattern:
1. REASON about what legal concepts are present
2. ACT by extracting each concept with full definition
3. OBSERVE the extracted content for completeness
4. REASON about relationships between concepts
5. ACT by defining properties and relationships
6. OBSERVE for any missing definitions or properties

CRITICAL VALIDATION CHECKS:
- Every subject entity has a formal definition
- Every object entity has a formal definition
- Every predicate has a clear definition
- At least 3 object properties are defined per document section
- At least 3 data properties are defined per document section
- All property domains and ranges are defined
- No undefined references in the semantic network

CRITICAL: Respond ONLY with valid JSON. Include comprehensive semantic information.

Output format:
{
  "rules": [
    {
      "id": "unique_rule_id",
      "type": "obligation|permission|prohibition|condition|exception",
      "subject": {
        "entity": "entity name",
        "type": "DataController|DataProcessor|Individual|Organization|System",
        "scope": "specific|general|conditional",
        "definition": "REQUIRED: Complete formal definition of the subject entity",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "predicate": {
        "action": "action name",
        "category": "processing|transfer|storage|access|notification",
        "semantic_type": "activity|state|relationship",
        "definition": "REQUIRED: Complete formal definition of the predicate",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "object": {
        "entity": "target entity",
        "type": "PersonalData|NonPersonalData|System|Process|Document",
        "characteristics": ["sensitive|public|confidential|etc"],
        "definition": "REQUIRED: Complete formal definition of the object entity",
        "informal_description": "REQUIRED: Plain language explanation"
      },
      "modality": {
        "strength": "must|may|shall|should|could",
        "polarity": "positive|negative",
        "certainty": "certain|probable|possible"
      },
      "conditions": [
        {
          "type": "temporal|spatial|logical|circumstantial",
          "description": "condition description",
          "trigger": "event or state that activates the rule"
        }
      ],
      "temporal_aspects": {
        "start_condition": "when rule becomes active",
        "end_condition": "when rule expires",
        "duration": "how long rule applies",
        "frequency": "how often rule applies"
      },
      "jurisdiction_scope": {
        "geographic": "local|regional|national|international",
        "legal_system": "common_law|civil_law|religious_law|etc",
        "enforcement_authority": "regulatory body or court"
      },
      "data_domain": "storage|usage|movement|privacy|security|access|entitlements",
      "original_text": "exact text from document",
      "confidence": 0.0-1.0,
      "legal_basis": "constitutional|statutory|regulatory|case_law|contract"
    }
  ],
  "definitions": [
    {
      "concept": "concept name",
      "uri_suffix": "CamelCaseURISuffix",
      "type": "class|individual|object_property|data_property",
      "formal_definition": "REQUIRED: precise legal definition from text",
      "informal_description": "REQUIRED: plain language explanation",
      "synonyms": ["alternative term 1", "alternative term 2"],
      "broader_concepts": ["parent concept 1", "parent concept 2"],
      "narrower_concepts": ["child concept 1", "child concept 2"],
      "related_concepts": ["related concept 1", "related concept 2"],
      "domain_range": {
        "domain": "applicable entity types with definitions",
        "range": "applicable value types or target entities with definitions"
      },
      "characteristics": ["transitive", "symmetric", "functional", "etc"],
      "examples": ["example 1", "example 2"],
      "legal_source": "section or article reference",
      "cardinality_constraints": "min/max/exact constraints if applicable"
    }
  ],
  "object_properties": [
    {
      "property_name": "hasLegalBasis",
      "uri_suffix": "hasLegalBasis",
      "domain": ["LegalRule", "ProcessingActivity"],
      "domain_definitions": {
        "LegalRule": "REQUIRED: Complete definition of LegalRule domain",
        "ProcessingActivity": "REQUIRED: Complete definition of ProcessingActivity domain"
      },
      "range": ["LegalBasis", "LegalProvision"],
      "range_definitions": {
        "LegalBasis": "REQUIRED: Complete definition of LegalBasis range",
        "LegalProvision": "REQUIRED: Complete definition of LegalProvision range"
      },
      "characteristics": ["functional"],
      "sub_property_of": ["hasJustification"],
      "inverse_property": "isLegalBasisFor",
      "definition": "REQUIRED: Complete definition of this property relationship",
      "informal_description": "REQUIRED: Plain language explanation of the property",
      "cardinality": "exactly_one",
      "legal_source": "source section or article"
    }
  ],
  "data_properties": [
    {
      "property_name": "hasEffectiveDate",
      "uri_suffix": "hasEffectiveDate",
      "domain": ["LegalRule", "Regulation"],
      "domain_definitions": {
        "LegalRule": "REQUIRED: Complete definition of LegalRule domain",
        "Regulation": "REQUIRED: Complete definition of Regulation domain"
      },
      "range": "xsd:date",
      "range_definition": "REQUIRED: Definition of what this date represents",
      "cardinality": "max_one",
      "definition": "REQUIRED: Complete definition of this data property",
      "informal_description": "REQUIRED: Plain language explanation",
      "validation_constraints": ["must be a valid date", "cannot be in the past for new rules"],
      "default_value": "none",
      "legal_source": "source section or article"
    }
  ],
  "semantic_relationships": [
    {
      "source": "concept1",
      "source_definition": "REQUIRED: Complete definition of source concept",
      "relationship_type": "subClassOf|broader|requires|implies|conflicts",
      "target": "concept2", 
      "target_definition": "REQUIRED: Complete definition of target concept",
      "strength": "strong|moderate|weak",
      "bidirectional": true|false,
      "explanation": "REQUIRED: Explanation of this semantic relationship"
    }
  ],
  "validation_summary": {
    "all_subjects_defined": true,
    "all_objects_defined": true,
    "all_predicates_defined": true,
    "object_properties_count": "minimum 3",
    "data_properties_count": "minimum 3",
    "undefined_references": []
  }
}"""

        user_prompt = f"""
ANALYZE THE FOLLOWING LEGAL TEXT using ReAct methodology:

REASONING STEP 1: What are the main legal concepts in this text?
ACTION STEP 1: Identify all subjects, predicates, and objects
OBSERVATION STEP 1: List what I found

REASONING STEP 2: Which concepts need definitions?
ACTION STEP 2: Extract formal definitions for every concept
OBSERVATION STEP 2: Ensure no concept lacks definition

REASONING STEP 3: What relationships exist between concepts?
ACTION STEP 3: Define object properties and data properties
OBSERVATION STEP 3: Verify comprehensive property coverage

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}
- Legal System: {context.get('legal_system', 'Unknown')}

LEGAL TEXT:
{text}

CRITICAL VALIDATION REQUIREMENTS:
โ Every subject entity MUST have a complete formal definition
โ Every object entity MUST have a complete formal definition  
โ Every predicate MUST have a clear definition
โ Minimum 3 object properties with full definitions
โ Minimum 3 data properties with full definitions
โ All property domains and ranges must be defined
โ No undefined references allowed

Extract all rules, definitions, object properties, data properties, and semantic relationships according to the specified JSON format. Focus on creating a comprehensive semantic network suitable for automated reasoning and compliance checking.

RESPOND WITH COMPLETE JSON ONLY - NO ADDITIONAL TEXT.
"""

        # Use ReAct agent for extraction
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        self.log_action("Invoking ReAct agent for comprehensive extraction")
        
        try:
            result = await self.invoke_agent(messages, f"extraction_{uuid.uuid4()}")
            
            # Extract the JSON from the agent response
            response_content = result['messages'][-1].content if result.get('messages') else ""
            
            self.log_action("Received response from ReAct agent, parsing JSON")
            
            # Parse JSON response
            response_text = response_content.strip()
            
            # Handle markdown-wrapped JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            parsed_result = json.loads(response_text)
            
            # Validate the result has required elements
            validation_issues = self._validate_extraction_result(parsed_result)
            if validation_issues:
                self.log_reasoning(f"Validation issues found: {validation_issues}")
                # Store issues in memory for future improvement
                asyncio.create_task(self._store_validation_issues(validation_issues))
            
            self.log_action(f"Successfully parsed {len(parsed_result.get('rules', []))} rules, {len(parsed_result.get('definitions', []))} definitions")
            
            # Store successful extraction pattern in memory
            asyncio.create_task(self._store_extraction_success(parsed_result, context))
            
            return parsed_result
            
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Attempting to clean and retry.")
            # Store failure pattern in memory
            asyncio.create_task(self._store_extraction_failure(str(e), context))
            
            # Return basic structure
            return self._get_empty_extraction_result()
        except Exception as e:
            self.log_action(f"ReAct agent extraction failed: {e}")
            asyncio.create_task(self._store_extraction_failure(str(e), context))
            return self._get_empty_extraction_result()
    
    def _validate_extraction_result(self, result: Dict) -> List[str]:
        """Validate that extraction result meets requirements"""
        issues = []
        
        # Check that every rule has defined subjects/objects/predicates
        for i, rule in enumerate(result.get('rules', [])):
            if not rule.get('subject', {}).get('definition'):
                issues.append(f"Rule {i}: Subject lacks definition")
            if not rule.get('object', {}).get('definition'):
                issues.append(f"Rule {i}: Object lacks definition")
            if not rule.get('predicate', {}).get('definition'):
                issues.append(f"Rule {i}: Predicate lacks definition")
        
        # Check minimum property counts
        if len(result.get('object_properties', [])) < 3:
            issues.append(f"Only {len(result.get('object_properties', []))} object properties found, minimum 3 required")
        
        if len(result.get('data_properties', [])) < 3:
            issues.append(f"Only {len(result.get('data_properties', []))} data properties found, minimum 3 required")
        
        # Check property definitions
        for i, prop in enumerate(result.get('object_properties', [])):
            if not prop.get('definition'):
                issues.append(f"Object property {i}: Lacks definition")
            if not prop.get('domain_definitions'):
                issues.append(f"Object property {i}: Lacks domain definitions")
            if not prop.get('range_definitions'):
                issues.append(f"Object property {i}: Lacks range definitions")
        
        for i, prop in enumerate(result.get('data_properties', [])):
            if not prop.get('definition'):
                issues.append(f"Data property {i}: Lacks definition")
            if not prop.get('domain_definitions'):
                issues.append(f"Data property {i}: Lacks domain definitions")
        
        return issues
    
    async def _store_validation_issues(self, issues: List[str]):
        """Store validation issues in memory for learning"""
        memory_content = {
            "type": "validation_issues",
            "timestamp": datetime.now().isoformat(),
            "issues": issues,
            "agent": self.name
        }
        
        # Store using memory manager (stateless)
        try:
            conversation = [
                {"role": "system", "content": f"Store validation issues for {self.name} agent"},
                {"role": "user", "content": f"Remember these validation issues: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store validation issues in memory: {e}")
    
    async def _store_extraction_success(self, result: Dict, context: Dict):
        """Store successful extraction patterns in memory"""
        memory_content = {
            "type": "extraction_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "rules_count": len(result.get('rules', [])),
            "definitions_count": len(result.get('definitions', [])),
            "object_properties_count": len(result.get('object_properties', [])),
            "data_properties_count": len(result.get('data_properties', [])),
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store successful extraction pattern for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful extraction: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store extraction success in memory: {e}")
    
    async def _store_extraction_failure(self, error: str, context: Dict):
        """Store extraction failures in memory for learning"""
        memory_content = {
            "type": "extraction_failure", 
            "timestamp": datetime.now().isoformat(),
            "error": error,
            "context": context,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store extraction failure for {self.name} agent"},
                {"role": "user", "content": f"Remember this failure to learn from: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store extraction failure in memory: {e}")
    
    def _get_empty_extraction_result(self) -> Dict:
        """Return empty but valid extraction result structure"""
        return {
            "rules": [],
            "definitions": [],
            "object_properties": [],
            "data_properties": [],
            "semantic_relationships": [],
            "validation_summary": {
                "all_subjects_defined": False,
                "all_objects_defined": False,
                "all_predicates_defined": False,
                "object_properties_count": 0,
                "data_properties_count": 0,
                "undefined_references": ["parsing_failed"]
            }
        }

class EnhancedConceptAnalysisAgent(ReActAgent):
    """Enhanced ReAct agent for concept analysis with taxonomic reasoning and memory"""
    
    def __init__(self, openai_client, memory_store: InMemoryStore):
        super().__init__(openai_client, "ConceptAnalyzer", memory_store)
    
    def _get_agent_specific_tools(self):
        """Add concept analysis specific tools"""
        
        @tool
        def build_taxonomy_hierarchy(concepts: str) -> str:
            """Build hierarchical taxonomy from concepts"""
            return f"Built taxonomy for concepts: {concepts[:100]}..."
        
        @tool
        def identify_concept_relationships(concepts: str) -> str:
            """Identify relationships between legal concepts"""
            return f"Identified relationships in: {concepts[:100]}..."
        
        return [build_taxonomy_hierarchy, identify_concept_relationships]
    
    async def analyze_concepts_and_taxonomy(self, extraction_result: Dict, context: Dict) -> Dict:
        """Analyze extracted concepts and build comprehensive taxonomies using ReAct pattern"""
        
        self.log_reasoning("Starting comprehensive concept analysis and taxonomy building")
        self.log_reasoning(f"Processing {len(extraction_result.get('rules', []))} rules and {len(extraction_result.get('definitions', []))} definitions")
        
        # ENHANCED PROMPT for concept analysis
        system_prompt = """You are a legal ontology expert specializing in creating comprehensive taxonomic structures for legal knowledge representation using ReAct methodology.

REASONING APPROACH:
1. REASON about the extracted concepts and their inherent relationships
2. ACT by creating hierarchical structures and concept schemes
3. OBSERVE the created structures for completeness and accuracy
4. REASON about compliance frameworks and cross-references
5. ACT by building logical axioms and constraints
6. OBSERVE final structure for comprehensive coverage

Your task is to analyze extracted legal rules and definitions to create:

1. HIERARCHICAL TAXONOMIES using SKOS relationships (EVERY concept must be placed in hierarchy)
2. CONCEPT SCHEMES for different legal domains (minimum 5 schemes required)
3. COMPLIANCE FRAMEWORKS linking rules to requirements (minimum 3 frameworks)
4. CROSS-REFERENCES between related concepts (extensive linking required)
5. AXIOMS and LOGICAL CONSTRAINTS (comprehensive rule coverage)

Build using established vocabularies:
- Data Privacy Vocabulary (DPV) concepts and relationships
- PROV-O provenance and activity concepts
- SKOS hierarchical and associative relationships
- ELI (European Legislation Identifier) for legal document structure

Focus on creating interconnected concept networks for:
- Legal Entities (controllers, processors, subjects, authorities)
- Data Categories (personal, sensitive, public, confidential)
- Processing Activities (collection, analysis, sharing, storage)
- Legal Bases (consent, contract, legal obligation, vital interests)
- Rights and Obligations (access, rectification, erasure, portability)
- Technical and Organizational Measures (encryption, access controls, audit)

CRITICAL REQUIREMENTS:
โ Every concept from extraction MUST appear in at least one taxonomy
โ Minimum 5 comprehensive concept schemes
โ Minimum 3 compliance frameworks with detailed requirements
โ Extensive cross-referencing between concepts
โ Complete logical axioms for rule validation
โ No orphaned concepts allowed

CRITICAL: Respond ONLY with valid JSON. Create comprehensive taxonomic structures.

Output format:
{
  "concept_schemes": [
    {
      "scheme_id": "legal_entities_scheme",
      "title": "Legal Entities in Data Protection",
      "description": "Classification of legal entities and their roles",
      "top_concepts": ["LegalEntity"],
      "concepts": [
        {
          "concept_id": "DataController",
          "pref_label": "Data Controller",
          "alt_labels": ["Controller", "Data Responsible"],
          "definition": "REQUIRED: Complete definition from extraction or legal sources",
          "broader": ["LegalEntity"],
          "narrower": ["PublicDataController", "PrivateDataController"],
          "related": ["DataProcessor", "DataSubject"],
          "dpv_mapping": "dpv:DataController",
          "examples": ["Company collecting customer data", "Government agency processing citizen data"],
          "legal_obligations": ["Must have lawful basis", "Must ensure data security"],
          "compliance_requirements": ["GDPR Article 5", "GDPR Article 24"]
        }
      ]
    }
  ],
  "taxonomic_hierarchies": {
    "data_categories": {
      "root": "Data",
      "hierarchy": {
        "PersonalData": {
          "definition": "Information relating to an identified or identifiable natural person",
          "legal_source": "GDPR Article 4(1)",
          "children": {
            "SensitivePersonalData": {
              "definition": "Personal data revealing sensitive attributes",
              "legal_source": "GDPR Article 9",
              "children": {
                "BiometricData": {
                  "definition": "Data from biometric measurements",
                  "examples": ["Fingerprints", "Facial recognition data"],
                  "special_protections": ["Explicit consent required", "Limited processing purposes"]
                },
                "HealthData": {
                  "definition": "Data concerning health status",
                  "examples": ["Medical records", "Fitness tracker data"],
                  "special_protections": ["Medical professional privilege", "Strict consent requirements"]
                }
              }
            },
            "ContactData": {
              "definition": "Data for contacting individuals",
              "children": {
                "EmailAddress": {"definition": "Electronic mail address"},
                "PhoneNumber": {"definition": "Telephone contact number"}
              }
            }
          }
        }
      }
    }
  },
  "compliance_frameworks": [
    {
      "framework_id": "gdpr_compliance",
      "title": "GDPR Compliance Framework",
      "description": "Requirements and obligations under GDPR",
      "legal_source": "Regulation (EU) 2016/679",
      "scope": "Personal data processing in EU",
      "requirements": [
        {
          "requirement_id": "lawful_basis_requirement",
          "title": "Lawful Basis for Processing",
          "description": "Processing must have a lawful basis under Article 6",
          "legal_reference": "GDPR Article 6",
          "applicable_to": ["DataController"],
          "mandatory": true,
          "related_rules": ["rule_id_1", "rule_id_2"],
          "verification_criteria": ["Legal basis documented", "Basis communicated to data subjects"],
          "penalties": "Up to 4% of annual turnover or โฌ20 million",
          "implementation_guidance": ["Document legal basis", "Review basis regularly", "Communicate to data subjects"]
        }
      ],
      "enforcement_authority": "Data Protection Authorities",
      "territorial_scope": "European Union and EEA"
    }
  ],
  "cross_references": [
    {
      "source_concept": "DataController",
      "target_concept": "LawfulBasis",
      "relationship": "must_have",
      "description": "Every data controller must have a lawful basis for processing",
      "legal_source": "GDPR Article 6",
      "cardinality": "one_to_many",
      "examples": ["Controller processes based on consent", "Controller processes for contract performance"]
    }
  ],
  "logical_axioms": [
    {
      "axiom_type": "universal_restriction",
      "subject": "ProcessingActivity",
      "property": "hasLawfulBasis",
      "constraint": "exactly_one",
      "description": "Every processing activity must have exactly one primary lawful basis",
      "formal_logic": "โx (ProcessingActivity(x) โ โ!y (LawfulBasis(y) โง hasLawfulBasis(x,y)))",
      "enforcement_rule": "System must validate lawful basis before allowing processing"
    }
  ],
  "semantic_mappings": {
    "dpv_alignments": [
      {
        "local_concept": "DataController",
        "dpv_concept": "dpv:DataController",
        "alignment_type": "exact_match",
        "confidence": 1.0
      }
    ],
    "prov_alignments": [
      {
        "local_concept": "ProcessingActivity", 
        "prov_concept": "prov:Activity",
        "alignment_type": "sub_class",
        "confidence": 0.9
      }
    ]
  },
  "data_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"],
  "quality_metrics": {
    "total_concepts": 150,
    "max_hierarchy_depth": 5,
    "cross_references_count": 45,
    "axioms_count": 23,
    "concept_schemes_count": 5,
    "compliance_frameworks_count": 3,
    "orphaned_concepts": []
  }
}"""

        user_prompt = f"""
ANALYZE EXTRACTION RESULTS using ReAct methodology:

REASONING STEP 1: What concepts were extracted and how should they be organized?
ACTION STEP 1: Create hierarchical taxonomies for all concepts
OBSERVATION STEP 1: Verify every concept is placed in a hierarchy

REASONING STEP 2: What concept schemes are needed for this legal domain?
ACTION STEP 2: Build comprehensive concept schemes (minimum 5)
OBSERVATION STEP 2: Ensure complete domain coverage

REASONING STEP 3: What compliance frameworks apply to these concepts?
ACTION STEP 3: Define compliance frameworks with detailed requirements
OBSERVATION STEP 3: Validate framework completeness

REASONING STEP 4: How do concepts relate to each other?
ACTION STEP 4: Create extensive cross-references and relationships
OBSERVATION STEP 4: Check for comprehensive interconnection

REASONING STEP 5: What logical constraints govern these concepts?
ACTION STEP 5: Build logical axioms and enforcement rules
OBSERVATION STEP 5: Ensure logical consistency

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}

EXTRACTION RESULTS:
{json.dumps(extraction_result, indent=2)}

CRITICAL REQUIREMENTS:
โ Every extracted concept must appear in taxonomies
โ Minimum 5 comprehensive concept schemes required
โ Minimum 3 detailed compliance frameworks required
โ Extensive cross-referencing between all concepts
โ Complete logical axioms for rule validation
โ No orphaned or undefined concepts allowed

Create hierarchical taxonomies, concept schemes, compliance frameworks, and logical constraints that represent the legal knowledge in a machine-readable format suitable for automated reasoning and compliance checking.

RESPOND WITH COMPLETE JSON ONLY - NO ADDITIONAL TEXT.
"""

        # Use ReAct agent for analysis
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        self.log_action("Invoking ReAct agent for taxonomic analysis")
        
        try:
            result = await self.invoke_agent(messages, f"analysis_{uuid.uuid4()}")
            
            # Extract the JSON from the agent response
            response_content = result['messages'][-1].content if result.get('messages') else ""
            
            self.log_action("Processing taxonomic analysis response")
            
            response_text = response_content.strip()
            
            # Handle markdown-wrapped JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            parsed_result = json.loads(response_text)
            
            # Validate the analysis result
            validation_issues = self._validate_analysis_result(parsed_result, extraction_result)
            if validation_issues:
                self.log_reasoning(f"Analysis validation issues: {validation_issues}")
                asyncio.create_task(self._store_analysis_issues(validation_issues))
            
            self.log_action(f"Successfully created {len(parsed_result.get('concept_schemes', []))} concept schemes")
            
            # Store successful analysis in memory
            asyncio.create_task(self._store_analysis_success(parsed_result, context))
            
            return parsed_result
            
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Returning basic structure.")
            asyncio.create_task(self._store_analysis_failure(str(e), context))
            return self._get_empty_analysis_result()
        except Exception as e:
            self.log_action(f"ReAct agent analysis failed: {e}")
            asyncio.create_task(self._store_analysis_failure(str(e), context))
            return self._get_empty_analysis_result()
    
    def _validate_analysis_result(self, result: Dict, extraction_result: Dict) -> List[str]:
        """Validate that analysis result meets requirements"""
        issues = []
        
        # Check minimum counts
        if len(result.get('concept_schemes', [])) < 5:
            issues.append(f"Only {len(result.get('concept_schemes', []))} concept schemes, minimum 5 required")
        
        if len(result.get('compliance_frameworks', [])) < 3:
            issues.append(f"Only {len(result.get('compliance_frameworks', []))} compliance frameworks, minimum 3 required")
        
        # Check that extracted concepts are covered
        extracted_concepts = set()
        for definition in extraction_result.get('definitions', []):
            extracted_concepts.add(definition.get('concept', ''))
        
        covered_concepts = set()
        for scheme in result.get('concept_schemes', []):
            for concept in scheme.get('concepts', []):
                covered_concepts.add(concept.get('concept_id', ''))
        
        uncovered = extracted_concepts - covered_concepts
        if uncovered:
            issues.append(f"Concepts not covered in taxonomies: {list(uncovered)}")
        
        return issues
    
    async def _store_analysis_issues(self, issues: List[str]):
        """Store analysis validation issues in memory"""
        memory_content = {
            "type": "analysis_issues",
            "timestamp": datetime.now().isoformat(),
            "issues": issues,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store analysis issues for {self.name} agent"},
                {"role": "user", "content": f"Remember these analysis issues: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis issues in memory: {e}")
    
    async def _store_analysis_success(self, result: Dict, context: Dict):
        """Store successful analysis patterns in memory"""
        memory_content = {
            "type": "analysis_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "concept_schemes_count": len(result.get('concept_schemes', [])),
            "compliance_frameworks_count": len(result.get('compliance_frameworks', [])),
            "cross_references_count": len(result.get('cross_references', [])),
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store successful analysis pattern for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful analysis: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis success in memory: {e}")
    
    async def _store_analysis_failure(self, error: str, context: Dict):
        """Store analysis failures in memory"""
        memory_content = {
            "type": "analysis_failure",
            "timestamp": datetime.now().isoformat(),
            "error": error,
            "context": context,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store analysis failure for {self.name} agent"},
                {"role": "user", "content": f"Remember this failure to learn from: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store analysis failure in memory: {e}")
    
    def _get_empty_analysis_result(self) -> Dict:
        """Return empty but valid analysis result structure"""
        return {
            "concept_schemes": [],
            "taxonomic_hierarchies": {},
            "compliance_frameworks": [],
            "cross_references": [],
            "logical_axioms": [],
            "semantic_mappings": {"dpv_alignments": [], "prov_alignments": []},
            "data_domains": ["storage", "usage", "movement", "privacy", "security", "access"],
            "quality_metrics": {
                "total_concepts": 0,
                "max_hierarchy_depth": 0,
                "cross_references_count": 0,
                "axioms_count": 0,
                "concept_schemes_count": 0,
                "compliance_frameworks_count": 0,
                "orphaned_concepts": ["analysis_failed"]
            }
        }

class EnhancedOntologyBuilderAgent(ReActAgent):
    """Enhanced ReAct agent for building sophisticated machine-readable OWL ontologies with formal semantics"""
    
    def __init__(self, memory_store: InMemoryStore):
        super().__init__(None, "OntologyBuilder", memory_store)
        self.ns = EnhancedOntologyNamespaces()
        
        # Legal ontology design patterns from research
        self.legal_patterns = {
            'obligation_pattern': {
                'description': 'Pattern for legal obligations with deontic modality',
                'axioms': ['hasObligation some (hasBearer some LegalAgent)', 'hasObligation some (hasObject some LegalObject)']
            },
            'permission_pattern': {
                'description': 'Pattern for legal permissions',
                'axioms': ['hasPermission some (hasBearer some LegalAgent)', 'hasPermission some (hasCondition some Condition)']
            },
            'prohibition_pattern': {
                'description': 'Pattern for legal prohibitions',
                'axioms': ['hasProhibition some (hasBearer some LegalAgent)', 'hasProhibition some (hasSanction some Sanction)']
            },
            'role_pattern': {
                'description': 'Pattern for legal roles and relationships',
                'axioms': ['LegalRole subClassOf (playsRole some LegalAgent)', 'hasRole some (hasContext some LegalContext)']
            },
            'temporal_pattern': {
                'description': 'Pattern for temporal aspects of legal rules',
                'axioms': ['TemporalRule subClassOf (hasTemporalExtent some TimeInterval)', 'validDuring some (intersects some TimePoint)']
            }
        }
    
    def build_comprehensive_ontology(self, extraction_result: Dict, taxonomy_result: Dict, context: Dict) -> Graph:
        """Build a sophisticated machine-readable OWL ontology with formal semantics and multi-level hierarchy"""
        
        self.log_reasoning("Building sophisticated OWL ontology with formal semantics and reasoning patterns")
        self.log_reasoning("Implementing: Complex class expressions, SWRL rules, multi-level hierarchies, and domain patterns")
        
        g = Graph()
        g = self.ns.bind_to_graph(g)
        
        # Add SWRL namespace for rules
        SWRL = Namespace("http://www.w3.org/2003/11/swrl#")
        g.bind("swrl", SWRL)
        
        # Safely encode context
        safe_country = self._safe_uri_encode(context['country'])
        safe_jurisdiction = self._safe_uri_encode(context['jurisdiction'])
        safe_organization = self._safe_uri_encode(context['organization'])
        
        self.log_action("ACTION: Creating sophisticated ontology metadata with versioning and imports")
        
        # Enhanced ontology metadata with formal semantics
        ontology_uri = URIRef(f"{self.ns.LEGAL}ontology_{safe_country}_{datetime.now().strftime('%Y%m%d')}")
        g.add((ontology_uri, RDF.type, OWL.Ontology))
        g.add((ontology_uri, DCTERMS.title, Literal(f"Machine-Readable Legal Ontology: {context['country']} Jurisdiction")))
        g.add((ontology_uri, DCTERMS.created, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        g.add((ontology_uri, DCTERMS.description, Literal(f"Formal OWL ontology with complex axioms and reasoning patterns for {context['country']} legal knowledge")))
        g.add((ontology_uri, DCTERMS.creator, Literal("Enhanced Legal Analysis System with ReAct Agents")))
        g.add((ontology_uri, OWL.versionInfo, Literal("2.0.0")))
        g.add((ontology_uri, OWL.versionIRI, URIRef(f"{ontology_uri}_v2")))
        
        # Add formal imports for foundational ontologies
        g.add((ontology_uri, OWL.imports, URIRef("https://w3id.org/dpv")))
        g.add((ontology_uri, OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        g.add((ontology_uri, OWL.imports, URIRef("http://www.w3.org/2006/time")))
        
        self.log_reasoning("REASONING: Creating multi-level class hierarchy with formal axioms")
        
        # Build sophisticated ontology structure
        self.log_action("ACTION: Building multi-level class hierarchy")
        self._build_sophisticated_class_hierarchy(g, context)
        
        self.log_action("ACTION: Creating complex object properties with formal axioms")
        self._create_sophisticated_object_properties(g, extraction_result.get('object_properties', []))
        
        self.log_action("ACTION: Creating data properties with constraints")
        self._create_sophisticated_data_properties(g, extraction_result.get('data_properties', []))
        
        self.log_action("ACTION: Adding formal definitions as complex class expressions")
        self._add_formal_definitions_as_complex_classes(g, extraction_result.get('definitions', []))
        
        self.log_action("ACTION: Building taxonomic hierarchies with subsumption axioms")
        self._build_formal_taxonomic_hierarchies(g, taxonomy_result.get('taxonomic_hierarchies', {}))
        
        self.log_action("ACTION: Adding rules as individuals with comprehensive axioms")
        self._add_rules_with_formal_axioms(g, extraction_result.get('rules', []), context)
        
        self.log_action("ACTION: Creating SWRL rules for automated reasoning")
        self._add_swrl_rules_for_reasoning(g, taxonomy_result.get('logical_axioms', []))
        
        self.log_action("ACTION: Adding compliance frameworks with formal constraints")
        self._add_formal_compliance_frameworks(g, taxonomy_result.get('compliance_frameworks', []))
        
        self.log_action("ACTION: Creating semantic equivalences and disjointness")
        self._add_semantic_equivalences_and_constraints(g, taxonomy_result.get('cross_references', []))
        
        self.log_action("ACTION: Adding legal ontology design patterns")
        self._apply_legal_ontology_patterns(g, extraction_result, taxonomy_result)
        
        self.log_action("ACTION: Creating complex axioms for automated reasoning")
        self._add_complex_reasoning_axioms(g, extraction_result, taxonomy_result)
        
        self.log_reasoning("OBSERVATION: Sophisticated ontology complete with formal semantics")
        self.log_action(f"Sophisticated ontology construction complete with {len(g)} triples and formal reasoning capabilities")
        
        # Store ontology building patterns in memory
        asyncio.create_task(self._store_ontology_success(len(g), context))
        
        return g
    
    def _build_sophisticated_class_hierarchy(self, g: Graph, context: Dict):
        """Build sophisticated multi-level class hierarchy with formal axioms"""
        
        self.log_action("Building sophisticated multi-level class hierarchy")
        
        # Top-level abstract classes with formal definitions
        top_level_classes = {
            "LegalEntity": {
                "definition": "Any entity that has legal standing or recognition within a legal system",
                "equivalent_class": "Thing and (hasLegalStatus some LegalStatus)",
                "children": ["NaturalPerson", "LegalPerson", "PublicBody", "PrivateOrganization"]
            },
            "LegalNorm": {
                "definition": "Any normative statement that creates legal obligations, permissions, or prohibitions",
                "equivalent_class": "Thing and (hasNormativeForce some NormativeForce)",
                "children": ["LegalRule", "LegalPrinciple", "LegalProcedure"]
            },
            "LegalRelation": {
                "definition": "A legally significant relationship between legal entities",
                "equivalent_class": "Thing and (relatesTo some LegalEntity) and (relatesTo some LegalEntity)",
                "children": ["ContractualRelation", "TortRelation", "PropertyRelation", "FamilyRelation"]
            },
            "LegalConcept": {
                "definition": "Abstract legal concept that provides semantic meaning in legal contexts",
                "equivalent_class": "Thing and (isDefinedIn some LegalSource)",
                "children": ["Right", "Duty", "Power", "Liability", "Immunity", "Disability"]
            },
            "LegalProcess": {
                "definition": "A sequence of legal actions or procedures",
                "equivalent_class": "Process and (governedBy some LegalNorm)",
                "children": ["LegalProcedure", "Litigation", "Transaction", "Compliance"]
            },
            "DataManagementActivity": {
                "definition": "Any activity involving the processing, storage, or handling of data",
                "equivalent_class": "Activity and (hasObject some Data)",
                "children": ["DataCollection", "DataStorage", "DataProcessing", "DataTransfer", "DataDeletion"]
            }
        }
        
        for class_name, class_info in top_level_classes.items():
            class_uri = URIRef(f"{self.ns.LEGAL}{class_name}")
            
            # Basic class declaration
            g.add((class_uri, RDF.type, OWL.Class))
            g.add((class_uri, RDFS.label, Literal(class_name)))
            g.add((class_uri, RDFS.comment, Literal(class_info["definition"])))
            g.add((class_uri, SKOS.definition, Literal(class_info["definition"])))
            
            # Add complex class expression (equivalent class)
            if "equivalent_class" in class_info:
                # This would be implemented with complex OWL expressions in a real reasoner
                equiv_comment = f"Equivalent to: {class_info['equivalent_class']}"
                g.add((class_uri, RDFS.comment, Literal(equiv_comment)))
            
            # Build multi-level hierarchy
            for child_class in class_info.get("children", []):
                child_uri = URIRef(f"{self.ns.LEGAL}{child_class}")
                g.add((child_uri, RDF.type, OWL.Class))
                g.add((child_uri, RDFS.subClassOf, class_uri))
                g.add((child_uri, RDFS.label, Literal(child_class)))
                
                # Add more specific subclasses
                self._add_specific_subclasses(g, child_uri, child_class)
        
        # Add disjointness axioms for sibling classes
        self._add_disjointness_axioms(g, top_level_classes)
        
        # Create complex class expressions for legal patterns
        self._create_complex_legal_classes(g, context)
    
    def _add_specific_subclasses(self, g: Graph, parent_uri: URIRef, parent_name: str):
        """Add specific subclasses based on legal domain knowledge"""
        
        specific_hierarchies = {
            "NaturalPerson": ["Adult", "Minor", "DataSubject", "Consumer"],
            "LegalPerson": ["Corporation", "Partnership", "NonProfit", "GovernmentEntity"],
            "LegalRule": ["ObligationRule", "PermissionRule", "ProhibitionRule", "ConstitutiveRule"],
            "Right": ["FundamentalRight", "PropertyRight", "ContractualRight", "PersonalDataRight"],
            "DataProcessing": ["PersonalDataProcessing", "SensitiveDataProcessing", "AutomatedProcessing"],
            "DataCollection": ["DirectCollection", "IndirectCollection", "ConsentBasedCollection"],
            "DataStorage": ["ShortTermStorage", "LongTermStorage", "ArchivalStorage", "BackupStorage"],
            "DataTransfer": ["InternalTransfer", "ThirdPartyTransfer", "CrossBorderTransfer", "PublicDisclosure"]
        }
        
        parent_class_name = parent_name
        if parent_class_name in specific_hierarchies:
            for subclass_name in specific_hierarchies[parent_class_name]:
                subclass_uri = URIRef(f"{self.ns.LEGAL}{subclass_name}")
                g.add((subclass_uri, RDF.type, OWL.Class))
                g.add((subclass_uri, RDFS.subClassOf, parent_uri))
                g.add((subclass_uri, RDFS.label, Literal(subclass_name)))
                
                # Add further specializations for important legal concepts
                if subclass_name == "PersonalDataProcessing":
                    for processing_type in ["Profiling", "AutomatedDecisionMaking", "DataMining", "Analytics"]:
                        processing_uri = URIRef(f"{self.ns.LEGAL}{processing_type}")
                        g.add((processing_uri, RDF.type, OWL.Class))
                        g.add((processing_uri, RDFS.subClassOf, subclass_uri))
                        g.add((processing_uri, RDFS.label, Literal(processing_type)))
    
    def _create_complex_legal_classes(self, g: Graph, context: Dict):
        """Create complex class expressions for sophisticated legal reasoning"""
        
        complex_classes = {
            "DataControllerWithLawfulBasis": {
                "definition": "A data controller that has established a lawful basis for processing",
                "expression": "DataController and (hasLawfulBasis some LawfulBasis)"
            },
            "SensitiveDataProcessingWithConsent": {
                "definition": "Processing of sensitive data that requires explicit consent",
                "expression": "DataProcessing and (processesData some SensitivePersonalData) and (hasLawfulBasis value ExplicitConsent)"
            },
            "CrossBorderTransferWithAdequacy": {
                "definition": "Cross-border data transfer to a country with adequacy decision",
                "expression": "CrossBorderTransfer and (hasDestination some (Country and hasAdequacyDecision))"
            },
            "HighRiskProcessingActivity": {
                "definition": "Data processing activity that requires data protection impact assessment",
                "expression": "DataProcessing and (hasRiskLevel value HighRisk) and (requiresDPIA value true)"
            },
            "AutomatedDecisionWithHumanReview": {
                "definition": "Automated decision-making with meaningful human involvement",
                "expression": "AutomatedDecisionMaking and (hasHumanReview some HumanReview) and (allowsContesting value true)"
            }
        }
        
        for class_name, class_info in complex_classes.items():
            class_uri = URIRef(f"{self.ns.LEGAL}{class_name}")
            g.add((class_uri, RDF.type, OWL.Class))
            g.add((class_uri, RDFS.label, Literal(class_name)))
            g.add((class_uri, RDFS.comment, Literal(class_info["definition"])))
            
            # Add complex expression as comment (in real implementation, this would be proper OWL)
            g.add((class_uri, RDFS.comment, Literal(f"Complex Expression: {class_info['expression']}")))
    
    def _create_sophisticated_object_properties(self, g: Graph, object_properties: List[Dict]):
        """Create sophisticated object properties with formal axioms and constraints"""
        
        # Enhanced legal object properties with formal semantics
        enhanced_properties = {
            "hasLegalBasis": {
                "domain": ["DataProcessing", "LegalAction"],
                "range": ["LawfulBasis", "LegalJustification"],
                "characteristics": ["functional"],
                "axioms": ["DataProcessing subClassOf (hasLegalBasis some LawfulBasis)"],
                "inverse": "isLegalBasisFor"
            },
            "exercisesRight": {
                "domain": ["LegalEntity"],
                "range": ["Right"],
                "characteristics": [],
                "axioms": ["LegalEntity and (exercisesRight some FundamentalRight) subClassOf ProtectedEntity"],
                "sub_property_of": ["hasLegalRelation"]
            },
            "hasObligation": {
                "domain": ["LegalEntity"],
                "range": ["Obligation"],
                "characteristics": [],
                "axioms": ["DataController subClassOf (hasObligation some DataProtectionObligation)"],
                "inverse": "isObligationOf"
            },
            "processesData": {
                "domain": ["DataProcessing"],
                "range": ["Data"],
                "characteristics": [],
                "axioms": ["PersonalDataProcessing subClassOf (processesData some PersonalData)"],
                "sub_property_of": ["involves"]
            },
            "transfersDataTo": {
                "domain": ["DataTransfer"],
                "range": ["LegalEntity", "Jurisdiction"],
                "characteristics": [],
                "axioms": ["CrossBorderTransfer subClassOf (transfersDataTo some ThirdCountry)"],
                "chain_axiom": "transfersDataTo o locatedIn subPropertyOf crossesBorderTo"
            },
            "hasTemporalExtent": {
                "domain": ["LegalRule", "LegalRelation"],
                "range": ["TemporalEntity"],
                "characteristics": ["functional"],
                "axioms": ["LegalRule subClassOf (hasTemporalExtent some TimeInterval)"]
            },
            "hasJurisdictionalScope": {
                "domain": ["LegalRule"],
                "range": ["Jurisdiction"],
                "characteristics": [],
                "axioms": ["NationalLaw subClassOf (hasJurisdictionalScope some Nation)"]
            },
            "derivedFrom": {
                "domain": ["LegalRule"],
                "range": ["LegalSource"],
                "characteristics": ["transitive"],
                "axioms": ["LegalRule subClassOf (derivedFrom some AuthoritativeLegalSource)"]
            }
        }
        
        # Add extracted properties
        all_properties = {}
        all_properties.update(enhanced_properties)
        
        # Add extracted object properties
        for prop in object_properties:
            prop_name = prop.get('property_name', '')
            if prop_name not in all_properties:
                all_properties[prop_name] = {
                    "domain": prop.get('domain', []),
                    "range": prop.get('range', []),
                    "characteristics": prop.get('characteristics', []),
                    "definition": prop.get('definition', ''),
                    "inverse": prop.get('inverse_property')
                }
        
        for prop_name, prop_info in all_properties.items():
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{prop_name}")
            
            g.add((prop_uri, RDF.type, OWL.ObjectProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop_info.get('definition', f"Object property: {prop_name}"))))
            
            # Add domain and range constraints
            for domain_class in prop_info.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{domain_class}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            for range_class in prop_info.get('range', []):
                range_uri = URIRef(f"{self.ns.LEGAL}{range_class}")
                g.add((prop_uri, RDFS.range, range_uri))
            
            # Add property characteristics
            characteristics = prop_info.get('characteristics', [])
            if 'functional' in characteristics:
                g.add((prop_uri, RDF.type, OWL.FunctionalProperty))
            if 'transitive' in characteristics:
                g.add((prop_uri, RDF.type, OWL.TransitiveProperty))
            if 'symmetric' in characteristics:
                g.add((prop_uri, RDF.type, OWL.SymmetricProperty))
            if 'irreflexive' in characteristics:
                g.add((prop_uri, RDF.type, OWL.IrreflexiveProperty))
            
            # Add inverse properties
            inverse_prop = prop_info.get('inverse')
            if inverse_prop:
                inverse_uri = URIRef(f"{self.ns.PROPERTIES}{inverse_prop}")
                g.add((prop_uri, OWL.inverseOf, inverse_uri))
            
            # Add sub-property relationships
            sub_prop_of = prop_info.get('sub_property_of')
            if sub_prop_of:
                for super_prop in sub_prop_of if isinstance(sub_prop_of, list) else [sub_prop_of]:
                    super_prop_uri = URIRef(f"{self.ns.PROPERTIES}{super_prop}")
                    g.add((prop_uri, RDFS.subPropertyOf, super_prop_uri))
        
        logger.info(f"Added {len(all_properties)} sophisticated object properties with formal constraints")
    
    def _create_sophisticated_data_properties(self, g: Graph, data_properties: List[Dict]):
        """Create sophisticated data properties with validation constraints"""
        
        # Enhanced data properties with formal constraints
        enhanced_data_props = {
            "hasEffectiveDate": {
                "domain": ["LegalRule", "LegalDocument"],
                "range": "xsd:date",
                "constraints": ["required", "future_date_allowed"],
                "cardinality": "exactly_one"
            },
            "hasExpirationDate": {
                "domain": ["LegalRule", "Consent", "Authorization"],
                "range": "xsd:date",
                "constraints": ["optional", "must_be_after_effective_date"],
                "cardinality": "max_one"
            },
            "hasRiskLevel": {
                "domain": ["DataProcessing", "DataTransfer"],
                "range": "xsd:string",
                "constraints": ["required", "enum:low,medium,high,very_high"],
                "cardinality": "exactly_one"
            },
            "requiresDPIA": {
                "domain": ["DataProcessing"],
                "range": "xsd:boolean",
                "constraints": ["required", "default:false"],
                "cardinality": "exactly_one"
            },
            "hasRetentionPeriod": {
                "domain": ["DataStorage", "PersonalData"],
                "range": "xsd:duration",
                "constraints": ["required_for_personal_data", "minimum:P1D"],
                "cardinality": "exactly_one"
            },
            "hasMaxPenalty": {
                "domain": ["LegalRule", "Violation"],
                "range": "xsd:decimal",
                "constraints": ["optional", "minimum:0"],
                "cardinality": "max_one"
            },
            "hasLegalReference": {
                "domain": ["LegalRule", "LegalConcept"],
                "range": "xsd:string",
                "constraints": ["required", "pattern:^(Art|Article|Sec|Section)\s+\d+.*"],
                "cardinality": "min_one"
            },
            "hasConfidenceScore": {
                "domain": ["ExtractedRule", "ExtractedConcept"],
                "range": "xsd:float",
                "constraints": ["required", "minimum:0.0", "maximum:1.0"],
                "cardinality": "exactly_one"
            }
        }
        
        # Add extracted data properties
        all_data_props = {}
        all_data_props.update(enhanced_data_props)
        
        for prop in data_properties:
            prop_name = prop.get('property_name', '')
            if prop_name not in all_data_props:
                all_data_props[prop_name] = {
                    "domain": prop.get('domain', []),
                    "range": prop.get('range', 'xsd:string'),
                    "definition": prop.get('definition', ''),
                    "cardinality": prop.get('cardinality', 'unrestricted')
                }
        
        for prop_name, prop_info in all_data_props.items():
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{prop_name}")
            
            g.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop_info.get('definition', f"Data property: {prop_name}"))))
            
            # Add domain constraints
            for domain_class in prop_info.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{domain_class}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            # Add range (datatype) constraints
            range_type = prop_info.get('range', 'xsd:string')
            if range_type.startswith('xsd:'):
                range_uri = getattr(XSD, range_type.split(':')[1], XSD.string)
                g.add((prop_uri, RDFS.range, range_uri))
            
            # Add cardinality constraints as comments (would be proper restrictions in full OWL)
            cardinality = prop_info.get('cardinality', 'unrestricted')
            if cardinality != 'unrestricted':
                g.add((prop_uri, RDFS.comment, Literal(f"Cardinality: {cardinality}")))
            
            # Add validation constraints as comments
            constraints = prop_info.get('constraints', [])
            if constraints:
                constraint_text = f"Validation constraints: {', '.join(constraints)}"
                g.add((prop_uri, RDFS.comment, Literal(constraint_text)))
        
        logger.info(f"Added {len(all_data_props)} sophisticated data properties with validation constraints")
    
    def _add_core_ontology_structure(self, g: Graph, context: Dict):
        """Add core ontology classes and structure"""
        
        self.log_action("Adding core ontology structure")
        
        # Core top-level classes
        core_classes = [
            ("LegalEntity", "An entity with legal standing or recognition"),
            ("LegalRule", "A rule, regulation, or legal provision"),
            ("ProcessingActivity", "An activity involving data processing"),
            ("DataCategory", "A classification of data"),
            ("LegalBasis", "Legal justification for an action"),
            ("Right", "A legal right or entitlement"),
            ("Obligation", "A legal duty or obligation"),
            ("TechnicalMeasure", "Technical safeguard or control"),
            ("OrganizationalMeasure", "Organizational safeguard or procedure"),
            ("JurisdictionEntity", "Entity representing legal jurisdiction"),
            ("ComplianceRequirement", "Requirement for legal compliance")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.LEGAL}{class_name}")
            g.add((class_uri, RDF.type, OWL.Class))
            g.add((class_uri, RDF.type, SKOS.Concept))
            g.add((class_uri, RDFS.label, Literal(class_name)))
            g.add((class_uri, RDFS.comment, Literal(definition)))
            g.add((class_uri, SKOS.definition, Literal(definition)))
        
        # Country/Jurisdiction/Organization as individuals
        safe_country = self._safe_uri_encode(context['country'])
        safe_jurisdiction = self._safe_uri_encode(context['jurisdiction'])
        safe_organization = self._safe_uri_encode(context['organization'])
        
        country_uri = URIRef(f"{self.ns.LEGAL}Country_{safe_country}")
        jurisdiction_uri = URIRef(f"{self.ns.LEGAL}Jurisdiction_{safe_jurisdiction}")
        org_uri = URIRef(f"{self.ns.LEGAL}Organization_{safe_organization}")
        
        g.add((country_uri, RDF.type, URIRef(f"{self.ns.LEGAL}JurisdictionEntity")))
        g.add((country_uri, RDFS.label, Literal(context['country'])))
        g.add((country_uri, SKOS.prefLabel, Literal(context['country'])))
        
        g.add((jurisdiction_uri, RDF.type, URIRef(f"{self.ns.LEGAL}JurisdictionEntity")))
        g.add((jurisdiction_uri, RDFS.label, Literal(context['jurisdiction'])))
        g.add((jurisdiction_uri, URIRef(f"{self.ns.LEGAL}isPartOf"), country_uri))
        
        g.add((org_uri, RDF.type, URIRef(f"{self.ns.LEGAL}LegalEntity")))
        g.add((org_uri, RDFS.label, Literal(context['organization'])))
    
    def _add_definitions_as_ontology_elements(self, g: Graph, definitions: List[Dict]):
        """Add extracted definitions as ontology classes, properties, or individuals"""
        
        self.log_action(f"Adding {len(definitions)} definitions to ontology")
        
        for definition in definitions:
            concept_name = definition.get('concept', '')
            uri_suffix = definition.get('uri_suffix', self._safe_uri_encode(concept_name))
            concept_type = definition.get('type', 'class')
            
            if concept_type == 'class':
                concept_uri = URIRef(f"{self.ns.LEGAL}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.Class))
                g.add((concept_uri, RDF.type, SKOS.Concept))
            elif concept_type == 'object_property':
                concept_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.ObjectProperty))
            elif concept_type == 'data_property':
                concept_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.DatatypeProperty))
            else:  # individual
                concept_uri = URIRef(f"{self.ns.LEGAL}{uri_suffix}")
                g.add((concept_uri, RDF.type, OWL.NamedIndividual))
            
            # Add labels and definitions
            g.add((concept_uri, RDFS.label, Literal(concept_name)))
            if definition.get('formal_definition'):
                g.add((concept_uri, SKOS.definition, Literal(definition['formal_definition'])))
            if definition.get('informal_description'):
                g.add((concept_uri, RDFS.comment, Literal(definition['informal_description'])))
            
            # Add alternative labels
            for alt_label in definition.get('synonyms', []):
                g.add((concept_uri, SKOS.altLabel, Literal(alt_label)))
            
            # Add hierarchical relationships
            for broader in definition.get('broader_concepts', []):
                broader_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(broader)}")
                if concept_type == 'class':
                    g.add((concept_uri, RDFS.subClassOf, broader_uri))
                g.add((concept_uri, SKOS.broader, broader_uri))
            
            # Add examples
            for example in definition.get('examples', []):
                g.add((concept_uri, SKOS.example, Literal(example)))
    
    def _add_object_properties(self, g: Graph, object_properties: List[Dict]):
        """Add object properties with detailed constraints"""
        
        self.log_action(f"Adding {len(object_properties)} object properties")
        
        for prop in object_properties:
            prop_name = prop.get('property_name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            g.add((prop_uri, RDF.type, OWL.ObjectProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain and range
            for domain_class in prop.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(domain_class)}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            for range_class in prop.get('range', []):
                range_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(range_class)}")
                g.add((prop_uri, RDFS.range, range_uri))
            
            # Add characteristics
            characteristics = prop.get('characteristics', [])
            if 'functional' in characteristics:
                g.add((prop_uri, RDF.type, OWL.FunctionalProperty))
            if 'transitive' in characteristics:
                g.add((prop_uri, RDF.type, OWL.TransitiveProperty))
            if 'symmetric' in characteristics:
                g.add((prop_uri, RDF.type, OWL.SymmetricProperty))
            
            # Add sub-property relationships
            for super_prop in prop.get('sub_property_of', []):
                super_prop_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(super_prop)}")
                g.add((prop_uri, RDFS.subPropertyOf, super_prop_uri))
            
            # Add inverse property
            if prop.get('inverse_property'):
                inverse_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(prop['inverse_property'])}")
                g.add((prop_uri, OWL.inverseOf, inverse_uri))
    
    def _add_data_properties(self, g: Graph, data_properties: List[Dict]):
        """Add data properties with constraints"""
        
        self.log_action(f"Adding {len(data_properties)} data properties")
        
        for prop in data_properties:
            prop_name = prop.get('property_name', '')
            uri_suffix = prop.get('uri_suffix', self._safe_uri_encode(prop_name))
            prop_uri = URIRef(f"{self.ns.PROPERTIES}{uri_suffix}")
            
            g.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            g.add((prop_uri, RDFS.label, Literal(prop_name)))
            g.add((prop_uri, RDFS.comment, Literal(prop.get('definition', ''))))
            
            # Add domain
            for domain_class in prop.get('domain', []):
                domain_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(domain_class)}")
                g.add((prop_uri, RDFS.domain, domain_uri))
            
            # Add range (datatype)
            range_type = prop.get('range', 'xsd:string')
            if range_type.startswith('xsd:'):
                range_uri = getattr(XSD, range_type.split(':')[1])
                g.add((prop_uri, RDFS.range, range_uri))
    
    def _add_taxonomic_hierarchies(self, g: Graph, hierarchies: Dict):
        """Add taxonomic hierarchies using SKOS"""
        
        self.log_action(f"Adding {len(hierarchies)} taxonomic hierarchies")
        
        for hierarchy_name, hierarchy_data in hierarchies.items():
            root_concept = hierarchy_data.get('root', hierarchy_name)
            self._add_hierarchy_recursive(g, hierarchy_data.get('hierarchy', {}), None)
    
    def _add_hierarchy_recursive(self, g: Graph, hierarchy: Dict, parent_uri: Optional[URIRef]):
        """Recursively add hierarchical concepts"""
        
        for concept_name, concept_data in hierarchy.items():
            safe_name = self._safe_uri_encode(concept_name)
            concept_uri = URIRef(f"{self.ns.LEGAL}{safe_name}")
            
            g.add((concept_uri, RDF.type, OWL.Class))
            g.add((concept_uri, RDF.type, SKOS.Concept))
            g.add((concept_uri, RDFS.label, Literal(concept_name)))
            
            if isinstance(concept_data, dict):
                if concept_data.get('definition'):
                    g.add((concept_uri, SKOS.definition, Literal(concept_data['definition'])))
                
                if parent_uri:
                    g.add((concept_uri, RDFS.subClassOf, parent_uri))
                    g.add((concept_uri, SKOS.broader, parent_uri))
                
                # Recursively add children
                children = concept_data.get('children', {})
                if children:
                    self._add_hierarchy_recursive(g, children, concept_uri)
    
    def _add_concept_schemes(self, g: Graph, concept_schemes: List[Dict]):
        """Add SKOS concept schemes"""
        
        self.log_action(f"Adding {len(concept_schemes)} concept schemes")
        
        for scheme in concept_schemes:
            scheme_id = scheme.get('scheme_id', '')
            scheme_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(scheme_id)}")
            
            g.add((scheme_uri, RDF.type, SKOS.ConceptScheme))
            g.add((scheme_uri, RDFS.label, Literal(scheme.get('title', scheme_id))))
            g.add((scheme_uri, RDFS.comment, Literal(scheme.get('description', ''))))
            
            # Add top concepts
            for top_concept in scheme.get('top_concepts', []):
                top_concept_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(top_concept)}")
                g.add((scheme_uri, SKOS.hasTopConcept, top_concept_uri))
    
    def _add_rules_as_individuals(self, g: Graph, rules: List[Dict], context: Dict):
        """Add rules as named individuals with detailed properties"""
        
        self.log_action(f"Adding {len(rules)} rules as individuals")
        
        for rule in rules:
            rule_id = rule.get('id', str(uuid.uuid4()))
            safe_rule_id = self._safe_uri_encode(rule_id)
            rule_uri = URIRef(f"{self.ns.LEGAL}Rule_{safe_rule_id}")
            
            # Basic rule properties
            g.add((rule_uri, RDF.type, OWL.NamedIndividual))
            g.add((rule_uri, RDF.type, URIRef(f"{self.ns.LEGAL}LegalRule")))
            
            if rule.get('type'):
                rule_type_uri = URIRef(f"{self.ns.LEGAL}{rule['type'].title()}Rule")
                g.add((rule_uri, RDF.type, rule_type_uri))
            
            # Add detailed rule components
            subject_info = rule.get('subject', {})
            if isinstance(subject_info, dict):
                if subject_info.get('entity'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasSubjectEntity"), Literal(subject_info['entity'])))
                if subject_info.get('type'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasSubjectType"), Literal(subject_info['type'])))
            
            # Add temporal aspects
            temporal_aspects = rule.get('temporal_aspects', {})
            if temporal_aspects:
                if temporal_aspects.get('start_condition'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasStartCondition"), Literal(temporal_aspects['start_condition'])))
                if temporal_aspects.get('duration'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasDuration"), Literal(temporal_aspects['duration'])))
            
            # Add jurisdiction scope
            jurisdiction_scope = rule.get('jurisdiction_scope', {})
            if jurisdiction_scope:
                if jurisdiction_scope.get('geographic'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasGeographicScope"), Literal(jurisdiction_scope['geographic'])))
                if jurisdiction_scope.get('enforcement_authority'):
                    g.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasEnforcementAuthority"), Literal(jurisdiction_scope['enforcement_authority'])))
            
            # Add provenance
            g.add((rule_uri, self.ns.PROV.wasDerivedFrom, Literal(rule.get('original_text', ''))))
            g.add((rule_uri, self.ns.PROV.wasGeneratedBy, URIRef(f"{self.ns.LEGAL}ExtractionProcess")))
            g.add((rule_uri, self.ns.PROV.generatedAtTime, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_logical_axioms(self, g: Graph, axioms: List[Dict]):
        """Add logical axioms and constraints"""
        
        self.log_action(f"Adding {len(axioms)} logical axioms")
        
        for axiom in axioms:
            axiom_type = axiom.get('axiom_type', '')
            subject_class = axiom.get('subject', '')
            property_name = axiom.get('property', '')
            constraint = axiom.get('constraint', '')
            
            if axiom_type == 'universal_restriction' and constraint == 'exactly_one':
                subject_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(subject_class)}")
                property_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(property_name)}")
                
                # Create cardinality restriction
                restriction = BNode()
                g.add((restriction, RDF.type, OWL.Restriction))
                g.add((restriction, OWL.onProperty, property_uri))
                g.add((restriction, OWL.cardinality, Literal(1)))
                g.add((subject_uri, RDFS.subClassOf, restriction))
    
    def _add_compliance_frameworks(self, g: Graph, frameworks: List[Dict]):
        """Add compliance frameworks"""
        
        self.log_action(f"Adding {len(frameworks)} compliance frameworks")
        
        for framework in frameworks:
            framework_id = framework.get('framework_id', '')
            framework_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(framework_id)}")
            
            g.add((framework_uri, RDF.type, URIRef(f"{self.ns.LEGAL}ComplianceFramework")))
            g.add((framework_uri, RDFS.label, Literal(framework.get('title', framework_id))))
            g.add((framework_uri, RDFS.comment, Literal(framework.get('description', ''))))
    
    def _add_cross_references(self, g: Graph, cross_refs: List[Dict]):
        """Add cross-references between concepts"""
        
        self.log_action(f"Adding {len(cross_refs)} cross-references")
        
        for ref in cross_refs:
            source_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(ref.get('source_concept', ''))}")
            target_uri = URIRef(f"{self.ns.LEGAL}{self._safe_uri_encode(ref.get('target_concept', ''))}")
            relationship = ref.get('relationship', 'related')
            
            if relationship == 'must_have':
                property_uri = URIRef(f"{self.ns.RELATIONS}mustHave")
            else:
                property_uri = SKOS.related
            
            g.add((source_uri, property_uri, target_uri))
    
    def _add_vocabulary_mappings(self, g: Graph):
        """Add mappings to external vocabularies (DPV, PROV-O)"""
        
        self.log_action("Adding vocabulary mappings to DPV and PROV-O")
        
        # Add imports
        g.add((URIRef(f"{self.ns.LEGAL}"), OWL.imports, URIRef("https://w3id.org/dpv")))
        g.add((URIRef(f"{self.ns.LEGAL}"), OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        
        # Map core concepts to DPV
        dpv_mappings = {
            "PersonalData": "dpv:PersonalData",
            "DataController": "dpv:DataController",
            "DataProcessor": "dpv:DataProcessor",
            "DataSubject": "dpv:DataSubject",
            "ProcessingActivity": "dpv:Processing",
            "LegalBasis": "dpv:LegalBasis",
            "Consent": "dpv:Consent"
        }
        
        for local_concept, dpv_concept in dpv_mappings.items():
            local_uri = URIRef(f"{self.ns.LEGAL}{local_concept}")
            dpv_uri = URIRef(f"{self.ns.DPV}{dpv_concept.split(':')[1]}")
            g.add((local_uri, SKOS.exactMatch, dpv_uri))
    
    async def _store_ontology_success(self, triple_count: int, context: Dict):
        """Store ontology building success in memory"""
        memory_content = {
            "type": "ontology_success",
            "timestamp": datetime.now().isoformat(),
            "context": context,
            "triple_count": triple_count,
            "agent": self.name
        }
        
        try:
            conversation = [
                {"role": "system", "content": f"Store ontology building success for {self.name} agent"},
                {"role": "user", "content": f"Remember this successful ontology construction: {json.dumps(memory_content)}"}
            ]
            await self.memory_manager(conversation)
        except Exception as e:
            logger.warning(f"Failed to store ontology success in memory: {e}")

# ====================================
# WEB INTERFACE FOR QUERYING
# ====================================

class LegalKnowledgeQueryInterface:
    """Web interface for querying the legal knowledge graph"""
    
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.knowledge_graph = Graph()
        
        if FLASK_AVAILABLE:
            self.app = Flask(__name__)
            CORS(self.app)
            self._setup_routes()
        else:
            self.app = None
            logger.warning("Flask not available - query interface disabled")
    
    def _setup_routes(self):
        """Setup Flask routes for the web interface"""
        
        @self.app.route('/')
        def index():
            return render_template_string(self._get_index_template())
        
        @self.app.route('/api/sparql', methods=['POST'])
        def sparql_query():
            try:
                query = request.json.get('query', '')
                results = self.execute_sparql_query(query)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/search', methods=['POST'])
        def semantic_search():
            try:
                query_text = request.json.get('query', '')
                country = request.json.get('country', '')
                results = self.execute_semantic_search(query_text, country)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/ontology/stats')
        def ontology_stats():
            try:
                stats = self.get_ontology_statistics()
                return jsonify(stats)
            except Exception as e:
                return jsonify({"error": str(e)})
        
        @self.app.route('/api/concept/<concept_id>')
        def get_concept_details(concept_id):
            try:
                details = self.get_concept_details(concept_id)
                return jsonify(details)
            except Exception as e:
                return jsonify({"error": str(e)})
    
    def execute_sparql_query(self, query: str) -> List[Dict]:
        """Execute SPARQL query against the knowledge graph"""
        try:
            results = self.knowledge_graph.query(query)
            result_list = []
            
            for row in results:
                row_dict = {}
                for var in results.vars:
                    value = row[var]
                    if value:
                        row_dict[str(var)] = str(value)
                result_list.append(row_dict)
            
            return result_list
        except Exception as e:
            raise Exception(f"SPARQL query failed: {e}")
    
    def execute_semantic_search(self, query_text: str, country: str = "") -> List[Dict]:
        """Execute semantic search using Elasticsearch"""
        try:
            # Build Elasticsearch query
            es_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "multi_match": {
                                    "query": query_text,
                                    "fields": ["content", "concepts", "actors", "objects"]
                                }
                            }
                        ]
                    }
                },
                "size": 20
            }
            
            if country:
                es_query["query"]["bool"]["must"].append({
                    "term": {"country.keyword": country}
                })
            
            # Execute search
            response = self.orchestrator.es_client.client.search(
                index=Config.ELASTICSEARCH_INDEX,
                body=es_query
            )
            
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                results.append({
                    "document_id": source.get('document_id'),
                    "country": source.get('country'),
                    "jurisdiction": source.get('jurisdiction'),
                    "organization": source.get('organization'),
                    "rules_count": len(source.get('rules', [])),
                    "concepts": source.get('concepts', [])[:10],  # Limit for display
                    "score": hit['_score']
                })
            
            return results
        except Exception as e:
            raise Exception(f"Semantic search failed: {e}")
    
    def get_ontology_statistics(self) -> Dict:
        """Get statistics about the ontology"""
        try:
            stats = {
                "total_triples": len(self.knowledge_graph),
                "total_classes": len(list(self.knowledge_graph.subjects(RDF.type, OWL.Class))),
                "total_properties": len(list(self.knowledge_graph.subjects(RDF.type, OWL.ObjectProperty))) + 
                                  len(list(self.knowledge_graph.subjects(RDF.type, OWL.DatatypeProperty))),
                "total_individuals": len(list(self.knowledge_graph.subjects(RDF.type, OWL.NamedIndividual))),
                "namespaces": [str(ns) for prefix, ns in self.knowledge_graph.namespaces()]
            }
            return stats
        except Exception as e:
            return {"error": str(e)}
    
    def get_concept_details(self, concept_id: str) -> Dict:
        """Get detailed information about a specific concept"""
        try:
            # Safely encode concept ID
            safe_concept_id = concept_id.replace(' ', '_').replace('/', '_')
            concept_uri = URIRef(f"https://legal-rules.org/ontology#{safe_concept_id}")
            
            details = {
                "concept_id": concept_id,
                "labels": [],
                "definitions": [],
                "broader_concepts": [],
                "narrower_concepts": [],
                "related_concepts": [],
                "properties": []
            }
            
            # Get labels
            for label in self.knowledge_graph.objects(concept_uri, RDFS.label):
                details["labels"].append(str(label))
            
            # Get definitions
            for definition in self.knowledge_graph.objects(concept_uri, SKOS.definition):
                details["definitions"].append(str(definition))
            
            # Get broader concepts
            for broader in self.knowledge_graph.objects(concept_uri, SKOS.broader):
                details["broader_concepts"].append(str(broader))
            
            # Get narrower concepts
            for narrower in self.knowledge_graph.subjects(SKOS.broader, concept_uri):
                details["narrower_concepts"].append(str(narrower))
            
            return details
        except Exception as e:
            return {"error": str(e)}
    
    def load_ontology(self, ontology_graph: Graph):
        """Load an ontology graph for querying"""
        self.knowledge_graph = ontology_graph
        logger.info(f"Loaded ontology with {len(ontology_graph)} triples")
    
    def _get_index_template(self) -> str:
        """Return HTML template for the web interface"""
        return """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Legal Knowledge Graph Query Interface</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 30px; color: #333; }
        .query-section { margin-bottom: 30px; }
        .query-area { width: 100%; height: 150px; font-family: monospace; border: 1px solid #ddd; border-radius: 4px; padding: 10px; }
        .search-input { width: 100%; padding: 10px; border: 1px solid #ddd; border-radius: 4px; margin-bottom: 10px; }
        .button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; margin-right: 10px; }
        .button:hover { background: #0056b3; }
        .results { border: 1px solid #ddd; border-radius: 4px; padding: 15px; margin-top: 20px; max-height: 400px; overflow-y: auto; }
        .result-item { border-bottom: 1px solid #eee; padding: 10px 0; }
        .error { color: #dc3545; background: #f8d7da; padding: 10px; border-radius: 4px; margin: 10px 0; }
        .success { color: #155724; background: #d4edda; padding: 10px; border-radius: 4px; margin: 10px 0; }
        .tab { display: inline-block; padding: 10px 20px; background: #e9ecef; margin-right: 5px; cursor: pointer; border-radius: 4px 4px 0 0; }
        .tab.active { background: #007bff; color: white; }
        .tab-content { display: none; }
        .tab-content.active { display: block; }
        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
        .stat-card { background: #f8f9fa; padding: 15px; border-radius: 4px; text-align: center; }
        .stat-number { font-size: 24px; font-weight: bold; color: #007bff; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Enhanced Legal Knowledge Graph Query Interface</h1>
            <p>Query and explore machine-readable legal rules and concepts with ReAct agents and LangMem memory</p>
        </div>
        
        <div class="tabs">
            <div class="tab active" onclick="showTab('sparql')">SPARQL Query</div>
            <div class="tab" onclick="showTab('search')">Semantic Search</div>
            <div class="tab" onclick="showTab('stats')">Ontology Statistics</div>
        </div>
        
        <div id="sparql" class="tab-content active">
            <div class="query-section">
                <h3>SPARQL Query</h3>
                <textarea id="sparqlQuery" class="query-area" placeholder="Enter your SPARQL query here...
Example:
PREFIX legal: <https://legal-rules.org/ontology#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>

SELECT ?concept ?label ?definition WHERE {
  ?concept a legal:LegalRule ;
           rdfs:label ?label ;
           skos:definition ?definition .
} LIMIT 10"></textarea>
                <button class="button" onclick="executeSparqlQuery()">Execute Query</button>
                <button class="button" onclick="clearResults()">Clear Results</button>
            </div>
        </div>
        
        <div id="search" class="tab-content">
            <div class="query-section">
                <h3>Semantic Search</h3>
                <input type="text" id="searchQuery" class="search-input" placeholder="Enter search terms (e.g., 'data protection', 'consent', 'GDPR')">
                <input type="text" id="countryFilter" class="search-input" placeholder="Filter by country (optional)">
                <button class="button" onclick="executeSemanticSearch()">Search</button>
                <button class="button" onclick="clearResults()">Clear Results</button>
            </div>
        </div>
        
        <div id="stats" class="tab-content">
            <div class="query-section">
                <h3>Ontology Statistics</h3>
                <button class="button" onclick="loadStatistics()">Load Statistics</button>
                <div id="statsDisplay"></div>
            </div>
        </div>
        
        <div id="results" class="results" style="display: none;">
            <h3>Results</h3>
            <div id="resultContent"></div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            document.querySelectorAll('.tab-content').forEach(tab => tab.classList.remove('active'));
            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
        
        async function executeSparqlQuery() {
            const query = document.getElementById('sparqlQuery').value;
            if (!query.trim()) {
                showError('Please enter a SPARQL query');
                return;
            }
            
            try {
                const response = await fetch('/api/sparql', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'sparql');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function executeSemanticSearch() {
            const query = document.getElementById('searchQuery').value;
            const country = document.getElementById('countryFilter').value;
            
            if (!query.trim()) {
                showError('Please enter search terms');
                return;
            }
            
            try {
                const response = await fetch('/api/search', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query, country })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'search');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function loadStatistics() {
            try {
                const response = await fetch('/api/ontology/stats');
                const data = await response.json();
                
                if (data.error) {
                    showError(data.error);
                } else {
                    displayStatistics(data);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        function displayResults(results, type) {
            const resultsDiv = document.getElementById('results');
            const contentDiv = document.getElementById('resultContent');
            
            if (results.length === 0) {
                contentDiv.innerHTML = '<p>No results found.</p>';
            } else if (type === 'sparql') {
                let html = '<table border="1" style="width: 100%; border-collapse: collapse;"><thead><tr>';
                
                // Get column headers from first result
                if (results.length > 0) {
                    Object.keys(results[0]).forEach(key => {
                        html += `<th style="padding: 8px; background: #f8f9fa;">${key}</th>`;
                    });
                    html += '</tr></thead><tbody>';
                    
                    results.forEach(row => {
                        html += '<tr>';
                        Object.values(row).forEach(value => {
                            html += `<td style="padding: 8px; border: 1px solid #ddd;">${value}</td>`;
                        });
                        html += '</tr>';
                    });
                    html += '</tbody></table>';
                }
                
                contentDiv.innerHTML = html;
            } else if (type === 'search') {
                let html = '<div>';
                results.forEach(result => {
                    html += `
                        <div class="result-item">
                            <h4>${result.country} - ${result.jurisdiction}</h4>
                            <p><strong>Organization:</strong> ${result.organization}</p>
                            <p><strong>Rules:</strong> ${result.rules_count}</p>
                            <p><strong>Key Concepts:</strong> ${result.concepts.join(', ')}</p>
                            <p><strong>Relevance Score:</strong> ${result.score.toFixed(2)}</p>
                        </div>
                    `;
                });
                html += '</div>';
                contentDiv.innerHTML = html;
            }
            
            resultsDiv.style.display = 'block';
        }
        
        function displayStatistics(stats) {
            const statsDiv = document.getElementById('statsDisplay');
            const html = `
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_triples || 0}</div>
                        <div>Total Triples</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_classes || 0}</div>
                        <div>Classes</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_properties || 0}</div>
                        <div>Properties</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${stats.total_individuals || 0}</div>
                        <div>Individuals</div>
                    </div>
                </div>
                <h4>Namespaces:</h4>
                <ul>
                    ${(stats.namespaces || []).map(ns => `<li>${ns}</li>`).join('')}
                </ul>
            `;
            statsDiv.innerHTML = html;
        }
        
        function showError(message) {
            const resultContent = document.getElementById('resultContent');
            resultContent.innerHTML = `<div class="error">Error: ${message}</div>`;
            document.getElementById('results').style.display = 'block';
        }
        
        function clearResults() {
            document.getElementById('results').style.display = 'none';
            document.getElementById('resultContent').innerHTML = '';
        }
    </script>
</body>
</html>
        """
    
    def start_server(self, host='localhost', port=5000):
        """Start the web server"""
        if not FLASK_AVAILABLE:
            logger.error("Flask not available - cannot start web server")
            return
            
        if not self.app:
            logger.error("Flask app not initialized - cannot start web server")
            return
            
        logger.info(f"Starting query interface server on {host}:{port}")
        self.app.run(host=host, port=port, debug=False)

# ====================================
# ENHANCED LEGAL ANALYSIS STATE
# ====================================

class EnhancedLegalAnalysisState(TypedDict):
    """Enhanced state for legal analysis workflow with comprehensive chunk processing"""
    messages: Annotated[list, add_messages]
    document_content: str
    document_chunks: List[Dict]  # NEW: Store all chunks for processing
    document_summary: str        # NEW: Document summary for context
    document_structure: Dict     # NEW: Document structure metadata
    total_chunks: int           # NEW: Total number of chunks
    processed_chunks: int       # NEW: Number of processed chunks
    country: str
    jurisdiction: str
    organization: str
    extraction_result: Dict
    taxonomy_result: Dict
    ontology_triples_count: int  # Store count instead of Graph object
    ontology_serialized: str     # Store serialized turtle string instead
    validation_info: Optional[Dict]
    reasoning_steps: List[Dict]
    action_steps: List[Dict]
    analysis_complete: bool
    memory_namespace: str

# ====================================
# ENHANCED MULTI-AGENT ORCHESTRATOR
# ====================================

class EnhancedLegalAnalysisOrchestrator:
    """Enhanced orchestrator with ReAct agents, LangMem, and query interface"""
    
    def __init__(self):
        # Initialize memory store for long-term memory across sessions
        self.memory_store = InMemoryStore(
            index={
                "dims": 3072,  # text-embedding-3-large dimensions
                "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
            }
        )
        
        # Initialize clients and agents
        self.openai_client = self._initialize_openai_client()
        self.es_client = self._initialize_elasticsearch_client()
        
        # Initialize document processor with tiktoken manager
        tiktoken_manager = getattr(self.openai_client, 'tiktoken_manager', None)
        self.doc_processor = DocumentProcessor(tiktoken_manager)
        
        # Initialize enhanced ReAct agents with memory
        self.rule_agent = EnhancedRuleExtractionAgent(self.openai_client, self.memory_store)
        self.concept_agent = EnhancedConceptAnalysisAgent(self.openai_client, self.memory_store)
        self.ontology_builder = EnhancedOntologyBuilderAgent(self.memory_store)
        self.shacl_validator = SHACLValidator()
        
        # Initialize query interface
        self.query_interface = LegalKnowledgeQueryInterface(self)
        
        # Build the enhanced workflow
        self.workflow = self._build_enhanced_workflow()
    
    def _initialize_openai_client(self):
        """Initialize OpenAI client with validation"""
        try:
            Config.validate_config()
            return OpenAIClient()
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _initialize_elasticsearch_client(self):
        """Initialize Elasticsearch client"""
        try:
            client = ElasticsearchClient()
            client.create_index()
            return client
        except Exception as e:
            logger.error(f"Failed to initialize Elasticsearch client: {e}")
            raise
    
    def _build_enhanced_workflow(self) -> StateGraph:
        """Build enhanced LangGraph workflow with ReAct agents and memory"""
        
        workflow = StateGraph(EnhancedLegalAnalysisState)
        
        # Add enhanced processing nodes
        workflow.add_node("document_processing", self._enhanced_document_processing_node)
        workflow.add_node("comprehensive_extraction", self._comprehensive_extraction_node)
        workflow.add_node("taxonomic_analysis", self._taxonomic_analysis_node)
        workflow.add_node("ontology_construction", self._ontology_construction_node)
        workflow.add_node("ontology_validation", self._ontology_validation_node)
        workflow.add_node("knowledge_storage", self._knowledge_storage_node)
        workflow.add_node("interface_setup", self._interface_setup_node)
        
        # Add edges
        workflow.add_edge(START, "document_processing")
        workflow.add_edge("document_processing", "comprehensive_extraction")
        workflow.add_edge("comprehensive_extraction", "taxonomic_analysis")
        workflow.add_edge("taxonomic_analysis", "ontology_construction")
        workflow.add_edge("ontology_construction", "ontology_validation")
        workflow.add_edge("ontology_validation", "knowledge_storage")
        workflow.add_edge("knowledge_storage", "interface_setup")
        workflow.add_edge("interface_setup", END)
        
        return workflow.compile(checkpointer=MemorySaver(), store=self.memory_store)
    
    def _enhanced_document_processing_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Enhanced document processing with COMPREHENSIVE chunking - NO information loss"""
        logger.info("Enhanced document processing with COMPREHENSIVE legal-aware chunking...")
        
        if not state.get("document_content"):
            logger.error("No document content provided")
            return state
        
        # Set up memory namespace for this processing session
        state["memory_namespace"] = f"session_{uuid.uuid4()}"
        
        # Create comprehensive legal chunks with NO truncation
        comprehensive_chunks = self.doc_processor.create_comprehensive_legal_chunks(
            state["document_content"], 
            max_tokens_per_chunk=2500  # Smaller chunks for better processing
        )
        
        logger.info(f"Created {len(comprehensive_chunks)} comprehensive chunks - ALL content will be processed")
        
        # Store ALL chunk information for iterative processing
        state["document_chunks"] = comprehensive_chunks
        state["total_chunks"] = len(comprehensive_chunks)
        state["processed_chunks"] = 0
        
        # Create document summary for global context
        full_text = state["document_content"]
        if self.doc_processor.tiktoken_manager:
            # Create intelligent summary for context
            summary_text = self.doc_processor.smart_extract_for_analysis(full_text, max_analysis_tokens=3000)
            state["document_summary"] = summary_text
        else:
            # Fallback - use first and last parts
            words = full_text.split()
            if len(words) > 1000:
                state["document_summary"] = ' '.join(words[:500]) + ' ... ' + ' '.join(words[-500:])
            else:
                state["document_summary"] = full_text
        
        # Store metadata about document structure
        state["document_structure"] = {
            "total_pages": getattr(self.doc_processor, 'page_metadata', []),
            "structural_elements": [chunk['structure'] for chunk in comprehensive_chunks if chunk.get('structure')],
            "chunk_types": {chunk['type']: chunk['type'] for chunk in comprehensive_chunks},
            "has_legal_structure": any(chunk['type'] == 'structural' for chunk in comprehensive_chunks)
        }
        
        logger.info(f"Document structure analysis: {len(state['document_structure']['structural_elements'])} structural elements found")
        
        return state
    
    def _comprehensive_extraction_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """COMPREHENSIVE rule extraction - processes ALL chunks with NO information loss"""
        logger.info("Starting COMPREHENSIVE extraction - processing ALL chunks...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown"),
            "legal_system": "Mixed",
            "document_summary": state.get("document_summary", ""),
            "total_chunks": state.get("total_chunks", 0)
        }
        
        # Initialize comprehensive results
        comprehensive_results = {
            "rules": [],
            "definitions": [],
            "object_properties": [],
            "data_properties": [],
            "semantic_relationships": [],
            "chunk_processing_log": [],
            "validation_summary": {
                "all_subjects_defined": True,
                "all_objects_defined": True,
                "all_predicates_defined": True,
                "object_properties_count": 0,
                "data_properties_count": 0,
                "undefined_references": []
            }
        }
        
        chunks_to_process = state.get("document_chunks", [])
        logger.info(f"Processing {len(chunks_to_process)} chunks - ensuring NO information loss")
        
        async def extract_from_all_chunks():
            for i, chunk_info in enumerate(chunks_to_process):
                chunk_text = chunk_info['text']
                chunk_id = chunk_info['chunk_id']
                chunk_structure = chunk_info.get('structure', {})
                
                logger.info(f"Processing chunk {i+1}/{len(chunks_to_process)} (ID: {chunk_id})")
                
                # Enhanced context for this chunk
                chunk_context = context.copy()
                chunk_context.update({
                    "chunk_id": chunk_id,
                    "chunk_index": i,
                    "chunk_structure": chunk_structure,
                    "chunk_type": chunk_info.get('type'),
                    "context_preserved": chunk_info.get('context_preserved', False)
                })
                
                try:
                    # Extract rules from this chunk
                    chunk_result = await self.rule_agent.extract_rules_and_definitions(chunk_text, chunk_context)
                    
                    # Merge results with comprehensive tracking
                    if chunk_result.get("rules"):
                        # Add chunk metadata to each rule
                        for rule in chunk_result["rules"]:
                            rule["source_chunk_id"] = chunk_id
                            rule["source_chunk_index"] = i
                            rule["source_structure"] = chunk_structure
                        comprehensive_results["rules"].extend(chunk_result["rules"])
                    
                    if chunk_result.get("definitions"):
                        for definition in chunk_result["definitions"]:
                            definition["source_chunk_id"] = chunk_id
                            definition["source_chunk_index"] = i
                        comprehensive_results["definitions"].extend(chunk_result["definitions"])
                    
                    if chunk_result.get("object_properties"):
                        for prop in chunk_result["object_properties"]:
                            prop["source_chunk_id"] = chunk_id
                        comprehensive_results["object_properties"].extend(chunk_result["object_properties"])
                    
                    if chunk_result.get("data_properties"):
                        for prop in chunk_result["data_properties"]:
                            prop["source_chunk_id"] = chunk_id
                        comprehensive_results["data_properties"].extend(chunk_result["data_properties"])
                    
                    if chunk_result.get("semantic_relationships"):
                        comprehensive_results["semantic_relationships"].extend(chunk_result["semantic_relationships"])
                    
                    # Log successful processing
                    comprehensive_results["chunk_processing_log"].append({
                        "chunk_id": chunk_id,
                        "chunk_index": i,
                        "structure": chunk_structure,
                        "status": "success",
                        "rules_extracted": len(chunk_result.get("rules", [])),
                        "definitions_extracted": len(chunk_result.get("definitions", [])),
                        "text_length": len(chunk_text),
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    logger.info(f"Successfully processed chunk {i+1}: {len(chunk_result.get('rules', []))} rules, {len(chunk_result.get('definitions', []))} definitions")
                    
                except Exception as e:
                    logger.error(f"Failed to process chunk {i+1} (ID: {chunk_id}): {e}")
                    comprehensive_results["chunk_processing_log"].append({
                        "chunk_id": chunk_id,
                        "chunk_index": i,
                        "structure": chunk_structure,
                        "status": "failed",
                        "error": str(e),
                        "text_length": len(chunk_text),
                        "timestamp": datetime.now().isoformat()
                    })
                    continue  # Continue processing other chunks
                
                # Update progress
                state["processed_chunks"] = i + 1
        
        # Execute comprehensive extraction
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, extract_from_all_chunks())
                    future.result()
            else:
                asyncio.run(extract_from_all_chunks())
        except Exception as e:
            logger.error(f"Comprehensive extraction failed: {e}")
        
        # Post-processing: Remove duplicates and merge similar entries
        comprehensive_results = self._merge_and_deduplicate_results(comprehensive_results)
        
        # Update validation summary
        comprehensive_results["validation_summary"]["object_properties_count"] = len(comprehensive_results["object_properties"])
        comprehensive_results["validation_summary"]["data_properties_count"] = len(comprehensive_results["data_properties"])
        
        # Log comprehensive summary
        successful_chunks = sum(1 for log in comprehensive_results["chunk_processing_log"] if log["status"] == "success")
        failed_chunks = sum(1 for log in comprehensive_results["chunk_processing_log"] if log["status"] == "failed")
        
        logger.info(f"COMPREHENSIVE EXTRACTION COMPLETE:")
        logger.info(f"  - Total chunks processed: {len(chunks_to_process)}")
        logger.info(f"  - Successful chunks: {successful_chunks}")
        logger.info(f"  - Failed chunks: {failed_chunks}")
        logger.info(f"  - Total rules extracted: {len(comprehensive_results['rules'])}")
        logger.info(f"  - Total definitions extracted: {len(comprehensive_results['definitions'])}")
        logger.info(f"  - Total object properties: {len(comprehensive_results['object_properties'])}")
        logger.info(f"  - Total data properties: {len(comprehensive_results['data_properties'])}")
        
        state["extraction_result"] = comprehensive_results
        state["reasoning_steps"] = self.rule_agent.reasoning_steps
        state["action_steps"] = self.rule_agent.action_steps
        
        return state
    
    def _merge_and_deduplicate_results(self, results: Dict) -> Dict:
        """Merge and deduplicate results from multiple chunks"""
        
        # Deduplicate rules based on content similarity
        unique_rules = []
        seen_rules = set()
        
        for rule in results["rules"]:
            rule_signature = f"{rule.get('subject', {}).get('entity', '')}_{rule.get('predicate', {}).get('action', '')}_{rule.get('object', {}).get('entity', '')}"
            if rule_signature not in seen_rules:
                seen_rules.add(rule_signature)
                unique_rules.append(rule)
        
        results["rules"] = unique_rules
        
        # Deduplicate definitions
        unique_definitions = []
        seen_definitions = set()
        
        for definition in results["definitions"]:
            def_signature = definition.get('concept', '').lower()
            if def_signature and def_signature not in seen_definitions:
                seen_definitions.add(def_signature)
                unique_definitions.append(definition)
        
        results["definitions"] = unique_definitions
        
        # Deduplicate properties
        unique_obj_props = []
        seen_obj_props = set()
        
        for prop in results["object_properties"]:
            prop_signature = prop.get('property_name', '').lower()
            if prop_signature and prop_signature not in seen_obj_props:
                seen_obj_props.add(prop_signature)
                unique_obj_props.append(prop)
        
        results["object_properties"] = unique_obj_props
        
        # Similar for data properties
        unique_data_props = []
        seen_data_props = set()
        
        for prop in results["data_properties"]:
            prop_signature = prop.get('property_name', '').lower()
            if prop_signature and prop_signature not in seen_data_props:
                seen_data_props.add(prop_signature)
                unique_data_props.append(prop)
        
        results["data_properties"] = unique_data_props
        
        logger.info(f"Deduplication complete - unique items: {len(results['rules'])} rules, {len(results['definitions'])} definitions")
        
        return results
    
    def _taxonomic_analysis_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Taxonomic analysis using ReAct agent with memory"""
        logger.info("Taxonomic analysis with ReAct agent and memory...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown")
        }
        
        async def analyze_async():
            return await self.concept_agent.analyze_concepts_and_taxonomy(state["extraction_result"], context)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, analyze_async())
                    result = future.result()
            else:
                result = asyncio.run(analyze_async())
        except Exception as e:
            logger.error(f"Taxonomic analysis failed: {e}")
            result = {"concept_schemes": [], "taxonomic_hierarchies": {}, "compliance_frameworks": []}
        
        state["taxonomy_result"] = result
        
        # Combine reasoning steps from both agents
        state["reasoning_steps"].extend(self.concept_agent.reasoning_steps)
        state["action_steps"].extend(self.concept_agent.action_steps)
        
        return state
    
    def _ontology_construction_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Comprehensive ontology construction with memory"""
        logger.info("Building comprehensive ontology with memory...")
        
        context = {
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown")
        }
        
        ontology = self.ontology_builder.build_comprehensive_ontology(
            state["extraction_result"],
            state["taxonomy_result"], 
            context
        )
        
        # Serialize ontology to avoid msgpack serialization issues
        try:
            ontology_turtle = ontology.serialize(format='turtle')
            state["ontology_serialized"] = ontology_turtle
            state["ontology_triples_count"] = len(ontology)
        except Exception as e:
            logger.error(f"Failed to serialize ontology: {e}")
            state["ontology_serialized"] = ""
            state["ontology_triples_count"] = 0
        
        # Add ontology builder reasoning steps
        state["reasoning_steps"].extend(self.ontology_builder.reasoning_steps)
        state["action_steps"].extend(self.ontology_builder.action_steps)
        
        return state
    
    def _fix_validation_issues(self, ontology_graph: Graph, validation_results: Graph, state: EnhancedLegalAnalysisState) -> Optional[Graph]:
        """Use LLM to fix SHACL validation issues"""
        logger.info("Using LLM to fix SHACL validation issues...")
        
        try:
            # Extract validation report details
            validation_report = validation_results.serialize(format='turtle')
            
            # Prepare prompt for LLM to fix issues
            fix_prompt = f"""
You are an expert in RDF/OWL ontologies and SHACL validation. I have an ontology that is failing SHACL validation.

VALIDATION REPORT:
{validation_report[:2000]}  # Truncate to avoid token limits

ONTOLOGY (first 3000 characters):
{ontology_graph.serialize(format='turtle')[:3000]}

CONTEXT:
- Country: {state.get('country', 'Unknown')}
- Jurisdiction: {state.get('jurisdiction', 'Unknown')}
- This is a legal knowledge ontology with rules, entities, and properties

COMMON SHACL VALIDATION ISSUES TO CHECK AND FIX:
1. Missing required properties (rdfs:label, skos:definition, etc.)
2. Incorrect property domains/ranges
3. Missing class declarations
4. Invalid datatype constraints
5. Cardinality violations

Please analyze the validation issues and provide the MINIMAL additions/changes needed to fix the ontology.
Respond with ONLY the Turtle RDF triples that need to be ADDED to fix the issues.

Example format:
```turtle
@prefix legal: <https://legal-rules.org/ontology#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix skos: <http://www.w3.org/2004/02/skos/core#> .

legal:SomeEntity rdfs:label "Some Entity" .
legal:SomeEntity skos:definition "Definition of the entity" .
```
"""

            # Use the OpenAI client to get fixes
            response = asyncio.run(self.openai_client.chat_completion([
                {"role": "system", "content": "You are an expert RDF/OWL ontology engineer specializing in SHACL validation fixes."},
                {"role": "user", "content": fix_prompt}
            ]))
            
            # Extract turtle fixes from response
            if "```turtle" in response:
                turtle_start = response.find("```turtle") + 9
                turtle_end = response.find("```", turtle_start)
                turtle_fixes = response[turtle_start:turtle_end].strip()
            elif "```" in response:
                turtle_start = response.find("```") + 3
                turtle_end = response.find("```", turtle_start)
                turtle_fixes = response[turtle_start:turtle_end].strip()
            else:
                turtle_fixes = response.strip()
            
            # Apply fixes to ontology
            if turtle_fixes:
                logger.info("Applying LLM-generated fixes to ontology...")
                try:
                    # Parse the fixes
                    fixes_graph = Graph()
                    fixes_graph.parse(data=turtle_fixes, format='turtle')
                    
                    # Add fixes to original ontology
                    for triple in fixes_graph:
                        ontology_graph.add(triple)
                    
                    logger.info(f"Applied {len(fixes_graph)} fix triples to ontology")
                    return ontology_graph
                    
                except Exception as parse_error:
                    logger.error(f"Failed to parse LLM fixes: {parse_error}")
                    logger.info(f"LLM response was: {turtle_fixes}")
                    return None
            else:
                logger.warning("No fixes generated by LLM")
                return None
                
        except Exception as e:
            logger.error(f"Error in LLM-based validation fix: {e}")
            return None
    
    def _ontology_validation_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Validate ontology using SHACL shapes"""
        logger.info("Validating ontology with SHACL...")
        
        if state.get("ontology_serialized"):
            try:
                # Deserialize ontology for validation
                ontology_graph = Graph()
                ontology_graph.parse(data=state["ontology_serialized"], format='turtle')
                
                conforms, validation_results = self.shacl_validator.validate_ontology(ontology_graph)
                
                validation_info = {
                    "conforms": conforms,
                    "validation_performed": True,
                    "timestamp": datetime.now().isoformat()
                }
                
                if not conforms:
                    logger.warning("Ontology validation failed - attempting to fix issues")
                    validation_info["status"] = "failed"
                    
                    # Try to fix validation issues using LLM
                    try:
                        fixed_ontology = self._fix_validation_issues(ontology_graph, validation_results, state)
                        if fixed_ontology:
                            # Re-serialize the fixed ontology
                            state["ontology_serialized"] = fixed_ontology.serialize(format='turtle')
                            state["ontology_triples_count"] = len(fixed_ontology)
                            
                            # Re-validate
                            conforms_fixed, _ = self.shacl_validator.validate_ontology(fixed_ontology)
                            validation_info["conforms"] = conforms_fixed
                            validation_info["status"] = "fixed" if conforms_fixed else "still_failed"
                            logger.info(f"Ontology fix attempt result: {validation_info['status']}")
                    except Exception as fix_error:
                        logger.error(f"Failed to fix validation issues: {fix_error}")
                        validation_info["fix_error"] = str(fix_error)
                else:
                    logger.info("Ontology validation passed successfully")
                    validation_info["status"] = "passed"
                
                state["validation_info"] = validation_info
                
            except Exception as e:
                logger.error(f"Ontology validation error: {e}")
                state["validation_info"] = {
                    "conforms": False,
                    "validation_performed": False,
                    "error": str(e),
                    "status": "error"
                }
        else:
            logger.warning("No ontology available for validation")
            state["validation_info"] = {
                "conforms": False,
                "validation_performed": False,
                "status": "no_ontology"
            }
        
        return state
    
    def _knowledge_storage_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Enhanced knowledge storage in Elasticsearch with memory integration"""
        logger.info("Storing knowledge in Elasticsearch with memory integration...")
        
        # Generate embeddings for enhanced search
        async def generate_embeddings_async():
            return await self.openai_client.generate_embeddings([state["document_content"]])
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, generate_embeddings_async())
                    embeddings = future.result()
            else:
                embeddings = asyncio.run(generate_embeddings_async())
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            embeddings = []
        
        # Enhanced document structure for storage
        doc = {
            "document_id": str(uuid.uuid4()),
            "country": state.get("country", "Unknown"),
            "jurisdiction": state.get("jurisdiction", "Unknown"),
            "organization": state.get("organization", "Unknown"),
            "title": f"{state.get('country', 'Unknown')} Legal Knowledge Base",
            "content": state["document_content"],
            
            # Enhanced rule storage
            "rules": state["extraction_result"].get("rules", []),
            "definitions": state["extraction_result"].get("definitions", []),
            "object_properties": state["extraction_result"].get("object_properties", []),
            "data_properties": state["extraction_result"].get("data_properties", []),
            
            # Taxonomic information
            "concept_schemes": state["taxonomy_result"].get("concept_schemes", []),
            "compliance_frameworks": state["taxonomy_result"].get("compliance_frameworks", []),
            
            # Simple keyword fields for search
            "concepts": self._extract_simple_concepts(state),
            "actors": self._extract_actors(state),
            "objects": self._extract_objects(state),
            "data_domains": state["taxonomy_result"].get("data_domains", []),
            
            # Enhanced metadata with memory tracking
            "extraction_stats": {
                "rules_count": len(state["extraction_result"].get("rules", [])),
                "definitions_count": len(state["extraction_result"].get("definitions", [])),
                "properties_count": len(state["extraction_result"].get("object_properties", [])) + len(state["extraction_result"].get("data_properties", [])),
                "reasoning_steps": len(state.get("reasoning_steps", [])),
                "action_steps": len(state.get("action_steps", [])),
                "memory_namespace": state.get("memory_namespace", "")
            },
            
            # Vector for semantic search
            "content_vector": embeddings[0] if embeddings else [],
            
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # Store in Elasticsearch
        try:
            self.es_client.client.index(
                index=Config.ELASTICSEARCH_INDEX,
                body=doc
            )
            logger.info("Knowledge stored successfully in Elasticsearch with memory integration")
        except Exception as e:
            logger.error(f"Failed to store knowledge: {e}")
        
        return state
    
    def _interface_setup_node(self, state: EnhancedLegalAnalysisState) -> EnhancedLegalAnalysisState:
        """Setup query interface with loaded ontology"""
        logger.info("Setting up query interface...")
        
        if state.get("ontology_serialized"):
            try:
                # Deserialize ontology for query interface
                ontology_graph = Graph()
                ontology_graph.parse(data=state["ontology_serialized"], format='turtle')
                self.query_interface.load_ontology(ontology_graph)
                logger.info(f"Loaded ontology with {len(ontology_graph)} triples to query interface")
            except Exception as e:
                logger.error(f"Failed to deserialize ontology for query interface: {e}")
        
        state["analysis_complete"] = True
        return state
    
    def _extract_simple_concepts(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract simple concept keywords for Elasticsearch"""
        concepts = []
        
        # From definitions
        for definition in state["extraction_result"].get("definitions", []):
            concepts.append(definition.get("concept", ""))
            concepts.extend(definition.get("synonyms", []))
        
        # From taxonomic hierarchies
        for hierarchy_name, hierarchy_data in state["taxonomy_result"].get("taxonomic_hierarchies", {}).items():
            concepts.append(hierarchy_name)
            concepts.extend(self._extract_hierarchy_concepts(hierarchy_data.get("hierarchy", {})))
        
        return [c for c in concepts if c and isinstance(c, str)]
    
    def _extract_hierarchy_concepts(self, hierarchy: Dict) -> List[str]:
        """Recursively extract concepts from hierarchy"""
        concepts = []
        for concept_name, concept_data in hierarchy.items():
            concepts.append(concept_name)
            if isinstance(concept_data, dict) and "children" in concept_data:
                concepts.extend(self._extract_hierarchy_concepts(concept_data["children"]))
        return concepts
    
    def _extract_actors(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract actor entities for search"""
        actors = []
        for rule in state["extraction_result"].get("rules", []):
            subject_info = rule.get("subject", {})
            if isinstance(subject_info, dict) and subject_info.get("entity"):
                actors.append(subject_info["entity"])
        return list(set([a for a in actors if a and isinstance(a, str)]))
    
    def _extract_objects(self, state: EnhancedLegalAnalysisState) -> List[str]:
        """Extract object entities for search"""
        objects = []
        for rule in state["extraction_result"].get("rules", []):
            object_info = rule.get("object", {})
            if isinstance(object_info, dict) and object_info.get("entity"):
                objects.append(object_info["entity"])
        return list(set([o for o in objects if o and isinstance(o, str)]))
    
    async def process_document_enhanced(self, document_path: str, metadata: Dict) -> Dict:
        """Enhanced document processing with comprehensive analysis, ReAct agents, and memory - NO information loss"""
        
        try:
            # Extract text from document
            text_content = self.doc_processor.extract_text_from_pdf(document_path)
            
            logger.info(f"Extracted {len(text_content)} characters from {document_path}")
            
            # Initialize enhanced state with comprehensive chunk processing
            initial_state: EnhancedLegalAnalysisState = {
                "messages": [],
                "document_content": text_content,
                "document_chunks": [],       # Will be populated in processing
                "document_summary": "",      # Will be created from full document
                "document_structure": {},   # Will be analyzed
                "total_chunks": 0,          # Will be set based on chunking
                "processed_chunks": 0,      # Progress tracking
                "country": metadata.get('country', 'Unknown'),
                "jurisdiction": metadata.get('jurisdiction', 'Unknown'),
                "organization": metadata.get('organization', 'Unknown'),
                "extraction_result": {},
                "taxonomy_result": {},
                "ontology_triples_count": 0,
                "ontology_serialized": "",
                "validation_info": None,
                "reasoning_steps": [],
                "action_steps": [],
                "analysis_complete": False,
                "memory_namespace": ""
            }
            
            logger.info(f"Starting COMPREHENSIVE processing for {initial_state['country']}/{initial_state['jurisdiction']} - NO information will be lost")
            
            # Run enhanced workflow with memory - processes ALL chunks
            config = {"configurable": {"thread_id": str(uuid.uuid4())}}
            final_state = self.workflow.invoke(initial_state, config=config)
            
            # Export ontology in multiple formats
            ontology_exports = {}
            if final_state.get("ontology_serialized"):
                try:
                    # Deserialize for export
                    ontology_graph = Graph()
                    ontology_graph.parse(data=final_state["ontology_serialized"], format='turtle')
                    ontology_exports = self._export_ontology_enhanced(
                        ontology_graph,
                        metadata['country']
                    )
                except Exception as e:
                    logger.error(f"Failed to deserialize ontology for export: {e}")
                    ontology_exports = {"error": str(e)}
            
            # Calculate comprehensive processing statistics
            chunk_log = final_state["extraction_result"].get("chunk_processing_log", [])
            successful_chunks = sum(1 for log in chunk_log if log["status"] == "success")
            failed_chunks = sum(1 for log in chunk_log if log["status"] == "failed")
            
            return {
                "success": True,
                "comprehensive_processing": {
                    "total_chunks_created": final_state.get("total_chunks", 0),
                    "chunks_processed_successfully": successful_chunks,
                    "chunks_failed": failed_chunks,
                    "processing_completion_rate": f"{(successful_chunks/(successful_chunks + failed_chunks)*100):.1f}%" if (successful_chunks + failed_chunks) > 0 else "0%",
                    "no_information_lost": failed_chunks == 0,
                    "chunk_processing_details": chunk_log
                },
                "extraction_stats": {
                    "rules_extracted": len(final_state["extraction_result"].get("rules", [])),
                    "definitions_extracted": len(final_state["extraction_result"].get("definitions", [])),
                    "object_properties": len(final_state["extraction_result"].get("object_properties", [])),
                    "data_properties": len(final_state["extraction_result"].get("data_properties", [])),
                    "semantic_relationships": len(final_state["extraction_result"].get("semantic_relationships", [])),
                    "concept_schemes": len(final_state["taxonomy_result"].get("concept_schemes", [])),
                    "compliance_frameworks": len(final_state["taxonomy_result"].get("compliance_frameworks", []))
                },
                "document_analysis": {
                    "original_text_length": len(text_content),
                    "structural_elements_found": len(final_state.get("document_structure", {}).get("structural_elements", [])),
                    "has_legal_structure": final_state.get("document_structure", {}).get("has_legal_structure", False),
                    "chunk_types_used": list(final_state.get("document_structure", {}).get("chunk_types", {}).values()),
                    "pages_processed": len(final_state.get("document_structure", {}).get("total_pages", []))
                },
                "reasoning_summary": {
                    "reasoning_steps": len(final_state.get("reasoning_steps", [])),
                    "action_steps": len(final_state.get("action_steps", [])),
                    "agents_involved": ["RuleExtractor", "ConceptAnalyzer", "OntologyBuilder"],
                    "memory_namespace": final_state.get("memory_namespace", "")
                },
                "validation_summary": final_state.get("validation_info", {"status": "not_performed"}),
                "ontology_exports": ontology_exports,
                "analysis_complete": final_state.get("analysis_complete", False),
                "query_interface_ready": True,
                "memory_integration": {
                    "long_term_memory_enabled": True,
                    "memory_store_active": True,
                    "session_memory_namespace": final_state.get("memory_namespace", "")
                },
                "quality_assurance": {
                    "comprehensive_processing": True,
                    "all_chunks_processed": final_state.get("processed_chunks", 0) == final_state.get("total_chunks", 0),
                    "context_preservation": True,
                    "legal_structure_awareness": final_state.get("document_structure", {}).get("has_legal_structure", False),
                    "deduplication_applied": True
                }
            }
            
        except Exception as e:
            logger.error(f"Enhanced comprehensive processing failed: {e}")
            return {"success": False, "error": str(e), "comprehensive_processing": False}
    
    def _export_ontology_enhanced(self, graph: Graph, country: str) -> Dict[str, str]:
        """Enhanced ontology export with validation"""
        
        safe_country = country.replace(' ', '_').replace('/', '_').replace('\\', '_')
        output_dir = Path(Config.OUTPUT_PATH) / safe_country
        output_dir.mkdir(parents=True, exist_ok=True)
        
        exports = {}
        formats = {
            'ttl': 'turtle',
            'jsonld': 'json-ld',
            'xml': 'xml'
        }
        
        for ext, format_name in formats.items():
            filename = output_dir / f"enhanced_ontology_{safe_country}.{ext}"
            try:
                graph.serialize(destination=str(filename), format=format_name)
                exports[ext] = str(filename)
                logger.info(f"Enhanced ontology exported to {filename}")
                
                # Validate the exported file
                if ext == 'ttl':
                    # Test parsing the turtle file
                    test_graph = Graph()
                    test_graph.parse(str(filename), format='turtle')
                    logger.info(f"Validated {ext} export: {len(test_graph)} triples")
                    
            except Exception as e:
                logger.error(f"Failed to export ontology in {format_name}: {e}")
        
        return exports
    
    def get_processing_progress(self, state: EnhancedLegalAnalysisState) -> Dict:
        """Get current processing progress"""
        total = state.get("total_chunks", 0)
        processed = state.get("processed_chunks", 0)
        
        if total == 0:
            return {"progress": 0, "status": "initializing"}
        
        progress = (processed / total) * 100
        
        return {
            "progress": round(progress, 1),
            "processed_chunks": processed,
            "total_chunks": total,
            "status": "processing" if processed < total else "complete",
            "estimated_time_remaining": None  # Could be calculated based on processing speed
        }
    
    def validate_no_information_loss(self, original_text: str, extraction_result: Dict) -> Dict:
        """Validate that no critical information was lost during processing"""
        
        validation_results = {
            "validation_performed": True,
            "timestamp": datetime.now().isoformat(),
            "critical_patterns_found": {},
            "coverage_analysis": {},
            "potential_losses": []
        }
        
        # Check for critical legal patterns in original text
        legal_indicators = {
            'obligations': r'(?i)\b(shall|must|required|obligation|duty)\b',
            'prohibitions': r'(?i)\b(shall not|must not|prohibited|forbidden)\b',
            'permissions': r'(?i)\b(may|permitted|allowed|can)\b',
            'articles': r'(?i)\b(article\s+\d+|art\.?\s*\d+)\b',
            'sections': r'(?i)\b(section\s+\d+|ยง\s*\d+)\b',
            'rights': r'(?i)\b(right|rights)\b',
            'penalties': r'(?i)\b(penalty|penalties|fine|sanctions?)\b'
        }
        
        import re
        for pattern_name, pattern in legal_indicators.items():
            matches = re.findall(pattern, original_text)
            validation_results["critical_patterns_found"][pattern_name] = len(matches)
        
        # Check coverage in extracted rules
        extracted_text = " ".join([
            rule.get("original_text", "") for rule in extraction_result.get("rules", [])
        ])
        
        for pattern_name, pattern in legal_indicators.items():
            extracted_matches = re.findall(pattern, extracted_text)
            original_count = validation_results["critical_patterns_found"][pattern_name]
            extracted_count = len(extracted_matches)
            
            if extracted_count < original_count * 0.8:  # Less than 80% coverage
                validation_results["potential_losses"].append({
                    "pattern": pattern_name,
                    "original_count": original_count,
                    "extracted_count": extracted_count,
                    "coverage_percentage": round((extracted_count / original_count * 100), 1) if original_count > 0 else 0
                })
        
        # Overall assessment
        total_potential_losses = len(validation_results["potential_losses"])
        validation_results["assessment"] = {
            "information_loss_detected": total_potential_losses > 0,
            "severity": "high" if total_potential_losses > 3 else "medium" if total_potential_losses > 1 else "low",
            "recommendation": "Review chunk processing" if total_potential_losses > 0 else "Processing appears comprehensive"
        }
        
        if total_potential_losses > 0:
            logger.warning(f"Potential information loss detected in {total_potential_losses} pattern categories")
        else:
            logger.info("Validation complete: No significant information loss detected")
        
    def start_query_interface(self, host='localhost', port=5000):
        """Start the web-based query interface"""
        if not FLASK_AVAILABLE:
            logger.warning("Flask not available - query interface disabled")
            return None
            
        def run_server():
            self.query_interface.start_server(host, port)
        
        # Start server in a separate thread
        server_thread = threading.Thread(target=run_server, daemon=True)
        server_thread.start()
        logger.info(f"Query interface started at http://{host}:{port}")
        return server_thread

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Enhanced main application with ReAct agents and LangMem memory"""
    
    load_dotenv()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("Starting Enhanced Legal Document Analysis System with ReAct Agents and LangMem")
    logger.info("๐ฅ COMPREHENSIVE PROCESSING ENABLED - NO INFORMATION LOSS ๐ฅ")
    logger.info("Features:")
    logger.info("  โ Legal structure-aware chunking (Articles, Sections, Paragraphs)")
    logger.info("  โ Processes ALL chunks with context preservation")
    logger.info("  โ Token-aware chunking with overlap for context")
    logger.info("  โ Comprehensive deduplication and merging")
    logger.info("  โ Progress tracking and validation")
    logger.info("  โ SHACL validation with LLM-based fixes")
    logger.info("  โ Long-term memory with LangMem integration")
    logger.info("  โ ReAct agents with explicit reasoning")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info("Configuration validation passed")
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info("Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize enhanced orchestrator with memory
    orchestrator = EnhancedLegalAnalysisOrchestrator()
    
    # Start query interface if enabled
    interface_enabled = config.get('processing_options', {}).get('enable_query_interface', True) and FLASK_AVAILABLE
    if interface_enabled:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        server_thread = orchestrator.start_query_interface(interface_host, interface_port)
    else:
        if not FLASK_AVAILABLE:
            logger.info("Query interface disabled - Flask not available")
        else:
            logger.info("Query interface disabled in configuration")
        server_thread = None
    
    # Process documents with enhanced ReAct agents and memory
    results = []
    for doc_config in config['documents']:
        logger.info(f"Enhanced processing with memory for {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f"Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_document_enhanced(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "result": result
            })
            
            logger.info(f"Enhanced processing with memory completed for {doc_config['country']}")
            
        except Exception as e:
            logger.error(f"Enhanced processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Summary with comprehensive processing details
    successful = sum(1 for r in results if r['result']['success'])
    logger.info(f"๐ COMPREHENSIVE PROCESSING COMPLETE: {successful}/{len(results)} documents processed")
    
    # Log detailed processing statistics
    total_chunks_processed = 0
    total_rules_extracted = 0
    total_definitions_extracted = 0
    
    for result in results:
        if result['result']['success']:
            comprehensive = result['result'].get('comprehensive_processing', {})
            extraction_stats = result['result'].get('extraction_stats', {})
            
            logger.info(f"๐ {result['country']}:")
            logger.info(f"   Chunks: {comprehensive.get('total_chunks_created', 0)} created, {comprehensive.get('chunks_processed_successfully', 0)} processed")
            logger.info(f"   Completion: {comprehensive.get('processing_completion_rate', '0%')}")
            logger.info(f"   Rules: {extraction_stats.get('rules_extracted', 0)} extracted")
            logger.info(f"   Definitions: {extraction_stats.get('definitions_extracted', 0)} extracted")
            logger.info(f"   No Info Lost: {comprehensive.get('no_information_lost', 'Unknown')}")
            
            total_chunks_processed += comprehensive.get('chunks_processed_successfully', 0)
            total_rules_extracted += extraction_stats.get('rules_extracted', 0)
            total_definitions_extracted += extraction_stats.get('definitions_extracted', 0)
    
    logger.info(f"๐ OVERALL TOTALS:")
    logger.info(f"   ๐ Total chunks processed: {total_chunks_processed}")
    logger.info(f"   โ๏ธ Total rules extracted: {total_rules_extracted}")
    logger.info(f"   ๐ Total definitions extracted: {total_definitions_extracted}")
    logger.info(f"   โ Comprehensive processing: ALL document content analyzed")
    
    # Save enhanced results
    summary_path = Path(Config.OUTPUT_PATH) / "comprehensive_processing_summary_with_memory.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    if interface_enabled and server_thread:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        logger.info(f"Query interface with memory features available at http://{interface_host}:{interface_port}")
        logger.info("Press Ctrl+C to stop the system")
        
        try:
            # Keep the main thread alive
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("Shutting down...")
    else:
        logger.info("System processing completed. Query interface not started.")

if __name__ == "__main__":
    asyncio.run(main())
