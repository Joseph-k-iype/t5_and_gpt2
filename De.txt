"""
RAG pipeline with:
- Custom OpenAI embeddings (base_url + api_key passed explicitly)
- PyPDFLoader for PDFs
- ChromaDB as vector store (persisted locally)
- Simple question-answering with retrieved context

Dependencies:
  pip install "openai>=1.35.0" langchain langchain-community chromadb pypdf
"""

import os
import time
from typing import List
from uuid import uuid4

from openai import OpenAI

# LangChain bits
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI


# ================== SETTINGS ==================
API_KEY = "your_api_key_here"
BASE_URL = "https://api.openai.com/v1"   # or your custom endpoint

EMBED_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o-mini"

PDF_FILE = "sample.pdf"       # path to your PDF file
PERSIST_DIR = "chroma_db"     # where Chroma will persist data
COLLECTION_NAME = "rag_pdf"

CHUNK_SIZE = 800
CHUNK_OVERLAP = 120
TOP_K = 4


# ================== CUSTOM EMBEDDINGS ==================
class CustomOpenAIEmbeddings(Embeddings):
    """
    Custom embedding class calling OpenAI embeddings API directly.
    Uses base_url + api_key explicitly.
    """

    def __init__(self, api_key: str, base_url: str, model: str = EMBED_MODEL, batch_size: int = 96):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.batch_size = batch_size

    def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        clean_texts = [t.replace("\n", " ") for t in texts]
        resp = self.client.embeddings.create(model=self.model, input=clean_texts)
        return [d.embedding for d in resp.data]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        out = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i+self.batch_size]
            out.extend(self._embed_batch(batch))
        return out

    def embed_query(self, text: str) -> List[float]:
        return self._embed_batch([text])[0]


# ================== UTILS ==================
def print_step(title: str):
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80)


def summarize_doc(doc: Document, max_chars=200) -> str:
    snippet = doc.page_content.strip().replace("\n", " ")
    if len(snippet) > max_chars:
        snippet = snippet[:max_chars] + "..."
    page = doc.metadata.get("page", "unknown")
    return f"[page {page}] {snippet}"


# ================== MAIN ==================
def main():
    t0 = time.time()

    # Step A: Load PDF
    print_step("STEP A: Loading PDF")
    loader = PyPDFLoader(PDF_FILE)
    docs = loader.load()
    print(f"Loaded {len(docs)} pages from {PDF_FILE}")
    print("Example page:")
    print("  " + summarize_doc(docs[0]))

    # Step B: Split into chunks
    print_step("STEP B: Splitting into chunks")
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)
    print(f"Created {len(chunks)} chunks")

    # Step C: Create embeddings + store in Chroma
    print_step("STEP C: Creating ChromaDB with custom embeddings")
    embedder = CustomOpenAIEmbeddings(api_key=API_KEY, base_url=BASE_URL, model=EMBED_MODEL)

    vectordb = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embedder,
        persist_directory=PERSIST_DIR,
    )

    if vectordb._collection.count() == 0:
        ids = [f"{i}:{uuid4().hex[:6]}" for i in range(len(chunks))]
        vectordb.add_documents(chunks, ids=ids)
        vectordb.persist()
        print("Chunks indexed and persisted.")
    else:
        print("Reusing existing Chroma collection.")

    # Step D: Ask a question
    print_step("STEP D: Retrieval")
    question = input("Enter your question:\n> ").strip()
    if not question:
        question = "Summarize this document."

    results = vectordb.similarity_search_with_score(question, k=TOP_K)
    print("Top matches:")
    for rank, (doc, score) in enumerate(results, 1):
        print(f"{rank}) score={score:.4f} | {summarize_doc(doc)}")

    # Step E: Build prompt
    print_step("STEP E: Building prompt")
    context = "\n\n".join([d.page_content for d, _ in results])
    system_msg = "You are a helpful assistant. Use the context to answer the question."
    template = ChatPromptTemplate.from_messages([
        ("system", system_msg),
        ("user", "Question:\n{question}\n\nContext:\n{context}")
    ])
    msgs = template.format_messages(question=question, context=context)

    # Step F: Call LLM
    print_step("STEP F: Calling OpenAI LLM")
    llm = ChatOpenAI(model=LLM_MODEL, api_key=API_KEY, base_url=BASE_URL, temperature=0)
    parser = StrOutputParser()
    answer = parser.invoke(llm.invoke(msgs))

    print_step("FINAL ANSWER")
    print(answer.strip())
    print("\nRuntime: %.1fs" % (time.time() - t0))


if __name__ == "__main__":
    main()
