import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
import io
from typing import List, Dict, Optional, Union, Any, Tuple
import warnings
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import base64
from pathlib import Path

# Suppress warnings
warnings.filterwarnings('ignore')

# Configure Streamlit page
st.set_page_config(
    page_title="Excel Analysis Dashboard",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .stButton > button {
        width: 100%;
    }
    .main > div {
        padding-top: 1rem;
    }
    .block-container {
        padding-top: 2rem;
    }
    .stProgress > div > div > div > div {
        background-color: #00cc00;
    }
    .styled-metric {
        padding: 10px;
        border-radius: 5px;
        background-color: #f0f2f6;
        margin: 5px;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

class DataValidator:
    """Class to handle data validation and error checking."""
    
    @staticmethod
    def validate_file(file) -> bool:
        """Validate uploaded file."""
        try:
            if file is None:
                return False
            
            # Check file size (max 200MB)
            if file.size > 200 * 1024 * 1024:
                st.error(f"File {file.name} is too large. Maximum size is 200MB.")
                return False
            
            # Check file extension
            if not file.name.lower().endswith(('.xlsx', '.xls')):
                st.error(f"File {file.name} is not an Excel file.")
                return False
            
            return True
        except Exception as e:
            st.error(f"Error validating file {file.name}: {str(e)}")
            return False
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        """Validate DataFrame structure."""
        try:
            if df is None or df.empty:
                st.error("DataFrame is empty or None.")
                return False
            
            if len(df.columns) == 0:
                st.error("DataFrame has no columns.")
                return False
            
            if len(df) == 0:
                st.error("DataFrame has no rows.")
                return False
            
            return True
        except Exception as e:
            st.error(f"Error validating DataFrame: {str(e)}")
            return False

class DataLoader:
    """Class to handle data loading and initial processing."""
    
    def __init__(self):
        self.validator = DataValidator()
    
    def load_excel(self, file) -> Optional[pd.DataFrame]:
        """Load Excel file with comprehensive error handling."""
        try:
            if not self.validator.validate_file(file):
                return None
            
            # Read Excel file with error handling
            df = pd.read_excel(
                file,
                engine='openpyxl',
                na_values=['NA', 'N/A', '', ' ']  # Handle common NA values
            )
            
            if not self.validator.validate_dataframe(df):
                return None
            
            # Clean and process DataFrame
            df = self._process_dataframe(df)
            
            return df
            
        except Exception as e:
            st.error(f"Error loading {file.name}: {str(e)}")
            return None
    
    def _process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process and clean DataFrame."""
        try:
            # Clean column names
            df.columns = df.columns.str.strip().str.replace(' ', '_')
            
            # Handle each column
            for col in df.columns:
                # Handle datetime columns
                if df[col].dtype == 'object':
                    try:
                        # Try to convert to datetime
                        df[col] = pd.to_datetime(df[col], errors='ignore')
                    except:
                        pass
                
                # Try converting string numbers to numeric
                if df[col].dtype == 'object':
                    try:
                        numeric_col = pd.to_numeric(df[col], errors='coerce')
                        if numeric_col.notna().mean() > 0.8:  # If >80% are valid numbers
                            df[col] = numeric_col
                    except:
                        pass
                
                # Convert boolean columns to int
                if df[col].dtype == bool:
                    df[col] = df[col].astype(int)
            
            return df
            
        except Exception as e:
            st.error(f"Error processing DataFrame: {str(e)}")
            return df

class DataProcessor:
    """Class to handle data merging, transformations, and cleaning with comprehensive reporting."""
    
    def __init__(self):
        self.cleaning_report = {
            'missing_values': {},
            'outliers': {},
            'duplicates': {},
            'summary': {}
        }

    @staticmethod
    def validate_merge_keys(dataframes: List[pd.DataFrame], merge_keys: List[str]) -> bool:
        """Validate merge keys exist in all dataframes and have compatible types."""
        try:
            if not dataframes or not merge_keys:
                st.error("No dataframes or merge keys provided.")
                return False
            
            # Check keys exist in all dataframes
            for idx, df in enumerate(dataframes):
                missing_keys = [key for key in merge_keys if key not in df.columns]
                if missing_keys:
                    st.error(f"Keys {missing_keys} not found in DataFrame {idx + 1}")
                    return False
            
            # Check key types are compatible
            base_df = dataframes[0]
            for key in merge_keys:
                base_type = base_df[key].dtype
                for idx, df in enumerate(dataframes[1:], 1):
                    curr_type = df[key].dtype
                    if not pd.api.types.is_dtype_equal(base_type, curr_type):
                        st.warning(f"Column '{key}' has different types: {base_type} vs {curr_type} in DataFrame {idx + 1}")
                        # Try converting to string if types don't match
                        for df in dataframes:
                            df[key] = df[key].astype(str)
            
            return True
            
        except Exception as e:
            st.error(f"Error validating merge keys: {str(e)}")
            return False

    @staticmethod
    def clean_merge_data(df: pd.DataFrame, keys: List[str]) -> pd.DataFrame:
        """Clean data before merging."""
        try:
            result = df.copy()
            
            # Clean merge key columns
            for key in keys:
                # Strip whitespace if string
                if pd.api.types.is_string_dtype(result[key]):
                    result[key] = result[key].str.strip()
                
                # Handle missing values in key columns
                if result[key].isna().any():
                    st.warning(f"Found missing values in merge key '{key}'. Filling with appropriate values.")
                    if pd.api.types.is_numeric_dtype(result[key]):
                        result[key] = result[key].fillna(-999999)  # Special value for missing numerics
                    else:
                        result[key] = result[key].fillna("MISSING")
            
            return result
            
        except Exception as e:
            st.error(f"Error cleaning merge data: {str(e)}")
            return df

    @staticmethod
    def merge_dataframes(dataframes: List[pd.DataFrame], 
                        merge_keys: List[str], 
                        merge_type: str = 'left') -> Optional[pd.DataFrame]:
        """Merge multiple DataFrames with validation and error handling."""
        try:
            if not dataframes:
                st.error("No DataFrames to merge.")
                return None
            
            # Validate merge keys
            if not DataProcessor.validate_merge_keys(dataframes, merge_keys):
                return None
            
            # Clean data for merging
            cleaned_dfs = [DataProcessor.clean_merge_data(df, merge_keys) for df in dataframes]
            
            # Initialize progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Perform merge
            result = cleaned_dfs[0]
            total_rows = len(result)
            
            for idx, df in enumerate(cleaned_dfs[1:], 1):
                # Update progress
                progress = idx / len(cleaned_dfs[1:])
                progress_bar.progress(progress)
                status_text.text(f"Merging DataFrame {idx + 1}...")
                
                # Store column counts for validation
                pre_merge_cols = len(result.columns) + len(df.columns) - len(merge_keys)
                
                # Add suffixes to avoid column name conflicts
                suffix_a = f"_1_{idx}" if idx > 1 else ""
                suffix_b = f"_2_{idx}"
                
                # Perform merge
                result = result.merge(
                    df,
                    on=merge_keys,
                    how=merge_type,
                    suffixes=(suffix_a, suffix_b),
                    validate='m:m'  # Validate many-to-many relationships
                )
                
                # Log merge results and validate
                new_total = len(result)
                post_merge_cols = len(result.columns)
                
                # Validate no columns were accidentally dropped
                if post_merge_cols < pre_merge_cols - len(merge_keys):
                    st.warning(f"Some columns may have been lost in merge step {idx}. Please check the results.")
                
                # Log row changes
                if merge_type in ['left', 'right'] and new_total != total_rows:
                    st.warning(f"Row count changed in {merge_type} merge: {total_rows} â†’ {new_total}")
                elif merge_type == 'inner' and new_total < min(total_rows, len(df)):
                    st.info(f"Inner merge reduced rows: {total_rows} â†’ {new_total}")
                
                st.success(f"Merge step {idx}: {total_rows} â†’ {new_total} rows")
                total_rows = new_total
            
            progress_bar.progress(1.0)
            status_text.text("Merge completed successfully!")
            
            # Final validation
            if result.empty:
                st.error("Merge resulted in empty DataFrame. Please check your merge keys and data.")
                return None
            
            # Report duplicate keys if any
            for key in merge_keys:
                dupe_count = result[key].duplicated().sum()
                if dupe_count > 0:
                    st.info(f"Found {dupe_count} duplicate values in key '{key}' after merge.")
            
            return result
            
        except pd.errors.MergeError as me:
            st.error(f"Merge error: {str(me)}")
            return None
        except Exception as e:
            st.error(f"Error during merge: {str(e)}")
            return None

    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Handle missing values and report details."""
        result = df.copy()
        missing_report = {}
        total_cells = len(df) * len(df.columns)
        total_missing = 0
        
        # Check each column for missing values
        for column in result.columns:
            missing_mask = result[column].isna()
            missing_count = missing_mask.sum()
            total_missing += missing_count
            
            if missing_count > 0:
                missing_report[column] = {
                    'count': missing_count,
                    'percentage': (missing_count / len(df)) * 100,
                    'missing_rows': df.index[missing_mask].tolist()
                }
                # Replace ALL missing values with "Data Not Available"
                result[column] = result[column].fillna("Data Not Available")
        
        self.cleaning_report['missing_values'] = {
            'columns_with_missing': len(missing_report),
            'total_missing_cells': total_missing,
            'total_cells': total_cells,
            'overall_missing_percentage': (total_missing / total_cells * 100) if total_cells > 0 else 0,
            'details_by_column': missing_report
        }
        
        return result

    def handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Handle outliers using IQR method for numeric columns."""
        result = df.copy()
        outlier_report = {}
        total_outliers = 0
        
        # Process only numeric columns
        numeric_columns = result.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            # Calculate IQR boundaries
            Q1 = result[column].quantile(0.25)
            Q3 = result[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Identify outliers
            outliers_mask = (result[column] < lower_bound) | (result[column] > upper_bound)
            outlier_count = outliers_mask.sum()
            total_outliers += outlier_count
            
            if outlier_count > 0:
                # Store outlier values before clipping
                outlier_values = result.loc[outliers_mask, column].tolist()
                
                outlier_report[column] = {
                    'count': outlier_count,
                    'percentage': (outlier_count / len(df)) * 100,
                    'boundaries': {
                        'lower': lower_bound,
                        'upper': upper_bound
                    },
                    'statistics': {
                        'Q1': Q1,
                        'Q3': Q3,
                        'IQR': IQR
                    },
                    'outlier_values': outlier_values,
                    'outlier_rows': df.index[outliers_mask].tolist()
                }
                
                # Clip values to boundaries
                result[column] = result[column].clip(lower=lower_bound, upper=upper_bound)
        
        self.cleaning_report['outliers'] = {
            'columns_processed': len(numeric_columns),
            'columns_with_outliers': len(outlier_report),
            'total_outliers': total_outliers,
            'details_by_column': outlier_report
        }
        
        return result

    def handle_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove exact duplicate rows and report details."""
        # Identify duplicates
        duplicate_mask = df.duplicated()
        duplicate_count = duplicate_mask.sum()
        duplicate_percentage = (duplicate_count / len(df)) * 100
        
        # Store duplicate rows for reporting
        duplicate_rows = df[duplicate_mask].copy()
        
        # Remove duplicates
        result = df.drop_duplicates()
        
        self.cleaning_report['duplicates'] = {
            'rows_before': len(df),
            'rows_after': len(result),
            'duplicates_removed': duplicate_count,
            'percentage_duplicate': duplicate_percentage,
            'duplicate_examples': duplicate_rows.head(5).to_dict('records') if not duplicate_rows.empty else []
        }
        
        return result

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Main method to clean data with comprehensive reporting."""
        # Reset cleaning report
        self.cleaning_report = {
            'missing_values': {},
            'outliers': {},
            'duplicates': {},
            'summary': {}
        }
        
        # Store initial state
        initial_state = {
            'rows': len(df),
            'columns': len(df.columns),
            'total_cells': len(df) * len(df.columns),
            'memory_usage': df.memory_usage(deep=True).sum() / (1024 * 1024)  # In MB
        }
        
        # Step 1: Handle Missing Values
        st.write("Step 1: Handling Missing Values...")
        df_cleaned = self.handle_missing_values(df)
        
        # Step 2: Handle Outliers
        st.write("Step 2: Processing Outliers...")
        df_cleaned = self.handle_outliers(df_cleaned)
        
        # Step 3: Remove Duplicates
        st.write("Step 3: Removing Duplicates...")
        df_cleaned = self.handle_duplicates(df_cleaned)
        
        # Final state
        final_state = {
            'rows': len(df_cleaned),
            'columns': len(df_cleaned.columns),
            'total_cells': len(df_cleaned) * len(df_cleaned.columns),
            'memory_usage': df_cleaned.memory_usage(deep=True).sum() / (1024 * 1024)  # In MB
        }
        
        # Generate final summary
        self.cleaning_report['summary'] = {
            'initial_state': initial_state,
            'final_state': final_state,
            'changes': {
                'rows_removed': initial_state['rows'] - final_state['rows'],
                'memory_reduction': initial_state['memory_usage'] - final_state['memory_usage'],
                'missing_values_handled': self.cleaning_report['missing_values'].get('total_missing_cells', 0),
                'outliers_handled': self.cleaning_report['outliers'].get('total_outliers', 0),
                'duplicates_removed': self.cleaning_report['duplicates'].get('duplicates_removed', 0)
            },
            'performance_metrics': {
                'data_reduction_percentage': ((initial_state['rows'] - final_state['rows']) / initial_state['rows'] * 100) if initial_state['rows'] > 0 else 0,
                'memory_reduction_percentage': ((initial_state['memory_usage'] - final_state['memory_usage']) / initial_state['memory_usage'] * 100) if initial_state['memory_usage'] > 0 else 0
            }
        }
        
        return df_cleaned

    def display_cleaning_report(self):
        """Display detailed cleaning report."""
        st.header("ðŸ§¹ Data Cleaning Report")
        
        # Overall Summary
        st.subheader("ðŸ“Š Summary of Changes")
        summary = self.cleaning_report['summary']
        
        # Create summary metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="Total Rows",
                value=f"{summary['final_state']['rows']:,}",
                delta=f"-{summary['changes']['rows_removed']:,}",
                delta_color="inverse"
            )
        
        with col2:
            st.metric(
                label="Memory Usage (MB)",
                value=f"{summary['final_state']['memory_usage']:.2f}",
                delta=f"-{summary['changes']['memory_reduction']:.2f}",
                delta_color="inverse"
            )
        
        with col3:
            st.metric(
                label="Data Reduction",
                value=f"{summary['performance_metrics']['data_reduction_percentage']:.1f}%",
                delta="reduction" if summary['performance_metrics']['data_reduction_percentage'] > 0 else "no change"
            )
        
        # Missing Values Section
        st.subheader("ðŸ” Missing Values")
        missing = self.cleaning_report['missing_values']
        if missing:
            st.write(f"Total Missing Cells: {missing['total_missing_cells']:,} ({missing['overall_missing_percentage']:.2f}%)")
            st.write(f"Columns with Missing Values: {missing['columns_with_missing']}")
            
            if missing.get('details_by_column'):
                # Create a DataFrame for missing values information
                missing_df = pd.DataFrame.from_dict(
                    missing['details_by_column'],
                    orient='index'
                )
                if not missing_df.empty:
                    missing_df = missing_df.sort_values('count', ascending=False)
                    st.write("Details by Column:")
                    
                    # Display as a styled table
                    st.dataframe(
                        missing_df[['count', 'percentage']].style.format({
                            'count': '{:,.0f}',
                            'percentage': '{:.2f}%'
                        })
                    )
        
        # Outliers Section
        st.subheader("ðŸ“Š Outliers")
        outliers = self.cleaning_report['outliers']
        if outliers:
            st.write(f"Numeric Columns Processed: {outliers['columns_processed']}")
            st.write(f"Columns with Outliers: {outliers['columns_with_outliers']}")
            st.write(f"Total Outliers Found: {outliers['total_outliers']:,}")
            
            if outliers.get('details_by_column'):
                st.write("Details by Column:")
                for col, info in outliers['details_by_column'].items():
                    with st.expander(f"{col} - {info['count']:,} outliers ({info['percentage']:.2f}%)"):
                        st.write("### Boundaries")
                        st.write(f"- Lower: {info['boundaries']['lower']:.2f}")
                        st.write(f"- Upper: {info['boundaries']['upper']:.2f}")
                        
                        st.write("### Statistics")
                        st.write(f"- Q1: {info['statistics']['Q1']:.2f}")
                        st.write(f"- Q3: {info['statistics']['Q3']:.2f}")
                        st.write(f"- IQR: {info['statistics']['IQR']:.2f}")
                        
                        if info.get('outlier_values'):
                            st.write("### Sample Outlier Values")
                            st.write(info['outlier_values'][:5])
        
        # Duplicates Section
        st.subheader("ðŸ”„ Duplicates")
        dupes = self.cleaning_report['duplicates']
        if dupes:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    label="Duplicates Removed",
                    value=f"{dupes['duplicates_removed']:,}",
                    delta=f"-{dupes['duplicates_removed']:,}",
                    delta_color="inverse"
                )
            
            with col2:
                st.metric(
                    label="Duplicate Percentage",
                    value=f"{dupes['percentage_duplicate']:.2f}%"
                )
            
            with col3:
                st.metric(
                    label="Final Row Count",
                    value=f"{dupes['rows_after']:,}",
                    delta=f"-{dupes['rows_before'] - dupes['rows_after']:,}",
                    delta_color="inverse"
                )
            
            if dupes.get('duplicate_examples'):
                with st.expander("View Sample Duplicate Rows"):
                    st.dataframe(pd.DataFrame(dupes['duplicate_examples']))
        
        # Final Data Quality Metrics
        st.subheader("ðŸ“ˆ Final Data Quality Metrics")
        
        quality_metrics = {
            'Completeness': (1 - missing['total_missing_cells'] / summary['final_state']['total_cells']) * 100 if missing else 100,
            'Uniqueness': (1 - dupes['percentage_duplicate'] / 100) * 100 if dupes else 100,
            'Validity': (1 - outliers['total_outliers'] / summary['final_state']['total_cells']) * 100 if outliers else 100
        }
        
        # Display metrics in columns
        cols = st.columns(3)
        for idx, (metric, value) in enumerate(quality_metrics.items()):
            with cols[idx]:
                st.metric(
                    label=f"{metric} Score",
                    value=f"{value:.2f}%"
                )
                
    def get_report_summary(self) -> Dict:
        """Get a dictionary summary of the cleaning report."""
        return {
            'total_rows_processed': self.cleaning_report['summary']['initial_state']['rows'],
            'final_rows': self.cleaning_report['summary']['final_state']['rows'],
            'missing_values_handled': self.cleaning_report['summary']['changes']['missing_values_handled'],
            'outliers_handled': self.cleaning_report['summary']['changes']['outliers_handled'],
            'duplicates_removed': self.cleaning_report['summary']['changes']['duplicates_removed'],
            'data_reduction_percentage': self.cleaning_report['summary']['performance_metrics']['data_reduction_percentage']
        }
        
class Analytics:
    """Class to handle all analytical operations."""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
    
    def create_pivot(self, 
                index_cols: List[str],
                value_cols: List[str],
                agg_funcs: List[str],
                filters: Dict = None) -> Optional[pd.DataFrame]:
        """Create pivot table with comprehensive error handling."""
        try:
            # Validate inputs
            if not index_cols or not value_cols or not agg_funcs:
                st.error("Missing required parameters for pivot table.")
                return None
            
            # Apply filters
            filtered_df = self.df.copy()
            if filters:
                for col, values in filters.items():
                    if values:
                        filtered_df = filtered_df[
                            filtered_df[col].astype(str).isin([str(v) for v in values])
                        ]
            
            # Create pivot tables for each combination
            pivot_tables = []
            for col in value_cols:
                # Get column type
                is_numeric = pd.api.types.is_numeric_dtype(filtered_df[col])
                is_datetime = pd.api.types.is_datetime64_any_dtype(filtered_df[col])
                
                for func in agg_funcs:
                    try:
                        # Handle different aggregation functions based on data type
                        if func == 'distinct_count':
                            agg_func = lambda x: len(x.unique())
                            display_name = 'Unique Count'
                        elif func == 'count':
                            agg_func = 'count'
                            display_name = 'Count'
                        elif is_numeric and func in ['sum', 'mean', 'max', 'min']:
                            agg_func = func
                            display_name = func.capitalize()
                        elif func in ['first', 'last']:
                            agg_func = func
                            display_name = func.capitalize()
                        elif func == 'list':
                            agg_func = lambda x: ', '.join(x.unique().astype(str))
                            display_name = 'Unique Values'
                        else:
                            # Skip invalid combinations
                            continue
                        
                        pivot = pd.pivot_table(
                            filtered_df,
                            values=col,
                            index=index_cols,
                            aggfunc=agg_func,
                            fill_value=0 if is_numeric else ''
                        )
                        
                        # Format column name
                        pivot.columns = [f"{col} ({display_name})"]
                        pivot_tables.append(pivot)
                        
                    except Exception as e:
                        st.warning(f"Error calculating {func} for {col}: {str(e)}")
                        continue
            
            if not pivot_tables:
                st.warning("No valid pivot tables could be created.")
                return None
            
            # Combine results
            result = pd.concat(pivot_tables, axis=1)
            
            # Add totals row if any numeric columns exist
            try:
                totals = pd.DataFrame(index=['Total'])
                for col in result.columns:
                    try:
                        if 'Count' in col:
                            totals[col] = result[col].sum()
                        elif any(x in col for x in ['Sum', 'Mean']):
                            totals[col] = result[col].sum()
                        elif 'Max' in col:
                            totals[col] = result[col].max()
                        elif 'Min' in col:
                            totals[col] = result[col].min()
                        else:
                            totals[col] = ''  # For non-numeric aggregations
                    except:
                        totals[col] = ''
                
                result = pd.concat([result, totals])
            except Exception as e:
                st.warning(f"Error calculating totals: {str(e)}")
            
            return result
            
        except Exception as e:
            st.error(f"Error creating pivot table: {str(e)}")
            return None
    
    def calculate_statistics(self, column: str) -> Dict:
        """Calculate comprehensive statistics for a column."""
        try:
            series = self.df[column]
            
            # Convert boolean to int
            if series.dtype == bool:
                series = series.astype(int)
            
            # Basic stats for all types
            stats = {
                'count': int(len(series)),
                'unique': int(series.nunique()),
                'missing': int(series.isna().sum()),
                'missing_pct': float(series.isna().mean() * 100)
            }
            
            # Additional stats for numeric columns
            if pd.api.types.is_numeric_dtype(series):
                numeric_series = pd.to_numeric(series, errors='coerce')
                stats.update({
                    'mean': float(numeric_series.mean()) if not pd.isna(numeric_series.mean()) else None,
                    'median': float(numeric_series.median()) if not pd.isna(numeric_series.median()) else None,
                    'std': float(numeric_series.std()) if not pd.isna(numeric_series.std()) else None,
                    'min': float(numeric_series.min()) if not pd.isna(numeric_series.min()) else None,
                    'max': float(numeric_series.max()) if not pd.isna(numeric_series.max()) else None,
                    'skew': float(numeric_series.skew()) if not pd.isna(numeric_series.skew()) else None,
                    'kurtosis': float(numeric_series.kurtosis()) if not pd.isna(numeric_series.kurtosis()) else None,
                    'percentiles': {
                        '25%': float(numeric_series.quantile(0.25)),
                        '50%': float(numeric_series.quantile(0.50)),
                        '75%': float(numeric_series.quantile(0.75))
                    }
                })
            
            return stats
            
        except Exception as e:
            st.error(f"Error calculating statistics for {column}: {str(e)}")
            return {}

class Visualizer:
    """Class to handle all visualization functionality."""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
    
    def create_pivot_chart(self, 
                          pivot_df: pd.DataFrame,
                          chart_type: str = 'bar',
                          color_theme: str = 'default',
                          show_values: bool = True,
                          normalize: bool = False) -> Optional[go.Figure]:
        """Create visualization for pivot table results."""
        try:
            if pivot_df is None or pivot_df.empty:
                st.error("No data available for visualization.")
                return None
            
            # Remove total row for visualization
            plot_df = pivot_df[pivot_df.index != 'Total'].copy()
            
            # Normalize values if requested
            if normalize:
                for col in plot_df.columns:
                    plot_df[col] = plot_df[col] / plot_df[col].max() * 100
            
            # Reset index to make it a column
            plot_df = plot_df.reset_index()
            
            # Get index column name
            index_col = pivot_df.index.name if pivot_df.index.name else 'index'
            
            fig = go.Figure()
            
            if chart_type == 'bar':
                # Add a trace for each value column
                for col in plot_df.columns[1:]:  # Skip index column
                    fig.add_trace(
                        go.Bar(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='auto',
                        )
                    )
                fig.update_layout(barmode='group')
                
            elif chart_type == 'line':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='lines+markers',
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'scatter':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='markers',
                            marker=dict(size=10),
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'area':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Scatter(
                            name=str(col),
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='lines',
                            fill='tonexty',
                            text=plot_df[col].round(2) if show_values else None,
                            textposition='top center',
                        )
                    )
            
            elif chart_type == 'box':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Box(
                            name=str(col),
                            y=plot_df[col],
                            boxpoints='outliers'
                        )
                    )
            
            # Update color theme if specified
            if color_theme != 'default':
                if hasattr(fig, 'data'):
                    for trace in fig.data:
                        if hasattr(trace, 'marker'):
                            trace.marker.color = color_theme
                        if hasattr(trace, 'line'):
                            trace.line.color = color_theme
            
            # Update layout
            fig.update_layout(
                title='Pivot Table Visualization',
                title_x=0.5,
                height=600,
                showlegend=True,
                template='plotly_white',
                xaxis_title=index_col,
                yaxis_title="Values",
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating pivot visualization: {str(e)}")
            return None
    
    def create_correlation_analysis(self) -> Tuple[Optional[go.Figure], Optional[pd.DataFrame]]:
        """Create correlation analysis with both heatmap and matrix."""
        try:
            # Get numeric columns
            numeric_df = self.df.select_dtypes(include=[np.number])
            
            if numeric_df.empty:
                st.warning("No numeric columns available for correlation analysis.")
                return None, None
            
            # Calculate correlation matrix
            corr_matrix = numeric_df.corr().round(3)
            
            # Create heatmap
            fig = go.Figure(data=go.Heatmap(
                z=corr_matrix.values,
                x=corr_matrix.columns,
                y=corr_matrix.columns,
                text=corr_matrix.values.round(3),
                texttemplate='%{text:.3f}',
                textfont={"size": 10},
                hoverongaps=False,
                colorscale='RdBu_r',
                zmid=0
            ))
            
            fig.update_layout(
                title='Correlation Heatmap',
                title_x=0.5,
                height=700,
                width=800,
                xaxis_title="Features",
                yaxis_title="Features",
                xaxis={'side': 'bottom'}
            )
            
            return fig, corr_matrix
            
        except Exception as e:
            st.error(f"Error creating correlation analysis: {str(e)}")
            return None, None
class ExportManager:
    """Class to handle all export functionality."""
    
    @staticmethod
    def to_excel(df: pd.DataFrame,
                 pivot_df: Optional[pd.DataFrame] = None,
                 stats: Optional[Dict] = None) -> Optional[bytes]:
        """Export data to Excel with formatting."""
        try:
            output = io.BytesIO()
            
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                # Write original data
                df.to_excel(writer, sheet_name='Data', index=False)
                
                # Write pivot results if available
                if pivot_df is not None and not pivot_df.empty:
                    pivot_df.to_excel(writer, sheet_name='Pivot Analysis')
                
                # Write statistics if available
                if stats:
                    pd.DataFrame(stats).to_excel(writer, sheet_name='Statistics')
                
                # Get workbook and add formats
                workbook = writer.book
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#D3D3D3',
                    'border': 1,
                    'text_wrap': True
                })
                
                number_format = workbook.add_format({
                    'num_format': '#,##0.00',
                    'border': 1
                })
                
                # Format each worksheet
                for sheet in writer.sheets.values():
                    # Format headers
                    sheet.set_row(0, None, header_format)
                    
                    # Set column widths
                    sheet.set_column(0, 50, 15)
                    
                    # Freeze panes
                    sheet.freeze_panes(1, 0)
            
            return output.getvalue()
            
        except Exception as e:
            st.error(f"Error exporting to Excel: {str(e)}")
            return None

class PivotHelpers:
    """Helper functions for enhanced pivot table functionality."""
    
    @staticmethod
    def get_sort_options(pivot_df: pd.DataFrame) -> dict:
        """Get available sorting options for pivot table."""
        sort_options = {
            'No Sorting': None,
            'Values (Ascending)': {'ascending': True},
            'Values (Descending)': {'ascending': False}
        }
        
        if isinstance(pivot_df.index, pd.MultiIndex):
            sort_options.update({
                'Index Levels (Ascending)': {'ascending': True, 'by_index': True},
                'Index Levels (Descending)': {'ascending': False, 'by_index': True}
            })
        
        return sort_options

    @staticmethod
    def apply_sorting(pivot_df: pd.DataFrame, sort_config: dict) -> pd.DataFrame:
        """Apply sorting to pivot table based on configuration."""
        if not sort_config:
            return pivot_df
            
        try:
            df = pivot_df.copy()
            if sort_config.get('by_index'):
                # Sort by index levels
                df = df.sort_index(ascending=sort_config['ascending'])
            else:
                # Sort by values
                if len(df.columns) > 0:
                    first_col = df.columns[0] if isinstance(df.columns, pd.MultiIndex) else df.columns[0]
                    df = df.sort_values(by=first_col, ascending=sort_config['ascending'])
            return df
        except Exception as e:
            st.error(f"Error applying sorting: {str(e)}")
            return pivot_df


    @staticmethod
    def safe_style_apply(df: pd.DataFrame) -> Any:
        """Safely apply styling to dataframe accounting for different pandas versions."""
        try:
            # Try getting style property
            return df.style
        except AttributeError:
            try:
                # Alternative for older pandas versions
                return df.style.format("{:.2f}")
            except:
                # If styling completely fails, return raw dataframe
                st.warning("Advanced formatting not available. Displaying raw data.")
                return df

    @staticmethod
    def apply_conditional_formatting(pivot_df: pd.DataFrame, format_type: str) -> Any:
        """Apply conditional formatting to pivot table with robust error handling."""
        try:
            if format_type == 'None':
                return PivotHelpers.safe_style_apply(pivot_df)
            
            # Get the style object safely
            styler = PivotHelpers.safe_style_apply(pivot_df)
            if isinstance(styler, pd.DataFrame):  # If styling failed
                return styler
                
            try:
                if format_type == 'Heatmap':
                    return styler.background_gradient(cmap='RdYlBu_r', axis=None)
                elif format_type == 'Color Scales':
                    return styler.background_gradient(cmap='viridis', axis=None)
                elif format_type == 'Bars':
                    return styler.bar(color=['#d65f5f', '#5fba7d'], align='zero')
            except AttributeError:
                st.warning(f"Selected formatting '{format_type}' not available. Using default formatting.")
                return styler
                
            return styler
        except Exception as e:
            st.error(f"Error applying formatting: {str(e)}")
            return pivot_df  # Return raw dataframe if all styling fails


    @staticmethod
    def create_enhanced_visualization(pivot_df: pd.DataFrame, chart_type: str, 
                                    color_theme: str = 'default',
                                    show_values: bool = True) -> go.Figure:
        """Create enhanced visualization for pivot table."""
        try:
            # Create figure based on chart type
            fig = go.Figure()
            
            # Get data ready for plotting
            if isinstance(pivot_df.index, pd.MultiIndex):
                x_values = [' - '.join(map(str, idx)) for idx in pivot_df.index]
            else:
                x_values = pivot_df.index.astype(str)
            
            # Handle different chart types
            if chart_type == 'bar':
                for col in pivot_df.columns:
                    fig.add_trace(go.Bar(
                        name=str(col),
                        x=x_values,
                        y=pivot_df[col],
                        text=pivot_df[col].round(2) if show_values else None,
                        textposition='auto',
                    ))
                fig.update_layout(barmode='group')
            
            elif chart_type == 'line':
                for col in pivot_df.columns:
                    fig.add_trace(go.Scatter(
                        name=str(col),
                        x=x_values,
                        y=pivot_df[col],
                        mode='lines+markers',
                        text=pivot_df[col].round(2) if show_values else None,
                        textposition='top center',
                    ))
            
            elif chart_type == 'area':
                for col in pivot_df.columns:
                    fig.add_trace(go.Scatter(
                        name=str(col),
                        x=x_values,
                        y=pivot_df[col],
                        mode='lines',
                        stackgroup='one',
                        text=pivot_df[col].round(2) if show_values else None,
                        textposition='top center',
                    ))
            
            elif chart_type == 'heatmap':
                # Transpose for better heatmap visualization
                pivot_transposed = pivot_df.transpose()
                fig = go.Figure(data=go.Heatmap(
                    z=pivot_transposed.values,
                    x=x_values,
                    y=pivot_transposed.index.astype(str),
                    text=pivot_transposed.values.round(2) if show_values else None,
                    texttemplate='%{text}',
                    colorscale=color_theme,
                ))
            
            # Update layout
            fig.update_layout(
                title_x=0.5,
                height=600,
                showlegend=True,
                template='plotly_white',
                xaxis_title="Categories",
                yaxis_title="Values",
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating visualization: {str(e)}")
            return None

    @staticmethod
    def apply_advanced_filters(df: pd.DataFrame, filter_config: dict) -> pd.DataFrame:
        """Apply advanced filters to dataframe."""
        try:
            filtered_df = df.copy()
            
            for col, settings in filter_config.items():
                if settings['type'] == 'range':
                    filtered_df = filtered_df[
                        (filtered_df[col] >= settings['min']) & 
                        (filtered_df[col] <= settings['max'])
                    ]
                
                elif settings['type'] == 'date_range':
                    filtered_df = filtered_df[
                        (filtered_df[col].dt.date >= settings['start']) & 
                        (filtered_df[col].dt.date <= settings['end'])
                    ]
                
                elif settings['type'] == 'select':
                    if settings.get('values'):
                        filtered_df = filtered_df[filtered_df[col].isin(settings['values'])]
                
                elif settings['type'] == 'text':
                    if settings.get('pattern'):
                        if settings.get('use_regex', False):
                            filtered_df = filtered_df[filtered_df[col].str.contains(
                                settings['pattern'], 
                                regex=True, 
                                case=not settings.get('case_sensitive', False),
                                na=False
                            )]
                        else:
                            filtered_df = filtered_df[filtered_df[col].str.contains(
                                settings['pattern'],
                                case=not settings.get('case_sensitive', False),
                                regex=False,
                                na=False
                            )]
                
                elif settings['type'] == 'top_n':
                    if settings.get('n'):
                        filtered_df = filtered_df.nlargest(settings['n'], col)
                
                elif settings['type'] == 'bottom_n':
                    if settings.get('n'):
                        filtered_df = filtered_df.nsmallest(settings['n'], col)
            
            return filtered_df
            
        except Exception as e:
            st.error(f"Error applying advanced filters: {str(e)}")
            return df

    @staticmethod
    def export_pivot_table(pivot_df: pd.DataFrame, 
                          format_type: str,
                          file_prefix: str = 'pivot_table',
                          include_styling: bool = True) -> tuple:
        """Export pivot table in various formats."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{file_prefix}_{timestamp}"
            
            if format_type == 'excel':
                # Export to Excel with formatting
                buffer = io.BytesIO()
                writer = pd.ExcelWriter(buffer, engine='xlsxwriter')
                
                if include_styling:
                    pivot_df.style.to_excel(writer, sheet_name='Pivot Table', index=True)
                else:
                    pivot_df.to_excel(writer, sheet_name='Pivot Table', index=True)
                
                writer.close()
                return buffer.getvalue(), f"{filename}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            
            elif format_type == 'csv':
                csv_data = pivot_df.to_csv(index=True)
                return csv_data, f"{filename}.csv", "text/csv"
            
            elif format_type == 'json':
                json_data = pivot_df.to_json(orient='split', date_format='iso')
                return json_data, f"{filename}.json", "application/json"
            
            elif format_type == 'html':
                html_data = pivot_df.style.to_html() if include_styling else pivot_df.to_html()
                return html_data, f"{filename}.html", "text/html"
            
            else:
                raise ValueError(f"Unsupported export format: {format_type}")
                
        except Exception as e:
            st.error(f"Error exporting pivot table: {str(e)}")
            return None, None, None

def main():
    """Main application function with enhanced data processing and analysis capabilities."""
    
    # Initialize session state variables
    if 'initial_merged_data' not in st.session_state:
        st.session_state.initial_merged_data = None  # Store initial merged data before filtering
    if 'merged_data' not in st.session_state:
        st.session_state.merged_data = None  # Store current working data
    if 'pivot_results' not in st.session_state:
        st.session_state.pivot_results = None
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'data_processor' not in st.session_state:
        st.session_state.data_processor = None
    if 'cleaning_report' not in st.session_state:
        st.session_state.cleaning_report = None

    # Configure the page
    # st.set_page_config(
    #     page_title="Advanced Excel Analysis Dashboard",
    #     page_icon="ðŸ“Š",
    #     layout="wide",
    #     initial_sidebar_state="expanded"
    # )

    # Apply custom styling
    st.markdown("""
        <style>
        .stButton > button {
            width: 100%;
        }
        .main > div {
            padding-top: 1rem;
        }
        .block-container {
            padding-top: 2rem;
        }
        .stProgress > div > div > div > div {
            background-color: #00cc00;
        }
        .styled-metric {
            padding: 10px;
            border-radius: 5px;
            background-color: #f0f2f6;
            margin: 5px;
            text-align: center;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("ðŸ“Š Advanced Excel Analysis Dashboard")

    # File Upload Section
    with st.sidebar:
        st.header("ðŸ“ File Upload")
        uploaded_files = st.file_uploader(
            "Upload Excel Files",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="Upload one or more Excel files for analysis"
        )

        if not uploaded_files:
            st.info("ðŸ‘† Please upload Excel files to begin analysis.")
            return

        # Data Cleaning Options
        st.header("âš™ï¸ Data Processing Settings")
        handle_missing = st.checkbox("Handle Missing Values", value=True,
                                   help="Replace missing values with 'Data Not Available'")
        handle_outliers = st.checkbox("Handle Outliers", value=True,
                                    help="Detect and clip outliers using IQR method")
        handle_duplicates = st.checkbox("Remove Duplicates", value=True,
                                      help="Remove exact duplicate rows")

    # Load and process files
    data_loader = DataLoader()
    dataframes = []
    
    with st.spinner('Processing uploaded files...'):
        for file in uploaded_files:
            df = data_loader.load_excel(file)
            if df is not None:
                dataframes.append(df)
                st.success(f"âœ“ Successfully loaded: {file.name}")
                
                # Enhanced file preview
                with st.expander(f"ðŸ“„ Preview: {file.name}"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("Data Sample:")
                        st.dataframe(df.head(), height=200)
                    with col2:
                        st.write("Summary:")
                        st.write(f"- Rows: {len(df):,}")
                        st.write(f"- Columns: {len(df.columns):,}")
                        st.write(f"- Missing Values: {df.isna().sum().sum():,}")
                        st.write(f"- Numeric Columns: {len(df.select_dtypes(include=[np.number]).columns)}")
                        st.write(f"- Categorical Columns: {len(df.select_dtypes(exclude=[np.number]).columns)}")

    if not dataframes:
        st.error("âŒ No valid data loaded.")
        return

    # Get common columns for merge
    common_columns = list(set.intersection(*[set(df.columns) for df in dataframes]))

    if not common_columns:
        st.error("âŒ No common columns found between files!")
        for idx, df in enumerate(dataframes):
            st.write(f"Columns in file {idx + 1}:", df.columns.tolist())
        return

    # Main Analysis Tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "ðŸ”„ Merge & Clean",
        "ðŸ“Š Analysis",
        "ðŸ“ˆ Visualization",
        "ðŸ’¾ Export"
    ])

    # Tab 1: Merge & Clean
    with tab1:
        st.header("Data Merge & Filter")
        
        # Step 1: Merge Configuration
        st.subheader("1ï¸âƒ£ Merge Configuration")
        merge_keys = st.multiselect(
            "Select columns to merge on:",
            common_columns,
            help="Select one or more columns to use as merge keys"
        )

        merge_type = st.selectbox(
            "Select merge type:",
            ['left', 'right', 'inner', 'outer'],
            help="Choose how to merge the files"
        )

        if st.button("ðŸ”„ Merge Data", use_container_width=True):
            if not merge_keys:
                st.warning("âš ï¸ Please select merge columns.")
                return

            with st.spinner("Merging data..."):
                # Perform merge
                merged_df = DataProcessor.merge_dataframes(dataframes, merge_keys, merge_type)

                if merged_df is not None:
                    # Store both initial and working copies of merged data
                    st.session_state.initial_merged_data = merged_df.copy()
                    st.session_state.merged_data = merged_df
                    st.success("âœ… Data merged successfully!")

        # Step 2: Data Filtering (only show if data is merged)
        if st.session_state.get('initial_merged_data') is not None:
            st.subheader("2ï¸âƒ£ Filter Data")
            
            with st.expander("Configure Filters", expanded=True):
                # Allow user to select columns for filtering
                filter_cols = st.multiselect(
                    "Select columns to filter:",
                    st.session_state.initial_merged_data.columns,
                    help="Choose columns you want to filter on"
                )

                # Create filters for selected columns
                active_filters = {}
                if filter_cols:
                    cols_per_row = 2
                    for i in range(0, len(filter_cols), cols_per_row):
                        cols = st.columns(cols_per_row)
                        for j, col in enumerate(filter_cols[i:i + cols_per_row]):
                            with cols[j]:
                                # Get unique values for the column
                                unique_vals = sorted(st.session_state.initial_merged_data[col].unique())
                                selected = st.multiselect(
                                    f"Filter {col}:",
                                    unique_vals,
                                    help=f"Select values to include for {col}"
                                )
                                if selected:
                                    active_filters[col] = selected

                # Apply filters button
                col1, col2 = st.columns(2)
                with col1:
                    if active_filters and st.button("Apply Filters", use_container_width=True):
                        filtered_df = st.session_state.initial_merged_data.copy()
                        for col, values in active_filters.items():
                            filtered_df = filtered_df[filtered_df[col].isin(values)]

                        # Update merged data with filtered data
                        st.session_state.merged_data = filtered_df

                        # Show filtering results
                        st.success(f"""
                            âœ… Filters applied successfully!
                            - Original rows: {len(st.session_state.initial_merged_data):,}
                            - Filtered rows: {len(filtered_df):,}
                            - Rows removed: {len(st.session_state.initial_merged_data) - len(filtered_df):,}
                        """)

                with col2:
                    if st.button("Reset Filters", use_container_width=True):
                        st.session_state.merged_data = st.session_state.initial_merged_data.copy()
                        st.success("ðŸ”„ Filters reset to original data")

            # Preview filtered data
            with st.expander("ðŸ‘€ Preview Filtered Data", expanded=False):
                if st.session_state.merged_data is not None:
                    st.dataframe(st.session_state.merged_data.head())
                    st.write(f"Total rows: {len(st.session_state.merged_data):,}")
                    st.write(f"Total columns: {len(st.session_state.merged_data.columns):,}")

            # Step 3: Data Cleaning
            st.subheader("3ï¸âƒ£ Clean Data")
            if st.button("ðŸ§¹ Clean Data", use_container_width=True):
                with st.spinner("Cleaning data..."):
                    # Initialize data processor
                    data_processor = DataProcessor()
                    st.session_state.data_processor = data_processor

                    # Clean data if selected
                    if handle_missing or handle_outliers or handle_duplicates:
                        merged_df = data_processor.clean_data(st.session_state.merged_data)

                        # Display cleaning report
                        data_processor.display_cleaning_report()
                        st.session_state.cleaning_report = data_processor.cleaning_report

                        # Store final cleaned data
                        st.session_state.merged_data = merged_df
                        st.success("âœ… Data cleaned successfully!")

    # Tab 2: Analysis (only show if we have processed data)
    if st.session_state.merged_data is not None:
        merged_df = st.session_state.merged_data

        with tab2:
            st.header("Analysis Options")

            analysis_type = st.multiselect(
                "Select Analysis Types:",
                ["Pivot Table", "Statistical Analysis", "Correlation Analysis"]
            )

            if "Pivot Table" in analysis_type:
                st.subheader("Pivot Table Settings")
                
                # Let user choose between basic and advanced pivot table
                pivot_type = st.radio(
                    "Select Pivot Table Type:",
                    ["Basic Pivot Table", "Advanced Excel-Style Pivot Table"],
                    help="Basic: Simple grouping and aggregation\nAdvanced: Excel-like functionality with rows, columns, filters"
                )
                
                if pivot_type == "Basic Pivot Table":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        group_cols = st.multiselect(
                            "Select Group By Columns:",
                            merged_df.columns,
                            help="Select columns to group by"
                        )
                    
                    with col2:
                        value_cols = st.multiselect(
                            "Select Value Columns:",
                            merged_df.columns,
                            help="Select columns to analyze"
                        )
                    
                    with col3:
                        available_aggs = ['count', 'distinct_count']
                        if any(pd.api.types.is_numeric_dtype(merged_df[col]) for col in value_cols):
                            available_aggs.extend(['sum', 'mean', 'max', 'min'])
                        if any(pd.api.types.is_string_dtype(merged_df[col]) for col in value_cols):
                            available_aggs.extend(['first', 'last', 'list'])
                        
                        agg_funcs = st.multiselect(
                            "Select Aggregations:",
                            available_aggs,
                            help="Choose aggregation functions"
                        )
                    
                    # Enhanced filtering section
                    st.subheader("ðŸ” Advanced Filters")
                    with st.expander("Configure Filters", expanded=True):
                        # Allow selecting any column for filtering
                        filter_cols = st.multiselect(
                            "Select columns to filter:",
                            merged_df.columns,
                            help="Choose any columns you want to filter on"
                        )
                        
                        adv_filters = {}
                        if filter_cols:
                            cols_per_row = 2
                            for i in range(0, len(filter_cols), cols_per_row):
                                filter_row = st.columns(cols_per_row)
                                
                                for j, col in enumerate(filter_cols[i:i + cols_per_row]):
                                    with filter_row[j]:
                                        st.write(f"Filter settings for {col}:")
                                        col_type = merged_df[col].dtype
                                        
                                        # Select filter type based on data type
                                        filter_types = ['Basic']
                                        if pd.api.types.is_numeric_dtype(col_type):
                                            filter_types.extend(['Range', 'Top N', 'Bottom N'])
                                        elif pd.api.types.is_string_dtype(col_type):
                                            filter_types.extend(['Text Search', 'Pattern Match'])
                                        
                                        filter_type = st.selectbox(
                                            "Filter type:",
                                            filter_types,
                                            key=f"filter_type_{col}"
                                        )
                                        
                                        if filter_type == 'Basic':
                                            # Multi-select filter
                                            unique_vals = sorted(merged_df[col].unique())
                                            selected = st.multiselect(
                                                f"Select values:",
                                                unique_vals,
                                                default=unique_vals,
                                                key=f"basic_sel_{col}"
                                            )
                                            adv_filters[col] = {
                                                'type': 'select',
                                                'values': selected
                                            }
                                        
                                        elif filter_type == 'Range':
                                            col1, col2 = st.columns(2)
                                            with col1:
                                                min_val = st.number_input(
                                                    "Minimum",
                                                    value=float(merged_df[col].min()),
                                                    key=f"range_min_{col}"
                                                )
                                            with col2:
                                                max_val = st.number_input(
                                                    "Maximum",
                                                    value=float(merged_df[col].max()),
                                                    key=f"range_max_{col}"
                                                )
                                            adv_filters[col] = {
                                                'type': 'range',
                                                'min': min_val,
                                                'max': max_val
                                            }
                                        
                                        elif filter_type in ['Top N', 'Bottom N']:
                                            n_val = st.number_input(
                                                f"Select number of {filter_type.lower()} records:",
                                                min_value=1,
                                                max_value=len(merged_df),
                                                value=min(5, len(merged_df)),
                                                key=f"{filter_type.lower()}_n_{col}"
                                            )
                                            adv_filters[col] = {
                                                'type': filter_type.lower().replace(' ', '_'),
                                                'n': n_val
                                            }
                                        elif filter_type in ['Text Search', 'Pattern Match']:
                                            pattern = st.text_input(
                                                "Enter search text/pattern:",
                                                key=f"pattern_{col}"
                                            )
                                            case_sensitive = st.checkbox(
                                                "Case sensitive",
                                                key=f"case_{col}"
                                            )
                                            if filter_type == 'Pattern Match':
                                                use_regex = st.checkbox(
                                                    "Use regex",
                                                    key=f"regex_{col}"
                                                )
                                            else:
                                                use_regex = False
                                            
                                            adv_filters[col] = {
                                                'type': 'text',
                                                'pattern': pattern,
                                                'case_sensitive': case_sensitive,
                                                'use_regex': use_regex
                                            }

                    # Sorting and Display Options
                    st.subheader("ðŸ“Š Display Options")
                    with st.expander("Configure Display Settings", expanded=False):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # Sorting options
                            sort_options = [
                                "No Sorting",
                                "Values (Ascending)",
                                "Values (Descending)",
                            ]
                            if group_cols:
                                sort_options.extend([
                                    "Group By (Ascending)",
                                    "Group By (Descending)"
                                ])
                            
                            sorting = st.selectbox(
                                "Sort Results:",
                                sort_options
                            )
                            
                            # Conditional formatting
                            formatting = st.selectbox(
                                "Apply Conditional Formatting:",
                                ["None", "Heatmap", "Color Scales", "Bars"]
                            )
                        
                        with col2:
                            # Show/hide options
                            show_totals = st.checkbox("Show Grand Totals", value=True)
                            show_subtotals = st.checkbox("Show Subtotals", value=True)
                            
                            # Number formatting
                            decimal_places = st.number_input(
                                "Decimal Places:",
                                min_value=0,
                                max_value=6,
                                value=2
                            )

                    # Visualization Options
                    st.subheader("ðŸ“ˆ Visualization")
                    with st.expander("Configure Visualization", expanded=False):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            chart_type = st.selectbox(
                                "Chart Type:",
                                ["bar", "line", "area", "heatmap"]
                            )
                            
                            color_theme = st.selectbox(
                                "Color Theme:",
                                ["default", "viridis", "plasma", "inferno", "magma"]
                            )
                        
                        with col2:
                            show_values = st.checkbox("Show Values on Chart", value=True)
                            show_legend = st.checkbox("Show Legend", value=True)

                    # Generate Button
                    if st.button("Generate Basic Pivot Table", use_container_width=True):
                        if not (group_cols and value_cols and agg_funcs):
                            st.warning("Please select all required pivot table parameters.")
                            return
                        
                        with st.spinner("Creating pivot table..."):
                            try:
                                # Apply advanced filters
                                filtered_df = PivotHelpers.apply_advanced_filters(merged_df, adv_filters)
                                rows_before = len(merged_df)
                                rows_after = len(filtered_df)
                                
                                # Create the pivot table
                                analytics = Analytics(filtered_df)
                                pivot_result = analytics.create_pivot(
                                    group_cols,
                                    value_cols,
                                    agg_funcs,
                                    {}  # No additional filters needed
                                )
                                
                                if pivot_result is not None:
                                    st.session_state.pivot_results = pivot_result
                                    
                                    # Show filtering results if any rows were filtered
                                    if rows_before != rows_after:
                                        st.info(f"""
                                            Filtering results:
                                            - Original rows: {rows_before:,}
                                            - Filtered rows: {rows_after:,}
                                            - Rows filtered out: {rows_before - rows_after:,}
                                            - % data filtered: {((rows_before - rows_after) / rows_before * 100):.1f}%
                                        """)
                                    
                                    # Apply sorting if selected
                                    if sorting != "No Sorting":
                                        sort_config = {
                                            'ascending': 'Ascending' in sorting,
                                            'by_index': 'Group By' in sorting
                                        }
                                        pivot_result = PivotHelpers.apply_sorting(pivot_result, sort_config)
                                    
                                    # Format and display the pivot table
                                    st.write("##### Pivot Table Results")
                                    
                                    # Apply conditional formatting and number formatting
                                    formatted_pivot = pivot_result.style  # Get style object first
                                    if formatting != 'None':
                                        formatted_pivot = PivotHelpers.apply_conditional_formatting(pivot_result, formatting)
                                    formatted_pivot = formatted_pivot.format("{:,." + str(decimal_places) + "f}")
                                    st.dataframe(formatted_pivot)
                                    
                                    # Create and display visualization if requested
                                    if chart_type:
                                        st.write("##### Visualization")
                                        fig = PivotHelpers.create_enhanced_visualization(
                                            pivot_result,
                                            chart_type,
                                            color_theme,
                                            show_values
                                        )
                                        if fig:
                                            st.plotly_chart(fig, use_container_width=True)
                                    
                                    # Export options
                                    st.write("##### Export Options")
                                    export_col1, export_col2 = st.columns(2)
                                    
                                    with export_col1:
                                        export_format = st.selectbox(
                                            "Select Export Format:",
                                            ["excel", "csv", "json", "html"]
                                        )
                                    
                                    with export_col2:
                                        include_styling = st.checkbox(
                                            "Include Formatting",
                                            value=True,
                                            help="Include conditional formatting and styling in export"
                                        )
                                    
                                    # Export button
                                    if st.button("Export Pivot Table", use_container_width=True):
                                        export_data, filename, mime_type = PivotHelpers.export_pivot_table(
                                            pivot_result,
                                            export_format,
                                            'pivot_table',
                                            include_styling
                                        )
                                        
                                        if export_data and filename and mime_type:
                                            st.download_button(
                                                label=f"ðŸ“¥ Download {export_format.upper()}",
                                                data=export_data,
                                                file_name=filename,
                                                mime=mime_type
                                            )
                            
                            except Exception as e:
                                st.error(f"Error creating pivot table: {str(e)}")
                                st.error("Please check your selections and try again.")
                
                else:  # Advanced Excel-Style Pivot Table
                    # Initialize variables
                    filter_values = {}
                    
                    # Create Excel-like interface
                    col1, col2 = st.columns([2, 3])
                    
                    with col1:
                        st.write("##### Pivot Table Fields")
                        
                        # Filters section
                        filter_cols = st.multiselect(
                            "Filters (Optional):",
                            merged_df.columns,
                            help="Select columns to filter the entire pivot table"
                        )
                        
                        # Rows section
                        row_cols = st.multiselect(
                            "Rows:",
                            merged_df.columns,
                            help="Select columns for row labels"
                        )
                        
                        # Columns section
                        column_cols = st.multiselect(
                            "Columns:",
                            merged_df.columns,
                            help="Select columns for column labels"
                        )
                        
                        # Values section
                        value_cols = st.multiselect(
                            "Values:",
                            merged_df.columns,
                            help="Select columns to analyze"
                        )
                    
                    # Value Settings
                    with col2:
                        if value_cols:
                            st.write("##### Value Settings")
                            value_settings = {}
                            
                            for col in value_cols:
                                st.write(f"Settings for {col}:")
                                settings_col1, settings_col2 = st.columns(2)
                                
                                with settings_col1:
                                    # Available aggregations based on column type
                                    available_aggs = ['count', 'distinct_count']
                                    if pd.api.types.is_numeric_dtype(merged_df[col]):
                                        available_aggs.extend(['sum', 'mean', 'max', 'min', 'median', 'std',
                                                            'first', 'last', 'var', 'sum%', 'running total'])
                                    elif pd.api.types.is_string_dtype(merged_df[col]):
                                        available_aggs.extend(['first', 'last', 'list', 'unique count'])
                                    
                                    agg_func = st.selectbox(
                                        f"Aggregation for {col}:",
                                        available_aggs,
                                        key=f"agg_{col}"
                                    )
                                
                                with settings_col2:
                                    # Show values as options
                                    show_as = st.selectbox(
                                        "Show values as:",
                                        ['Values',
                                         '% of Row Total',
                                         '% of Column Total',
                                         '% of Grand Total',
                                         'Difference From',
                                         '% Difference From',
                                         'Running Total',
                                         'Rank'],
                                        key=f"show_{col}"
                                    )
                                    
                                    if show_as in ['Difference From', '% Difference From']:
                                        base_field = st.selectbox(
                                            "Base field:",
                                            ['(Previous)', '(Next)', '(First)', '(Last)'] + 
                                            [c for c in pivot_df.columns if c != col],
                                            key=f"base_{col}"
                                        )
                                    else:
                                        base_field = None
                                
                                # Number formatting options
                                format_col1, format_col2 = st.columns(2)
                                with format_col1:
                                    decimal_places = st.number_input(
                                        "Decimal places:",
                                        min_value=0,
                                        max_value=6,
                                        value=2,
                                        key=f"decimal_{col}"
                                    )
                                
                                with format_col2:
                                    if pd.api.types.is_numeric_dtype(merged_df[col]):
                                        number_format = st.selectbox(
                                            "Number format:",
                                            ['Normal',
                                             'Currency',
                                             'Percentage',
                                             'Scientific',
                                             'Thousands Separator'],
                                            key=f"format_{col}"
                                        )
                                    else:
                                        number_format = 'Normal'
                                
                                value_settings[col] = {
                                    'aggregation': agg_func,
                                    'show_as': show_as,
                                    'base_field': base_field,
                                    'decimal_places': decimal_places,
                                    'number_format': number_format
                                }
                    
                    # Filter interface
                    if filter_cols:
                        st.write("##### Filter Settings")
                        with st.expander("Configure Filters", expanded=True):
                            for col in filter_cols:
                                st.write(f"ðŸ“ Filter for {col}")
                                filter_col1, filter_col2 = st.columns(2)
                                
                                with filter_col1:
                                    filter_type = st.selectbox(
                                        "Filter type:",
                                        ["Value Filter", "Label Filter", "Advanced Filter"],
                                        key=f"ftype_{col}"
                                    )
                                
                                with filter_col2:
                                    if filter_type == "Value Filter":
                                        filter_method = st.selectbox(
                                            "Method:",
                                            ["Equals", "Does not equal",
                                             "Greater than", "Less than",
                                             "Between", "Top N", "Bottom N"],
                                            key=f"fmethod_{col}"
                                        )
                                    elif filter_type == "Label Filter":
                                        filter_method = st.selectbox(
                                            "Method:",
                                            ["Contains", "Does not contain",
                                             "Begins with", "Ends with",
                                             "Regex match"],
                                            key=f"fmethod_{col}"
                                        )
                                    else:  # Advanced Filter
                                        filter_method = st.selectbox(
                                            "Method:",
                                            ["Custom condition", "Date range",
                                             "Relative date", "NULL handling"],
                                            key=f"fmethod_{col}"
                                        )
                                
                                # Filter criteria based on type and method
                                if filter_type == "Value Filter":
                                    if filter_method in ["Top N", "Bottom N"]:
                                        n_val = st.number_input(
                                            f"Select number of {filter_method.lower()} items:",
                                            min_value=1,
                                            max_value=len(merged_df),
                                            value=min(5, len(merged_df)),
                                            key=f"n_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': filter_method.lower().replace(' ', '_'),
                                            'n': n_val
                                        }
                                    elif filter_method == "Between":
                                        min_val = st.number_input("Minimum value:", key=f"min_{col}")
                                        max_val = st.number_input("Maximum value:", key=f"max_{col}")
                                        filter_values[col] = {
                                            'type': 'range',
                                            'min': min_val,
                                            'max': max_val
                                        }
                                    else:
                                        comparison_value = st.number_input(
                                            "Value:",
                                            key=f"val_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': 'comparison',
                                            'method': filter_method.lower().replace(' ', '_'),
                                            'value': comparison_value
                                        }
                                
                                elif filter_type == "Label Filter":
                                    pattern = st.text_input(
                                        "Enter pattern:",
                                        key=f"pattern_{col}"
                                    )
                                    case_sensitive = st.checkbox(
                                        "Case sensitive",
                                        key=f"case_{col}"
                                    )
                                    filter_values[col] = {
                                        'type': 'text',
                                        'method': filter_method.lower().replace(' ', '_'),
                                        'pattern': pattern,
                                        'case_sensitive': case_sensitive
                                    }
                                
                                else:  # Advanced Filter
                                    if filter_method == "Custom condition":
                                        condition = st.text_input(
                                            "Enter Python condition (e.g., 'x > 0 and x < 100'):",
                                            key=f"condition_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': 'custom',
                                            'condition': condition
                                        }
                                    elif filter_method == "Date range":
                                        start_date = st.date_input(
                                            "Start date:",
                                            key=f"start_{col}"
                                        )
                                        end_date = st.date_input(
                                            "End date:",
                                            key=f"end_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': 'date_range',
                                            'start': start_date,
                                            'end': end_date
                                        }
                                    elif filter_method == "Relative date":
                                        relative_period = st.selectbox(
                                            "Period:",
                                            ["Last 7 days", "Last 30 days",
                                             "Last 90 days", "Last 12 months",
                                             "Year to date", "Custom"],
                                            key=f"period_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': 'relative_date',
                                            'period': relative_period
                                        }
                                    else:  # NULL handling
                                        null_option = st.selectbox(
                                            "NULL handling:",
                                            ["Include NULL", "Exclude NULL",
                                             "Show only NULL"],
                                            key=f"null_{col}"
                                        )
                                        filter_values[col] = {
                                            'type': 'null_handling',
                                            'option': null_option
                                        }
                    
                    # Layout and Display Options
                    st.write("##### Layout Options")
                    with st.expander("Configure Layout", expanded=False):
                        layout_col1, layout_col2 = st.columns(2)
                        
                        with layout_col1:
                            layout_type = st.selectbox(
                                "Layout:",
                                ["Compact", "Outline", "Tabular"]
                            )
                            repeat_labels = st.checkbox("Repeat row labels", value=False)
                            merge_cells = st.checkbox("Merge cells", value=True)
                        
                        with layout_col2:
                            show_empty = st.selectbox(
                                "Empty cells show:",
                                ["(Blank)", "0", "NULL", "N/A"]
                            )
                            subtotals_position = st.selectbox(
                                "Subtotals position:",
                                ["Top", "Bottom", "Off"]
                            )
                    # Visualization Settings
                    st.write("##### Visualization Options")
                    with st.expander("Configure Visualization", expanded=False):
                        viz_col1, viz_col2 = st.columns(2)
                        
                        with viz_col1:
                            chart_type = st.selectbox(
                                "Chart Type:",
                                ["Bar", "Line", "Area", "Scatter",
                                 "Heatmap", "Treemap", "Sunburst",
                                 "Box Plot", "Violin Plot"]
                            )
                            
                            color_theme = st.selectbox(
                                "Color Theme:",
                                ["Default", "Viridis", "Plasma", "Magma",
                                 "Blues", "Reds", "Greens", "Custom"]
                            )
                            
                            if color_theme == "Custom":
                                primary_color = st.color_picker("Pick primary color", "#1f77b4")
                                secondary_color = st.color_picker("Pick secondary color", "#ff7f0e")
                        
                        with viz_col2:
                            show_values = st.checkbox("Show values on chart", value=True)
                            show_legend = st.checkbox("Show legend", value=True)
                            interactive = st.checkbox("Interactive features", value=True)
                            
                            if chart_type == "Bar":
                                bar_mode = st.selectbox(
                                    "Bar Mode:",
                                    ["Group", "Stack", "Relative"]
                                )
                                orientation = st.selectbox(
                                    "Orientation:",
                                    ["Vertical", "Horizontal"]
                                )
                    
                    # Generate Advanced Pivot Table
                    if st.button("Generate Advanced Pivot Table", use_container_width=True):
                        if not (row_cols or column_cols) or not value_cols:
                            st.warning("Please select at least one row/column field and one value field.")
                            return
                        
                        with st.spinner("Creating advanced pivot table..."):
                            try:
                                # Apply filters first
                                filtered_df = PivotHelpers.apply_advanced_filters(merged_df, filter_values)
                                rows_before = len(merged_df)
                                rows_after = len(filtered_df)
                                
                                # Create pivot table with all settings
                                agg_funcs = {col: settings['aggregation'] for col, settings in value_settings.items()}
                                
                                # Handle special aggregations
                                for col, agg in agg_funcs.items():
                                    if agg == 'distinct_count':
                                        agg_funcs[col] = lambda x: len(x.unique())
                                    elif agg == 'sum%':
                                        agg_funcs[col] = lambda x: (x.sum() / x.sum().sum()) * 100
                                    elif agg == 'running total':
                                        agg_funcs[col] = lambda x: x.cumsum()
                                
                                # Create pivot table
                                pivot_table = pd.pivot_table(
                                    filtered_df,
                                    values=value_cols,
                                    index=row_cols,
                                    columns=column_cols if column_cols else None,
                                    aggfunc=agg_funcs,
                                    margins=True,
                                    margins_name="Grand Total",
                                    fill_value=0 if show_empty == "0" else None
                                )
                                
                                # Apply value transformations
                                for col, settings in value_settings.items():
                                    if settings['show_as'] != 'Values':
                                        if settings['show_as'] == '% of Row Total':
                                            pivot_table[col] = pivot_table[col].div(pivot_table[col].sum(axis=1), axis=0) * 100
                                        elif settings['show_as'] == '% of Column Total':
                                            pivot_table[col] = pivot_table[col].div(pivot_table[col].sum(axis=0), axis=1) * 100
                                        elif settings['show_as'] == '% of Grand Total':
                                            total = pivot_table[col].iloc[-1, -1]
                                            pivot_table[col] = pivot_table[col] / total * 100
                                        elif settings['show_as'] == 'Difference From':
                                            base = settings['base_field']
                                            if base == '(Previous)':
                                                pivot_table[col] = pivot_table[col].diff()
                                            elif base == '(Next)':
                                                pivot_table[col] = -pivot_table[col].diff(-1)
                                            elif base == '(First)':
                                                first_val = pivot_table[col].iloc[0]
                                                pivot_table[col] = pivot_table[col] - first_val
                                            elif base == '(Last)':
                                                last_val = pivot_table[col].iloc[-1]
                                                pivot_table[col] = pivot_table[col] - last_val
                                        elif settings['show_as'] == 'Running Total':
                                            pivot_table[col] = pivot_table[col].cumsum()
                                        elif settings['show_as'] == 'Rank':
                                            pivot_table[col] = pivot_table[col].rank(ascending=False)
                                
                                st.session_state.pivot_results = pivot_table
                                
                                # Show filtering results
                                if rows_before != rows_after:
                                    st.info(f"""
                                        Filtering results:
                                        - Original rows: {rows_before:,}
                                        - Filtered rows: {rows_after:,}
                                        - Rows filtered out: {rows_before - rows_after:,}
                                        - % data filtered: {((rows_before - rows_after) / rows_before * 100):.1f}%
                                    """)
                                
                                # Display pivot table with formatting
                                st.write("##### Pivot Table Results")
                                
                                # Format numbers based on settings
                                formatted_pivot = pivot_table.style
                                
                                for col, settings in value_settings.items():
                                    format_str = "{:,." + str(settings['decimal_places']) + "f}"
                                    if settings['number_format'] == 'Percentage':
                                        format_str = "{:." + str(settings['decimal_places']) + "%}"
                                    elif settings['number_format'] == 'Currency':
                                        format_str = "${:,." + str(settings['decimal_places']) + "f}"
                                    elif settings['number_format'] == 'Scientific':
                                        format_str = "{:." + str(settings['decimal_places']) + "e}"
                                    
                                    formatted_pivot = formatted_pivot.format({col: format_str})
                                
                                # Display the formatted pivot table
                                st.dataframe(formatted_pivot)
                                
                                # Create visualization if requested
                                if chart_type:
                                    st.write("##### Visualization")
                                    
                                    # Create base figure
                                    fig = go.Figure()
                                    
                                    # Get data ready for plotting
                                    if isinstance(pivot_table.index, pd.MultiIndex):
                                        x_values = [' - '.join(map(str, idx)) for idx in pivot_table.index]
                                    else:
                                        x_values = pivot_table.index.astype(str)
                                    
                                    # Create traces based on chart type
                                    for col in pivot_table.columns:
                                        if chart_type == "Bar":
                                            if orientation == "Horizontal":
                                                fig.add_trace(go.Bar(
                                                    name=str(col),
                                                    x=pivot_table[col],
                                                    y=x_values,
                                                    orientation='h',
                                                    text=pivot_table[col].round(2) if show_values else None,
                                                    textposition='auto'
                                                ))
                                            else:
                                                fig.add_trace(go.Bar(
                                                    name=str(col),
                                                    x=x_values,
                                                    y=pivot_table[col],
                                                    text=pivot_table[col].round(2) if show_values else None,
                                                    textposition='auto'
                                                ))
                                            
                                            # Set bar mode
                                            fig.update_layout(barmode=bar_mode.lower())
                                            
                                        elif chart_type == "Line":
                                            fig.add_trace(go.Scatter(
                                                name=str(col),
                                                x=x_values,
                                                y=pivot_table[col],
                                                mode='lines+markers',
                                                text=pivot_table[col].round(2) if show_values else None,
                                                textposition='top center'
                                            ))
                                            
                                        elif chart_type == "Area":
                                            fig.add_trace(go.Scatter(
                                                name=str(col),
                                                x=x_values,
                                                y=pivot_table[col],
                                                mode='lines',
                                                fill='tonexty',
                                                stackgroup='one',
                                                text=pivot_table[col].round(2) if show_values else None,
                                                textposition='top center'
                                            ))
                                    
                                    # Update layout
                                    fig.update_layout(
                                        title=f'{chart_type} Chart of Pivot Data',
                                        title_x=0.5,
                                        height=600,
                                        showlegend=show_legend,
                                        template='plotly_white',
                                        margin=dict(t=100, l=50, r=50, b=50)
                                    )
                                    
                                    # Add axis titles
                                    if orientation == "Horizontal" and chart_type == "Bar":
                                        fig.update_layout(
                                            xaxis_title="Values",
                                            yaxis_title="Categories"
                                        )
                                    else:
                                        fig.update_layout(
                                            xaxis_title="Categories",
                                            yaxis_title="Values"
                                        )
                                    
                                    # Apply color theme
                                    if color_theme != "Default":
                                        if color_theme == "Custom":
                                            colors = [primary_color, secondary_color]
                                        else:
                                            colors = color_theme.lower()
                                        
                                        for trace in fig.data:
                                            if hasattr(trace, 'marker'):
                                                trace.marker.color = colors
                                            if hasattr(trace, 'line'):
                                                trace.line.color = colors
                                    
                                    # Add interactivity
                                    if interactive:
                                        fig.update_layout(
                                            clickmode='event+select',
                                            dragmode='zoom',
                                            hovermode='closest'
                                        )
                                    
                                    # Display the chart
                                    st.plotly_chart(fig, use_container_width=True)
                                
                                # Export options section
                                st.write("##### Export Options")
                                export_col1, export_col2, export_col3 = st.columns(3)
                                
                                with export_col1:
                                    export_format = st.selectbox(
                                        "Export Format:",
                                        ["Excel", "CSV", "JSON", "HTML"],
                                        key="adv_export_format"
                                    )
                                
                                with export_col2:
                                    include_styling = st.checkbox(
                                        "Include Formatting",
                                        value=True,
                                        key="adv_include_styling"
                                    )
                                
                                with export_col3:
                                    if export_format == "Excel":
                                        include_charts = st.checkbox(
                                            "Include Charts",
                                            value=True,
                                            key="adv_include_charts"
                                        )
                                
                                # Export buttons
                                if st.button("Export Full Report", key="adv_export_button", use_container_width=True):
                                    # Export pivot table
                                    export_data, filename, mime_type = PivotHelpers.export_pivot_table(
                                        pivot_table,
                                        export_format.lower(),
                                        'advanced_pivot',
                                        include_styling
                                    )
                                    
                                    if export_data and filename and mime_type:
                                        st.download_button(
                                            label=f"ðŸ“¥ Download {export_format}",
                                            data=export_data,
                                            file_name=filename,
                                            mime=mime_type,
                                            key="adv_download_data"
                                        )
                                    
                                    # Export visualization if available
                                    if 'fig' in locals() and include_charts:
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            # Export as HTML
                                            html_fig = fig.to_html(
                                                include_plotlyjs='cdn',
                                                full_html=False
                                            )
                                            st.download_button(
                                                "ðŸ“¥ Download Interactive Chart (HTML)",
                                                html_fig,
                                                file_name=f"chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                                                mime="text/html",
                                                key="adv_download_html"
                                            )
                                        
                                        with col2:
                                            # Export as PNG
                                            img_bytes = fig.to_image(format="png")
                                            st.download_button(
                                                "ðŸ“¥ Download Chart Image (PNG)",
                                                img_bytes,
                                                file_name=f"chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                                                mime="image/png",
                                                key="adv_download_png"
                                            )
                            
                            except Exception as e:
                                st.error(f"Error creating advanced pivot table: {str(e)}")
                                st.error("Please check your selections and try again.")
                                # Print the full error traceback for debugging
                                import traceback
                                st.error(traceback.format_exc())

            # Statistical Analysis Section
            if "Statistical Analysis" in analysis_type:
                st.subheader("Statistical Analysis")

                # Select columns for analysis
                stat_columns = st.multiselect(
                    "Select columns for statistical analysis:",
                    merged_df.columns
                )

                if stat_columns and st.button("Calculate Statistics", use_container_width=True):
                    with st.spinner("Calculating statistics..."):
                        analytics = Analytics(merged_df)
                        stats_results = {}

                        for col in stat_columns:
                            stats_results[col] = analytics.calculate_statistics(col)

                        st.session_state.analysis_results = stats_results

                        # Display results in expandable sections
                        for col, stats in stats_results.items():
                            with st.expander(f"ðŸ“Š {col} Statistics", expanded=True):
                                col1, col2, col3 = st.columns(3)

                                with col1:
                                    st.write("Basic Statistics")
                                    st.write(f"- Count: {stats['count']:,}")
                                    if pd.api.types.is_numeric_dtype(merged_df[col]):
                                        st.write(f"- Mean: {stats['mean']:,.2f}")
                                        st.write(f"- Median: {stats['median']:,.2f}")
                                        st.write(f"- Std Dev: {stats['std']:,.2f}")

                                with col2:
                                    st.write("Distribution")
                                    if pd.api.types.is_numeric_dtype(merged_df[col]):
                                        st.write(f"- Skewness: {stats['skew']:,.2f}")
                                        st.write(f"- Kurtosis: {stats['kurtosis']:,.2f}")
                                        if 'percentiles' in stats:
                                            st.write("Percentiles:")
                                            for p, v in stats['percentiles'].items():
                                                st.write(f"  - {p}: {v:,.2f}")

                                with col3:
                                    st.write("Data Quality")
                                    st.write(f"- Unique Values: {stats['unique']:,}")
                                    st.write(f"- Missing Values: {stats['missing']:,}")
                                    st.write(f"- Missing %: {stats['missing_pct']:,.2f}%")

            # Correlation Analysis Section
            if "Correlation Analysis" in analysis_type:
                st.subheader("Correlation Analysis")

                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    # Select columns for correlation
                    corr_cols = st.multiselect(
                        "Select columns for correlation:",
                        numeric_cols,
                        default=list(numeric_cols)[:5]
                    )

                    if corr_cols and len(corr_cols) > 1:
                        if st.button("Generate Correlation Analysis", use_container_width=True):
                            with st.spinner("Calculating correlations..."):
                                corr_df = merged_df[corr_cols]
                                viz = Visualizer(corr_df)

                                # Create correlation analysis
                                corr_fig, corr_matrix = viz.create_correlation_analysis()

                                if corr_fig and corr_matrix is not None:
                                    st.write("### Correlation Matrix")
                                    st.dataframe(
                                        corr_matrix.style
                                        .background_gradient(cmap='RdBu_r', vmin=-1, vmax=1)
                                        .format("{:.3f}")
                                    )

                                    st.write("### Correlation Heatmap")
                                    st.plotly_chart(corr_fig, use_container_width=True)

                                    # Add correlation insights
                                    st.write("### Key Insights")
                                    strong_corr = []
                                    moderate_corr = []

                                    for i in range(len(corr_cols)):
                                        for j in range(i + 1, len(corr_cols)):
                                            corr_val = abs(corr_matrix.iloc[i, j])
                                            if corr_val >= 0.7:
                                                strong_corr.append((corr_cols[i], corr_cols[j], corr_val))
                                            elif corr_val >= 0.4:
                                                moderate_corr.append((corr_cols[i], corr_cols[j], corr_val))

                                    if strong_corr:
                                        st.write("Strong Correlations (|r| â‰¥ 0.7):")
                                        for col1, col2, val in strong_corr:
                                            st.write(f"- {col1} vs {col2}: {val:.3f}")

                                    if moderate_corr:
                                        st.write("Moderate Correlations (0.4 â‰¤ |r| < 0.7):")
                                        for col1, col2, val in moderate_corr:
                                            st.write(f"- {col1} vs {col2}: {val:.3f}")
                else:
                    st.warning("Need at least 2 numeric columns for correlation analysis.")

        # Tab 3: Visualization
        with tab3:
            st.header("Visualization Options")

            if st.session_state.pivot_results is not None:
                st.subheader("Pivot Table Visualization")

                # Chart configuration
                col1, col2 = st.columns(2)
                with col1:
                    chart_type = st.selectbox(
                        "Chart Type:",
                        ['bar', 'line', 'scatter', 'area', 'box'],
                        key="viz_chart_type"
                    )

                with col2:
                    color_theme = st.selectbox(
                        "Color Theme:",
                        ['default', 'viridis', 'plasma', 'inferno', 'magma'],
                        key="viz_color_theme"
                    )

                # Additional visualization options
                col3, col4 = st.columns(2)
                with col3:
                    show_values = st.checkbox(
                        "Show Values on Chart", 
                        value=True,
                        key="viz_show_values"
                    )
                with col4:
                    normalize = st.checkbox(
                        "Normalize Values (0-100%)", 
                        value=False,
                        key="viz_normalize"
                    )

                # Create and display the visualization
                viz = Visualizer(merged_df)
                fig = viz.create_pivot_chart(
                    pivot_df=st.session_state.pivot_results,
                    chart_type=chart_type,
                    color_theme=color_theme,
                    show_values=show_values,
                    normalize=normalize
                )

                if fig:
                    st.plotly_chart(fig, use_container_width=True)

                    # Export options
                    st.subheader("Export Visualization")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Download as HTML
                        html_buf = io.StringIO()
                        fig.write_html(html_buf)
                        st.download_button(
                            label="Download as Interactive HTML",
                            data=html_buf.getvalue(),
                            file_name=f"visualization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                            mime="text/html",
                            key="viz_download_html"
                        )

                    with col2:
                        # Download as PNG
                        img_buf = io.BytesIO()
                        fig.write_image(img_buf)
                        st.download_button(
                            label="Download as PNG Image",
                            data=img_buf.getvalue(),
                            file_name=f"visualization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                            mime="image/png",
                            key="viz_download_png"
                        )
            else:
                st.info("Create a pivot table first to see visualizations.")

        # Tab 4: Export
        with tab4:
            st.header("Export Options")

            export_format = st.selectbox(
                "Select Export Format:",
                ["Excel Report", "CSV Data", "Full Analysis Report"]
            )

            if export_format == "Excel Report":
                st.write("Excel report will include:")
                st.write("- Processed data")
                st.write("- Pivot tables (if created)")
                st.write("- Statistical analysis (if performed)")
                st.write("- Data cleaning report")

                if st.button("Generate Excel Report", use_container_width=True):
                    with st.spinner("Generating Excel report..."):
                        export_manager = ExportManager()
                        excel_data = export_manager.to_excel(
                            merged_df,
                            st.session_state.pivot_results,
                            st.session_state.analysis_results,
                            st.session_state.cleaning_report
                        )

                        if excel_data:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                label="ðŸ“¥ Download Excel Report",
                                data=excel_data,
                                file_name=f"analysis_report_{timestamp}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )

            elif export_format == "CSV Data":
                st.write("Export options:")
                include_index = st.checkbox("Include Row Index", value=False)
                compression = st.selectbox(
                    "Compression:",
                    [None, 'zip', 'gzip']
                )

                if st.button("Export to CSV", use_container_width=True):
                    with st.spinner("Preparing CSV export..."):
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        csv = merged_df.to_csv(index=include_index)

                        if compression == 'zip':
                            import zipfile
                            zip_buffer = io.BytesIO()
                            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                                zf.writestr(f"data_{timestamp}.csv", csv)
                            
                            st.download_button(
                                label="ðŸ“¥ Download Zipped CSV",
                                data=zip_buffer.getvalue(),
                                file_name=f"data_{timestamp}.zip",
                                mime="application/zip"
                            )
                        
                        elif compression == 'gzip':
                            import gzip
                            gz_buffer = io.BytesIO()
                            with gzip.GzipFile(fileobj=gz_buffer, mode='w') as gz:
                                gz.write(csv.encode('utf-8'))
                            
                            st.download_button(
                                label="ðŸ“¥ Download Gzipped CSV",
                                data=gz_buffer.getvalue(),
                                file_name=f"data_{timestamp}.csv.gz",
                                mime="application/gzip"
                            )
                        
                        else:
                            st.download_button(
                                label="ðŸ“¥ Download CSV",
                                data=csv,
                                file_name=f"data_{timestamp}.csv",
                                mime="text/csv"
                            )

            elif export_format == "Full Analysis Report":
                st.write("Full report will include:")
                st.write("- Data cleaning summary")
                st.write("- Statistical analysis results")
                st.write("- Correlation analysis")
                st.write("- Visualization exports")

                if st.button("Generate Full Report", use_container_width=True):
                    report = []
                    report.append("# Data Analysis Report")
                    report.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

                    # Add cleaning report
                    if st.session_state.cleaning_report:
                        report.append("## Data Cleaning Summary")
                        summary = st.session_state.cleaning_report['summary']
                        report.append(f"- Initial rows: {summary['initial_state']['rows']:,}")
                        report.append(f"- Final rows: {summary['final_state']['rows']:,}")
                        report.append(f"- Missing values handled: {summary['changes']['missing_values_handled']:,}")
                        report.append(f"- Outliers handled: {summary['changes']['outliers_handled']:,}")
                        report.append(f"- Duplicates removed: {summary['changes']['duplicates_removed']:,}\n")

                    # Add analysis results
                    if st.session_state.analysis_results:
                        report.append("## Statistical Analysis Results")
                        for col, stats in st.session_state.analysis_results.items():
                            report.append(f"\n### {col}")
                            for metric, value in stats.items():
                                if isinstance(value, (int, float)):
                                    report.append(f"- {metric}: {value:,.2f}")
                                else:
                                    report.append(f"- {metric}: {value}")

                    # Convert to markdown and offer download
                    report_md = "\n".join(report)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    st.download_button(
                        label="ðŸ“¥ Download Full Report",
                        data=report_md,
                        file_name=f"analysis_report_{timestamp}.md",
                        mime="text/markdown"
                    )

if __name__ == "__main__":
    main()
