import json
import os
from datetime import datetime
from openai import OpenAI
from typing import Dict, Any, List, Optional
import hashlib
from rdflib import Graph, Literal, RDF, RDFS, Namespace, URIRef
from rdflib.namespace import XSD, FOAF
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveAIJsonAnalyzer:
    def __init__(self, api_key: Optional[str] = None):
        """Initialize with OpenAI API key and RDF namespace"""
        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        
        # Define RDF namespaces
        self.SCHEMA = Namespace("http://schema.org/")
        self.JSON_SCHEMA = Namespace("http://json-schema.org/")
        self.CUSTOM = Namespace("http://example.org/jsonschema/")
        
        # Initialize RDF graph
        self.rdf_graph = Graph()
        self.rdf_graph.bind("schema", self.SCHEMA)
        self.rdf_graph.bind("jsonschema", self.JSON_SCHEMA)
        self.rdf_graph.bind("custom", self.CUSTOM)
        
    def generate_comprehensive_analysis(self, json_file_path: str, output_dir: str = "analysis_output"):
        """Generate comprehensive analysis with multiple AI models and RDF graph"""
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print("üöÄ Starting Comprehensive AI JSON Analysis...")
        
        # Load and validate JSON
        try:
            with open(json_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                data = json.load(f)
            print(f"‚úÖ Successfully loaded JSON from {json_file_path}")
        except Exception as e:
            logger.error(f"Failed to load JSON: {e}")
            return
        
        # Generate unique analysis ID
        analysis_id = hashlib.md5(f"{json_file_path}_{datetime.now()}".encode()).hexdigest()[:8]
        
        # Phase 1: Deep Schema Analysis with o3-mini
        print("üß† Phase 1: Deep Schema Analysis with o3-mini...")
        schema_analysis = self._deep_schema_analysis(data, "o3-mini")
        
        # Phase 2: Business Domain Analysis with GPT-4o
        print("üè¢ Phase 2: Business Domain Analysis...")
        business_analysis = self._business_domain_analysis(data, "gpt-4o")
        
        # Phase 3: Entity Relationship Mapping with o3-mini high reasoning
        print("üîç Phase 3: Entity Relationship Mapping...")
        entity_analysis = self._entity_relationship_analysis(data, "o3-mini")
        
        # Phase 4: Data Quality Assessment
        print("üìä Phase 4: Data Quality Assessment...")
        quality_analysis = self._data_quality_analysis(data, "gpt-4o-mini")
        
        # Phase 5: Generate RDF Graph
        print("üåê Phase 5: Generating RDF Graph...")
        self._generate_rdf_graph(data, schema_analysis, entity_analysis)
        
        # Phase 6: Security and Privacy Analysis
        print("üîí Phase 6: Security and Privacy Analysis...")
        security_analysis = self._security_privacy_analysis(data, "o3-mini")
        
        # Phase 7: Performance and Optimization Recommendations
        print("‚ö° Phase 7: Performance Analysis...")
        performance_analysis = self._performance_analysis(data, "gpt-4o")
        
        # Compile comprehensive report
        report_content = self._compile_comprehensive_report(
            json_file_path, analysis_id, schema_analysis, business_analysis,
            entity_analysis, quality_analysis, security_analysis, performance_analysis
        )
        
        # Save all outputs
        self._save_analysis_outputs(output_dir, analysis_id, report_content, data)
        
        print(f"üéâ Analysis complete! Results saved in {output_dir}/")
        
    def _deep_schema_analysis(self, data: Any, model: str = "o3-mini") -> str:
        """Deep schema analysis using o3-mini reasoning capabilities"""
        
        # Convert data to comprehensive schema representation
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As an expert data architect and schema analyst, perform a comprehensive deep analysis of this JSON schema.
        Use your advanced reasoning capabilities to identify patterns, structures, and relationships.

        JSON Data (COMPLETE - NO TRUNCATION):
        {full_data_str}

        Provide a detailed analysis covering:

        ## 1. STRUCTURAL ANALYSIS
        - Complete hierarchical breakdown of all nested structures
        - Data type patterns and consistency analysis
        - Array patterns and item type analysis
        - Null value patterns and optional field identification

        ## 2. SCHEMA COMPLEXITY METRICS
        - Nesting depth analysis
        - Field count statistics
        - Data type distribution
        - Structural complexity score (1-10)

        ## 3. PATTERN RECOGNITION
        - Recurring structural patterns
        - Naming conventions analysis
        - Value format patterns (dates, IDs, emails, etc.)
        - Relationship indicators (foreign keys, references)

        ## 4. SCHEMA EVOLUTION INDICATORS
        - Fields that suggest versioning
        - Deprecated or legacy patterns
        - Extension points for future growth

        ## 5. FORMAL SCHEMA GENERATION
        - JSON Schema specification
        - GraphQL schema equivalent
        - SQL DDL equivalent structure

        Be extremely thorough and provide detailed justifications for all observations.
        Include specific examples from the data to support your analysis.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1  # Low temperature for analytical precision
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Schema Analysis Error: {str(e)}"
    
    def _business_domain_analysis(self, data: Any, model: str = "gpt-4o") -> str:
        """Business domain and context analysis"""
        
        # Prepare sample for business analysis
        sample_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a business analyst and domain expert, analyze this JSON data to understand the business context and domain.

        Data:
        {sample_str}

        Provide comprehensive business analysis:

        ## 1. BUSINESS DOMAIN IDENTIFICATION
        - Primary business domain (e-commerce, healthcare, finance, etc.)
        - Sub-domains and business areas represented
        - Industry-specific terminology detected
        - Regulatory compliance indicators

        ## 2. BUSINESS ENTITY ANALYSIS
        - Core business entities identified
        - Business entity hierarchies and relationships
        - Entity lifecycle stages represented
        - Key business identifiers and references

        ## 3. BUSINESS PROCESS MAPPING
        - Business processes represented in the data
        - Workflow stages and state transitions
        - Decision points and business rules
        - Integration touchpoints with external systems

        ## 4. STAKEHOLDER ANALYSIS
        - User roles and personas represented
        - Permission and access control patterns
        - Audit trail and accountability structures

        ## 5. BUSINESS VALUE ASSESSMENT
        - Key performance indicators present
        - Revenue/cost tracking capabilities
        - Analytics and reporting potential
        - Business intelligence opportunities

        ## 6. DOMAIN-SPECIFIC RECOMMENDATIONS
        - Industry best practices alignment
        - Missing critical business data
        - Opportunities for business process optimization
        - Strategic data governance recommendations

        Provide specific examples and justifications for all findings.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Business Analysis Error: {str(e)}"
    
    def _entity_relationship_analysis(self, data: Any, model: str = "o3-mini") -> str:
        """Deep entity relationship analysis with o3-mini reasoning"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a database architect and data modeling expert, use advanced reasoning to analyze entity relationships in this JSON data.

        Complete Data:
        {full_data_str}

        Perform comprehensive entity relationship analysis:

        ## 1. ENTITY IDENTIFICATION AND CLASSIFICATION
        - Primary entities (main business objects)
        - Secondary entities (supporting/reference data)
        - Bridge entities (many-to-many relationships)
        - Value objects vs entities classification

        ## 2. RELATIONSHIP MAPPING
        - One-to-one relationships identified
        - One-to-many relationships with cardinality
        - Many-to-many relationships and junction patterns
        - Self-referential relationships
        - Hierarchical relationships (parent-child, tree structures)

        ## 3. KEY ANALYSIS
        - Primary key patterns and strategies
        - Foreign key relationships (explicit and implicit)
        - Composite key structures
        - Natural vs surrogate key usage

        ## 4. NORMALIZATION ANALYSIS
        - Current normalization level (1NF, 2NF, 3NF, BCNF)
        - Denormalization patterns detected
        - Functional dependencies identified
        - Redundancy and consistency issues

        ## 5. GRAPH STRUCTURE ANALYSIS
        - Network topology of entity relationships
        - Centrality analysis (which entities are most connected)
        - Clustering and modularity patterns
        - Data flow direction analysis

        ## 6. CONCEPTUAL DATA MODEL
        - Entity-Relationship Diagram description
        - Attribute classification (required, optional, calculated)
        - Domain constraints and business rules
        - Data integrity constraints

        ## 7. IMPLEMENTATION PATTERNS
        - Aggregation vs composition patterns
        - Inheritance hierarchies detected
        - Polymorphic data patterns
        - Event sourcing or CQRS patterns

        Use step-by-step reasoning to justify all relationship identifications.
        Provide concrete examples from the data for each relationship type found.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Entity Analysis Error: {str(e)}"
    
    def _data_quality_analysis(self, data: Any, model: str = "gpt-4o-mini") -> str:
        """Comprehensive data quality assessment"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a data quality specialist, perform comprehensive data quality analysis on this JSON data.

        Data:
        {full_data_str}

        Analyze data quality across all dimensions:

        ## 1. COMPLETENESS ANALYSIS
        - Missing value patterns and percentages
        - Required vs optional field analysis
        - Data coverage assessment
        - Completeness scoring by entity/section

        ## 2. ACCURACY AND VALIDITY
        - Data format validation (dates, emails, URLs, etc.)
        - Value range validation
        - Business rule validation
        - Cross-field validation issues

        ## 3. CONSISTENCY ANALYSIS
        - Naming convention consistency
        - Data type consistency
        - Format consistency across similar fields
        - Referential integrity issues

        ## 4. UNIQUENESS AND DUPLICATION
        - Primary key uniqueness validation
        - Potential duplicate detection
        - Near-duplicate identification patterns

        ## 5. TIMELINESS AND CURRENCY
        - Timestamp patterns and currency
        - Data freshness indicators
        - Update frequency patterns

        ## 6. DATA QUALITY METRICS
        - Overall quality score (1-100)
        - Quality scores by data dimension
        - Critical quality issues prioritized
        - Quality improvement recommendations

        ## 7. DATA PROFILING INSIGHTS
        - Statistical summaries where applicable
        - Value distribution analysis
        - Outlier detection
        - Pattern anomalies

        Provide specific examples of quality issues found and actionable recommendations.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Data Quality Analysis Error: {str(e)}"
    
    def _security_privacy_analysis(self, data: Any, model: str = "o3-mini") -> str:
        """Security and privacy analysis"""
        
        # For security analysis, we'll analyze patterns without exposing sensitive data
        structure_only = self._get_structure_analysis(data)
        
        prompt = f"""
        As a cybersecurity and privacy expert, analyze this JSON structure for security and privacy implications.

        Structure Analysis:
        {structure_only}

        Perform comprehensive security and privacy assessment:

        ## 1. SENSITIVE DATA IDENTIFICATION
        - PII (Personally Identifiable Information) patterns
        - Financial data indicators
        - Healthcare data (PHI) patterns
        - Authentication/credential patterns

        ## 2. PRIVACY COMPLIANCE ANALYSIS
        - GDPR compliance considerations
        - CCPA compliance requirements
        - HIPAA considerations (if applicable)
        - Data minimization principles

        ## 3. SECURITY VULNERABILITY ASSESSMENT
        - Injection attack vectors in data structure
        - Data exposure risks
        - Access control implications
        - Audit trail adequacy

        ## 4. DATA PROTECTION RECOMMENDATIONS
        - Encryption requirements by field
        - Masking/anonymization strategies
        - Access control matrix suggestions
        - Data retention policy implications

        ## 5. REGULATORY COMPLIANCE
        - Industry-specific compliance requirements
        - Cross-border data transfer considerations
        - Right to be forgotten implications
        - Consent management requirements

        Provide specific, actionable security and privacy recommendations.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Security Analysis Error: {str(e)}"
    
    def _performance_analysis(self, data: Any, model: str = "gpt-4o") -> str:
        """Performance and optimization analysis"""
        
        size_analysis = self._get_size_analysis(data)
        
        prompt = f"""
        As a performance engineer and database optimization expert, analyze this JSON structure for performance implications.

        Size and Structure Analysis:
        {size_analysis}

        Provide comprehensive performance analysis:

        ## 1. SIZE AND COMPLEXITY METRICS
        - Total data size and memory footprint
        - Nesting complexity impact
        - Array size distributions
        - Processing complexity estimation

        ## 2. QUERY PERFORMANCE IMPLICATIONS
        - Index strategy recommendations
        - Query optimization opportunities
        - Join performance considerations
        - Search and filter efficiency

        ## 3. SERIALIZATION PERFORMANCE
        - JSON parsing/serialization overhead
        - Alternative format recommendations (protobuf, avro, etc.)
        - Compression opportunities
        - Network transfer optimization

        ## 4. STORAGE OPTIMIZATION
        - Data normalization vs denormalization tradeoffs
        - Partitioning strategies
        - Archival and tiering recommendations
        - Storage cost optimization

        ## 5. CACHING STRATEGIES
        - Cacheable data identification
        - Cache invalidation patterns
        - CDN optimization opportunities
        - Application-level caching recommendations

        ## 6. SCALABILITY ANALYSIS
        - Horizontal scaling considerations
        - Sharding strategies
        - Load balancing implications
        - Bottleneck identification

        Provide specific, measurable optimization recommendations with expected impact.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Performance Analysis Error: {str(e)}"
    
    def _generate_rdf_graph(self, data: Any, schema_analysis: str, entity_analysis: str):
        """Generate comprehensive RDF graph representation"""
        
        # Create root node for the schema
        schema_uri = self.CUSTOM.JsonSchema
        self.rdf_graph.add((schema_uri, RDF.type, self.JSON_SCHEMA.Schema))
        self.rdf_graph.add((schema_uri, RDFS.label, Literal("JSON Schema Analysis")))
        self.rdf_graph.add((schema_uri, self.CUSTOM.analysisDate, Literal(datetime.now(), datatype=XSD.dateTime)))
        
        # Add schema properties
        self._add_schema_properties_to_rdf(data, schema_uri)
        
        # Add entities identified from AI analysis
        self._add_entities_to_rdf(entity_analysis, schema_uri)
        
        # Add data quality metrics
        self._add_quality_metrics_to_rdf(schema_uri)
    
    def _add_schema_properties_to_rdf(self, data: Any, schema_uri: URIRef, path: str = ""):
        """Recursively add schema properties to RDF graph"""
        
        if isinstance(data, dict):
            for key, value in data.items():
                prop_uri = self.CUSTOM[f"property_{path}_{key}".replace(".", "_")]
                self.rdf_graph.add((schema_uri, self.CUSTOM.hasProperty, prop_uri))
                self.rdf_graph.add((prop_uri, RDFS.label, Literal(key)))
                self.rdf_graph.add((prop_uri, self.CUSTOM.propertyPath, Literal(f"{path}.{key}" if path else key)))
                
                # Add type information
                if isinstance(value, str):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.XSD.string))
                elif isinstance(value, int):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.XSD.integer))
                elif isinstance(value, float):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.XSD.decimal))
                elif isinstance(value, bool):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.XSD.boolean))
                elif isinstance(value, list):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.CUSTOM.Array))
                    if value:
                        self.rdf_graph.add((prop_uri, self.CUSTOM.arraySize, Literal(len(value))))
                elif isinstance(value, dict):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.CUSTOM.Object))
                    self._add_schema_properties_to_rdf(value, prop_uri, f"{path}.{key}" if path else key)
    
    def _add_entities_to_rdf(self, entity_analysis: str, schema_uri: URIRef):
        """Add entity information to RDF graph based on AI analysis"""
        
        # This would parse the entity analysis and create RDF triples
        # For now, we'll add a placeholder
        entity_uri = self.CUSTOM.EntityAnalysis
        self.rdf_graph.add((schema_uri, self.CUSTOM.hasEntityAnalysis, entity_uri))
        self.rdf_graph.add((entity_uri, RDFS.label, Literal("Entity Relationship Analysis")))
        self.rdf_graph.add((entity_uri, self.CUSTOM.analysisResult, Literal(entity_analysis)))
    
    def _add_quality_metrics_to_rdf(self, schema_uri: URIRef):
        """Add data quality metrics to RDF graph"""
        
        quality_uri = self.CUSTOM.QualityMetrics
        self.rdf_graph.add((schema_uri, self.CUSTOM.hasQualityMetrics, quality_uri))
        self.rdf_graph.add((quality_uri, RDFS.label, Literal("Data Quality Metrics")))
    
    def _get_structure_analysis(self, data: Any) -> str:
        """Get structure analysis without exposing sensitive data"""
        
        def analyze_structure(obj, path=""):
            if isinstance(obj, dict):
                return {
                    "type": "object",
                    "properties": {k: analyze_structure(v, f"{path}.{k}") for k, v in obj.items()},
                    "property_count": len(obj)
                }
            elif isinstance(obj, list):
                return {
                    "type": "array",
                    "length": len(obj),
                    "item_type": analyze_structure(obj[0], f"{path}[0]") if obj else None
                }
            else:
                return {"type": type(obj).__name__}
        
        return json.dumps(analyze_structure(data), indent=2)
    
    def _get_size_analysis(self, data: Any) -> str:
        """Get size and complexity analysis"""
        
        def count_elements(obj):
            if isinstance(obj, dict):
                return 1 + sum(count_elements(v) for v in obj.values())
            elif isinstance(obj, list):
                return 1 + sum(count_elements(item) for item in obj)
            else:
                return 1
        
        total_size = len(json.dumps(data))
        element_count = count_elements(data)
        max_depth = self._get_max_depth(data)
        
        return f"""
        Total JSON size: {total_size:,} characters
        Total elements: {element_count:,}
        Maximum nesting depth: {max_depth}
        Average element size: {total_size/element_count:.1f} characters
        """
    
    def _get_max_depth(self, obj, current_depth=0):
        """Calculate maximum nesting depth"""
        
        if isinstance(obj, dict):
            return max([self._get_max_depth(v, current_depth + 1) for v in obj.values()], default=current_depth)
        elif isinstance(obj, list):
            return max([self._get_max_depth(item, current_depth + 1) for item in obj], default=current_depth)
        else:
            return current_depth
    
    def _compile_comprehensive_report(self, json_file_path: str, analysis_id: str, 
                                    schema_analysis: str, business_analysis: str,
                                    entity_analysis: str, quality_analysis: str,
                                    security_analysis: str, performance_analysis: str) -> str:
        """Compile comprehensive analysis report"""
        
        report = f"""# üöÄ COMPREHENSIVE AI JSON SCHEMA ANALYSIS REPORT

**Analysis ID:** {analysis_id}  
**Source File:** {json_file_path}  
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**AI Models Used:** o3-mini, gpt-4o, gpt-4o-mini  

---

## üìä EXECUTIVE SUMMARY

This comprehensive analysis leverages multiple OpenAI models including the latest o3-mini reasoning model to provide deep insights into the JSON schema structure, business context, and optimization opportunities.

---

## üß† DEEP SCHEMA ANALYSIS (o3-mini)

{schema_analysis}

---

## üè¢ BUSINESS DOMAIN ANALYSIS (gpt-4o)

{business_analysis}

---

## üîç ENTITY RELATIONSHIP ANALYSIS (o3-mini)

{entity_analysis}

---

## üìä DATA QUALITY ASSESSMENT (gpt-4o-mini)

{quality_analysis}

---

## üîí SECURITY & PRIVACY ANALYSIS (o3-mini)

{security_analysis}

---

## ‚ö° PERFORMANCE & OPTIMIZATION ANALYSIS (gpt-4o)

{performance_analysis}

---

## üåê RDF GRAPH REPRESENTATION

An RDF graph has been generated representing the schema structure and relationships. This graph can be used for:
- Semantic data integration
- Automated reasoning about data relationships
- Compliance and governance tracking
- Data lineage and provenance

---

## üìà ANALYSIS METHODOLOGY

This analysis utilized a multi-model AI approach:

1. **o3-mini**: Used for deep reasoning about schema structure and entity relationships
2. **gpt-4o**: Applied for comprehensive business domain analysis and performance optimization
3. **gpt-4o-mini**: Employed for efficient data quality assessment

Each model was selected based on its specific strengths and optimization for the analysis domain.

---

## üéØ KEY RECOMMENDATIONS SUMMARY

[This section would be populated with a synthesis of key recommendations from all analysis phases]

---

## üìã APPENDICES

### A. Technical Specifications
- JSON Schema validation rules
- Database DDL equivalents
- API documentation templates

### B. Implementation Guides
- Migration strategies
- Performance optimization steps
- Security implementation checklist

### C. Monitoring and Maintenance
- Data quality monitoring setup
- Performance benchmarking
- Continuous improvement processes

---

*Report generated using OpenAI's latest AI models including o3-mini for advanced reasoning capabilities.*
"""
        return report
    
    def _save_analysis_outputs(self, output_dir: str, analysis_id: str, report_content: str, original_data: Any):
        """Save all analysis outputs to files"""
        
        # Save comprehensive report
        report_file = f"{output_dir}/comprehensive_analysis_{analysis_id}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"üìÑ Comprehensive report saved: {report_file}")
        
        # Save RDF graph in multiple formats
        rdf_ttl_file = f"{output_dir}/schema_graph_{analysis_id}.ttl"
        rdf_xml_file = f"{output_dir}/schema_graph_{analysis_id}.rdf"
        rdf_json_file = f"{output_dir}/schema_graph_{analysis_id}.jsonld"
        
        self.rdf_graph.serialize(destination=rdf_ttl_file, format='turtle')
        self.rdf_graph.serialize(destination=rdf_xml_file, format='xml')
        self.rdf_graph.serialize(destination=rdf_json_file, format='json-ld')
        
        print(f"üåê RDF graph saved in multiple formats:")
        print(f"   - Turtle: {rdf_ttl_file}")
        print(f"   - RDF/XML: {rdf_xml_file}")
        print(f"   - JSON-LD: {rdf_json_file}")
        
        # Save original data with metadata
        metadata_file = f"{output_dir}/analysis_metadata_{analysis_id}.json"
        metadata = {
            "analysis_id": analysis_id,
            "timestamp": datetime.now().isoformat(),
            "models_used": ["o3-mini", "gpt-4o", "gpt-4o-mini"],
            "data_size": len(json.dumps(original_data)),
            "max_depth": self._get_max_depth(original_data),
            "analysis_files": {
                "report": report_file,
                "rdf_turtle": rdf_ttl_file,
                "rdf_xml": rdf_xml_file,
                "rdf_jsonld": rdf_json_file
            }
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"üìã Analysis metadata saved: {metadata_file}")

# Usage functions
def analyze_json_comprehensive(json_file_path: str, api_key: Optional[str] = None, output_dir: str = "analysis_output"):
    """Main function to run comprehensive analysis"""
    
    analyzer = ComprehensiveAIJsonAnalyzer(api_key)
    analyzer.generate_comprehensive_analysis(json_file_path, output_dir)

def quick_analysis_with_rdf(json_file_path: str, api_key: Optional[str] = None):
    """Quick analysis with RDF generation"""
    
    analyzer = ComprehensiveAIJsonAnalyzer(api_key)
    
    with open(json_file_path, 'r', encoding='utf-8', errors='ignore') as f:
        data = json.load(f)
    
    # Quick schema analysis with o3-mini
    schema_analysis = analyzer._deep_schema_analysis(data)
    
    # Generate RDF
    analyzer._generate_rdf_graph(data, schema_analysis, "")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analyzer.rdf_graph.serialize(destination=f"quick_analysis_{timestamp}.ttl", format='turtle')
    
    with open(f"quick_analysis_{timestamp}.md", 'w', encoding='utf-8') as f:
        f.write(f"# Quick Analysis Report\n\n{schema_analysis}")
    
    print(f"‚úÖ Quick analysis complete! Files saved with timestamp {timestamp}")

if __name__ == "__main__":
    # Example usage
    
    # For comprehensive analysis (recommended):
    # analyze_json_comprehensive('your_file.json')
    
    # For quick analysis with RDF:
    # quick_analysis_with_rdf('your_file.json')
    
    # With custom API key:
    # analyze_json_comprehensive('your_file.json', api_key='your-api-key')
    
    print("ü§ñ Comprehensive AI JSON Analyzer ready!")
    print("üìã Required dependencies: pip install openai rdflib")
    print("üîë Set OPENAI_API_KEY environment variable or pass api_key parameter")
    print("üìÅ Run: analyze_json_comprehensive('your_file.json')")
