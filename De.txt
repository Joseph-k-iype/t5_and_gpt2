# backend/app/core/config.py
from pydantic import BaseSettings, Field
from typing import List
import os
from functools import lru_cache

class Settings(BaseSettings):
    """Application settings with environment variable support"""
    
    # API Configuration
    API_V1_STR: str = "/api/v1"
    PROJECT_NAME: str = "Deep Research Chatbot"
    
    # CORS settings
    ALLOWED_ORIGINS: List[str] = Field(
        default=["http://localhost:3000", "http://127.0.0.1:3000"],
        description="Allowed CORS origins"
    )
    
    # Session settings
    SESSION_TIMEOUT_MINUTES: int = Field(default=60, description="Session timeout in minutes")
    MAX_CONCURRENT_SESSIONS: int = Field(default=100, description="Maximum concurrent sessions")
    
    # Research engine settings
    MAX_RESEARCH_TIME_MINUTES: int = Field(default=10, description="Maximum research time")
    ENABLE_KNOWLEDGE_GRAPH: bool = Field(default=True, description="Enable knowledge graph generation")
    
    # Logging
    LOG_LEVEL: str = Field(default="INFO", description="Logging level")
    
    # WebSocket settings
    WEBSOCKET_HEARTBEAT_INTERVAL: int = Field(default=30, description="WebSocket heartbeat interval")
    
    # Knowledge graph settings
    MAX_GRAPH_NODES: int = Field(default=50, description="Maximum nodes in knowledge graph")
    MAX_GRAPH_EDGES: int = Field(default=100, description="Maximum edges in knowledge graph")
    
    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache()
def get_settings() -> Settings:
    """Get cached settings instance"""
    return Settings()

# backend/app/core/session_manager.py
import uuid
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Optional, Any
import logging
from dataclasses import dataclass, field
from .config import get_settings

logger = logging.getLogger(__name__)

@dataclass
class SessionData:
    """Session data container"""
    session_id: str
    user_id: str
    created_at: datetime
    last_activity: datetime
    namespace: str
    conversation_history: list = field(default_factory=list)
    research_cache: dict = field(default_factory=dict)
    metadata: dict = field(default_factory=dict)
    
    def update_activity(self):
        """Update last activity timestamp"""
        self.last_activity = datetime.utcnow()
    
    def is_expired(self, timeout_minutes: int) -> bool:
        """Check if session has expired"""
        return datetime.utcnow() - self.last_activity > timedelta(minutes=timeout_minutes)
    
    def to_dict(self) -> dict:
        """Convert to dictionary for serialization"""
        return {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "created_at": self.created_at.isoformat(),
            "last_activity": self.last_activity.isoformat(),
            "namespace": self.namespace,
            "conversation_count": len(self.conversation_history),
            "research_cache_size": len(self.research_cache),
            "metadata": self.metadata
        }

class SessionManager:
    """Manages user sessions with automatic cleanup and dynamic namespaces"""
    
    def __init__(self):
        self.sessions: Dict[str, SessionData] = {}
        self.settings = get_settings()
        self._cleanup_task = None
        self._start_cleanup_task()
    
    def _start_cleanup_task(self):
        """Start background task for session cleanup"""
        if not self._cleanup_task:
            self._cleanup_task = asyncio.create_task(self._cleanup_expired_sessions())
    
    async def _cleanup_expired_sessions(self):
        """Background task to clean up expired sessions"""
        while True:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes
                await self._remove_expired_sessions()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in session cleanup: {e}")
    
    async def _remove_expired_sessions(self):
        """Remove expired sessions"""
        expired_sessions = []
        timeout_minutes = self.settings.SESSION_TIMEOUT_MINUTES
        
        for session_id, session in self.sessions.items():
            if session.is_expired(timeout_minutes):
                expired_sessions.append(session_id)
        
        for session_id in expired_sessions:
            logger.info(f"Removing expired session: {session_id}")
            del self.sessions[session_id]
    
    def _generate_namespace(self, user_id: str, session_id: str) -> str:
        """Generate dynamic namespace based on user and session"""
        # Create a unique namespace that's deterministic but private
        timestamp = datetime.utcnow().strftime("%Y%m%d")
        short_session = session_id[:8]
        return f"research_{user_id}_{timestamp}_{short_session}"
    
    async def create_session(self, user_id: Optional[str] = None) -> SessionData:
        """Create a new session with dynamic namespace"""
        # Check session limits
        if len(self.sessions) >= self.settings.MAX_CONCURRENT_SESSIONS:
            # Remove oldest session
            oldest_session_id = min(
                self.sessions.keys(),
                key=lambda x: self.sessions[x].last_activity
            )
            logger.warning(f"Session limit reached, removing oldest: {oldest_session_id}")
            del self.sessions[oldest_session_id]
        
        # Generate session ID and user ID
        session_id = str(uuid.uuid4())
        user_id = user_id or f"user_{uuid.uuid4().hex[:8]}"
        
        # Create session with dynamic namespace
        now = datetime.utcnow()
        session = SessionData(
            session_id=session_id,
            user_id=user_id,
            created_at=now,
            last_activity=now,
            namespace=self._generate_namespace(user_id, session_id)
        )
        
        self.sessions[session_id] = session
        logger.info(f"Created session {session_id} for user {user_id} with namespace {session.namespace}")
        
        return session
    
    async def get_session(self, session_id: str) -> Optional[SessionData]:
        """Get session by ID"""
        session = self.sessions.get(session_id)
        if session:
            session.update_activity()
            return session
        return None
    
    async def update_session_data(self, session_id: str, data: dict) -> bool:
        """Update session data"""
        session = await self.get_session(session_id)
        if session:
            session.metadata.update(data)
            session.update_activity()
            return True
        return False
    
    async def add_conversation_message(self, session_id: str, message: dict) -> bool:
        """Add message to conversation history"""
        session = await self.get_session(session_id)
        if session:
            session.conversation_history.append({
                **message,
                "timestamp": datetime.utcnow().isoformat()
            })
            session.update_activity()
            
            # Limit conversation history size
            if len(session.conversation_history) > 100:
                session.conversation_history = session.conversation_history[-50:]
            
            return True
        return False
    
    async def cache_research_result(self, session_id: str, query: str, result: dict) -> bool:
        """Cache research result for the session"""
        session = await self.get_session(session_id)
        if session:
            # Create a cache key from the query
            cache_key = str(hash(query.lower().strip()))
            session.research_cache[cache_key] = {
                "query": query,
                "result": result,
                "timestamp": datetime.utcnow().isoformat()
            }
            session.update_activity()
            
            # Limit cache size
            if len(session.research_cache) > 10:
                # Remove oldest cached result
                oldest_key = min(
                    session.research_cache.keys(),
                    key=lambda x: session.research_cache[x]["timestamp"]
                )
                del session.research_cache[oldest_key]
            
            return True
        return False
    
    async def get_cached_research(self, session_id: str, query: str) -> Optional[dict]:
        """Get cached research result"""
        session = await self.get_session(session_id)
        if session:
            cache_key = str(hash(query.lower().strip()))
            cached = session.research_cache.get(cache_key)
            if cached:
                # Check if cache is not too old (1 hour)
                cached_time = datetime.fromisoformat(cached["timestamp"])
                if datetime.utcnow() - cached_time < timedelta(hours=1):
                    session.update_activity()
                    return cached["result"]
        return None
    
    async def delete_session(self, session_id: str) -> bool:
        """Delete a session"""
        if session_id in self.sessions:
            logger.info(f"Deleting session: {session_id}")
            del self.sessions[session_id]
            return True
        return False
    
    async def get_session_stats(self) -> dict:
        """Get session statistics"""
        total_sessions = len(self.sessions)
        active_sessions = sum(
            1 for session in self.sessions.values()
            if not session.is_expired(self.settings.SESSION_TIMEOUT_MINUTES)
        )
        
        return {
            "total_sessions": total_sessions,
            "active_sessions": active_sessions,
            "expired_sessions": total_sessions - active_sessions,
            "max_sessions": self.settings.MAX_CONCURRENT_SESSIONS
        }
    
    async def cleanup(self):
        """Cleanup resources"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        self.sessions.clear()
        logger.info("Session manager cleanup complete")
