import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union, TypedDict
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import rdflib
from rdflib import Graph, Literal, URIRef
from rdflib.namespace import RDF, RDFS
from langchain_community.graphs import RdfGraph
import chromadb
from chromadb.config import Settings
import os

# Disable ChromaDB telemetry
os.environ["ANONYMIZED_TELEMETRY"] = "False"
from langchain.chains import GraphSparqlQAChain
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Environment settings from original code
ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

class Triple:
    def __init__(self, subject, predicate, object_):
        self.subject = subject
        self.predicate = predicate
        self.object = object_
    
    def __str__(self):
        return f"{self.subject} {self.predicate} {self.object}"

class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        # Initialize credential for reuse
        self.credential = self._get_credential()
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"), 
                client_id=self.get("AZURE_CLIENT_ID"), 
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                raise FileNotFoundError(f"The file '{dotenvfile}' does not exist or is not readable")
                
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None

def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

class TTLParser:
    """
    Parser for TTL (Turtle) files that extracts triples and creates RDF graphs.
    """
    
    def __init__(self, ttl_file: str):
        """
        Initialize the parser with a TTL file path.
        
        Args:
            ttl_file: Path to the TTL file to parse
        """
        self.ttl_file = ttl_file
        self.rdf_graph = None
        self.nx_graph = None
        self.namespaces = {}
        self.triples = []
        
    def parse(self):
        """
        Parse the TTL file and create both RDF and NetworkX graphs.
        """
        logger.info(f"Parsing TTL file: {self.ttl_file}")
        
        # Check if the file exists
        if not os.path.exists(self.ttl_file):
            raise FileNotFoundError(f"TTL file not found: {self.ttl_file}")
        
        # Create a new RDF graph
        self.rdf_graph = rdflib.Graph()
        
        # Parse the TTL file
        try:
            self.rdf_graph.parse(self.ttl_file, format="turtle")
            logger.info(f"Successfully parsed TTL file with {len(self.rdf_graph)} triples")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        # Extract namespaces
        self.namespaces = dict(self.rdf_graph.namespaces())
        logger.info(f"Found {len(self.namespaces)} namespaces")
        
        # Create a NetworkX directed graph
        self.nx_graph = nx.DiGraph()
        
        # Add all triples to the NetworkX graph and store them
        for s, p, o in self.rdf_graph:
            subject_str = str(s)
            predicate_str = str(p)
            object_str = str(o)
            
            # Store the triple
            self.triples.append(Triple(subject_str, predicate_str, object_str))
            
            # Add nodes to the graph
            if subject_str not in self.nx_graph:
                self.nx_graph.add_node(subject_str)
            
            # Add literals as node attributes, URIs as edges
            if isinstance(o, Literal):
                # Get the Python value
                value = o.value if hasattr(o, 'value') else str(o)
                
                # Add as node attribute
                if predicate_str not in self.nx_graph.nodes[subject_str]:
                    self.nx_graph.nodes[subject_str][predicate_str] = [value]
                else:
                    self.nx_graph.nodes[subject_str][predicate_str].append(value)
            else:
                # Add the object as a node if it doesn't exist
                if object_str not in self.nx_graph:
                    self.nx_graph.add_node(object_str)
                
                # Add an edge from subject to object
                self.nx_graph.add_edge(subject_str, object_str, relation=predicate_str)
        
        logger.info(f"Created NetworkX graph with {len(self.nx_graph.nodes)} nodes and {len(self.nx_graph.edges)} edges")
        
        return self.rdf_graph, self.nx_graph
    
    def get_ontology_schema(self):
        """Extract and return the schema of the ontology in a structured format"""
        schema = {
            "classes": [],
            "properties": [],
            "class_hierarchy": {},
            "property_domains_ranges": {}
        }
        
        # Get classes
        for cls in self.rdf_graph.subjects(RDF.type, RDFS.Class):
            class_info = {
                "uri": str(cls),
                "label": str(self.rdf_graph.value(cls, RDFS.label)) if self.rdf_graph.value(cls, RDFS.label) else self._get_name_from_uri(str(cls)),
                "comment": str(self.rdf_graph.value(cls, RDFS.comment)) if self.rdf_graph.value(cls, RDFS.comment) else ""
            }
            schema["classes"].append(class_info)
            
            # Get subclass relationships
            subclasses = list(self.rdf_graph.subjects(RDFS.subClassOf, cls))
            if subclasses:
                schema["class_hierarchy"][str(cls)] = [str(sc) for sc in subclasses]
        
        # Get properties
        for prop in self.rdf_graph.subjects(RDF.type, RDF.Property):
            prop_info = {
                "uri": str(prop),
                "label": str(self.rdf_graph.value(prop, RDFS.label)) if self.rdf_graph.value(prop, RDFS.label) else self._get_name_from_uri(str(prop)),
                "comment": str(self.rdf_graph.value(prop, RDFS.comment)) if self.rdf_graph.value(prop, RDFS.comment) else ""
            }
            schema["properties"].append(prop_info)
            
            # Get domain and range
            domain = self.rdf_graph.value(prop, RDFS.domain)
            range_ = self.rdf_graph.value(prop, RDFS.range)
            
            if domain or range_:
                schema["property_domains_ranges"][str(prop)] = {
                    "domain": str(domain) if domain else None,
                    "range": str(range_) if range_ else None
                }
                
        return schema
    
    def get_ontology_as_text(self):
        """Convert the ontology schema to a human-readable text description"""
        schema = self.get_ontology_schema()
        text = "ONTOLOGY SCHEMA:\n\n"
        
        # Classes
        text += "CLASSES:\n"
        for cls in schema["classes"]:
            text += f"- Class: {cls['label']} ({cls['uri']})\n"
            if cls['comment']:
                text += f"  Description: {cls['comment']}\n"
        
        # Properties
        text += "\nPROPERTIES:\n"
        for prop in schema["properties"]:
            text += f"- Property: {prop['label']} ({prop['uri']})\n"
            if prop['comment']:
                text += f"  Description: {prop['comment']}\n"
            
            # Add domain and range information
            if prop['uri'] in schema["property_domains_ranges"]:
                domain_range = schema["property_domains_ranges"][prop['uri']]
                if domain_range['domain']:
                    domain_name = self._get_name_from_uri(domain_range['domain'])
                    text += f"  Domain: {domain_name}\n"
                if domain_range['range']:
                    range_name = self._get_name_from_uri(domain_range['range'])
                    text += f"  Range: {range_name}\n"
        
        # Class hierarchy
        if schema["class_hierarchy"]:
            text += "\nCLASS HIERARCHY:\n"
            for parent, children in schema["class_hierarchy"].items():
                parent_name = self._get_name_from_uri(parent)
                text += f"- {parent_name} has subclasses:\n"
                for child in children:
                    child_name = self._get_name_from_uri(child)
                    text += f"  - {child_name}\n"
        
        return text
    
    def _get_name_from_uri(self, uri):
        """Extract a readable name from a URI"""
        # Try to use namespace prefixes
        for prefix, namespace in self.namespaces.items():
            if uri.startswith(namespace):
                return f"{prefix}:{uri[len(namespace):]}"
        
        # If URI can't be simplified, extract the last part after the last slash or hash
        if '#' in uri:
            return uri.split('#')[-1]
        elif '/' in uri:
            return uri.split('/')[-1]
        
        return uri
    
    def to_documents(self):
        """Convert RDF triples to LangChain Document objects for vector storage"""
        documents = []
        
        # Convert each triple to a document
        for s, p, o in self.rdf_graph:
            subject_str = str(s)
            predicate_str = str(p)
            object_str = str(o)
            
            # Create a human-readable representation of the triple
            if isinstance(o, Literal):
                content = f"The {self._get_name_from_uri(subject_str)} has {self._get_name_from_uri(predicate_str)} with value {object_str}"
                triple_type = "literal"
            else:
                content = f"The {self._get_name_from_uri(subject_str)} has relationship {self._get_name_from_uri(predicate_str)} with {self._get_name_from_uri(object_str)}"
                triple_type = "uri"
            
            # Create metadata
            metadata = {
                "subject": subject_str,
                "predicate": predicate_str,
                "object": object_str,
                "triple_type": triple_type
            }
            
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)
        
        logger.info(f"Created {len(documents)} documents from RDF triples")
        return documents

class SPARQLOutputParser(BaseModel):
    query: str = Field(description="The generated SPARQL query")
    
    @classmethod
    def from_llm_output(cls, output: str):
        # Extract SPARQL query from LLM output
        query_parts = []
        in_query = False
        
        for line in output.split("\n"):
            if line.strip().lower() == "sparql query:" or line.strip() == "```sparql":
                in_query = True
                continue
            elif line.strip() == "```" and in_query:
                in_query = False
                continue
            
            if in_query:
                query_parts.append(line)
        
        query = "\n".join(query_parts).strip()
        
        # If no query was found, use the whole output
        if not query:
            query = output.strip()
        
        return cls(query=query)

class GraphState(TypedDict):
    query: str
    schema: str
    sparql_query: Optional[str]
    sparql_results: Optional[List[Dict[str, Any]]]
    answer: Optional[str]
    error: Optional[str]
    chat_history: List[Dict[str, str]]

class AzureGraphRAGSystem:
    """
    A GraphRAG system that uses TTL ontologies and Azure OpenAI 
    to create a chat interface with SPARQL query generation.
    """
    
    def __init__(
        self,
        ttl_file_path: str,
        config_file: str = CONFIG_PATH,
        creds_file: str = CREDS_PATH,
        cert_file: str = CERT_PATH,
        persistence_dir: str = "./graph_db",
        embedding_model_name: str = "text-embedding-3-large",
        llm_model_name: str = "gpt-4o-mini",
        temperature: float = 0.7
    ):
        """
        Initialize the GraphRAG system with Azure credentials.
        
        Args:
            ttl_file_path: Path to the TTL ontology file
            config_file: Path to config file for Azure
            creds_file: Path to credentials file for Azure
            cert_file: Path to certificate file for Azure
            persistence_dir: Directory to persist ChromaDB
            embedding_model_name: Name of the embedding model to use
            llm_model_name: Name of the LLM model to use
            temperature: Temperature for the LLM
        """
        self.ttl_file_path = ttl_file_path
        self.config_file = config_file
        self.creds_file = creds_file
        self.cert_file = cert_file
        self.persistence_dir = persistence_dir
        self.embedding_model_name = embedding_model_name
        self.llm_model_name = llm_model_name
        self.temperature = temperature
        
        # Initialize components
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.parser = None
        self.rdf_graph = None
        self.nx_graph = None
        self.langchain_rdf_graph = None
        self.vector_store = None
        self.embeddings = None
        self.llm = None
        self.workflow = None
        self.chat_history = []
        
        # Initialize the system
        self._init_system()
    
    def _init_system(self):
        """Initialize all components of the GraphRAG system"""
        logger.info("Initializing GraphRAG system...")
        
        # Set up Azure embeddings
        self.embeddings = self._setup_azure_embeddings()
        
        # Set up Azure LLM
        self.llm = self._setup_azure_llm()
        
        # Parse TTL file
        self._parse_ttl_file()
        
        # Create vector store
        self._create_vector_store()
        
        # Create LangGraph workflow
        self._create_workflow()
        
        logger.info("GraphRAG system initialized")
    
    def _setup_azure_embeddings(self):
        """Set up Azure OpenAI embeddings"""
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        return AzureOpenAIEmbeddings(
            azure_deployment=self.embedding_model_name,
            api_version=self.env.get("API_VERSION", "2023-05-15"),
            azure_endpoint=self.env.get("AZURE_ENDPOINT", ""),
            azure_ad_token_provider=token_provider
        )
    
    def _setup_azure_llm(self):
        """Set up Azure OpenAI LLM"""
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        return AzureChatOpenAI(
            azure_deployment=self.llm_model_name,
            temperature=self.temperature,
            max_tokens=800,
            api_version=self.env.get("API_VERSION", "2023-05-15"),
            azure_endpoint=self.env.get("AZURE_ENDPOINT", ""),
            azure_ad_token_provider=token_provider
        )
    
    def _parse_ttl_file(self):
        """Parse the TTL file and create graphs"""
        logger.info(f"Parsing TTL file: {self.ttl_file_path}")
        
        # Create TTL parser
        self.parser = TTLParser(self.ttl_file_path)
        
        # Parse the file
        self.rdf_graph, self.nx_graph = self.parser.parse()
        
        # Create RdfGraph for LangChain
        self.langchain_rdf_graph = RdfGraph(
            source_file=self.ttl_file_path,
            serialization="ttl",
            local_copy="local_graph.ttl"
        )
        
        logger.info("TTL file parsed successfully")
    
    def _create_vector_store(self):
        """Create a vector store from the graph documents"""
        logger.info("Creating vector store...")
        
        # Get documents from the parser
        documents = self.parser.to_documents()
        
        # Configure ChromaDB with telemetry disabled
        client_settings = Settings(
            anonymized_telemetry=False,
            persist_directory=self.persistence_dir
        )
        
        # Create vector store
        self.vector_store = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persistence_dir,
            collection_name="ttl_graph",
            client_settings=client_settings
        )
        
        logger.info(f"Created vector store with {len(documents)} documents")
    
    def _create_workflow(self):
        """Create LangGraph workflow for processing queries"""
        logger.info("Creating LangGraph workflow...")
        
        # Step 1: Generate SPARQL from user query and schema
        def generate_sparql(state: GraphState) -> GraphState:
            try:
                query = state["query"]
                schema = state["schema"]
                
                # Create a prompt for SPARQL generation
                sparql_prompt = ChatPromptTemplate.from_messages([
                    SystemMessage(content=f"""You are an expert in RDF and SPARQL who can convert natural language questions into SPARQL queries.
Based on the provided ontology schema, generate a valid SPARQL query to answer the user's question.
Use proper prefixes based on the namespaces in the schema.
Keep the query focused and relevant to the question.
Include PREFIX declarations in your query.
If you cannot create a SPARQL query for the question, explain why.

Ontology Schema:
{schema}"""),
                    HumanMessage(content=f"Generate a SPARQL query to answer this question: {query}")
                ])
                
                # Generate SPARQL query
                response = self.llm.invoke(sparql_prompt)
                
                # Parse the response to extract SPARQL query
                parser = SPARQLOutputParser.from_llm_output(response.content)
                sparql_query = parser.query
                
                return {**state, "sparql_query": sparql_query}
            except Exception as e:
                logger.error(f"Error generating SPARQL: {e}")
                return {**state, "error": f"Failed to generate SPARQL query: {str(e)}"}
        
        # Step 2: Execute SPARQL query
        def execute_sparql(state: GraphState) -> GraphState:
            try:
                sparql_query = state["sparql_query"]
                
                if not sparql_query:
                    return {**state, "error": "No SPARQL query to execute"}
                
                # Execute the query
                results = list(self.langchain_rdf_graph.query(sparql_query))
                
                # Convert results to JSON-serializable format
                formatted_results = []
                if results:
                    if hasattr(results, 'vars'):
                        # SPARQL SELECT results
                        for row in results:
                            row_dict = {}
                            for i, var in enumerate(results.vars):
                                value = row[i] if i < len(row) else None
                                row_dict[str(var)] = str(value) if value is not None else None
                            formatted_results.append(row_dict)
                    else:
                        # Other query types
                        formatted_results = [{"result": str(r)} for r in results]
                
                return {**state, "sparql_results": formatted_results}
            except Exception as e:
                logger.error(f"Error executing SPARQL: {e}")
                return {**state, "error": f"Failed to execute SPARQL query: {str(e)}"}
        
        # Step 3: Generate answer using vector store for additional context
        def generate_answer(state: GraphState) -> GraphState:
            try:
                query = state["query"]
                sparql_results = state.get("sparql_results", [])
                error = state.get("error")
                
                # Get additional context from vector store
                retriever = self.vector_store.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 5}
                )
                
                retrieved_docs = retriever.invoke(query)
                retrieved_context = "\n".join([doc.page_content for doc in retrieved_docs])
                
                # Format SPARQL results as text
                sparql_results_text = ""
                if sparql_results:
                    sparql_results_text = json.dumps(sparql_results, indent=2)
                
                # Create a prompt for answer generation
                answer_prompt = ChatPromptTemplate.from_messages([
                    SystemMessage(content=f"""You are a helpful assistant that answers questions about a knowledge graph.
You should use both the SPARQL query results and additional context to provide a complete answer.
If there was an error with the SPARQL query, explain what might have happened and still try to answer based on the context.
Always provide clear, concise answers based on the data."""),
                    HumanMessage(content=f"""Question: {query}

SPARQL Query Results:
{sparql_results_text}

Additional Context:
{retrieved_context}

Error (if any):
{error or "None"}

Please provide a comprehensive answer to the question.""")
                ])
                
                # Generate answer
                response = self.llm.invoke(answer_prompt)
                
                return {**state, "answer": response.content}
            except Exception as e:
                logger.error(f"Error generating answer: {e}")
                return {**state, "error": f"Failed to generate answer: {str(e)}"}
                
        # Create the workflow
        workflow = StateGraph(GraphState)
        
        # Add nodes
        workflow.add_node("generate_sparql", generate_sparql)
        workflow.add_node("execute_sparql", execute_sparql)
        workflow.add_node("generate_answer", generate_answer)
        
        # Add edges
        workflow.add_edge("generate_sparql", "execute_sparql")
        workflow.add_edge("execute_sparql", "generate_answer")
        workflow.add_edge("generate_answer", END)
        
        # Set entry point
        workflow.set_entry_point("generate_sparql")
        
        # Compile the workflow
        self.workflow = workflow.compile()
        
        logger.info("LangGraph workflow created")
    
    def query(self, user_query: str) -> str:
        """
        Process a user query and return an answer.
        
        Args:
            user_query: The user's question
            
        Returns:
            str: The answer to the question
        """
        # Record the question in chat history
        self.chat_history.append({"role": "user", "content": user_query})
        
        try:
            # Get the ontology schema as text
            schema = self.parser.get_ontology_as_text()
            
            # Initialize the state
            initial_state = {
                "query": user_query,
                "schema": schema,
                "sparql_query": None,
                "sparql_results": None,
                "answer": None,
                "error": None,
                "chat_history": self.chat_history
            }
            
            # Run the workflow
            final_state = self.workflow.invoke(initial_state)
            
            # Get the answer
            answer = final_state.get("answer", "Sorry, I couldn't process your query.")
            
            # Add the answer to chat history
            self.chat_history.append({"role": "assistant", "content": answer})
            
            return answer
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            error_msg = f"Sorry, I encountered an error: {str(e)}"
            
            # Record the error in chat history
            self.chat_history.append({"role": "assistant", "content": error_msg})
            
            return error_msg
    
    def get_debug_info(self, user_query: str) -> Dict[str, Any]:
        """
        Get debug information for a query.
        
        Args:
            user_query: The user's question
            
        Returns:
            Dict: Debug information
        """
        try:
            # Get the ontology schema as text
            schema = self.parser.get_ontology_as_text()
            
            # Initialize the state
            initial_state = {
                "query": user_query,
                "schema": schema,
                "sparql_query": None,
                "sparql_results": None,
                "answer": None,
                "error": None,
                "chat_history": self.chat_history
            }
            
            # Run the workflow
            final_state = self.workflow.invoke(initial_state)
            
            # Return the final state
            return {
                "query": user_query,
                "sparql_query": final_state.get("sparql_query"),
                "sparql_results": final_state.get("sparql_results"),
                "answer": final_state.get("answer"),
                "error": final_state.get("error")
            }
        except Exception as e:
            logger.error(f"Error getting debug info: {e}")
            return {
                "query": user_query,
                "error": f"Failed to get debug info: {str(e)}"
            }
    
    def clear_chat_history(self):
        """Clear the chat history"""
        self.chat_history = []
        return "Chat history cleared."

# Example usage
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Azure GraphRAG Chat Interface")
    parser.add_argument("--ttl-file", required=True, help="Path to the TTL ontology file")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to certificate file")
    parser.add_argument("--db-dir", default="./graph_db", help="Directory to persist the vector store")
    parser.add_argument("--temperature", type=float, default=0.7, help="Temperature for the LLM")
    
    args = parser.parse_args()
    
    # Create the GraphRAG system
    graphrag = AzureGraphRAGSystem(
        ttl_file_path=args.ttl_file,
        config_file=args.config,
        creds_file=args.creds,
        cert_file=args.cert,
        persistence_dir=args.db_dir,
        temperature=args.temperature
    )
    
    # Start a simple command-line chat interface
    print("Azure GraphRAG Chat Interface")
    print("Type 'exit' to quit")
    print("Type 'clear' to clear chat history")
    print("Type 'debug <query>' to see debug information")
    
    while True:
        user_input = input("\nYou: ")
        
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        elif user_input.lower() == 'clear':
            result = graphrag.clear_chat_history()
            print(f"System: {result}")
        elif user_input.lower().startswith('debug '):
            query = user_input[6:]  # Remove 'debug ' prefix
            debug_info = graphrag.get_debug_info(query)
            print("\nDEBUG INFORMATION:")
            print(f"Query: {debug_info['query']}")
            print(f"\nSPARQL Query: {debug_info.get('sparql_query', 'None')}")
            print(f"\nSPARQL Results: {json.dumps(debug_info.get('sparql_results', []), indent=2)}")
            print(f"\nError: {debug_info.get('error', 'None')}")
            print(f"\nAnswer: {debug_info.get('answer', 'None')}")
        else:
            response = graphrag.query(user_input)
            print(f"\nAssistant: {response}")
