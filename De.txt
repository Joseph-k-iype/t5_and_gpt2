# backend/app/api/chat.py
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List
import logging

from app.models.chat import (
    QuickChatRequest, QuickChatResponse, 
    ConversationHistory, ChatMessage
)
from app.core.session_manager import SessionManager, SessionData
from app.core.research_engine import ResearchEngineWrapper

logger = logging.getLogger(__name__)
router = APIRouter()

async def get_session_manager():
    """Dependency to get session manager"""
    # This will be injected by main.py
    from fastapi import Request
    def _get_session_manager(request: Request):
        return request.app.state.session_manager
    return _get_session_manager

async def get_research_engine():
    """Dependency to get research engine"""
    from fastapi import Request
    def _get_research_engine(request: Request):
        return request.app.state.research_engine
    return _get_research_engine

@router.post("/quick", response_model=QuickChatResponse)
async def quick_chat(
    request: QuickChatRequest,
    background_tasks: BackgroundTasks,
    session_manager: SessionManager = Depends(get_session_manager),
    research_engine: ResearchEngineWrapper = Depends(get_research_engine)
):
    """
    Quick chat endpoint for fast AI responses
    """
    try:
        # Get or create session
        if request.session_id:
            session = await session_manager.get_session(request.session_id)
            if not session:
                raise HTTPException(status_code=404, detail="Session not found")
        else:
            session = await session_manager.create_session(request.user_id)
        
        # Add user message to conversation history
        user_message = {
            "role": "user",
            "content": request.message,
            "timestamp": session.last_activity.isoformat()
        }
        await session_manager.add_conversation_message(session.session_id, user_message)
        
        # Process with research engine
        result = await research_engine.quick_chat(
            question=request.message,
            user_id=session.user_id,
            session_id=session.session_id
        )
        
        # Add assistant response to conversation history
        assistant_message = {
            "role": "assistant", 
            "content": result.get("answer", ""),
            "metadata": {
                "confidence": result.get("confidence"),
                "approach": result.get("approach")
            }
        }
        background_tasks.add_task(
            session_manager.add_conversation_message,
            session.session_id,
            assistant_message
        )
        
        # Return structured response
        return QuickChatResponse(
            answer=result.get("answer", "No response generated"),
            confidence=result.get("confidence", "unknown"),
            approach=result.get("approach", "unknown"),
            session_id=session.session_id,
            user_id=session.user_id,
            timestamp=session.last_activity,
            metadata=result.get("metadata", {})
        )
        
    except Exception as e:
        logger.error(f"Error in quick chat: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@router.get("/conversation/{session_id}", response_model=ConversationHistory)
async def get_conversation_history(
    session_id: str,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Get conversation history for a session
    """
    try:
        session = await session_manager.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        messages = [
            ChatMessage(
                role=msg.get("role", "unknown"),
                content=msg.get("content", ""),
                timestamp=msg.get("timestamp"),
                metadata=msg.get("metadata", {})
            )
            for msg in session.conversation_history
        ]
        
        return ConversationHistory(
            messages=messages,
            session_id=session_id,
            total_messages=len(messages)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting conversation history: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.delete("/session/{session_id}")
async def delete_session(
    session_id: str,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Delete a chat session
    """
    try:
        success = await session_manager.delete_session(session_id)
        if not success:
            raise HTTPException(status_code=404, detail="Session not found")
        
        return {"message": "Session deleted successfully", "session_id": session_id}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting session: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

# backend/app/api/research.py
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from typing import Dict, Any
import asyncio
import json
import logging

from app.models.research import (
    DeepResearchRequest, ResearchResult, ResearchStatus,
    ResearchProgressUpdate, ResearchStage
)
from app.core.session_manager import SessionManager
from app.core.research_engine import ResearchEngineWrapper

logger = logging.getLogger(__name__)
router = APIRouter()

# Store active research sessions
active_research: Dict[str, Dict[str, Any]] = {}

async def get_session_manager():
    """Dependency to get session manager"""
    from fastapi import Request
    def _get_session_manager(request: Request):
        return request.app.state.session_manager
    return _get_session_manager

async def get_research_engine():
    """Dependency to get research engine"""
    from fastapi import Request
    def _get_research_engine(request: Request):
        return request.app.state.research_engine
    return _get_research_engine

@router.post("/deep", response_model=ResearchResult)
async def start_deep_research(
    request: DeepResearchRequest,
    background_tasks: BackgroundTasks,
    session_manager: SessionManager = Depends(get_session_manager),
    research_engine: ResearchEngineWrapper = Depends(get_research_engine)
):
    """
    Start deep research process
    """
    try:
        # Get or create session
        if request.session_id:
            session = await session_manager.get_session(request.session_id)
            if not session:
                raise HTTPException(status_code=404, detail="Session not found")
        else:
            session = await session_manager.create_session(request.user_id)
        
        # Check if research is already running for this session
        if session.session_id in active_research:
            raise HTTPException(
                status_code=409, 
                detail="Research already in progress for this session"
            )
        
        # Check cache first
        cached_result = await session_manager.get_cached_research(
            session.session_id, request.topic
        )
        if cached_result:
            logger.info(f"Returning cached research result for: {request.topic}")
            return ResearchResult(**cached_result)
        
        # Mark research as active
        active_research[session.session_id] = {
            "status": "running",
            "stage": ResearchStage.INITIALIZATION,
            "progress": 0
        }
        
        try:
            # Start deep research
            result = await research_engine.deep_research(
                topic=request.topic,
                user_id=session.user_id,
                session_id=session.session_id
            )
            
            # Cache the result
            background_tasks.add_task(
                session_manager.cache_research_result,
                session.session_id,
                request.topic,
                result
            )
            
            # Return structured response
            research_result = ResearchResult(
                final_synthesis=result.get("final_synthesis", "No synthesis available"),
                overall_confidence=result.get("overall_confidence", 0.0),
                agents_used=result.get("agents_used", []),
                iterations_completed=result.get("iterations_completed", 0),
                session_id=session.session_id,
                user_id=session.user_id,
                timestamp=session.last_activity,
                processing_time=result.get("processing_time", "extended"),
                metadata=result.get("metadata", {})
            )
            
            return research_result
            
        finally:
            # Remove from active research
            active_research.pop(session.session_id, None)
        
    except HTTPException:
        # Remove from active research on HTTP errors
        active_research.pop(request.session_id or "unknown", None)
        raise
    except Exception as e:
        # Remove from active research on any error
        active_research.pop(request.session_id or "unknown", None)
        logger.error(f"Error in deep research: {e}")
        raise HTTPException(status_code=500, detail=f"Research failed: {str(e)}")

@router.get("/status/{session_id}", response_model=ResearchStatus)
async def get_research_status(
    session_id: str,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Get research status for a session
    """
    try:
        session = await session_manager.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # Check if research is active
        if session_id in active_research:
            research_info = active_research[session_id]
            return ResearchStatus(
                session_id=session_id,
                is_active=True,
                current_stage=research_info.get("stage"),
                progress_percentage=research_info.get("progress", 0),
                estimated_completion=None,  # Could be calculated based on progress
                error_message=research_info.get("error")
            )
        else:
            return ResearchStatus(
                session_id=session_id,
                is_active=False,
                current_stage=None,
                progress_percentage=100,
                estimated_completion=None,
                error_message=None
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting research status: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/stream/{session_id}")
async def stream_research_progress(
    session_id: str,
    session_manager: SessionManager = Depends(get_session_manager),
    research_engine: ResearchEngineWrapper = Depends(get_research_engine)
):
    """
    Stream research progress updates
    """
    async def generate_progress():
        """Generate progress updates as Server-Sent Events"""
        try:
            session = await session_manager.get_session(session_id)
            if not session:
                yield f"data: {json.dumps({'error': 'Session not found'})}\n\n"
                return
            
            # Mock progress updates for demonstration
            # In real implementation, this would connect to the research engine's progress
            stages = [
                (ResearchStage.INITIALIZATION, "Initializing research system...", 10),
                (ResearchStage.PLANNING, "Creating research plan...", 25),
                (ResearchStage.RESEARCH, "Conducting research...", 70),
                (ResearchStage.SYNTHESIS, "Synthesizing findings...", 90),
                (ResearchStage.COMPLETION, "Research completed!", 100)
            ]
            
            for stage, message, progress in stages:
                if session_id not in active_research:
                    break
                
                update = ResearchProgressUpdate(
                    stage=stage,
                    message=message,
                    progress=progress,
                    details={"session_id": session_id}
                )
                
                # Update active research status
                active_research[session_id].update({
                    "stage": stage,
                    "progress": progress
                })
                
                yield f"data: {update.json()}\n\n"
                await asyncio.sleep(2)  # Simulate processing time
            
        except Exception as e:
            error_update = ResearchProgressUpdate(
                stage=ResearchStage.ERROR,
                message=f"Error: {str(e)}",
                progress=0,
                error=True
            )
            yield f"data: {error_update.json()}\n\n"
    
    return StreamingResponse(
        generate_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )

# backend/app/api/knowledge_graph.py
from fastapi import APIRouter, HTTPException, Depends
from typing import List, Dict, Any
import logging

from app.models.knowledge_graph import (
    KnowledgeGraphRequest, KnowledgeGraphResponse,
    GraphNode, GraphEdge
)
from app.core.session_manager import SessionManager
from app.utils.knowledge_extractor import KnowledgeExtractor

logger = logging.getLogger(__name__)
router = APIRouter()

async def get_session_manager():
    """Dependency to get session manager"""
    from fastapi import Request
    def _get_session_manager(request: Request):
        return request.app.state.session_manager
    return _get_session_manager

@router.post("/generate", response_model=KnowledgeGraphResponse)
async def generate_knowledge_graph(
    request: KnowledgeGraphRequest,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Generate knowledge graph from research results or conversation
    """
    try:
        # Validate session if provided
        if request.session_id:
            session = await session_manager.get_session(request.session_id)
            if not session:
                raise HTTPException(status_code=404, detail="Session not found")
        
        # Initialize knowledge extractor
        extractor = KnowledgeExtractor()
        
        # Extract concepts and relationships
        graph_data = await extractor.extract_knowledge_graph(
            text=request.content,
            context=request.context,
            max_nodes=request.max_nodes,
            max_edges=request.max_edges
        )
        
        # Convert to response model
        nodes = [
            GraphNode(
                id=node["id"],
                label=node["label"],
                type=node.get("type", "concept"),
                properties=node.get("properties", {}),
                size=node.get("size", 20),
                color=node.get("color", "#4A90E2")
            )
            for node in graph_data["nodes"]
        ]
        
        edges = [
            GraphEdge(
                id=edge["id"],
                source=edge["source"],
                target=edge["target"],
                label=edge.get("label", ""),
                type=edge.get("type", "related"),
                properties=edge.get("properties", {}),
                weight=edge.get("weight", 1.0)
            )
            for edge in graph_data["edges"]
        ]
        
        return KnowledgeGraphResponse(
            nodes=nodes,
            edges=edges,
            metadata={
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "generation_method": "nlp_extraction",
                "confidence": graph_data.get("confidence", 0.8)
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating knowledge graph: {e}")
        raise HTTPException(status_code=500, detail=f"Knowledge graph generation failed: {str(e)}")

@router.get("/session/{session_id}", response_model=KnowledgeGraphResponse)
async def get_session_knowledge_graph(
    session_id: str,
    session_manager: SessionManager = Depends(get_session_manager)
):
    """
    Generate knowledge graph from entire session conversation
    """
    try:
        session = await session_manager.get_session(session_id)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # Combine all conversation content
        content_parts = []
        for message in session.conversation_history:
            if message.get("role") == "assistant":
                content_parts.append(message.get("content", ""))
        
        combined_content = " ".join(content_parts)
        
        if not combined_content.strip():
            # Return empty graph for sessions with no content
            return KnowledgeGraphResponse(
                nodes=[],
                edges=[],
                metadata={
                    "total_nodes": 0,
                    "total_edges": 0,
                    "generation_method": "session_conversation",
                    "session_id": session_id
                }
            )
        
        # Generate graph from combined content
        extractor = KnowledgeExtractor()
        graph_data = await extractor.extract_knowledge_graph(
            text=combined_content,
            context={"session_id": session_id, "type": "conversation"},
            max_nodes=30,
            max_edges=50
        )
        
        # Convert to response model
        nodes = [
            GraphNode(
                id=node["id"],
                label=node["label"],
                type=node.get("type", "concept"),
                properties=node.get("properties", {}),
                size=node.get("size", 20),
                color=node.get("color", "#4A90E2")
            )
            for node in graph_data["nodes"]
        ]
        
        edges = [
            GraphEdge(
                id=edge["id"],
                source=edge["source"],
                target=edge["target"],
                label=edge.get("label", ""),
                type=edge.get("type", "related"),
                properties=edge.get("properties", {}),
                weight=edge.get("weight", 1.0)
            )
            for edge in graph_data["edges"]
        ]
        
        return KnowledgeGraphResponse(
            nodes=nodes,
            edges=edges,
            metadata={
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "generation_method": "session_conversation",
                "session_id": session_id,
                "confidence": graph_data.get("confidence", 0.8)
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating session knowledge graph: {e}")
        raise HTTPException(status_code=500, detail="Knowledge graph generation failed")

# backend/app/models/knowledge_graph.py
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

class GraphNode(BaseModel):
    """Knowledge graph node model"""
    id: str = Field(..., description="Unique node identifier")
    label: str = Field(..., description="Node display label")
    type: str = Field(default="concept", description="Node type (concept, entity, etc.)")
    properties: Dict[str, Any] = Field(default_factory=dict, description="Node properties")
    size: int = Field(default=20, ge=5, le=100, description="Node size for visualization")
    color: str = Field(default="#4A90E2", description="Node color hex code")

class GraphEdge(BaseModel):
    """Knowledge graph edge model"""
    id: str = Field(..., description="Unique edge identifier")
    source: str = Field(..., description="Source node ID")
    target: str = Field(..., description="Target node ID")
    label: str = Field(default="", description="Edge label")
    type: str = Field(default="related", description="Edge type")
    properties: Dict[str, Any] = Field(default_factory=dict, description="Edge properties")
    weight: float = Field(default=1.0, ge=0.0, le=1.0, description="Edge weight/strength")

class KnowledgeGraphRequest(BaseModel):
    """Request model for knowledge graph generation"""
    content: str = Field(..., min_length=10, description="Content to analyze")
    context: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Additional context")
    session_id: Optional[str] = Field(None, description="Session ID for context")
    max_nodes: int = Field(default=25, ge=5, le=100, description="Maximum number of nodes")
    max_edges: int = Field(default=50, ge=5, le=200, description="Maximum number of edges")

class KnowledgeGraphResponse(BaseModel):
    """Response model for knowledge graph"""
    nodes: List[GraphNode] = Field(..., description="Graph nodes")
    edges: List[GraphEdge] = Field(..., description="Graph edges")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Graph metadata")

# backend/app/utils/knowledge_extractor.py
import re
import asyncio
from typing import Dict, List, Any, Set, Tuple
import logging
from collections import defaultdict, Counter
import hashlib

logger = logging.getLogger(__name__)

class KnowledgeExtractor:
    """
    Extract knowledge graphs from text using NLP techniques
    """
    
    def __init__(self):
        # Common stop words and noise words to filter out
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you',
            'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'
        }
        
        # Legal/privacy domain specific terms that are important
        self.domain_terms = {
            'gdpr', 'ccpa', 'lgpd', 'privacy', 'data protection', 'consent',
            'personal data', 'processing', 'controller', 'processor', 'breach',
            'notification', 'rights', 'deletion', 'portability', 'rectification',
            'restriction', 'objection', 'automated decision', 'profiling',
            'lawful basis', 'legitimate interest', 'compliance', 'regulation',
            'directive', 'framework', 'jurisdiction', 'cross-border', 'transfer',
            'adequacy', 'safeguards', 'binding corporate rules', 'standard contractual clauses'
        }
    
    async def extract_knowledge_graph(self, text: str, context: Dict[str, Any] = None,
                                    max_nodes: int = 25, max_edges: int = 50) -> Dict[str, Any]:
        """
        Extract knowledge graph from text
        """
        try:
            # Clean and preprocess text
            cleaned_text = self._clean_text(text)
            
            # Extract entities and concepts
            entities = await self._extract_entities(cleaned_text)
            
            # Extract relationships
            relationships = await self._extract_relationships(cleaned_text, entities)
            
            # Build graph structure
            nodes, edges = self._build_graph(entities, relationships, max_nodes, max_edges)
            
            # Calculate confidence based on extraction quality
            confidence = self._calculate_confidence(nodes, edges, len(cleaned_text))
            
            return {
                "nodes": nodes,
                "edges": edges,
                "confidence": confidence,
                "extraction_stats": {
                    "original_entities": len(entities),
                    "filtered_nodes": len(nodes),
                    "potential_relationships": len(relationships),
                    "final_edges": len(edges)
                }
            }
            
        except Exception as e:
            logger.error(f"Error extracting knowledge graph: {e}")
            return {
                "nodes": [],
                "edges": [],
                "confidence": 0.0,
                "error": str(e)
            }
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep important punctuation
        text = re.sub(r'[^\w\s\-\.\,\;\:\(\)\/]', ' ', text)
        
        # Normalize whitespace again
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract entities and concepts from text
        """
        entities = []
        
        # Extract capitalized phrases (likely proper nouns/entities)
        capitalized_patterns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        
        # Extract acronyms
        acronym_patterns = re.findall(r'\b[A-Z]{2,}\b', text)
        
        # Extract domain-specific terms
        domain_matches = []
        text_lower = text.lower()
        for term in self.domain_terms:
            if term in text_lower:
                domain_matches.append(term.title())
        
        # Extract quoted terms (likely important concepts)
        quoted_terms = re.findall(r'"([^"]+)"', text)
        quoted_terms.extend(re.findall(r"'([^']+)'", text))
        
        # Combine and score entities
        all_candidates = (
            [(term, 'proper_noun', 0.8) for term in capitalized_patterns] +
            [(term, 'acronym', 0.9) for term in acronym_patterns] +
            [(term, 'domain_term', 1.0) for term in domain_matches] +
            [(term, 'quoted_term', 0.7) for term in quoted_terms]
        )
        
        # Count frequency and filter
        entity_counts = Counter([candidate[0].lower() for candidate in all_candidates])
        
        for term, entity_type, base_score in all_candidates:
            if len(term) < 2 or term.lower() in self.stop_words:
                continue
            
            frequency = entity_counts[term.lower()]
            # Boost score based on frequency
            score = min(base_score + (frequency - 1) * 0.1, 1.0)
            
            entities.append({
                'text': term,
                'type': entity_type,
                'score': score,
                'frequency': frequency
            })
        
        # Remove duplicates and sort by score
        seen = set()
        unique_entities = []
        for entity in sorted(entities, key=lambda x: x['score'], reverse=True):
            key = entity['text'].lower()
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities[:50]  # Limit to top 50 entities
    
    async def _extract_relationships(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract relationships between entities
        """
        relationships = []
        entity_texts = [e['text'] for e in entities]
        
        # Simple co-occurrence based relationships
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
            
            # Find entities in this sentence
            sentence_entities = []
            for entity in entity_texts:
                if entity.lower() in sentence.lower():
                    sentence_entities.append(entity)
            
            # Create relationships between co-occurring entities
            for i, entity1 in enumerate(sentence_entities):
                for entity2 in sentence_entities[i+1:]:
                    relationship_type = self._determine_relationship_type(sentence, entity1, entity2)
                    strength = self._calculate_relationship_strength(sentence, entity1, entity2)
                    
                    relationships.append({
                        'source': entity1,
                        'target': entity2,
                        'type': relationship_type,
                        'strength': strength,
                        'context': sentence[:100] + '...' if len(sentence) > 100 else sentence
                    })
        
        return relationships
    
    def _determine_relationship_type(self, sentence: str, entity1: str, entity2: str) -> str:
        """
        Determine the type of relationship between entities based on context
        """
        sentence_lower = sentence.lower()
        
        # Look for specific relationship indicators
        if any(word in sentence_lower for word in ['requires', 'mandates', 'must', 'shall']):
            return 'requires'
        elif any(word in sentence_lower for word in ['implements', 'provides', 'ensures']):
            return 'implements'
        elif any(word in sentence_lower for word in ['under', 'according to', 'pursuant to']):
            return 'governed_by'
        elif any(word in sentence_lower for word in ['applies to', 'covers', 'includes']):
            return 'applies_to'
        elif any(word in sentence_lower for word in ['differs from', 'unlike', 'whereas']):
            return 'differs_from'
        elif any(word in sentence_lower for word in ['similar to', 'like', 'comparable']):
            return 'similar_to'
        else:
            return 'related_to'
    
    def _calculate_relationship_strength(self, sentence: str, entity1: str, entity2: str) -> float:
        """
        Calculate the strength of relationship between entities
        """
        # Distance between entities in the sentence
        pos1 = sentence.lower().find(entity1.lower())
        pos2 = sentence.lower().find(entity2.lower())
        
        if pos1 == -1 or pos2 == -1:
            return 0.1
        
        distance = abs(pos1 - pos2)
        
        # Closer entities have stronger relationships
        if distance < 20:
            return 1.0
        elif distance < 50:
            return 0.8
        elif distance < 100:
            return 0.6
        else:
            return 0.3
    
    def _build_graph(self, entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]],
                    max_nodes: int, max_edges: int) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Build the final graph structure
        """
        # Sort entities by score and take top nodes
        top_entities = sorted(entities, key=lambda x: x['score'], reverse=True)[:max_nodes]
        entity_names = {e['text'] for e in top_entities}
        
        # Create nodes
        nodes = []
        for i, entity in enumerate(top_entities):
            node_id = f"node_{i}"
            color = self._get_node_color(entity['type'])
            size = max(15, min(50, int(entity['score'] * 40 + 10)))
            
            nodes.append({
                'id': node_id,
                'label': entity['text'],
                'type': entity['type'],
                'properties': {
                    'score': entity['score'],
                    'frequency': entity['frequency'],
                    'original_text': entity['text']
                },
                'size': size,
                'color': color
            })
        
        # Create mapping from entity text to node ID
        text_to_id = {entity['text']: f"node_{i}" for i, entity in enumerate(top_entities)}
        
        # Filter relationships to only include entities in our node set
        valid_relationships = [
            rel for rel in relationships
            if rel['source'] in entity_names and rel['target'] in entity_names
        ]
        
        # Sort by strength and take top edges
        top_relationships = sorted(valid_relationships, key=lambda x: x['strength'], reverse=True)[:max_edges]
        
        # Create edges
        edges = []
        for i, rel in enumerate(top_relationships):
            edge_id = f"edge_{i}"
            source_id = text_to_id[rel['source']]
            target_id = text_to_id[rel['target']]
            
            edges.append({
                'id': edge_id,
                'source': source_id,
                'target': target_id,
                'label': rel['type'].replace('_', ' ').title(),
                'type': rel['type'],
                'properties': {
                    'strength': rel['strength'],
                    'context': rel['context']
                },
                'weight': rel['strength']
            })
        
        return nodes, edges
    
    def _get_node_color(self, node_type: str) -> str:
        """
        Get color for node based on type
        """
        color_map = {
            'domain_term': '#E74C3C',     # Red for domain-specific terms
            'proper_noun': '#3498DB',     # Blue for proper nouns
            'acronym': '#9B59B6',         # Purple for acronyms
            'quoted_term': '#F39C12',     # Orange for quoted terms
            'concept': '#2ECC71',         # Green for general concepts
        }
        return color_map.get(node_type, '#95A5A6')  # Default gray
    
    def _calculate_confidence(self, nodes: List[Dict], edges: List[Dict], text_length: int) -> float:
        """
        Calculate confidence score for the extracted graph
        """
        if not nodes:
            return 0.0
        
        # Base confidence on number of nodes and edges
        node_score = min(len(nodes) / 20, 1.0)  # Optimal around 20 nodes
        edge_score = min(len(edges) / 30, 1.0)  # Optimal around 30 edges
        
        # Boost confidence for longer, more detailed text
        text_score = min(text_length / 5000, 1.0)  # Optimal around 5000 characters
        
        # Average domain term ratio
        domain_nodes = sum(1 for node in nodes if node.get('type') == 'domain_term')
        domain_ratio = domain_nodes / len(nodes) if nodes else 0
        domain_score = min(domain_ratio * 2, 1.0)  # Boost for domain relevance
        
        # Weighted average
        confidence = (node_score * 0.3 + edge_score * 0.3 + text_score * 0.2 + domain_score * 0.2)
        
        return round(confidence, 2)
