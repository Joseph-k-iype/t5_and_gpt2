"""
ISO 11179 Compliant Data Enrichment and Mapping System
=====================================================
This system enriches field names according to ISO/IEC 11179-1:2023 standards,
generates logical descriptions, maps fields to object-property combinations,
and identifies PII using LangGraph ReAct agents with mixture of experts reasoning.
"""

import json
import os
import pandas as pd
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Sequence
import numpy as np
from openai import OpenAI
import time

# LangChain and LangGraph imports
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_REASONING_MODEL = "o3-mini"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"

openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

INPUT_JSON_PATH = "cib_long_json.json"
EXCEL_PATH = "pbt.xlsx"
OUTPUT_CSV_PATH = "enriched_mapped_output.csv"


# ============================================================================
# CUSTOM EMBEDDINGS CLASS FOR OPENAI API
# ============================================================================

class OpenAIEmbeddings(Embeddings):
    """Custom embeddings class that uses OpenAI API directly."""
    
    def __init__(self, model: str = OPENAI_EMBEDDING_MODEL):
        self.model = model
        self.client = openai_client
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents using OpenAI API."""
        embeddings = []
        total = len(texts)
        for idx, text in enumerate(texts, 1):
            print(f"      ðŸ“Š Embedding document {idx}/{total}")
            embedding = self.embed_query(text)
            embeddings.append(embedding)
            time.sleep(0.1)  # Small delay between requests
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query using OpenAI API."""
        truncated_text = text[:8000]
        print(f"      ðŸ”¹ Generating embedding for: {truncated_text[:80]}...")
        response = self.client.embeddings.create(
            model=self.model,
            input=[truncated_text]
        )
        print(f"      âœ“ Embedding generated (dimension: {len(response.data[0].embedding)})")
        return response.data[0].embedding


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def call_openai_with_retry(messages: List[Dict], max_retries: int = 10) -> str:
    """Call OpenAI API with retry logic until success."""
    print(f"      ðŸ¤– Calling {OPENAI_REASONING_MODEL} model...")
    for attempt in range(max_retries):
        try:
            response = openai_client.chat.completions.create(
                model=OPENAI_REASONING_MODEL,
                messages=messages
            )
            print(f"      âœ“ LLM response received (tokens: ~{len(response.choices[0].message.content.split())} words)")
            return response.choices[0].message.content
        except Exception as e:
            print(f"      âš  Attempt {attempt + 1}/{max_retries} failed: {e}")
            time.sleep(2 ** attempt)  # Exponential backoff
            if attempt == max_retries - 1:
                raise
    raise Exception(f"Failed after {max_retries} attempts")


# ============================================================================
# ISO 11179 STANDARDS SYSTEM PROMPTS
# ============================================================================

ISO_11179_NAMING_PRINCIPLES = """
ISO/IEC 11179-1:2023 and ISO/IEC 11179-5:2015 NAMING PRINCIPLES FOR METADATA REGISTRIES

Core Principles for Data Element Naming:

1. SEMANTIC CLARITY AND PRECISION:
   - Names must be semantically precise and unambiguous
   - Use natural language terms that clearly convey meaning
   - Each word should contribute meaningful information
   - Avoid abbreviations, acronyms, and technical jargon unless universally understood
   - Names should be understandable by both technical and non-technical stakeholders

2. NAMING CONVENTION STRUCTURE:
   - Use proper spacing between words (NO camelCase, NO snake_case, NO kebab-case)
   - Capitalize first letter of each significant word (Title Case)
   - Avoid special characters, underscores, hyphens in the semantic name
   - Example: "Customer Primary Email Address" not "customer_primary_email_address"

3. CONCEPT-BASED NAMING:
   - Names should reflect the conceptual meaning, not physical implementation
   - Focus on WHAT the data represents, not HOW it is stored
   - Example: "Customer Birth Date" not "CUST_DOB_FLD"

4. OBJECT CLASS - PROPERTY - REPRESENTATION STRUCTURE:
   - Object Class: The entity or concept being described (e.g., Customer, Account, Transaction)
   - Property: The characteristic or attribute (e.g., Name, Address, Balance)
   - Representation: The form or data type (usually implicit in naming)
   - Standard structure: [Object Class] [Qualifier(s)] [Property] [Representation]
   - Example: "Customer Mailing Address Text" or "Account Current Balance Amount"

5. MEANINGFUL AND DESCRIPTIVE:
   - Names should be self-documenting
   - A reader should understand what data is stored without additional documentation
   - Balance between brevity and completeness

EXAMPLES OF PROPER ISO 11179 NAMING:

Poor Name â†’ Good ISO 11179 Name
------------------------------
cust_id â†’ Customer Identifier
emp_addr_1 â†’ Employee Primary Street Address
DOB â†’ Person Birth Date
phone_num â†’ Contact Phone Number
acc_bal â†’ Account Current Balance Amount
email_addr â†’ Contact Email Address
SSN â†’ Person Social Security Number
start_dt â†’ Employment Start Date
cust_first_nm â†’ Customer Given Name
prod_desc_txt â†’ Product Description Text

OUTPUT FORMAT:
Provide ONLY the enriched field name, nothing else. No explanation, no additional text.
"""

ISO_11179_DEFINITION_PRINCIPLES = """
ISO/IEC 11179-4 PRINCIPLES FOR FORMULATING DATA DEFINITIONS

Core Principles for Data Element Definitions:

1. DEFINITION STRUCTURE:
   - State what the concept IS, not what it is not
   - Begin with the concept itself or a noun phrase that categorizes it
   - Use complete sentences with proper grammar and punctuation
   - Definitions must be precise, unambiguous, and concise

2. ESSENTIAL CHARACTERISTICS:
   - Include only essential characteristics that distinguish this element
   - Specify the concept domain or classification
   - Describe the semantic meaning, not implementation details
   - Avoid circular definitions

3. CONTEXT AND SCOPE:
   - Provide context for understanding the element within the domain
   - Specify any constraints, valid ranges, or business rules
   - Indicate relationships to other data elements if relevant

4. DEFINITION TEMPLATE:
   "The [category/type] [that/which] [essential characteristics describing what it represents]. 
   [Purpose and business context]. [Additional constraints, formats, or specifications]."

EXAMPLES:

Data Element: Customer Birth Date
Definition: "The calendar date on which a customer was born, used for age verification, 
demographic analysis, and regulatory compliance purposes. Format: YYYY-MM-DD."

Data Element: Account Current Balance Amount
Definition: "The monetary value representing the current financial position of an account, 
calculated as the sum of all credits minus all debits up to the present moment. Expressed in 
the account's designated currency with two decimal places for fractional units."

Data Element: Customer Primary Email Address
Definition: "The principal electronic mail address designated by a customer for official 
communications and correspondence. Must conform to RFC 5322 email address specification format."

OUTPUT FORMAT:
Provide ONLY the definition text, no preamble or additional commentary.
"""

PII_CLASSIFICATION_FRAMEWORK = """
PII CLASSIFICATION FRAMEWORK

CLASSIFICATION CATEGORIES (ONLY THREE):

1. NON PERSONAL DATA:
   - Definition: Data that cannot identify an individual directly or indirectly
   - Examples: Aggregate statistics, anonymized data, system metadata, transaction types, 
     product categories, company information, general business data
   - Risk: None
   - Protection Required: Standard data security practices

2. PERSONAL DATA:
   - Definition: Information that can identify individuals when combined with other data, 
     or direct identifiers with moderate sensitivity
   - Examples:
     * Full name
     * Email address (personal or business)
     * Phone number
     * Home address / mailing address
     * Date of birth
     * Employee identification number
     * Customer identification number
     * Account numbers
     * IP addresses
     * Device identifiers
     * Photographs
     * Location data
     * Gender
     * Age
   - Risk: Can enable identification, moderate privacy impact
   - Protection Required: Access controls, encryption in transit and at rest, audit logging

3. SENSITIVE PERSONAL DATA:
   - Definition: Highly sensitive data that if compromised could cause severe harm, 
     including special categories under GDPR and critical identifiers
   - Examples:
     * Social Security Number (SSN)
     * Passport number
     * Driver's license number
     * National identification number
     * Tax identification number
     * Financial account numbers (bank account, credit card, debit card)
     * Financial PINs or passwords
     * Medical record numbers
     * Health insurance numbers
     * Medical/health information
     * Mental health data
     * Genetic information
     * Biometric data (fingerprints, facial recognition, iris scan, voice print)
     * Sexual orientation
     * Racial or ethnic origin
     * Political opinions
     * Religious beliefs
     * Trade union membership
     * Criminal history
     * Credentials (username + password combinations)
     * Security questions and answers
   - Risk: Critical - Identity theft, fraud, severe privacy harm, discrimination
   - Protection Required: Maximum security - encryption at rest and in transit, 
     strict access controls, multi-factor authentication, comprehensive audit logging, 
     data loss prevention, tokenization

CLASSIFICATION DECISION TREE:

1. Is this a critical identifier (SSN, passport, financial account, medical record, biometric)?
   â†’ YES: SENSITIVE PERSONAL DATA

2. Is this a GDPR special category (health, genetic, biometric, racial origin, religious beliefs, 
   political opinions, sexual orientation, trade union membership)?
   â†’ YES: SENSITIVE PERSONAL DATA

3. Could this cause severe harm if disclosed (identity theft, financial fraud, discrimination)?
   â†’ YES: SENSITIVE PERSONAL DATA

4. Can this identify a person directly or indirectly (name, email, phone, address, ID numbers)?
   â†’ YES: PERSONAL DATA

5. Is this completely unrelated to any individual person?
   â†’ YES: NON PERSONAL DATA

CLASSIFICATION OUTPUT FORMAT:

Provide a JSON object with:

{
    "pii_category": "<NON PERSONAL DATA | PERSONAL DATA | SENSITIVE PERSONAL DATA>",
    "is_pii": <boolean>,
    "regulatory_considerations": ["<GDPR | HIPAA | CCPA | etc.>"],
    "detailed_rationale": "<Comprehensive explanation of classification decision>"
}

IMPORTANT: Use ONLY these three categories:
- NON PERSONAL DATA
- PERSONAL DATA  
- SENSITIVE PERSONAL DATA

Provide ONLY valid JSON, no additional text.
"""


# ============================================================================
# AGENT STATE DEFINITION
# ============================================================================

class EnrichmentState(TypedDict):
    """State schema for the enrichment and mapping agent system."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    current_field: Dict[str, Any]
    enriched_name: str
    enriched_description: str
    object_name: str
    property_name: str
    pii_classification: Dict[str, Any]
    enrichment_rationale: str
    mapping_rationale: str
    vector_store: Any
    all_objects: List[str]
    all_properties: List[str]
    expert_opinions: List[str]
    reasoning_chain: List[str]


# ============================================================================
# TOOLS FOR REACT AGENTS
# ============================================================================

@tool
def iso_11179_name_enrichment_expert(field_name: str, field_context: str) -> str:
    """
    Expert agent for enriching field names according to ISO 11179 standards.
    Transforms technical field names into semantically precise, human-readable names
    with proper spacing and clear meaning.
    
    Args:
        field_name: The original field name to enrich
        field_context: Context about the field (application, description)
        
    Returns:
        Enriched field name following ISO 11179 naming principles
    """
    prompt = f"""You are an ISO 11179 metadata naming standards expert with deep expertise in 
data governance, semantic modeling, and enterprise metadata management.

TASK: Transform the given technical field name into a semantically precise, ISO 11179 compliant name.

ORIGINAL FIELD NAME: {field_name}

CONTEXT: {field_context}

APPLY THESE ISO 11179 PRINCIPLES:

{ISO_11179_NAMING_PRINCIPLES}

REASONING PROCESS (Think step-by-step):

1. ANALYZE ORIGINAL NAME:
   - Identify the core concept or entity (Object Class)
   - Identify the property or characteristic being described
   - Recognize any qualifiers or modifiers
   - Understand the semantic intent behind abbreviations or technical notation

2. DECONSTRUCT TECHNICAL ELEMENTS:
   - Break down camelCase: "customerEmailAddr" â†’ Customer + Email + Address
   - Break down snake_case: "cust_birth_dt" â†’ Customer + Birth + Date
   - Expand abbreviations with domain knowledge

3. CONSTRUCT ISO 11179 COMPLIANT NAME:
   - Start with Object Class (the entity): Customer, Account, Transaction, etc.
   - Add qualifiers if needed: Primary, Secondary, Current, Previous, etc.
   - Add the property: Name, Address, Date, Amount, Status, etc.
   - Use Title Case with proper spacing

4. VALIDATE SEMANTIC CLARITY:
   - Is the name self-documenting?
   - Would a non-technical business user understand it?
   - Is it unambiguous and precise?

Now provide the enriched name:"""

    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_retry(messages)
    return response.strip()


@tool
def iso_11179_definition_expert(enriched_name: str, original_name: str, context: str) -> str:
    """
    Expert agent for creating ISO 11179 compliant data element definitions.
    Generates precise, formal definitions that explain what the data element represents.
    
    Args:
        enriched_name: The ISO 11179 enriched field name
        original_name: The original technical field name
        context: Business and application context
        
    Returns:
        ISO 11179 compliant definition
    """
    prompt = f"""You are an ISO 11179 metadata definition standards expert specializing in 
formulating precise, formal data element definitions.

TASK: Create a comprehensive, ISO 11179 compliant definition for the given data element.

ENRICHED FIELD NAME: {enriched_name}
ORIGINAL FIELD NAME: {original_name}
CONTEXT: {context}

APPLY THESE ISO 11179 DEFINITION PRINCIPLES:

{ISO_11179_DEFINITION_PRINCIPLES}

REASONING PROCESS FOR DEFINITION FORMULATION:

1. IDENTIFY THE CONCEPT CATEGORY:
   - What type of thing is this? (a date, an amount, an identifier, a description, etc.)
   - What is the superordinate category?

2. SPECIFY ESSENTIAL CHARACTERISTICS:
   - What distinguishes this element from similar elements?
   - What are the key semantic properties?
   - What business purpose does it serve?

3. PROVIDE NECESSARY CONTEXT:
   - How is this element used in business processes?
   - What are valid values or constraints?

4. ADD TECHNICAL SPECIFICATIONS (if relevant):
   - Format specifications (e.g., date format, decimal precision)
   - Valid ranges or enumerated values

Now create the ISO 11179 compliant definition:"""

    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_retry(messages)
    return response.strip()


@tool
def vector_similarity_search(query: str, vector_store_obj: str, top_k: int = 5) -> str:
    """
    Perform semantic similarity search using the in-memory vector store to find 
    best matching object-property combinations.
    
    Args:
        query: The search query (enriched field name + description)
        vector_store_obj: Placeholder for vector store (passed via state)
        top_k: Number of top results to return
        
    Returns:
        JSON string with top similar matches
    """
    # This tool declaration is for the ReAct agent to understand its capabilities
    # Actual implementation happens in the coordinator node using state
    return json.dumps({
        "top_matches": [],
        "note": "This tool performs semantic similarity search using the in-memory vector store"
    })


@tool
def semantic_mapping_expert(
    enriched_name: str, 
    enriched_description: str,
    similar_matches: str,
    context: str
) -> str:
    """
    Expert agent for mapping enriched field names to object-property combinations
    using semantic similarity results from vector store and domain reasoning.
    
    Args:
        enriched_name: The enriched field name
        enriched_description: The enriched description
        similar_matches: Top similar object-property matches from vector search
        context: Business context
        
    Returns:
        JSON string with best_object and best_property
    """
    prompt = f"""You are a semantic data modeling expert specializing in ontology alignment 
and metadata mapping.

TASK: Map the given enriched data element to the most semantically appropriate Object and 
Property from the available vocabulary, using similarity search results as guidance.

ENRICHED FIELD NAME: {enriched_name}
ENRICHED DESCRIPTION: {enriched_description}
CONTEXT: {context}

TOP SIMILAR MATCHES FROM VECTOR SIMILARITY SEARCH:
{similar_matches}

REASONING PROCESS FOR SEMANTIC MAPPING:

1. UNDERSTAND THE DATA ELEMENT:
   - What is the primary entity or concept? (This suggests the Object)
   - What characteristic or attribute is being described? (This suggests the Property)
   - Parse the enriched name: [Object Class] [Qualifiers] [Property] [Representation]

2. ANALYZE SIMILARITY SEARCH RESULTS:
   - Review the top matches from the vector similarity search
   - These represent semantically similar object-property combinations
   - Consider the similarity scores as indicators of semantic closeness

3. IDENTIFY BEST OBJECT:
   - From the enriched name, what is the subject/entity?
   - Look at the objects in the top similarity matches
   - Choose the object that best represents the entity

4. IDENTIFY BEST PROPERTY:
   - What attribute/characteristic is being captured?
   - Look at the properties in the top similarity matches
   - Choose the property that best represents the attribute

5. SEMANTIC VALIDATION:
   - Does this mapping make sense in the business domain?
   - Is the object-property relationship logical?
   - Would this mapping be useful for data integration?

6. CONFIDENCE ASSESSMENT:
   - HIGH: Exact or very close semantic match with top similarity results
   - MEDIUM: Good semantic fit but not perfect alignment
   - LOW: Best available option but significant semantic gap

OUTPUT FORMAT (JSON):
{{
    "best_object": "The most appropriate object from the similarity matches",
    "best_property": "The most appropriate property from the similarity matches",
    "confidence": "HIGH/MEDIUM/LOW",
    "reasoning": "Detailed explanation of why this mapping was chosen, referencing similarity scores and semantic analysis"
}}

Provide ONLY valid JSON, no additional text before or after."""

    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_retry(messages)
    return response.strip()


@tool
def pii_classification_expert(
    enriched_name: str, 
    enriched_description: str,
    object_name: str,
    property_name: str
) -> str:
    """
    Expert agent for classifying data elements according to PII categories:
    NON PERSONAL DATA, PERSONAL DATA, or SENSITIVE PERSONAL DATA.
    
    Args:
        enriched_name: The enriched field name
        enriched_description: The field description
        object_name: Mapped object name
        property_name: Mapped property name
        
    Returns:
        JSON string with comprehensive PII classification
    """
    prompt = f"""You are a data privacy and PII classification expert with deep knowledge of 
NIST SP 800-122, GDPR, CCPA, HIPAA, and global data protection regulations.

TASK: Classify the given data element into one of THREE categories: 
NON PERSONAL DATA, PERSONAL DATA, or SENSITIVE PERSONAL DATA

DATA ELEMENT INFORMATION:
- Enriched Name: {enriched_name}
- Description: {enriched_description}
- Object: {object_name}
- Property: {property_name}

APPLY THIS PII CLASSIFICATION FRAMEWORK:

{PII_CLASSIFICATION_FRAMEWORK}

DETAILED REASONING PROCESS FOR PII CLASSIFICATION:

1. INITIAL PII DETERMINATION:
   - Does this data element relate to an identified or identifiable person?
   - Can it be used to distinguish or trace an individual's identity?
   - Is it linked or linkable to a specific person?

2. CRITICAL IDENTIFIER CHECK:
   - Is this a critical identifier (SSN, passport, financial account, medical record, biometric)?
   â†’ If YES: SENSITIVE PERSONAL DATA

3. GDPR SPECIAL CATEGORY CHECK:
   - Is this a GDPR special category (health, genetic, biometric, racial origin, religious beliefs, 
     political opinions, sexual orientation, trade union membership)?
   â†’ If YES: SENSITIVE PERSONAL DATA

4. HARM ASSESSMENT:
   - Could this cause severe harm if disclosed (identity theft, financial fraud, discrimination)?
   â†’ If YES: SENSITIVE PERSONAL DATA

5. DIRECT/INDIRECT IDENTIFIER:
   - Can this identify a person directly or indirectly (name, email, phone, address, ID numbers)?
   â†’ If YES: PERSONAL DATA

6. NON-PERSONAL CHECK:
   - Is this completely unrelated to any individual person?
   â†’ If YES: NON PERSONAL DATA

7. REGULATORY ANALYSIS:
   - What regulations apply? (GDPR, HIPAA, CCPA, etc.)
   - What are the compliance requirements?

8. CLASSIFICATION ASSIGNMENT:
   - Based on all factors above, assign one of the three categories
   - Provide comprehensive rationale

Now analyze and classify the given data element. Use ONLY these three categories:
- NON PERSONAL DATA
- PERSONAL DATA
- SENSITIVE PERSONAL DATA

Provide ONLY valid JSON:"""

    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_retry(messages)
    return response.strip()


# ============================================================================
# SUPERVISOR AGENT
# ============================================================================

def supervisor_validation_agent(state: EnrichmentState) -> EnrichmentState:
    """
    Supervisor agent that validates the enrichment and mapping results,
    ensures quality, consistency, and provides final rationale.
    """
    prompt = f"""You are a SUPERVISOR AGENT responsible for validating and ensuring quality 
of data enrichment and mapping results. You have oversight of all expert agents and must 
verify their work meets the highest standards.

REVIEW THE FOLLOWING RESULTS:

ORIGINAL FIELD: {state['current_field'].get('Field Name', 'N/A')}
APPLICATION: {state['current_field'].get('Application Name', 'N/A')}
DESCRIPTION: {state['current_field'].get('Application Description', 'N/A')}

EXPERT AGENT RESULTS:
- Enriched Name: {state['enriched_name']}
- Enriched Description: {state['enriched_description']}
- Mapped Object: {state['object_name']}
- Mapped Property: {state['property_name']}
- PII Classification: {state['pii_classification']}

EXPERT OPINIONS RECEIVED:
{chr(10).join(f"- {opinion}" for opinion in state['expert_opinions'])}

REASONING CHAIN:
{chr(10).join(f"{i+1}. {step}" for i, step in enumerate(state['reasoning_chain']))}

YOUR SUPERVISORY VALIDATION TASKS:

1. VALIDATE ENRICHMENT QUALITY:
   - Does the enriched name follow ISO 11179 standards?
   - Is it clear, unambiguous, and semantically precise?
   - Are there any remaining technical artifacts (abbreviations, underscores)?
   - Is the description comprehensive and well-formed?

2. VALIDATE MAPPING CORRECTNESS:
   - Is the Object-Property mapping logically sound?
   - Does it make semantic sense?
   - Is there strong alignment between the enriched name and the mapping?

3. VALIDATE PII CLASSIFICATION:
   - Is the PII classification appropriate and well-reasoned?
   - Is it one of: NON PERSONAL DATA, PERSONAL DATA, or SENSITIVE PERSONAL DATA?
   - Are all privacy considerations accounted for?

4. CONSISTENCY CHECK:
   - Do all components (name, description, mapping, PII) align coherently?
   - Are there any contradictions or inconsistencies?

5. QUALITY ASSURANCE:
   - Would this result be useful for data governance?
   - Could it be understood by stakeholders?
   - Is it production-ready?

PROVIDE YOUR SUPERVISOR ASSESSMENT:

Respond with a JSON object:
{{
    "validation_passed": <boolean>,
    "quality_score": <1-10>,
    "enrichment_rationale": "<Comprehensive explanation validating the enrichment decisions>",
    "mapping_rationale": "<Comprehensive explanation validating the mapping decisions>",
    "concerns": ["<Any concerns or areas for improvement>"],
    "recommendations": ["<Any recommendations for enhancement>"],
    "final_approval": <boolean>
}}

Provide ONLY valid JSON:"""

    messages = [{"role": "user", "content": prompt}]
    response = call_openai_with_retry(messages)
    supervisor_result = json.loads(response.strip())
    
    state['enrichment_rationale'] = supervisor_result.get('enrichment_rationale', '')
    state['mapping_rationale'] = supervisor_result.get('mapping_rationale', '')
    
    state['reasoning_chain'].append(
        f"SUPERVISOR VALIDATION: Quality Score {supervisor_result.get('quality_score', 0)}/10. "
        f"Approval: {supervisor_result.get('final_approval', False)}"
    )
    
    return state


# ============================================================================
# REACT AGENT WORKFLOW NODES
# ============================================================================

def enrichment_coordinator_node(state: EnrichmentState) -> EnrichmentState:
    """
    Coordinator node that orchestrates the enrichment process using dynamic
    chain of thought and mixture of experts reasoning.
    """
    current_field = state['current_field']
    field_name = current_field.get('Field Name', '')
    context = f"""
    Application: {current_field.get('Application Name', '')}
    Description: {current_field.get('Application Description', '')}
    Current Classification: {current_field.get('ISR Classification', '')}
    """
    
    print(f"\n{'='*80}")
    print(f"PROCESSING FIELD: {field_name}")
    print(f"{'='*80}")
    
    state['reasoning_chain'] = []
    state['expert_opinions'] = []
    
    # STEP 1: Name Enrichment using expert agent
    print("\n[STEP 1] ISO 11179 Name Enrichment...")
    state['reasoning_chain'].append(f"ANALYZING: Original field name '{field_name}'")
    print(f"   â†’ Calling name enrichment expert...")
    
    try:
        enriched_name = iso_11179_name_enrichment_expert.invoke({
            "field_name": field_name,
            "field_context": context
        })
        state['enriched_name'] = enriched_name
        state['expert_opinions'].append(f"Name Enrichment Expert: '{enriched_name}'")
        state['reasoning_chain'].append(f"ENRICHED NAME: '{enriched_name}'")
        print(f"   âœ“ Enriched Name: {enriched_name}")
    except Exception as e:
        print(f"   âŒ Name enrichment failed: {e}")
        state['enriched_name'] = field_name  # Use original as fallback
        state['reasoning_chain'].append(f"ERROR: Name enrichment failed, using original name")
    
    # STEP 2: Definition Generation
    print("\n[STEP 2] ISO 11179 Definition Generation...")
    print(f"   â†’ Calling definition expert...")
    
    try:
        enriched_description = iso_11179_definition_expert.invoke({
            "enriched_name": state['enriched_name'],
            "original_name": field_name,
            "context": context
        })
        state['enriched_description'] = enriched_description
        state['expert_opinions'].append(f"Definition Expert: Generated formal definition")
        state['reasoning_chain'].append(f"DEFINITION CREATED: {enriched_description[:100]}...")
        print(f"   âœ“ Definition: {enriched_description[:100]}...")
    except Exception as e:
        print(f"   âŒ Definition generation failed: {e}")
        state['enriched_description'] = f"A data element representing {state['enriched_name'].lower()}."
        state['reasoning_chain'].append(f"ERROR: Definition generation failed, using default")
    
    # STEP 3: Semantic Mapping with Vector Similarity Search
    print("\n[STEP 3] Semantic Mapping with Vector Similarity Search...")
    
    try:
        # Perform vector similarity search using in-memory vector store
        query_text = f"{state['enriched_name']} {state['enriched_description']}"
        print(f"   â†’ Query text (first 80 chars): {query_text[:80]}...")
        print(f"      ðŸ”¹ Generating query embedding...")
        print(f"   â†’ Querying vector store with embedding...")
        print(f"      ðŸ” Performing similarity search (k=5)...")
        
        vector_store = state['vector_store']
        similar_docs = vector_store.similarity_search_with_score(query_text, k=5)
        
        print(f"      âœ“ Found {len(similar_docs)} similar matches")
        
        # Format similarity results for the mapping expert
        similar_matches = []
        for idx, (doc, score) in enumerate(similar_docs, 1):
            obj = doc.metadata.get('object', '')
            prop = doc.metadata.get('property', '')
            print(f"         {idx}. {obj}.{prop} (score: {score:.4f})")
            similar_matches.append({
                "object": obj,
                "property": prop,
                "similarity_score": float(score),
                "text": doc.page_content
            })
        
        similar_matches_json = json.dumps(similar_matches, indent=2)
        state['reasoning_chain'].append(f"VECTOR SIMILARITY SEARCH: Found {len(similar_matches)} matches")
        
        # Use mapping expert to select best match based on similarity results
        print(f"   â†’ Calling semantic mapping expert to analyze results...")
        mapping_result = semantic_mapping_expert.invoke({
            "enriched_name": state['enriched_name'],
            "enriched_description": state['enriched_description'],
            "similar_matches": similar_matches_json,
            "context": context
        })
        
        # Try to parse JSON with error handling
        try:
            mapping_data = json.loads(mapping_result)
            state['object_name'] = mapping_data.get('best_object', 'Unknown')
            state['property_name'] = mapping_data.get('best_property', 'Unknown')
            confidence = mapping_data.get('confidence', 'UNKNOWN')
        except json.JSONDecodeError as je:
            print(f"   âš  JSON decode error in mapping result, extracting manually...")
            # Try to extract object and property from the response text
            state['object_name'] = similar_matches[0]['object'] if similar_matches else 'Unknown'
            state['property_name'] = similar_matches[0]['property'] if similar_matches else 'Unknown'
            confidence = 'LOW'
        
        state['expert_opinions'].append(
            f"Mapping Expert: {state['object_name']}.{state['property_name']} "
            f"(Confidence: {confidence})"
        )
        state['reasoning_chain'].append(
            f"MAPPED TO: Object='{state['object_name']}', Property='{state['property_name']}'"
        )
        print(f"   âœ“ Mapped to: {state['object_name']}.{state['property_name']}")
        
    except Exception as e:
        print(f"   âŒ Semantic mapping failed: {e}")
        state['object_name'] = 'Unknown'
        state['property_name'] = 'Unknown'
        state['reasoning_chain'].append(f"ERROR: Semantic mapping failed")
    
    # STEP 4: PII Classification
    print("\n[STEP 4] PII Classification Analysis...")
    print(f"   â†’ Calling PII classification expert...")
    
    try:
        pii_result = pii_classification_expert.invoke({
            "enriched_name": state['enriched_name'],
            "enriched_description": state['enriched_description'],
            "object_name": state['object_name'],
            "property_name": state['property_name']
        })
        
        # Try to parse JSON with error handling
        try:
            pii_data = json.loads(pii_result)
        except json.JSONDecodeError as je:
            print(f"   âš  JSON decode error in PII result, using defaults...")
            pii_data = {
                "pii_category": "NON PERSONAL DATA",
                "is_pii": False,
                "regulatory_considerations": [],
                "detailed_rationale": "Classification failed due to JSON parsing error"
            }
        
        state['pii_classification'] = pii_data
        state['expert_opinions'].append(
            f"PII Expert: {pii_data.get('pii_category', 'UNKNOWN')}"
        )
        state['reasoning_chain'].append(
            f"PII CLASSIFIED: {pii_data.get('pii_category', 'UNKNOWN')}"
        )
        print(f"   âœ“ PII Category: {pii_data.get('pii_category', 'UNKNOWN')}")
        
    except Exception as e:
        print(f"   âŒ PII classification failed: {e}")
        state['pii_classification'] = {
            "pii_category": "NON PERSONAL DATA",
            "is_pii": False,
            "regulatory_considerations": [],
            "detailed_rationale": "Classification failed due to error"
        }
        state['reasoning_chain'].append(f"ERROR: PII classification failed")
    
    # STEP 5: Supervisor Validation
    print("\n[STEP 5] Supervisor Validation...")
    print(f"   â†’ Calling supervisor agent...")
    
    try:
        state = supervisor_validation_agent(state)
        print(f"   âœ“ Validation completed")
    except Exception as e:
        print(f"   âš  Supervisor validation failed: {e}")
        state['enrichment_rationale'] = "Automated validation completed with errors"
        state['mapping_rationale'] = "Automated validation completed with errors"
    
    print(f"\n{'='*80}\n")
    
    return state


def should_continue(state: EnrichmentState) -> Literal["continue", "end"]:
    """
    Conditional edge to determine if more processing is needed.
    """
    return "end"


# ============================================================================
# BUILD LANGGRAPH WORKFLOW
# ============================================================================

def create_enrichment_graph():
    """
    Create the LangGraph workflow for data enrichment and mapping.
    """
    workflow = StateGraph(EnrichmentState)
    
    workflow.add_node("enrichment_coordinator", enrichment_coordinator_node)
    
    workflow.set_entry_point("enrichment_coordinator")
    
    workflow.add_conditional_edges(
        "enrichment_coordinator",
        should_continue,
        {
            "continue": "enrichment_coordinator",
            "end": END
        }
    )
    
    return workflow.compile()


# ============================================================================
# MAIN PROCESSING FUNCTION
# ============================================================================

def process_data_enrichment_and_mapping():
    """
    Main function to process the entire dataset.
    """
    print("="*80)
    print("ISO 11179 DATA ENRICHMENT AND MAPPING SYSTEM")
    print("="*80)
    
    # Load input JSON
    print("\n[1] Loading input JSON...")
    with open(INPUT_JSON_PATH, 'r') as f:
        input_data = json.load(f)
    print(f"   â†’ Loaded {len(input_data)} records")
    
    # Load Excel and create optimized dictionaries
    print("\n[2] Loading Excel and creating optimized dictionaries...")
    excel_df = pd.read_excel(EXCEL_PATH)
    
    objects_dict = {}
    all_objects = []
    all_properties = []
    
    for _, row in excel_df.iterrows():
        obj = str(row['Object name']).strip()
        prop = str(row['Property name']).strip()
        
        if obj not in objects_dict:
            objects_dict[obj] = []
            all_objects.append(obj)
        
        if prop not in objects_dict[obj]:
            objects_dict[obj].append(prop)
        
        if prop not in all_properties:
            all_properties.append(prop)
    
    print(f"   â†’ Found {len(all_objects)} unique objects")
    print(f"   â†’ Found {len(all_properties)} unique properties")
    
    # Create in-memory vector store with embeddings
    print("\n[3] Creating in-memory vector store with embeddings...")
    print(f"   â†’ Initializing OpenAI embeddings model: {OPENAI_EMBEDDING_MODEL}")
    
    embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)
    
    # Create documents for vector store (row by row)
    documents = []
    print(f"   â†’ Creating {len(all_objects)} object documents...")
    for obj in all_objects:
        for prop in objects_dict[obj]:
            doc_text = f"{obj} {prop}"
            doc = Document(
                page_content=doc_text,
                metadata={"object": obj, "property": prop}
            )
            documents.append(doc)
    
    print(f"   â†’ Total documents created: {len(documents)}")
    print(f"   â†’ Generating embeddings for {len(documents)} documents (row by row)...")
    vector_store = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    print(f"   â†’ In-memory vector store created with {len(documents)} embeddings")
    
    # Create LangGraph workflow
    print("\n[4] Initializing LangGraph workflow...")
    app = create_enrichment_graph()
    print("   â†’ Workflow initialized")
    
    # Process each field (row by row)
    print(f"\n[5] Processing {len(input_data)} fields...")
    print(f"   â†’ Results will be written incrementally to: {OUTPUT_CSV_PATH}")
    
    # Check if output file already exists to resume from where we left off
    processed_ids = set()
    resume_mode = False
    if os.path.exists(OUTPUT_CSV_PATH):
        try:
            existing_df = pd.read_csv(OUTPUT_CSV_PATH)
            processed_ids = set(existing_df['EIM ID'].tolist())
            print(f"   â†’ Found existing output file with {len(processed_ids)} processed records")
            print(f"   â†’ RESUME MODE: Will skip already processed records and continue")
            resume_mode = True
        except Exception as e:
            print(f"   â†’ Could not read existing file ({e}), starting fresh")
    
    results = []
    successful_count = 0
    failed_count = 0
    skipped_count = 0
    
    for idx, record in enumerate(input_data):
        eim_id = record.get("EIM ID", "")
        
        # Skip if already processed
        if eim_id in processed_ids:
            skipped_count += 1
            print(f"\n{'='*80}")
            print(f"Skipping Record {idx + 1}/{len(input_data)} (EIM ID: {eim_id}) - Already Processed")
            print(f"{'='*80}")
            continue
        
        print(f"\n{'='*80}")
        print(f"Processing Record {idx + 1}/{len(input_data)} (EIM ID: {eim_id})")
        print(f"{'='*80}")
        
        try:
            initial_state = {
                "messages": [],
                "current_field": record,
                "enriched_name": "",
                "enriched_description": "",
                "object_name": "",
                "property_name": "",
                "pii_classification": {},
                "enrichment_rationale": "",
                "mapping_rationale": "",
                "vector_store": vector_store,
                "all_objects": all_objects,
                "all_properties": all_properties,
                "expert_opinions": [],
                "reasoning_chain": []
            }
            
            final_state = app.invoke(initial_state)
            
            pii_class = final_state['pii_classification']
            
            result = {
                "EIM ID": eim_id,
                "Application Name": record.get("Application Name", ""),
                "Application Description": record.get("Application Description", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": final_state['enriched_name'],
                "Enriched Description": final_state['enriched_description'],
                "Mapped Object": final_state['object_name'],
                "Mapped Property": final_state['property_name'],
                "PII Category": pii_class.get('pii_category', ''),
                "Is PII": pii_class.get('is_pii', False),
                "Regulatory Considerations": ", ".join(pii_class.get('regulatory_considerations', [])),
                "PII Rationale": pii_class.get('detailed_rationale', ''),
                "Enrichment Rationale": final_state['enrichment_rationale'],
                "Mapping Rationale": final_state['mapping_rationale'],
                "Original ISR Classification": record.get("ISR Classification", ""),
                "Processing Status": "SUCCESS"
            }
            
            successful_count += 1
            print(f"\nâœ“ Record {idx + 1} processed successfully")
            
        except Exception as e:
            print(f"\nâŒ Error processing record {idx + 1}: {e}")
            print(f"   â†’ Creating fallback result with error information")
            
            result = {
                "EIM ID": eim_id,
                "Application Name": record.get("Application Name", ""),
                "Application Description": record.get("Application Description", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": record.get("Field Name", ""),
                "Enriched Description": f"Error during processing: {str(e)[:200]}",
                "Mapped Object": "Unknown",
                "Mapped Property": "Unknown",
                "PII Category": "NON PERSONAL DATA",
                "Is PII": False,
                "Regulatory Considerations": "",
                "PII Rationale": f"Processing failed: {str(e)[:200]}",
                "Enrichment Rationale": "Error during enrichment",
                "Mapping Rationale": "Error during mapping",
                "Original ISR Classification": record.get("ISR Classification", ""),
                "Processing Status": "FAILED"
            }
            
            failed_count += 1
        
        results.append(result)
        
        # Write to CSV incrementally after each record
        try:
            result_df = pd.DataFrame([result])
            if not resume_mode and len(results) == 1:
                # First record in a fresh run - write with header
                result_df.to_csv(OUTPUT_CSV_PATH, index=False, mode='w')
                print(f"   â†’ Created output file and wrote first record")
            else:
                # Append without header (resume mode or subsequent records)
                result_df.to_csv(OUTPUT_CSV_PATH, index=False, mode='a', header=False)
                print(f"   â†’ Appended record to output file")
        except Exception as e:
            print(f"   âš  Warning: Failed to write to CSV: {e}")
            print(f"   â†’ Data is still in memory and will be saved at the end")
    
    print(f"\n[6] Processing Summary...")
    print(f"   â†’ Total records in input: {len(input_data)}")
    if skipped_count > 0:
        print(f"   â†’ Skipped (already processed): {skipped_count}")
    print(f"   â†’ Successfully processed (new): {successful_count}")
    print(f"   â†’ Failed (new): {failed_count}")
    if successful_count + failed_count > 0:
        print(f"   â†’ Success rate: {(successful_count/(successful_count+failed_count)*100):.1f}%")
    print(f"   â†’ Output file: {OUTPUT_CSV_PATH}")
    
    print("\n" + "="*80)
    print("PROCESSING COMPLETE!")
    print("="*80)
    print(f"\nTotal records in input: {len(input_data)}")
    if skipped_count > 0:
        print(f"Already processed (skipped): {skipped_count}")
    print(f"Newly processed (success): {successful_count}")
    print(f"Newly processed (failed): {failed_count}")
    if successful_count + failed_count > 0:
        print(f"Success rate for new records: {(successful_count/(successful_count+failed_count)*100):.1f}%")
    
    print(f"\nOutput file: {OUTPUT_CSV_PATH}")
    print(f"Total records in output file: {skipped_count + successful_count + failed_count}")
    print(f"\nâœ“ All results have been written incrementally to the CSV file")
    
    if failed_count > 0:
        print(f"\nâš  Note: {failed_count} record(s) failed processing but were saved with error information")
        print(f"   Check the 'Processing Status' column in the output CSV for 'FAILED' entries")
    
    if resume_mode and skipped_count > 0:
        print(f"\nðŸ’¡ Resume mode was active - {skipped_count} previously processed records were skipped")
        print(f"   To reprocess all records, delete {OUTPUT_CSV_PATH} and run again")


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    process_data_enrichment_and_mapping()
