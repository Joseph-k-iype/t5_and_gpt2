"""
Data Processing Purpose Mapper using LangGraph, LangChain, and OpenAI o3-mini
Maps case data to processing purposes with dynamic chain of thought and mixture of experts
"""

import os
import csv
import json
import asyncio
import uuid
from typing import Dict, List, TypedDict, Annotated, Sequence
from dataclasses import dataclass
import numpy as np
from collections import Counter
import statistics

# HTTP Client
import httpx

# OpenAI
import openai
from openai import OpenAI, AsyncOpenAI

# Pydantic
from pydantic import BaseModel, Field

# LangChain
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import ChatOpenAI

# LangGraph
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# AMToken Configuration
AMTOKEN = os.getenv("AMTOKEN", "your-amtoken-here")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_MODEL = "o3-mini"
EMBEDDING_MODEL = "text-embedding-3-large"
USER_ID = os.getenv("USER_ID", "UC0002731")

# Setup httpx client with custom configuration
httpx_client = httpx.Client(http2=True, verify=False)
async_httpx_client = httpx.AsyncClient(http2=True, verify=False)

# Default headers
default_headers = {
    "AMToken": AMTOKEN,
    "Content-Type": "application/json",
    "Token_Type": "SESSION_TOKEN",
    "x-correlation-id": str(uuid.uuid4()),
    "x-usersession-id": "abcdef"
}

# Extra headers
extra_headers = {
    "Authorization": f"session {AMTOKEN}"
}

# Initialize OpenAI clients with custom configuration
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    http_client=httpx_client,
    default_headers=default_headers
)

async_openai_client = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    http_client=async_httpx_client,
    default_headers=default_headers
)

# Target categories
TARGET_CATEGORIES = [
    "Manage Relationship (clients/employees/vendors)",
    "Internal Management",
    "Merger and acquisition",
    "Establishment, exercise or defense of legal claims"
]


# ============================================================================
# DATA MODELS
# ============================================================================

class CaseData(BaseModel):
    """Case data model"""
    case_id: str
    project_title: str
    project_overview: str
    purpose_of_processing: str
    dv_purpose_of_processing: str
    m_rop_purpose_of_processing: str


class ExpertOpinion(BaseModel):
    """Expert opinion model"""
    expert_name: str
    categories: List[str]
    reasoning: str
    confidence: float


class MappingResult(BaseModel):
    """Final mapping result"""
    case_id: str
    mapped_categories: List[str]
    reasoning: str
    confidence_score: float
    expert_opinions: List[ExpertOpinion]
    semantic_scores: Dict[str, float]
    keyword_scores: Dict[str, float]


class GraphState(TypedDict):
    """State for LangGraph workflow"""
    case_data: CaseData
    context_analysis: str
    expert_opinions: List[ExpertOpinion]
    vector_store: InMemoryVectorStore
    mapping_result: MappingResult
    messages: Sequence[HumanMessage | AIMessage | SystemMessage]


# ============================================================================
# EMBEDDING UTILITY (Direct OpenAI API Client)
# ============================================================================

class DirectEmbeddings:
    """Direct OpenAI embeddings using API client (not LangChain's way)"""
    
    def __init__(self, model: str = EMBEDDING_MODEL):
        self.model = model
        self.client = openai_client
        self.async_client = async_openai_client
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query"""
        response = self.client.embeddings.create(
            model=self.model,
            input=text,
            user=USER_ID,
            extra_headers=extra_headers
        )
        return response.data[0].embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed multiple documents"""
        response = self.client.embeddings.create(
            model=self.model,
            input=texts,
            user=USER_ID,
            extra_headers=extra_headers
        )
        return [item.embedding for item in response.data]
    
    async def aembed_query(self, text: str) -> List[float]:
        """Async embed a single query"""
        response = await self.async_client.embeddings.create(
            model=self.model,
            input=text,
            user=USER_ID,
            extra_headers=extra_headers
        )
        return response.data[0].embedding
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """Async embed multiple documents"""
        response = await self.async_client.embeddings.create(
            model=self.model,
            input=texts,
            user=USER_ID,
            extra_headers=extra_headers
        )
        return [item.embedding for item in response.data]


# ============================================================================
# LANGCHAIN CHAT MODEL SETUP
# ============================================================================

def get_chat_model() -> ChatOpenAI:
    """Get configured ChatOpenAI instance"""
    return ChatOpenAI(
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL,
        http_client=httpx_client,
        default_headers=default_headers,
        model_kwargs={"extra_headers": extra_headers},
        model_name=OPENAI_MODEL,
        extra_body={"user": USER_ID}
    )


# ============================================================================
# VECTOR STORE SETUP
# ============================================================================

def setup_vector_store() -> InMemoryVectorStore:
    """Setup InMemoryVectorStore with category descriptions"""
    try:
        print("Initializing embeddings...")
        embeddings = DirectEmbeddings()
        
        print("Creating vector store...")
        vector_store = InMemoryVectorStore(embedding=embeddings)
        
        # Category descriptions for semantic matching
        category_docs = [
            Document(
                page_content="Managing relationships with clients, customers, employees, vendors, suppliers, partners, and other stakeholders. Customer relationship management, employee management, vendor management, supplier relations, client communications, partnership management.",
                metadata={"category": TARGET_CATEGORIES[0]}
            ),
            Document(
                page_content="Internal management, business operations, administrative tasks, organizational management, internal processes, operational efficiency, business administration, internal controls, management systems, organizational structure.",
                metadata={"category": TARGET_CATEGORIES[1]}
            ),
            Document(
                page_content="Mergers and acquisitions, M&A, corporate transactions, due diligence, company acquisition, merger process, business combination, corporate restructuring, acquisition planning, post-merger integration.",
                metadata={"category": TARGET_CATEGORIES[2]}
            ),
            Document(
                page_content="Legal claims, establishment of legal claims, exercise of legal rights, defense against legal claims, litigation, legal proceedings, legal disputes, lawful claims, legal defense, claim management, dispute resolution.",
                metadata={"category": TARGET_CATEGORIES[3]}
            )
        ]
        
        print("Adding category documents to vector store...")
        vector_store.add_documents(category_docs)
        print("Vector store setup complete!")
        
        return vector_store
        
    except Exception as e:
        print(f"Error setting up vector store: {e}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        raise


# ============================================================================
# CHAIN OF THOUGHT PROMPT GENERATOR
# ============================================================================

def generate_dynamic_cot_prompt(case_data: CaseData, context: str) -> str:
    """Generate dynamic chain of thought prompt based on case context"""
    
    prompt = f"""You are an expert data privacy analyst specializing in purpose of processing categorization.

**Case Information:**
- Case ID: {case_data.case_id}
- Project Title: {case_data.project_title}
- Project Overview: {case_data.project_overview}
- Purpose of Processing: {case_data.purpose_of_processing}
- DV Purpose of Processing: {case_data.dv_purpose_of_processing}
- M ROP Purpose of Processing: {case_data.m_rop_purpose_of_processing}

**Context Analysis:**
{context}

**Available Categories:**
1. Manage Relationship (clients/employees/vendors)
2. Internal Management
3. Merger and acquisition
4. Establishment, exercise or defense of legal claims

**Your Task:**
Using chain of thought reasoning, analyze this case step by step:

1. **Initial Understanding**: What is the primary business activity described?
2. **Key Indicators**: What keywords, phrases, or concepts indicate specific purposes?
3. **Category Evaluation**: For each category, assess relevance (0-10 scale):
   - Manage Relationship: Does it involve client, employee, or vendor interactions?
   - Internal Management: Does it relate to internal operations or administration?
   - Merger and acquisition: Does it involve M&A activities or due diligence?
   - Legal claims: Does it relate to legal proceedings or claim management?
4. **Reasoning**: Explain your reasoning for each potential match
5. **Final Decision**: Select one or more categories that best fit

Provide your analysis in JSON format:
{{
    "categories": ["category1", "category2"],
    "reasoning": "detailed reasoning",
    "confidence": 0.85
}}"""
    
    return prompt


# ============================================================================
# LANGGRAPH NODES
# ============================================================================

async def analyze_context(state: GraphState) -> GraphState:
    """Node: Analyze case context using o3-mini"""
    case_data = state["case_data"]
    
    # Combine all case information for context
    full_context = f"""
    Project Title: {case_data.project_title}
    Overview: {case_data.project_overview}
    Purpose: {case_data.purpose_of_processing}
    DV Purpose: {case_data.dv_purpose_of_processing}
    M ROP Purpose: {case_data.m_rop_purpose_of_processing}
    """
    
    # Use o3-mini for context analysis
    response = await async_openai_client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": "You are a data processing expert. Analyze the following case and provide a comprehensive summary of the key activities, stakeholders, and purposes."},
            {"role": "user", "content": full_context}
        ],
        user=USER_ID,
        extra_headers=extra_headers
    )
    
    context_analysis = response.choices[0].message.content
    state["context_analysis"] = context_analysis
    
    return state


async def mixture_of_experts(state: GraphState) -> GraphState:
    """Node: Get opinions from multiple expert perspectives"""
    case_data = state["case_data"]
    context = state["context_analysis"]
    
    # Define expert perspectives
    experts = [
        {
            "name": "Legal Compliance Expert",
            "system_prompt": "You are a legal compliance expert focusing on GDPR and data protection regulations. Analyze from a legal compliance perspective."
        },
        {
            "name": "Business Operations Expert",
            "system_prompt": "You are a business operations expert. Analyze from a business process and operational perspective."
        },
        {
            "name": "Data Privacy Expert",
            "system_prompt": "You are a data privacy specialist. Analyze from a data processing and privacy perspective."
        }
    ]
    
    expert_opinions = []
    
    for expert in experts:
        prompt = generate_dynamic_cot_prompt(case_data, context)
        
        response = await async_openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": expert["system_prompt"]},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            user=USER_ID,
            extra_headers=extra_headers
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            opinion = ExpertOpinion(
                expert_name=expert["name"],
                categories=result.get("categories", []),
                reasoning=result.get("reasoning", ""),
                confidence=result.get("confidence", 0.5)
            )
            expert_opinions.append(opinion)
        except Exception as e:
            print(f"Error parsing expert opinion: {e}")
    
    state["expert_opinions"] = expert_opinions
    return state


async def calculate_semantic_similarity(state: GraphState) -> GraphState:
    """Node: Calculate semantic similarity scores"""
    case_data = state["case_data"]
    vector_store = state["vector_store"]
    
    try:
        # Combine case text for semantic search
        case_text = f"{case_data.project_title} {case_data.project_overview} {case_data.purpose_of_processing} {case_data.dv_purpose_of_processing} {case_data.m_rop_purpose_of_processing}"
        
        print(f"  Calculating semantic similarity for case {case_data.case_id}...")
        
        # Search for similar categories
        results = vector_store.similarity_search_with_score(case_text, k=4)
        
        # Store semantic scores
        semantic_scores = {}
        for doc, score in results:
            category = doc.metadata["category"]
            semantic_scores[category] = float(1 - score)  # Convert distance to similarity
        
        # Ensure all categories have a score
        for category in TARGET_CATEGORIES:
            if category not in semantic_scores:
                semantic_scores[category] = 0.0
        
        # Store in a temporary state variable
        if "mapping_result" not in state:
            state["mapping_result"] = MappingResult(
                case_id=case_data.case_id,
                mapped_categories=[],
                reasoning="",
                confidence_score=0.0,
                expert_opinions=[],
                semantic_scores={},
                keyword_scores={}
            )
        
        state["mapping_result"].semantic_scores = semantic_scores
        print(f"  Semantic similarity calculated successfully")
        
    except Exception as e:
        print(f"  Error in semantic similarity calculation: {e}")
        print(f"  Error type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        
        # Initialize with default scores on error
        if "mapping_result" not in state:
            state["mapping_result"] = MappingResult(
                case_id=case_data.case_id,
                mapped_categories=[],
                reasoning="",
                confidence_score=0.0,
                expert_opinions=[],
                semantic_scores={},
                keyword_scores={}
            )
        state["mapping_result"].semantic_scores = {cat: 0.0 for cat in TARGET_CATEGORIES}
    
    return state


def calculate_keyword_scores(state: GraphState) -> GraphState:
    """Node: Calculate keyword matching scores"""
    case_data = state["case_data"]
    
    # Define keywords for each category
    category_keywords = {
        TARGET_CATEGORIES[0]: ["client", "customer", "employee", "vendor", "supplier", "partner", "relationship", "crm", "stakeholder"],
        TARGET_CATEGORIES[1]: ["internal", "management", "operation", "administrative", "organization", "business", "control", "process"],
        TARGET_CATEGORIES[2]: ["merger", "acquisition", "m&a", "due diligence", "transaction", "corporate", "integration"],
        TARGET_CATEGORIES[3]: ["legal", "claim", "litigation", "defense", "lawsuit", "dispute", "court", "proceeding", "lawful"]
    }
    
    # Combine all case text
    case_text = f"{case_data.project_title} {case_data.project_overview} {case_data.purpose_of_processing} {case_data.dv_purpose_of_processing} {case_data.m_rop_purpose_of_processing}".lower()
    
    keyword_scores = {}
    
    for category, keywords in category_keywords.items():
        matches = sum(1 for keyword in keywords if keyword in case_text)
        keyword_scores[category] = matches / len(keywords)
    
    state["mapping_result"].keyword_scores = keyword_scores
    return state


def aggregate_and_score(state: GraphState) -> GraphState:
    """Node: Aggregate expert opinions and calculate final confidence score"""
    expert_opinions = state["expert_opinions"]
    mapping_result = state["mapping_result"]
    
    # Count category votes from experts
    category_votes = Counter()
    for opinion in expert_opinions:
        for category in opinion.categories:
            category_votes[category] += 1
    
    # Get most voted categories
    total_experts = len(expert_opinions)
    threshold = total_experts * 0.5  # At least 50% agreement
    
    selected_categories = [cat for cat, votes in category_votes.items() if votes >= threshold]
    
    # If no category meets threshold, take top categories
    if not selected_categories and category_votes:
        max_votes = max(category_votes.values())
        selected_categories = [cat for cat, votes in category_votes.items() if votes == max_votes]
    
    # Calculate mathematical confidence score
    confidence_components = []
    
    # 1. Expert consensus (weight: 0.4)
    if selected_categories:
        avg_expert_votes = statistics.mean([category_votes.get(cat, 0) / total_experts for cat in selected_categories])
        confidence_components.append(avg_expert_votes * 0.4)
    
    # 2. Semantic similarity (weight: 0.3)
    if selected_categories and mapping_result.semantic_scores:
        avg_semantic = statistics.mean([mapping_result.semantic_scores.get(cat, 0) for cat in selected_categories])
        confidence_components.append(avg_semantic * 0.3)
    
    # 3. Keyword matching (weight: 0.2)
    if selected_categories and mapping_result.keyword_scores:
        avg_keyword = statistics.mean([mapping_result.keyword_scores.get(cat, 0) for cat in selected_categories])
        confidence_components.append(avg_keyword * 0.2)
    
    # 4. Expert confidence average (weight: 0.1)
    expert_confidences = [op.confidence for op in expert_opinions if any(cat in selected_categories for cat in op.categories)]
    if expert_confidences:
        avg_expert_conf = statistics.mean(expert_confidences)
        confidence_components.append(avg_expert_conf * 0.1)
    
    # Final confidence score
    final_confidence = sum(confidence_components) if confidence_components else 0.0
    
    # Compile reasoning
    reasoning_parts = []
    for opinion in expert_opinions:
        if any(cat in selected_categories for cat in opinion.categories):
            reasoning_parts.append(f"{opinion.expert_name}: {opinion.reasoning}")
    
    combined_reasoning = "\n\n".join(reasoning_parts)
    
    # Update mapping result
    mapping_result.mapped_categories = selected_categories
    mapping_result.reasoning = combined_reasoning
    mapping_result.confidence_score = round(final_confidence, 4)
    mapping_result.expert_opinions = expert_opinions
    
    state["mapping_result"] = mapping_result
    
    return state


# ============================================================================
# LANGGRAPH WORKFLOW
# ============================================================================

def create_workflow() -> StateGraph:
    """Create LangGraph workflow"""
    workflow = StateGraph(GraphState)
    
    # Add nodes
    workflow.add_node("analyze_context", analyze_context)
    workflow.add_node("mixture_of_experts", mixture_of_experts)
    workflow.add_node("calculate_semantic", calculate_semantic_similarity)
    workflow.add_node("calculate_keywords", calculate_keyword_scores)
    workflow.add_node("aggregate_score", aggregate_and_score)
    
    # Add edges
    workflow.set_entry_point("analyze_context")
    workflow.add_edge("analyze_context", "mixture_of_experts")
    workflow.add_edge("mixture_of_experts", "calculate_semantic")
    workflow.add_edge("calculate_semantic", "calculate_keywords")
    workflow.add_edge("calculate_keywords", "aggregate_score")
    workflow.add_edge("aggregate_score", END)
    
    return workflow.compile()


# ============================================================================
# MAIN PROCESSING FUNCTION
# ============================================================================

async def process_csv(csv_path: str, output_path: str = "mapping_results.json"):
    """Process CSV file and generate mappings"""
    
    print("Setting up vector store...")
    vector_store = setup_vector_store()
    
    print(f"Reading CSV from {csv_path}...")
    cases = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            case = CaseData(
                case_id=row.get('CaseId', ''),
                project_title=row.get('ProjectTitle', ''),
                project_overview=row.get('ProjectOverview', ''),
                purpose_of_processing=row.get('Purpose of Processing', ''),
                dv_purpose_of_processing=row.get('DV Purpose of Processing', ''),
                m_rop_purpose_of_processing=row.get('M ROP Purpose of Processing', '')
            )
            cases.append(case)
    
    print(f"Processing {len(cases)} cases...")
    
    # Create workflow
    workflow = create_workflow()
    
    results = []
    
    for i, case in enumerate(cases, 1):
        print(f"\nProcessing case {i}/{len(cases)}: {case.case_id}")
        
        # Initialize state
        initial_state = GraphState(
            case_data=case,
            context_analysis="",
            expert_opinions=[],
            vector_store=vector_store,
            mapping_result=None,
            messages=[]
        )
        
        # Run workflow
        final_state = await workflow.ainvoke(initial_state)
        
        # Get result
        result = final_state["mapping_result"]
        results.append(result.model_dump())
        
        print(f"  Mapped to: {', '.join(result.mapped_categories)}")
        print(f"  Confidence: {result.confidence_score:.4f}")
    
    # Save results
    print(f"\nSaving results to {output_path}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("Processing complete!")
    return results


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python purpose_mapper.py <csv_file_path> [output_path]")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else "mapping_results.json"
    
    asyncio.run(process_csv(csv_file, output_file))
