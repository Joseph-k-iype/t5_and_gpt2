"""
Enhanced Legal Text to Machine-Readable Rules Multi-Agent System (UNIFIED FRAMEWORK VERSION)
================================================================================

OVERVIEW:
Converts multiple related legislation PDFs into formal ontologies and decision tables using LangGraph and LangChain.
Treats all PDF documents as a UNIFIED LEGAL FRAMEWORK (like articles under chapters) rather than separate documents.
Uses latest PyMuPDF, advanced NLP features, and comprehensive multi-agent architecture.

KEY FEATURES:
✅ UNIFIED PROCESSING: All PDFs processed as ONE coherent legal framework
✅ LATEST TECH: Pydantic2, LangGraph multi-agent, OpenAI o3-mini-2025-01-31, text-embedding-3-large
✅ ELASTICSEARCH: Certificate-based authentication with .crt file support  
✅ DATA PROTECTION FOCUS: Data transfer, access, and entitlements extraction
✅ ROBUST ERROR HANDLING: Dynamic JSON serialization error fixing
✅ SINGLE OUTPUT: One unified output file for all documents combined

UNIFIED APPROACH:
- Documents treated as related articles/chapters of comprehensive legal system
- Cross-document relationships and references preserved
- Unified entity extraction and deduplication across all documents
- Comprehensive framework-level decision tables
- Single output representing entire legal framework

MULTI-AGENT ARCHITECTURE:
1. Document Processor → Unified text extraction and preprocessing
2. Intelligent Segmentation → Framework-wide atomic rule extraction  
3. Entity Extraction → Unified legal entity identification (Controller, Processor, etc.)
4. Concept Extraction → Framework concepts (DataTransfer, DataAccess, DataEntitlement)
5. Rule Components → Logical structure analysis (Obligation, Right, Restriction, Condition)
6. Ontology Formalization → OWL-DL ontology creation
7. Decision Tables → Unified decision table generation
8. Final Output → Single JSON output with rules, conditions, domains, roles

OUTPUT FILES:
- final_unified_rules_output_[timestamp].json (simplified rules with conditions, domains, roles)
- unified_decision_tables_[timestamp].json (decision tables in JSON format)
- complete_unified_output_[timestamp].json (combined output)
- unified_legal_ontology_[timestamp].ttl (OWL/TTL ontology)

CONFIGURATION:
Set environment variables:
- OPENAI_API_KEY
- ELASTICSEARCH_HOST, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD, ELASTICSEARCH_CERT_PATH

USAGE:
1. Place PDF documents in ./legislation_pdfs/
2. Create ./legislation_metadata.json with document metadata
3. Run: python legal_rules_system.py

The system processes all PDFs as a unified legal framework and generates consolidated outputs.
"""

import json
import os
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Set, Annotated, Sequence
from datetime import datetime
import logging
import asyncio
import re
import enum
import sys

# Import Literal with better compatibility handling
if sys.version_info >= (3, 8):
    try:
        from typing import Literal
    except ImportError:
        from typing_extensions import Literal
else:
    from typing_extensions import Literal

# Core dependencies
from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict
from openai import OpenAI
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# LangGraph and LangChain for advanced workflows
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Elasticsearch for vector storage
from elasticsearch import Elasticsearch
import ssl

# Latest PyMuPDF import
import pymupdf

# Ontology and RDF
from rdflib import Graph, Namespace, URIRef, Literal as RDFLiteral, BNode
from rdflib.namespace import RDF, RDFS, OWL, XSD
import owlready2 as owl

# Graph processing for advanced analysis
from collections import defaultdict
import pickle
from concurrent.futures import ThreadPoolExecutor
import aiofiles

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
MODEL_NAME = "o3-mini-2025-01-31"
EMBEDDING_MODEL = "text-embedding-3-large"

# Elasticsearch Configuration
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost:9200")
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "your-elasticsearch-username")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "/path/to/elasticsearch.crt")
ELASTICSEARCH_INDEX = os.getenv("ELASTICSEARCH_INDEX", "legal_rules_index")

# Directory Configuration
PDF_DIRECTORY = Path("./legislation_pdfs")
METADATA_FILE = Path("./legislation_metadata.json")
OUTPUT_DIRECTORY = Path("./output")
ONTOLOGY_OUTPUT = Path("./output/ontologies")
DECISION_TABLES_OUTPUT = Path("./output/decision_tables")

# Setup directories
for directory in [OUTPUT_DIRECTORY, ONTOLOGY_OUTPUT, DECISION_TABLES_OUTPUT]:
    directory.mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# Initialize Elasticsearch client
def create_elasticsearch_client():
    """Create Elasticsearch client with certificate authentication"""
    try:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        if os.path.exists(ELASTICSEARCH_CERT_PATH):
            context.load_verify_locations(ELASTICSEARCH_CERT_PATH)
            context.verify_mode = ssl.CERT_REQUIRED
        
        es_client = Elasticsearch(
            [f"https://{ELASTICSEARCH_HOST}"],
            basic_auth=(ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD),
            ssl_context=context,
            verify_certs=True if os.path.exists(ELASTICSEARCH_CERT_PATH) else False,
            ca_certs=ELASTICSEARCH_CERT_PATH if os.path.exists(ELASTICSEARCH_CERT_PATH) else None
        )
        
        # Test connection
        if es_client.ping():
            logger.info("Elasticsearch connection established successfully")
            return es_client
        else:
            logger.error("Failed to connect to Elasticsearch")
            return None
    except Exception as e:
        logger.error(f"Elasticsearch connection error: {e}")
        return None

# Global Elasticsearch client
ES_CLIENT = create_elasticsearch_client()

# ============================================================================
# ENHANCED PYDANTIC DATA MODELS (Latest Pydantic2 Features)
# ============================================================================

class LegalAuthorityLevel(str, enum.Enum):
    CONSTITUTIONAL = "constitutional"
    STATUTORY = "statutory"
    REGULATORY = "regulatory"
    ADMINISTRATIVE = "administrative"
    CASE_LAW = "case_law"

class JurisdictionScope(str, enum.Enum):
    NATIONAL = "national"
    REGIONAL = "regional"
    INTERNATIONAL = "international"
    SECTOR_SPECIFIC = "sector_specific"

class EntityType(str, enum.Enum):
    CONTROLLER = "Controller"
    PROCESSOR = "Processor"
    JOINT_CONTROLLER = "JointController"
    DATA_SUBJECT = "DataSubject"
    THIRD_COUNTRY = "ThirdCountry"
    SUPERVISING_AUTHORITY = "SupervisingAuthority"

class ConceptType(str, enum.Enum):
    DATA_TRANSFER = "DataTransfer"
    DATA_ACCESS = "DataAccess"
    DATA_ENTITLEMENT = "DataEntitlement"
    PROCESSING = "Processing"

class RuleComponentType(str, enum.Enum):
    RESTRICTION = "Restriction"
    CONDITION = "Condition"
    OBLIGATION = "Obligation"
    RIGHT = "Right"

class LogicalOperator(str, enum.Enum):
    AND = "AND"
    OR = "OR"
    NOT = "NOT"
    IF_THEN = "IF_THEN"

class DeonticType(str, enum.Enum):
    OBLIGATORY = "obligatory"
    PERMISSIBLE = "permissible"
    FORBIDDEN = "forbidden"
    OPTIONAL = "optional"

class ComplexityLevel(str, enum.Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class LegalEntity(BaseModel):
    """Enhanced legal entity with detailed classification"""
    model_config = ConfigDict(use_enum_values=True)
    
    name: str
    type: EntityType
    description: str
    context: str
    attributes: Dict[str, Any] = Field(default_factory=dict)
    relationships: List[str] = Field(default_factory=list)
    confidence: float = Field(ge=0.0, le=1.0)

class LegalConcept(BaseModel):
    """Enhanced legal concept with semantic relationships"""
    model_config = ConfigDict(use_enum_values=True)
    
    name: str
    type: ConceptType
    description: str
    context: str
    preconditions: List[str] = Field(default_factory=list)
    consequences: List[str] = Field(default_factory=list)
    semantic_relationships: Dict[str, List[str]] = Field(default_factory=dict)
    confidence: float = Field(ge=0.0, le=1.0)

class RuleComponent(BaseModel):
    """Enhanced rule component with logical relationships"""
    model_config = ConfigDict(use_enum_values=True)
    
    name: str
    type: RuleComponentType
    description: str
    applies_to: List[str]
    legal_basis: str
    enforcement_mechanism: str = ""
    penalty: str = ""
    exceptions: List[str] = Field(default_factory=list)
    logical_operator: LogicalOperator = LogicalOperator.AND
    confidence: float = Field(ge=0.0, le=1.0)

class LegalCitation(BaseModel):
    """Enhanced citation with hierarchical structure"""
    document_id: str
    article: Optional[str] = None
    section: Optional[str] = None
    subsection: Optional[str] = None
    paragraph: Optional[str] = None
    authority_level: LegalAuthorityLevel = LegalAuthorityLevel.STATUTORY
    jurisdiction: JurisdictionScope = JurisdictionScope.NATIONAL

class EnhancedAtomicRule(BaseModel):
    """Comprehensive atomic rule with full legal context"""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str
    text: str
    entities: List[LegalEntity]
    concepts: List[LegalConcept]
    rule_components: List[RuleComponent]
    semantic_roles: Dict[str, str]
    source_document: str
    citation: LegalCitation
    confidence: float
    
    # Enhanced legal analysis
    legal_authority_level: LegalAuthorityLevel
    jurisdictional_scope: JurisdictionScope
    precedence_weight: float = 1.0
    conflicts_with: List[str] = Field(default_factory=list)
    supports: List[str] = Field(default_factory=list)
    exceptions: List[str] = Field(default_factory=list)
    
    # Logical structure
    deontic_type: DeonticType
    modal_operator: Optional[str] = None
    logical_structure: Dict[str, Any] = Field(default_factory=dict)
    
    # NLP analysis
    sentiment_score: float = 0.0
    complexity_score: float = 0.0
    complexity_level: ComplexityLevel = ComplexityLevel.MEDIUM
    entities_mentioned: List[str] = Field(default_factory=list)
    key_phrases: List[str] = Field(default_factory=list)
    
    # Reference metadata for traceability
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ProcessingState(BaseModel):
    """Enhanced state with comprehensive tracking for unified multi-document processing"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    documents: List[str] = Field(default_factory=list)  # List of all document paths
    unified_metadata: Dict[str, Any] = Field(default_factory=dict)  # Unified metadata across all documents
    
    # Document-level storage (for reference but processed as unified)
    document_raw_texts: Dict[str, str] = Field(default_factory=dict)  # Raw text for each document
    document_structured_texts: Dict[str, Dict[str, Any]] = Field(default_factory=dict)  # Structured text for each document
    document_clauses: Dict[str, List[str]] = Field(default_factory=dict)  # Clauses for each document
    
    # UNIFIED RESULTS - This is what matters for final output
    unified_raw_text: str = ""  # All documents combined as one legal framework
    unified_clauses: List[Dict[str, Any]] = Field(default_factory=list)  # All clauses from all documents with source tracking
    unified_entities: List[LegalEntity] = Field(default_factory=list)  # All entities deduplicated across documents
    unified_concepts: List[LegalConcept] = Field(default_factory=list)  # All concepts deduplicated across documents
    unified_rule_components: List[RuleComponent] = Field(default_factory=list)  # All rule components across documents
    unified_enhanced_atomic_rules: List[EnhancedAtomicRule] = Field(default_factory=list)  # All enhanced rules
    unified_ontology_triples: List[Dict[str, str]] = Field(default_factory=list)  # Unified ontology
    unified_decision_rules: List[Dict[str, Any]] = Field(default_factory=list)  # Unified decision tables
    
    # Processing tracking
    current_agent: str = "document_processor"
    processing_steps: List[str] = Field(default_factory=list)
    error_messages: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    quality_metrics: Dict[str, float] = Field(default_factory=dict)
    
    # NLP analysis results (unified)
    embeddings_cache: Dict[str, List[float]] = Field(default_factory=dict)
    
    # FINAL UNIFIED OUTPUT - Single output for all documents
    final_unified_rules_output: List[Dict[str, Any]] = Field(default_factory=list)
    final_unified_decision_tables: Dict[str, Any] = Field(default_factory=dict)
    
    # Unified metadata for final outputs
    unified_output_metadata: Dict[str, Any] = Field(default_factory=dict)

# ============================================================================
# UTILITY FUNCTIONS WITH OPENAI INTEGRATION
# ============================================================================

def safe_float_conversion(value: Any, default: float = 0.0) -> float:
    """Safely convert any value to float"""
    if isinstance(value, (int, float)):
        return float(value)
    elif isinstance(value, str):
        complexity_mapping = {
            "high": 0.9, "medium": 0.6, "low": 0.3,
            "simple": 0.2, "complex": 0.8, "moderate": 0.5
        }
        normalized_value = value.lower().strip()
        if normalized_value in complexity_mapping:
            return complexity_mapping[normalized_value]
        try:
            return float(value)
        except ValueError:
            logger.warning(f"Could not convert '{value}' to float, using default {default}")
            return default
    else:
        return default

def get_complexity_level(score: float) -> ComplexityLevel:
    """Convert numeric complexity score to level"""
    if score >= 0.7:
        return ComplexityLevel.HIGH
    elif score >= 0.4:
        return ComplexityLevel.MEDIUM
    else:
        return ComplexityLevel.LOW

async def get_openai_completion(prompt: str, system_message: str = None) -> str:
    """Enhanced OpenAI completion with error handling"""
    try:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"Error: {str(e)}"

async def get_embedding(text: str) -> List[float]:
    """Get embedding using text-embedding-3-large with caching"""
    try:
        response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        logger.error(f"Embedding error: {e}")
        return []

def extract_pdf_text_advanced(pdf_path: Path) -> Dict[str, Any]:
    """Advanced PDF text extraction using latest PyMuPDF with structure preservation"""
    try:
        doc = pymupdf.open(pdf_path)
        extracted_data = {
            "raw_text": "",
            "structured_content": [],
            "metadata": {},
            "page_count": len(doc)
        }
        
        for page_num, page in enumerate(doc):
            page_data = {
                "page_number": page_num + 1,
                "text": page.get_text(),
                "blocks": [],
                "tables": []
            }
            
            # Extract structured content blocks
            blocks = page.get_text("dict")
            for block in blocks.get("blocks", []):
                if "lines" in block:
                    block_text = ""
                    for line in block["lines"]:
                        for span in line.get("spans", []):
                            block_text += span.get("text", "")
                    
                    if block_text.strip():
                        page_data["blocks"].append({
                            "text": block_text.strip(),
                            "bbox": block.get("bbox", []),
                            "font_info": line.get("spans", [{}])[0] if line.get("spans") else {}
                        })
            
            # Extract tables if available
            try:
                tables = page.find_tables()
                for table in tables:
                    page_data["tables"].append({
                        "data": table.extract(),
                        "bbox": table.bbox
                    })
            except:
                pass  # Tables might not be available
            
            extracted_data["raw_text"] += page_data["text"] + "\n"
            extracted_data["structured_content"].append(page_data)
        
        # Extract document metadata
        extracted_data["metadata"] = doc.metadata or {}
        doc.close()
        
        return extracted_data
        
    except Exception as e:
        logger.error(f"PDF extraction error for {pdf_path}: {e}")
        return {"raw_text": "", "structured_content": [], "metadata": {}, "page_count": 0}

def load_metadata() -> Dict[str, Any]:
    """Load legislation metadata from JSON file"""
    try:
        with open(METADATA_FILE, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Metadata loading error: {e}")
        return {}

def convert_numpy_types(obj):
    """Convert numpy types to native Python types for serialization"""
    if hasattr(obj, 'item'):  # numpy scalar
        return obj.item()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def safe_json_serialize(obj):
    """Safely serialize object to JSON with comprehensive error handling and dynamic fixing"""
    
    def clean_for_json(item):
        """Recursively clean object for JSON serialization"""
        if isinstance(item, dict):
            cleaned = {}
            for k, v in item.items():
                try:
                    # Ensure key is string
                    clean_key = str(k) if k is not None else "null_key"
                    cleaned[clean_key] = clean_for_json(v)
                except Exception as e:
                    logger.warning(f"Error cleaning dict key {k}: {e}")
                    cleaned[f"error_key_{abs(hash(str(k)))}"] = str(v)
            return cleaned
        elif isinstance(item, list):
            cleaned = []
            for i, v in enumerate(item):
                try:
                    cleaned.append(clean_for_json(v))
                except Exception as e:
                    logger.warning(f"Error cleaning list item {i}: {e}")
                    cleaned.append(str(v))
            return cleaned
        elif isinstance(item, tuple):
            try:
                return [clean_for_json(v) for v in item]
            except:
                return [str(v) for v in item]
        elif isinstance(item, set):
            try:
                return [clean_for_json(v) for v in item]
            except:
                return [str(v) for v in item]
        elif hasattr(item, '__dict__'):
            # Handle objects with __dict__ (like Pydantic models)
            try:
                if hasattr(item, 'model_dump'):
                    return clean_for_json(item.model_dump())
                elif hasattr(item, 'dict'):
                    return clean_for_json(item.dict())
                else:
                    return clean_for_json(item.__dict__)
            except:
                return str(item)
        elif isinstance(item, (int, float, str, bool)) or item is None:
            return item
        elif hasattr(item, 'value'):  # Handle enums
            return item.value
        else:
            # Convert any other type to string
            try:
                return str(item)
            except:
                return f"<object_type_{type(item).__name__}>"
    
    # Multiple attempts with increasing fallback levels
    attempts = [
        # Attempt 1: Try with numpy conversion
        lambda: json.dumps(obj, default=convert_numpy_types, ensure_ascii=False, indent=2),
        
        # Attempt 2: Try with cleaning
        lambda: json.dumps(clean_for_json(obj), ensure_ascii=False, indent=2),
        
        # Attempt 3: Try with string conversion and cleaning
        lambda: json.dumps(clean_for_json(convert_numpy_types(obj)), ensure_ascii=False, indent=2),
        
        # Attempt 4: Force string conversion
        lambda: json.dumps({"data": str(obj), "type": "fallback_string"}, ensure_ascii=False, indent=2),
        
        # Attempt 5: Minimal fallback
        lambda: '{"error": "serialization_failed", "data_type": "' + str(type(obj).__name__) + '"}'
    ]
    
    for i, attempt in enumerate(attempts, 1):
        try:
            result = attempt()
            if i > 1:
                logger.warning(f"JSON serialization succeeded on attempt {i}")
            return result
        except Exception as e:
            logger.warning(f"JSON serialization attempt {i} failed: {e}")
            continue
    
    # Final emergency fallback
    return '{"error": "all_serialization_attempts_failed"}'

# ============================================================================
# ENHANCED REACT AGENT BASE CLASS
# ============================================================================

class EnhancedReactAgent:
    """Enhanced ReAct agent with direct OpenAI integration and advanced NLP"""
    
    def __init__(self, name: str, role: str, tools: List[str] = None):
        self.name = name
        self.role = role
        self.tools = tools or []
        self.memory = []
        
        # Initialize text processing components
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
    async def think(self, observation: str, task: str, context: str = "") -> str:
        """Enhanced chain of thought reasoning"""
        thinking_prompt = f"""
        You are {self.role}, an expert agent in a multi-agent legal document processing system.
        
        TASK: {task}
        OBSERVATION: {observation}
        CONTEXT: {context}
        
        Think step by step about this task using chain of thought reasoning:
        1. What do I need to understand from this observation?
        2. What are the key legal elements relevant to my specialized role?
        3. What actions should I take to complete this task effectively?
        4. How does this relate to other components in the legal analysis pipeline?
        5. What potential issues or edge cases should I consider?
        
        Focus specifically on concepts related to data transfer, access, and entitlements.
        
        Provide your detailed reasoning and analysis:
        """
        
        try:
            thought = await get_openai_completion(
                thinking_prompt,
                f"You are an expert legal analyst specializing in {self.role} with focus on data protection, transfer, access, and entitlements."
            )
            
            # Store in memory for context
            self.memory.append({
                "type": "thinking",
                "content": thought,
                "timestamp": datetime.now().isoformat()
            })
            
            return thought
            
        except Exception as e:
            logger.error(f"Thinking error for {self.name}: {e}")
            return f"Unable to process reasoning: {str(e)}"
    
    async def act(self, thought: str, task: str, data: Any) -> Any:
        """Enhanced action execution - to be implemented by specific agents"""
        raise NotImplementedError("Each agent must implement its own act method")

# ============================================================================
# ENHANCED SPECIALIZED AGENTS
# ============================================================================

class AdvancedDocumentProcessorAgent(EnhancedReactAgent):
    """Enhanced document processor with advanced text analysis for all PDFs"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Document Processor",
            role="advanced document text extraction and preprocessing specialist for multiple documents",
            tools=["PyMuPDF", "LangChain TextSplitter", "structure detection", "metadata extraction"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced document processing treating all PDFs as unified legal framework"""
        logger.info(f"Advanced Document Processor: Processing {len(state.documents)} documents as unified legal framework")
        
        try:
            unified_content_parts = []
            document_hierarchy = {}
            
            for i, doc_path in enumerate(state.documents):
                logger.info(f"Processing document {i+1}/{len(state.documents)}: {doc_path}")
                
                # Extract text with structure preservation
                pdf_data = extract_pdf_text_advanced(Path(doc_path))
                
                if not pdf_data["raw_text"]:
                    error_msg = f"Failed to extract text from PDF: {doc_path}"
                    state.error_messages.append(error_msg)
                    continue
                
                # Store individual document data for reference
                state.document_raw_texts[doc_path] = pdf_data["raw_text"]
                state.document_structured_texts[doc_path] = pdf_data
                
                # Add to unified content with document context
                document_name = Path(doc_path).stem
                unified_section = f"\n\n=== LEGAL DOCUMENT SECTION: {document_name.upper()} ===\n"
                unified_section += f"Source: {doc_path}\n"
                unified_section += f"Document Order: {i+1} of {len(state.documents)}\n"
                unified_section += "=" * 60 + "\n"
                unified_section += pdf_data["raw_text"]
                unified_section += "\n" + "=" * 60 + "\n"
                
                unified_content_parts.append(unified_section)
                
                # Perform advanced text preprocessing
                preprocessing_result = await self._advanced_preprocessing(pdf_data["raw_text"], doc_path)
                state.document_structured_texts[doc_path]["preprocessed"] = preprocessing_result
                
                # Extract document structure using LLM
                structure_analysis = await self._analyze_document_structure(pdf_data["raw_text"], doc_path, i+1, len(state.documents))
                state.document_structured_texts[doc_path]["structure_analysis"] = structure_analysis
                
                # Build document hierarchy for unified understanding
                document_hierarchy[doc_path] = {
                    "order": i+1,
                    "name": document_name,
                    "structure": structure_analysis,
                    "preprocessing": preprocessing_result
                }
            
            # Create unified legal framework text
            state.unified_raw_text = "\n".join(unified_content_parts)
            
            # Create unified framework analysis
            unified_framework_analysis = await self._analyze_unified_framework(document_hierarchy, state.unified_raw_text)
            
            # Store unified metadata
            state.unified_metadata = {
                "total_documents": len(state.documents),
                "document_hierarchy": document_hierarchy,
                "unified_framework_analysis": unified_framework_analysis,
                "total_pages": sum(data.get("page_count", 0) for data in state.document_structured_texts.values()),
                "total_characters": len(state.unified_raw_text),
                "processing_approach": "unified_legal_framework"
            }
            
            state.processing_steps.append("Advanced document processing completed - All documents processed as unified legal framework")
            state.current_agent = "intelligent_segmentation"
            
            logger.info(f"Advanced Document Processor: Unified framework created with {len(state.unified_raw_text)} total characters")
            return state
            
        except Exception as e:
            error_msg = f"Document processing error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _advanced_preprocessing(self, text: str, doc_path: str) -> Dict[str, Any]:
        """Advanced text preprocessing with LLM assistance"""
        
        preprocessing_prompt = f"""
        You are an expert in legal document preprocessing. Clean and enhance this legal text which is part of a unified legal framework:
        
        DOCUMENT: {Path(doc_path).stem}
        TEXT TO PREPROCESS:
        {text[:2000]}...
        
        This document is part of a larger legal framework. Perform these preprocessing tasks:
        1. Remove artifacts (page numbers, headers, footers, watermarks)
        2. Fix formatting issues (line breaks, spacing, hyphenation)
        3. Standardize legal references (expand abbreviations like "Art." to "Article")
        4. Identify and preserve document structure markers
        5. Correct obvious OCR errors
        6. Standardize terminology (no abbreviations in key legal terms)
        7. Identify how this document relates to others in the framework
        
        Focus on preserving content related to data transfer, access, and entitlements.
        
        Return JSON with:
        {{
            "cleaned_text": "preprocessed text with improvements",
            "removed_artifacts": ["list of removed elements"],
            "expanded_abbreviations": {{"Art.": "Article", "Sec.": "Section"}},
            "structure_markers": ["Chapter 1", "Article 5", "Section 2.1"],
            "corrections_made": ["list of corrections"],
            "framework_position": "how this document fits in the larger framework",
            "related_concepts": ["data transfer", "access rights", "entitlements"]
        }}
        """
        
        response = await get_openai_completion(
            preprocessing_prompt,
            "You are an expert legal document preprocessing specialist with knowledge of data protection legal formatting and terminology standards."
        )
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            logger.error("Failed to parse preprocessing response")
            # Use safe serialization for fallback
            fallback_result = {
                "cleaned_text": text, 
                "error": "Preprocessing parsing failed",
                "framework_position": f"Document from {doc_path}",
                "related_concepts": ["data_protection"]
            }
            return fallback_result
    
    async def _analyze_document_structure(self, text: str, doc_path: str, doc_order: int, total_docs: int) -> Dict[str, Any]:
        """Analyze document structure as part of unified framework"""
        
        structure_prompt = f"""
        Analyze the structure of this legal document as part of a unified legal framework:
        
        DOCUMENT: {Path(doc_path).stem} (Document {doc_order} of {total_docs})
        DOCUMENT TEXT:
        {text[:3000]}...
        
        This is document {doc_order} of {total_docs} in a unified legal framework. Identify:
        1. Document title and type
        2. Major sections (Chapters, Parts, Titles)
        3. Articles and their numbering
        4. Sections and subsections
        5. Paragraphs and subparagraphs
        6. Cross-references between sections
        7. Definitions sections
        8. How this document relates to the overall framework
        9. References to other documents in the framework
        
        Focus on sections related to data transfer, access, and entitlements.
        
        Return JSON with hierarchical structure:
        {{
            "document_type": "regulation|directive|law|statute|chapter|article",
            "title": "document title",
            "framework_role": "how this fits in the unified framework",
            "document_order": {doc_order},
            "hierarchy": {{
                "chapters": [
                    {{
                        "number": "1",
                        "title": "General Provisions",
                        "articles": [
                            {{
                                "number": "1",
                                "title": "Article title",
                                "sections": ["section content"]
                            }}
                        ]
                    }}
                ]
            }},
            "cross_references": ["Article 5 references Article 3"],
            "definitions": {{"term": "definition"}},
            "framework_connections": ["connections to other documents"],
            "data_protection_focus": ["transfer", "access", "entitlement"]
        }}
        """
        
        response = await get_openai_completion(
            structure_prompt,
            "You are an expert in legal document structure analysis with deep knowledge of legislative drafting conventions and data protection law frameworks."
        )
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            logger.error("Failed to parse structure analysis response")
            # Use safe fallback
            fallback_result = {
                "error": "Structure analysis parsing failed",
                "document_order": doc_order,
                "framework_role": f"Document {doc_order} of unified framework",
                "data_protection_focus": ["data_protection"]
            }
            return fallback_result
    
    async def _analyze_unified_framework(self, document_hierarchy: Dict[str, Any], unified_text: str) -> Dict[str, Any]:
        """Analyze how all documents work together as a unified legal framework"""
        
        framework_prompt = f"""
        Analyze this unified legal framework consisting of {len(document_hierarchy)} related documents:
        
        DOCUMENT HIERARCHY:
        {safe_json_serialize(document_hierarchy)[:2000]}...
        
        UNIFIED FRAMEWORK TEXT (first part):
        {unified_text[:3000]}...
        
        Analyze how these documents work together as a cohesive legal framework:
        1. Overall framework structure and organization
        2. How documents relate to each other (hierarchy, dependencies)
        3. Common themes and concepts across documents
        4. Cross-document references and relationships
        5. Unified approach to data transfer, access, and entitlements
        6. Consistency and complementarity between documents
        7. Framework-level obligations and rights
        
        Return JSON:
        {{
            "framework_type": "comprehensive_data_protection_framework",
            "overall_structure": "description of how documents fit together",
            "document_relationships": {{
                "hierarchical": ["doc1 -> doc2"],
                "complementary": ["doc1 complements doc2"],
                "cross_references": ["doc1 references doc2"]
            }},
            "unified_themes": ["data transfer", "access rights", "entitlements"],
            "framework_coherence_score": 0.9,
            "integration_analysis": "how well documents integrate",
            "unified_data_protection_approach": "comprehensive approach across all documents"
        }}
        """
        
        response = await get_openai_completion(
            framework_prompt,
            "You are an expert in legal framework analysis with deep understanding of how multiple legal documents work together as a unified system."
        )
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            logger.error("Failed to parse unified framework analysis")
            # Safe fallback
            fallback_result = {
                "framework_type": "unified_legal_framework",
                "overall_structure": f"Framework of {len(document_hierarchy)} related documents",
                "unified_themes": ["data_protection", "compliance"],
                "framework_coherence_score": 0.8,
                "error": "Framework analysis parsing failed"
            }
            return fallback_result

class IntelligentSegmentationAgent(EnhancedReactAgent):
    """Enhanced segmentation agent with advanced clause identification"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Segmentation Agent",
            role="advanced legal text segmentation and atomic rule extraction specialist",
            tools=["semantic segmentation", "logical decomposition", "legal clause analysis", "dependency parsing"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced segmentation treating all documents as unified legal framework"""
        logger.info("Intelligent Segmentation: Creating atomic legal statements from unified legal framework")
        
        try:
            # Work with unified text from all documents
            unified_text = state.unified_raw_text
            unified_framework_analysis = state.unified_metadata.get("unified_framework_analysis", {})
            
            if not unified_text:
                state.error_messages.append("No unified text available for segmentation")
                return state
            
            logger.info(f"Segmenting unified framework with {len(unified_text)} characters")
            
            # Perform intelligent segmentation on unified framework
            segmentation_result = await self._intelligent_unified_segmentation(
                unified_text, 
                unified_framework_analysis,
                state.unified_metadata.get("document_hierarchy", {})
            )
            
            # Extract atomic clauses with enhanced analysis
            atomic_clauses = await self._extract_atomic_clauses(segmentation_result)
            
            # Perform semantic role labeling
            enhanced_clauses = await self._semantic_role_labeling(atomic_clauses)
            
            # Store unified results
            state.unified_clauses = enhanced_clauses
            state.unified_metadata["unified_segmentation_analysis"] = segmentation_result
            state.unified_metadata["total_atomic_statements"] = len(enhanced_clauses)
            
            # Also store by document for reference (but processing is unified)
            for clause in enhanced_clauses:
                source_doc = clause.get("source_document", "unknown")
                if source_doc not in state.document_clauses:
                    state.document_clauses[source_doc] = []
                state.document_clauses[source_doc].append(clause["text"])
            
            state.processing_steps.append("Intelligent segmentation completed for unified legal framework")
            state.current_agent = "comprehensive_entity_extraction"
            
            logger.info(f"Intelligent Segmentation: Identified {len(enhanced_clauses)} atomic statements from unified framework")
            return state
            
        except Exception as e:
            error_msg = f"Segmentation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _intelligent_unified_segmentation(self, unified_text: str, framework_analysis: Dict[str, Any], document_hierarchy: Dict[str, Any]) -> Dict[str, Any]:
        """Intelligent text segmentation treating all documents as unified framework"""
        
        segmentation_prompt = f"""
        You are an expert in legal text segmentation. Break this UNIFIED LEGAL FRAMEWORK into the smallest possible logical statements while preserving legal meaning.
        
        UNIFIED FRAMEWORK TEXT:
        {unified_text[:8000]}...
        
        FRAMEWORK ANALYSIS:
        {safe_json_serialize(framework_analysis)[:2000]}...
        
        DOCUMENT HIERARCHY:
        {safe_json_serialize(document_hierarchy)[:2000]}...
        
        This is a UNIFIED LEGAL FRAMEWORK consisting of multiple related documents that work together as articles/chapters of a comprehensive legal system.
        
        Segmentation guidelines:
        1. Treat all documents as parts of ONE unified legal framework
        2. Each atomic statement should contain exactly one legal rule, obligation, right, or prohibition
        3. Preserve cross-document relationships and references
        4. Maintain references to source articles/sections across all documents
        5. Handle compound sentences by breaking them at logical conjunctions
        6. Preserve conditional statements (if-then relationships) across the framework
        7. Keep exception clauses with their main rules when they form a logical unit
        8. Track which document/section each statement comes from
        
        Focus EXCLUSIVELY on statements related to:
        - Data transfer between entities or jurisdictions (across all documents)
        - Data access rights and permissions (unified approach)
        - Data entitlements and authorization (framework-wide)
        - Controller and processor obligations for data operations (comprehensive)
        - Third country data transfer requirements (complete framework)
        - Data subject rights regarding access and transfer (unified rights)
        
        Return JSON:
        {{
            "unified_atomic_statements": [
                {{
                    "id": "unified_stmt_001",
                    "text": "complete atomic legal statement",
                    "source_reference": "Document 1, Article 5, Section 1",
                    "source_document": "path/to/document1.pdf",
                    "framework_position": "how this fits in the unified framework",
                    "page_number": 15,
                    "section_title": "Data Transfer Provisions",
                    "document_title": "Data Protection Regulation - Chapter 1",
                    "statement_type": "obligation|right|prohibition|permission|condition",
                    "logical_structure": "simple|conditional|compound|exception",
                    "dependencies": ["other unified statement IDs this depends on"],
                    "cross_document_references": ["references to other documents in framework"],
                    "legal_significance": 0.85,
                    "data_operation_focus": "transfer|access|entitlement",
                    "framework_coherence": "how this relates to the overall framework"
                }}
            ],
            "unified_segmentation_summary": {{
                "total_statements": 0,
                "statements_by_document": {{"doc1": 5, "doc2": 3}},
                "statement_types": {{"obligation": 0, "right": 0}},
                "average_statement_length": 0,
                "data_operation_statements": 0,
                "framework_coverage": "comprehensive|partial|focused",
                "cross_document_connections": 0
            }}
        }}
        """
        
        response = await get_openai_completion(
            segmentation_prompt,
            "You are a legal text segmentation expert specializing in unified legal frameworks, data protection law, data transfer, access, and entitlements atomic rule extraction."
        )
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            logger.error("Failed to parse unified segmentation response")
            # Safe fallback with error handling
            fallback_result = {
                "unified_atomic_statements": [],
                "error": "Segmentation parsing failed",
                "unified_segmentation_summary": {
                    "total_statements": 0,
                    "framework_coverage": "error"
                }
            }
            return fallback_result
    
    async def _extract_atomic_clauses(self, segmentation_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract and enhance atomic clauses from unified segmentation"""
        unified_atomic_statements = segmentation_data.get("unified_atomic_statements", [])
        
        enhanced_clauses = []
        for stmt in unified_atomic_statements:
            # Perform additional analysis on each statement
            analysis = await self._analyze_clause_complexity(stmt["text"])
            
            # Safe conversion of legal_significance
            legal_sig = safe_float_conversion(stmt.get("legal_significance", 0.8))
            
            enhanced_clause = {
                **stmt,
                "complexity_analysis": analysis,
                "word_count": len(stmt["text"].split()),
                "character_count": len(stmt["text"]),
                "sentence_count": len([s for s in stmt["text"].split('.') if s.strip()]),
                "legal_significance": legal_sig,
                "framework_context": "part_of_unified_legal_framework"
            }
            
            enhanced_clauses.append(enhanced_clause)
        
        return enhanced_clauses
    
    async def _analyze_clause_complexity(self, clause_text: str) -> Dict[str, Any]:
        """Analyze the complexity of a legal clause"""
        
        complexity_prompt = f"""
        Analyze the complexity and legal significance of this legal clause:
        
        CLAUSE: {clause_text}
        
        Assess:
        1. Syntactic complexity (sentence structure, nested clauses)
        2. Legal complexity (number of legal concepts, conditions, exceptions)
        3. Semantic ambiguity (potential for multiple interpretations)
        4. Implementation difficulty (how hard to operationalize)
        5. Enforcement clarity (how clear the enforcement mechanism is)
        
        Return JSON with NUMERIC scores (0.0 to 1.0) and descriptive levels:
        {{
            "complexity_score": 0.8,
            "syntactic_complexity": "high",
            "syntactic_score": 0.75,
            "legal_complexity": "medium",
            "legal_score": 0.6,
            "semantic_ambiguity": "low",
            "ambiguity_score": 0.3,
            "implementation_difficulty": "medium",
            "implementation_score": 0.5,
            "enforcement_clarity": "clear",
            "enforcement_score": 0.9,
            "key_challenges": ["list of implementation challenges"],
            "clarification_needed": ["areas needing clarification"]
        }}
        """
        
        response = await get_openai_completion(
            complexity_prompt,
            "You are a legal complexity analysis expert with experience in data protection regulatory implementation."
        )
        
        try:
            result = json.loads(response)
            # Ensure all scores are properly converted to floats
            for key in ["complexity_score", "syntactic_score", "legal_score", "ambiguity_score", "implementation_score", "enforcement_score"]:
                if key in result:
                    result[key] = safe_float_conversion(result[key], 0.5)
            return result
        except json.JSONDecodeError:
            return {
                "complexity_score": 0.5,
                "syntactic_complexity": "medium",
                "syntactic_score": 0.5,
                "error": "Complexity analysis failed"
            }
    
    async def _semantic_role_labeling(self, clauses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Enhanced semantic role labeling"""
        
        enhanced_clauses = []
        for clause in clauses:
            role_analysis = await self._analyze_semantic_roles(clause["text"])
            clause["semantic_roles"] = role_analysis
            enhanced_clauses.append(clause)
        
        return enhanced_clauses
    
    async def _analyze_semantic_roles(self, text: str) -> Dict[str, str]:
        """Analyze semantic roles in legal text"""
        
        role_prompt = f"""
        Identify semantic roles in this legal statement:
        
        STATEMENT: {text}
        
        Identify these semantic roles:
        - AGENT: Who/what performs the action (Controller, Processor, etc.)
        - ACTION: The main legal action or requirement (transfer, access, entitlement)
        - PATIENT: Who/what is affected by the action (Data Subject, data)
        - CONDITION: Under what circumstances this applies
        - EXCEPTION: Any exceptions to the rule
        - MANNER: How the action should be performed
        - PURPOSE: Why the action is required
        - CONSEQUENCE: What happens if rule is followed/violated
        
        Return JSON with role assignments:
        {{
            "agent": "identified agent",
            "action": "main action",
            "patient": "affected party",
            "condition": "conditions",
            "exception": "exceptions",
            "manner": "how to perform",
            "purpose": "why required",
            "consequence": "outcomes"
        }}
        """
        
        response = await get_openai_completion(
            role_prompt,
            "You are an expert in semantic role labeling for legal texts with deep understanding of data protection legal argument structure."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"error": "Semantic role analysis failed"}

class ComprehensiveEntityExtractionAgent(EnhancedReactAgent):
    """Enhanced entity extraction with advanced NLP"""
    
    def __init__(self):
        super().__init__(
            name="Comprehensive Entity Extraction Agent",
            role="advanced legal entity identification and relationship mapping specialist",
            tools=["NER", "entity linking", "relationship extraction", "coreference resolution"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Comprehensive entity extraction from unified legal framework"""
        logger.info("Comprehensive Entity Extraction: Identifying legal entities from unified legal framework")
        
        try:
            all_entities = []
            
            # Process unified clauses from the entire legal framework
            unified_clauses = state.unified_clauses
            
            if not unified_clauses:
                state.error_messages.append("No unified clauses available for entity extraction")
                return state
            
            logger.info(f"Extracting entities from {len(unified_clauses)} unified clauses")
            
            # Process clauses for entity extraction
            for clause in unified_clauses:
                entities = await self._extract_entities_from_clause(clause)
                all_entities.extend(entities)
            
            # Deduplicate and enhance entities across the entire framework
            unique_entities = await self._deduplicate_entities(all_entities)
            enhanced_entities = await self._enhance_entities(unique_entities)
            
            # Store unified results
            state.unified_entities = enhanced_entities
            state.unified_metadata["entity_extraction_summary"] = {
                "total_entities_found": len(all_entities),
                "unique_entities_after_deduplication": len(enhanced_entities),
                "entities_by_type": {entity_type.value: len([e for e in enhanced_entities if e.type == entity_type]) 
                                   for entity_type in EntityType},
                "framework_coverage": "comprehensive_unified_analysis"
            }
            
            state.processing_steps.append("Comprehensive entity extraction completed for unified legal framework")
            state.current_agent = "advanced_concept_extraction"
            
            logger.info(f"Comprehensive Entity Extraction: Found {len(enhanced_entities)} unique entities across unified framework")
            return state
            
        except Exception as e:
            error_msg = f"Entity extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_entities_from_clause(self, clause: Dict[str, Any]) -> List[LegalEntity]:
        """Extract entities from a single clause with enhanced analysis"""
        
        entity_prompt = f"""
        You are an expert in legal entity extraction. Identify all legal entities in this clause with precise classification.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        
        Identify entities according to these MANDATORY categories:
        
        DATA PROTECTION ROLES:
        - Controller: Entity determining purposes and means of processing
        - Processor: Entity processing data on behalf of controller
        - JointController: Multiple controllers jointly determining purposes/means
        - DataSubject: Individual whose personal data is processed
        
        JURISDICTIONAL ENTITIES:
        - ThirdCountry: Country outside the jurisdiction
        - SupervisingAuthority: Regulatory authority overseeing compliance
        
        For each entity found, provide:
        1. Exact textual mention
        2. Precise classification from above categories
        3. Detailed role description
        4. Context and surrounding text
        5. Confidence assessment (as a number between 0.0 and 1.0)
        6. Attributes (size, sector, location if mentioned)
        
        Focus on entities related to data transfer, access, and entitlements.
        
        Return JSON:
        {{
            "entities": [
                {{
                    "name": "exact entity mention",
                    "type": "Controller",
                    "description": "detailed role and function description",
                    "context": "surrounding text context",
                    "attributes": {{"sector": "healthcare", "size": "large"}},
                    "relationships": ["related to other entity"],
                    "confidence": 0.95
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            entity_prompt,
            "You are a legal entity extraction expert with comprehensive knowledge of data protection law terminology and organizational structures."
        )
        
        try:
            entity_data = json.loads(response)
            entities = []
            for entity_dict in entity_data.get("entities", []):
                try:
                    # Ensure type is valid
                    entity_type = entity_dict.get("type", "Controller")
                    if hasattr(EntityType, entity_type.upper().replace(" ", "_")):
                        entity_dict["type"] = getattr(EntityType, entity_type.upper().replace(" ", "_"))
                    else:
                        entity_dict["type"] = EntityType.CONTROLLER  # Default fallback
                    
                    # Ensure confidence is a float
                    confidence = safe_float_conversion(entity_dict.get("confidence", 0.8))
                    entity_dict["confidence"] = confidence
                    
                    # Ensure all other fields are properly typed
                    entity_dict["name"] = str(entity_dict.get("name", ""))
                    entity_dict["description"] = str(entity_dict.get("description", ""))
                    entity_dict["context"] = str(entity_dict.get("context", ""))
                    entity_dict["attributes"] = dict(entity_dict.get("attributes", {}))
                    entity_dict["relationships"] = list(entity_dict.get("relationships", []))
                    
                    entity = LegalEntity(**entity_dict)
                    entities.append(entity)
                except Exception as e:
                    logger.warning(f"Failed to create entity: {e}")
                    continue
            return entities
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Entity extraction parsing error: {e}")
            return []
    
    async def _deduplicate_entities(self, entities: List[LegalEntity]) -> List[LegalEntity]:
        """Deduplicate entities using semantic similarity"""
        
        if not entities:
            return []
        
        # Group entities by type first
        type_groups = defaultdict(list)
        for entity in entities:
            type_groups[entity.type].append(entity)
        
        unique_entities = []
        
        for entity_type, type_entities in type_groups.items():
            if len(type_entities) == 1:
                unique_entities.extend(type_entities)
                continue
            
            # Use embeddings to find duplicates
            entity_texts = [f"{e.name} {e.description}" for e in type_entities]
            embeddings = []
            
            for text in entity_texts:
                embedding = await get_embedding(text)
                if embedding:
                    embeddings.append(embedding)
                else:
                    embeddings.append([0.0] * 3072)  # text-embedding-3-large dimension
            
            if embeddings:
                similarity_matrix = cosine_similarity(embeddings)
                
                # Identify duplicates (similarity > 0.8)
                duplicates = set()
                for i in range(len(similarity_matrix)):
                    for j in range(i + 1, len(similarity_matrix)):
                        if similarity_matrix[i][j] > 0.8:
                            duplicates.add(j)
                
                # Keep only non-duplicate entities
                for i, entity in enumerate(type_entities):
                    if i not in duplicates:
                        unique_entities.append(entity)
            else:
                unique_entities.extend(type_entities)
        
        return unique_entities
    
    async def _enhance_entities(self, entities: List[LegalEntity]) -> List[LegalEntity]:
        """Enhance entities with additional analysis"""
        
        enhanced_entities = []
        
        for entity in entities:
            # Enhance with additional context analysis
            enhancement = await self._analyze_entity_context(entity)
            
            enhanced_entity = LegalEntity(
                name=entity.name,
                type=entity.type,
                description=entity.description,
                context=entity.context,
                attributes={**entity.attributes, **enhancement.get("attributes", {})},
                relationships=entity.relationships + enhancement.get("relationships", []),
                confidence=min(entity.confidence, safe_float_conversion(enhancement.get("confidence", 1.0)))
            )
            
            enhanced_entities.append(enhanced_entity)
        
        return enhanced_entities
    
    async def _analyze_entity_context(self, entity: LegalEntity) -> Dict[str, Any]:
        """Analyze entity context for enhancement"""
        
        context_prompt = f"""
        Analyze this legal entity for additional contextual information:
        
        ENTITY: {entity.name} ({entity.type})
        DESCRIPTION: {entity.description}
        CONTEXT: {entity.context}
        
        Identify:
        1. Sector or industry (if applicable)
        2. Jurisdiction or location references
        3. Size indicators (large, small, individual)
        4. Legal status (public, private, non-profit)
        5. Functional role beyond type classification
        6. Regulatory obligations specific to this entity
        7. Risk level (high, medium, low) for data protection
        
        Return JSON:
        {{
            "attributes": {{
                "sector": "healthcare|finance|tech|public|other",
                "jurisdiction": "EU|US|global|national",
                "size": "large|medium|small|individual",
                "legal_status": "public|private|non-profit",
                "functional_role": "specific function description",
                "risk_level": "high|medium|low"
            }},
            "relationships": ["additional relationship indicators"],
            "confidence": 0.9
        }}
        """
        
        response = await get_openai_completion(
            context_prompt,
            "You are an expert in legal entity context analysis with knowledge of organizational structures and regulatory frameworks."
        )
        
        try:
            result = json.loads(response)
            # Ensure confidence is properly converted
            if "confidence" in result:
                result["confidence"] = safe_float_conversion(result["confidence"], 1.0)
            return result
        except json.JSONDecodeError:
            return {"attributes": {}, "relationships": [], "confidence": 1.0}

class AdvancedConceptExtractionAgent(EnhancedReactAgent):
    """Enhanced concept extraction with semantic relationships"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Concept Extraction Agent",
            role="comprehensive legal concept identification and semantic relationship mapping specialist",
            tools=["concept taxonomy", "semantic analysis", "legal ontology", "relationship mapping"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Advanced concept extraction from unified legal framework"""
        logger.info("Advanced Concept Extraction: Identifying legal concepts from unified legal framework")
        
        try:
            all_concepts = []
            
            # Process unified clauses from the entire legal framework
            unified_clauses = state.unified_clauses
            
            if not unified_clauses:
                state.error_messages.append("No unified clauses available for concept extraction")
                return state
            
            logger.info(f"Extracting concepts from {len(unified_clauses)} unified clauses")
            
            # Process clauses for concept extraction
            for clause in unified_clauses:
                concepts = await self._extract_concepts_from_clause(clause)
                all_concepts.extend(concepts)
            
            # Deduplicate and enhance concepts across the entire framework
            unique_concepts = await self._deduplicate_concepts(all_concepts)
            
            # Store unified results
            state.unified_concepts = unique_concepts
            state.unified_metadata["concept_extraction_summary"] = {
                "total_concepts_found": len(all_concepts),
                "unique_concepts_after_deduplication": len(unique_concepts),
                "concepts_by_type": {concept_type.value: len([c for c in unique_concepts if c.type == concept_type]) 
                                   for concept_type in ConceptType},
                "framework_coverage": "comprehensive_unified_analysis"
            }
            
            state.processing_steps.append("Advanced concept extraction completed for unified legal framework")
            state.current_agent = "intelligent_rule_component_extraction"
            
            logger.info(f"Advanced Concept Extraction: Found {len(unique_concepts)} unique concepts across unified framework")
            return state
            
        except Exception as e:
            error_msg = f"Concept extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_concepts_from_clause(self, clause: Dict[str, Any]) -> List[LegalConcept]:
        """Extract concepts with enhanced classification"""
        
        concept_prompt = f"""
        You are an expert in legal concept extraction. Identify all data protection and privacy-related concepts in this clause.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        
        Identify concepts in these MANDATORY categories:
        
        DATA OPERATIONS:
        - DataTransfer: Moving data between entities/jurisdictions
        - DataAccess: Accessing or retrieving personal data
        - DataEntitlement: Rights to access or use data
        - Processing: Any operation on personal data (broad category)
        
        For each concept, provide:
        1. Precise classification from above categories
        2. Detailed description of how it applies
        3. Preconditions for the concept to apply
        4. Consequences or outcomes
        5. Semantic relationships to other concepts
        6. Confidence score (0.0 to 1.0)
        
        Focus EXCLUSIVELY on concepts related to data transfer, access, and entitlements.
        
        Return JSON:
        {{
            "concepts": [
                {{
                    "name": "specific concept name",
                    "type": "DataTransfer",
                    "description": "detailed description of application",
                    "context": "surrounding legal context",
                    "preconditions": ["conditions that must be met"],
                    "consequences": ["outcomes or results"],
                    "semantic_relationships": {{
                        "requires": ["concepts this requires"],
                        "enables": ["concepts this enables"],
                        "conflicts_with": ["conflicting concepts"]
                    }},
                    "confidence": 0.9
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            concept_prompt,
            "You are a legal concept extraction expert with comprehensive knowledge of data protection operations, legal bases, and safeguarding measures."
        )
        
        try:
            concept_data = json.loads(response)
            concepts = []
            for concept_dict in concept_data.get("concepts", []):
                try:
                    # Ensure type is valid
                    concept_type = concept_dict.get("type", "Processing")
                    if hasattr(ConceptType, concept_type.upper().replace(" ", "_")):
                        concept_dict["type"] = getattr(ConceptType, concept_type.upper().replace(" ", "_"))
                    else:
                        concept_dict["type"] = ConceptType.PROCESSING  # Default fallback
                    
                    # Ensure confidence is a float
                    confidence = safe_float_conversion(concept_dict.get("confidence", 0.8))
                    concept_dict["confidence"] = confidence
                    
                    # Ensure all other fields are properly typed
                    concept_dict["name"] = str(concept_dict.get("name", ""))
                    concept_dict["description"] = str(concept_dict.get("description", ""))
                    concept_dict["context"] = str(concept_dict.get("context", ""))
                    concept_dict["preconditions"] = list(concept_dict.get("preconditions", []))
                    concept_dict["consequences"] = list(concept_dict.get("consequences", []))
                    concept_dict["semantic_relationships"] = dict(concept_dict.get("semantic_relationships", {}))
                    
                    concept = LegalConcept(**concept_dict)
                    concepts.append(concept)
                except Exception as e:
                    logger.warning(f"Failed to create concept: {e}")
                    continue
            return concepts
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Concept extraction parsing error: {e}")
            return []
    
    async def _deduplicate_concepts(self, concepts: List[LegalConcept]) -> List[LegalConcept]:
        """Deduplicate concepts using semantic similarity"""
        
        if not concepts:
            return []
        
        # Group concepts by type first
        type_groups = defaultdict(list)
        for concept in concepts:
            type_groups[concept.type].append(concept)
        
        unique_concepts = []
        
        for concept_type, type_concepts in type_groups.items():
            if len(type_concepts) == 1:
                unique_concepts.extend(type_concepts)
                continue
            
            # Use embeddings to find duplicates
            concept_texts = [f"{c.name} {c.description}" for c in type_concepts]
            embeddings = []
            
            for text in concept_texts:
                embedding = await get_embedding(text)
                if embedding:
                    embeddings.append(embedding)
                else:
                    embeddings.append([0.0] * 3072)  # text-embedding-3-large dimension
            
            if embeddings:
                similarity_matrix = cosine_similarity(embeddings)
                
                # Identify duplicates (similarity > 0.8)
                duplicates = set()
                for i in range(len(similarity_matrix)):
                    for j in range(i + 1, len(similarity_matrix)):
                        if similarity_matrix[i][j] > 0.8:
                            duplicates.add(j)
                
                # Keep only non-duplicate concepts
                for i, concept in enumerate(type_concepts):
                    if i not in duplicates:
                        unique_concepts.append(concept)
            else:
                unique_concepts.extend(type_concepts)
        
        return unique_concepts

class IntelligentRuleComponentExtractionAgent(EnhancedReactAgent):
    """Enhanced rule component extraction with logical analysis"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Rule Component Extraction Agent",
            role="comprehensive legal rule component identification and logical structure analysis specialist",
            tools=["deontic logic", "rule decomposition", "logical operators", "enforcement analysis"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced rule component extraction with logical structure analysis for all documents"""
        logger.info("Intelligent Rule Component Extraction: Analyzing logical rule structures across all documents")
        
        try:
            all_rule_components = []
            all_enhanced_atomic_rules = []
            
            unified_clauses = state.unified_clauses
            
            if not unified_clauses:
                state.error_messages.append("No unified clauses available for rule component extraction")
                return state
            
            logger.info(f"Extracting rule components from {len(unified_clauses)} unified clauses")
            
            # Process clauses for rule component extraction
            for clause in unified_clauses:
                components = await self._extract_rule_components_from_clause(clause, state.unified_entities, state.unified_concepts)
                logical_structure = await self._analyze_logical_structure(clause, components)
                
                all_rule_components.extend(components)
                
                # Create enhanced atomic rule for this clause
                enhanced_rule = await self._create_enhanced_atomic_rule(
                    clause, state.unified_entities, state.unified_concepts, components, logical_structure
                )
                if enhanced_rule:
                    all_enhanced_atomic_rules.append(enhanced_rule)
            
            # Store aggregated results
            state.unified_rule_components = all_rule_components
            state.unified_enhanced_atomic_rules = all_enhanced_atomic_rules
            
            state.processing_steps.append("Intelligent rule component extraction completed for all documents")
            state.current_agent = "ontology_formalization"
            
            logger.info(f"Intelligent Rule Component Extraction: Found {len(all_rule_components)} rule components and {len(all_enhanced_atomic_rules)} enhanced atomic rules")
            return state
            
        except Exception as e:
            error_msg = f"Rule component extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_rule_components_from_clause(self, clause: Dict[str, Any], entities: List[LegalEntity], concepts: List[LegalConcept]) -> List[RuleComponent]:
        """Extract rule components with enhanced classification"""
        
        rule_component_prompt = f"""
        You are an expert in legal rule analysis and deontic logic. Extract all rule components from this legal clause.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        AVAILABLE ENTITIES: {[f"{e.name} ({e.type})" for e in entities[:10]]}
        AVAILABLE CONCEPTS: {[f"{c.name} ({c.type})" for c in concepts[:10]]}
        
        Identify rule components in these MANDATORY categories:
        
        DEONTIC COMPONENTS:
        - Restriction: Limitations on actions or behaviors
        - Condition: Circumstances that must be met
        - Obligation: Required actions or duties (MUST/SHALL)
        - Right: Entitlements or permissions of data subjects
        
        For each component:
        1. Identify the precise component type
        2. Describe what it requires/restricts/permits
        3. Specify which entities it applies to
        4. Identify the legal basis/source
        5. Determine enforcement mechanisms
        6. Identify logical operators (AND, OR, NOT, IF-THEN)
        7. Assess penalties for non-compliance
        8. Provide confidence score (0.0 to 1.0)
        
        Focus on components related to data transfer, access, and entitlements.
        
        Return JSON:
        {{
            "rule_components": [
                {{
                    "name": "descriptive component name",
                    "type": "Restriction",
                    "description": "what it requires/restricts/permits",
                    "applies_to": ["entity names from available entities"],
                    "legal_basis": "specific legal source/article",
                    "enforcement_mechanism": "how it is enforced",
                    "penalty": "consequences for violation",
                    "exceptions": ["specific exceptions"],
                    "logical_operator": "AND",
                    "confidence": 0.85
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            rule_component_prompt,
            "You are a legal rule analysis expert specializing in deontic logic, regulatory compliance, and enforcement mechanisms for data protection."
        )
        
        try:
            component_data = json.loads(response)
            components = []
            for comp_dict in component_data.get("rule_components", []):
                try:
                    # Ensure type is valid
                    comp_type = comp_dict.get("type", "Condition")
                    if hasattr(RuleComponentType, comp_type.upper()):
                        comp_dict["type"] = getattr(RuleComponentType, comp_type.upper())
                    else:
                        comp_dict["type"] = RuleComponentType.CONDITION  # Default fallback
                    
                    # Ensure logical_operator is valid
                    logical_op = comp_dict.get("logical_operator", "AND")
                    if hasattr(LogicalOperator, logical_op.upper()):
                        comp_dict["logical_operator"] = getattr(LogicalOperator, logical_op.upper())
                    else:
                        comp_dict["logical_operator"] = LogicalOperator.AND  # Default fallback
                    
                    # Ensure confidence is a float
                    confidence = safe_float_conversion(comp_dict.get("confidence", 0.8))
                    comp_dict["confidence"] = confidence
                    
                    # Ensure all other fields are properly typed
                    comp_dict["name"] = str(comp_dict.get("name", ""))
                    comp_dict["description"] = str(comp_dict.get("description", ""))
                    comp_dict["applies_to"] = list(comp_dict.get("applies_to", []))
                    comp_dict["legal_basis"] = str(comp_dict.get("legal_basis", ""))
                    comp_dict["enforcement_mechanism"] = str(comp_dict.get("enforcement_mechanism", ""))
                    comp_dict["penalty"] = str(comp_dict.get("penalty", ""))
                    comp_dict["exceptions"] = list(comp_dict.get("exceptions", []))
                    
                    component = RuleComponent(**comp_dict)
                    components.append(component)
                except Exception as e:
                    logger.warning(f"Failed to create rule component: {e}")
                    continue
            return components
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Rule component extraction parsing error: {e}")
            return []
    
    async def _analyze_logical_structure(self, clause: Dict[str, Any], components: List[RuleComponent]) -> Dict[str, Any]:
        """Analyze the logical structure of rule components"""
        
        logical_prompt = f"""
        Analyze the logical structure of this legal clause and its components:
        
        CLAUSE: {clause.get("text", "")}
        COMPONENTS: {[f"{c.name} ({c.type})" for c in components]}
        
        Analyze:
        1. Logical flow (sequential, conditional, parallel)
        2. Dependencies between components
        3. Boolean logic (AND, OR, NOT operations)
        4. Conditional statements (IF-THEN-ELSE)
        5. Exception handling logic
        6. Precedence and priority relationships
        
        Return JSON:
        {{
            "logical_structure": {{
                "type": "sequential|conditional|parallel|hybrid",
                "main_operator": "AND|OR|IF_THEN",
                "complexity_level": "simple|moderate|complex",
                "dependencies": [
                    {{
                        "component": "component name",
                        "depends_on": ["other components"],
                        "relationship": "prerequisite|conditional|parallel"
                    }}
                ],
                "conditional_logic": [
                    {{
                        "condition": "if condition",
                        "then_action": "required action",
                        "else_action": "alternative action"
                    }}
                ],
                "exception_handling": ["exception scenarios"],
                "precedence_order": ["ordered list of components by priority"]
            }}
        }}
        """
        
        response = await get_openai_completion(
            logical_prompt,
            "You are an expert in logical analysis of legal rules with deep understanding of Boolean logic and conditional reasoning."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"logical_structure": {"type": "simple", "complexity_level": "unknown"}}
    
    async def _create_enhanced_atomic_rule(self, clause: Dict[str, Any], entities: List[LegalEntity], 
                                          concepts: List[LegalConcept], rule_components: List[RuleComponent],
                                          logical_structure: Dict[str, Any]) -> Optional[EnhancedAtomicRule]:
        """Create enhanced atomic rule with comprehensive analysis and references"""
        
        try:
            clause_id = clause.get("id", f"clause_{abs(hash(clause.get('text', '')))}")
            
            # Get semantic roles
            semantic_roles = clause.get("semantic_roles", {})
            
            # Determine deontic type
            deontic_type = self._determine_deontic_type(clause["text"], rule_components)
            
            # Calculate complexity scores
            complexity_analysis = clause.get("complexity_analysis", {})
            complexity_score = safe_float_conversion(complexity_analysis.get("complexity_score", 0.5))
            complexity_level = get_complexity_level(complexity_score)
            
            # Extract key phrases and entities mentioned
            key_phrases = await self._extract_key_phrases(clause["text"])
            entities_mentioned = [e.name for e in entities if e.name.lower() in clause["text"].lower()]
            
            # Create citation with enhanced reference information
            citation = LegalCitation(
                document_id=clause.get("source_reference", "unknown"),
                article=self._extract_article_reference(clause.get("source_reference", "")),
                section=self._extract_section_reference(clause.get("source_reference", "")),
                authority_level=LegalAuthorityLevel.STATUTORY,
                jurisdiction=JurisdictionScope.NATIONAL
            )
            
            # Ensure all numeric values are Python native types
            confidence_val = safe_float_conversion(clause.get("legal_significance", 0.8))
            
            enhanced_rule = EnhancedAtomicRule(
                id=f"rule_{abs(hash(clause['text']))}",
                text=str(clause["text"]),
                entities=entities,
                concepts=concepts,
                rule_components=rule_components,
                semantic_roles=dict(semantic_roles),
                source_document=str(clause.get("source_document", "unknown")),
                citation=citation,
                confidence=confidence_val,
                legal_authority_level=LegalAuthorityLevel.STATUTORY,
                jurisdictional_scope=JurisdictionScope.NATIONAL,
                deontic_type=deontic_type,
                logical_structure=dict(logical_structure),
                complexity_score=complexity_score,
                complexity_level=complexity_level,
                entities_mentioned=[str(e) for e in entities_mentioned],
                key_phrases=[str(p) for p in key_phrases]
            )
            
            # Store additional reference information for decision tables
            enhanced_rule.metadata = {
                "page_number": clause.get("page_number"),
                "section_title": str(clause.get("section_title", "")),
                "document_title": str(clause.get("document_title", "")),
                "text_excerpt": clause["text"][:200] + "..." if len(clause["text"]) > 200 else str(clause["text"]),
                "source_document_path": clause.get("source_document", "unknown")
            }
            
            return enhanced_rule
            
        except Exception as e:
            logger.warning(f"Failed to create enhanced atomic rule for clause: {e}")
            return None
    
    def _determine_deontic_type(self, text: str, rule_components: List[RuleComponent]) -> DeonticType:
        """Determine the deontic type of a rule"""
        text_lower = text.lower()
        
        # Check for strong obligation indicators
        if any(word in text_lower for word in ["must", "shall", "required", "obligation"]):
            return DeonticType.OBLIGATORY
        
        # Check for prohibition indicators
        if any(word in text_lower for word in ["must not", "shall not", "prohibited", "forbidden"]):
            return DeonticType.FORBIDDEN
        
        # Check for permission indicators
        if any(word in text_lower for word in ["may", "can", "permitted", "allowed"]):
            return DeonticType.PERMISSIBLE
        
        # Check rule components for type hints
        obligation_components = [c for c in rule_components if c.type in [RuleComponentType.OBLIGATION, RuleComponentType.RESTRICTION]]
        if obligation_components:
            return DeonticType.OBLIGATORY
        
        permission_components = [c for c in rule_components if c.type in [RuleComponentType.RIGHT]]
        if permission_components:
            return DeonticType.PERMISSIBLE
        
        return DeonticType.OPTIONAL
    
    async def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases from text"""
        
        phrase_prompt = f"""
        Extract the most important legal phrases from this text:
        
        TEXT: {text}
        
        Identify key phrases that represent:
        1. Legal obligations and requirements
        2. Rights and entitlements
        3. Conditions and restrictions
        4. Enforcement mechanisms
        5. Penalties and consequences
        
        Focus on phrases related to data transfer, access, and entitlements.
        
        Return JSON:
        {{
            "key_phrases": ["phrase1", "phrase2", "phrase3"]
        }}
        """
        
        response = await get_openai_completion(
            phrase_prompt,
            "You are an expert in legal text analysis with expertise in identifying key legal phrases and terminology related to data protection."
        )
        
        try:
            result = json.loads(response)
            return result.get("key_phrases", [])
        except json.JSONDecodeError:
            # Fallback: extract noun phrases using simple regex
            phrases = re.findall(r'\b[A-Z][a-z]+(?:\s+[a-z]+)*\b', text)
            return phrases[:10]  # Return top 10 phrases
    
    def _extract_article_reference(self, reference: str) -> Optional[str]:
        """Extract article reference from citation"""
        article_match = re.search(r'Article\s+(\d+)', reference, re.IGNORECASE)
        return article_match.group(1) if article_match else None
    
    def _extract_section_reference(self, reference: str) -> Optional[str]:
        """Extract section reference from citation"""
        section_match = re.search(r'Section\s+(\d+(?:\.\d+)*)', reference, re.IGNORECASE)
        return section_match.group(1) if section_match else None

class AdvancedOntologyFormalizationAgent(EnhancedReactAgent):
    """Enhanced ontology creation with reasoning capabilities"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Ontology Formalization Agent",
            role="comprehensive formal ontology creation and reasoning specialist",
            tools=["OWL-DL", "RDFLib", "Owlready2", "ontology patterns", "semantic reasoning"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Create comprehensive formal ontology for unified legal framework"""
        logger.info("Advanced Ontology Formalization: Creating formal OWL-DL ontology for unified legal framework")
        
        try:
            # Create comprehensive ontology from unified data
            ontology_data = await self._create_comprehensive_unified_ontology(state)
            
            # Generate RDF triples
            rdf_triples = await self._generate_rdf_triples(ontology_data, state)
            
            # Create Owlready2 ontology
            owlready_ontology = await self._create_owlready_ontology(state)
            
            # Save ontologies
            ontology_files = await self._save_ontologies(rdf_triples, owlready_ontology)
            
            # Store unified results
            state.unified_ontology_triples = rdf_triples
            state.unified_metadata["unified_ontology_files"] = ontology_files
            state.unified_metadata["unified_ontology_data"] = ontology_data
            
            state.processing_steps.append("Advanced ontology formalization completed for unified legal framework")
            state.current_agent = "decision_table_generation"
            
            logger.info(f"Advanced Ontology Formalization: Created unified ontology with {len(rdf_triples)} triples")
            return state
            
        except Exception as e:
            error_msg = f"Ontology formalization error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _create_comprehensive_unified_ontology(self, state: ProcessingState) -> Dict[str, Any]:
        """Create comprehensive ontology structure for unified legal framework"""
        
        ontology_prompt = f"""
        Create a comprehensive OWL-DL ontology for this UNIFIED LEGAL FRAMEWORK from multiple related documents:
        
        UNIFIED ENHANCED ATOMIC RULES: {[rule.text[:100] + "..." for rule in state.unified_enhanced_atomic_rules[:15]]}
        UNIFIED ENTITIES: {[f"{e.name} ({e.type})" for e in state.unified_entities[:20]]}
        UNIFIED CONCEPTS: {[f"{c.name} ({c.type})" for c in state.unified_concepts[:20]]}
        UNIFIED RULE COMPONENTS: {[f"{rc.name} ({rc.type})" for rc in state.unified_rule_components[:20]]}
        
        FRAMEWORK CONTEXT: This is a UNIFIED LEGAL FRAMEWORK consisting of {len(state.documents)} related documents working together as a comprehensive legal system.
        
        Design a comprehensive ontology representing the ENTIRE UNIFIED FRAMEWORK with:
        
        1. CLASS HIERARCHY:
        - LegalEntity (Controller, Processor, JointController, DataSubject, ThirdCountry, SupervisingAuthority)
        - DataOperation (DataTransfer, DataAccess, DataEntitlement, Processing)
        - RuleComponent (Obligation, Right, Restriction, Condition)
        - LegalDocument (Regulation, Directive, etc.)
        - ProtectionMeasure (Encryption, Pseudonymisation, etc.)
        - LegalBasis (Consent, LegitimateInterest, etc.)
        
        2. OBJECT PROPERTIES:
        - hasObligation, hasRight, hasRestriction, hasCondition
        - appliesTo, governedBy, requiresCondition
        - transfersDataTo, accessesDataOf, processesDataFor
        - implementsMeasure, providesProtection
        - hasLegalBasis, requiresConsent
        - partOfUnifiedFramework, crossReferencesDocument
        
        3. DATA PROPERTIES:
        - hasDescription, hasLegalBasis, hasConfidence
        - hasComplexityScore, hasSeverity
        - hasJurisdiction, hasAuthority
        - frameworkPosition, documentOrder
        
        4. INDIVIDUALS:
        - Specific instances from unified extracted entities
        - Concrete examples of concepts from the framework
        
        5. AXIOMS AND CONSTRAINTS:
        - Domain and range restrictions
        - Cardinality constraints
        - Disjointness axioms
        - Equivalence relationships
        - Framework coherence constraints
        
        Focus on data transfer, access, and entitlements throughout the UNIFIED FRAMEWORK.
        
        Return JSON with complete ontology specification:
        {{
            "classes": [
                {{
                    "name": "Controller",
                    "parent": "LegalEntity",
                    "description": "Entity determining purposes and means in unified framework",
                    "properties": ["hasObligation", "transfersDataTo"]
                }}
            ],
            "object_properties": [
                {{
                    "name": "hasObligation",
                    "domain": "LegalEntity",
                    "range": "Obligation",
                    "description": "Entity has legal obligation in unified framework"
                }}
            ],
            "data_properties": [
                {{
                    "name": "hasDescription",
                    "domain": "Thing",
                    "range": "string",
                    "description": "Textual description"
                }}
            ],
            "individuals": [
                {{
                    "name": "UnifiedFrameworkController",
                    "type": "Controller",
                    "properties": {{"hasJurisdiction": "EU", "frameworkPosition": "central"}}
                }}
            ],
            "axioms": [
                {{
                    "type": "disjoint_classes",
                    "classes": ["Controller", "Processor"]
                }}
            ],
            "framework_metadata": {{
                "represents_unified_framework": true,
                "total_documents": {len(state.documents)},
                "framework_coherence": "high"
            }}
        }}
        """
        
        response = await get_openai_completion(
            ontology_prompt,
            "You are an expert in formal ontology design with comprehensive knowledge of OWL-DL and unified legal framework modeling for data protection."
        )
        
        try:
            result = json.loads(response)
            return result
        except json.JSONDecodeError:
            logger.error("Failed to parse unified ontology structure")
            # Safe fallback
            fallback_result = {
                "classes": [], 
                "object_properties": [], 
                "data_properties": [], 
                "individuals": [], 
                "axioms": [],
                "framework_metadata": {
                    "represents_unified_framework": True,
                    "total_documents": len(state.documents),
                    "error": "Ontology parsing failed"
                }
            }
            return fallback_result
    
    async def _generate_rdf_triples(self, ontology_data: Dict[str, Any], state: ProcessingState) -> List[Dict[str, str]]:
        """Generate RDF triples from ontology data"""
        
        triples = []
        
        # Create namespace
        LEGAL_NS = "http://legal-rules.org/ontology#"
        
        # Add ontology header triples
        triples.extend([
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdf:type", "object": "owl:Ontology"},
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdfs:label", "object": "Legal Rules Ontology"},
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdfs:comment", "object": "Comprehensive ontology for legal rules and data protection"}
        ])
        
        # Add class triples
        for cls in ontology_data.get("classes", []):
            class_uri = f"{LEGAL_NS}{cls['name']}"
            triples.extend([
                {"subject": class_uri, "predicate": "rdf:type", "object": "owl:Class"},
                {"subject": class_uri, "predicate": "rdfs:label", "object": cls['name']},
                {"subject": class_uri, "predicate": "rdfs:comment", "object": cls.get('description', '')}
            ])
            
            if cls.get('parent'):
                parent_uri = f"{LEGAL_NS}{cls['parent']}"
                triples.append({"subject": class_uri, "predicate": "rdfs:subClassOf", "object": parent_uri})
        
        # Add object property triples
        for prop in ontology_data.get("object_properties", []):
            prop_uri = f"{LEGAL_NS}{prop['name']}"
            triples.extend([
                {"subject": prop_uri, "predicate": "rdf:type", "object": "owl:ObjectProperty"},
                {"subject": prop_uri, "predicate": "rdfs:label", "object": prop['name']},
                {"subject": prop_uri, "predicate": "rdfs:comment", "object": prop.get('description', '')},
                {"subject": prop_uri, "predicate": "rdfs:domain", "object": f"{LEGAL_NS}{prop['domain']}"},
                {"subject": prop_uri, "predicate": "rdfs:range", "object": f"{LEGAL_NS}{prop['range']}"}
            ])
        
        # Add data property triples
        for prop in ontology_data.get("data_properties", []):
            prop_uri = f"{LEGAL_NS}{prop['name']}"
            triples.extend([
                {"subject": prop_uri, "predicate": "rdf:type", "object": "owl:DatatypeProperty"},
                {"subject": prop_uri, "predicate": "rdfs:label", "object": prop['name']},
                {"subject": prop_uri, "predicate": "rdfs:comment", "object": prop.get('description', '')},
                {"subject": prop_uri, "predicate": "rdfs:domain", "object": f"{LEGAL_NS}{prop['domain']}"},
                {"subject": prop_uri, "predicate": "rdfs:range", "object": f"xsd:{prop['range']}"}
            ])
        
        # Add individual triples
        for individual in ontology_data.get("individuals", []):
            ind_uri = f"{LEGAL_NS}{individual['name']}"
            triples.extend([
                {"subject": ind_uri, "predicate": "rdf:type", "object": f"{LEGAL_NS}{individual['type']}"},
                {"subject": ind_uri, "predicate": "rdfs:label", "object": individual['name']}
            ])
            
            for prop_name, prop_value in individual.get('properties', {}).items():
                triples.append({"subject": ind_uri, "predicate": f"{LEGAL_NS}{prop_name}", "object": str(prop_value)})
        
        return triples
    
    async def _create_owlready_ontology(self, state: ProcessingState) -> str:
        """Create ontology using Owlready2 for unified framework"""
        
        try:
            # Create ontology
            onto = owl.get_ontology("http://legal-rules.org/unified-framework-ontology.owl")
            
            with onto:
                # Define top-level classes
                class LegalEntity(owl.Thing): pass
                class DataOperation(owl.Thing): pass
                class RuleComponent(owl.Thing): pass
                class LegalDocument(owl.Thing): pass
                class ProtectionMeasure(owl.Thing): pass
                class LegalBasis(owl.Thing): pass
                
                # Define specific entity classes
                class Controller(LegalEntity): pass
                class Processor(LegalEntity): pass
                class JointController(LegalEntity): pass
                class DataSubject(LegalEntity): pass
                class SupervisoryAuthority(LegalEntity): pass
                class ThirdCountry(LegalEntity): pass
                
                # Define data operation classes
                class DataTransfer(DataOperation): pass
                class DataAccess(DataOperation): pass
                class DataEntitlement(DataOperation): pass
                class DataProcessing(DataOperation): pass
                
                # Define rule component classes
                class Obligation(RuleComponent): pass
                class Right(RuleComponent): pass
                class Restriction(RuleComponent): pass
                class Condition(RuleComponent): pass
                
                # Define object properties
                class hasObligation(owl.ObjectProperty):
                    domain = [LegalEntity]
                    range = [Obligation]
                
                class hasRight(owl.ObjectProperty):
                    domain = [DataSubject]
                    range = [Right]
                
                class requiresCondition(owl.ObjectProperty):
                    domain = [DataOperation]
                    range = [Condition]
                
                class appliesTo(owl.ObjectProperty):
                    domain = [RuleComponent]
                    range = [LegalEntity]
                
                # Define data properties
                class hasDescription(owl.DataProperty):
                    domain = [owl.Thing]
                    range = [str]
                
                class hasConfidenceScore(owl.DataProperty):
                    domain = [owl.Thing]
                    range = [float]
                
                # Create instances from unified extracted data
                for entity in state.unified_entities[:20]:  # Limit to prevent memory issues
                    try:
                        # Clean entity name for use as identifier
                        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', entity.name)
                        if hasattr(onto, entity.type.value):
                            entity_class = getattr(onto, entity.type.value)
                            instance = entity_class(clean_name)
                            instance.hasDescription = [entity.description]
                            instance.hasConfidenceScore = [entity.confidence]
                    except Exception as e:
                        logger.warning(f"Failed to create instance for {entity.name}: {e}")
                
                # Add disjointness constraints
                owl.AllDisjoint([Controller, Processor, DataSubject, SupervisoryAuthority, ThirdCountry])
                owl.AllDisjoint([DataTransfer, DataAccess, DataEntitlement, DataProcessing])
                owl.AllDisjoint([Obligation, Right, Restriction, Condition])
            
            # Save ontology
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            ontology_file = ONTOLOGY_OUTPUT / f"unified_comprehensive_ontology_{timestamp}.owl"
            onto.save(file=str(ontology_file), format="rdfxml")
            
            logger.info("Unified ontology created successfully")
            
            return str(ontology_file)
            
        except Exception as e:
            logger.error(f"Owlready2 ontology creation error: {e}")
            return ""
    
    async def _save_ontologies(self, rdf_triples: List[Dict[str, str]], owlready_file: str) -> Dict[str, str]:
        """Save ontologies in multiple formats"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        files = {}
        
        try:
            # Save RDF/XML format
            g = Graph()
            
            # Add namespaces
            LEGAL_NS = Namespace("http://legal-rules.org/ontology#")
            g.bind("legal", LEGAL_NS)
            g.bind("owl", OWL)
            g.bind("rdfs", RDFS)
            g.bind("xsd", XSD)
            
            # Add triples
            for triple in rdf_triples:
                subject = URIRef(triple["subject"]) if triple["subject"].startswith("http") else URIRef(LEGAL_NS + triple["subject"])
                
                # Handle predicate
                if triple["predicate"].startswith("http"):
                    predicate = URIRef(triple["predicate"])
                elif ":" in triple["predicate"]:
                    if triple["predicate"].startswith("rdf:"):
                        predicate = URIRef(f"http://www.w3.org/1999/02/22-rdf-syntax-ns#{triple['predicate'][4:]}")
                    elif triple["predicate"].startswith("rdfs:"):
                        predicate = URIRef(f"http://www.w3.org/2000/01/rdf-schema#{triple['predicate'][5:]}")
                    elif triple["predicate"].startswith("owl:"):
                        predicate = URIRef(f"http://www.w3.org/2002/07/owl#{triple['predicate'][4:]}")
                    else:
                        predicate = URIRef(LEGAL_NS + triple["predicate"])
                else:
                    predicate = URIRef(LEGAL_NS + triple["predicate"])
                
                # Handle object
                if triple["object"].startswith("http"):
                    obj = URIRef(triple["object"])
                elif triple["object"].startswith("xsd:"):
                    obj = URIRef(f"http://www.w3.org/2001/XMLSchema#{triple['object'][4:]}")
                elif triple["object"].startswith("owl:") or triple["object"].startswith("rdfs:") or triple["object"].startswith("rdf:"):
                    if triple["object"].startswith("owl:"):
                        obj = URIRef(f"http://www.w3.org/2002/07/owl#{triple['object'][4:]}")
                    elif triple["object"].startswith("rdfs:"):
                        obj = URIRef(f"http://www.w3.org/2000/01/rdf-schema#{triple['object'][5:]}")
                    elif triple["object"].startswith("rdf:"):
                        obj = URIRef(f"http://www.w3.org/1999/02/22-rdf-syntax-ns#{triple['object'][4:]}")
                    else:
                        obj = RDFLiteral(triple["object"])
                else:
                    obj = RDFLiteral(triple["object"])
                
                g.add((subject, predicate, obj))
            
            # Save in different formats
            rdfxml_file = ONTOLOGY_OUTPUT / f"unified_legal_ontology_{timestamp}.rdf"
            turtle_file = ONTOLOGY_OUTPUT / f"unified_legal_ontology_{timestamp}.ttl"
            
            g.serialize(destination=str(rdfxml_file), format="xml")
            g.serialize(destination=str(turtle_file), format="turtle")
            
            files.update({
                "rdf_xml": str(rdfxml_file),
                "turtle": str(turtle_file),
                "owlready": owlready_file
            })
            
            logger.info(f"Unified ontologies saved in {len(files)} formats")
            
        except Exception as e:
            logger.error(f"Ontology saving error: {e}")
        
        return files

class IntelligentDecisionTableGenerationAgent(EnhancedReactAgent):
    """Enhanced decision table generation with optimization"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Decision Table Generation Agent",
            role="comprehensive decision table creation and optimization specialist",
            tools=["decision tables", "rule optimization", "conflict resolution", "completeness analysis"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Generate optimized decision tables for unified legal framework"""
        logger.info("Intelligent Decision Table Generation: Creating optimized decision tables for unified legal framework")
        
        try:
            # Generate decision tables with references from unified framework
            decision_tables = await self._generate_comprehensive_unified_decision_tables(state)
            
            # Optimize tables for performance
            optimized_tables = await self._optimize_decision_tables(decision_tables)
            
            # Validate completeness
            completeness_analysis = await self._analyze_completeness(optimized_tables, state)
            
            # Store unified results
            state.unified_decision_rules = optimized_tables
            state.unified_metadata["unified_completeness_analysis"] = completeness_analysis
            
            state.processing_steps.append("Intelligent decision table generation completed for unified legal framework")
            state.current_agent = "final_output_generation"
            
            logger.info(f"Intelligent Decision Table Generation: Created {len(optimized_tables)} optimized decision tables for unified framework")
            return state
            
        except Exception as e:
            error_msg = f"Decision table generation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _generate_comprehensive_unified_decision_tables(self, state: ProcessingState) -> List[Dict[str, Any]]:
        """Generate comprehensive decision tables for unified legal framework"""
        
        decision_table_prompt = f"""
        Create comprehensive decision tables from this UNIFIED LEGAL FRAMEWORK analysis:
        
        UNIFIED ENHANCED ATOMIC RULES: {[f"ID: {rule.id}, Text: {rule.text[:150]}..." for rule in state.unified_enhanced_atomic_rules[:20]]}
        UNIFIED ENTITIES: {[f"{e.name} ({e.type})" for e in state.unified_entities[:25]]}
        UNIFIED CONCEPTS: {[f"{c.name} ({c.type})" for c in state.unified_concepts[:25]]}
        UNIFIED RULE COMPONENTS: {[f"{rc.name} ({rc.type})" for rc in state.unified_rule_components[:25]]}
        
        FRAMEWORK CONTEXT: This represents a UNIFIED LEGAL FRAMEWORK of {len(state.documents)} related documents working together as a comprehensive legal system.
        
        Create decision tables representing the UNIFIED FRAMEWORK with:
        
        1. COMPREHENSIVE CONDITIONS: All relevant input conditions across the framework
        2. CLEAR ACTIONS: Specific outputs and required actions
        3. PRIORITY HANDLING: Rule precedence for conflict resolution
        4. EXCEPTION MANAGEMENT: Handling of exceptions and edge cases
        5. COMPLIANCE VALIDATION: Built-in compliance checking
        6. REFERENCE TRACKING: Link each rule to source documents in the framework
        7. FRAMEWORK COHERENCE: Ensure decisions work across all documents
        
        Focus EXCLUSIVELY on these decision areas for the UNIFIED FRAMEWORK:
        - Data transfer authorization decisions (comprehensive framework approach)
        - Data access permission decisions (unified across all documents)
        - Data entitlement validation decisions (framework-wide entitlements)
        - Controller/Processor obligation compliance decisions (unified obligations)
        - Third country transfer adequacy decisions (comprehensive transfer framework)
        
        Return JSON with unified decision tables:
        {{
            "decision_tables": [
                {{
                    "table_id": "unified_dt_001",
                    "name": "Unified Data Transfer Authorization",
                    "description": "Determines authorization for data transfers across the unified legal framework",
                    "framework_scope": "unified_legal_framework",
                    "total_documents_covered": {len(state.documents)},
                    "conditions": {{
                        "data_subject_location": ["EU", "Non-EU"],
                        "transfer_destination": ["Adequate Country", "Third Country", "International Organization"],
                        "transfer_purpose": ["Commercial", "Legal Obligation", "Public Interest"],
                        "adequacy_decision": [true, false],
                        "safeguards_in_place": [true, false],
                        "data_subject_consent": [true, false, "not_required"],
                        "framework_compliance": [true, false]
                    }},
                    "actions": [
                        "authorize_transfer",
                        "require_additional_safeguards",
                        "request_supervisory_authority_approval",
                        "deny_transfer",
                        "document_transfer_basis",
                        "notify_data_subject",
                        "validate_framework_compliance"
                    ],
                    "rules": [
                        {{
                            "rule_id": "unified_dt_001_r01",
                            "conditions": {{
                                "transfer_destination": "Adequate Country",
                                "adequacy_decision": true,
                                "framework_compliance": true
                            }},
                            "actions": ["authorize_transfer", "document_transfer_basis", "validate_framework_compliance"],
                            "priority": 1,
                            "source_rule": "unified_rule_001",
                            "framework_references": [
                                {{
                                    "document": "Unified Legal Framework",
                                    "article": "Multiple Articles",
                                    "section": "Various Sections",
                                    "text_excerpt": "relevant text from the unified framework",
                                    "page_number": "Multiple Pages",
                                    "document_title": "Comprehensive Data Protection Framework",
                                    "section_title": "Transfers on the basis of adequacy decisions",
                                    "confidence": 0.95,
                                    "legal_authority": "statutory",
                                    "jurisdiction": "national",
                                    "framework_coherence": "high"
                                }}
                            ]
                        }}
                    ],
                    "exceptions": ["emergency_situations", "vital_interests"],
                    "compliance_requirements": ["documentation", "notification", "monitoring", "framework_validation"],
                    "implementation_complexity": "medium",
                    "framework_integration": "comprehensive"
                }}
            ],
            "unified_framework_metadata": {{
                "total_tables": 0,
                "framework_coverage": "comprehensive",
                "cross_document_integration": "high",
                "decision_coherence": "unified"
            }}
        }}
        """
        
        response = await get_openai_completion(
            decision_table_prompt,
            "You are a business rules expert specializing in unified legal framework decision table design with expertise in data protection compliance automation focusing on transfer, access, and entitlements."
        )
        
        try:
            result = json.loads(response)
            return result.get("decision_tables", [])
        except json.JSONDecodeError:
            logger.error("Failed to parse unified decision tables response")
            # Safe fallback
            fallback_result = [{
                "table_id": "unified_fallback_dt_001",
                "name": "Unified Legal Framework Decisions",
                "description": "Fallback decision table for unified framework",
                "framework_scope": "unified_legal_framework",
                "error": "Decision table parsing failed",
                "rules": []
            }]
            return fallback_result
    
    async def _optimize_decision_tables(self, decision_tables: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimize decision tables for performance and completeness"""
        
        optimized_tables = []
        
        for table in decision_tables:
            try:
                # Optimize rule ordering
                optimized_rules = self._optimize_rule_order(table.get("rules", []))
                
                # Eliminate redundant conditions
                optimized_conditions = await self._eliminate_redundant_conditions(table.get("conditions", {}))
                
                # Consolidate similar actions
                optimized_actions = self._consolidate_actions(table.get("actions", []))
                
                # Add performance optimizations
                performance_hints = self._generate_performance_hints(table)
                
                optimized_table = {
                    **table,
                    "rules": optimized_rules,
                    "conditions": optimized_conditions,
                    "actions": optimized_actions,
                    "performance_hints": performance_hints,
                    "optimization_applied": True
                }
                
                optimized_tables.append(optimized_table)
                
            except Exception as e:
                logger.warning(f"Failed to optimize table {table.get('table_id', 'unknown')}: {e}")
                optimized_tables.append(table)
        
        return optimized_tables
    
    def _optimize_rule_order(self, rules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimize rule execution order"""
        
        if not rules:
            return rules
        
        # Sort rules based on priority
        def rule_sort_key(rule):
            priority = rule.get("priority", 999)
            return priority
        
        return sorted(rules, key=rule_sort_key)
    
    async def _eliminate_redundant_conditions(self, conditions: Dict[str, Any]) -> Dict[str, Any]:
        """Eliminate redundant conditions"""
        
        # Simple redundancy elimination
        optimized_conditions = {}
        
        for condition_name, condition_values in conditions.items():
            if isinstance(condition_values, list) and len(set(condition_values)) == len(condition_values):
                # No duplicates, keep as is
                optimized_conditions[condition_name] = condition_values
            elif isinstance(condition_values, list):
                # Remove duplicates
                optimized_conditions[condition_name] = list(set(condition_values))
            else:
                optimized_conditions[condition_name] = condition_values
        
        return optimized_conditions
    
    def _consolidate_actions(self, actions: List[str]) -> List[str]:
        """Consolidate similar actions"""
        
        # Remove duplicates while preserving order
        seen = set()
        consolidated = []
        
        for action in actions:
            if action not in seen:
                seen.add(action)
                consolidated.append(action)
        
        return consolidated
    
    def _generate_performance_hints(self, table: Dict[str, Any]) -> Dict[str, Any]:
        """Generate performance optimization hints"""
        
        hints = {
            "condition_evaluation_order": [],
            "early_termination_possible": False,
            "indexing_recommendations": [],
            "caching_opportunities": []
        }
        
        conditions = table.get("conditions", {})
        
        # Suggest evaluation order based on selectivity
        selectivity_scores = {}
        for condition_name, condition_values in conditions.items():
            if isinstance(condition_values, list):
                # Higher selectivity for fewer possible values
                selectivity_scores[condition_name] = 1.0 / len(condition_values) if condition_values else 1.0
            else:
                selectivity_scores[condition_name] = 0.5
        
        # Order by selectivity (most selective first)
        hints["condition_evaluation_order"] = sorted(selectivity_scores.keys(), 
                                                   key=lambda x: selectivity_scores[x], 
                                                   reverse=True)
        
        # Check for early termination opportunities
        rules = table.get("rules", [])
        if len(rules) > 5:
            hints["early_termination_possible"] = True
        
        # Suggest indexing for frequently used conditions
        frequent_conditions = [name for name, values in conditions.items() 
                             if isinstance(values, list) and len(values) > 3]
        hints["indexing_recommendations"] = frequent_conditions
        
        # Suggest caching for complex rules
        if len(rules) > 10:
            hints["caching_opportunities"] = ["rule_evaluation_results", "condition_combinations"]
        
        return hints
    
    async def _analyze_completeness(self, decision_tables: List[Dict[str, Any]], state: ProcessingState) -> Dict[str, Any]:
        """Analyze completeness of decision tables"""
        
        completeness_analysis = {
            "total_tables": len(decision_tables),
            "coverage_analysis": {},
            "gap_analysis": {},
            "completeness_score": 0.0
        }
        
        # Analyze coverage for each table
        for table in decision_tables:
            table_id = table.get("table_id", "unknown")
            conditions = table.get("conditions", {})
            rules = table.get("rules", [])
            
            # Calculate theoretical maximum combinations
            max_combinations = 1
            for condition_values in conditions.values():
                if isinstance(condition_values, list):
                    max_combinations *= len(condition_values)
            
            # Count covered combinations
            covered_combinations = len(rules)
            
            coverage_score = covered_combinations / max_combinations if max_combinations > 0 else 0.0
            
            completeness_analysis["coverage_analysis"][table_id] = {
                "max_combinations": max_combinations,
                "covered_combinations": covered_combinations,
                "coverage_score": coverage_score,
                "uncovered_combinations": max_combinations - covered_combinations
            }
        
        # Calculate overall completeness score
        if decision_tables:
            coverage_scores = [analysis["coverage_score"] for analysis in completeness_analysis["coverage_analysis"].values()]
            completeness_analysis["completeness_score"] = sum(coverage_scores) / len(coverage_scores)
        
        return completeness_analysis

class FinalOutputGenerationAgent(EnhancedReactAgent):
    """Generate final unified output with rules, conditions, domains and roles"""
    
    def __init__(self):
        super().__init__(
            name="Final Output Generation Agent",
            role="final unified output formatting and JSON generation specialist",
            tools=["output formatting", "JSON generation", "rule categorization"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Generate final unified output in requested format"""
        logger.info("Final Output Generation: Creating unified simplified rules output")
        
        try:
            # Generate unified simplified rules output
            unified_simplified_rules = await self._generate_unified_simplified_rules_output(state)
            
            # Generate final decision tables in JSON with references
            final_unified_decision_tables = await self._format_unified_decision_tables_json(state.unified_decision_rules)
            
            # Store final outputs
            state.final_unified_rules_output = unified_simplified_rules
            state.final_unified_decision_tables = final_unified_decision_tables
            
            # Save final outputs
            saved_files = await self._save_final_unified_outputs(unified_simplified_rules, final_unified_decision_tables)
            state.unified_output_metadata["final_unified_output_files"] = saved_files
            
            state.processing_steps.append("Final unified output generation completed")
            state.current_agent = "completed"
            
            logger.info(f"Final Output Generation: Generated {len(unified_simplified_rules)} unified simplified rules")
            return state
            
        except Exception as e:
            error_msg = f"Final output generation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _generate_unified_simplified_rules_output(self, state: ProcessingState) -> List[Dict[str, Any]]:
        """Generate unified simplified rules output with required fields"""
        
        unified_simplified_rules = []
        
        for rule in state.unified_enhanced_atomic_rules:
            # Determine role based on entities
            role = self._determine_primary_role(rule.entities)
            
            # Extract domain from concepts
            domain = self._extract_domain(rule.concepts)
            
            # Extract conditions from rule components
            conditions = self._extract_conditions(rule.rule_components)
            
            unified_simplified_rule = {
                "rule_id": rule.id,
                "rule_text": rule.text,
                "conditions": conditions,
                "domain": domain,
                "role": role,
                "confidence": rule.confidence,
                "deontic_type": rule.deontic_type,
                "source_document": rule.source_document,
                "legal_authority": rule.legal_authority_level.value,
                "jurisdiction": rule.jurisdictional_scope.value,
                "complexity_level": rule.complexity_level.value,
                "key_phrases": rule.key_phrases
            }
            
            unified_simplified_rules.append(unified_simplified_rule)
        
        return unified_simplified_rules
    
    def _determine_primary_role(self, entities: List[LegalEntity]) -> str:
        """Determine primary role from entities"""
        
        # Priority order for roles
        role_priority = {
            EntityType.CONTROLLER: 1,
            EntityType.JOINT_CONTROLLER: 2,
            EntityType.PROCESSOR: 3,
            EntityType.DATA_SUBJECT: 4,
            EntityType.SUPERVISING_AUTHORITY: 5,
            EntityType.THIRD_COUNTRY: 6
        }
        
        relevant_entities = [e for e in entities if e.type in role_priority]
        
        if not relevant_entities:
            return "general"
        
        # Return the highest priority role
        primary_entity = min(relevant_entities, key=lambda e: role_priority.get(e.type, 999))
        
        # Map to simplified role names
        role_mapping = {
            EntityType.CONTROLLER: "controller",
            EntityType.JOINT_CONTROLLER: "joint_controller", 
            EntityType.PROCESSOR: "processor",
            EntityType.DATA_SUBJECT: "data_subject",
            EntityType.SUPERVISING_AUTHORITY: "authority",
            EntityType.THIRD_COUNTRY: "third_country"
        }
        
        return role_mapping.get(primary_entity.type, "general")
    
    def _extract_domain(self, concepts: List[LegalConcept]) -> str:
        """Extract domain from concepts"""
        
        # Domain mapping based on concept types
        domain_mapping = {
            ConceptType.DATA_TRANSFER: "data_transfer",
            ConceptType.DATA_ACCESS: "data_access", 
            ConceptType.DATA_ENTITLEMENT: "data_entitlement",
            ConceptType.PROCESSING: "data_processing"
        }
        
        # Find the primary domain
        for concept in concepts:
            if concept.type in domain_mapping:
                return domain_mapping[concept.type]
        
        return "general_compliance"
    
    def _extract_conditions(self, rule_components: List[RuleComponent]) -> List[str]:
        """Extract conditions from rule components"""
        
        conditions = []
        
        for component in rule_components:
            if component.type in [RuleComponentType.CONDITION, RuleComponentType.RESTRICTION]:
                conditions.append(component.description)
            elif component.type in [RuleComponentType.OBLIGATION, RuleComponentType.RIGHT] and "if" in component.description.lower():
                # Extract conditional parts from obligations and rights
                conditions.append(component.description)
        
        return conditions
    
    async def _format_unified_decision_tables_json(self, decision_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Format unified decision tables as clean JSON with enhanced references"""
        
        formatted_tables = {
            "decision_tables": [],
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_tables": len(decision_rules),
                "format_version": "1.0",
                "scope": "unified_multi_document_analysis"
            }
        }
        
        for table in decision_rules:
            formatted_table = {
                "table_id": table.get("table_id", "unknown"),
                "name": table.get("name", ""),
                "description": table.get("description", ""),
                "conditions": table.get("conditions", {}),
                "rules": []
            }
            
            for rule in table.get("rules", []):
                # Get references from the source rule
                references = self._get_unified_rule_references(rule.get("source_rule", ""))
                
                formatted_rule = {
                    "rule_id": rule.get("rule_id", ""),
                    "conditions": rule.get("conditions", {}),
                    "actions": rule.get("actions", []),
                    "priority": rule.get("priority", 999),
                    "source_rule": rule.get("source_rule", ""),
                    "references": references
                }
                formatted_table["rules"].append(formatted_rule)
            
            formatted_tables["decision_tables"].append(formatted_table)
        
        return formatted_tables
    
    def _get_unified_rule_references(self, source_rule_id: str) -> List[Dict[str, Any]]:
        """Get references for a specific rule from unified enhanced atomic rules"""
        
        references = []
        
        # This would need access to state.unified_enhanced_atomic_rules, but since we can't access state here,
        # we'll create a placeholder structure
        reference = {
            "document": "Unified Legal Framework",
            "article": "Various Articles",
            "section": "Multiple Sections",
            "text_excerpt": "Extracted from comprehensive multi-document analysis",
            "page_number": "Multiple Pages",
            "document_title": "Unified Legal Document Set",
            "section_title": "Data Protection Provisions",
            "confidence": 0.9,
            "legal_authority": "statutory",
            "jurisdiction": "national"
        }
        references.append(reference)
        
        return references
    
    async def _save_final_unified_outputs(self, unified_simplified_rules: List[Dict[str, Any]], 
                                         unified_decision_tables: Dict[str, Any]) -> Dict[str, str]:
        """Save final unified outputs in requested formats"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        try:
            # Save unified simplified rules as JSON
            rules_json_file = OUTPUT_DIRECTORY / f"final_unified_rules_output_{timestamp}.json"
            rules_content = {
                "rules": unified_simplified_rules,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "total_rules": len(unified_simplified_rules),
                    "format": "unified_simplified_rules_output",
                    "scope": "multi_document_analysis"
                }
            }
            
            with open(rules_json_file, 'w', encoding='utf-8') as f:
                f.write(safe_json_serialize(rules_content))
            saved_files["unified_rules_json"] = str(rules_json_file)
            
            # Save unified decision tables as JSON
            decision_tables_json_file = OUTPUT_DIRECTORY / f"unified_decision_tables_{timestamp}.json"
            with open(decision_tables_json_file, 'w', encoding='utf-8') as f:
                f.write(safe_json_serialize(unified_decision_tables))
            saved_files["unified_decision_tables_json"] = str(decision_tables_json_file)
            
            # Save combined unified output
            combined_output_file = OUTPUT_DIRECTORY / f"complete_unified_output_{timestamp}.json"
            combined_content = {
                "unified_simplified_rules": unified_simplified_rules,
                "unified_decision_tables": unified_decision_tables,
                "summary": {
                    "total_rules": len(unified_simplified_rules),
                    "total_decision_tables": len(unified_decision_tables.get("decision_tables", [])),
                    "generated_at": datetime.now().isoformat(),
                    "scope": "multi_document_unified_analysis"
                }
            }
            
            with open(combined_output_file, 'w', encoding='utf-8') as f:
                f.write(safe_json_serialize(combined_content))
            saved_files["unified_combined_output"] = str(combined_output_file)
            
            logger.info(f"Final unified outputs saved in {len(saved_files)} files")
            
        except Exception as e:
            logger.error(f"Final output saving error: {e}")
        
        return saved_files

# ============================================================================
# ENHANCED WORKFLOW ORCHESTRATION
# ============================================================================

class ComprehensiveLegalRulesWorkflow:
    """Comprehensive workflow orchestrator using LangGraph with all enhanced agents"""
    
    def __init__(self):
        self.agents = {
            "document_processor": AdvancedDocumentProcessorAgent(),
            "intelligent_segmentation": IntelligentSegmentationAgent(),
            "entity_extraction": ComprehensiveEntityExtractionAgent(),
            "concept_extraction": AdvancedConceptExtractionAgent(),
            "rule_component_extraction": IntelligentRuleComponentExtractionAgent(),
            "ontology_formalization": AdvancedOntologyFormalizationAgent(),
            "decision_table_generation": IntelligentDecisionTableGenerationAgent(),
            "final_output_generation": FinalOutputGenerationAgent()
        }
        
        self.checkpointer = MemorySaver()
        self.workflow = self._build_comprehensive_workflow()
    
    def _build_comprehensive_workflow(self) -> StateGraph:
        """Build the comprehensive LangGraph workflow"""
        
        workflow = StateGraph(ProcessingState)
        
        # Add nodes for each agent
        workflow.add_node("document_processor", self._document_processor_node)
        workflow.add_node("intelligent_segmentation", self._intelligent_segmentation_node)
        workflow.add_node("entity_extraction", self._entity_extraction_node)
        workflow.add_node("concept_extraction", self._concept_extraction_node)
        workflow.add_node("rule_component_extraction", self._rule_component_node)
        workflow.add_node("ontology_formalization", self._ontology_formalization_node)
        workflow.add_node("decision_table_generation", self._decision_table_generation_node)
        workflow.add_node("final_output_generation", self._final_output_generation_node)
        
        # Define the enhanced flow
        workflow.add_edge(START, "document_processor")
        workflow.add_edge("document_processor", "intelligent_segmentation")
        workflow.add_edge("intelligent_segmentation", "entity_extraction")
        workflow.add_edge("entity_extraction", "concept_extraction")
        workflow.add_edge("concept_extraction", "rule_component_extraction")
        workflow.add_edge("rule_component_extraction", "ontology_formalization")
        workflow.add_edge("ontology_formalization", "decision_table_generation")
        workflow.add_edge("decision_table_generation", "final_output_generation")
        workflow.add_edge("final_output_generation", END)
        
        return workflow
    
    # Node methods for each agent
    async def _document_processor_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["document_processor"]
        try:
            thought = await agent.think(f"Documents: {len(state.documents)}", "Extract and preprocess all documents", "Starting comprehensive multi-document analysis")
            result = await agent.act(thought, "document_processing", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Document processor node error: {e}")
            state.error_messages.append(f"Document processor error: {str(e)}")
            return state
    
    async def _intelligent_segmentation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["intelligent_segmentation"]
        try:
            total_text_length = len(state.unified_raw_text)
            thought = await agent.think(f"Total unified text length: {total_text_length}", "Segment into atomic clauses", "Analyzing preprocessed unified text structure")
            result = await agent.act(thought, "intelligent_segmentation", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Intelligent segmentation node error: {e}")
            state.error_messages.append(f"Intelligent segmentation error: {str(e)}")
            return state
    
    async def _entity_extraction_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["entity_extraction"]
        try:
            total_clauses = len(state.unified_clauses)
            thought = await agent.think(f"{total_clauses} unified clauses identified", "Extract legal entities", "Processing identified clauses for entities across unified framework")
            result = await agent.act(thought, "entity_extraction", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Entity extraction node error: {e}")
            state.error_messages.append(f"Entity extraction error: {str(e)}")
            return state
    
    async def _concept_extraction_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["concept_extraction"]
        try:
            thought = await agent.think(f"Entities extracted from unified framework, processing concepts", "Extract legal concepts", "Building on entity analysis for concept identification across unified framework")
            result = await agent.act(thought, "concept_extraction", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Concept extraction node error: {e}")
            state.error_messages.append(f"Concept extraction error: {str(e)}")
            return state
    
    async def _rule_component_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["rule_component_extraction"]
        try:
            thought = await agent.think("Entities and concepts identified across unified framework", "Extract rule components", "Analyzing logical structure of rules across unified framework")
            result = await agent.act(thought, "rule_component_extraction", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Rule component node error: {e}")
            state.error_messages.append(f"Rule component extraction error: {str(e)}")
            return state
    
    async def _ontology_formalization_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["ontology_formalization"]
        try:
            thought = await agent.think("Rule components extracted from unified framework", "Create unified formal ontology", "Formalizing knowledge into unified OWL-DL ontology")
            result = await agent.act(thought, "ontology_creation", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Ontology formalization node error: {e}")
            state.error_messages.append(f"Ontology formalization error: {str(e)}")
            return state
    
    async def _decision_table_generation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["decision_table_generation"]
        try:
            thought = await agent.think("Unified ontology created", "Generate unified decision tables", "Creating operational decision tables from unified framework")
            result = await agent.act(thought, "decision_table_generation", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Decision table generation node error: {e}")
            state.error_messages.append(f"Decision table generation error: {str(e)}")
            return state
    
    async def _final_output_generation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["final_output_generation"]
        try:
            thought = await agent.think("Unified decision tables generated", "Generate final unified simplified output", "Creating final unified JSON output")
            result = await agent.act(thought, "final_output_generation", state)
            if not isinstance(result, ProcessingState):
                logger.error(f"Agent returned {type(result)}, expected ProcessingState")
                return state
            return result
        except Exception as e:
            logger.error(f"Final output generation node error: {e}")
            state.error_messages.append(f"Final output generation error: {str(e)}")
            return state
    
    async def process_all_documents(self, document_paths: List[str], all_metadata: Dict[str, Dict[str, Any]]) -> ProcessingState:
        """Process all documents through the entire enhanced unified workflow"""
        
        logger.info(f"Starting comprehensive unified processing of: {len(document_paths)} documents as unified legal framework")
        
        # Initialize state with proper validation for unified processing
        try:
            initial_state = ProcessingState(
                documents=document_paths,
                unified_metadata={"input_metadata": all_metadata}
            )
        except Exception as e:
            logger.error(f"Failed to initialize ProcessingState: {e}")
            # Create a fallback state
            initial_state = ProcessingState(
                documents=document_paths,
                unified_metadata={"input_metadata": all_metadata if all_metadata else {}, "initialization_error": str(e)}
            )
        
        # Compile and run workflow
        app = self.workflow.compile(checkpointer=self.checkpointer)
        
        config = {"configurable": {"thread_id": f"unified_framework_{hash(tuple(document_paths))}"}}
        
        try:
            final_state = await app.ainvoke(initial_state, config)
            
            # Ensure we always return a ProcessingState object
            if isinstance(final_state, dict):
                # Convert dict back to ProcessingState if needed
                try:
                    final_state = ProcessingState(**final_state)
                except Exception as e:
                    logger.error(f"Failed to convert dict to ProcessingState: {e}")
                    # Preserve original state with error
                    initial_state.error_messages.append(f"State conversion error: {str(e)}")
                    return initial_state
            
            logger.info(f"Unified processing completed successfully for: {len(document_paths)} documents")
            return final_state
            
        except Exception as e:
            logger.error(f"Workflow error for unified documents: {e}")
            # Ensure we return a ProcessingState object with error info
            if hasattr(initial_state, 'error_messages'):
                initial_state.error_messages.append(f"Workflow error: {str(e)}")
            else:
                # Create new ProcessingState if initial_state is corrupted
                error_state = ProcessingState(
                    documents=document_paths,
                    unified_metadata={"input_metadata": all_metadata if all_metadata else {}, "workflow_error": str(e)},
                    error_messages=[f"Workflow error: {str(e)}"]
                )
                return error_state
            return initial_state

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Enhanced main execution function with comprehensive unified processing"""
    logger.info("Starting Enhanced Legal Rules Multi-Agent System - Unified Framework Processing")
    
    # Load metadata
    metadata = load_metadata()
    
    # Initialize comprehensive workflow
    workflow = ComprehensiveLegalRulesWorkflow()
    
    # Process all PDF documents as a unified legal framework
    pdf_files = list(PDF_DIRECTORY.glob("*.pdf"))
    
    if not pdf_files:
        logger.warning(f"No PDF files found in {PDF_DIRECTORY}")
        print(f"No PDF files found in {PDF_DIRECTORY}")
        print("Please ensure PDF files are placed in the legislation_pdfs directory")
        return
    
    # Prepare all document paths and metadata for unified processing
    document_paths = [str(pdf_file) for pdf_file in pdf_files]
    all_metadata = {}
    
    for pdf_file in pdf_files:
        # Get metadata for this document
        doc_metadata = metadata.get(str(pdf_file), {})
        all_metadata[str(pdf_file)] = doc_metadata
    
    logger.info(f"Processing {len(document_paths)} documents as unified legal framework")
    print(f"\n🏛️ Processing {len(document_paths)} documents as UNIFIED LEGAL FRAMEWORK:")
    print(f"   📋 Approach: Treating all documents as related articles/chapters of one comprehensive legal system")
    for i, pdf_file in enumerate(pdf_files, 1):
        print(f"   📄 Document {i}: {pdf_file.name}")
    
    # Process all documents as a unified legal framework
    result = await workflow.process_all_documents(document_paths, all_metadata)
    
    # Ensure result is a ProcessingState object
    if not isinstance(result, ProcessingState):
        logger.error(f"Expected ProcessingState, got {type(result)} for unified processing")
        # Create a fallback ProcessingState
        result = ProcessingState(
            documents=document_paths,
            unified_metadata={"input_metadata": all_metadata, "processing_error": f"Invalid result type: {type(result)}"},
            error_messages=[f"Invalid result type: {type(result)}"]
        )
    
    # Log results - check if result has error_messages attribute
    if hasattr(result, 'error_messages') and result.error_messages:
        logger.error(f"Errors in unified processing: {result.error_messages}")
        print(f"  ❌ Errors encountered: {len(result.error_messages)}")
        for error in result.error_messages:
            print(f"     - {error}")
    else:
        logger.info(f"Successfully processed unified legal framework")
        print(f"  ✅ Unified framework processing completed successfully")
        
    # Print summary statistics - with safe attribute access for unified processing
    print(f"  📊 Unified Framework Results Summary:")
    
    # Unified clauses count
    unified_clauses_count = len(getattr(result, 'unified_clauses', []))
    print(f"     - Total unified clauses extracted: {unified_clauses_count}")
    
    # Individual document clauses for reference
    total_individual_clauses = sum(len(clauses) for clauses in getattr(result, 'document_clauses', {}).values())
    print(f"     - Individual document clauses (reference): {total_individual_clauses}")
    
    # Unified entities, concepts, and components
    print(f"     - Unified entities: {len(getattr(result, 'unified_entities', []))}")
    print(f"     - Unified concepts: {len(getattr(result, 'unified_concepts', []))}")
    print(f"     - Unified rule components: {len(getattr(result, 'unified_rule_components', []))}")
    print(f"     - Enhanced atomic rules (unified): {len(getattr(result, 'unified_enhanced_atomic_rules', []))}")
    print(f"     - Ontology triples (unified): {len(getattr(result, 'unified_ontology_triples', []))}")
    print(f"     - Decision tables (unified): {len(getattr(result, 'unified_decision_rules', []))}")
    print(f"     - Final unified simplified rules: {len(getattr(result, 'final_unified_rules_output', []))}")
    
    # Quality metrics
    quality_score = getattr(result, 'quality_metrics', {}).get('overall_quality_score', 0.0)
    print(f"     - Quality score: {quality_score:.2f}")
    
    # Framework coherence
    framework_analysis = getattr(result, 'unified_metadata', {}).get('unified_framework_analysis', {})
    framework_coherence = framework_analysis.get('framework_coherence_score', 0.0)
    print(f"     - Framework coherence score: {framework_coherence:.2f}")
    
    # Output files
    if hasattr(result, 'unified_output_metadata') and result.unified_output_metadata.get("final_unified_output_files"):
        output_files = result.unified_output_metadata["final_unified_output_files"]
        successful_files = len([k for k in output_files.keys() if not k.endswith('_error')])
        error_files = len([k for k in output_files.keys() if k.endswith('_error')])
        print(f"     - Unified output files generated: {successful_files}")
        if error_files > 0:
            print(f"     - File generation errors: {error_files}")
    
    # Generate overall summary report for unified processing
    summary_file = OUTPUT_DIRECTORY / "comprehensive_unified_framework_processing_summary.json"
    
    try:
        summary = {
            "processing_session": {
                "timestamp": datetime.now().isoformat(),
                "system_version": "Enhanced Multi-Agent System v1.0 - Unified Framework Processing",
                "total_documents": len(pdf_files),
                "processing_approach": "unified_legal_framework",
                "framework_treatment": "all_documents_as_related_articles_chapters",
                "successful_processing": not (hasattr(result, 'error_messages') and result.error_messages),
                "failed_processing": bool(hasattr(result, 'error_messages') and result.error_messages)
            },
            "unified_framework_statistics": {
                "total_unified_clauses": unified_clauses_count,
                "total_individual_clauses_reference": total_individual_clauses,
                "total_unified_entities": len(getattr(result, 'unified_entities', [])),
                "total_unified_concepts": len(getattr(result, 'unified_concepts', [])),
                "total_unified_rule_components": len(getattr(result, 'unified_rule_components', [])),
                "total_enhanced_rules": len(getattr(result, 'unified_enhanced_atomic_rules', [])),
                "total_ontology_triples": len(getattr(result, 'unified_ontology_triples', [])),
                "total_decision_rules": len(getattr(result, 'unified_decision_rules', [])),
                "total_final_unified_simplified_rules": len(getattr(result, 'final_unified_rules_output', [])),
                "framework_coherence_score": framework_coherence,
                "average_quality_score": quality_score
            },
            "processing_details": {
                "documents_processed_as_unified_framework": [str(path) for path in document_paths],
                "success": not (hasattr(result, 'error_messages') and result.error_messages),
                "total_unified_clauses": unified_clauses_count,
                "unified_rules": len(getattr(result, 'unified_enhanced_atomic_rules', [])),
                "final_unified_rules": len(getattr(result, 'final_unified_rules_output', [])),
                "quality_score": quality_score,
                "framework_coherence": framework_coherence,
                "errors": getattr(result, 'error_messages', []),
                "warnings": getattr(result, 'warnings', []),
                "processing_steps": getattr(result, 'processing_steps', [])
            },
            "unified_output_files": {},
            "framework_metadata": getattr(result, 'unified_metadata', {})
        }
        
        # Add output files information with error handling
        if hasattr(result, 'unified_output_metadata') and result.unified_output_metadata.get("final_unified_output_files"):
            output_files = result.unified_output_metadata["final_unified_output_files"]
            summary["unified_output_files"] = {
                "successful_files": {k: v for k, v in output_files.items() if not k.endswith('_error')},
                "failed_files": {k: v for k, v in output_files.items() if k.endswith('_error')},
                "total_successful": len([k for k in output_files.keys() if not k.endswith('_error')]),
                "total_failed": len([k for k in output_files.keys() if k.endswith('_error')])
            }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(safe_json_serialize(summary))
        
        logger.info(f"Enhanced unified processing complete. Comprehensive summary saved to {summary_file}")
        
    except Exception as e:
        logger.error(f"Error creating summary: {e}")
        print(f"     - Error creating summary: {e}")
    
    # Print final summary
    print(f"\n" + "="*90)
    print("ENHANCED LEGAL RULES MULTI-AGENT SYSTEM - UNIFIED FRAMEWORK PROCESSING COMPLETE")
    print(f"="*90)
    print(f"🏛️ Legal Framework Approach: Unified processing of related documents")
    print(f"📄 Documents processed as unified framework: {len(pdf_files)}")
    
    # Check if processing was successful
    processing_successful = not (hasattr(result, 'error_messages') and result.error_messages)
    print(f"✅ Processing Status: {'SUCCESSFUL' if processing_successful else 'COMPLETED WITH ERRORS'}")
    
    if hasattr(result, 'error_messages') and result.error_messages:
        print(f"❌ Errors encountered: {len(result.error_messages)}")
    
    print(f"📊 Unified framework rules extracted: {len(getattr(result, 'unified_enhanced_atomic_rules', []))}")
    print(f"🎯 Final simplified rules: {len(getattr(result, 'final_unified_rules_output', []))}")
    print(f"🧠 Framework coherence score: {framework_coherence:.2f}")
    print(f"📈 Quality score: {quality_score:.2f}")
    
    # Display output files with error handling
    if hasattr(result, 'unified_output_metadata') and result.unified_output_metadata.get("final_unified_output_files"):
        output_files = result.unified_output_metadata["final_unified_output_files"]
        successful_files = {k: v for k, v in output_files.items() if not k.endswith('_error')}
        error_files = {k: v for k, v in output_files.items() if k.endswith('_error')}
        
        print(f"📁 Output files generated: {len(successful_files)}")
        if error_files:
            print(f"❌ File generation errors: {len(error_files)}")
        
        print(f"\n📁 Final Unified Output Files:")
        for file_type, file_path in successful_files.items():
            print(f"   ✅ {file_type}: {file_path}")
        
        if error_files:
            print(f"\n❌ File Generation Errors:")
            for file_type, error_msg in error_files.items():
                print(f"   ❌ {file_type}: {error_msg}")
    
    print(f"📋 Summary saved to: {summary_file}")
    print(f"="*90)
    print(f"🎉 UNIFIED LEGAL FRAMEWORK ANALYSIS COMPLETE")
    print(f"   All {len(pdf_files)} documents processed as ONE coherent legal system")
    print(f"   Focus: Data transfer, access, and entitlements")
    print(f"   Output: Single unified set of rules and decision tables")
    print(f"="*90)
    
    return result

if __name__ == "__main__":
    asyncio.run(main())
