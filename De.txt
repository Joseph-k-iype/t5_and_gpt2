import os
import sys
import json
import logging
import pandas as pd
import numpy as np
from typing import Optional, Any, Dict, List
from pathlib import Path
from openai import OpenAI
from pydantic import BaseModel, ValidationError, field_validator
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
import re

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI API Configuration
OPENAI_API_KEY = "your-openai-api-key"  # Replace with your actual API key
OPENAI_BASE_URL = "https://api.openai.com/v1"  # Replace with your base URL if different
OPENAI_MODEL = "gpt-4"  # Fixed to GPT-4
OPENAI_TEMPERATURE = 0.7
OPENAI_MAX_TOKENS = 800

# File Paths
INPUT_CSV_PATH = "incidents.csv"  # Input CSV file path
OUTPUT_CSV_PATH = "classified_incidents_streaming.csv"  # Output CSV file path
LOG_FILE_PATH = "data_issue_analysis.log"  # Log file path

# Processing Configuration
BATCH_SIZE = 10  # Number of records to process before logging progress
AUTO_SAVE_FREQUENCY = 50  # Save progress every N records

# Input CSV Column Mapping
REQUIRED_COLUMNS = {
    'incident_id': 'IT_INCIDENT_ID',
    'incident_desc': 'IT_INCIDENT_DESC',
    'incident_area_category': 'IT_INCIDENT_AREA_CATEGORY',
    'incident_area_subcategory': 'IT_INCIDENT_AREA_SUBCATEGORY_NAME',
    'incident_location': 'IT_INCIDENT_IMPACTED_LOCATION_NAME',
    'incident_cause': 'IT_INCIDENT_CAUSE_TEXT',
    'incident_summary': 'MAJOR_IT_INCIDENT_SUMMARY_TEXT',
    'incident_findings': 'MAJOR_IT_INCIDENT_FINDINGS_TEXT',
    'incident_resolution': 'IT_INCIDENT_RESOLUTION_DESC'
}

# ============================================================================

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

## Document class

class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    metadata: Dict[str, Any] = {}

## Chatbot components

class OpenAIChatbot:
    def __init__(self):
        self.api_key = OPENAI_API_KEY
        self.base_url = OPENAI_BASE_URL
        self.model_name = OPENAI_MODEL
        self.temperature = OPENAI_TEMPERATURE
        self.max_tokens = OPENAI_MAX_TOKENS
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
    
    def _setup_chat_model(self):
        try:
            self.llm = ChatOpenAI(
                model_name=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                openai_api_key=self.api_key,
                openai_api_base=self.base_url
            )
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise

# Enhanced Data Issue Classifier with Confidence Scoring

class DataIssueReasoning(BaseModel):
    """Model for reasoning behind data issue classification"""
    supporting_reasons: List[str] = []
    contrary_reasons: List[str] = []
    confidence_calculation: Dict[str, float] = {}

class DataIssueClassifier(BaseModel):
    """Enhanced model for classifying incidents as data issues with confidence scoring"""
    incident_id: str
    is_data_issue: bool
    data_quality_dimension: Optional[str] = None
    confidence_score: float = 0.0
    reasoning: DataIssueReasoning
    summary: str = ""

    @field_validator('data_quality_dimension')
    @classmethod
    def validate_dimension(cls, v):
        if v is not None:
            valid_dimensions = [
                # HSBC categories
                "Consistency",
                "Accuracy",
                "Completeness",
                "Validity",
                "Conformity",
                "Uniqueness",
                "Timeliness",
                "Reasonableness",
                "Integrity",
                
                # Industry categories
                "Incomplete Data",
                "Inaccurate Data",
                "Duplicate Data",
                "Inconsistent Data",
                "Outdated Data",
                "Invalid Data",
                "Ambiguous Data",
                "Data Integrity Issues",
                "Data Redundancy"
            ]
            if v not in valid_dimensions:
                raise ValueError(f"Must be one of {valid_dimensions}")
        return v

class DataIssueAnalyzer:
    def __init__(self, chatbot: OpenAIChatbot):
        self.chatbot = chatbot
        self.classification_prompt_template = """
        You are a data quality expert at HSBC analyzing incidents to determine if they are data issues.
        
        A data quality issue is a problem with data that impacts a process or a system through, for example:
        erroneous, inaccurate, incomplete, invalid, duplicate, irrelevant, nonstandard data.
        
        The data quality issue is NOT an issue related to:
        - system configuration,
        - system maintenance,
        - issues with logging to the system,
        - issues connected to system UI refresh time,
        - issues connected to phone/fax/email
        
        The data quality could be related to system and application issues, but not all system and applications issues are classified as data quality issue. In order to classify system and/or application issue as data quality issue, the problem has to be connected to the data quality.
        
        The examples of DQ Issues:
        - Customers with date of birth in future
        - Legal Entities missing Legal Entity Identifier
        - Customer records missing Customer Risk Rating
        - Customers invalid email address
        - Inconsistent legal entity names and identifiers across multiple systems and jurisdictions;
        - Inconsistent institutional client names and identifiers across multiple banking systems used by institutional clients;
        - Inter-company transfer amounts and details are inconsistent across multiple accounting systems by HSBC legal entity;
        - Lack of timely accessibility to vendor payment amounts by the Finance team when creating quarterly financials; and
        - Multiple vendors listed multiple times and in many different addresses and variations of names in Vendor Payment systems.
        
        HSBC definitions of DQ issue dimensions (categories):
        
        Consistency: where data is represented differently across various records or systems. For example, Inconsistent categorization of expenses in financial statements.
        
        Accuracy: refers to information that is incorrect or misleading. For example, incorrect sales figures reported in financial statement.
        
        Completeness: arises when records are missing essential information, such as missing fields or values that are necessary for analysis or reporting. For example, missing customer email addresses in a CRM system.
        
        Validity: data that does not conform to predefined formats or rules. For example, out-of-range values in numerical fields, such as age or salary.
        
        Conformity: data that does not conform to predefined formats or rules. For example, email addresses that do not conform to standard formats.
        
        Uniqueness: duplicate data occurs when the same record is entered multiple times within a dataset, leading to redundancy and inflated counts, multiple entries for the same customer in a database.
        
        Timeliness: refers to information that is no longer current or relevant. For example, old customer contact information that hasn't been updated.
        
        Reasonableness: reasonableness asks whether a data pattern meets expectations. For example, whether a distribution of sales across a geographic idea makes sense based on what is known about the customers in that area.
        
        Integrity: data integrity issues occur when the accuracy and consistency of data are compromised, often due to unauthorized changes, corruption, or system failures. For example, corrupted data files leading to loss of critical information.
        
        Note: HSBC categorization is slightly different than industry categorization, but both types of categorization should be used to learn what Data Quality issue is.
        
        Industry categorization of data quality dimensions:
        
        1. Incomplete Data: This issue arises when records are missing essential information, such as missing fields or values that are necessary for analysis or reporting. Incomplete data can lead to inaccurate conclusions and hinder decision-making processes.
        
        2. Inaccurate Data: Inaccurate data refers to information that is incorrect or misleading, often due to human error, outdated information, or faulty data entry processes. This can result in flawed analyses and poor business decisions.
        
        3. Duplicate Data: Duplicate data occurs when the same record is entered multiple times within a dataset, leading to redundancy and inflated counts. This can skew analysis results and complicate data management efforts. In HSBC, the sheer scale of data sources often causes redundancy and overlap via duplicate records. Data issues like duplication of contact information may also increase the likelihood of distorted analytical outcomes. Marketing initiatives suffer when certain prospects are overlooked while others are addressed repeatedly. Duplicate records may also lead to distorted analysis outcomes.
        
        4. Inconsistent Data: Inconsistent data arises when the same data element is represented differently across various records or systems, such as variations in naming conventions or formats. This can create confusion and hinder data integration efforts. When you're working with various data sources, the differences might be in formats, units, or spellings. Inconsistencies in data values tend to accumulate and degrade the usefulness of data.
        
        5. Outdated Data: Outdated data refers to information that is no longer current or relevant, often due to changes in business processes, regulations, or market conditions. Relying on outdated data can lead to misguided strategies and decisions.
        
        6. Invalid Data: Invalid data includes entries that do not conform to predefined formats or rules, such as incorrect data types or values outside acceptable ranges. This can cause errors in processing and analysis.
        
        7. Ambiguous Data: Ambiguous data is information that can be interpreted in multiple ways, leading to confusion or misinterpretation. Clear definitions and standards are necessary to mitigate this issue.
        
        8. Data Integrity Issues: Data integrity issues occur when the accuracy and consistency of data are compromised, often due to unauthorized changes, corruption, or system failures. Maintaining data integrity is crucial for reliable reporting and analysis.
        
        9. Data Redundancy: Data redundancy refers to the unnecessary duplication of data within a database or system, which can lead to increased storage costs and maintenance challenges. It can also complicate data retrieval and analysis.
        
        Examples of HSBC specific data quality issues:
        
        Incomplete Data:
        - Missing customer email addresses in a CRM system.
        - Incomplete transaction records lacking item descriptions.
        - Missing fields in employee records, such as job titles or departments.
        - Incomplete loan applications without required financial information.
        - Missing product specifications in inventory records.
        
        Inaccurate Data:
        - Incorrect sales figures reported in financial statements.
        - Wrong customer addresses leading to failed deliveries.
        - Misreported inventory levels due to data entry errors.
        - Incorrect pricing information in product catalogs.
        - Inaccurate employee salary records affecting payroll.
        
        Duplicate Data:
        - Multiple entries for the same customer in a database.
        - Duplicate product listings in an e-commerce platform.
        - Repeated records of transactions in a financial system.
        - Duplicate patient records in a healthcare database.
        - Multiple entries for the same invoice in accounting software.
        
        Inconsistent Data:
        - Different formats for date entries (MM/DD/YYYY vs. DD/MM/YYYY).
        - Variations in product names across different systems (e.g., "Widget A" vs. "Widget A1").
        - Inconsistent currency formats in financial reports.
        - Different naming conventions for departments in HR records.
        - Inconsistent categorization of expenses in financial statements.
        
        Outdated Data:
        - Old customer contact information that hasn't been updated.
        - Expired product listings still visible on a website.
        - Outdated employee records not reflecting recent job changes.
        - Old marketing campaign data that no longer applies.
        - Historical sales data that is no longer relevant for current analysis.
        
        Invalid Data:
        - Incorrect phone number formats (e.g., letters included).
        - Invalid email addresses that do not conform to standard formats.
        - Out-of-range values in numerical fields, such as age or salary.
        - Incorrectly formatted social security numbers.
        - Invalid product codes that do not match any existing items.
        
        Ambiguous Data:
        - Vague product descriptions that do not specify features.
        - Customer feedback that lacks context or specifics.
        - Multiple meanings for a term used in data, leading to confusion.
        - Unclear instructions in data entry forms.
        - Ambiguous survey responses that cannot be easily categorized.
        
        Data Integrity Issues:
        - Unauthorized changes made to financial records.
        - Corrupted data files leading to loss of critical information.
        - Inconsistent data across different databases due to synchronization issues.
        - Data breaches resulting in compromised sensitive information.
        - Loss of data integrity due to system failures or crashes.
        
        Data Redundancy:
        - Multiple databases storing the same customer information.
        - Repeated entries of the same transaction in different systems.
        - Overlapping datasets in data warehouses leading to increased storage costs.
        
        Analyze the following IT incident:
        ID: {incident_id}
        Description: {description}
        Area Category: {area_category}
        Area Subcategory: {area_subcategory}
        Impacted Location: {location}
        Cause: {cause}
        Summary: {summary}
        Findings: {findings}
        Resolution: {resolution}
        
        Provide a detailed analysis with:
        1. Whether this is a data issue according to HSBC's definition
        2. If yes, the most appropriate data quality dimension from HSBC's categorization
        3. List of supporting reasons (factors that indicate this is a data issue)
        4. List of contrary reasons (factors that indicate this is not a data issue)
        5. A confidence score explanation based on the reasons
        
        You must respond ONLY with a valid JSON object using the following exact format:
        {{
            "is_data_issue": true/false,
            "data_quality_dimension": "dimension here or null if not a data issue",
            "supporting_reasons": ["reason1", "reason2", ...],
            "contrary_reasons": ["reason1", "reason2", ...],
            "confidence_calculation": {{
                "base_score": 0.5,
                "supporting_weight": 0.15,
                "contrary_weight": -0.1,
                "final_score": 0.0-1.0
            }},
            "summary": "brief explanation of the classification"
        }}
        
        Do not include any text before or after the JSON object. The response must be valid JSON that can be parsed directly.
        """
        
    def calculate_confidence_score(self, supporting_reasons: List[str], contrary_reasons: List[str]) -> Dict[str, float]:
        """Calculate confidence score based on supporting and contrary reasons"""
        base_score = 0.5  # Start neutral
        supporting_weight = 0.15  # Each supporting reason adds this much
        contrary_weight = -0.1    # Each contrary reason subtracts this much
        
        # Calculate adjustments
        supporting_adjustment = len(supporting_reasons) * supporting_weight
        contrary_adjustment = len(contrary_reasons) * contrary_weight
        
        # Calculate raw score
        raw_score = base_score + supporting_adjustment + contrary_adjustment
        
        # Normalize to 0-1 range using sigmoid function
        final_score = 1 / (1 + np.exp(-4 * (raw_score - 0.5)))
        
        return {
            "base_score": base_score,
            "supporting_weight": supporting_weight,
            "contrary_weight": contrary_weight,
            "supporting_adjustment": supporting_adjustment,
            "contrary_adjustment": contrary_adjustment,
            "raw_score": raw_score,
            "final_score": float(final_score)
        }
        
    def analyze_incident(self, incident_id: str, description: str, area_category: str, 
                        area_subcategory: str, location: str, cause: str, summary: str, 
                        findings: str, resolution: str) -> DataIssueClassifier:
        """Analyze a single incident for data quality issues with enhanced reasoning"""
        try:
            prompt = self.classification_prompt_template.format(
                incident_id=incident_id,
                description=description,
                area_category=area_category,
                area_subcategory=area_subcategory,
                location=location,
                cause=cause,
                summary=summary,
                findings=findings,
                resolution=resolution
            )
            
            response = self.chatbot.llm.predict(prompt)
            
            # Extract JSON from response (in case there's additional text)
            json_match = re.search(r'({[\s\S]*})', response)
            if json_match:
                json_str = json_match.group(1)
                try:
                    response_json = json.loads(json_str)
                except json.JSONDecodeError:
                    # Try cleaning the JSON string if initial parsing fails
                    json_str = re.sub(r'```json|```', '', json_str).strip()
                    response_json = json.loads(json_str)
            else:
                # If no JSON pattern found, try parsing the whole response
                response_json = json.loads(response)
            
            # Extract reasoning components
            supporting_reasons = response_json.get("supporting_reasons", [])
            contrary_reasons = response_json.get("contrary_reasons", [])
            
            # Calculate confidence score
            confidence_calculation = self.calculate_confidence_score(supporting_reasons, contrary_reasons)
            
            # Handle potential missing confidence_calculation in response
            if "confidence_calculation" not in response_json:
                response_json["confidence_calculation"] = {}
            
            # Update with our algorithm
            response_json["confidence_calculation"].update(confidence_calculation)
            
            reasoning = DataIssueReasoning(
                supporting_reasons=supporting_reasons,
                contrary_reasons=contrary_reasons,
                confidence_calculation=confidence_calculation
            )
            
            return DataIssueClassifier(
                incident_id=incident_id,
                is_data_issue=response_json.get("is_data_issue", False),
                data_quality_dimension=response_json.get("data_quality_dimension"),
                confidence_score=confidence_calculation["final_score"],
                reasoning=reasoning,
                summary=response_json.get("summary", "")
            )
            
        except (json.JSONDecodeError, ValidationError) as e:
            logger.error(f"Error parsing response for incident {incident_id}: {e}")
            logger.error(f"Raw response: {response}")
            
            # Fallback classification
            reasoning = DataIssueReasoning(
                supporting_reasons=[],
                contrary_reasons=["Error in classification: Unable to parse response"],
                confidence_calculation={"final_score": 0.0}
            )
            return DataIssueClassifier(
                incident_id=incident_id,
                is_data_issue=False,
                data_quality_dimension=None,
                confidence_score=0.0,
                reasoning=reasoning,
                summary="Error in classification: Unable to parse AI response"
            )
    
    def process_csv(self, file_path: str = None, output_path: str = None) -> Dict[str, any]:
        """Process a CSV file and classify all incidents, writing results immediately row by row"""
        # Use global paths if not provided
        if file_path is None:
            file_path = INPUT_CSV_PATH
        if output_path is None:
            output_path = OUTPUT_CSV_PATH
            
        try:
            # Read CSV file
            df = pd.read_csv(file_path)
            logger.info(f"Loaded CSV file: {file_path} with {len(df)} records")
            
            # Validate required columns
            missing_columns = []
            for logical_name, actual_column in REQUIRED_COLUMNS.items():
                if actual_column not in df.columns:
                    missing_columns.append(actual_column)
            
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            logger.info(f"All required columns found: {list(REQUIRED_COLUMNS.values())}")
            
            # Define additional columns for results
            result_columns = [
                'is_data_issue', 
                'data_quality_dimension', 
                'confidence_score',
                'supporting_reasons', 
                'contrary_reasons',
                'classification_summary', 
                'confidence_calculation'
            ]
            
            # Create output file with headers
            all_columns = df.columns.tolist() + result_columns
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                f.write(','.join(all_columns) + '\n')
            logger.info(f"Created output file: {output_path}")
            
            # Initialize counters for summary
            total_incidents = len(df)
            processed_count = 0
            data_issues_count = 0
            dimension_counts = {}
            
            # Process each incident and write results immediately
            for idx, row in df.iterrows():
                try:
                    # Process the incident with all available fields
                    classification = self.analyze_incident(
                        incident_id=str(row[REQUIRED_COLUMNS['incident_id']]),
                        description=str(row[REQUIRED_COLUMNS['incident_desc']]),
                        area_category=str(row[REQUIRED_COLUMNS['incident_area_category']]),
                        area_subcategory=str(row[REQUIRED_COLUMNS['incident_area_subcategory']]),
                        location=str(row[REQUIRED_COLUMNS['incident_location']]),
                        cause=str(row[REQUIRED_COLUMNS['incident_cause']]),
                        summary=str(row[REQUIRED_COLUMNS['incident_summary']]),
                        findings=str(row[REQUIRED_COLUMNS['incident_findings']]),
                        resolution=str(row[REQUIRED_COLUMNS['incident_resolution']])
                    )
                    
                    # Update counters
                    processed_count += 1
                    if classification.is_data_issue:
                        data_issues_count += 1
                        dim = classification.data_quality_dimension
                        if dim:
                            dimension_counts[dim] = dimension_counts.get(dim, 0) + 1
                    
                    # Prepare row data
                    result_data = {
                        'is_data_issue': classification.is_data_issue,
                        'data_quality_dimension': classification.data_quality_dimension or "",
                        'confidence_score': classification.confidence_score,
                        'supporting_reasons': '; '.join(classification.reasoning.supporting_reasons),
                        'contrary_reasons': '; '.join(classification.reasoning.contrary_reasons),
                        'classification_summary': classification.summary,
                        'confidence_calculation': json.dumps(classification.reasoning.confidence_calculation)
                    }
                    
                    # Combine original row with results
                    combined_row = {**row.to_dict(), **result_data}
                    
                    # Convert to a pandas Series to ensure proper ordering
                    result_series = pd.Series(combined_row)
                    
                    # Write the row to CSV immediately (save on the fly)
                    with open(output_path, 'a', newline='', encoding='utf-8') as f:
                        # Escape any commas in the values and wrap in quotes if needed
                        csv_row = []
                        for col in all_columns:
                            val = str(result_series.get(col, ""))
                            if ',' in val or '"' in val or '\n' in val:
                                # Replace double quotes with two double quotes for CSV escaping
                                val = val.replace('"', '""')
                                val = f'"{val}"'
                            csv_row.append(val)
                        f.write(','.join(csv_row) + '\n')
                    
                    # Log progress at specified intervals
                    if processed_count % BATCH_SIZE == 0 or processed_count == total_incidents:
                        logger.info(f"Processed {processed_count}/{total_incidents} incidents ({(processed_count/total_incidents)*100:.1f}%)")
                        logger.info(f"Data issues found so far: {data_issues_count}")
                    
                    # Auto-save progress summary every N records for large datasets
                    if processed_count % AUTO_SAVE_FREQUENCY == 0:
                        logger.info(f"Auto-save checkpoint: {processed_count} records processed")
                        logger.info(f"Current data issues: {data_issues_count} ({(data_issues_count/processed_count)*100:.1f}%)")
                
                except Exception as e:
                    logger.error(f"Error processing row {idx}, incident ID {row.get(REQUIRED_COLUMNS['incident_id'], 'Unknown')}: {e}")
                    # Continue with the next row instead of failing the entire process
                    continue
            
            # Generate summary
            summary = {
                "total_incidents": total_incidents,
                "processed_count": processed_count,
                "data_issues_count": data_issues_count,
                "dimension_counts": dimension_counts,
                "output_path": output_path,
                "input_path": file_path
            }
            
            # Log final summary
            logger.info("="*60)
            logger.info("FINAL PROCESSING SUMMARY")
            logger.info("="*60)
            logger.info(f"Input file: {file_path}")
            logger.info(f"Output file: {output_path}")
            logger.info(f"Total incidents: {total_incidents}")
            logger.info(f"Successfully processed: {processed_count}")
            logger.info(f"Data issues identified: {data_issues_count}")
            if processed_count > 0:
                data_issue_percentage = (data_issues_count / processed_count) * 100
                logger.info(f"Data issues percentage: {data_issue_percentage:.1f}%")
            logger.info("\nData Quality Dimension Distribution:")
            for dimension, count in dimension_counts.items():
                percentage = (count / data_issues_count) * 100 if data_issues_count > 0 else 0
                logger.info(f"  {dimension}: {count} ({percentage:.1f}%)")
            logger.info("="*60)
            
            return summary
        
        except Exception as e:
            logger.error(f"Error processing CSV file: {e}")
            raise

# Example usage
if __name__ == "__main__":
    logger.info("="*60)
    logger.info("STARTING DATA ISSUE CLASSIFICATION")
    logger.info("="*60)
    logger.info(f"OpenAI Model: {OPENAI_MODEL}")
    logger.info(f"Input CSV: {INPUT_CSV_PATH}")
    logger.info(f"Output CSV: {OUTPUT_CSV_PATH}")
    logger.info(f"Batch Size: {BATCH_SIZE}")
    logger.info(f"Auto-save Frequency: {AUTO_SAVE_FREQUENCY}")
    logger.info("="*60)
    
    try:
        # Initialize OpenAI chatbot with global configuration
        logger.info("Initializing OpenAI chatbot...")
        chatbot = OpenAIChatbot()
        
        # Initialize Data Issue Analyzer
        logger.info("Initializing Data Issue Analyzer...")
        analyzer = DataIssueAnalyzer(chatbot)
        
        # Process CSV file with streaming output for large datasets
        logger.info("Starting CSV processing...")
        summary = analyzer.process_csv()
        
        # Display final summary statistics
        logger.info("\nFINAL STATISTICS:")
        logger.info(f"Total incidents: {summary['total_incidents']}")
        logger.info(f"Successfully processed: {summary['processed_count']}")
        logger.info(f"Data issues identified: {summary['data_issues_count']}")
        
        # Calculate percentages
        if summary['processed_count'] > 0:
            data_issue_percentage = (summary['data_issues_count'] / summary['processed_count']) * 100
            logger.info(f"Data issues percentage: {data_issue_percentage:.1f}%")
        
        # Show high vs low confidence classifications
        try:
            # Read the completed CSV to get confidence statistics
            result_df = pd.read_csv(summary['output_path'])
            high_confidence = result_df[result_df['confidence_score'] >= 0.8]
            medium_confidence = result_df[(result_df['confidence_score'] >= 0.5) & (result_df['confidence_score'] < 0.8)]
            low_confidence = result_df[result_df['confidence_score'] < 0.5]
            
            logger.info(f"\nCONFIDENCE DISTRIBUTION:")
            logger.info(f"High confidence (>=0.8): {len(high_confidence)} ({len(high_confidence)/len(result_df)*100:.1f}%)")
            logger.info(f"Medium confidence (0.5-0.8): {len(medium_confidence)} ({len(medium_confidence)/len(result_df)*100:.1f}%)")
            logger.info(f"Low confidence (<0.5): {len(low_confidence)} ({len(low_confidence)/len(result_df)*100:.1f}%)")
            
        except Exception as e:
            logger.error(f"Error generating confidence statistics: {e}")
            
        logger.info("="*60)
        logger.info("PROCESSING COMPLETED SUCCESSFULLY")
        logger.info("="*60)
            
    except FileNotFoundError:
        logger.error(f"CSV file '{INPUT_CSV_PATH}' not found. Please ensure the file exists.")
    except Exception as e:
        logger.error(f"An error occurred: {e}")
        raise
