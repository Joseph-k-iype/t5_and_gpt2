"""
Unified File to ODRL Converter
Automatically detects and processes CSV or Excel files.

Usage:
    python unified_file_converter.py input.csv
    python unified_file_converter.py input.xlsx
    python unified_file_converter.py input.xlsx --sheet "Rules"

Location: unified_file_converter.py (project root)
"""
import argparse
import asyncio
import json
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.processors.csv_processor import CSVProcessor, RuleFrameworkEntry as CSVEntry
from src.processors.excel_processor import ExcelProcessor, RuleFrameworkEntry as ExcelEntry
from src.analyzers.guidance_analyzer import GuidanceAnalyzer, ODRLComponents
from src.managers.data_category_manager import DataCategoryManager
from src.generators.odrl_rule_generator import ODRLRuleGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class UnifiedFileConverter:
    """
    Unified converter that automatically handles CSV or Excel files.
    """
    
    def __init__(self):
        """Initialize all components."""
        self.csv_processor = CSVProcessor()
        self.excel_processor = ExcelProcessor()
        self.guidance_analyzer = GuidanceAnalyzer()
        self.data_category_manager = DataCategoryManager()
        self.odrl_generator = ODRLRuleGenerator()
        
        self.statistics = {
            'total_entries': 0,
            'successful': 0,
            'failed': 0,
            'processing_time': 0.0,
            'file_type': None
        }
    
    def detect_file_type(self, filepath: str) -> str:
        """
        Detect file type from extension.
        
        Args:
            filepath: Path to input file
            
        Returns:
            'csv', 'excel', or raises ValueError
        """
        file_path = Path(filepath)
        suffix = file_path.suffix.lower()
        
        if suffix == '.csv':
            return 'csv'
        elif suffix in ['.xlsx', '.xlsm', '.xls']:
            return 'excel'
        else:
            raise ValueError(
                f"Unsupported file type: {suffix}. "
                f"Supported types: .csv, .xlsx, .xlsm, .xls"
            )
    
    async def convert_file_to_odrl(
        self,
        filepath: str,
        output_filepath: str = None,
        filter_framework: str = None,
        enrich_categories: bool = False,
        sheet_name: str = None,
        header_row: int = 1
    ) -> List[Dict[str, Any]]:
        """
        Main conversion process with automatic file type detection.
        
        Args:
            filepath: Path to input file (CSV or Excel)
            output_filepath: Path for output JSON file
            filter_framework: Filter by framework (DSS or DataVISA)
            enrich_categories: Whether to enrich data categories with LLM
            sheet_name: Excel only - Name of sheet to read
            header_row: Excel only - Row number containing headers
            
        Returns:
            List of ODRL policies
        """
        start_time = datetime.utcnow()
        
        # Detect file type
        file_type = self.detect_file_type(filepath)
        self.statistics['file_type'] = file_type
        
        print("\n" + "="*80)
        print(f"UNIFIED FILE TO ODRL CONVERTER ({file_type.upper()})")
        print("="*80)
        print(f"Input File: {filepath}")
        print(f"File Type: {file_type.upper()}")
        if file_type == 'excel' and sheet_name:
            print(f"Sheet: {sheet_name}")
        if file_type == 'excel':
            print(f"Header Row: {header_row}")
        print(f"Output: {output_filepath or 'stdout'}")
        if filter_framework:
            print(f"Filter: {filter_framework}")
        print("="*80 + "\n")
        
        # Step 1: Read file based on type
        print(f"üìÑ Reading {file_type.upper()} file...")
        try:
            if file_type == 'csv':
                entries = self.csv_processor.read_csv(filepath)
                self.csv_processor.print_statistics()
                processor = self.csv_processor
            else:  # excel
                entries = self.excel_processor.read_excel(
                    filepath,
                    sheet_name=sheet_name,
                    header_row=header_row
                )
                self.excel_processor.print_statistics()
                processor = self.excel_processor
                
        except Exception as e:
            logger.error(f"Failed to read file: {e}")
            return []
        
        # Filter by framework if specified
        if filter_framework:
            entries = processor.filter_by_framework(filter_framework)
            print(f"\nüîç Filtered to {len(entries)} entries for framework: {filter_framework}")
        
        if not entries:
            print("‚ö†Ô∏è  No entries to process!")
            return []
        
        self.statistics['total_entries'] = len(entries)
        
        # Step 2: Process each entry (same for both CSV and Excel)
        print(f"\nüîÑ Processing {len(entries)} entries...")
        odrl_policies = []
        
        for idx, entry in enumerate(entries, 1):
            print(f"\n[{idx}/{len(entries)}] Processing: {entry.id}")
            print(f"  Framework: {entry.rule_framework}")
            print(f"  Type: {entry.restriction_condition}")
            print(f"  Rule: {entry.rule_name[:60]}..." if len(entry.rule_name) > 60 else f"  Rule: {entry.rule_name}")
            
            try:
                # Step 2a: Analyze guidance with LLM
                print("  ü§ñ Analyzing guidance...")
                odrl_components: ODRLComponents = await self.guidance_analyzer.analyze_guidance(
                    guidance_text=entry.guidance,
                    rule_name=entry.rule_name,
                    framework_type=entry.rule_framework,
                    restriction_condition=entry.restriction_condition,
                    rule_id=entry.id
                )
                
                # Step 2b: Extract and enrich data categories
                print("  üìä Processing data categories...")
                data_categories = self.data_category_manager.extract_categories(
                    odrl_components.data_categories
                )
                
                if enrich_categories and data_categories:
                    print("  üîç Enriching categories with LLM...")
                    enriched = await self.data_category_manager.enrich_categories(
                        data_categories
                    )
                    data_categories = enriched
                
                self.data_category_manager.save_categories()
                
                # Step 2c: Generate ODRL policy
                print("  üìù Generating ODRL policy...")
                policy = self.odrl_generator.generate_policy(
                    entry=entry,
                    odrl_components=odrl_components,
                    data_categories=data_categories
                )
                
                odrl_policies.append(policy)
                self.statistics['successful'] += 1
                print("  ‚úÖ Success")
                
            except Exception as e:
                logger.error(f"Failed to process entry {entry.id}: {e}")
                self.statistics['failed'] += 1
                print(f"  ‚ùå Failed: {e}")
                continue
        
        # Step 3: Save output
        end_time = datetime.utcnow()
        self.statistics['processing_time'] = (end_time - start_time).total_seconds()
        
        if output_filepath and odrl_policies:
            print(f"\nüíæ Saving to: {output_filepath}")
            output_path = Path(output_filepath)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(odrl_policies, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ Saved {len(odrl_policies)} policies to {output_filepath}")
        
        # Step 4: Print summary
        self._print_summary()
        
        return odrl_policies
    
    def _print_summary(self):
        """Print conversion summary statistics."""
        print("\n" + "="*80)
        print("CONVERSION SUMMARY")
        print("="*80)
        print(f"File Type:            {self.statistics['file_type'].upper()}")
        print(f"Total Entries:        {self.statistics['total_entries']}")
        print(f"Successful:           {self.statistics['successful']}")
        print(f"Failed:               {self.statistics['failed']}")
        print(f"Success Rate:         {self.statistics['successful'] / max(1, self.statistics['total_entries']) * 100:.1f}%")
        print(f"Processing Time:      {self.statistics['processing_time']:.2f} seconds")
        if self.statistics['successful'] > 0:
            print(f"Avg Time per Entry:   {self.statistics['processing_time'] / self.statistics['successful']:.2f} seconds")
        print("="*80)


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Convert CSV or Excel rule framework entries to ODRL policies (auto-detects file type)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # CSV files
    python unified_file_converter.py rules.csv
    python unified_file_converter.py rules.csv --output odrl_policies.json
    
    # Excel files
    python unified_file_converter.py rules.xlsx
    python unified_file_converter.py rules.xlsx --output odrl_policies.json
    python unified_file_converter.py rules.xlsx --sheet "Rules"
    python unified_file_converter.py rules.xlsx --sheet "Rules" --header-row 2
    
    # Framework filtering (works for both CSV and Excel)
    python unified_file_converter.py rules.csv --framework DSS
    python unified_file_converter.py rules.xlsx --framework DataVISA
    
    # Category enrichment (works for both CSV and Excel)
    python unified_file_converter.py rules.csv --enrich-categories
    python unified_file_converter.py rules.xlsx --enrich-categories

File Format:
    Automatically detects file type from extension (.csv, .xlsx, .xlsm, .xls)
    
    Required columns (case-insensitive for Excel):
    - id: Unique identifier
    - rule_framework: DSS or DataVISA
    - restriction_condition: restriction or condition
    - rule_name: Title of the rule
    - guidance: Complete guidance text with details, actions, evidence

Output:
    JSON file containing ODRL 2.2 compliant policies
    Data categories saved to: config/data_categories.json
        """
    )
    
    parser.add_argument(
        'input_file',
        help='Input file path (.csv, .xlsx, .xlsm, or .xls)'
    )
    parser.add_argument(
        '--output', '-o',
        help='Output JSON file path (default: stdout)',
        default=None
    )
    parser.add_argument(
        '--framework', '-f',
        choices=['DSS', 'DataVISA', 'dss', 'datavisa'],
        help='Filter by framework type',
        default=None
    )
    parser.add_argument(
        '--enrich-categories',
        action='store_true',
        help='Use LLM to enrich data categories (slower but more detailed)'
    )
    parser.add_argument(
        '--sheet', '-s',
        help='[Excel only] Sheet name to read (default: active sheet)',
        default=None
    )
    parser.add_argument(
        '--header-row', '-r',
        type=int,
        help='[Excel only] Row number containing headers (1-indexed, default: 1)',
        default=1
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Verbose output'
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Validate input file exists
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"‚ùå Error: Input file not found: {args.input_file}")
        sys.exit(1)
    
    # Create converter and run
    converter = UnifiedFileConverter()
    
    try:
        # Detect file type first to validate
        file_type = converter.detect_file_type(args.input_file)
        
        # Warn if Excel-specific options used with CSV
        if file_type == 'csv':
            if args.sheet:
                logger.warning("--sheet option ignored for CSV files")
            if args.header_row != 1:
                logger.warning("--header-row option ignored for CSV files")
        
        policies = await converter.convert_file_to_odrl(
            filepath=args.input_file,
            output_filepath=args.output,
            filter_framework=args.framework.upper() if args.framework else None,
            enrich_categories=args.enrich_categories,
            sheet_name=args.sheet,
            header_row=args.header_row
        )
        
        # If no output file specified, print to stdout
        if not args.output and policies:
            print("\n" + "="*80)
            print("ODRL POLICIES (JSON)")
            print("="*80)
            print(json.dumps(policies, indent=2, ensure_ascii=False))
        
        print("\n‚úÖ Conversion complete!")
        
    except ValueError as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Conversion interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        logger.error(f"Conversion failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
