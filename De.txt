"""
TermMatchingAgent - Agent responsible for finding and ranking business terms.

This agent uses vector similarity search as a first pass and can incorporate
LLM-based re-ranking or filtering to provide more contextually relevant term matches.
It forms part of the RAG (Retrieval Augmented Generation) pattern for PBT tagging.
"""
import logging
from typing import List, Dict, Any, Optional, Tuple

# Assuming BusinessTermManager is in app.core.business_terms
# This creates a circular dependency if BusinessTermManager imports TermMatchingAgent directly at the top level.
# It's better if BusinessTermManager instantiates TermMatchingAgent and passes itself (or necessary components)
# to the agent's constructor if the agent needs to call back into the manager (e.g., for vector search).

# from app.core.business_terms import BusinessTermManager # Avoid direct import if manager imports this
from app.core.embedding import EmbeddingClient, MyDocument
# from app.config.settings import get_llm # If LLM is used directly by this agent
# from langchain_openai import AzureChatOpenAI # If LLM is used directly

logger = logging.getLogger(__name__)

class TermMatchingAgent:
    def __init__(self, business_term_manager): # Accept BusinessTermManager instance
        """
        Initialize the TermMatchingAgent.

        Args:
            business_term_manager: An instance of BusinessTermManager to access
                                   vector store and embedding capabilities.
        """
        self.bt_manager = business_term_manager
        self.embedding_client = business_term_manager.embedding_client
        # self.llm = get_llm() # Initialize LLM if this agent uses it directly for re-ranking/RAG
        logger.info("TermMatchingAgent initialized.")

    async def find_matching_terms(
        self,
        element_id: str,
        element_name: str,
        element_description: str,
        top_k: int,
        cdm_context: Optional[str] = None,
        example_context: Optional[str] = None,
        process_name_context: Optional[str] = None,
        process_description_context: Optional[str] = None,
        initial_threshold: float = 0.3
    ) -> Tuple[List[Dict[str, Any]], List[float]]:
        """
        Finds matching business terms for a given element name and description,
        incorporating contextual information.

        This method should implement the RAG pattern:
        1. Retrieve: Fetch initial candidates from the vector store.
        2. Augment: (Optional) Prepare context for an LLM.
        3. Generate/Rank: Use an LLM to re-rank, filter, or generate explanations for matches,
           or apply other business logic for hybrid search.

        Args:
            element_id: ID of the element being tagged.
            element_name: Name of the element.
            element_description: Description of the element.
            top_k: The final number of top terms to return.
            cdm_context: CDM associated with the element.
            example_context: Example values/usage of the element.
            process_name_context: Related business process name.
            process_description_context: Related business process description.
            initial_threshold: Initial similarity threshold for vector search.

        Returns:
            A tuple containing:
                - A list of dictionaries, where each dictionary represents a matched term
                  (including 'id', 'name', 'description', 'metadata', 'similarity').
                - A list of confidence scores corresponding to the matched terms.
        """
        logger.info(f"TermMatchingAgent: Finding terms for '{element_name}' (ID: {element_id}) with top_k={top_k}, threshold={initial_threshold}")
        logger.debug(f"Context: CDM='{cdm_context}', Example='{example_context[:50]}...', Process='{process_name_context}'")

        # 1. Construct a comprehensive query text for embedding
        query_parts = [
            f"Item Name: {element_name}",
            f"Description: {element_description}"
        ]
        if example_context:
            query_parts.append(f"Examples: {example_context}")
        if process_name_context:
            query_parts.append(f"Related Business Process: {process_name_context}")
        if process_description_context:
            query_parts.append(f"Process Description: {process_description_context}")
        if cdm_context: # Include element's CDM in the query for semantic matching
            query_parts.append(f"Associated Common Data Model (CDM): {cdm_context}")
        
        query_text = ". ".join(query_parts)
        logger.debug(f"Constructed query text for embedding: {query_text[:300]}...")

        # 2. Generate embedding for the query
        query_doc = MyDocument(id=f"query_{element_id}", text=query_text)
        try:
            embedded_query_doc = self.embedding_client.generate_embeddings(query_doc)
            if not embedded_query_doc.embedding:
                logger.error("Failed to generate embedding for the query.")
                return [], []
        except Exception as e:
            logger.error(f"Error during embedding generation for query: {e}", exc_info=True)
            return [], []
        
        # 3. Retrieve initial candidates from the vector store
        # Fetch more than top_k initially to allow for re-ranking/filtering.
        # The vector_store.find_similar_vectors should return dicts with 'id', 'name', 'description', 'metadata', 'similarity'
        try:
            # We might want to fetch more candidates if we plan to re-rank heavily.
            # Let's say fetch 3*top_k or a fixed number like 20-50.
            num_candidates_to_fetch = max(top_k * 3, 20) 
            logger.debug(f"Fetching {num_candidates_to_fetch} candidates from vector store with threshold {initial_threshold}...")
            initial_candidates = self.bt_manager.vector_store.find_similar_vectors(
                query_vector=embedded_query_doc.embedding,
                top_k=num_candidates_to_fetch,
                threshold=initial_threshold 
            )
        except Exception as e:
            logger.error(f"Error retrieving candidates from vector store: {e}", exc_info=True)
            return [], []

        logger.info(f"Retrieved {len(initial_candidates)} initial candidates from vector store.")
        if not initial_candidates:
            return [], []

        # 4. Augment & Generate/Rank (RAG step - Placeholder for your advanced logic)
        # This is where you would use an LLM or other sophisticated ranking.
        # For now, we'll implement a basic hybrid approach:
        # - Semantic score (from vector search)
        # - Keyword relevance (simple check)
        # - CDM preference (if cdm_context is provided)

        final_ranked_terms = []
        for candidate in initial_candidates:
            term_name = candidate.get("name", "").lower()
            term_desc = candidate.get("description", "").lower()
            term_cdm = candidate.get("metadata", {}).get("cdm", "").lower()
            
            score = candidate.get("similarity", 0.0) # Start with semantic similarity

            # Keyword relevance boost (simple example)
            if element_name.lower() in term_name or element_name.lower() in term_desc:
                score += 0.05 # Small boost for keyword match
            
            # CDM preference boost
            if cdm_context and term_cdm == cdm_context.lower():
                score += 0.1 # Boost if PBT's CDM matches the element's CDM context

            # Ensure score doesn't exceed 1.0 if you're adding boosts
            score = min(score, 1.0) 
            
            candidate_with_final_score = candidate.copy()
            candidate_with_final_score['final_score'] = score 
            final_ranked_terms.append(candidate_with_final_score)

        # Sort by the new 'final_score'
        final_ranked_terms.sort(key=lambda x: x['final_score'], reverse=True)
        
        # Select top_k results
        top_results = final_ranked_terms[:top_k]

        # Prepare the output format
        matched_terms_output = []
        confidence_scores_output = []

        for term in top_results:
            # The 'similarity' from vector store is a good candidate for confidence,
            # or use 'final_score' if your ranking logic produces a comparable metric.
            # For this example, let's use 'final_score' as the confidence.
            confidence = term.get('final_score', term.get('similarity', 0.0))
            
            # Ensure the term dictionary has the expected keys for TaggingResult
            # The find_similar_vectors should already return 'id', 'name', 'description', 'metadata', 'similarity'
            term_details_for_output = {
                "id": term.get("id"),
                "name": term.get("name"),
                "description": term.get("description"),
                "metadata": term.get("metadata", {}),
                "similarity": term.get("similarity"), # Original semantic similarity
                # You could add 'final_score' to metadata if needed for transparency
            }
            matched_terms_output.append(term_details_for_output)
            confidence_scores_output.append(confidence)
            
        logger.info(f"TermMatchingAgent: Returning {len(matched_terms_output)} terms after ranking/filtering.")
        return matched_terms_output, confidence_scores_output

