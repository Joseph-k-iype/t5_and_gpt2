"""
Azure OpenAI service that directly uses the REST API without tiktoken.
"""

import os
import logging
import json
import time
import requests
from typing import List, Dict, Any, Optional, Union
from app.core.auth_helper import get_azure_token_cached, refresh_token_if_needed
from dotenv import dotenv_values

logger = logging.getLogger(__name__)

# Load credentials directly from file
credentials_path = os.path.join("env", "credentials.env")
config_path = os.path.join("env", "config.env")
credentials_values = {}
config_values = {}

try:
    if os.path.isfile(credentials_path):
        logger.info(f"AzureOpenAIService loading credentials from {credentials_path}")
        credentials_values = dotenv_values(credentials_path)
        logger.info(f"Loaded {len(credentials_values)} values from {credentials_path}")
    else:
        logger.warning(f"Credentials file not found: {credentials_path}")
        
    if os.path.isfile(config_path):
        logger.info(f"AzureOpenAIService loading config from {config_path}")
        config_values = dotenv_values(config_path)
        logger.info(f"Loaded {len(config_values)} values from {config_path}")
    else:
        logger.warning(f"Config file not found: {config_path}")
except Exception as e:
    logger.error(f"Error loading env files: {e}")

# Combine values, with credentials taking precedence
all_values = {**config_values, **credentials_values}

# Extract values directly
AZURE_TENANT_ID = all_values.get("AZURE_TENANT_ID", "")
AZURE_CLIENT_ID = all_values.get("AZURE_CLIENT_ID", "")
AZURE_CLIENT_SECRET = all_values.get("AZURE_CLIENT_SECRET", "")
AZURE_OPENAI_ENDPOINT = all_values.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_EMBEDDING_MODEL = all_values.get("AZURE_EMBEDDING_MODEL", "text-embedding-3-large")
AZURE_EMBEDDING_DEPLOYMENT = all_values.get("AZURE_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
AZURE_LLM_MODEL = all_values.get("AZURE_LLM_MODEL", "gpt-4o-mini")
AZURE_LLM_DEPLOYMENT = all_values.get("AZURE_LLM_DEPLOYMENT", "gpt-4o-mini")

# Log values (masked for security)
if AZURE_TENANT_ID:
    masked_tenant = f"{AZURE_TENANT_ID[:4]}...{AZURE_TENANT_ID[-4:]}" if len(AZURE_TENANT_ID) > 8 else "***"
    logger.info(f"Using Azure tenant ID: {masked_tenant}")
else:
    logger.warning("AZURE_TENANT_ID is missing")

if AZURE_CLIENT_ID:
    masked_client = f"{AZURE_CLIENT_ID[:4]}...{AZURE_CLIENT_ID[-4:]}" if len(AZURE_CLIENT_ID) > 8 else "***"
    logger.info(f"Using Azure client ID: {masked_client}")
else:
    logger.warning("AZURE_CLIENT_ID is missing")

if AZURE_CLIENT_SECRET:
    logger.info(f"Azure client secret is set (length: {len(AZURE_CLIENT_SECRET)} characters)")
else:
    logger.warning("AZURE_CLIENT_SECRET is missing")

logger.info(f"Using Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}")
logger.info(f"Using embedding model: {AZURE_EMBEDDING_MODEL}, deployment: {AZURE_EMBEDDING_DEPLOYMENT}")
logger.info(f"Using LLM model: {AZURE_LLM_MODEL}, deployment: {AZURE_LLM_DEPLOYMENT}")

class AzureOpenAIService:
    """Service for interacting with Azure OpenAI models using direct API calls."""
    
    def __init__(self):
        """Initialize the Azure OpenAI service."""
        self.tenant_id = AZURE_TENANT_ID
        self.client_id = AZURE_CLIENT_ID
        self.client_secret = AZURE_CLIENT_SECRET
        self.endpoint = AZURE_OPENAI_ENDPOINT
        self.embedding_model = AZURE_EMBEDDING_MODEL
        self.embedding_deployment = AZURE_EMBEDDING_DEPLOYMENT
        self.llm_model = AZURE_LLM_MODEL
        self.llm_deployment = AZURE_LLM_DEPLOYMENT
        self.api_version = "2023-05-15"
        
        # Show the actual values being used (partially masked for security)
        masked_tenant = f"{self.tenant_id[:4]}...{self.tenant_id[-4:]}" if len(self.tenant_id) > 8 else "***"
        masked_client = f"{self.client_id[:4]}...{self.client_id[-4:]}" if len(self.client_id) > 8 else "***"
        logger.info(f"AzureOpenAIService initialized with:")
        logger.info(f"  - Tenant ID: {masked_tenant}")
        logger.info(f"  - Client ID: {masked_client}") 
        logger.info(f"  - Client secret length: {len(self.client_secret)} characters")
        logger.info(f"  - OpenAI endpoint: {self.endpoint}")
        
        # Get an initial token
        logger.info("Acquiring Azure AD token...")
        self.token = get_azure_token_cached(
            tenant_id=self.tenant_id,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scope="https://cognitiveservices.azure.com/.default"
        )
        
        if not self.token:
            logger.error("Failed to get Azure AD token for OpenAI. Check your Azure AD credentials.")
            raise ValueError("Failed to get Azure AD token for OpenAI")
        
        logger.info("Successfully acquired Azure AD token")
        logger.info(f"AzureOpenAIService initialized successfully")
    
    def refresh_tokens(self):
        """Refresh the Azure tokens."""
        logger.info("Refreshing Azure AD token...")
        token = get_azure_token_cached(
            tenant_id=self.tenant_id,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scope="https://cognitiveservices.azure.com/.default"
        )
        if token:
            self.token = token
            logger.info("Azure OpenAI token refreshed")
            return True
        else:
            logger.error("Failed to refresh Azure OpenAI token")
            return False
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts using direct Azure OpenAI API calls.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        try:
            # Return empty list if no texts provided
            if not texts:
                return []
                
            # Create URL for embeddings API
            url = f"{self.endpoint}/openai/deployments/{self.embedding_deployment}/embeddings?api-version={self.api_version}"
            
            # Request headers
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.token}"
            }
            
            # Batch the requests to avoid overloading the API
            # Process in chunks of 20 texts at a time
            batch_size = 20
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # Prepare request data
                payload = {
                    "input": batch_texts,
                    "dimensions": 3072  # Maximum dimensions for text-embedding-3-large
                }
                
                # Make the request
                response = requests.post(url, headers=headers, json=payload)
                
                # Check for successful response
                if response.status_code == 200:
                    result = response.json()
                    batch_embeddings = [item["embedding"] for item in result["data"]]
                    all_embeddings.extend(batch_embeddings)
                    logger.debug(f"Generated embeddings for batch of {len(batch_texts)} texts")
                elif response.status_code == 401:
                    logger.info("Token might be expired, attempting to refresh...")
                    if self.refresh_tokens():
                        # Update headers with new token
                        headers["Authorization"] = f"Bearer {self.token}"
                        
                        # Retry the request
                        response = requests.post(url, headers=headers, json=payload)
                        
                        if response.status_code == 200:
                            result = response.json()
                            batch_embeddings = [item["embedding"] for item in result["data"]]
                            all_embeddings.extend(batch_embeddings)
                            logger.debug(f"Generated embeddings for batch of {len(batch_texts)} texts after token refresh")
                        else:
                            logger.error(f"Error generating embeddings after token refresh: {response.status_code}, {response.text}")
                            raise Exception(f"Error generating embeddings: {response.status_code}, {response.text}")
                else:
                    logger.error(f"Error generating embeddings: {response.status_code}, {response.text}")
                    raise Exception(f"Error generating embeddings: {response.status_code}, {response.text}")
                
                # Add a small delay between batches to avoid rate limiting
                if i + batch_size < len(texts):
                    time.sleep(0.5)
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate embeddings for a single text using direct Azure OpenAI API calls.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        try:
            # Create URL for embeddings API
            url = f"{self.endpoint}/openai/deployments/{self.embedding_deployment}/embeddings?api-version={self.api_version}"
            
            # Request headers
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.token}"
            }
            
            # Prepare request data
            payload = {
                "input": text,
                "dimensions": 3072  # Maximum dimensions for text-embedding-3-large
            }
            
            # Make the request
            response = requests.post(url, headers=headers, json=payload)
            
            # Check for successful response
            if response.status_code == 200:
                result = response.json()
                embedding = result["data"][0]["embedding"]
                return embedding
            elif response.status_code == 401:
                logger.info("Token might be expired, attempting to refresh...")
                if self.refresh_tokens():
                    # Update headers with new token
                    headers["Authorization"] = f"Bearer {self.token}"
                    
                    # Retry the request
                    response = requests.post(url, headers=headers, json=payload)
                    
                    if response.status_code == 200:
                        result = response.json()
                        embedding = result["data"][0]["embedding"]
                        return embedding
                    else:
                        logger.error(f"Error generating embedding after token refresh: {response.status_code}, {response.text}")
                        raise Exception(f"Error generating embedding: {response.status_code}, {response.text}")
            else:
                logger.error(f"Error generating embedding: {response.status_code}, {response.text}")
                raise Exception(f"Error generating embedding: {response.status_code}, {response.text}")
                
        except Exception as e:
            logger.error(f"Error generating single embedding: {e}")
            raise
    
    async def generate_completion(self, 
                                messages: List[Dict[str, str]], 
                                temperature: float = 0.0,
                                max_tokens: int = 2000) -> str:
        """
        Generate a completion using direct Azure OpenAI API calls.
        
        Args:
            messages: List of messages (system, user, assistant)
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        try:
            # Create URL for completions API
            url = f"{self.endpoint}/openai/deployments/{self.llm_deployment}/chat/completions?api-version={self.api_version}"
            
            # Request headers
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.token}"
            }
            
            # Prepare request data
            payload = {
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "model": self.llm_model
            }
            
            # Make the request
            response = requests.post(url, headers=headers, json=payload)
            
            # Check for successful response
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                return content
            elif response.status_code == 401:
                logger.info("Token might be expired, attempting to refresh...")
                if self.refresh_tokens():
                    # Update headers with new token
                    headers["Authorization"] = f"Bearer {self.token}"
                    
                    # Retry the request
                    response = requests.post(url, headers=headers, json=payload)
                    
                    if response.status_code == 200:
                        result = response.json()
                        content = result["choices"][0]["message"]["content"]
                        return content
                    else:
                        logger.error(f"Error generating completion after token refresh: {response.status_code}, {response.text}")
                        raise Exception(f"Error generating completion: {response.status_code}, {response.text}")
            else:
                logger.error(f"Error generating completion: {response.status_code}, {response.text}")
                raise Exception(f"Error generating completion: {response.status_code}, {response.text}")
                
        except Exception as e:
            logger.error(f"Error generating completion: {e}")
            raise
