"""
Azure OpenAI service with direct token provider that exactly matches your working code.
"""

import os
import logging
import time
from typing import List, Dict, Any, Optional
from dotenv import dotenv_values
from openai import AzureOpenAI
from app.core.auth_helper import get_direct_token_provider

logger = logging.getLogger(__name__)

# Load environment variables
ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"

# Load environment variables directly
config_values = {}
creds_values = {}

try:
    if os.path.isfile(CONFIG_PATH):
        logger.info(f"Loading configuration from {CONFIG_PATH}")
        config_values = dotenv_values(CONFIG_PATH)
    else:
        logger.warning(f"Config file not found: {CONFIG_PATH}")
except Exception as e:
    logger.error(f"Error loading config file: {e}")

try:
    if os.path.isfile(CREDS_PATH):
        logger.info(f"Loading credentials from {CREDS_PATH}")
        creds_values = dotenv_values(CREDS_PATH)
    else:
        logger.warning(f"Credentials file not found: {CREDS_PATH}")
except Exception as e:
    logger.error(f"Error loading credentials file: {e}")

# Combine both sets of values
all_values = {**config_values, **creds_values}

# Extract values directly
AZURE_API_VERSION = all_values.get("AZURE_API_VERSION", "2023-05-15")
AZURE_OPENAI_ENDPOINT = all_values.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_EMBEDDING_MODEL = all_values.get("AZURE_EMBEDDING_MODEL", "text-embedding-3-large")
AZURE_EMBEDDING_DEPLOYMENT = all_values.get("AZURE_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
AZURE_LLM_MODEL = all_values.get("AZURE_LLM_MODEL", "gpt-4o-mini")
AZURE_LLM_DEPLOYMENT = all_values.get("AZURE_LLM_DEPLOYMENT", "gpt-4o-mini")

class AzureOpenAIService:
    """Service for interacting with Azure OpenAI models using direct token provider."""
    
    def __init__(self):
        """Initialize the Azure OpenAI service."""
        self.api_version = AZURE_API_VERSION
        self.endpoint = AZURE_OPENAI_ENDPOINT
        self.embedding_model = AZURE_EMBEDDING_MODEL
        self.embedding_deployment = AZURE_EMBEDDING_DEPLOYMENT
        self.llm_model = AZURE_LLM_MODEL
        self.llm_deployment = AZURE_LLM_DEPLOYMENT
        
        logger.info(f"AzureOpenAIService initializing with:")
        logger.info(f"  - API Version: {self.api_version}")
        logger.info(f"  - Endpoint: {self.endpoint}")
        logger.info(f"  - Embedding Model: {self.embedding_model}")
        logger.info(f"  - Embedding Deployment: {self.embedding_deployment}")
        logger.info(f"  - LLM Model: {self.llm_model}")
        logger.info(f"  - LLM Deployment: {self.llm_deployment}")
        
        # Get the token provider
        self.token_provider = get_direct_token_provider()
        
        # Initialize the client
        self._initialize_client()
        logger.info("AzureOpenAIService initialized successfully")
    
    def _initialize_client(self):
        """Initialize the Azure OpenAI client."""
        try:
            # Create client with token provider
            # Using the exact pattern from your working code
            self.client = AzureOpenAI(
                azure_endpoint=self.endpoint,
                api_version=self.api_version,
                azure_ad_token_provider=self.token_provider
            )
            
            logger.info("Azure OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing Azure OpenAI client: {e}")
            raise
    
    def refresh_tokens(self):
        """Refresh the token provider and client."""
        try:
            logger.info("Refreshing token provider...")
            self.token_provider = get_direct_token_provider(force_refresh=True)
            
            # Re-initialize client with new token provider
            self._initialize_client()
            
            logger.info("Token provider and client refreshed successfully")
            return True
        except Exception as e:
            logger.error(f"Error refreshing tokens: {e}")
            return False
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        if not texts:
            return []
        
        try:
            logger.info(f"Generating embeddings for {len(texts)} texts")
            
            # Process in small batches to avoid rate limiting
            batch_size = 3  # Very small batch size for reliability
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (len(texts) + batch_size - 1) // batch_size
                
                logger.info(f"Processing batch {batch_num}/{total_batches}")
                
                try:
                    # Generate embeddings
                    response = self.client.embeddings.create(
                        input=batch_texts,
                        model=self.embedding_deployment
                    )
                    
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    
                    logger.info(f"Successfully processed batch {batch_num}/{total_batches}")
                    
                except Exception as batch_error:
                    logger.error(f"Error processing batch {batch_num}: {batch_error}")
                    
                    # Try refreshing token and retrying
                    try:
                        self.refresh_tokens()
                        
                        # Retry the batch
                        response = self.client.embeddings.create(
                            input=batch_texts,
                            model=self.embedding_deployment
                        )
                        
                        batch_embeddings = [item.embedding for item in response.data]
                        all_embeddings.extend(batch_embeddings)
                        
                        logger.info(f"Successfully processed batch {batch_num} after token refresh")
                    except Exception as retry_error:
                        logger.error(f"Retry failed for batch {batch_num}: {retry_error}")
                        # Add empty vectors for failed embeddings
                        for _ in batch_texts:
                            all_embeddings.append([0.0] * 1536)  # Default embedding dimension
                
                # Add a significant delay between batches to avoid rate limits
                if i + batch_size < len(texts):
                    delay = 2.0  # 2 seconds between batches
                    logger.info(f"Waiting {delay} seconds before next batch...")
                    time.sleep(delay)
            
            logger.info(f"Successfully generated {len(all_embeddings)} embeddings")
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        try:
            logger.info("Generating single embedding")
            
            # Use the batch function with a single text
            embeddings = await self.generate_embeddings([text])
            
            if embeddings and len(embeddings) > 0:
                return embeddings[0]
            else:
                logger.error("Failed to generate embedding")
                return [0.0] * 1536  # Default embedding dimension
                
        except Exception as e:
            logger.error(f"Error generating single embedding: {e}")
            return [0.0] * 1536  # Default embedding dimension
    
    async def generate_completion(self, 
                                messages: List[Dict[str, str]], 
                                temperature: float = 0.0,
                                max_tokens: int = 2000) -> str:
        """
        Generate a completion.
        
        Args:
            messages: List of messages
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        try:
            logger.info("Generating completion")
            
            # Generate completion
            response = self.client.chat.completions.create(
                model=self.llm_deployment,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            logger.info("Completion generated successfully")
            
            return content
            
        except Exception as e:
            logger.error(f"Error generating completion: {e}")
            
            # Try refreshing token and retrying
            try:
                self.refresh_tokens()
                
                # Retry the completion
                response = self.client.chat.completions.create(
                    model=self.llm_deployment,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                content = response.choices[0].message.content
                logger.info("Completion generated successfully after token refresh")
                
                return content
            except Exception as retry_error:
                logger.error(f"Retry failed: {retry_error}")
                return "Error generating completion. Please try again."
