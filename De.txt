"""
FalkorDB Polars Loader - Blazing Fast for Massive Datasets
Polars is 5-10x faster than pandas for large data operations
Ideal for datasets > 1M rows

Install: pip install polars pandas openpyxl
Note: fastexcel is optional for even faster Excel reading
"""

import polars as pl
import pandas as pd
import asyncio
from falkordb.asyncio import FalkorDB
from redis.asyncio import BlockingConnectionPool
from typing import Dict, List, Set
from collections import defaultdict
import logging
from datetime import datetime
from tqdm.asyncio import tqdm as async_tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PolarsFalkorDBLoader:
    """
    Ultra-fast loader using Polars for data processing
    Polars advantages:
    - 5-10x faster than pandas for large datasets
    - Better memory efficiency
    - Native parallel processing
    - Lazy evaluation for query optimization
    """
    
    def __init__(
        self,
        host: str = 'localhost',
        port: int = 6379,
        password: str = None,
        graph_name: str = 'DataTransferGraph',
        batch_size: int = 15000,  # Can handle larger batches
        max_connections: int = 16
    ):
        self.host = host
        self.port = port
        self.password = password
        self.graph_name = graph_name
        self.batch_size = batch_size
        self.max_connections = max_connections
        
        self.pool = None
        self.db = None
        self.graph = None
        
        self.stats = {
            'total_cases': 0,
            'nodes_created': defaultdict(int),
            'relationships_created': defaultdict(int),
            'preprocessing_time': 0,
            'loading_time': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def connect(self):
        """Initialize connection pool"""
        logger.info(f"Connecting to FalkorDB with {self.max_connections} connections")
        
        self.pool = BlockingConnectionPool(
            host=self.host,
            port=self.port,
            password=self.password,
            max_connections=self.max_connections,
            timeout=None,
            decode_responses=True
        )
        
        self.db = FalkorDB(connection_pool=self.pool)
        self.graph = self.db.select_graph(self.graph_name)
    
    async def close(self):
        if self.pool:
            await self.pool.disconnect()
    
    def load_excel_polars(self, file_path: str) -> pl.DataFrame:
        """
        Load Excel with Polars - faster than pandas
        Uses fastexcel for even better performance
        """
        logger.info(f"Loading Excel with Polars: {file_path}")
        
        try:
            # Try fastexcel for maximum speed (if available)
            from fastexcel import read_excel
            # fastexcel returns list of sheets, get first sheet
            excel_sheets = read_excel(file_path)
            # Convert to polars - get the first sheet
            if hasattr(excel_sheets, 'load_sheet'):
                # Newer fastexcel API
                df = excel_sheets.load_sheet(0).to_polars()
            else:
                # Alternative API
                df = pl.from_pandas(excel_sheets.to_pandas())
            logger.info(f"  Using fastexcel (fastest)")
        except (ImportError, AttributeError) as e:
            # Fallback: Load with pandas then convert to Polars
            logger.info(f"  Fastexcel not available, using pandas->polars conversion")
            import pandas as pd
            pdf = pd.read_excel(file_path, engine='openpyxl')
            df = pl.from_pandas(pdf)
            logger.info(f"  Using pandas->polars conversion")
        
        logger.info(f"Loaded {len(df):,} rows")
        return df
    
    def preprocess_polars_fast(self, df: pl.DataFrame) -> List[Dict]:
        """
        Ultra-fast preprocessing with Polars vectorized operations
        10x faster than pandas for large datasets
        """
        logger.info(f"Preprocessing {len(df):,} records with Polars...")
        start_time = datetime.now()
        
        # Define split and deduplicate function
        def split_and_dedupe(s: str) -> List[str]:
            if not s or s == 'null' or s == 'None' or pd.isna(s):
                return []
            values = [v.strip() for v in str(s).split('|') if v.strip()]
            # Deduplicate while preserving order
            seen = set()
            result = []
            for v in values:
                if v not in seen:
                    seen.add(v)
                    result.append(v)
            return result
        
        # Vectorized operations on all columns
        logger.info("  Processing with vectorized Polars operations...")
        
        # Fill nulls and convert to string - handle all columns
        string_columns = df.columns
        df = df.with_columns([
            pl.col(col).cast(pl.Utf8).fill_null("")
            for col in string_columns
        ])
        
        # Apply split function to pipe-delimited columns
        logger.info("  Splitting pipe-delimited columns...")
        df = df.with_columns([
            pl.col("ReceivingJurisdictions").map_elements(
                split_and_dedupe, 
                return_dtype=pl.List(pl.Utf8),
                skip_nulls=False
            ).alias("receiving_jurisdictions"),
            pl.col("LegalProcessingPurposeNames").map_elements(
                split_and_dedupe,
                return_dtype=pl.List(pl.Utf8),
                skip_nulls=False
            ).alias("purposes"),
            pl.col("PersonalDataCategoryNames").map_elements(
                split_and_dedupe,
                return_dtype=pl.List(pl.Utf8),
                skip_nulls=False
            ).alias("personal_data_categories"),
            pl.col("PersonalDataNames").map_elements(
                split_and_dedupe,
                return_dtype=pl.List(pl.Utf8),
                skip_nulls=False
            ).alias("personal_data"),
            pl.col("CategoryNames").map_elements(
                split_and_dedupe,
                return_dtype=pl.List(pl.Utf8),
                skip_nulls=False
            ).alias("categories"),
        ])
        
        # Convert to dictionaries (Polars is very fast at this)
        logger.info("  Converting to records...")
        records = df.select([
            pl.col("CaseId").alias("case_id"),
            pl.when(pl.col("EimId") != "").then(pl.col("EimId")).otherwise(None).alias("eim_id"),
            pl.when(pl.col("BusinessApp_Id") != "").then(pl.col("BusinessApp_Id")).otherwise(None).alias("business_app_id"),
            pl.when(pl.col("OriginatingCountryName") != "").then(pl.col("OriginatingCountryName")).otherwise(None).alias("originating_country"),
            pl.col("receiving_jurisdictions"),
            pl.col("purposes"),
            pl.col("personal_data_categories"),
            pl.col("personal_data"),
            pl.col("categories"),
            pl.when(pl.col("pia_module") != "").then(pl.col("pia_module")).otherwise(None).alias("pia_module"),
            pl.when(pl.col("tia_module") != "").then(pl.col("tia_module")).otherwise(None).alias("tia_module"),
            pl.when(pl.col("hrpr_module") != "").then(pl.col("hrpr_module")).otherwise(None).alias("hrpr_module"),
        ]).to_dicts()
        
        duration = (datetime.now() - start_time).total_seconds()
        self.stats['preprocessing_time'] = duration
        
        logger.info(f"Preprocessed {len(records):,} records in {duration:.2f}s")
        logger.info(f"  Speed: {len(records)/duration:,.0f} records/second")
        
        return records
    
    def collect_unique_entities(self, records: List[Dict]) -> Dict[str, Set]:
        """Collect unique entities"""
        logger.info("Collecting unique entities...")
        
        entities = {
            'countries': set(),
            'jurisdictions': set(),
            'purposes': set(),
            'personal_data_categories': set(),
            'personal_data': set(),
            'categories': set(),
        }
        
        for record in records:
            if record['originating_country']:
                entities['countries'].add(record['originating_country'])
            entities['jurisdictions'].update(record['receiving_jurisdictions'])
            entities['purposes'].update(record['purposes'])
            entities['personal_data_categories'].update(record['personal_data_categories'])
            entities['personal_data'].update(record['personal_data'])
            entities['categories'].update(record['categories'])
        
        for entity_type, values in entities.items():
            logger.info(f"  {entity_type}: {len(values):,}")
        
        return entities
    
    async def create_indexes(self):
        """Create indexes"""
        indexes = [
            "CREATE INDEX FOR (c:Case) ON (c.case_id)",
            "CREATE INDEX FOR (c:Country) ON (c.name)",
            "CREATE INDEX FOR (j:Jurisdiction) ON (j.name)",
            "CREATE INDEX FOR (p:Purpose) ON (p.name)",
            "CREATE INDEX FOR (pdc:PersonalDataCategory) ON (pdc.name)",
            "CREATE INDEX FOR (pd:PersonalData) ON (pd.name)",
            "CREATE INDEX FOR (cat:Category) ON (cat.name)",
        ]
        
        tasks = [self._create_index(idx) for idx in indexes]
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _create_index(self, query: str):
        try:
            await self.graph.query(query)
        except:
            pass
    
    async def bulk_create_nodes(self, node_label: str, property_name: str, values: List[str]):
        """Bulk create nodes"""
        if not values:
            return
        
        logger.info(f"Creating {len(values):,} {node_label} nodes...")
        
        tasks = []
        for i in range(0, len(values), self.batch_size):
            batch = values[i:i + self.batch_size]
            tasks.append(self._create_node_batch(node_label, property_name, batch))
        
        for task in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f"{node_label}"):
            count = await task
            self.stats['nodes_created'][node_label] += count
    
    async def _create_node_batch(self, node_label: str, property_name: str, batch: List[str]) -> int:
        query = f"""
        UNWIND $values AS value
        MERGE (n:{node_label} {{{property_name}: value}})
        RETURN count(n) as count
        """
        
        try:
            result = await self.graph.query(query, params={'values': batch})
            return result.result_set[0][0] if result.result_set else 0
        except Exception as e:
            logger.error(f"Error: {e}")
            return 0
    
    async def bulk_create_cases_and_relationships(self, records: List[Dict]):
        """Bulk create cases and relationships"""
        logger.info("Creating cases and relationships...")
        
        tasks = []
        for i in range(0, len(records), self.batch_size):
            batch = records[i:i + self.batch_size]
            tasks.append(self._process_batch(batch))
        
        for task in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Processing"):
            result = await task
            self.stats['nodes_created']['Case'] += result['cases']
            self.stats['relationships_created']['total'] += result['relationships']
    
    async def _process_batch(self, batch: List[Dict]) -> Dict[str, int]:
        """Process a batch"""
        # Create cases
        case_data = [
            {
                'case_id': r['case_id'],
                'eim_id': r['eim_id'],
                'business_app_id': r['business_app_id'],
                'pia_module': r['pia_module'],
                'tia_module': r['tia_module'],
                'hrpr_module': r['hrpr_module']
            }
            for r in batch
        ]
        
        query = """
        UNWIND $cases AS case_data
        MERGE (c:Case {case_id: case_data.case_id})
        SET c.eim_id = case_data.eim_id,
            c.business_app_id = case_data.business_app_id,
            c.pia_module = case_data.pia_module,
            c.tia_module = case_data.tia_module,
            c.hrpr_module = case_data.hrpr_module
        RETURN count(c) as count
        """
        
        try:
            result = await self.graph.query(query, params={'cases': case_data})
            case_count = result.result_set[0][0] if result.result_set else 0
        except Exception as e:
            logger.error(f"Error: {e}")
            case_count = 0
        
        # Create relationships
        rel_count = await self._create_relationships(batch)
        
        return {'cases': case_count, 'relationships': rel_count}
    
    async def _create_relationships(self, batch: List[Dict]) -> int:
        """Create relationships"""
        rel_data = []
        
        for record in batch:
            case_id = record['case_id']
            
            if record['originating_country']:
                rel_data.append({'case_id': case_id, 'type': 'country', 'value': record['originating_country']})
            
            for v in record['receiving_jurisdictions']:
                rel_data.append({'case_id': case_id, 'type': 'jurisdiction', 'value': v})
            
            for v in record['purposes']:
                rel_data.append({'case_id': case_id, 'type': 'purpose', 'value': v})
            
            for v in record['personal_data_categories']:
                rel_data.append({'case_id': case_id, 'type': 'pdc', 'value': v})
            
            for v in record['personal_data']:
                rel_data.append({'case_id': case_id, 'type': 'pd', 'value': v})
            
            for v in record['categories']:
                rel_data.append({'case_id': case_id, 'type': 'category', 'value': v})
        
        query = """
        UNWIND $rels AS rel
        MATCH (c:Case {case_id: rel.case_id})
        CALL {
            WITH c, rel
            WITH c, rel WHERE rel.type = 'country'
            MATCH (t:Country {name: rel.value})
            MERGE (c)-[:ORIGINATES_FROM]->(t)
            RETURN 1 as cnt
            UNION
            WITH c, rel
            WITH c, rel WHERE rel.type = 'jurisdiction'
            MATCH (t:Jurisdiction {name: rel.value})
            MERGE (c)-[:TRANSFERS_TO]->(t)
            RETURN 1 as cnt
            UNION
            WITH c, rel
            WITH c, rel WHERE rel.type = 'purpose'
            MATCH (t:Purpose {name: rel.value})
            MERGE (c)-[:HAS_PURPOSE]->(t)
            RETURN 1 as cnt
            UNION
            WITH c, rel
            WITH c, rel WHERE rel.type = 'pdc'
            MATCH (t:PersonalDataCategory {name: rel.value})
            MERGE (c)-[:HAS_PERSONAL_DATA_CATEGORY]->(t)
            RETURN 1 as cnt
            UNION
            WITH c, rel
            WITH c, rel WHERE rel.type = 'pd'
            MATCH (t:PersonalData {name: rel.value})
            MERGE (c)-[:HAS_PERSONAL_DATA]->(t)
            RETURN 1 as cnt
            UNION
            WITH c, rel
            WITH c, rel WHERE rel.type = 'category'
            MATCH (t:Category {name: rel.value})
            MERGE (c)-[:HAS_CATEGORY]->(t)
            RETURN 1 as cnt
        }
        RETURN sum(cnt) as total
        """
        
        try:
            result = await self.graph.query(query, params={'rels': rel_data})
            return result.result_set[0][0] if result.result_set else 0
        except:
            return 0
    
    async def load_data(self, file_path: str):
        """Main loading method"""
        self.stats['start_time'] = datetime.now()
        
        try:
            await self.connect()
            
            # Load with Polars
            df = self.load_excel_polars(file_path)
            
            # Preprocess
            records = self.preprocess_polars_fast(df)
            self.stats['total_cases'] = len(records)
            
            # Create indexes
            await self.create_indexes()
            
            # Collect entities
            entities = self.collect_unique_entities(records)
            
            load_start = datetime.now()
            
            # Create nodes
            await asyncio.gather(
                self.bulk_create_nodes('Country', 'name', list(entities['countries'])),
                self.bulk_create_nodes('Jurisdiction', 'name', list(entities['jurisdictions'])),
                self.bulk_create_nodes('Purpose', 'name', list(entities['purposes'])),
                self.bulk_create_nodes('PersonalDataCategory', 'name', list(entities['personal_data_categories'])),
                self.bulk_create_nodes('PersonalData', 'name', list(entities['personal_data'])),
                self.bulk_create_nodes('Category', 'name', list(entities['categories'])),
            )
            
            # Create cases
            await self.bulk_create_cases_and_relationships(records)
            
            self.stats['loading_time'] = (datetime.now() - load_start).total_seconds()
            self.stats['end_time'] = datetime.now()
            
            self.print_statistics()
            
        except Exception as e:
            logger.error(f"Error: {e}")
            raise
        finally:
            await self.close()
    
    def print_statistics(self):
        """Print statistics"""
        total = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        logger.info("\n" + "="*70)
        logger.info("POLARS LOADER STATISTICS")
        logger.info("="*70)
        logger.info(f"Total cases: {self.stats['total_cases']:,}")
        logger.info(f"\nPreprocessing: {self.stats['preprocessing_time']:.2f}s ({self.stats['total_cases']/self.stats['preprocessing_time']:,.0f} rec/s)")
        logger.info(f"Graph loading: {self.stats['loading_time']:.2f}s ({self.stats['total_cases']/self.stats['loading_time']:,.0f} rec/s)")
        logger.info(f"Total: {total:.2f}s ({self.stats['total_cases']/total:,.0f} rec/s)")
        logger.info(f"\nNodes:")
        for k, v in sorted(self.stats['nodes_created'].items()):
            logger.info(f"  {k}: {v:,}")
        logger.info(f"\nRelationships: {self.stats['relationships_created']['total']:,}")
        logger.info("="*70)


async def main():
    CONFIG = {
        'host': 'localhost',
        'port': 6379,
        'graph_name': 'DataTransferGraph',
        'batch_size': 15000,
        'max_connections': 16,
        'excel_file': 'your_data_file.xlsx'
    }
    
    loader = PolarsFalkorDBLoader(
        host=CONFIG['host'],
        port=CONFIG['port'],
        graph_name=CONFIG['graph_name'],
        batch_size=CONFIG['batch_size'],
        max_connections=CONFIG['max_connections']
    )
    
    await loader.load_data(CONFIG['excel_file'])


if __name__ == "__main__":
    asyncio.run(main())
