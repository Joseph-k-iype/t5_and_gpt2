import json
import os
from datetime import datetime
from openai import OpenAI
from typing import Dict, Any, List, Optional
import hashlib
from rdflib import Graph, Literal, RDF, RDFS, Namespace, URIRef
from rdflib.namespace import XSD, FOAF
import logging

# ========================================
# GLOBAL CONFIGURATION
# ========================================

# Global configuration - modify these values as needed
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')
BASE_URL = "https://api.openai.com/v1"  # Change this to your custom base URL if needed
JSON_FILE_PATH = "your_file.json"  # Global path to JSON file for analysis
MODEL_NAME = "o3-mini-2025-01-31"  # Specific o3-mini model version

# ========================================
# GLOBAL INITIALIZATION
# ========================================

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client globally
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=BASE_URL
)

print(f"ü§ñ Initialized with model: {MODEL_NAME}")
print(f"üåê Base URL: {BASE_URL}")
print(f"üìÅ JSON file path: {JSON_FILE_PATH}")

class O3MiniJsonAnalyzer:
    def __init__(self):
        """Initialize analyzer with global configuration"""
        
        # Define RDF namespaces
        self.SCHEMA = Namespace("http://schema.org/")
        self.JSON_SCHEMA = Namespace("http://json-schema.org/")
        self.CUSTOM = Namespace("http://example.org/jsonschema/")
        
        # Initialize RDF graph
        self.rdf_graph = Graph()
        self.rdf_graph.bind("schema", self.SCHEMA)
        self.rdf_graph.bind("jsonschema", self.JSON_SCHEMA)
        self.rdf_graph.bind("custom", self.CUSTOM)
        
    def generate_comprehensive_analysis(self, output_dir: str = "analysis_output"):
        """Generate comprehensive analysis using only o3-mini model"""
        
        global JSON_FILE_PATH
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"üöÄ Starting O3-Mini Comprehensive Analysis...")
        print(f"üìÅ Analyzing: {JSON_FILE_PATH}")
        print(f"ü§ñ Model: {MODEL_NAME}")
        
        # Load and validate JSON
        try:
            with open(JSON_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:
                data = json.load(f)
            print(f"‚úÖ Successfully loaded JSON from {JSON_FILE_PATH}")
            print(f"üìä Data size: {len(json.dumps(data)):,} characters")
        except Exception as e:
            logger.error(f"Failed to load JSON: {e}")
            return
        
        # Generate unique analysis ID
        analysis_id = hashlib.md5(f"{JSON_FILE_PATH}_{datetime.now()}".encode()).hexdigest()[:8]
        
        # Phase 1: Deep Schema Analysis
        print("üß† Phase 1: Deep Schema Analysis with o3-mini...")
        schema_analysis = self._o3_mini_schema_analysis(data)
        
        # Phase 2: Business Domain Analysis
        print("üè¢ Phase 2: Business Domain Analysis with o3-mini...")
        business_analysis = self._o3_mini_business_analysis(data)
        
        # Phase 3: Entity Relationship Mapping
        print("üîç Phase 3: Entity Relationship Mapping with o3-mini...")
        entity_analysis = self._o3_mini_entity_analysis(data)
        
        # Phase 4: Data Quality Assessment
        print("üìä Phase 4: Data Quality Assessment with o3-mini...")
        quality_analysis = self._o3_mini_quality_analysis(data)
        
        # Phase 5: Security and Privacy Analysis
        print("üîí Phase 5: Security and Privacy Analysis with o3-mini...")
        security_analysis = self._o3_mini_security_analysis(data)
        
        # Phase 6: Performance and Optimization Analysis
        print("‚ö° Phase 6: Performance Analysis with o3-mini...")
        performance_analysis = self._o3_mini_performance_analysis(data)
        
        # Phase 7: Generate RDF Graph
        print("üåê Phase 7: Generating RDF Graph...")
        self._generate_rdf_graph(data, schema_analysis, entity_analysis)
        
        # Phase 8: Integration and Architecture Analysis
        print("üèóÔ∏è Phase 8: Integration and Architecture Analysis with o3-mini...")
        architecture_analysis = self._o3_mini_architecture_analysis(data)
        
        # Compile comprehensive report
        report_content = self._compile_comprehensive_report(
            analysis_id, schema_analysis, business_analysis, entity_analysis,
            quality_analysis, security_analysis, performance_analysis, architecture_analysis
        )
        
        # Save all outputs
        self._save_analysis_outputs(output_dir, analysis_id, report_content, data)
        
        print(f"üéâ Analysis complete! Results saved in {output_dir}/")
        return analysis_id
        
    def _call_o3_mini(self, prompt: str, phase_name: str) -> str:
        """Centralized function to call o3-mini with consistent parameters"""
        
        try:
            print(f"üîÑ Calling {MODEL_NAME} for {phase_name}...")
            
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}]
                # No temperature parameter - using model defaults
                # No max_tokens parameter - using model defaults
            )
            
            result = response.choices[0].message.content
            print(f"‚úÖ {phase_name} completed - {len(result):,} characters generated")
            return result
            
        except Exception as e:
            error_msg = f"‚ùå {phase_name} failed: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _o3_mini_schema_analysis(self, data: Any) -> str:
        """Deep schema analysis using o3-mini reasoning capabilities"""
        
        # Convert data to comprehensive schema representation (no truncation)
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As an expert data architect and schema analyst, perform a comprehensive deep analysis of this JSON schema.
        Use your advanced reasoning capabilities to identify patterns, structures, and relationships.

        COMPLETE JSON DATA (NO TRUNCATION):
        {full_data_str}

        Provide an exhaustive analysis covering:

        ## 1. STRUCTURAL ANALYSIS
        - Complete hierarchical breakdown of ALL nested structures
        - Data type patterns and consistency analysis across ALL fields
        - Array patterns and item type analysis for EVERY array
        - Null value patterns and optional field identification
        - Field naming convention analysis
        - Structural symmetry and asymmetry patterns

        ## 2. SCHEMA COMPLEXITY METRICS
        - Exact nesting depth analysis with path mappings
        - Comprehensive field count statistics by level
        - Data type distribution with percentages
        - Structural complexity score (1-10) with detailed justification
        - Cyclic reference detection
        - Schema size and memory impact analysis

        ## 3. PATTERN RECOGNITION AND REASONING
        - Recurring structural patterns with frequency analysis
        - Naming conventions analysis with consistency scoring
        - Value format patterns (dates, IDs, emails, URLs, etc.) with regex suggestions
        - Relationship indicators (foreign keys, references) with confidence levels
        - Data serialization patterns
        - Polymorphic structure detection

        ## 4. SCHEMA EVOLUTION AND VERSIONING
        - Fields that suggest versioning or schema evolution
        - Deprecated or legacy patterns identified
        - Extension points for future growth
        - Backward compatibility considerations
        - Schema migration complexity assessment

        ## 5. FORMAL SCHEMA GENERATION
        - Complete JSON Schema specification with all constraints
        - GraphQL schema equivalent with type definitions
        - SQL DDL equivalent structure with proper normalization
        - TypeScript interface definitions
        - Python dataclass equivalents

        ## 6. REASONING AND JUSTIFICATION
        - Step-by-step reasoning for each pattern identified
        - Evidence from specific data points to support conclusions
        - Confidence levels for each observation
        - Alternative interpretations considered

        Be extremely thorough and provide detailed justifications for ALL observations.
        Include specific examples from the data to support your analysis.
        Use your reasoning capabilities to think through complex relationships and patterns.
        """
        
        return self._call_o3_mini(prompt, "Deep Schema Analysis")
    
    def _o3_mini_business_analysis(self, data: Any) -> str:
        """Business domain and context analysis using o3-mini"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a business analyst and domain expert with advanced reasoning capabilities, analyze this JSON data to understand the business context and domain.

        COMPLETE DATA:
        {full_data_str}

        Use step-by-step reasoning to provide comprehensive business analysis:

        ## 1. BUSINESS DOMAIN IDENTIFICATION AND REASONING
        - Primary business domain identification with reasoning process
        - Sub-domains and business areas represented with evidence
        - Industry-specific terminology detected with confidence levels
        - Regulatory compliance indicators with applicable regulations
        - Business model implications derived from data structure

        ## 2. BUSINESS ENTITY ANALYSIS WITH REASONING
        - Core business entities identified through pattern analysis
        - Business entity hierarchies and relationships with reasoning
        - Entity lifecycle stages represented in the data
        - Key business identifiers and references with validation
        - Entity interdependencies and coupling analysis

        ## 3. BUSINESS PROCESS MAPPING AND WORKFLOW ANALYSIS
        - Business processes represented in the data with evidence
        - Workflow stages and state transitions identified
        - Decision points and business rules inferred
        - Integration touchpoints with external systems
        - Process efficiency and bottleneck indicators

        ## 4. STAKEHOLDER AND ORGANIZATIONAL ANALYSIS
        - User roles and personas represented with characteristics
        - Permission and access control patterns identified
        - Audit trail and accountability structures
        - Organizational hierarchy indicators
        - Collaboration and communication patterns

        ## 5. BUSINESS VALUE AND ANALYTICS ASSESSMENT
        - Key performance indicators present with business impact
        - Revenue/cost tracking capabilities identified
        - Analytics and reporting potential with specific use cases
        - Business intelligence opportunities with ROI potential
        - Competitive advantage indicators

        ## 6. DOMAIN-SPECIFIC RECOMMENDATIONS WITH REASONING
        - Industry best practices alignment assessment
        - Missing critical business data identified
        - Opportunities for business process optimization
        - Strategic data governance recommendations
        - Digital transformation readiness indicators

        ## 7. REASONING AND EVIDENCE
        - Detailed reasoning process for each business insight
        - Evidence mapping from data structure to business conclusions
        - Risk assessment for business interpretations
        - Alternative business models considered

        Think through each aspect systematically and provide specific examples and justifications for all findings.
        Use your reasoning capabilities to connect data patterns to business insights.
        """
        
        return self._call_o3_mini(prompt, "Business Domain Analysis")
    
    def _o3_mini_entity_analysis(self, data: Any) -> str:
        """Deep entity relationship analysis with o3-mini reasoning"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a database architect and data modeling expert, use advanced reasoning to analyze entity relationships in this JSON data.

        COMPLETE DATA:
        {full_data_str}

        Perform comprehensive entity relationship analysis with step-by-step reasoning:

        ## 1. ENTITY IDENTIFICATION AND CLASSIFICATION WITH REASONING
        - Primary entities (main business objects) with classification reasoning
        - Secondary entities (supporting/reference data) with dependency analysis
        - Bridge entities (many-to-many relationships) with junction reasoning
        - Value objects vs entities classification with detailed justification
        - Entity strength analysis (strong vs weak entities)

        ## 2. RELATIONSHIP MAPPING WITH DETAILED REASONING
        - One-to-one relationships identified with cardinality reasoning
        - One-to-many relationships with cardinality constraints and reasoning
        - Many-to-many relationships and junction patterns with complexity analysis
        - Self-referential relationships with recursive reasoning
        - Hierarchical relationships (parent-child, tree structures) with depth analysis
        - Temporal relationships and time-series patterns

        ## 3. KEY ANALYSIS WITH REASONING
        - Primary key patterns and strategies with uniqueness reasoning
        - Foreign key relationships (explicit and implicit) with referential integrity analysis
        - Composite key structures with normalization reasoning
        - Natural vs surrogate key usage with pros/cons analysis
        - Candidate key identification with reasoning

        ## 4. NORMALIZATION ANALYSIS WITH REASONING
        - Current normalization level (1NF, 2NF, 3NF, BCNF) with step-by-step analysis
        - Denormalization patterns detected with performance reasoning
        - Functional dependencies identified with dependency reasoning
        - Redundancy and consistency issues with impact analysis
        - Normal form violation analysis with correction suggestions

        ## 5. GRAPH STRUCTURE ANALYSIS WITH REASONING
        - Network topology of entity relationships with connectivity analysis
        - Centrality analysis (which entities are most connected) with business reasoning
        - Clustering and modularity patterns with cohesion analysis
        - Data flow direction analysis with dependency reasoning
        - Critical path analysis through entity relationships

        ## 6. CONCEPTUAL DATA MODEL WITH REASONING
        - Entity-Relationship Diagram description with relationship reasoning
        - Attribute classification (required, optional, calculated) with reasoning
        - Domain constraints and business rules with validation reasoning
        - Data integrity constraints with enforcement reasoning
        - Conceptual vs logical vs physical model considerations

        ## 7. IMPLEMENTATION PATTERNS WITH REASONING
        - Aggregation vs composition patterns with ownership reasoning
        - Inheritance hierarchies detected with polymorphism analysis
        - Polymorphic data patterns with type reasoning
        - Event sourcing or CQRS patterns with temporal reasoning
        - Microservice boundary implications with bounded context analysis

        ## 8. ADVANCED RELATIONSHIP REASONING
        - Transitive relationships and closure analysis
        - Relationship strength and coupling analysis
        - Circular dependency detection and resolution
        - Relationship cardinality optimization
        - Entity relationship evolution patterns

        Use step-by-step reasoning to justify all relationship identifications.
        Provide concrete examples from the data for each relationship type found.
        Think through complex dependency chains and provide detailed analysis.
        """
        
        return self._call_o3_mini(prompt, "Entity Relationship Analysis")
    
    def _o3_mini_quality_analysis(self, data: Any) -> str:
        """Comprehensive data quality assessment using o3-mini"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a data quality specialist with advanced reasoning capabilities, perform comprehensive data quality analysis on this JSON data.

        COMPLETE DATA:
        {full_data_str}

        Analyze data quality across all dimensions with detailed reasoning:

        ## 1. COMPLETENESS ANALYSIS WITH REASONING
        - Missing value patterns and percentages with statistical analysis
        - Required vs optional field analysis with business reasoning
        - Data coverage assessment with gap analysis
        - Completeness scoring by entity/section with methodology
        - Null vs empty vs missing distinction with semantic reasoning

        ## 2. ACCURACY AND VALIDITY WITH REASONING
        - Data format validation (dates, emails, URLs, etc.) with pattern matching
        - Value range validation with statistical reasoning
        - Business rule validation with constraint reasoning
        - Cross-field validation issues with dependency analysis
        - Semantic accuracy assessment with domain reasoning

        ## 3. CONSISTENCY ANALYSIS WITH REASONING
        - Naming convention consistency with pattern analysis
        - Data type consistency with type system reasoning
        - Format consistency across similar fields with standardization analysis
        - Referential integrity issues with relationship reasoning
        - Temporal consistency analysis with timeline reasoning

        ## 4. UNIQUENESS AND DUPLICATION WITH REASONING
        - Primary key uniqueness validation with collision analysis
        - Potential duplicate detection with similarity reasoning
        - Near-duplicate identification patterns with fuzzy matching analysis
        - Data redundancy assessment with normalization reasoning

        ## 5. TIMELINESS AND CURRENCY WITH REASONING
        - Timestamp patterns and currency with temporal analysis
        - Data freshness indicators with staleness reasoning
        - Update frequency patterns with change detection analysis
        - Temporal data quality issues with timeline reasoning

        ## 6. DATA QUALITY METRICS WITH REASONING
        - Overall quality score (1-100) with calculation methodology
        - Quality scores by data dimension with weighting reasoning
        - Critical quality issues prioritized with impact analysis
        - Quality improvement recommendations with cost-benefit reasoning
        - Quality trend analysis and predictive assessment

        ## 7. DATA PROFILING INSIGHTS WITH REASONING
        - Statistical summaries with distribution analysis
        - Value distribution analysis with outlier reasoning
        - Pattern anomalies with deviation analysis
        - Data skewness and bias detection with statistical reasoning

        ## 8. QUALITY ASSURANCE FRAMEWORK
        - Data quality rules and constraints with validation reasoning
        - Quality monitoring strategies with metric reasoning
        - Data cleansing recommendations with priority reasoning
        - Quality governance framework with process reasoning

        Provide specific examples of quality issues found and actionable recommendations.
        Use statistical reasoning where applicable and provide confidence levels.
        Think through complex quality interdependencies and cascading effects.
        """
        
        return self._call_o3_mini(prompt, "Data Quality Analysis")
    
    def _o3_mini_security_analysis(self, data: Any) -> str:
        """Security and privacy analysis using o3-mini"""
        
        # For security analysis, we'll analyze structure and patterns
        structure_str = self._get_structure_analysis(data)
        sample_values = self._get_sample_values_for_security(data)
        
        prompt = f"""
        As a cybersecurity and privacy expert with advanced reasoning capabilities, analyze this JSON structure and sample values for security and privacy implications.

        STRUCTURE ANALYSIS:
        {structure_str}

        SAMPLE VALUE PATTERNS (sanitized):
        {sample_values}

        Perform comprehensive security and privacy assessment with reasoning:

        ## 1. SENSITIVE DATA IDENTIFICATION WITH REASONING
        - PII (Personally Identifiable Information) patterns with classification reasoning
        - Financial data indicators with sensitivity analysis
        - Healthcare data (PHI) patterns with HIPAA reasoning
        - Authentication/credential patterns with security reasoning
        - Biometric data indicators with privacy reasoning

        ## 2. PRIVACY COMPLIANCE ANALYSIS WITH REASONING
        - GDPR compliance considerations with article-specific analysis
        - CCPA compliance requirements with regulation mapping
        - HIPAA considerations with healthcare reasoning
        - Data minimization principles with necessity reasoning
        - Cross-border data transfer implications with jurisdiction analysis

        ## 3. SECURITY VULNERABILITY ASSESSMENT WITH REASONING
        - Injection attack vectors with payload reasoning
        - Data exposure risks with threat modeling
        - Access control implications with privilege analysis
        - Audit trail adequacy with forensic reasoning
        - Encryption requirements with risk assessment

        ## 4. THREAT MODELING WITH REASONING
        - Threat actor analysis with motivation reasoning
        - Attack surface assessment with vector analysis
        - Risk prioritization with impact reasoning
        - Mitigation strategy development with cost-benefit analysis

        ## 5. DATA PROTECTION RECOMMENDATIONS WITH REASONING
        - Encryption requirements by field with sensitivity reasoning
        - Masking/anonymization strategies with utility preservation
        - Access control matrix suggestions with role-based reasoning
        - Data retention policy implications with lifecycle reasoning

        ## 6. REGULATORY COMPLIANCE WITH REASONING
        - Industry-specific compliance requirements with standard mapping
        - Cross-border data transfer considerations with legal reasoning
        - Right to be forgotten implications with implementation reasoning
        - Consent management requirements with user experience reasoning

        ## 7. SECURITY ARCHITECTURE WITH REASONING
        - Security by design principles with implementation reasoning
        - Zero trust architecture implications with verification reasoning
        - Incident response considerations with workflow reasoning
        - Security monitoring requirements with detection reasoning

        Use threat modeling methodology and provide risk-based prioritization.
        Think through attack chains and provide defense-in-depth recommendations.
        Consider both technical and procedural security controls.
        """
        
        return self._call_o3_mini(prompt, "Security and Privacy Analysis")
    
    def _o3_mini_performance_analysis(self, data: Any) -> str:
        """Performance and optimization analysis using o3-mini"""
        
        size_analysis = self._get_comprehensive_size_analysis(data)
        
        prompt = f"""
        As a performance engineer and optimization expert with advanced reasoning capabilities, analyze this JSON structure for performance implications.

        COMPREHENSIVE SIZE AND STRUCTURE ANALYSIS:
        {size_analysis}

        Provide comprehensive performance analysis with detailed reasoning:

        ## 1. SIZE AND COMPLEXITY METRICS WITH REASONING
        - Total data size and memory footprint with allocation reasoning
        - Nesting complexity impact with processing reasoning
        - Array size distributions with iteration reasoning
        - Processing complexity estimation with algorithmic analysis
        - Storage efficiency analysis with compression reasoning

        ## 2. QUERY PERFORMANCE IMPLICATIONS WITH REASONING
        - Index strategy recommendations with selectivity reasoning
        - Query optimization opportunities with execution plan analysis
        - Join performance considerations with complexity reasoning
        - Search and filter efficiency with algorithm analysis
        - Full-text search optimization with indexing reasoning

        ## 3. SERIALIZATION PERFORMANCE WITH REASONING
        - JSON parsing/serialization overhead with benchmark reasoning
        - Alternative format recommendations (protobuf, avro, etc.) with trade-off analysis
        - Compression opportunities with algorithm reasoning
        - Network transfer optimization with payload reasoning
        - Streaming vs batch processing considerations

        ## 4. STORAGE OPTIMIZATION WITH REASONING
        - Data normalization vs denormalization tradeoffs with performance reasoning
        - Partitioning strategies with distribution reasoning
        - Archival and tiering recommendations with lifecycle reasoning
        - Storage cost optimization with economics reasoning
        - Backup and recovery performance implications

        ## 5. CACHING STRATEGIES WITH REASONING
        - Cacheable data identification with volatility reasoning
        - Cache invalidation patterns with consistency reasoning
        - CDN optimization opportunities with geographic reasoning
        - Application-level caching recommendations with hit ratio analysis
        - Cache hierarchy design with performance reasoning

        ## 6. SCALABILITY ANALYSIS WITH REASONING
        - Horizontal scaling considerations with distribution reasoning
        - Sharding strategies with key distribution analysis
        - Load balancing implications with traffic reasoning
        - Bottleneck identification with performance profiling
        - Elastic scaling patterns with demand reasoning

        ## 7. REAL-TIME PROCESSING WITH REASONING
        - Stream processing opportunities with latency reasoning
        - Event-driven architecture implications with messaging reasoning
        - Real-time analytics considerations with processing reasoning
        - Microservice decomposition with boundary reasoning

        ## 8. OPTIMIZATION RECOMMENDATIONS WITH REASONING
        - Performance tuning priorities with impact reasoning
        - Resource allocation optimization with utilization analysis
        - Performance monitoring strategies with metric reasoning
        - Capacity planning with growth reasoning

        Provide specific, measurable optimization recommendations with expected impact.
        Use performance engineering principles and provide benchmarking suggestions.
        Think through performance trade-offs and provide cost-benefit analysis.
        """
        
        return self._call_o3_mini(prompt, "Performance and Optimization Analysis")
    
    def _o3_mini_architecture_analysis(self, data: Any) -> str:
        """Integration and architecture analysis using o3-mini"""
        
        full_data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        prompt = f"""
        As a solution architect and integration expert with advanced reasoning capabilities, analyze this JSON data for integration and architecture implications.

        COMPLETE DATA:
        {full_data_str}

        Perform comprehensive architecture analysis with reasoning:

        ## 1. INTEGRATION PATTERNS WITH REASONING
        - API design patterns identified with RESTful reasoning
        - Message queue integration patterns with asynchronous reasoning
        - Event-driven architecture patterns with decoupling analysis
        - Microservice boundary identification with domain reasoning
        - Data pipeline architecture with flow reasoning

        ## 2. SYSTEM ARCHITECTURE WITH REASONING
        - Layered architecture implications with separation reasoning
        - Service-oriented architecture patterns with service reasoning
        - Event sourcing architecture with temporal reasoning
        - CQRS patterns with command-query reasoning
        - Hexagonal architecture with port-adapter reasoning

        ## 3. DATA ARCHITECTURE WITH REASONING
        - Data lake vs data warehouse patterns with use case reasoning
        - Master data management implications with governance reasoning
        - Data mesh architecture with domain reasoning
        - Real-time vs batch processing with latency reasoning
        - Data lineage and provenance with traceability reasoning

        ## 4. CLOUD ARCHITECTURE WITH REASONING
        - Cloud-native patterns with scalability reasoning
        - Serverless architecture opportunities with event reasoning
        - Container orchestration implications with deployment reasoning
        - Multi-cloud strategies with vendor reasoning
        - Edge computing considerations with latency reasoning

        ## 5. INTEGRATION RECOMMENDATIONS WITH REASONING
        - API gateway patterns with traffic reasoning
        - Message broker selection with throughput reasoning
        - Data synchronization strategies with consistency reasoning
        - Legacy system integration with modernization reasoning
        - Third-party service integration with reliability reasoning

        ## 6. ARCHITECTURE QUALITY ATTRIBUTES
        - Scalability assessment with growth reasoning
        - Reliability and availability with fault tolerance reasoning
        - Security architecture with defense reasoning
        - Performance architecture with optimization reasoning
        - Maintainability with evolution reasoning

        Use architectural thinking and provide strategic recommendations.
        Consider non-functional requirements and quality attributes.
        Think through architectural trade-offs and provide decision rationale.
        """
        
        return self._call_o3_mini(prompt, "Integration and Architecture Analysis")
    
    def _get_structure_analysis(self, data: Any) -> str:
        """Get structure analysis without exposing sensitive data"""
        
        def analyze_structure(obj, path="", level=0):
            if isinstance(obj, dict):
                structure = {
                    "type": "object",
                    "level": level,
                    "path": path,
                    "property_count": len(obj),
                    "properties": {}
                }
                for key, value in obj.items():
                    structure["properties"][key] = analyze_structure(value, f"{path}.{key}" if path else key, level + 1)
                return structure
            elif isinstance(obj, list):
                return {
                    "type": "array",
                    "level": level,
                    "path": path,
                    "length": len(obj),
                    "item_structure": analyze_structure(obj[0], f"{path}[0]", level + 1) if obj else None
                }
            else:
                return {
                    "type": type(obj).__name__,
                    "level": level,
                    "path": path
                }
        
        return json.dumps(analyze_structure(data), indent=2)
    
    def _get_sample_values_for_security(self, data: Any, max_samples: int = 5) -> str:
        """Get sanitized sample values for security analysis"""
        
        samples = {}
        
        def extract_samples(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    if isinstance(value, (str, int, float, bool)):
                        if current_path not in samples:
                            samples[current_path] = []
                        # Sanitize sensitive-looking values
                        if len(samples[current_path]) < max_samples:
                            sanitized_value = self._sanitize_value(value, key)
                            samples[current_path].append(sanitized_value)
                    elif isinstance(value, (dict, list)):
                        extract_samples(value, current_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj[:max_samples]):
                    extract_samples(item, f"{path}[{i}]")
        
        extract_samples(data)
        return json.dumps(samples, indent=2)
    
    def _sanitize_value(self, value: Any, key: str) -> str:
        """Sanitize potentially sensitive values for security analysis"""
        
        if not isinstance(value, str):
            return f"<{type(value).__name__}:{len(str(value))}chars>"
        
        # Check for sensitive patterns
        key_lower = key.lower()
        sensitive_keys = ['password', 'token', 'key', 'secret', 'auth', 'credential']
        
        if any(sensitive in key_lower for sensitive in sensitive_keys):
            return f"<SENSITIVE:{len(value)}chars>"
        
        # Check for email patterns
        if '@' in value and '.' in value:
            return f"<EMAIL_PATTERN:{len(value)}chars>"
        
        # Check for ID patterns
        if len(value) > 10 and (value.isalnum() or '-' in value):
            return f"<ID_PATTERN:{len(value)}chars>"
        
        # Return truncated value for analysis
        return value[:20] + "..." if len(value) > 20 else value
    
    def _get_comprehensive_size_analysis(self, data: Any) -> str:
        """Get comprehensive size and complexity analysis"""
        
        def analyze_complexity(obj, level=0):
            if isinstance(obj, dict):
                size = 1
                max_depth = level
                for value in obj.values():
                    sub_size, sub_depth = analyze_complexity(value, level + 1)
                    size += sub_size
                    max_depth = max(max_depth, sub_depth)
                return size, max_depth
            elif isinstance(obj, list):
                size = 1
                max_depth = level
                for item in obj:
                    sub_size, sub_depth = analyze_complexity(item, level + 1)
                    size += sub_size
                    max_depth = max(max_depth, sub_depth)
                return size, max_depth
            else:
                return 1, level
        
        total_size = len(json.dumps(data, ensure_ascii=False))
        element_count, max_depth = analyze_complexity(data)
        
        # Calculate array statistics
        array_stats = self._get_array_statistics(data)
        
        return f"""
        Total JSON size: {total_size:,} characters ({total_size/1024:.1f} KB)
        Total elements: {element_count:,}
        Maximum nesting depth: {max_depth}
        Average element size: {total_size/element_count:.1f} characters
        
        Array Statistics:
        {array_stats}
        
        Memory Estimation:
        - Raw JSON: {total_size} bytes
        - Parsed object (estimate): {element_count * 50} bytes
        - With indexes (estimate): {element_count * 100} bytes
        """
    
    def _get_array_statistics(self, data: Any, path: str = "") -> str:
        """Get detailed array statistics"""
        
        arrays = []
        
        def find_arrays(obj, current_path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{current_path}.{key}" if current_path else key
                    find_arrays(value, new_path)
            elif isinstance(obj, list):
                arrays.append({
                    "path": current_path,
                    "length": len(obj),
                    "item_types": list(set(type(item).__name__ for item in obj))
                })
                for i, item in enumerate(obj):
                    find_arrays(item, f"{current_path}[{i}]")
        
        find_arrays(data)
        
        if not arrays:
            return "No arrays found"
        
        stats = []
        for arr in arrays:
            stats.append(f"  - {arr['path']}: {arr['length']} items ({', '.join(arr['item_types'])})")
        
        return "\n".join(stats)
    
    def _generate_rdf_graph(self, data: Any, schema_analysis: str, entity_analysis: str):
        """Generate comprehensive RDF graph representation"""
        
        # Create root node for the schema
        schema_uri = self.CUSTOM.JsonSchema
        self.rdf_graph.add((schema_uri, RDF.type, self.JSON_SCHEMA.Schema))
        self.rdf_graph.add((schema_uri, RDFS.label, Literal("O3-Mini JSON Schema Analysis")))
        self.rdf_graph.add((schema_uri, self.CUSTOM.analysisDate, Literal(datetime.now(), datatype=XSD.dateTime)))
        self.rdf_graph.add((schema_uri, self.CUSTOM.modelUsed, Literal(MODEL_NAME)))
        self.rdf_graph.add((schema_uri, self.CUSTOM.sourceFile, Literal(JSON_FILE_PATH)))
        
        # Add schema properties
        self._add_schema_properties_to_rdf(data, schema_uri)
        
        # Add analysis results
        self._add_analysis_results_to_rdf(schema_analysis, entity_analysis, schema_uri)
        
        # Add metadata
        self._add_metadata_to_rdf(data, schema_uri)
    
    def _add_schema_properties_to_rdf(self, data: Any, schema_uri: URIRef, path: str = ""):
        """Recursively add schema properties to RDF graph"""
        
        if isinstance(data, dict):
            for key, value in data.items():
                prop_uri = self.CUSTOM[f"property_{path}_{key}".replace(".", "_").replace("[", "_").replace("]", "_")]
                self.rdf_graph.add((schema_uri, self.CUSTOM.hasProperty, prop_uri))
                self.rdf_graph.add((prop_uri, RDFS.label, Literal(key)))
                self.rdf_graph.add((prop_uri, self.CUSTOM.propertyPath, Literal(f"{path}.{key}" if path else key)))
                
                # Add type information
                if isinstance(value, str):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, XSD.string))
                elif isinstance(value, int):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, XSD.integer))
                elif isinstance(value, float):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, XSD.decimal))
                elif isinstance(value, bool):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, XSD.boolean))
                elif isinstance(value, list):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.CUSTOM.Array))
                    if value:
                        self.rdf_graph.add((prop_uri, self.CUSTOM.arraySize, Literal(len(value))))
                elif isinstance(value, dict):
                    self.rdf_graph.add((prop_uri, self.CUSTOM.dataType, self.CUSTOM.Object))
                    self._add_schema_properties_to_rdf(value, prop_uri, f"{path}.{key}" if path else key)
    
    def _add_analysis_results_to_rdf(self, schema_analysis: str, entity_analysis: str, schema_uri: URIRef):
        """Add analysis results to RDF graph"""
        
        # Schema analysis
        schema_analysis_uri = self.CUSTOM.SchemaAnalysis
        self.rdf_graph.add((schema_uri, self.CUSTOM.hasSchemaAnalysis, schema_analysis_uri))
        self.rdf_graph.add((schema_analysis_uri, RDFS.label, Literal("O3-Mini Schema Analysis")))
        self.rdf_graph.add((schema_analysis_uri, self.CUSTOM.analysisResult, Literal(schema_analysis)))
        self.rdf_graph.add((schema_analysis_uri, self.CUSTOM.modelUsed, Literal(MODEL_NAME)))
        
        # Entity analysis
        entity_analysis_uri = self.CUSTOM.EntityAnalysis
        self.rdf_graph.add((schema_uri, self.CUSTOM.hasEntityAnalysis, entity_analysis_uri))
        self.rdf_graph.add((entity_analysis_uri, RDFS.label, Literal("O3-Mini Entity Analysis")))
        self.rdf_graph.add((entity_analysis_uri, self.CUSTOM.analysisResult, Literal(entity_analysis)))
        self.rdf_graph.add((entity_analysis_uri, self.CUSTOM.modelUsed, Literal(MODEL_NAME)))
    
    def _add_metadata_to_rdf(self, data: Any, schema_uri: URIRef):
        """Add metadata to RDF graph"""
        
        metadata_uri = self.CUSTOM.AnalysisMetadata
        self.rdf_graph.add((schema_uri, self.CUSTOM.hasMetadata, metadata_uri))
        self.rdf_graph.add((metadata_uri, RDFS.label, Literal("Analysis Metadata")))
        self.rdf_graph.add((metadata_uri, self.CUSTOM.dataSize, Literal(len(json.dumps(data)))))
        self.rdf_graph.add((metadata_uri, self.CUSTOM.maxDepth, Literal(self._get_max_depth(data))))
        self.rdf_graph.add((metadata_uri, self.CUSTOM.analysisTimestamp, Literal(datetime.now(), datatype=XSD.dateTime)))
    
    def _get_max_depth(self, obj, current_depth=0):
        """Calculate maximum nesting depth"""
        
        if isinstance(obj, dict):
            return max([self._get_max_depth(v, current_depth + 1) for v in obj.values()], default=current_depth)
        elif isinstance(obj, list):
            return max([self._get_max_depth(item, current_depth + 1) for item in obj], default=current_depth)
        else:
            return current_depth
    
    def _compile_comprehensive_report(self, analysis_id: str, schema_analysis: str, 
                                    business_analysis: str, entity_analysis: str,
                                    quality_analysis: str, security_analysis: str,
                                    performance_analysis: str, architecture_analysis: str) -> str:
        """Compile comprehensive analysis report"""
        
        report = f"""# ü§ñ O3-MINI COMPREHENSIVE JSON SCHEMA ANALYSIS REPORT

**Analysis ID:** {analysis_id}  
**Source File:** {JSON_FILE_PATH}  
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**AI Model:** {MODEL_NAME}  
**Base URL:** {BASE_URL}  

---

## üìã EXECUTIVE SUMMARY

This comprehensive analysis leverages OpenAI's latest o3-mini-2025-01-31 reasoning model to provide deep insights into the JSON schema structure, business context, and optimization opportunities. The analysis was performed without temperature or max_tokens limitations to ensure maximum reasoning depth and completeness.

---

## üß† DEEP SCHEMA ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Approach:** Advanced reasoning with step-by-step analysis
- **Data Processing:** Complete data analysis without truncation
- **Reasoning Depth:** Maximum reasoning capability utilized

### Results

{schema_analysis}

---

## üè¢ BUSINESS DOMAIN ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Focus:** Business context and domain understanding
- **Reasoning:** Pattern-to-business insight mapping

### Results

{business_analysis}

---

## üîç ENTITY RELATIONSHIP ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Approach:** Database architecture and relationship reasoning
- **Depth:** Comprehensive relationship mapping and analysis

### Results

{entity_analysis}

---

## üìä DATA QUALITY ASSESSMENT

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Framework:** Multi-dimensional quality analysis
- **Metrics:** Quantitative and qualitative assessment

### Results

{quality_analysis}

---

## üîí SECURITY & PRIVACY ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Approach:** Threat modeling and compliance analysis
- **Focus:** Risk assessment and mitigation strategies

### Results

{security_analysis}

---

## ‚ö° PERFORMANCE & OPTIMIZATION ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Framework:** Performance engineering principles
- **Scope:** End-to-end optimization analysis

### Results

{performance_analysis}

---

## üèóÔ∏è INTEGRATION & ARCHITECTURE ANALYSIS

### Analysis Methodology
- **Model:** {MODEL_NAME}
- **Perspective:** Solution architecture and integration patterns
- **Scope:** Enterprise architecture considerations

### Results

{architecture_analysis}

---

## üåê RDF GRAPH REPRESENTATION

An RDF graph has been generated representing the schema structure and relationships using the following approach:

### RDF Generation Methodology
- **Namespaces:** Schema.org, JSON Schema, Custom ontology
- **Model Used:** {MODEL_NAME}
- **Triples Generated:** Comprehensive property and relationship mapping
- **Formats:** Turtle (.ttl), RDF/XML (.rdf), JSON-LD (.jsonld)

### Use Cases for RDF Graph
- Semantic data integration and interoperability
- Automated reasoning about data relationships
- Compliance and governance tracking
- Data lineage and provenance analysis
- Knowledge graph construction

---

## üéØ ANALYSIS CONFIGURATION

### Global Configuration
- **OpenAI API Key:** {"Set" if OPENAI_API_KEY != "your-api-key-here" else "Not configured"}
- **Base URL:** {BASE_URL}
- **Model:** {MODEL_NAME}
- **JSON File:** {JSON_FILE_PATH}

### Model Parameters
- **Temperature:** Not specified (model default)
- **Max Tokens:** Not specified (model default) 
- **Reasoning Mode:** Full reasoning capability utilized

---

## üìà KEY INSIGHTS SUMMARY

### Schema Insights
[Automatically extracted from schema analysis]

### Business Insights  
[Automatically extracted from business analysis]

### Technical Insights
[Automatically extracted from technical analyses]

---

## üöÄ IMPLEMENTATION ROADMAP

### Phase 1: Immediate Actions
[High-priority recommendations from all analyses]

### Phase 2: Medium-term Improvements
[Strategic improvements and optimizations]

### Phase 3: Long-term Transformation
[Architectural and strategic transformations]

---

## üìã APPENDICES

### A. Technical Specifications
- Complete JSON Schema validation rules
- Database DDL equivalents with full normalization
- API documentation templates
- TypeScript interface definitions

### B. Implementation Guides
- Step-by-step migration strategies
- Performance optimization implementation
- Security implementation checklist
- Quality monitoring setup procedures

### C. Monitoring and Maintenance
- Continuous data quality monitoring
- Performance benchmarking frameworks
- Security monitoring strategies
- Continuous improvement processes

---

*Report generated using OpenAI o3-mini-2025-01-31 with maximum reasoning capabilities and no truncation.*
"""
        return report
    
    def _save_analysis_outputs(self, output_dir: str, analysis_id: str, report_content: str, original_data: Any):
        """Save all analysis outputs to files"""
        
        # Save comprehensive report
        report_file = f"{output_dir}/o3_mini_analysis_{analysis_id}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"üìÑ Comprehensive report saved: {report_file}")
        
        # Save RDF graph in multiple formats
        rdf_ttl_file = f"{output_dir}/schema_graph_{analysis_id}.ttl"
        rdf_xml_file = f"{output_dir}/schema_graph_{analysis_id}.rdf"
        rdf_json_file = f"{output_dir}/schema_graph_{analysis_id}.jsonld"
        
        self.rdf_graph.serialize(destination=rdf_ttl_file, format='turtle')
        self.rdf_graph.serialize(destination=rdf_xml_file, format='xml')
        self.rdf_graph.serialize(destination=rdf_json_file, format='json-ld')
        
        print(f"üåê RDF graph saved in multiple formats:")
        print(f"   - Turtle: {rdf_ttl_file}")
        print(f"   - RDF/XML: {rdf_xml_file}")
        print(f"   - JSON-LD: {rdf_json_file}")
        
        # Save configuration and metadata
        config_file = f"{output_dir}/analysis_config_{analysis_id}.json"
        config_data = {
            "analysis_id": analysis_id,
            "timestamp": datetime.now().isoformat(),
            "model_used": MODEL_NAME,
            "base_url": BASE_URL,
            "json_file_path": JSON_FILE_PATH,
            "data_size": len(json.dumps(original_data)),
            "max_depth": self._get_max_depth(original_data),
            "analysis_files": {
                "report": report_file,
                "rdf_turtle": rdf_ttl_file,
                "rdf_xml": rdf_xml_file,
                "rdf_jsonld": rdf_json_file
            },
            "configuration": {
                "temperature": "Not specified (model default)",
                "max_tokens": "Not specified (model default)",
                "reasoning_mode": "Full capability"
            }
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2)
        
        print(f"‚öôÔ∏è Configuration and metadata saved: {config_file}")

# ========================================
# MAIN EXECUTION FUNCTIONS
# ========================================

def run_comprehensive_analysis(output_dir: str = "o3_mini_analysis"):
    """Run comprehensive analysis using global configuration"""
    
    global JSON_FILE_PATH
    
    print(f"üöÄ Starting O3-Mini Comprehensive Analysis")
    print(f"üìÅ JSON File: {JSON_FILE_PATH}")
    print(f"ü§ñ Model: {MODEL_NAME}")
    print(f"üåê Base URL: {BASE_URL}")
    
    # Check if JSON file exists
    if not os.path.exists(JSON_FILE_PATH):
        print(f"‚ùå JSON file not found: {JSON_FILE_PATH}")
        print("Update the JSON_FILE_PATH global variable with the correct path")
        return None
    
    analyzer = O3MiniJsonAnalyzer()
    analysis_id = analyzer.generate_comprehensive_analysis(output_dir)
    
    return analysis_id

def update_global_config(api_key: str = None, base_url: str = None, json_path: str = None):
    """Update global configuration parameters"""
    
    global OPENAI_API_KEY, BASE_URL, JSON_FILE_PATH, client
    
    if api_key:
        OPENAI_API_KEY = api_key
        print(f"‚úÖ Updated API key")
    
    if base_url:
        BASE_URL = base_url
        print(f"‚úÖ Updated base URL: {BASE_URL}")
    
    if json_path:
        JSON_FILE_PATH = json_path
        print(f"‚úÖ Updated JSON file path: {JSON_FILE_PATH}")
    
    # Reinitialize client with new configuration
    client = OpenAI(
        api_key=OPENAI_API_KEY,
        base_url=BASE_URL
    )
    
    print("üîÑ Client reinitialized with new configuration")

def quick_analysis():
    """Run a quick analysis with current global configuration"""
    
    print(f"üöÄ Quick O3-Mini Analysis")
    print(f"üìÅ File: {JSON_FILE_PATH}")
    
    if not os.path.exists(JSON_FILE_PATH):
        print(f"‚ùå File not found: {JSON_FILE_PATH}")
        return
    
    analyzer = O3MiniJsonAnalyzer()
    
    # Load data
    with open(JSON_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:
        data = json.load(f)
    
    # Quick schema analysis
    schema_analysis = analyzer._o3_mini_schema_analysis(data)
    
    # Generate RDF
    analyzer._generate_rdf_graph(data, schema_analysis, "")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save quick report
    with open(f"quick_o3_analysis_{timestamp}.md", 'w', encoding='utf-8') as f:
        f.write(f"""# Quick O3-Mini Analysis Report

**Generated:** {datetime.now()}
**Model:** {MODEL_NAME}
**File:** {JSON_FILE_PATH}

## Schema Analysis

{schema_analysis}
""")
    
    # Save RDF
    analyzer.rdf_graph.serialize(destination=f"quick_schema_{timestamp}.ttl", format='turtle')
    
    print(f"‚úÖ Quick analysis complete! Files saved with timestamp {timestamp}")

if __name__ == "__main__":
    print("ü§ñ O3-Mini JSON Schema Analyzer Initialized")
    print("üìã Required: pip install openai rdflib")
    print(f"‚öôÔ∏è Current configuration:")
    print(f"   - Model: {MODEL_NAME}")
    print(f"   - Base URL: {BASE_URL}")
    print(f"   - JSON File: {JSON_FILE_PATH}")
    print(f"   - API Key: {'Set' if OPENAI_API_KEY != 'your-api-key-here' else 'Not configured'}")
    print()
    print("üöÄ Usage:")
    print("   run_comprehensive_analysis()  # Full analysis")
    print("   quick_analysis()              # Quick analysis")
    print("   update_global_config(...)     # Update configuration")
