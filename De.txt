"""
GDC Record Class Mapping System using OpenAI o3-mini with LangGraph ReAct Agents
Implements mixture of experts pattern with dynamic chain of thought reasoning
Uses Pydantic v2 for validation and latest LangChain/LangGraph
"""

import json
import os
from typing import List, Dict, Optional, Annotated
from operator import add
import pandas as pd
from pydantic import BaseModel, Field, ValidationError, field_validator
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import re

# ==================== GLOBAL CONFIGURATION ====================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_MODEL = "o3-mini"
REASONING_EFFORT = "high"

# Initialize global LLM instance
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    model_kwargs={"reasoning_effort": REASONING_EFFORT}
)

# ==================== PYDANTIC V2 MODELS ====================

class GDCMaster(BaseModel):
    """Pydantic model for GDC Master data"""
    data_domain: str = Field(alias="Data Domain", default="")
    gdc_name: str = Field(alias="GDC Name")
    definition: str = Field(alias="Definition", default="")
    
    class Config:
        populate_by_name = True

class ProcessInfo(BaseModel):
    """Pydantic model for Process information"""
    process_name: str = Field(alias="Process Name", default="")
    process_description: str = Field(alias="Process Description", default="")
    
    class Config:
        populate_by_name = True

class AppInfo(BaseModel):
    """Pydantic model for Application information"""
    app_id: str = Field(alias="App ID", default="")
    app_name: str = Field(alias="App Name", default="")
    app_description: str = Field(alias="App Description", default="")
    processes: List[ProcessInfo] = Field(default_factory=list, alias="Processes")
    
    class Config:
        populate_by_name = True

class PBTInfo(BaseModel):
    """Pydantic model for PBT information"""
    pbt_id: str = Field(alias="PBT ID", default="")
    pbt_name: str = Field(alias="PBT Name", default="")
    pbt_desc: str = Field(alias="PBT Desc", default="")
    apps: List[AppInfo] = Field(default_factory=list, alias="Apps")
    
    class Config:
        populate_by_name = True

class GDCWithContext(BaseModel):
    """Pydantic model for GDC with Context data"""
    gdc_id: str = Field(alias="GDC ID", default="")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    pbts: List[PBTInfo] = Field(default_factory=list, alias="PBTs")
    
    class Config:
        populate_by_name = True

class ValidationEntry(BaseModel):
    """Pydantic model for Validation data"""
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class RecordClass(BaseModel):
    """Pydantic model for Record Class data"""
    guid: str = Field(alias="Guid", default="")
    code: str = Field(alias="Code", default="")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description", default="")
    
    class Config:
        populate_by_name = True

class SemanticMatch(BaseModel):
    """Pydantic model for Semantic Match result"""
    gdc_name: str = Field(description="Name of the matched GDC")
    gdc_description: str = Field(description="Description of the matched GDC", default="")
    similarity_score: float = Field(ge=0, le=100, description="Similarity score between 0-100")
    reasoning: str = Field(description="Detailed reasoning for the match")
    
    @field_validator('similarity_score')
    @classmethod
    def validate_score(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('Similarity score must be between 0 and 100')
        return v

class SemanticMatchResponse(BaseModel):
    """Pydantic model for Semantic Matching Expert response"""
    matches: List[SemanticMatch] = Field(description="List of semantic matches")

class ContextEvidence(BaseModel):
    """Pydantic model for Context Evidence"""
    gdc_name: str = Field(description="Name of the GDC")
    context_evidence: List[str] = Field(description="List of contextual evidence", default_factory=list)
    alignment_score: float = Field(ge=0, le=100, description="Alignment score")
    confidence_adjustment: str = Field(description="Confidence adjustment with reasoning")
    reasoning: str = Field(description="Detailed reasoning for context analysis")

class ContextAnalysisResponse(BaseModel):
    """Pydantic model for Context Analysis Expert response"""
    context_analysis: List[ContextEvidence] = Field(description="List of context analyses")
    recommended_gdc: str = Field(description="Recommended GDC name")

class ValidationMatch(BaseModel):
    """Pydantic model for Validation Match"""
    gdc_name: str = Field(alias="GDC Name", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class ValidationResponse(BaseModel):
    """Pydantic model for Validation Expert response"""
    validation_found: bool = Field(description="Whether validation entry was found")
    matching_entry: Optional[ValidationMatch] = Field(None, description="Matching validation entry")
    validation_status: str = Field(description="Status: CONFIRMED/CONFLICTED/NOT_FOUND")
    validation_reasoning: str = Field(description="Detailed validation reasoning")

class FinalMappingDecision(BaseModel):
    """Pydantic model for Final Mapping Decision"""
    final_gdc_name: str = Field(description="Final selected GDC name")
    final_gdc_description: str = Field(description="Final GDC description")
    reasoning: str = Field(description="Comprehensive reasoning for the decision")
    evidence_summary: List[str] = Field(description="Summary of evidence", default_factory=list)

class MappingResult(BaseModel):
    """Pydantic model for final mapping result"""
    guid: str = Field(alias="GUID")
    code: str = Field(alias="Code")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description")
    reasoning: str = Field(alias="Reasoning")
    
    class Config:
        populate_by_name = True

# ==================== UTILITY FUNCTIONS ====================

def extract_json_from_text(text: str) -> str:
    """Extract JSON from text that might contain markdown code blocks"""
    # Remove any markdown code blocks
    text = re.sub(r'```(?:json)?\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    
    # Try to find JSON object or array
    json_match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
    if json_match:
        return json_match.group(1)
    
    return text

def parse_llm_response(response_text: str, model_class: BaseModel):
    """Parse and validate LLM response using Pydantic model"""
    try:
        json_str = extract_json_from_text(response_text)
        json_data = json.loads(json_str)
        return model_class.model_validate(json_data)
    except (json.JSONDecodeError, ValidationError) as e:
        print(f"Validation error: {e}")
        raise

def load_json_file(filepath: str, model_class: BaseModel) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except ValidationError as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str, gdc_candidates: str) -> str:
    """
    Expert for semantic similarity analysis between record and GDC candidates.
    Uses o3-mini's reasoning capabilities to find best semantic matches.
    
    Args:
        record_name: Name of the record class
        record_desc: Description of the record class
        gdc_candidates: JSON string of GDC candidates
    
    Returns:
        JSON string with top 5 semantic matches and similarity scores
    """
    prompt = f"""You are a semantic similarity expert specializing in data classification and entity matching.

TASK: Analyze the semantic similarity between the given Record Class and GDC candidates.

RECORD CLASS:
Name: {record_name}
Description: {record_desc}

GDC CANDIDATES:
{gdc_candidates}

INSTRUCTIONS:
1. Carefully analyze the record name and description
2. Compare against each GDC name and definition
3. Consider domain overlap, terminology similarity, and conceptual alignment
4. Score each match from 0-100 based on semantic similarity (for internal ranking only, not as confidence)
5. Return top 5 matches with detailed reasoning

OUTPUT FORMAT - Must be valid JSON matching this exact structure:
{{
  "matches": [
    {{
      "gdc_name": "GDC Name",
      "gdc_description": "GDC Definition",
      "similarity_score": 85.5,
      "reasoning": "Detailed explanation of why this is a strong match based on semantic overlap, domain alignment, and terminology consistency"
    }}
  ]
}}

CRITICAL: Return ONLY valid JSON, no additional text or markdown. Think step-by-step and provide thorough reasoning for each match."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = SemanticMatchResponse(matches=[])
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, gdc_with_context: str, top_matches: str) -> str:
    """
    Expert for analyzing GDC context (PBTs, Apps, Processes) to refine matching.
    
    Args:
        record_name: Name of the record class
        record_desc: Description of the record class
        gdc_with_context: JSON string of GDC context data
        top_matches: JSON string of top semantic matches
    
    Returns:
        JSON string with context-enriched analysis
    """
    prompt = f"""You are a context analysis expert specializing in hierarchical data relationships.

TASK: Analyze the contextual evidence to validate and refine GDC matches.

RECORD CLASS:
Name: {record_name}
Description: {record_desc}

TOP SEMANTIC MATCHES:
{top_matches}

GDC CONTEXT DATA (with PBTs, Apps, Processes):
{gdc_with_context}

INSTRUCTIONS:
1. For each top match, find its context in the GDC context data
2. Analyze PBTs, Applications, and Processes associated with the GDC
3. Determine if the record class aligns with these contexts
4. Look for domain-specific terminology and use cases
5. Provide evidence-based analysis and adjustments

OUTPUT FORMAT - Must be valid JSON matching this exact structure:
{{
  "context_analysis": [
    {{
      "gdc_name": "GDC Name",
      "context_evidence": [
        "Found in PBT: X with description showing alignment in domain Y",
        "Used in Application: A for purpose B which relates to the record class",
        "Related Process: P which handles similar data types"
      ],
      "alignment_score": 90.0,
      "confidence_adjustment": "Strong contextual alignment due to matching PBT domain and application usage",
      "reasoning": "Detailed explanation of how context supports or refutes the match, including specific PBT names, application purposes, and process descriptions that relate to the record class"
    }}
  ],
  "recommended_gdc": "Final recommended GDC name based on strongest contextual evidence"
}}

Note: alignment_score is for internal ranking, not a confidence measure.

CRITICAL: Return ONLY valid JSON, no additional text or markdown. Reason through each piece of contextual evidence systematically."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            recommended_gdc="UNKNOWN"
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdc: str, validation_data: str) -> str:
    """
    Expert for validating proposed mapping against known validation set.
    
    Args:
        record_name: Name of the record class
        proposed_gdc: Proposed GDC mapping
        validation_data: JSON string of validation set
    
    Returns:
        JSON string with validation results
    """
    prompt = f"""You are a validation expert specializing in data quality and accuracy verification.

TASK: Validate the proposed GDC mapping against the validation dataset.

RECORD CLASS NAME: {record_name}
PROPOSED GDC: {proposed_gdc}

VALIDATION DATASET:
{validation_data}

INSTRUCTIONS:
1. Search for similar record names in the validation set
2. Check if the proposed GDC matches validation data
3. Look for related ILM categories that might provide hints
4. Identify any conflicts or confirmations
5. Provide detailed validation reasoning

OUTPUT FORMAT - Must be valid JSON matching this exact structure:
{{
  "validation_found": true,
  "matching_entry": {{
    "GDC Name": "Validated GDC Name",
    "ILM Category Name": "Related ILM Category"
  }},
  "validation_status": "CONFIRMED",
  "validation_reasoning": "Detailed explanation of validation results including any matches found, conflicts identified, or related patterns in the validation dataset"
}}

Note: validation_status must be one of: CONFIRMED, CONFLICTED, NOT_FOUND
Note: If no match found, set validation_found to false and matching_entry to null

CRITICAL: Return ONLY valid JSON, no additional text or markdown. Be thorough and consider partial matches or related terms."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = ValidationResponse(
            validation_found=False,
            matching_entry=None,
            validation_status="NOT_FOUND",
            validation_reasoning=f"Error: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    Expert for making final mapping decision by synthesizing all expert inputs.
    
    Args:
        record_info: JSON string with record information
        all_analyses: JSON string with all expert analyses
    
    Returns:
        JSON string with final decision and comprehensive reasoning
    """
    prompt = f"""You are a final decision expert specializing in evidence synthesis and mapping decisions.

TASK: Make the final GDC mapping decision by synthesizing all expert analyses.

RECORD INFORMATION:
{record_info}

ALL EXPERT ANALYSES:
{all_analyses}

INSTRUCTIONS:
1. Review semantic similarity scores and reasoning
2. Consider contextual evidence from PBTs, Apps, and Processes
3. Factor in validation results
4. Weigh all evidence systematically
5. Make a well-reasoned final decision
6. Provide comprehensive reasoning chain

OUTPUT FORMAT - Must be valid JSON matching this exact structure:
{{
  "final_gdc_name": "Selected GDC Name",
  "final_gdc_description": "GDC Definition",
  "reasoning": "Multi-paragraph comprehensive reasoning that includes:\n\n1. DECISION SUMMARY: Clear statement of which GDC was selected and why\n\n2. SEMANTIC ANALYSIS: Key findings from semantic matching including similarity scores and reasoning\n\n3. CONTEXTUAL EVIDENCE: Specific PBTs, Applications, and Processes that support this mapping, with detailed explanations of how they align\n\n4. VALIDATION RESULTS: Whether the mapping was confirmed, conflicted, or not found in validation data\n\n5. ALTERNATIVE CONSIDERATIONS: Brief discussion of other candidates and why they were not chosen\n\n6. FINAL JUSTIFICATION: Clear synthesis of all evidence supporting this decision",
  "evidence_summary": [
    "Semantic similarity: Strong match with score of 88/100 based on domain alignment",
    "Context alignment: Found in PBT 'X' supporting Application 'Y' for Process 'Z'",
    "Validation: Confirmed in validation set with matching ILM category",
    "Alternative analysis: Second choice was GDC 'A' but lacked contextual support"
  ]
}}

CRITICAL: 
- Return ONLY valid JSON, no additional text or markdown
- Provide thorough, well-structured reasoning with specific references to PBTs, Applications, and Processes
- Do not include confidence scores or percentages in your reasoning - focus on evidence quality and alignment
- Reference specific tool outputs and findings in your reasoning"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        error_response = FinalMappingDecision(
            final_gdc_name="UNKNOWN",
            final_gdc_description="",
            reasoning=f"Error during decision making: {str(e)}",
            evidence_summary=[]
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent with expert tools"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an expert GDC (Group Data Category) mapping coordinator using a mixture of experts approach.

Your task is to map Record Classes to GDCs by coordinating multiple expert tools in a systematic workflow:

1. SEMANTIC MATCHING: First, use the semantic_similarity_expert tool to find top 5 semantically similar GDCs
2. CONTEXT ANALYSIS: Then, use the context_analysis_expert tool to analyze PBTs, Applications, and Processes
3. VALIDATION: Next, use the validation_expert tool to validate against known mappings
4. FINAL DECISION: Finally, use the final_decision_expert tool to synthesize all evidence

WORKFLOW RULES:
- Always call tools in this exact order
- Pass outputs from one tool as inputs to the next
- Ensure all JSON data is properly formatted
- Validate outputs at each step
- Provide the final mapping with comprehensive reasoning that references specific PBTs, Applications, and Processes
- Do not provide confidence scores - focus on evidence quality

You must be systematic and thorough in your analysis."""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        state_modifier=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    gdc_master: List[GDCMaster],
    gdc_context: List[GDCWithContext],
    validation_set: List[ValidationEntry]
) -> MappingResult:
    """Process a single record through the ReAct agent"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    # Prepare data for the agent
    gdc_master_json = json.dumps([g.model_dump() for g in gdc_master], indent=2)
    gdc_context_json = json.dumps([g.model_dump() for g in gdc_context], indent=2)
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    # Create the query for the agent
    query = f"""Please map the following Record Class to the most appropriate GDC using the complete workflow:

RECORD CLASS:
- GUID: {record.guid}
- Code: {record.code}
- Name: {record.name}
- Description: {record.description}

Follow these steps exactly:
1. Call semantic_similarity_expert with record_name="{record.name}", record_desc="{record.description}", and gdc_candidates=<GDC_MASTER_DATA>
2. Call context_analysis_expert with the semantic matches and GDC context data
3. Call validation_expert with the recommended GDC and validation data
4. Call final_decision_expert to synthesize all analyses and make the final decision

Provide the complete mapping with comprehensive reasoning that references specific PBTs, Applications, and Processes."""
    
    try:
        # Invoke the ReAct agent
        result = agent.invoke({
            "messages": [
                HumanMessage(content=query)
            ]
        })
        
        # Extract the final response
        messages = result.get("messages", [])
        final_message = messages[-1] if messages else None
        
        if not final_message:
            raise ValueError("No response from agent")
        
        # Parse the final decision from the agent's response
        response_content = final_message.content
        
        # Extract final decision from the response
        final_decision = extract_final_decision(response_content)
        
        return MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name=final_decision.get("final_gdc_name", "UNKNOWN"),
            gdc_description=final_decision.get("final_gdc_description", ""),
            reasoning=format_comprehensive_reasoning(final_decision)
        )
        
    except Exception as e:
        print(f"Error processing record: {e}")
        return MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="ERROR",
            gdc_description="",
            reasoning=f"Error during processing: {str(e)}"
        )

def extract_final_decision(response_text: str) -> Dict:
    """Extract final decision from agent response"""
    # Look for JSON in the response
    try:
        json_str = extract_json_from_text(response_text)
        decision_data = json.loads(json_str)
        
        # Validate with Pydantic
        final_decision = FinalMappingDecision.model_validate(decision_data)
        return final_decision.model_dump()
    except:
        # Return default if parsing fails
        return {
            "final_gdc_name": "UNKNOWN",
            "final_gdc_description": "",
            "reasoning": "Unable to extract final decision from agent response",
            "evidence_summary": []
        }

def format_comprehensive_reasoning(final_decision: Dict) -> str:
    """Format comprehensive reasoning for output"""
    parts = []
    
    reasoning = final_decision.get("reasoning", "")
    if reasoning:
        parts.append(f"DECISION REASONING:\n{reasoning}")
    
    evidence = final_decision.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nEVIDENCE SUMMARY:\n" + "\n".join(f"• {e}" for e in evidence))
    
    return "\n".join(parts)

# ==================== MAIN EXECUTION ====================

def main():
    """Main execution function"""
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM")
    print("Using OpenAI o3-mini with LangGraph ReAct Agents")
    print("Pydantic v2 Validation | Latest LangChain & LangGraph")
    print("=" * 80)
    
    # Validate API key
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY environment variable not set")
        print("Please set it using: export OPENAI_API_KEY='your-api-key'")
        return
    
    # Load and validate data files
    print("\n📁 Loading and validating data files...")
    
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded and validated {len(gdc_master)} GDC master entries")
    print(f"✓ Loaded and validated {len(gdc_context)} GDC context entries")
    print(f"✓ Loaded and validated {len(validation_set)} validation entries")
    print(f"✓ Loaded and validated {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load required data files")
        return
    
    # Create ReAct agent
    print("\n🤖 Creating LangGraph ReAct agent with expert tools...")
    agent = create_react_agent_workflow()
    print("✓ ReAct agent created with 4 expert tools")
    print(f"✓ Using model: {OPENAI_MODEL}")
    print(f"✓ Reasoning effort: {REASONING_EFFORT}")
    
    # Process each record
    print("\n🚀 Starting mapping process...\n")
    results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\nRecord {i}/{len(record_classes)}")
        
        try:
            result = process_single_record(
                agent=agent,
                record=record,
                gdc_master=gdc_master,
                gdc_context=gdc_context,
                validation_set=validation_set
            )
            results.append(result)
            print(f"✓ Completed: Mapped to '{result.gdc_name}'")
            
        except Exception as e:
            print(f"✗ Error: {e}")
            error_result = MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="ERROR",
                gdc_description="",
                reasoning=f"Error: {str(e)}"
            )
            results.append(error_result)
    
    # Convert to DataFrame and save
    print("\n" + "="*80)
    print("💾 Saving results...")
    
    results_dict = [r.model_dump(by_alias=True) for r in results]
    df = pd.DataFrame(results_dict)
    
    output_file = "GDC_Mapping_Results.csv"
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"✓ Results saved to {output_file}")
    
    # Print summary
    print("\n" + "="*80)
    print("📊 SUMMARY")
    print("="*80)
    print(f"Total records processed: {len(results)}")
    successful = sum(1 for r in results if r.gdc_name not in ["ERROR", "UNKNOWN"])
    errors = sum(1 for r in results if r.gdc_name == "ERROR")
    unknown = sum(1 for r in results if r.gdc_name == "UNKNOWN")
    
    print(f"✓ Successful mappings: {successful}")
    print(f"⚠ Unknown mappings: {unknown}")
    print(f"✗ Errors: {errors}")
    print("="*80)

if __name__ == "__main__":
    main()
