#!/usr/bin/env python3
"""
Legal Document to RDF Conversion System
======================================

A comprehensive system for converting legal documents into machine-readable RDF format
using LangGraph React agents, OpenAI o3-mini reasoning model, and semantic web standards.

Features:
- LangGraph React agents with proper tooling
- SKOS and PROV-O ontology compliance
- Chain of thought prompting
- Batch processing for large documents
- Error-free TTL output generation
- Advanced entity extraction and analysis
- Performance optimizations with caching and smart processing
"""

import os
import json
import uuid
import hashlib
import asyncio
import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime, timezone
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import quote, unquote
import unicodedata

# Core libraries
import openai
import PyPDF2
import mammoth
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, DCTERMS, XSD

# LangGraph and LangChain imports
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('legal_rdf_converter.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# JSON PARSING AND URL ENCODING UTILITIES
# =============================================================================

class SafeJSONParser:
    """Safe JSON parser that handles malformed JSON from LLMs."""
    
    @staticmethod
    def clean_json_string(json_str: str) -> str:
        """Clean and prepare JSON string for parsing."""
        # Remove markdown code blocks
        json_str = re.sub(r'```json\s*', '', json_str)
        json_str = re.sub(r'```\s*$', '', json_str)
        json_str = re.sub(r'^```\s*', '', json_str)
        
        # Remove any leading/trailing whitespace
        json_str = json_str.strip()
        
        # Remove any text before the first { or [
        start_match = re.search(r'[{\[]', json_str)
        if start_match:
            json_str = json_str[start_match.start():]
        
        # Remove any text after the last } or ]
        end_match = None
        for match in re.finditer(r'[}\]]', json_str):
            end_match = match
        if end_match:
            json_str = json_str[:end_match.end()]
        
        # Fix common JSON issues
        json_str = SafeJSONParser._fix_common_json_issues(json_str)
        
        return json_str
    
    @staticmethod
    def _fix_common_json_issues(json_str: str) -> str:
        """Fix common JSON formatting issues."""
        # Fix trailing commas
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # Fix missing quotes around keys
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)
        
        # Fix single quotes to double quotes
        json_str = re.sub(r"'([^']*)'", r'"\1"', json_str)
        
        # Fix escaped quotes that might be causing issues
        json_str = json_str.replace('\\"', '"')
        
        # Fix newlines inside strings
        json_str = re.sub(r'"\s*\n\s*"', '" "', json_str)
        
        # Fix missing commas between objects/arrays
        json_str = re.sub(r'}\s*{', '}, {', json_str)
        json_str = re.sub(r']\s*\[', '], [', json_str)
        
        return json_str
    
    @staticmethod
    def parse_json_safely(json_str: str, fallback_structure: Dict[str, Any] = None) -> Dict[str, Any]:
        """Safely parse JSON with multiple fallback strategies."""
        if not json_str or not json_str.strip():
            return fallback_structure or {}
        
        # Strategy 1: Try parsing as-is
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
        
        # Strategy 2: Clean and try again
        try:
            cleaned = SafeJSONParser.clean_json_string(json_str)
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass
        
        # Strategy 3: Extract JSON-like content using regex
        try:
            return SafeJSONParser._extract_json_content(json_str, fallback_structure)
        except Exception:
            pass
        
        # Strategy 4: Parse line by line for key-value pairs
        try:
            return SafeJSONParser._parse_key_value_pairs(json_str, fallback_structure)
        except Exception:
            pass
        
        # Final fallback
        logger.warning(f"Could not parse JSON, using fallback structure: {json_str[:200]}...")
        return fallback_structure or {}
    
    @staticmethod
    def _extract_json_content(json_str: str, fallback_structure: Dict[str, Any]) -> Dict[str, Any]:
        """Extract JSON content using regex patterns."""
        result = fallback_structure.copy() if fallback_structure else {}
        
        # Extract arrays
        array_pattern = r'"(\w+)":\s*\[(.*?)\]'
        for match in re.finditer(array_pattern, json_str, re.DOTALL):
            key = match.group(1)
            array_content = match.group(2)
            
            # Try to parse individual objects in the array
            objects = []
            object_pattern = r'\{(.*?)\}'
            for obj_match in re.finditer(object_pattern, array_content, re.DOTALL):
                obj_content = obj_match.group(1)
                obj = SafeJSONParser._parse_object_content(obj_content)
                if obj:
                    objects.append(obj)
            
            result[key] = objects
        
        return result
    
    @staticmethod
    def _parse_object_content(obj_content: str) -> Dict[str, Any]:
        """Parse object content from string."""
        obj = {}
        
        # Extract key-value pairs
        kv_pattern = r'"([^"]+)":\s*"([^"]*)"'
        for match in re.finditer(kv_pattern, obj_content):
            key = match.group(1)
            value = match.group(2)
            obj[key] = value
        
        # Extract numeric values
        num_pattern = r'"([^"]+)":\s*(\d+\.?\d*)'
        for match in re.finditer(num_pattern, obj_content):
            key = match.group(1)
            value = match.group(2)
            try:
                obj[key] = float(value) if '.' in value else int(value)
            except ValueError:
                obj[key] = value
        
        return obj
    
    @staticmethod
    def _parse_key_value_pairs(text: str, fallback_structure: Dict[str, Any]) -> Dict[str, Any]:
        """Parse key-value pairs from text when JSON parsing fails."""
        result = fallback_structure.copy() if fallback_structure else {}
        
        # Look for patterns like "key": "value"
        kv_pattern = r'"([^"]+)":\s*"([^"]*)"'
        for match in re.finditer(kv_pattern, text):
            key = match.group(1)
            value = match.group(2)
            result[key] = value
        
        return result

class URLEncoder:
    """Utility class for proper URL encoding in RDF URIs."""
    
    @staticmethod
    def encode_uri_component(component: str) -> str:
        """Encode a URI component with proper handling of special characters."""
        if not component:
            return ""
        
        # Normalize unicode characters
        normalized = unicodedata.normalize('NFKC', str(component))
        
        # Replace spaces and special characters
        normalized = re.sub(r'\s+', '_', normalized)  # Replace spaces with underscores
        normalized = re.sub(r'[^\w\-_.]', '_', normalized)  # Replace special chars with underscores
        
        # Remove multiple consecutive underscores
        normalized = re.sub(r'_+', '_', normalized)
        
        # Remove leading/trailing underscores
        normalized = normalized.strip('_')
        
        # Ensure it doesn't start with a number (invalid for XML names)
        if normalized and normalized[0].isdigit():
            normalized = f"item_{normalized}"
        
        # URL encode the result
        encoded = quote(normalized, safe='_-.')
        
        return encoded
    
    @staticmethod
    def create_legal_uri(namespace: str, entity_type: str, identifier: str) -> str:
        """Create a properly encoded legal entity URI."""
        encoded_type = URLEncoder.encode_uri_component(entity_type)
        encoded_id = URLEncoder.encode_uri_component(identifier)
        
        # Ensure namespace ends with appropriate separator
        if not namespace.endswith(('#', '/')):
            namespace += '#'
        
        return f"{namespace}{encoded_type}_{encoded_id}"
    
    @staticmethod
    def create_document_uri(base_namespace: str, document_id: str) -> str:
        """Create a properly encoded document URI."""
        encoded_id = URLEncoder.encode_uri_component(document_id)
        
        if not base_namespace.endswith('/'):
            base_namespace += '/'
        
        return f"{base_namespace}document/{encoded_id}"
    
    @staticmethod
    def create_instance_uri(namespace: str, class_name: str, instance_data: str, chunk_index: int = 0) -> str:
        """Create a unique instance URI for extracted entities."""
        # Create a hash from the instance data to ensure uniqueness
        data_hash = hashlib.md5(instance_data.encode('utf-8')).hexdigest()[:8]
        
        encoded_class = URLEncoder.encode_uri_component(class_name)
        identifier = f"{encoded_class}_{data_hash}_{chunk_index}"
        
        if not namespace.endswith(('#', '/')):
            namespace += '#'
        
        return f"{namespace}{identifier}"
    
    @staticmethod
    def validate_uri(uri: str) -> bool:
        """Validate that a URI is properly formatted."""
        try:
            # Check for valid URI characters
            if not re.match(r'^[a-zA-Z][a-zA-Z0-9+.-]*:', uri):
                return False
            
            # Check for proper encoding
            decoded = unquote(uri)
            re_encoded = quote(decoded, safe=':/?#[]@!$&\'()*+,;=')
            
            return True
        except Exception:
            return False

class RDFSerializer:
    """Enhanced RDF serializer with proper URI encoding."""
    
    @staticmethod
    def serialize_triple(subject: str, predicate: str, obj: str, is_literal: bool = False) -> str:
        """Serialize a single RDF triple with proper URI encoding."""
        # Encode URIs
        if not URLEncoder.validate_uri(subject):
            subject = f"<{subject}>"
        else:
            subject = f"<{subject}>"
        
        if not URLEncoder.validate_uri(predicate):
            predicate = f"<{predicate}>"
        else:
            predicate = f"<{predicate}>"
        
        # Handle object encoding
        if is_literal:
            # Escape quotes and newlines in literals
            escaped_obj = obj.replace('"', '\\"').replace('\n', '\\n').replace('\r', '\\r')
            obj_str = f'"{escaped_obj}"'
        else:
            if not URLEncoder.validate_uri(obj):
                obj_str = f"<{obj}>"
            else:
                obj_str = f"<{obj}>"
        
        return f"{subject} {predicate} {obj_str} ."
    
    @staticmethod
    def clean_turtle_output(turtle_content: str) -> str:
        """Clean and validate Turtle RDF output."""
        lines = turtle_content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#') or line.startswith('@prefix'):
                cleaned_lines.append(line)
                continue
            
            # Fix common Turtle syntax issues
            if line and not line.endswith('.') and not line.endswith(',') and not line.endswith(';'):
                if '<' in line and '>' in line:  # Looks like a triple
                    line += ' .'
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)

# =============================================================================
# PERFORMANCE OPTIMIZATION UTILITIES
# =============================================================================

class ResponseCache:
    """Intelligent caching system for LLM responses."""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache = {}
        
    def _get_cache_key(self, content: str, agent_type: str) -> str:
        """Generate cache key from content and agent type."""
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        return f"{agent_type}_{content_hash}"
    
    def get_cached_response(self, content: str, agent_type: str) -> Optional[Dict]:
        """Get cached response if available."""
        cache_key = self._get_cache_key(content, agent_type)
        
        # Check memory cache first
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # Check disk cache
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                # Add to memory cache
                self.memory_cache[cache_key] = cached_data
                return cached_data
            except Exception as e:
                logger.warning(f"Error reading cache file: {e}")
        
        return None
    
    def cache_response(self, content: str, agent_type: str, response: Dict):
        """Cache a response."""
        cache_key = self._get_cache_key(content, agent_type)
        
        # Add to memory cache
        self.memory_cache[cache_key] = response
        
        # Save to disk cache
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(response, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"Error writing cache file: {e}")

class ContentPrioritizer:
    """Intelligent content prioritization for faster processing."""
    
    @staticmethod
    def calculate_content_priority(text: str) -> float:
        """Calculate priority score for content (0.0 to 1.0)."""
        score = 0.0
        text_lower = text.lower()
        
        # High priority keywords
        high_priority_terms = [
            'article', 'section', 'regulation', 'directive', 'shall', 'must',
            'prohibited', 'required', 'obligation', 'penalty', 'fine',
            'data controller', 'data processor', 'personal data', 'gdpr',
            'compliance', 'violation', 'rights', 'consent', 'lawful basis'
        ]
        
        # Medium priority keywords
        medium_priority_terms = [
            'may', 'should', 'can', 'procedure', 'process', 'implement',
            'ensure', 'appropriate', 'necessary', 'reasonable', 'technical',
            'organizational', 'measure', 'safeguard', 'security'
        ]
        
        # Count high priority terms (weight: 0.1 each)
        for term in high_priority_terms:
            score += text_lower.count(term) * 0.1
        
        # Count medium priority terms (weight: 0.05 each)
        for term in medium_priority_terms:
            score += text_lower.count(term) * 0.05
        
        # Bonus for structured content (articles, sections)
        if re.search(r'article\s+\d+|section\s+\d+|\(\d+\)', text_lower):
            score += 0.3
        
        # Bonus for cross-references
        if re.search(r'regulation\s+\([^)]+\)|directive\s+\d+', text_lower):
            score += 0.2
        
        # Penalty for very short content
        if len(text) < 100:
            score *= 0.5
        
        return min(1.0, score)
    
    @staticmethod
    def prioritize_chunks(chunks: List['DocumentChunk']) -> List['DocumentChunk']:
        """Sort chunks by priority score."""
        for chunk in chunks:
            chunk.metadata['priority_score'] = ContentPrioritizer.calculate_content_priority(chunk.content)
        
        # Sort by priority (highest first)
        return sorted(chunks, key=lambda c: c.metadata.get('priority_score', 0), reverse=True)

class SmartChunker:
    """Content-aware intelligent chunking."""
    
    def __init__(self, max_chunk_size: int = 3000, overlap: int = 150):
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap
        
        # Legal document structure patterns
        self.section_patterns = [
            r'CHAPTER\s+[IVXLC]+',
            r'SECTION\s+\d+',
            r'Article\s+\d+',
            r'Section\s+\d+',
            r'\(\d+\)\s*[A-Z]',
            r'[A-Z]\.\s+[A-Z]'
        ]
    
    def smart_chunk_text(self, text: str, document_id: str) -> List['DocumentChunk']:
        """Create chunks with awareness of legal document structure."""
        chunks = []
        
        # First, identify major sections
        sections = self._identify_sections(text)
        
        if not sections:
            # Fallback to sentence-based chunking
            return self._sentence_based_chunking(text, document_id)
        
        # Process each section
        chunk_index = 0
        
        for section_start, section_end, section_title in sections:
            section_text = text[section_start:section_end]
            
            # If section is small enough, create single chunk
            if len(section_text) <= self.max_chunk_size:
                chunk = DocumentChunk(
                    content=section_text,
                    page_number=self._extract_page_number(section_text),
                    chunk_index=chunk_index,
                    start_char=section_start,
                    end_char=section_end,
                    document_id=document_id,
                    metadata={'section_title': section_title, 'is_complete_section': True}
                )
                chunks.append(chunk)
                chunk_index += 1
            else:
                # Split large section into smaller chunks
                section_chunks = self._split_section(section_text, section_start, document_id, chunk_index, section_title)
                chunks.extend(section_chunks)
                chunk_index += len(section_chunks)
        
        return chunks
    
    def _identify_sections(self, text: str) -> List[Tuple[int, int, str]]:
        """Identify major sections in the document."""
        sections = []
        
        for pattern in self.section_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                sections.append((match.start(), match.end(), match.group()))
        
        # Sort by position and merge overlapping
        sections.sort(key=lambda x: x[0])
        
        # Add section boundaries
        merged_sections = []
        for i, (start, _, title) in enumerate(sections):
            if i + 1 < len(sections):
                end = sections[i + 1][0]
            else:
                end = len(text)
            merged_sections.append((start, end, title))
        
        return merged_sections
    
    def _split_section(self, section_text: str, section_start: int, document_id: str, 
                      start_chunk_index: int, section_title: str) -> List['DocumentChunk']:
        """Split a large section into smaller chunks."""
        chunks = []
        sentences = re.split(r'(?<=\.)\s+', section_text)
        
        current_chunk = ""
        current_start = section_start
        chunk_index = start_chunk_index
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.max_chunk_size and current_chunk:
                # Create chunk
                chunk = DocumentChunk(
                    content=current_chunk.strip(),
                    page_number=self._extract_page_number(current_chunk),
                    chunk_index=chunk_index,
                    start_char=current_start,
                    end_char=current_start + len(current_chunk),
                    document_id=document_id,
                    metadata={'section_title': section_title, 'is_complete_section': False}
                )
                chunks.append(chunk)
                
                # Start new chunk with overlap
                overlap_text = current_chunk[-self.overlap:] if len(current_chunk) > self.overlap else current_chunk
                current_chunk = overlap_text + sentence + ". "
                current_start += len(current_chunk) - len(overlap_text)
                chunk_index += 1
            else:
                current_chunk += sentence + ". "
        
        # Add final chunk
        if current_chunk.strip():
            chunk = DocumentChunk(
                content=current_chunk.strip(),
                page_number=self._extract_page_number(current_chunk),
                chunk_index=chunk_index,
                start_char=current_start,
                end_char=current_start + len(current_chunk),
                document_id=document_id,
                metadata={'section_title': section_title, 'is_complete_section': False}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _sentence_based_chunking(self, text: str, document_id: str) -> List['DocumentChunk']:
        """Fallback to sentence-based chunking."""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.max_chunk_size,
            chunk_overlap=self.overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunks = []
        text_chunks = text_splitter.split_text(text)
        
        for chunk_index, chunk_content in enumerate(text_chunks):
            chunk = DocumentChunk(
                content=chunk_content,
                page_number=self._extract_page_number(chunk_content),
                chunk_index=chunk_index,
                start_char=0,  # Simplified for fallback
                end_char=len(chunk_content),
                document_id=document_id,
                metadata={'chunking_method': 'sentence_based'}
            )
            chunks.append(chunk)
        
        return chunks
    
    def _extract_page_number(self, text: str) -> int:
        """Extract page number from text chunk."""
        page_match = re.search(r'\[Page (\d+)\]', text)
        return int(page_match.group(1)) if page_match else 1

class OptimizedPromptTemplates:
    """Optimized prompt templates for faster, more efficient processing."""
    
    ENTITY_EXTRACTION_TEMPLATE = """Extract legal entities from this text. Focus on the actual content and rules, not just article references.

Text: {content}

Return JSON with ONLY the most important entities:
{{"entities": [{{"type": "DataController|DataProcessor|Condition|Restriction|Action|Jurisdiction|Organisation", "text": "exact text", "section_reference": "section", "confidence": 0.9}}]}}

Focus on: data controllers, processors, explicit obligations, prohibitions, and jurisdictions. Extract the actual rules and restrictions, not just references to articles."""

    CROSS_REFERENCE_TEMPLATE = """Find legal cross-references in this text. Focus on explicit references to other legal instruments.

Text: {content}

Return JSON:
{{"cross_references": [{{"type": "regulation|directive|act|treaty", "reference_text": "exact text", "referenced_instrument": "full instrument name", "relationship": "implements|modifies|references"}}]}}

Look for: Regulation (EU) numbers, Directive references, specific acts. Focus on substantial cross-references, not minor mentions."""

    TEMPORAL_TEMPLATE = """Extract dates and deadlines from this text. Only extract explicit temporal information.

Text: {content}

Return JSON:
{{"temporal_elements": [{{"type": "effective_date|deadline", "date_text": "exact text", "parsed_date": "YYYY-MM-DD", "applies_to": "what it applies to"}}]}}

Focus on: effective dates, compliance deadlines, transition periods."""

    COMPLIANCE_TEMPLATE = """Extract compliance requirements from this text. Focus on actual obligations and restrictions, not article numbers.

Text: {content}

Return JSON:
{{"compliance_requirements": [{{"type": "obligation|prohibition", "requirement_text": "exact text describing the rule", "obligation_level": "mandatory|optional", "subject": "who must comply", "action": "required action"}}]}}

Extract the actual rules and restrictions. Instead of "Article 6 requires...", extract "Data controllers must obtain lawful basis before processing"."""

    STANDARDIZATION_TEMPLATE = """Standardize and validate this extracted legal data. Remove abbreviations, expand references, and focus on actual rules.

Data: {content}

Return JSON with standardized data:
{{"standardized_data": {{"entities": [], "cross_references": [], "temporal_elements": [], "compliance_requirements": []}}, "validation_notes": ["list of issues found and fixed"]}}

Rules for standardization:
1. Replace "Art." with "Article" but better yet, extract the actual rule content
2. Replace "Sect." with "Section" 
3. Instead of "Article 6 applies", describe what Article 6 actually requires
4. Expand all abbreviations (GDPR → General Data Protection Regulation)
5. Focus on substance: what are the actual obligations, restrictions, and conditions?
6. Remove redundant references - focus on the content
7. Ensure all entities make logical sense and are properly categorized
8. Validate that obligations specify who must do what under what conditions"""

# =============================================================================
# GLOBAL CONFIGURATION
# =============================================================================

@dataclass
class GlobalConfig:
    """Global configuration for the legal document RDF conversion system."""
    
    # OpenAI Configuration
    openai_base_url: str = "https://api.openai.com/v1"
    openai_api_key: str = field(default_factory=lambda: os.getenv("OPENAI_API_KEY", ""))
    reasoning_model: str = "o3-mini"
    embedding_model: str = "text-embedding-3-large"
    
    # Performance Optimization Settings
    reasoning_effort_level: str = "low"  # low/medium/high - low for speed
    request_timeout: int = 60  # Increased timeout for individual requests
    enable_streaming: bool = True  # Enable streaming responses
    enable_caching: bool = True  # Enable response caching
    connection_pool_size: int = 20  # HTTP connection pool size
    
    # Smart Processing Settings
    content_priority_threshold: float = 0.7  # Skip low-priority content
    enable_progressive_processing: bool = True  # Process high-priority first
    smart_chunking: bool = True  # Use content-aware chunking
    early_stopping_threshold: int = 50  # Stop if enough entities found
    
    # File paths
    input_folder: str = "input_documents"
    output_folder: str = "output_rdf"
    config_file: str = "document_config.json"
    ontology_file: str = "legal_ontology.ttl"
    cache_folder: str = "cache"
    
    # Processing configuration
    batch_size: int = 3  # Smaller batches for faster processing
    max_chunk_size: int = 3000  # Slightly smaller chunks
    chunk_overlap: int = 150  # Reduced overlap
    max_workers: int = 8  # Increased workers
    
    # RDF Configuration
    base_namespace: str = "http://example.org/legal/"
    ontology_namespace: str = "http://example.org/legal/ontology/"
    
    def __post_init__(self):
        if not self.openai_api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable.")
        
        # Ensure directories exist
        Path(self.input_folder).mkdir(exist_ok=True)
        Path(self.output_folder).mkdir(exist_ok=True)
        Path(self.cache_folder).mkdir(exist_ok=True)

# Global configuration instance
CONFIG = GlobalConfig()

# =============================================================================
# ONTOLOGY DEFINITIONS
# =============================================================================

# Define namespaces
LEGAL = Namespace(CONFIG.ontology_namespace)
PROV = Namespace("http://www.w3.org/ns/prov#")

class LegalOntologyBuilder:
    """Builds the legal ontology structure with SKOS and PROV-O compliance."""
    
    def __init__(self):
        self.graph = Graph()
        self._setup_namespaces()
        self._create_ontology_structure()
    
    def _setup_namespaces(self):
        """Setup RDF namespaces."""
        self.graph.bind("legal", LEGAL)
        self.graph.bind("skos", SKOS)
        self.graph.bind("prov", PROV)
        self.graph.bind("rdfs", RDFS)
        self.graph.bind("owl", OWL)
        self.graph.bind("dcterms", DCTERMS)
        self.graph.bind("xsd", XSD)
    
    def _create_ontology_structure(self):
        """Create the core ontology structure."""
        # Main ontology declaration
        ontology_uri = URIRef(CONFIG.ontology_namespace)
        self.graph.add((ontology_uri, RDF.type, OWL.Ontology))
        self.graph.add((ontology_uri, DCTERMS.title, Literal("Legal Document Ontology")))
        self.graph.add((ontology_uri, DCTERMS.description, 
                       Literal("An ontology for representing legal documents and their components")))
        self.graph.add((ontology_uri, DCTERMS.created, 
                       Literal(datetime.now(timezone.utc).isoformat(), datatype=XSD.dateTime)))
        
        # Define main classes
        main_classes = [
            "DataController", "DataProcessor", "DataCategory", "RuleCategory",
            "Conditions", "Restrictions", "Actions", "Country", "Jurisdiction",
            "Organisation", "AdequacyDecisions", "CrossReference", "TemporalElement",
            "ComplianceRequirement", "Document"
        ]
        
        for class_name in main_classes:
            class_uri = LEGAL[class_name]
            self.graph.add((class_uri, RDF.type, OWL.Class))
            self.graph.add((class_uri, RDF.type, SKOS.Concept))
            self.graph.add((class_uri, RDFS.label, Literal(class_name)))
            self.graph.add((class_uri, SKOS.prefLabel, Literal(class_name)))
        
        # Define RuleCategory subclasses
        rule_subclasses = [
            "DataProvision", "DataAssetCreation", "DataStorage", "DataRetention",
            "DataSeparability", "DataCatalogue", "DataMapping", "DataQuality",
            "DataIntegrity", "ReferenceData", "TrustedSource", "DataLineage",
            "DataAccess", "DataEntitlement"
        ]
        
        rule_category_uri = LEGAL["RuleCategory"]
        for subclass_name in rule_subclasses:
            subclass_uri = LEGAL[subclass_name]
            self.graph.add((subclass_uri, RDF.type, OWL.Class))
            self.graph.add((subclass_uri, RDF.type, SKOS.Concept))
            self.graph.add((subclass_uri, RDFS.subClassOf, rule_category_uri))
            self.graph.add((subclass_uri, SKOS.broader, rule_category_uri))
            self.graph.add((subclass_uri, RDFS.label, Literal(subclass_name)))
            self.graph.add((subclass_uri, SKOS.prefLabel, Literal(subclass_name)))
        
        # Define properties
        self._create_properties()
    
    def _create_properties(self):
        """Create object and data properties."""
        # Object properties
        object_properties = [
            ("appliesTo", "Indicates what entity a rule or restriction applies to"),
            ("hasCondition", "Links to applicable conditions"),
            ("hasRestriction", "Links to applicable restrictions"),
            ("hasJurisdiction", "Indicates the legal jurisdiction"),
            ("regulatedBy", "Indicates the regulating organization"),
            ("derivedFrom", "Indicates source of derivation"),
            ("referencesInstrument", "References another legal instrument"),
            ("relationshipType", "Type of relationship between entities")
        ]
        
        for prop_name, description in object_properties:
            prop_uri = LEGAL[prop_name]
            self.graph.add((prop_uri, RDF.type, OWL.ObjectProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(prop_name)))
            self.graph.add((prop_uri, RDFS.comment, Literal(description)))
        
        # Data properties
        data_properties = [
            ("regulationSection", "Section reference in the source regulation"),
            ("effectiveDate", "Date when the rule becomes effective"),
            ("lastModified", "Date of last modification"),
            ("confidence", "Confidence score of the extraction"),
            ("documentTitle", "Title of the source document"),
            ("pageNumber", "Page number in source document"),
            ("referenceContext", "Context where a reference appears"),
            ("temporalType", "Type of temporal element"),
            ("relativeExpression", "Relative time expression"),
            ("obligationLevel", "Level of obligation (mandatory/optional)"),
            ("complianceSubject", "Subject who must comply"),
            ("requiredAction", "Required action for compliance"),
            ("penalty", "Penalty for non-compliance"),
            ("processingTime", "Time taken to process"),
            ("entitiesExtracted", "Number of entities extracted"),
            ("chunksProcessed", "Number of chunks processed")
        ]
        
        for prop_name, description in data_properties:
            prop_uri = LEGAL[prop_name]
            self.graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
            self.graph.add((prop_uri, RDFS.label, Literal(prop_name)))
            self.graph.add((prop_uri, RDFS.comment, Literal(description)))
    
    def serialize(self, format="turtle"):
        """Serialize the ontology graph."""
        return self.graph.serialize(format=format)

# =============================================================================
# DOCUMENT PROCESSING UTILITIES
# =============================================================================

@dataclass
class DocumentChunk:
    """Represents a chunk of a document."""
    content: str
    page_number: int
    chunk_index: int
    start_char: int
    end_char: int
    document_id: str
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DocumentInfo:
    """Information about a document being processed."""
    file_path: str
    jurisdiction: str
    document_type: str = "regulation"
    title: str = ""
    total_pages: int = 0
    total_chunks: int = 0

class DocumentProcessor:
    """Enhanced document processing with smart chunking and content prioritization."""
    
    def __init__(self, config: GlobalConfig):
        self.config = config
        self.smart_chunker = SmartChunker(config.max_chunk_size, config.chunk_overlap)
        self.content_prioritizer = ContentPrioritizer()
        
        # Traditional text splitter as fallback
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.max_chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def extract_text_from_pdf(self, file_path: str) -> Tuple[str, int]:
        """Extract text from PDF file with optimization."""
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text_parts = []
            page_count = len(pdf_reader.pages)
            
            # Use concurrent processing for large PDFs
            if page_count > 10:
                return self._extract_pdf_concurrent(pdf_reader, page_count)
            
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text_parts.append(f"\n[Page {page_num + 1}]\n{page_text}\n")
            
            return ''.join(text_parts), page_count
    
    def _extract_pdf_concurrent(self, pdf_reader, page_count: int) -> Tuple[str, int]:
        """Extract PDF text using concurrent processing."""
        import concurrent.futures
        
        def extract_page(page_info):
            page_num, page = page_info
            try:
                page_text = page.extract_text()
                return page_num, f"\n[Page {page_num + 1}]\n{page_text}\n"
            except Exception as e:
                logger.warning(f"Error extracting page {page_num + 1}: {e}")
                return page_num, f"\n[Page {page_num + 1}]\n[Error extracting text]\n"
        
        # Process pages in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            page_items = list(enumerate(pdf_reader.pages))
            results = list(executor.map(extract_page, page_items))
        
        # Sort by page number and combine
        results.sort(key=lambda x: x[0])
        text = ''.join(result[1] for result in results)
        
        return text, page_count
    
    def extract_text_from_docx(self, file_path: str) -> Tuple[str, int]:
        """Extract text from DOCX file."""
        with open(file_path, 'rb') as file:
            result = mammoth.extract_raw_text(file)
            text = result.value
            # Approximate page count (assuming ~500 words per page)
            page_count = max(1, len(text.split()) // 500)
            return text, page_count
    
    def chunk_text_optimized(self, text: str, document_id: str) -> List[DocumentChunk]:
        """Optimized text chunking with smart content-aware processing."""
        if self.config.smart_chunking:
            chunks = self.smart_chunker.smart_chunk_text(text, document_id)
        else:
            chunks = self._traditional_chunking(text, document_id)
        
        # Add priority scores to chunks
        if self.config.enable_progressive_processing:
            chunks = self.content_prioritizer.prioritize_chunks(chunks)
        
        # Filter low-priority chunks if threshold is set
        if self.config.content_priority_threshold > 0:
            filtered_chunks = [
                chunk for chunk in chunks 
                if chunk.metadata.get('priority_score', 0) >= self.config.content_priority_threshold
            ]
            
            if filtered_chunks:
                logger.info(f"Filtered {len(chunks)} chunks to {len(filtered_chunks)} high-priority chunks")
                chunks = filtered_chunks
        
        return chunks
    
    def _traditional_chunking(self, text: str, document_id: str) -> List[DocumentChunk]:
        """Traditional chunking method as fallback."""
        chunks = []
        text_chunks = self.text_splitter.split_text(text)
        
        current_position = 0
        for chunk_index, chunk_content in enumerate(text_chunks):
            start_pos = text.find(chunk_content, current_position)
            if start_pos == -1:
                start_pos = current_position
            
            end_pos = start_pos + len(chunk_content)
            page_number = self._extract_page_number(chunk_content)
            
            chunk = DocumentChunk(
                content=chunk_content,
                page_number=page_number,
                chunk_index=chunk_index,
                start_char=start_pos,
                end_char=end_pos,
                document_id=document_id,
                metadata={'chunking_method': 'traditional'}
            )
            chunks.append(chunk)
            current_position = end_pos
        
        return chunks
    
    def _extract_page_number(self, text: str) -> int:
        """Extract page number from text chunk."""
        page_match = re.search(r'\[Page (\d+)\]', text)
        return int(page_match.group(1)) if page_match else 1

# =============================================================================
# OPENAI INTEGRATION
# =============================================================================

class OpenAIManager:
    """Optimized OpenAI API manager with connection pooling and no rate limiting."""
    
    def __init__(self, config: GlobalConfig):
        self.config = config
        
        # Create HTTP client with connection pooling for better performance
        import httpx
        self.http_client = httpx.AsyncClient(
            timeout=config.request_timeout,
            limits=httpx.Limits(
                max_keepalive_connections=config.connection_pool_size,
                max_connections=config.connection_pool_size * 2,
                keepalive_expiry=30.0
            )
        )
        
        self.client = openai.AsyncOpenAI(
            api_key=config.openai_api_key,
            base_url=config.openai_base_url,
            timeout=config.request_timeout,
            max_retries=1,  # Reduced retries for faster failure handling
            http_client=self.http_client
        )
        
        self.llm = ChatOpenAI(
            model=config.reasoning_model,
            api_key=config.openai_api_key,
            base_url=config.openai_base_url,
            temperature=0.1,
            max_tokens=3000,
            streaming=config.enable_streaming
        )
        
        # Initialize caching system
        self.cache = ResponseCache(config.cache_folder) if config.enable_caching else None
    
    async def get_optimized_completion(self, 
                                     content: str, 
                                     agent_type: str,
                                     reasoning_effort: str = None) -> Dict[str, Any]:
        """Get completion with caching and optimization - no rate limiting."""
        
        # Use configured reasoning effort if not specified
        if reasoning_effort is None:
            reasoning_effort = self.config.reasoning_effort_level
        
        # Check cache first
        if self.cache:
            cached_response = self.cache.get_cached_response(content, agent_type)
            if cached_response:
                logger.debug(f"Cache hit for {agent_type}")
                return cached_response
        
        # No semaphore blocking - direct API call for maximum speed
        try:
            # Use optimized prompt template
            template = getattr(OptimizedPromptTemplates, f"{agent_type.upper()}_TEMPLATE", 
                             "Analyze this content: {content}")
            
            prompt = template.format(content=content[:2500])  # Truncate to stay within limits
            
            # Use asyncio.wait_for for timeout handling without blocking
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=self.config.reasoning_model,
                    messages=[{"role": "user", "content": prompt}],
                    reasoning_effort=reasoning_effort,
                    max_completion_tokens=2000,
                    temperature=0.1,
                    stream=False
                ),
                timeout=self.config.request_timeout
            )
            
            result = {
                "content": response.choices[0].message.content,
                "usage": response.usage.model_dump() if response.usage else {},
                "agent_type": agent_type
            }
            
            # Cache the response asynchronously (non-blocking)
            if self.cache:
                asyncio.create_task(self._cache_response_async(content, agent_type, result))
            
            return result
            
        except asyncio.TimeoutError:
            logger.warning(f"Timeout for {agent_type} - using fallback")
            return self._get_fallback_response(agent_type)
        except Exception as e:
            logger.warning(f"API error for {agent_type}: {e} - using fallback")
            return self._get_fallback_response(agent_type)
    
    async def _cache_response_async(self, content: str, agent_type: str, result: Dict):
        """Cache response asynchronously without blocking."""
        try:
            self.cache.cache_response(content, agent_type, result)
        except Exception as e:
            logger.debug(f"Cache write failed: {e}")
    
    def _get_fallback_response(self, agent_type: str) -> Dict[str, Any]:
        """Get fallback response when API fails."""
        fallback_content = '{"entities": [], "cross_references": [], "temporal_elements": [], "compliance_requirements": []}'
        return {
            "content": fallback_content,
            "usage": {},
            "agent_type": agent_type,
            "fallback": True
        }
    
    async def get_reasoning_completion(self, messages: List[Dict], reasoning_effort: str = "low") -> str:
        """Legacy method for backward compatibility - no rate limiting."""
        try:
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=self.config.reasoning_model,
                    messages=messages,
                    reasoning_effort=reasoning_effort,
                    max_completion_tokens=2000,
                    temperature=0.1
                ),
                timeout=self.config.request_timeout
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"Reasoning completion failed: {e}")
            return '{"entities": [], "cross_references": [], "temporal_elements": [], "compliance_requirements": []}'
    
    async def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings with efficient batching - no rate limiting."""
        if not texts:
            return []
        
        # Process all texts in parallel batches for maximum speed
        batch_size = 100  # Larger batches for efficiency
        all_embeddings = []
        
        # Create tasks for all batches
        tasks = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            task = self._get_embedding_batch(batch)
            tasks.append(task)
        
        # Execute all batches concurrently
        try:
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.warning(f"Embedding batch failed: {result}")
                    # Add zero embeddings for failed batch
                    all_embeddings.extend([[0.0] * 1536 for _ in range(batch_size)])
                else:
                    all_embeddings.extend(result)
                    
        except Exception as e:
            logger.error(f"Embedding processing failed: {e}")
            # Return zero embeddings as fallback
            all_embeddings = [[0.0] * 1536 for _ in texts]
        
        return all_embeddings[:len(texts)]  # Ensure exact length match
    
    async def _get_embedding_batch(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings for a single batch."""
        response = await asyncio.wait_for(
            self.client.embeddings.create(
                model=self.config.embedding_model,
                input=texts
            ),
            timeout=self.config.request_timeout
        )
        return [embedding.embedding for embedding in response.data]
    
    async def close(self):
        """Close HTTP client connections."""
        if hasattr(self, 'http_client'):
            await self.http_client.aclose()

# =============================================================================
# REACT AGENT TOOLS
# =============================================================================

# Global storage for extracted data
extraction_storage = {
    "entities": [],
    "cross_references": [],
    "temporal_elements": [],
    "compliance_requirements": [],
    "current_chunk": None,
    "document_uri": None
}

@tool
def extract_legal_entities(chunk_content: str, page_number: int, chunk_index: int) -> str:
    """
    Extract legal entities and concepts from document chunk.
    
    Args:
        chunk_content: The text content of the document chunk
        page_number: Page number where this chunk appears
        chunk_index: Index of this chunk in the document
    
    Returns:
        JSON string with extracted entities and relationships
    """
    # Store chunk info for processing
    extraction_storage["current_chunk"] = {
        "content": chunk_content,
        "page_number": page_number,
        "chunk_index": chunk_index
    }
    
    # Return instruction for the agent
    return f"""Please analyze this legal document chunk and extract entities in the following JSON format:
    {{
        "entities": [
            {{
                "type": "DataController|DataProcessor|Condition|Restriction|Action|Jurisdiction|Organisation",
                "text": "exact text from document",
                "description": "brief description",
                "section_reference": "specific section/article reference",
                "page_number": {page_number},
                "confidence": 0.0-1.0
            }}
        ],
        "relationships": [
            {{
                "subject": "entity1",
                "predicate": "appliesTo|hasCondition|hasRestriction|regulatedBy",
                "object": "entity2",
                "section_reference": "specific section reference"
            }}
        ]
    }}
    
    Content to analyze:
    {chunk_content}
    
    Focus on identifying legal concepts, rules, conditions, and restrictions.
    Only extract entities that are explicitly mentioned in the text.
    """

@tool
def extract_cross_references(chunk_content: str, page_number: int) -> str:
    """
    Extract cross-references to other regulations and legal instruments.
    
    Args:
        chunk_content: The text content to analyze
        page_number: Page number where this chunk appears
    
    Returns:
        Instructions for cross-reference extraction
    """
    return f"""Find all cross-references in this legal text and return them in JSON format:
    {{
        "cross_references": [
            {{
                "type": "article|regulation|directive|case|treaty",
                "reference_text": "exact text from document",
                "referenced_instrument": "name of referenced document/article",
                "relationship": "implements|modifies|references|supersedes|amends",
                "context": "surrounding context where reference appears",
                "section_reference": "section where found",
                "confidence": 0.0-1.0
            }}
        ]
    }}
    
    Text to analyze:
    {chunk_content}
    
    Look for references like "Article 6", "Regulation (EU) 2016/679", "Directive 95/46/EC", etc.
    """

@tool
def extract_temporal_elements(chunk_content: str, page_number: int) -> str:
    """
    Extract temporal information including dates, deadlines, and time periods.
    
    Args:
        chunk_content: The text content to analyze
        page_number: Page number where this chunk appears
    
    Returns:
        Instructions for temporal extraction
    """
    return f"""Extract temporal information from this legal text in JSON format:
    {{
        "temporal_elements": [
            {{
                "type": "effective_date|deadline|transition_period|expiration|review_date",
                "date_text": "exact text from document",
                "parsed_date": "YYYY-MM-DD or null if relative",
                "relative_expression": "relative time expression if applicable",
                "applies_to": "what this date applies to",
                "condition": "any conditions for this temporal element",
                "section_reference": "section where found",
                "confidence": 0.0-1.0
            }}
        ]
    }}
    
    Text to analyze:
    {chunk_content}
    
    Look for effective dates, deadlines, transition periods, and temporal conditions.
    """

@tool
def extract_compliance_requirements(chunk_content: str, page_number: int) -> str:
    """
    Extract compliance requirements and obligations.
    
    Args:
        chunk_content: The text content to analyze
        page_number: Page number where this chunk appears
    
    Returns:
        Instructions for compliance extraction
    """
    return f"""Extract compliance requirements from this legal text in JSON format:
    {{
        "compliance_requirements": [
            {{
                "type": "obligation|prohibition|permission|procedure|penalty",
                "requirement_text": "exact text from document",
                "obligation_level": "mandatory|optional|conditional",
                "subject": "who must comply",
                "action": "what action is required/prohibited",
                "condition": "under what conditions",
                "penalty": "consequences of non-compliance if mentioned",
                "section_reference": "section where found",
                "confidence": 0.0-1.0
            }}
        ]
    }}
    
    Text to analyze:
    {chunk_content}
    
    Focus on obligations ("shall", "must"), prohibitions ("shall not"), and permissions ("may").
    """

@tool
def generate_rdf_triples(extracted_data: str, document_uri: str) -> str:
    """
    Generate RDF triples from extracted legal entities and data.
    
    Args:
        extracted_data: JSON string containing extracted entities and other data
        document_uri: URI of the source document
    
    Returns:
        RDF triples in Turtle format
    """
    extraction_storage["document_uri"] = document_uri
    
    # Parse the extracted data safely
    fallback = {
        "entities": [],
        "cross_references": [],
        "temporal_elements": [],
        "compliance_requirements": []
    }
    
    data = SafeJSONParser.parse_json_safely(extracted_data, fallback)
    
    # Generate RDF with proper URI encoding
    rdf_parts = []
    
    # Add namespace declarations
    rdf_parts.append("@prefix legal: <http://example.org/legal/ontology/> .")
    rdf_parts.append("@prefix skos: <http://www.w3.org/2004/02/skos/core#> .")
    rdf_parts.append("@prefix prov: <http://www.w3.org/ns/prov#> .")
    rdf_parts.append("@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .")
    rdf_parts.append("@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .")
    rdf_parts.append("")
    
    # Generate triples for entities
    for entity in data.get("entities", []):
        entity_uri = URLEncoder.create_instance_uri(
            "http://example.org/legal/ontology/",
            entity.get("type", "Entity"),
            entity.get("text", ""),
            0
        )
        
        entity_type = entity.get("type", "Entity")
        legal_type_uri = f"http://example.org/legal/ontology/{URLEncoder.encode_uri_component(entity_type)}"
        
        rdf_parts.append(f"# Entity: {entity.get('text', '')[:50]}...")
        rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", legal_type_uri))
        rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, "http://www.w3.org/2004/02/skos/core#prefLabel", entity.get("text", ""), True))
        
        if entity.get("section_reference"):
            rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, "http://example.org/legal/ontology/regulationSection", entity["section_reference"], True))
        
        if entity.get("confidence"):
            rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, "http://example.org/legal/ontology/confidence", str(entity["confidence"]), True))
        
        rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, "http://www.w3.org/ns/prov#wasDerivedFrom", document_uri))
        rdf_parts.append("")
    
    # Generate triples for cross-references
    for cross_ref in data.get("cross_references", []):
        ref_uri = URLEncoder.create_instance_uri(
            "http://example.org/legal/ontology/",
            "CrossReference",
            cross_ref.get("reference_text", ""),
            0
        )
        
        rdf_parts.append(f"# Cross-reference: {cross_ref.get('reference_text', '')}")
        rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", "http://example.org/legal/ontology/CrossReference"))
        rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, "http://www.w3.org/2004/02/skos/core#prefLabel", cross_ref.get("reference_text", ""), True))
        
        if cross_ref.get("referenced_instrument"):
            rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, "http://example.org/legal/ontology/referencesInstrument", cross_ref["referenced_instrument"], True))
        
        if cross_ref.get("relationship"):
            rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, "http://example.org/legal/ontology/relationshipType", cross_ref["relationship"], True))
        
        rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, "http://www.w3.org/ns/prov#wasDerivedFrom", document_uri))
        rdf_parts.append("")
    
    # Generate triples for temporal elements
    for temporal in data.get("temporal_elements", []):
        temp_uri = URLEncoder.create_instance_uri(
            "http://example.org/legal/ontology/",
            "TemporalElement",
            temporal.get("date_text", ""),
            0
        )
        
        rdf_parts.append(f"# Temporal element: {temporal.get('date_text', '')}")
        rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", "http://example.org/legal/ontology/TemporalElement"))
        rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, "http://www.w3.org/2004/02/skos/core#prefLabel", temporal.get("date_text", ""), True))
        
        if temporal.get("type"):
            rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, "http://example.org/legal/ontology/temporalType", temporal["type"], True))
        
        if temporal.get("parsed_date"):
            rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, "http://example.org/legal/ontology/effectiveDate", temporal["parsed_date"], True))
        
        rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, "http://www.w3.org/ns/prov#wasDerivedFrom", document_uri))
        rdf_parts.append("")
    
    # Generate triples for compliance requirements
    for compliance in data.get("compliance_requirements", []):
        comp_uri = URLEncoder.create_instance_uri(
            "http://example.org/legal/ontology/",
            "ComplianceRequirement",
            compliance.get("requirement_text", ""),
            0
        )
        
        rdf_parts.append(f"# Compliance requirement: {compliance.get('requirement_text', '')[:50]}...")
        rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, "http://www.w3.org/1999/02/22-rdf-syntax-ns#type", "http://example.org/legal/ontology/ComplianceRequirement"))
        rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, "http://www.w3.org/2004/02/skos/core#prefLabel", compliance.get("requirement_text", ""), True))
        
        if compliance.get("obligation_level"):
            rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, "http://example.org/legal/ontology/obligationLevel", compliance["obligation_level"], True))
        
        if compliance.get("subject"):
            rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, "http://example.org/legal/ontology/complianceSubject", compliance["subject"], True))
        
        rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, "http://www.w3.org/ns/prov#wasDerivedFrom", document_uri))
        rdf_parts.append("")
    
    # Join all parts and clean the output
    turtle_output = "\n".join(rdf_parts)
    cleaned_output = RDFSerializer.clean_turtle_output(turtle_output)
    
    return cleaned_output

@tool
def standardize_and_validate_data(extracted_data: str, chunk_info: str) -> str:
    """
    Standardize and validate extracted legal data to ensure quality and consistency.
    
    Args:
        extracted_data: JSON string containing all extracted data (entities, cross_references, etc.)
        chunk_info: Information about the chunk being processed
    
    Returns:
        Instructions for data standardization and validation
    """
    return f"""Standardize and validate this extracted legal data. Focus on substance over form.

Extracted Data:
{extracted_data}

Chunk Info: {chunk_info}

Standardization Rules:
1. NO abbreviations: "Art." → "Article", "Sect." → "Section", "GDPR" → "General Data Protection Regulation"
2. Focus on CONTENT not references: Instead of "Article 6 applies", extract "Data processing requires lawful basis"
3. Extract actual RULES and RESTRICTIONS: What must be done? What is prohibited? Under what conditions?
4. Standardize entity types to exact ontology classes: DataController, DataProcessor, Conditions, Restrictions, Actions, Jurisdiction, Organisation
5. Ensure compliance requirements specify: WHO must do WHAT under WHICH conditions
6. Remove redundant article/section references - focus on the substantive content
7. Validate logical consistency: Do the extracted entities make sense together?

Return JSON:
{{
    "standardized_data": {{
        "entities": [{{
            "type": "exact ontology class name",
            "text": "substantive content without abbreviations",
            "description": "what this entity actually means/requires",
            "section_reference": "section where found",
            "confidence": 0.0-1.0
        }}],
        "cross_references": [{{
            "type": "regulation|directive|act|treaty",
            "reference_text": "full reference without abbreviations",
            "referenced_instrument": "complete instrument name",
            "relationship": "implements|modifies|references|supersedes",
            "substantive_content": "what this reference actually establishes"
        }}],
        "temporal_elements": [{{
            "type": "effective_date|deadline|transition_period",
            "date_text": "full date expression",
            "parsed_date": "YYYY-MM-DD or null",
            "applies_to": "what obligation/requirement this date governs"
        }}],
        "compliance_requirements": [{{
            "type": "obligation|prohibition|permission",
            "requirement_text": "clear statement of what must/must not be done",
            "obligation_level": "mandatory|optional|conditional",
            "subject": "who has this obligation (data controller, processor, etc.)",
            "action": "specific action required or prohibited",
            "conditions": "under what circumstances this applies"
        }}]
    }},
    "validation_notes": [
        "List of issues found and corrections made",
        "Quality improvements applied",
        "Standardizations performed"
    ],
    "quality_score": 0.0-1.0
}}

Focus on extracting the SUBSTANCE of legal requirements, not just structural references."""

@tool
def store_extraction_results(json_data: str, data_type: str) -> str:
    """
    Store extraction results for later processing.
    
    Args:
        json_data: JSON string containing extraction results
        data_type: Type of data (entities, cross_references, temporal_elements, compliance_requirements)
    
    Returns:
        Confirmation message
    """
    # Define fallback structure based on data type
    fallback_structures = {
        "entities": {"entities": []},
        "cross_references": {"cross_references": []},
        "temporal_elements": {"temporal_elements": []},
        "compliance_requirements": {"compliance_requirements": []}
    }
    
    fallback = fallback_structures.get(data_type, {data_type: []})
    
    # Use safe JSON parsing
    data = SafeJSONParser.parse_json_safely(json_data, fallback)
    
    if data_type in extraction_storage:
        if isinstance(extraction_storage[data_type], list):
            items_to_add = data.get(data_type, [])
            if isinstance(items_to_add, list):
                extraction_storage[data_type].extend(items_to_add)
            else:
                extraction_storage[data_type].append(items_to_add)
        else:
            extraction_storage[data_type] = data.get(data_type, {})
    
    items_count = len(data.get(data_type, [])) if isinstance(data.get(data_type), list) else 1
    return f"Successfully stored {items_count} {data_type} items"

# =============================================================================
# AGENT STATE
# =============================================================================

@dataclass
class AgentState:
    """State for the LangGraph workflow."""
    messages: List[BaseMessage] = field(default_factory=list)
    current_chunk: Optional[DocumentChunk] = None
    document_id: str = ""
    processing_step: str = "initial"
    extracted_entities: List[Dict[str, Any]] = field(default_factory=list)
    cross_references: List[Dict[str, Any]] = field(default_factory=list)
    temporal_elements: List[Dict[str, Any]] = field(default_factory=list)
    compliance_requirements: List[Dict[str, Any]] = field(default_factory=list)
    standardized_entities: List[Dict[str, Any]] = field(default_factory=list)
    standardized_cross_references: List[Dict[str, Any]] = field(default_factory=list)
    standardized_temporal_elements: List[Dict[str, Any]] = field(default_factory=list)
    standardized_compliance_requirements: List[Dict[str, Any]] = field(default_factory=list)
    validation_notes: List[str] = field(default_factory=list)
    quality_score: float = 0.0
    rdf_output: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# LANGGRAPH MULTI-AGENT ORCHESTRATOR
# =============================================================================

class LegalDocumentProcessor:
    """Main orchestrator using LangGraph React agents."""
    
    def __init__(self, config: GlobalConfig):
        self.config = config
        self.openai_manager = OpenAIManager(config)
        self.document_processor = DocumentProcessor(config)
        self.ontology_builder = LegalOntologyBuilder()
        
        # Initialize LangGraph agents
        self._setup_agents()
        self._build_workflow()
    
    def _setup_agents(self):
        """Setup LangGraph React agents."""
        # Entity extraction agent
        entity_tools = [extract_legal_entities, store_extraction_results]
        self.entity_agent = create_react_agent(
            self.openai_manager.llm,
            entity_tools,
            prompt="You are a legal entity extraction specialist. "
                   "Extract legal concepts, entities, and relationships from regulatory documents. "
                   "Always use the provided tools to analyze text and store results. "
                   "Focus on data controllers, processors, conditions, restrictions, and jurisdictions."
        )
        
        # Cross-reference agent
        cross_ref_tools = [extract_cross_references, store_extraction_results]
        self.cross_ref_agent = create_react_agent(
            self.openai_manager.llm,
            cross_ref_tools,
            prompt="You are a cross-reference specialist. "
                   "Find references to other regulations, articles, and legal instruments. "
                   "Use the provided tools to extract and store cross-reference data."
        )
        
        # Temporal analysis agent
        temporal_tools = [extract_temporal_elements, store_extraction_results]
        self.temporal_agent = create_react_agent(
            self.openai_manager.llm,
            temporal_tools,
            prompt="You are a temporal analysis specialist. "
                   "Extract dates, deadlines, and time-related information from legal documents. "
                   "Use the provided tools to analyze and store temporal data."
        )
        
        # Compliance agent
        compliance_tools = [extract_compliance_requirements, store_extraction_results]
        self.compliance_agent = create_react_agent(
            self.openai_manager.llm,
            compliance_tools,
            prompt="You are a compliance analysis specialist. "
                   "Extract obligations, prohibitions, and compliance requirements. "
                   "Use the provided tools to analyze and store compliance data."
        )
        
        # RDF generation agent
        rdf_tools = [generate_rdf_triples]
        self.rdf_agent = create_react_agent(
            self.openai_manager.llm,
            rdf_tools,
            prompt="You are an RDF generation specialist. "
                   "Convert extracted legal entities into semantic RDF triples. "
                   "Use SKOS and PROV-O compliance and ensure valid Turtle syntax."
        )
        
        # Data standardization and validation agent
        standardization_tools = [standardize_and_validate_data]
        self.standardization_agent = create_react_agent(
            self.openai_manager.llm,
            standardization_tools,
            prompt="You are a legal data standardization and validation specialist. "
                   "Your job is to clean, standardize, and validate extracted legal data. "
                   "Remove abbreviations, focus on substantive content over structural references, "
                   "and ensure data quality and logical consistency. "
                   "Extract actual rules, restrictions, and obligations rather than just article numbers."
        )
    
    def _build_workflow(self):
        """Build the LangGraph workflow."""
        workflow = StateGraph(AgentState)
        
        # Add agent nodes
        workflow.add_node("entity_extraction", self._run_entity_agent)
        workflow.add_node("cross_reference_extraction", self._run_cross_ref_agent)
        workflow.add_node("temporal_analysis", self._run_temporal_agent)
        workflow.add_node("compliance_analysis", self._run_compliance_agent)
        workflow.add_node("data_standardization", self._run_standardization_agent)
        workflow.add_node("rdf_generation", self._run_rdf_agent)
        workflow.add_node("consolidation", self._consolidate_results)
        
        # Define workflow edges
        workflow.add_edge(START, "entity_extraction")
        workflow.add_edge("entity_extraction", "cross_reference_extraction")
        workflow.add_edge("cross_reference_extraction", "temporal_analysis")
        workflow.add_edge("temporal_analysis", "compliance_analysis")
        workflow.add_edge("compliance_analysis", "data_standardization")
        workflow.add_edge("data_standardization", "rdf_generation")
        workflow.add_edge("rdf_generation", "consolidation")
        workflow.add_edge("consolidation", END)
        
        # Compile workflow
        memory = MemorySaver()
        self.workflow = workflow.compile(checkpointer=memory)
    
    async def _run_entity_agent(self, state: AgentState) -> AgentState:
        """Run the entity extraction agent with optimization."""
        if not state.current_chunk:
            return state
        
        chunk = state.current_chunk
        
        # Use optimized completion
        try:
            result = await self.openai_manager.get_optimized_completion(
                chunk.content, 
                "entity_extraction",
                self.config.reasoning_effort_level
            )
            
            # Parse the response safely
            content = result.get("content", "")
            parsed_data = SafeJSONParser.parse_json_safely(content, {"entities": []})
            
            state.extracted_entities = parsed_data.get("entities", [])
            
            # Add usage metrics to metadata
            if "usage" in result:
                state.metadata["entity_extraction_usage"] = result["usage"]
            
        except Exception as e:
            logger.warning(f"Entity extraction failed for chunk {chunk.chunk_index}: {e}")
            state.extracted_entities = []
        
        state.processing_step = "entities_extracted"
        logger.info(f"Extracted {len(state.extracted_entities)} entities from chunk {chunk.chunk_index}")
        return state
    
    async def _run_cross_ref_agent(self, state: AgentState) -> AgentState:
        """Run the cross-reference extraction agent with optimization."""
        if not state.current_chunk:
            return state
        
        chunk = state.current_chunk
        
        try:
            result = await self.openai_manager.get_optimized_completion(
                chunk.content, 
                "cross_reference",
                self.config.reasoning_effort_level
            )
            
            content = result.get("content", "")
            parsed_data = SafeJSONParser.parse_json_safely(content, {"cross_references": []})
            
            state.cross_references = parsed_data.get("cross_references", [])
            
            if "usage" in result:
                state.metadata["cross_reference_usage"] = result["usage"]
                
        except Exception as e:
            logger.warning(f"Cross-reference extraction failed for chunk {chunk.chunk_index}: {e}")
            state.cross_references = []
        
        state.processing_step = "cross_references_extracted"
        logger.info(f"Extracted {len(state.cross_references)} cross-references from chunk {chunk.chunk_index}")
        return state
    
    async def _run_temporal_agent(self, state: AgentState) -> AgentState:
        """Run the temporal analysis agent with optimization."""
        if not state.current_chunk:
            return state
        
        chunk = state.current_chunk
        
        try:
            result = await self.openai_manager.get_optimized_completion(
                chunk.content, 
                "temporal",
                self.config.reasoning_effort_level
            )
            
            content = result.get("content", "")
            parsed_data = SafeJSONParser.parse_json_safely(content, {"temporal_elements": []})
            
            state.temporal_elements = parsed_data.get("temporal_elements", [])
            
            if "usage" in result:
                state.metadata["temporal_usage"] = result["usage"]
                
        except Exception as e:
            logger.warning(f"Temporal extraction failed for chunk {chunk.chunk_index}: {e}")
            state.temporal_elements = []
        
        state.processing_step = "temporal_extracted"
        logger.info(f"Extracted {len(state.temporal_elements)} temporal elements from chunk {chunk.chunk_index}")
        return state
    
    async def _run_compliance_agent(self, state: AgentState) -> AgentState:
        """Run the compliance analysis agent with optimization."""
        if not state.current_chunk:
            return state
        
        chunk = state.current_chunk
        
        try:
            result = await self.openai_manager.get_optimized_completion(
                chunk.content, 
                "compliance",
                self.config.reasoning_effort_level
            )
            
            content = result.get("content", "")
            parsed_data = SafeJSONParser.parse_json_safely(content, {"compliance_requirements": []})
            
            state.compliance_requirements = parsed_data.get("compliance_requirements", [])
            
            if "usage" in result:
                state.metadata["compliance_usage"] = result["usage"]
                
        except Exception as e:
            logger.warning(f"Compliance extraction failed for chunk {chunk.chunk_index}: {e}")
            state.compliance_requirements = []
        
        state.processing_step = "compliance_extracted"
        logger.info(f"Extracted {len(state.compliance_requirements)} compliance requirements from chunk {chunk.chunk_index}")
        return state
    
    async def _run_standardization_agent(self, state: AgentState) -> AgentState:
        """Run the data standardization and validation agent."""
        if not state.current_chunk:
            return state
        
        # Compile all extracted data for standardization
        all_extracted_data = {
            "entities": state.extracted_entities,
            "cross_references": state.cross_references,
            "temporal_elements": state.temporal_elements,
            "compliance_requirements": state.compliance_requirements
        }
        
        chunk_info = f"Chunk {state.current_chunk.chunk_index}, Page {state.current_chunk.page_number}"
        
        try:
            # Create input message for standardization agent
            input_msg = HumanMessage(
                content=f"Please standardize and validate this extracted legal data:\n\n"
                       f"Data to standardize: {json.dumps(all_extracted_data, indent=2)}\n"
                       f"Chunk info: {chunk_info}"
            )
            
            result = await self.standardization_agent.ainvoke({
                "messages": [input_msg]
            })
            
            # Extract standardized data from agent response
            standardized_content = ""
            for msg in result.get("messages", []):
                if hasattr(msg, 'content') and isinstance(msg.content, str):
                    content = str(msg.content)
                    if 'standardized_data' in content:
                        standardized_content = content
                        break
            
            # Parse the standardized response
            if standardized_content:
                parsed_response = SafeJSONParser.parse_json_safely(
                    standardized_content, 
                    {
                        "standardized_data": {
                            "entities": [],
                            "cross_references": [],
                            "temporal_elements": [],
                            "compliance_requirements": []
                        },
                        "validation_notes": [],
                        "quality_score": 0.0
                    }
                )
                
                standardized_data = parsed_response.get("standardized_data", {})
                state.standardized_entities = standardized_data.get("entities", [])
                state.standardized_cross_references = standardized_data.get("cross_references", [])
                state.standardized_temporal_elements = standardized_data.get("temporal_elements", [])
                state.standardized_compliance_requirements = standardized_data.get("compliance_requirements", [])
                state.validation_notes = parsed_response.get("validation_notes", [])
                state.quality_score = parsed_response.get("quality_score", 0.0)
                
                logger.info(f"Standardized data - Quality score: {state.quality_score:.2f}")
                if state.validation_notes:
                    logger.info(f"Validation notes: {'; '.join(state.validation_notes[:3])}")
            else:
                # Fallback: use original data if standardization fails
                logger.warning("Standardization failed, using original data")
                state.standardized_entities = state.extracted_entities
                state.standardized_cross_references = state.cross_references
                state.standardized_temporal_elements = state.temporal_elements
                state.standardized_compliance_requirements = state.compliance_requirements
                state.quality_score = 0.5
                
        except Exception as e:
            logger.warning(f"Data standardization failed for chunk {state.current_chunk.chunk_index}: {e}")
            # Use original data as fallback
            state.standardized_entities = state.extracted_entities
            state.standardized_cross_references = state.cross_references
            state.standardized_temporal_elements = state.temporal_elements
            state.standardized_compliance_requirements = state.compliance_requirements
            state.quality_score = 0.5
        
        state.processing_step = "data_standardized"
        total_standardized = (len(state.standardized_entities) + 
                            len(state.standardized_cross_references) + 
                            len(state.standardized_temporal_elements) + 
                            len(state.standardized_compliance_requirements))
        logger.info(f"Standardized {total_standardized} data items from chunk {state.current_chunk.chunk_index}")
        return state
    
    async def _run_rdf_agent(self, state: AgentState) -> AgentState:
        """Run the RDF generation agent using standardized data."""
        # Use standardized data if available, otherwise fall back to raw data
        entities_to_use = state.standardized_entities if state.standardized_entities else state.extracted_entities
        cross_refs_to_use = state.standardized_cross_references if state.standardized_cross_references else state.cross_references
        temporal_to_use = state.standardized_temporal_elements if state.standardized_temporal_elements else state.temporal_elements
        compliance_to_use = state.standardized_compliance_requirements if state.standardized_compliance_requirements else state.compliance_requirements
        
        # Compile all standardized data
        all_data = {
            "entities": entities_to_use,
            "cross_references": cross_refs_to_use,
            "temporal_elements": temporal_to_use,
            "compliance_requirements": compliance_to_use,
            "quality_score": state.quality_score,
            "validation_notes": state.validation_notes
        }
        
        # Create properly encoded document URI
        document_uri = URLEncoder.create_document_uri(CONFIG.base_namespace, state.document_id)
        
        input_msg = HumanMessage(
            content=f"Generate RDF triples from this extracted data:\n\n"
                   f"Document URI: {document_uri}\n"
                   f"Data: {json.dumps(all_data, indent=2)}"
        )
        
        try:
            result = await self.rdf_agent.ainvoke({
                "messages": [input_msg]
            })
            
            # Extract and clean RDF from the agent's response
            rdf_content = ""
            for msg in result.get("messages", []):
                if hasattr(msg, 'content') and isinstance(msg.content, str):
                    content = str(msg.content)
                    
                    # Look for RDF content (Turtle format)
                    if '@prefix' in content or ('<' in content and '>' in content and '.' in content):
                        # Clean and validate the RDF content
                        rdf_content = RDFSerializer.clean_turtle_output(content)
                        break
            
            # If no RDF found in messages, generate it directly
            if not rdf_content:
                logger.info("No RDF found in agent response, generating directly")
                rdf_content = self._generate_direct_rdf(all_data, document_uri)
            
            state.rdf_output = rdf_content
            state.messages.extend(result.get("messages", []))
            
        except Exception as e:
            logger.warning(f"RDF agent failed, generating directly: {e}")
            state.rdf_output = self._generate_direct_rdf(all_data, document_uri)
        
        state.processing_step = "rdf_generated"
        
        logger.info(f"Generated RDF output for chunk {state.current_chunk.chunk_index if state.current_chunk else 'unknown'}")
        return state
    
    def _generate_direct_rdf(self, all_data: Dict[str, Any], document_uri: str) -> str:
        """Generate RDF directly when agent fails to produce valid output."""
        rdf_parts = []
        
        # Add namespace declarations
        rdf_parts.append("@prefix legal: <http://example.org/legal/ontology/> .")
        rdf_parts.append("@prefix skos: <http://www.w3.org/2004/02/skos/core#> .")
        rdf_parts.append("@prefix prov: <http://www.w3.org/ns/prov#> .")
        rdf_parts.append("@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .")
        rdf_parts.append("@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .")
        rdf_parts.append("")
        
        # Process entities
        for i, entity in enumerate(all_data.get("entities", [])):
            entity_uri = URLEncoder.create_instance_uri(
                CONFIG.ontology_namespace,
                entity.get("type", "Entity"),
                entity.get("text", f"entity_{i}"),
                i
            )
            
            entity_type_uri = f"{CONFIG.ontology_namespace}{URLEncoder.encode_uri_component(entity.get('type', 'Entity'))}"
            
            rdf_parts.append(f"# Entity: {entity.get('text', '')}")
            rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, str(RDF.type), entity_type_uri))
            rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, str(SKOS.prefLabel), entity.get("text", ""), True))
            
            if entity.get("section_reference"):
                rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, f"{CONFIG.ontology_namespace}regulationSection", entity["section_reference"], True))
            
            if entity.get("confidence"):
                rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, f"{CONFIG.ontology_namespace}confidence", str(entity["confidence"]), True))
            
            rdf_parts.append(RDFSerializer.serialize_triple(entity_uri, str(PROV.wasDerivedFrom), document_uri))
            rdf_parts.append("")
        
        # Process cross-references
        for i, cross_ref in enumerate(all_data.get("cross_references", [])):
            ref_uri = URLEncoder.create_instance_uri(
                CONFIG.ontology_namespace,
                "CrossReference",
                cross_ref.get("reference_text", f"crossref_{i}"),
                i
            )
            
            rdf_parts.append(f"# Cross-reference: {cross_ref.get('reference_text', '')}")
            rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, str(RDF.type), f"{CONFIG.ontology_namespace}CrossReference"))
            rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, str(SKOS.prefLabel), cross_ref.get("reference_text", ""), True))
            
            if cross_ref.get("referenced_instrument"):
                rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, f"{CONFIG.ontology_namespace}referencesInstrument", cross_ref["referenced_instrument"], True))
            
            if cross_ref.get("relationship"):
                rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, f"{CONFIG.ontology_namespace}relationshipType", cross_ref["relationship"], True))
            
            rdf_parts.append(RDFSerializer.serialize_triple(ref_uri, str(PROV.wasDerivedFrom), document_uri))
            rdf_parts.append("")
        
        # Process temporal elements
        for i, temporal in enumerate(all_data.get("temporal_elements", [])):
            temp_uri = URLEncoder.create_instance_uri(
                CONFIG.ontology_namespace,
                "TemporalElement",
                temporal.get("date_text", f"temporal_{i}"),
                i
            )
            
            rdf_parts.append(f"# Temporal element: {temporal.get('date_text', '')}")
            rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, str(RDF.type), f"{CONFIG.ontology_namespace}TemporalElement"))
            rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, str(SKOS.prefLabel), temporal.get("date_text", ""), True))
            
            if temporal.get("type"):
                rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, f"{CONFIG.ontology_namespace}temporalType", temporal["type"], True))
            
            if temporal.get("parsed_date"):
                rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, f"{CONFIG.ontology_namespace}effectiveDate", temporal["parsed_date"], True))
            
            rdf_parts.append(RDFSerializer.serialize_triple(temp_uri, str(PROV.wasDerivedFrom), document_uri))
            rdf_parts.append("")
        
        # Process compliance requirements
        for i, compliance in enumerate(all_data.get("compliance_requirements", [])):
            comp_uri = URLEncoder.create_instance_uri(
                CONFIG.ontology_namespace,
                "ComplianceRequirement",
                compliance.get("requirement_text", f"compliance_{i}"),
                i
            )
            
            rdf_parts.append(f"# Compliance requirement: {compliance.get('requirement_text', '')[:50]}...")
            rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, str(RDF.type), f"{CONFIG.ontology_namespace}ComplianceRequirement"))
            rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, str(SKOS.prefLabel), compliance.get("requirement_text", ""), True))
            
            if compliance.get("obligation_level"):
                rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, f"{CONFIG.ontology_namespace}obligationLevel", compliance["obligation_level"], True))
            
            if compliance.get("subject"):
                rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, f"{CONFIG.ontology_namespace}complianceSubject", compliance["subject"], True))
            
            rdf_parts.append(RDFSerializer.serialize_triple(comp_uri, str(PROV.wasDerivedFrom), document_uri))
            rdf_parts.append("")
        
        return "\n".join(rdf_parts)
    
    async def _consolidate_results(self, state: AgentState) -> AgentState:
        """Consolidate the processing results with performance metrics."""
        # Use standardized data if available
        entities_count = len(state.standardized_entities) if state.standardized_entities else len(state.extracted_entities)
        cross_refs_count = len(state.standardized_cross_references) if state.standardized_cross_references else len(state.cross_references)
        temporal_count = len(state.standardized_temporal_elements) if state.standardized_temporal_elements else len(state.temporal_elements)
        compliance_count = len(state.standardized_compliance_requirements) if state.standardized_compliance_requirements else len(state.compliance_requirements)
        
        total_items = entities_count + cross_refs_count + temporal_count + compliance_count
        
        state.metadata.update({
            "entities_count": entities_count,
            "cross_references_count": cross_refs_count,
            "temporal_elements_count": temporal_count,
            "compliance_requirements_count": compliance_count,
            "total_extracted_items": total_items,
            "standardization_applied": bool(state.standardized_entities or state.standardized_cross_references or 
                                          state.standardized_temporal_elements or state.standardized_compliance_requirements),
            "quality_score": state.quality_score,
            "validation_notes_count": len(state.validation_notes),
            "rdf_triples_generated": bool(state.rdf_output),
            "processing_completed": True,
            "chunk_priority_score": state.current_chunk.metadata.get('priority_score', 0.0) if state.current_chunk else 0.0
        })
        
        # Calculate quality score based on extraction density
        if state.current_chunk:
            content_length = len(state.current_chunk.content)
            extraction_density = total_items / (content_length / 1000) if content_length > 0 else 0
            state.metadata["extraction_density"] = extraction_density
        
        state.processing_step = "completed"
        logger.debug(f"Chunk processing consolidated: {total_items} items extracted")
        return state
    
    async def process_document(self, doc_info: DocumentInfo) -> Graph:
        """Process a complete document with optimizations and return RDF graph."""
        start_time = asyncio.get_event_loop().time()
        logger.info(f"Processing document: {doc_info.file_path}")
        
        # Extract text from document
        file_path = Path(doc_info.file_path)
        if file_path.suffix.lower() == '.pdf':
            text, page_count = self.document_processor.extract_text_from_pdf(str(file_path))
        elif file_path.suffix.lower() == '.docx':
            text, page_count = self.document_processor.extract_text_from_docx(str(file_path))
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")
        
        doc_info.total_pages = page_count
        
        # Generate document ID
        document_id = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        
        # Chunk the document using optimized chunking
        chunks = self.document_processor.chunk_text_optimized(text, document_id)
        doc_info.total_chunks = len(chunks)
        
        logger.info(f"Created {len(chunks)} chunks from {page_count} pages")
        
        # Initialize final RDF graph with ontology
        final_graph = Graph()
        final_graph.parse(data=self.ontology_builder.serialize(), format="turtle")
        
        # Track extraction statistics for early stopping
        total_entities_found = 0
        processed_chunks = 0
        
        # Process chunks with optimizations
        for i in range(0, len(chunks), self.config.batch_size):
            batch = chunks[i:i + self.config.batch_size]
            batch_num = i // self.config.batch_size + 1
            total_batches = (len(chunks) - 1) // self.config.batch_size + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)")
            
            # Process batch with maximum concurrency - no blocking
            batch_tasks = [
                self._process_single_chunk_optimized(chunk, document_id) 
                for chunk in batch
            ]
            
            # Execute all chunks in parallel with exception handling
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Consolidate results with metrics tracking
            batch_entities = 0
            for result in batch_results:
                # Handle exceptions from individual chunks
                if isinstance(result, Exception):
                    logger.warning(f"Chunk processing failed: {result}")
                    continue
                
                state_result = result
                if state_result and state_result.rdf_output:
                    # Count entities for early stopping (use standardized if available)
                    if state_result.standardized_entities:
                        batch_entities += len(state_result.standardized_entities)
                        batch_entities += len(state_result.standardized_cross_references)
                        batch_entities += len(state_result.standardized_temporal_elements)
                        batch_entities += len(state_result.standardized_compliance_requirements)
                    else:
                        batch_entities += len(state_result.extracted_entities)
                        batch_entities += len(state_result.cross_references)
                        batch_entities += len(state_result.temporal_elements)
                        batch_entities += len(state_result.compliance_requirements)
                    
                    # Parse and add RDF to final graph with error handling
                    try:
                        cleaned_rdf = RDFSerializer.clean_turtle_output(state_result.rdf_output)
                        temp_graph = Graph()
                        temp_graph.parse(data=cleaned_rdf, format="turtle")
                        validated_graph = self._validate_and_fix_graph_uris(temp_graph)
                        final_graph += validated_graph
                        
                    except Exception as e:
                        logger.warning(f"Error parsing RDF from chunk: {e}")
                        try:
                            recovered_graph = self._recover_rdf_from_text(state_result.rdf_output)
                            final_graph += recovered_graph
                        except Exception:
                            continue
            
            total_entities_found += batch_entities
            processed_chunks += len([r for r in batch_results if not isinstance(r, Exception)])
            
            # Early stopping check
            if (self.config.early_stopping_threshold > 0 and 
                total_entities_found >= self.config.early_stopping_threshold and
                processed_chunks >= len(chunks) * 0.3):  # At least 30% processed
                
                logger.info(f"Early stopping: Found {total_entities_found} entities after processing "
                           f"{processed_chunks}/{len(chunks)} chunks")
                break
            
            # Performance logging
            elapsed_time = asyncio.get_event_loop().time() - start_time
            logger.info(f"Batch {batch_num} completed in {elapsed_time:.2f}s. "
                       f"Total entities found: {total_entities_found}")
        
        # Close OpenAI connections
        await self.openai_manager.close()
        
        # Add document-level metadata with proper URI encoding
        doc_uri = URIRef(URLEncoder.create_document_uri(CONFIG.base_namespace, document_id))
        final_graph.add((doc_uri, RDF.type, LEGAL["Document"]))
        final_graph.add((doc_uri, RDFS.label, Literal(doc_info.title or file_path.name)))
        final_graph.add((doc_uri, LEGAL["jurisdiction"], Literal(doc_info.jurisdiction)))
        final_graph.add((doc_uri, PROV["generatedAtTime"], 
                        Literal(datetime.now(timezone.utc).isoformat(), datatype=XSD.dateTime)))
        
        # Add processing metadata
        processing_time = asyncio.get_event_loop().time() - start_time
        final_graph.add((doc_uri, LEGAL["processingTime"], Literal(f"{processing_time:.2f}s")))
        final_graph.add((doc_uri, LEGAL["entitiesExtracted"], Literal(total_entities_found)))
        final_graph.add((doc_uri, LEGAL["chunksProcessed"], Literal(processed_chunks)))
        
        # Validate the final graph
        try:
            test_serialization = final_graph.serialize(format='turtle')
            logger.info("RDF graph validation successful")
        except Exception as e:
            logger.warning(f"RDF graph validation warning: {e}")
            final_graph = self._clean_rdf_graph(final_graph)
        
        logger.info(f"Document processing completed in {processing_time:.2f}s. "
                   f"Generated {len(final_graph)} RDF triples from {total_entities_found} entities.")
        return final_graph
    
    async def _process_single_chunk_optimized(self, chunk: DocumentChunk, document_id: str) -> AgentState:
        """Process a single chunk with maximum speed optimizations."""
        # Create initial state
        initial_state = AgentState(
            current_chunk=chunk,
            document_id=document_id,
            processing_step="initial"
        )
        
        try:
            # Run all agents concurrently for maximum speed
            agent_tasks = [
                self._run_entity_agent(initial_state),
                self._run_cross_ref_agent(initial_state),
                self._run_temporal_agent(initial_state),
                self._run_compliance_agent(initial_state)
            ]
            
            # Execute all agents in parallel
            agent_results = await asyncio.gather(*agent_tasks, return_exceptions=True)
            
            # Consolidate results from all agents
            final_state = initial_state
            for i, result in enumerate(agent_results):
                if isinstance(result, Exception):
                    logger.warning(f"Agent {i} failed: {result}")
                    continue
                
                # Merge results based on agent type
                if i == 0:  # Entity agent
                    final_state.extracted_entities = result.extracted_entities
                elif i == 1:  # Cross-ref agent
                    final_state.cross_references = result.cross_references
                elif i == 2:  # Temporal agent
                    final_state.temporal_elements = result.temporal_elements
                elif i == 3:  # Compliance agent
                    final_state.compliance_requirements = result.compliance_requirements
                
                # Merge metadata
                final_state.metadata.update(result.metadata)
            
            # Run standardization on consolidated results
            final_state = await self._run_standardization_agent(final_state)
            
            # Generate RDF from standardized results
            final_state = await self._run_rdf_agent(final_state)
            final_state = await self._consolidate_results(final_state)
            
            return final_state
            
        except Exception as e:
            logger.error(f"Error processing chunk {chunk.chunk_index}: {e}")
            # Return state with empty results rather than failing
            initial_state.processing_step = "failed"
            initial_state.metadata["error"] = str(e)
            return initial_state
    
    def _clean_rdf_graph(self, graph: Graph) -> Graph:
        """Clean and validate RDF graph, removing problematic triples."""
        cleaned_graph = Graph()
        
        # Copy namespace bindings
        for prefix, namespace in graph.namespaces():
            cleaned_graph.bind(prefix, namespace)
        
        # Add triples one by one, skipping problematic ones
        for subject, predicate, obj in graph:
            try:
                # Validate URIs
                if isinstance(subject, URIRef) and not URLEncoder.validate_uri(str(subject)):
                    continue
                if isinstance(predicate, URIRef) and not URLEncoder.validate_uri(str(predicate)):
                    continue
                if isinstance(obj, URIRef) and not URLEncoder.validate_uri(str(obj)):
                    continue
                
                # Add the triple
                cleaned_graph.add((subject, predicate, obj))
            except Exception as e:
                logger.warning(f"Skipping problematic triple: {subject} {predicate} {obj} - {e}")
                continue
        
        return cleaned_graph
    
    def _validate_and_fix_graph_uris(self, graph: Graph) -> Graph:
        """Validate and fix URIs in an RDF graph."""
        fixed_graph = Graph()
        
        # Copy namespace bindings
        for prefix, namespace in graph.namespaces():
            fixed_graph.bind(prefix, namespace)
        
        for subject, predicate, obj in graph:
            try:
                # Fix subject URI if needed
                if isinstance(subject, URIRef):
                    subject_str = str(subject)
                    if not URLEncoder.validate_uri(subject_str):
                        # Try to fix the URI
                        if subject_str.startswith('http'):
                            # Re-encode the path part
                            parts = subject_str.split('/')
                            if len(parts) > 3:
                                path_part = parts[-1]
                                encoded_path = URLEncoder.encode_uri_component(path_part)
                                subject = URIRef('/'.join(parts[:-1]) + '/' + encoded_path)
                
                # Fix predicate URI if needed
                if isinstance(predicate, URIRef):
                    predicate_str = str(predicate)
                    if not URLEncoder.validate_uri(predicate_str):
                        # Skip problematic predicates or try to fix
                        continue
                
                # Fix object URI if needed
                if isinstance(obj, URIRef):
                    obj_str = str(obj)
                    if not URLEncoder.validate_uri(obj_str):
                        # Try to fix the URI
                        if obj_str.startswith('http'):
                            parts = obj_str.split('/')
                            if len(parts) > 3:
                                path_part = parts[-1]
                                encoded_path = URLEncoder.encode_uri_component(path_part)
                                obj = URIRef('/'.join(parts[:-1]) + '/' + encoded_path)
                
                # Add the fixed triple
                fixed_graph.add((subject, predicate, obj))
                
            except Exception as e:
                logger.debug(f"Skipping problematic triple during URI validation: {e}")
                continue
        
        return fixed_graph
    
    def _recover_rdf_from_text(self, rdf_text: str) -> Graph:
        """Attempt to recover RDF triples from malformed text."""
        recovery_graph = Graph()
        
        # Set up namespaces
        recovery_graph.bind("legal", LEGAL)
        recovery_graph.bind("skos", SKOS)
        recovery_graph.bind("prov", PROV)
        recovery_graph.bind("rdfs", RDFS)
        
        try:
            # Look for triple patterns in the text
            triple_pattern = r'<([^>]+)>\s+<([^>]+)>\s+(?:<([^>]+)>|"([^"]*)")\s*\.'
            
            for match in re.finditer(triple_pattern, rdf_text):
                try:
                    subject_uri = match.group(1)
                    predicate_uri = match.group(2)
                    object_uri = match.group(3)
                    object_literal = match.group(4)
                    
                    # Create proper URIs
                    subject = URIRef(subject_uri)
                    predicate = URIRef(predicate_uri)
                    
                    if object_uri:
                        obj = URIRef(object_uri)
                    else:
                        obj = Literal(object_literal)
                    
                    recovery_graph.add((subject, predicate, obj))
                    
                except Exception as e:
                    logger.debug(f"Failed to recover triple from match: {e}")
                    continue
        
        except Exception as e:
            logger.warning(f"RDF recovery failed: {e}")
        
        return recovery_graph

# =============================================================================
# BATCH PROCESSING MANAGER
# =============================================================================

class BatchProcessor:
    """Manages batch processing of multiple documents."""
    
    def __init__(self, config: GlobalConfig):
        self.config = config
        self.processor = LegalDocumentProcessor(config)
    
    def load_document_config(self) -> List[DocumentInfo]:
        """Load document configuration from JSON file."""
        config_path = Path(self.config.config_file)
        if not config_path.exists():
            # Create example config
            example_config = {
                "documents": [
                    {
                        "file_path": "sample_regulation.pdf",
                        "jurisdiction": "EU",
                        "document_type": "regulation",
                        "title": "Sample EU Regulation"
                    }
                ]
            }
            with open(config_path, 'w') as f:
                json.dump(example_config, f, indent=2)
            logger.info(f"Created example config file: {config_path}")
            return []
        
        with open(config_path, 'r') as f:
            config_data = json.load(f)
        
        documents = []
        for doc_config in config_data.get("documents", []):
            doc_info = DocumentInfo(
                file_path=os.path.join(self.config.input_folder, doc_config["file_path"]),
                jurisdiction=doc_config["jurisdiction"],
                document_type=doc_config.get("document_type", "regulation"),
                title=doc_config.get("title", "")
            )
            
            if Path(doc_info.file_path).exists():
                documents.append(doc_info)
            else:
                logger.warning(f"Document not found: {doc_info.file_path}")
        
        return documents
    
    async def process_all_documents(self):
        """Process all documents in the configuration."""
        documents = self.load_document_config()
        
        if not documents:
            logger.warning("No documents to process. Check configuration file.")
            return
        
        logger.info(f"Starting batch processing of {len(documents)} documents")
        
        # Process documents
        for i, doc_info in enumerate(documents):
            try:
                logger.info(f"Processing document {i+1}/{len(documents)}: {doc_info.file_path}")
                
                # Process document
                rdf_graph = await self.processor.process_document(doc_info)
                
                # Save output
                output_filename = f"{Path(doc_info.file_path).stem}_{doc_info.jurisdiction}.ttl"
                output_path = Path(self.config.output_folder) / output_filename
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(rdf_graph.serialize(format='turtle'))
                
                logger.info(f"Saved RDF output to: {output_path}")
                
                # Generate summary report
                self._generate_report(doc_info, rdf_graph, output_path)
                
            except Exception as e:
                logger.error(f"Error processing document {doc_info.file_path}: {e}")
                continue
        
        logger.info("Batch processing completed")
    
    def _generate_report(self, doc_info: DocumentInfo, rdf_graph: Graph, output_path: Path):
        """Generate processing report with validation metrics."""
        report = {
            "document": doc_info.file_path,
            "jurisdiction": doc_info.jurisdiction,
            "processing_timestamp": datetime.now(timezone.utc).isoformat(),
            "total_pages": doc_info.total_pages,
            "total_chunks": doc_info.total_chunks,
            "rdf_triples_count": len(rdf_graph),
            "output_file": str(output_path),
            "ontology_classes_used": [],
            "namespaces": {},
            "validation_metrics": {
                "valid_uris": 0,
                "invalid_uris": 0,
                "total_subjects": 0,
                "total_predicates": 0,
                "total_objects": 0,
                "literal_objects": 0,
                "uri_objects": 0
            },
            "standardization_metrics": {
                "standardization_applied": False,
                "average_quality_score": 0.0,
                "total_validation_notes": 0,
                "chunks_standardized": 0
            }
        }
        
        # Extract used classes and namespaces with validation
        classes_used = set()
        namespaces = {}
        validation_metrics = report["validation_metrics"]
        
        for s, p, o in rdf_graph:
            # Count subjects, predicates, objects
            validation_metrics["total_subjects"] += 1
            validation_metrics["total_predicates"] += 1
            validation_metrics["total_objects"] += 1
            
            # Validate URIs
            if isinstance(s, URIRef):
                if URLEncoder.validate_uri(str(s)):
                    validation_metrics["valid_uris"] += 1
                else:
                    validation_metrics["invalid_uris"] += 1
            
            if isinstance(p, URIRef):
                if URLEncoder.validate_uri(str(p)):
                    validation_metrics["valid_uris"] += 1
                else:
                    validation_metrics["invalid_uris"] += 1
            
            if isinstance(o, URIRef):
                validation_metrics["uri_objects"] += 1
                if URLEncoder.validate_uri(str(o)):
                    validation_metrics["valid_uris"] += 1
                else:
                    validation_metrics["invalid_uris"] += 1
            elif isinstance(o, Literal):
                validation_metrics["literal_objects"] += 1
            
            # Collect classes
            if p == RDF.type and isinstance(o, URIRef):
                classes_used.add(str(o))
            
            # Extract namespaces
            for term in [s, p, o]:
                if isinstance(term, URIRef):
                    term_str = str(term)
                    if '#' in term_str:
                        namespace = term_str.rsplit('#', 1)[0] + '#'
                    else:
                        namespace = term_str.rsplit('/', 1)[0] + '/'
                    namespaces[namespace] = namespaces.get(namespace, 0) + 1
        
        report["ontology_classes_used"] = list(classes_used)
        report["namespaces"] = namespaces
        
        # Calculate quality scores
        total_uris = validation_metrics["valid_uris"] + validation_metrics["invalid_uris"]
        if total_uris > 0:
            report["uri_quality_score"] = validation_metrics["valid_uris"] / total_uris
        else:
            report["uri_quality_score"] = 0.0
        
        # Save report
        report_path = output_path.with_suffix('.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Generated report: {report_path}")
        logger.info(f"URI Quality Score: {report['uri_quality_score']:.2f}")
        logger.info(f"Valid URIs: {validation_metrics['valid_uris']}, Invalid URIs: {validation_metrics['invalid_uris']}")
        logger.info(f"Standardization Quality: {report['standardization_metrics']['average_quality_score']:.2f}")

# =============================================================================
# PERFORMANCE OPTIMIZATION EXAMPLES AND CONFIGURATIONS
# =============================================================================

class PerformanceOptimizer:
    """Performance optimization utilities and configurations."""
    
    @staticmethod
    def get_speed_optimized_config() -> GlobalConfig:
        """Get configuration optimized for maximum speed."""
        config = GlobalConfig()
        config.reasoning_effort_level = "low"
        config.request_timeout = 45  # Shorter timeout for speed
        config.batch_size = 5  # Larger batches for more parallelism
        config.max_chunk_size = 2500
        config.chunk_overlap = 100
        config.content_priority_threshold = 0.8  # Only process high-priority content
        config.early_stopping_threshold = 30
        config.enable_streaming = True
        config.enable_caching = True
        config.smart_chunking = True
        config.enable_progressive_processing = True
        config.connection_pool_size = 30  # More connections for speed
        return config
    
    @staticmethod
    def get_quality_optimized_config() -> GlobalConfig:
        """Get configuration optimized for maximum quality."""
        config = GlobalConfig()
        config.reasoning_effort_level = "high"
        config.request_timeout = 90  # Longer timeout for quality
        config.batch_size = 2  # Smaller batches for thorough processing
        config.max_chunk_size = 4000
        config.chunk_overlap = 300
        config.content_priority_threshold = 0.0  # Process all content
        config.early_stopping_threshold = 0  # No early stopping
        config.enable_streaming = False
        config.enable_caching = True
        config.smart_chunking = True
        config.enable_progressive_processing = False
        config.connection_pool_size = 10  # Fewer connections for stability
        return config
    
    @staticmethod
    def get_balanced_config() -> GlobalConfig:
        """Get configuration with balanced speed/quality tradeoff."""
        config = GlobalConfig()
        config.reasoning_effort_level = "medium"
        config.request_timeout = 60  # Balanced timeout
        config.batch_size = 3
        config.max_chunk_size = 3000
        config.chunk_overlap = 200
        config.content_priority_threshold = 0.5
        config.early_stopping_threshold = 50
        config.enable_streaming = True
        config.enable_caching = True
        config.smart_chunking = True
        config.enable_progressive_processing = True
        config.connection_pool_size = 20
        return config
    
    @staticmethod
    def estimate_processing_time(doc_info: DocumentInfo, config: GlobalConfig) -> float:
        """Estimate processing time based on document size and configuration."""
        # Base time per page (seconds)
        base_time_per_page = {
            "low": 1.5,      # Faster with no rate limiting
            "medium": 3.0,   # Reduced from previous estimates
            "high": 8.0      # Much faster without blocking
        }.get(config.reasoning_effort_level, 3.0)
        
        # Adjust for batch size (larger batches = more parallelism = faster)
        batch_factor = max(0.5, 1.0 / config.batch_size)
        
        # Adjust for early stopping
        early_stop_factor = 0.6 if config.early_stopping_threshold > 0 else 1.0
        
        # Adjust for content filtering
        filtering_factor = 0.5 if config.content_priority_threshold > 0.5 else 1.0
        
        # Adjust for connection pooling (more connections = faster)
        connection_factor = max(0.3, 10.0 / config.connection_pool_size)
        
        estimated_time = (doc_info.total_pages * base_time_per_page * 
                         batch_factor * early_stop_factor * filtering_factor * connection_factor)
        
        return estimated_time
    
    @staticmethod
    def log_performance_metrics(start_time: float, end_time: float, 
                               total_entities: int, chunks_processed: int):
        """Log comprehensive performance metrics."""
        processing_time = end_time - start_time
        entities_per_second = total_entities / processing_time if processing_time > 0 else 0
        chunks_per_second = chunks_processed / processing_time if processing_time > 0 else 0
        
        logger.info("=" * 50)
        logger.info("PERFORMANCE METRICS")
        logger.info("=" * 50)
        logger.info(f"Total Processing Time: {processing_time:.2f} seconds")
        logger.info(f"Entities Extracted: {total_entities}")
        logger.info(f"Chunks Processed: {chunks_processed}")
        logger.info(f"Entities/Second: {entities_per_second:.2f}")
        logger.info(f"Chunks/Second: {chunks_per_second:.2f}")
        logger.info("=" * 50)

# =============================================================================
# OPTIMIZED MAIN EXECUTION WITH PERFORMANCE OPTIONS
# =============================================================================

async def main_optimized(performance_mode: str = "balanced"):
    """
    Main execution function with performance optimization options.
    
    Args:
        performance_mode: "speed", "quality", or "balanced"
    """
    start_time = asyncio.get_event_loop().time()
    
    # Select configuration based on performance mode
    if performance_mode == "speed":
        config = PerformanceOptimizer.get_speed_optimized_config()
        logger.info("Running in SPEED optimization mode")
    elif performance_mode == "quality":
        config = PerformanceOptimizer.get_quality_optimized_config()
        logger.info("Running in QUALITY optimization mode")
    else:
        config = PerformanceOptimizer.get_balanced_config()
        logger.info("Running in BALANCED optimization mode")
    
    logger.info(f"Configuration: reasoning_effort={config.reasoning_effort_level}, "
               f"batch_size={config.batch_size}, "
               f"connection_pool={config.connection_pool_size}")
    
    # Initialize batch processor with optimized config
    batch_processor = BatchProcessor(config)
    
    # Create ontology file
    ontology_builder = LegalOntologyBuilder()
    ontology_path = Path(config.ontology_file)
    with open(ontology_path, 'w', encoding='utf-8') as f:
        f.write(ontology_builder.serialize())
    logger.info(f"Created ontology file: {ontology_path}")
    
    # Process all documents
    await batch_processor.process_all_documents()
    
    end_time = asyncio.get_event_loop().time()
    logger.info(f"System execution completed in {end_time - start_time:.2f} seconds")

async def main():
    """Main execution function with default balanced optimization."""
    await main_optimized("balanced")

if __name__ == "__main__":
    # Set up event loop for async execution
    import sys
    
    # Allow performance mode selection from command line
    performance_mode = "balanced"
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        if mode in ["speed", "quality", "balanced"]:
            performance_mode = mode
    
    logger.info(f"Starting Legal Document to RDF Conversion System in {performance_mode} mode")
    
    try:
        asyncio.run(main_optimized(performance_mode))
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)
