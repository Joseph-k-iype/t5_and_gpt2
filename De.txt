"""
Document Chunking Utilities - COMPLETE with Watermark Filtering
NO TRUNCATION - All methods fully implemented
Filters: INTERNAL, CONFIDENTIAL, RESTRICTED, DRAFT, PROPRIETARY, CLASSIFIED
Location: src/utils/document_chunker.py
"""
import re
from typing import List, Dict, Any
from dataclasses import dataclass


@dataclass
class DocumentChunk:
    """Represents a chunk of document text"""
    content: str
    chunk_index: int
    start_position: int
    end_position: int
    
    # For compatibility with dict access
    def get(self, key: str, default=None):
        """Dict-like get method for backward compatibility"""
        if key == "content" or key == "text":
            return self.content
        elif key == "chunk_id" or key == "index":
            return self.chunk_index
        elif key == "start":
            return self.start_position
        elif key == "end":
            return self.end_position
        return default
    
    @property
    def text(self):
        """Alias for content"""
        return self.content


class DocumentChunker:
    """
    Intelligent document chunker that:
    1. Filters watermarks (INTERNAL, CONFIDENTIAL, etc.)
    2. Respects semantic boundaries
    3. Returns consistent format
    4. Maintains complete coverage
    """
    
    def __init__(
        self,
        chunk_size: int = 3000,
        chunk_overlap: int = 200,
        respect_boundaries: bool = True
    ):
        """
        Initialize document chunker
        
        Args:
            chunk_size: Target size for each chunk (in characters)
            chunk_overlap: Number of characters to overlap between chunks
            respect_boundaries: Whether to respect paragraph/sentence boundaries
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_boundaries = respect_boundaries
        
        # Watermarks to filter out
        self.watermarks = [
            'INTERNAL',
            'CONFIDENTIAL',
            'RESTRICTED',
            'DRAFT',
            'PROPRIETARY',
            'CLASSIFIED',
            'PRIVATE',
            'SECRET',
            'SENSITIVE'
        ]
        
        # Compile regex patterns for efficiency
        self._compile_patterns()
    
    def _compile_patterns(self):
        """Compile regex patterns for watermark removal"""
        self.watermark_patterns = []
        
        for watermark in self.watermarks:
            # Pattern 1: Watermark on own line
            self.watermark_patterns.append(
                re.compile(rf'\n\s*{watermark}\s*\n', re.IGNORECASE | re.MULTILINE)
            )
            
            # Pattern 2: Watermark at end of line
            self.watermark_patterns.append(
                re.compile(rf'\n\s*{watermark}\s*$', re.IGNORECASE | re.MULTILINE)
            )
            
            # Pattern 3: Watermark at start of line
            self.watermark_patterns.append(
                re.compile(rf'^\s*{watermark}\s*\n', re.IGNORECASE | re.MULTILINE)
            )
            
            # Pattern 4: Watermark with page number at end
            self.watermark_patterns.append(
                re.compile(rf'\s{watermark}\s*\d*\s*$', re.IGNORECASE | re.MULTILINE)
            )
            
            # Pattern 5: Watermark with page number on its own line
            self.watermark_patterns.append(
                re.compile(rf'\n\s*{watermark}\s*\d+\s*\n', re.IGNORECASE | re.MULTILINE)
            )
        
        # Regex patterns for semantic boundaries
        self.section_pattern = re.compile(r'\n\s*(?:Section|Article|Chapter|ยง|ARTICLE|SECTION)\s+\d+', re.IGNORECASE)
        self.paragraph_pattern = re.compile(r'\n\s*\(\d+\)|^\d+\.\s+', re.MULTILINE)
        self.sentence_pattern = re.compile(r'(?<=[.!?])\s+(?=[A-Z])')
    
    def clean_text(self, text: str) -> str:
        """
        Clean text by removing watermarks and artifacts
        
        Args:
            text: Raw text from document
            
        Returns:
            Cleaned text without watermarks
        """
        if not text:
            return ""
        
        cleaned = text
        
        # Apply all watermark removal patterns
        for pattern in self.watermark_patterns:
            cleaned = pattern.sub('\n', cleaned)
        
        # Remove page number patterns (common in PDFs)
        cleaned = re.sub(r'\n\s*Page\s+\d+\s*\n', '\n', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\n\s*\d+\s*/\s*\d+\s*\n', '\n', cleaned)
        
        # Remove excessive whitespace
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        cleaned = re.sub(r' +', ' ', cleaned)
        
        # Remove trailing/leading whitespace
        cleaned = cleaned.strip()
        
        return cleaned
    
    def chunk_text(self, text: str) -> List[DocumentChunk]:
        """
        Chunk text into segments while respecting semantic boundaries
        NOTE: This is the main public method used by legal_document_analyzer.py
        
        Args:
            text: Text to chunk
            
        Returns:
            List of DocumentChunk objects
        """
        if not text or len(text.strip()) == 0:
            return []
        
        # CRITICAL: Clean text first to remove watermarks
        cleaned_text = self.clean_text(text)
        
        if not cleaned_text or len(cleaned_text.strip()) == 0:
            return []
        
        # If text is smaller than chunk_size, return as single chunk
        if len(cleaned_text) <= self.chunk_size:
            return [DocumentChunk(
                content=cleaned_text,
                chunk_index=0,
                start_position=0,
                end_position=len(cleaned_text)
            )]
        
        chunks = []
        current_position = 0
        chunk_index = 0
        
        while current_position < len(cleaned_text):
            # Determine end position for this chunk
            end_position = min(current_position + self.chunk_size, len(cleaned_text))
            
            # If respecting boundaries and not at end of text, find good break point
            if self.respect_boundaries and end_position < len(cleaned_text):
                end_position = self._find_break_point(cleaned_text, current_position, end_position)
            
            # Extract chunk text
            chunk_text = cleaned_text[current_position:end_position].strip()
            
            if chunk_text and len(chunk_text) >= 50:  # Minimum chunk size
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    chunk_index=chunk_index,
                    start_position=current_position,
                    end_position=end_position
                ))
                chunk_index += 1
            
            # Move to next position with overlap
            if current_position + self.chunk_size < len(cleaned_text):
                current_position = end_position - self.chunk_overlap
            else:
                current_position = end_position
            
            # Ensure we make progress
            if chunks and current_position <= chunks[-1].start_position:
                current_position = end_position
        
        return chunks
    
    def _find_break_point(self, text: str, start: int, end: int) -> int:
        """
        Find a good break point that respects semantic boundaries
        
        Priority:
        1. Section/Article boundaries
        2. Paragraph boundaries
        3. Sentence boundaries
        4. Whitespace
        
        Args:
            text: Full text
            start: Start position of current chunk
            end: Proposed end position
            
        Returns:
            Adjusted end position at semantic boundary
        """
        # Define search window (last 20% of chunk)
        search_window_size = int(self.chunk_size * 0.2)
        search_start = max(start, end - search_window_size)
        search_window = text[search_start:end]
        
        # Try to find section boundary (highest priority)
        section_matches = list(self.section_pattern.finditer(search_window))
        if section_matches:
            match = section_matches[-1]
            return search_start + match.start()
        
        # Try to find paragraph boundary
        paragraph_matches = list(self.paragraph_pattern.finditer(search_window))
        if paragraph_matches:
            match = paragraph_matches[-1]
            return search_start + match.start()
        
        # Try to find sentence boundary
        sentence_matches = list(self.sentence_pattern.finditer(search_window))
        if sentence_matches:
            match = sentence_matches[-1]
            return search_start + match.end()
        
        # Fall back to last whitespace
        last_space = search_window.rfind(' ')
        if last_space != -1 and last_space > len(search_window) // 2:
            return search_start + last_space
        
        last_newline = search_window.rfind('\n')
        if last_newline != -1 and last_newline > len(search_window) // 2:
            return search_start + last_newline
        
        # No good break point found, use original end
        return end
    
    def get_chunk_metadata(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """
        Get metadata about the chunking
        
        Args:
            chunks: List of document chunks
            
        Returns:
            Dictionary with chunking statistics
        """
        if not chunks:
            return {
                "total_chunks": 0,
                "total_length": 0,
                "avg_chunk_size": 0,
                "min_chunk_size": 0,
                "max_chunk_size": 0
            }
        
        chunk_sizes = [len(chunk.content) for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_length": sum(chunk_sizes),
            "avg_chunk_size": sum(chunk_sizes) / len(chunks) if chunks else 0,
            "min_chunk_size": min(chunk_sizes) if chunk_sizes else 0,
            "max_chunk_size": max(chunk_sizes) if chunk_sizes else 0,
            "chunk_overlap": self.chunk_overlap,
            "target_chunk_size": self.chunk_size,
            "watermarks_filtered": self.watermarks
        }
    
    def validate_chunks(self, chunks: List[DocumentChunk], original_text: str) -> Dict[str, Any]:
        """
        Validate that chunks cover the entire document without loss
        
        Args:
            chunks: List of document chunks
            original_text: Original text before chunking
            
        Returns:
            Validation report
        """
        if not chunks:
            return {
                "valid": False,
                "issues": ["No chunks generated"],
                "coverage": 0.0
            }
        
        issues = []
        
        # Check for gaps
        for i in range(len(chunks) - 1):
            current_end = chunks[i].end_position
            next_start = chunks[i + 1].start_position
            
            if next_start > current_end + self.chunk_overlap:
                issues.append(f"Gap detected between chunks {i} and {i+1}")
        
        # Check coverage
        cleaned_original = self.clean_text(original_text)
        total_content = "".join([c.content for c in chunks])
        
        coverage = len(total_content) / len(cleaned_original) if cleaned_original else 0
        
        if coverage < 0.95:  # Should have at least 95% coverage
            issues.append(f"Low coverage: {coverage:.1%}")
        
        # Check chunk sizes
        for i, chunk in enumerate(chunks):
            if len(chunk.content) < 50:
                issues.append(f"Chunk {i} is too small ({len(chunk.content)} chars)")
            if len(chunk.content) > self.chunk_size * 1.5:
                issues.append(f"Chunk {i} is too large ({len(chunk.content)} chars)")
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "coverage": coverage,
            "total_chunks": len(chunks),
            "average_chunk_size": sum(len(c.content) for c in chunks) / len(chunks)
        }
