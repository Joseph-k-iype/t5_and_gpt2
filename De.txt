#!/usr/bin/env python3
"""
Production IT Incident Data Quality Analysis with LangGraph Multi-Agent Architecture
Robust implementation with streaming output for large datasets.

Architecture:
- Supervisor Agent: Orchestrates the entire workflow
- Data Quality Analyzer Agent: Identifies DQ issues and dimensions  
- Evidence Assessment Agent: Evaluates supporting/contradicting evidence
- Confidence Scoring Agent: Calculates weighted confidence scores
- Validation Agent: Ensures output quality and consistency

Features:
- Streaming output to disk for large datasets
- No rate limits (handled by platform)
- Robust multi-agent processing
- Real-time progress tracking

Required packages:
pip install openai pandas pydantic langgraph langchain langchain-openai langchain-community

Usage:
1. Set your OpenAI API key in the API_KEY variable below
2. Set the correct path to your CSV file in INPUT_CSV_PATH  
3. Run: python production_incident_analysis.py
"""

import pandas as pd
import json
import time
import logging
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Literal, TypedDict, Annotated
from datetime import datetime
import csv

# Core imports
from openai import OpenAI
from pydantic import BaseModel, Field, ConfigDict, field_validator, model_validator
from pydantic.types import PositiveFloat
from pydantic import ValidationError

# LangGraph and LangChain imports  
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate

# LangGraph imports
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.types import Command
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Global Configuration
# ===================
MODEL = "gpt-4.1"
BASE_URL = "https://api.openai.com/v1"  
API_KEY = "your-openai-api-key-here"  # REQUIRED: Replace with your actual OpenAI API key

# File paths
INPUT_CSV_PATH = "incidents.csv"
OUTPUT_JSON_PATH = "production_incident_analysis_results.json"
OUTPUT_CSV_PATH = "production_incident_analysis_results.csv"
PROGRESS_LOG_PATH = "analysis_progress.log"

# Batch processing configuration
BATCH_SIZE = 100  # Number of incidents to buffer before writing to disk
PROGRESS_INTERVAL = 10  # Log progress every N incidents

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('production_incident_analysis.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Set environment variables to ensure all OpenAI clients use the correct configuration
os.environ["OPENAI_API_KEY"] = API_KEY
os.environ["OPENAI_API_BASE"] = BASE_URL
os.environ["OPENAI_BASE_URL"] = BASE_URL

# Initialize OpenAI client with proper configuration
openai_client = OpenAI(
    api_key=API_KEY,
    base_url=BASE_URL
)

# Initialize LangChain LLM with explicit configuration
llm = ChatOpenAI(
    model=MODEL,
    api_key=API_KEY,
    base_url=BASE_URL,
    temperature=0.1,
    openai_api_key=API_KEY,  # Explicit parameter
    openai_api_base=BASE_URL  # Explicit parameter
)

# Pydantic Models
# ===============

class IncidentData(BaseModel):
    """Incident data model with validation."""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        extra='allow'
    )
    
    INCIDENT_ID: str = Field(default="", description="Unique incident identifier")
    IT_INCIDENT_DESC: str = Field(default="", description="Incident description")
    IT_INCIDENT_AREA_CATEGORY_NAME: str = Field(default="", description="Area category")
    IT_INCIDENT_AREA_SUBCATEGORY_NAME: str = Field(default="", description="Area subcategory")
    IT_INCIDENT_CAUSE_TEXT: str = Field(default="", description="Incident cause")
    MAJOR_IT_INCIDENT_SUMMARY_TEXT: str = Field(default="", description="Major incident summary")
    MAJOR_IT_INCIDENT_FINDINGS_TEXT: str = Field(default="", description="Major incident findings")
    IT_INCIDENT_RESOLUTION_DESC: str = Field(default="", description="Resolution description")

class DQAnalysis(BaseModel):
    """Data Quality analysis result."""
    is_data_related: bool = Field(description="Whether incident is data-related")
    dq_dimensions: List[str] = Field(default_factory=list, description="Applicable DQ dimensions")
    primary_indicators: List[str] = Field(default_factory=list, description="Primary data quality indicators")
    reasoning: str = Field(description="Analysis reasoning")

class EvidenceAssessment(BaseModel):
    """Evidence assessment result."""
    supporting_evidence: List[str] = Field(default_factory=list, description="Supporting evidence")
    contradicting_evidence: List[str] = Field(default_factory=list, description="Contradicting evidence")
    evidence_strength: Dict[str, int] = Field(default_factory=dict, description="Evidence strength scores")
    assessment_reasoning: str = Field(description="Evidence assessment reasoning")

class ConfidenceScore(BaseModel):
    """Confidence scoring result."""
    confidence_score: float = Field(ge=0, le=100, description="Confidence score 0-100")
    calculation_method: str = Field(description="How confidence was calculated")
    contributing_factors: List[str] = Field(default_factory=list, description="Factors affecting confidence")

class ValidationResult(BaseModel):
    """Final validation result."""
    is_valid: bool = Field(description="Whether analysis is valid")
    validation_issues: List[str] = Field(default_factory=list, description="Any validation issues")
    final_recommendation: str = Field(description="Final recommendation")

class MultiAgentState(TypedDict):
    """State shared across all agents."""
    messages: List[BaseMessage]
    incident_data: Dict[str, Any]
    dq_analysis: Optional[DQAnalysis]
    evidence_assessment: Optional[EvidenceAssessment]
    confidence_score: Optional[ConfidenceScore]
    validation_result: Optional[ValidationResult]
    current_agent: str
    workflow_step: int
    errors: List[str]

class FinalAnalysisResult(BaseModel):
    """Final comprehensive analysis result."""
    model_config = ConfigDict(extra='allow')
    
    # Original incident data
    INCIDENT_ID: str = ""
    IT_INCIDENT_DESC: str = ""
    IT_INCIDENT_AREA_CATEGORY_NAME: str = ""
    IT_INCIDENT_AREA_SUBCATEGORY_NAME: str = ""
    IT_INCIDENT_CAUSE_TEXT: str = ""
    MAJOR_IT_INCIDENT_SUMMARY_TEXT: str = ""
    MAJOR_IT_INCIDENT_FINDINGS_TEXT: str = ""
    IT_INCIDENT_RESOLUTION_DESC: str = ""
    
    # Enhanced analysis results
    IS_DATA_ISSUE: str = Field(description="Y or N flag")
    DQ_DIMENSIONS: str = Field(default="", description="Comma-separated DQ dimensions")
    PRIMARY_INDICATORS: str = Field(default="", description="Primary DQ indicators")
    SUPPORTING_EVIDENCE: str = Field(default="", description="Supporting evidence")
    CONTRADICTING_EVIDENCE: str = Field(default="", description="Contradicting evidence")
    EVIDENCE_STRENGTH_SCORES: str = Field(default="", description="Evidence strength scores")
    CONFIDENCE_SCORE: float = Field(description="Confidence score")
    CONFIDENCE_CALCULATION: str = Field(default="", description="Confidence calculation method")
    VALIDATION_STATUS: str = Field(default="", description="Validation status")
    FINAL_REASONING: str = Field(description="Final comprehensive reasoning")
    PROCESSING_STATUS: str = Field(default="SUCCESS", description="Processing status")
    PROCESSING_TIMESTAMP: str = Field(default_factory=lambda: datetime.now().isoformat())
    WORKFLOW_STEPS: int = Field(default=0, description="Number of workflow steps")

# Agent Tools
# ===========

@tool
def analyze_dq_patterns(incident_text: str) -> str:
    """Analyze text for data quality patterns and indicators."""
    dq_patterns = {
        "Accuracy": ["incorrect", "wrong", "inaccurate", "error", "mismatch", "discrepancy"],
        "Completeness": ["missing", "incomplete", "null", "empty", "blank", "absent"],
        "Consistency": ["inconsistent", "conflicting", "contradiction", "sync", "duplicate"],
        "Validity": ["invalid", "format", "schema", "constraint", "rule violation"],
        "Uniqueness": ["duplicate", "repeated", "multiple", "redundant"],
        "Timeliness": ["outdated", "stale", "delayed", "late", "timestamp"],
        "Integrity": ["corrupted", "damaged", "broken", "altered", "modified"],
        "Availability": ["unavailable", "inaccessible", "down", "offline"],
        "Reasonableness": ["unreasonable", "outlier", "anomaly", "suspicious"]
    }
    
    found_patterns = {}
    text_lower = incident_text.lower()
    
    for dimension, keywords in dq_patterns.items():
        matches = [kw for kw in keywords if kw in text_lower]
        if matches:
            found_patterns[dimension] = matches
    
    return json.dumps(found_patterns)

@tool
def assess_evidence_strength(evidence_text: str, evidence_type: str) -> int:
    """Assess the strength of evidence (1-3 scale)."""
    strong_indicators = ["corruption", "missing records", "data loss", "invalid format", "constraint violation"]
    medium_indicators = ["sync issue", "mismatch", "incorrect value", "duplicate", "inconsistent"]
    weak_indicators = ["possible", "might", "could be", "potentially", "appears to"]
    
    text_lower = evidence_text.lower()
    
    if any(indicator in text_lower for indicator in strong_indicators):
        return 3
    elif any(indicator in text_lower for indicator in medium_indicators):
        return 2
    elif any(indicator in text_lower for indicator in weak_indicators):
        return 1
    else:
        return 1

@tool
def calculate_weighted_confidence(supporting_weights: List[int], contradicting_weights: List[int]) -> float:
    """Calculate weighted confidence score based on evidence."""
    if not supporting_weights and not contradicting_weights:
        return 50.0
    
    total_supporting = sum(supporting_weights) if supporting_weights else 0
    total_contradicting = sum(contradicting_weights) if contradicting_weights else 0
    total_evidence = total_supporting + total_contradicting
    
    if total_evidence == 0:
        return 50.0
    
    confidence = (total_supporting / total_evidence) * 100
    
    # Adjustments for edge cases
    if total_contradicting == 0 and total_supporting >= 6:
        confidence = min(95.0, confidence + 10)
    elif total_supporting == 0 and total_contradicting >= 6:
        confidence = max(5.0, confidence - 10)
    
    return round(confidence, 1)

# Specialized Agents
# ==================

class DataQualityAnalyzerAgent:
    """Specialized agent for data quality analysis."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [analyze_dq_patterns]
        
        # Ensure the agent uses the configured LLM with proper base_url and api_key
        logger.info(f"Creating DQ Analyzer agent with model: {MODEL}, base_url: {BASE_URL}")
        self.agent = create_react_agent(
            model=self.llm,  # Use the pre-configured LLM
            tools=self.tools,
            prompt="""You are a Data Quality Analysis Expert. Your job is to:

1. Analyze incident descriptions for data quality issues
2. Identify relevant DQ dimensions (Accuracy, Completeness, Consistency, Validity, Uniqueness, Timeliness, Integrity, Availability, Reasonableness)
3. Find primary indicators that suggest data quality problems
4. Provide clear reasoning for your analysis

Use the analyze_dq_patterns tool to systematically examine the incident text.

Always respond with a JSON object containing:
{
    "is_data_related": boolean,
    "dq_dimensions": [list of applicable dimensions],
    "primary_indicators": [list of key indicators found],
    "reasoning": "detailed explanation"
}"""
        )
    
    def analyze(self, state: MultiAgentState) -> MultiAgentState:
        """Perform data quality analysis."""
        incident_data = state["incident_data"]
        
        # Combine all incident text for analysis
        combined_text = " ".join([
            incident_data.get("IT_INCIDENT_DESC", ""),
            incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
            incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
            incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
            incident_data.get("IT_INCIDENT_RESOLUTION_DESC", "")
        ]).strip()
        
        if not combined_text:
            state["dq_analysis"] = DQAnalysis(
                is_data_related=False,
                dq_dimensions=[],
                primary_indicators=[],
                reasoning="No incident text available for analysis"
            )
        else:
            # Use ReAct agent for analysis
            messages = [HumanMessage(content=f"Analyze this IT incident for data quality issues:\n\n{combined_text}")]
            response = self.agent.invoke({"messages": messages})
            
            # Extract JSON from response
            response_text = response["messages"][-1].content
            
            # Parse the analysis result
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    analysis_data = json.loads(json_match.group())
                    state["dq_analysis"] = DQAnalysis(**analysis_data)
                except (json.JSONDecodeError, ValidationError) as e:
                    logger.warning(f"Failed to parse DQ analysis: {e}")
                    state["dq_analysis"] = DQAnalysis(
                        is_data_related=False,
                        dq_dimensions=[],
                        primary_indicators=[],
                        reasoning=f"Analysis parsing failed: {str(e)}"
                    )
            else:
                state["dq_analysis"] = DQAnalysis(
                    is_data_related=False,
                    dq_dimensions=[],
                    primary_indicators=[],
                    reasoning="No valid JSON response from analysis"
                )
        
        state["current_agent"] = "evidence_assessor"
        state["workflow_step"] = state.get("workflow_step", 0) + 1
        return state

class EvidenceAssessmentAgent:
    """Specialized agent for evidence assessment."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [assess_evidence_strength]
        
        # Ensure the agent uses the configured LLM with proper base_url and api_key
        logger.info(f"Creating Evidence Assessment agent with model: {MODEL}, base_url: {BASE_URL}")
        self.agent = create_react_agent(
            model=self.llm,  # Use the pre-configured LLM
            tools=self.tools,
            prompt="""You are an Evidence Assessment Expert. Your job is to:

1. Identify supporting evidence for data quality issues
2. Identify contradicting evidence against data quality issues  
3. Assess the strength of each piece of evidence (1-3 scale)
4. Provide reasoning for your assessment

Use the assess_evidence_strength tool to evaluate evidence strength.

Always respond with a JSON object containing:
{
    "supporting_evidence": [list of supporting evidence],
    "contradicting_evidence": [list of contradicting evidence],
    "evidence_strength": {"evidence_item": strength_score},
    "assessment_reasoning": "detailed explanation"
}"""
        )
    
    def assess(self, state: MultiAgentState) -> MultiAgentState:
        """Perform evidence assessment."""
        incident_data = state["incident_data"]
        dq_analysis = state.get("dq_analysis")
        
        if not dq_analysis:
            state["evidence_assessment"] = EvidenceAssessment(
                supporting_evidence=[],
                contradicting_evidence=[],
                evidence_strength={},
                assessment_reasoning="No DQ analysis available"
            )
        else:
            combined_text = " ".join([
                incident_data.get("IT_INCIDENT_DESC", ""),
                incident_data.get("IT_INCIDENT_CAUSE_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", ""),
                incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", ""),
                incident_data.get("IT_INCIDENT_RESOLUTION_DESC", "")
            ]).strip()
            
            # Create assessment prompt
            assessment_prompt = f"""
            Based on the DQ analysis that found: {dq_analysis.reasoning}
            
            Analyze this incident text for supporting and contradicting evidence:
            {combined_text}
            
            Look for:
            - Supporting evidence: indicators that confirm this is a data quality issue
            - Contradicting evidence: indicators that suggest this is NOT a data quality issue (hardware, network, user error, etc.)
            """
            
            messages = [HumanMessage(content=assessment_prompt)]
            response = self.agent.invoke({"messages": messages})
            
            response_text = response["messages"][-1].content
            
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    assessment_data = json.loads(json_match.group())
                    state["evidence_assessment"] = EvidenceAssessment(**assessment_data)
                except (json.JSONDecodeError, ValidationError) as e:
                    logger.warning(f"Failed to parse evidence assessment: {e}")
                    state["evidence_assessment"] = EvidenceAssessment(
                        supporting_evidence=[],
                        contradicting_evidence=[],
                        evidence_strength={},
                        assessment_reasoning=f"Assessment parsing failed: {str(e)}"
                    )
            else:
                state["evidence_assessment"] = EvidenceAssessment(
                    supporting_evidence=[],
                    contradicting_evidence=[],
                    evidence_strength={},
                    assessment_reasoning="No valid JSON response from assessment"
                )
        
        state["current_agent"] = "confidence_scorer"
        state["workflow_step"] = state.get("workflow_step", 0) + 1
        return state

class ConfidenceScoringAgent:
    """Specialized agent for confidence scoring."""
    
    def __init__(self, llm):
        self.llm = llm
        self.tools = [calculate_weighted_confidence]
        
        # Ensure the agent uses the configured LLM with proper base_url and api_key
        logger.info(f"Creating Confidence Scoring agent with model: {MODEL}, base_url: {BASE_URL}")
        self.agent = create_react_agent(
            model=self.llm,  # Use the pre-configured LLM
            tools=self.tools,
            prompt="""You are a Confidence Scoring Expert. Your job is to:

1. Calculate confidence scores based on evidence strength
2. Use weighted scoring methodology
3. Consider contributing factors
4. Explain calculation method

Use the calculate_weighted_confidence tool with evidence strength weights.

Always respond with a JSON object containing:
{
    "confidence_score": float (0-100),
    "calculation_method": "explanation of how score was calculated",
    "contributing_factors": [list of factors that influenced the score]
}"""
        )
    
    def score(self, state: MultiAgentState) -> MultiAgentState:
        """Calculate confidence score."""
        evidence_assessment = state.get("evidence_assessment")
        
        if not evidence_assessment:
            state["confidence_score"] = ConfidenceScore(
                confidence_score=50.0,
                calculation_method="Default score due to missing evidence assessment",
                contributing_factors=["No evidence assessment available"]
            )
        else:
            # Extract evidence strengths
            supporting_weights = []
            contradicting_weights = []
            
            for evidence in evidence_assessment.supporting_evidence:
                strength = evidence_assessment.evidence_strength.get(evidence, 1)
                supporting_weights.append(strength)
            
            for evidence in evidence_assessment.contradicting_evidence:
                strength = evidence_assessment.evidence_strength.get(evidence, 1)
                contradicting_weights.append(strength)
            
            # Calculate score using tool
            score = calculate_weighted_confidence.func(supporting_weights, contradicting_weights)
            
            state["confidence_score"] = ConfidenceScore(
                confidence_score=score,
                calculation_method="Weighted average of evidence strengths",
                contributing_factors=[f"Supporting: {len(supporting_weights)}", f"Contradicting: {len(contradicting_weights)}"]
            )
        
        state["current_agent"] = "validator"
        state["workflow_step"] = state.get("workflow_step", 0) + 1
        return state

class ValidationAgent:
    """Specialized agent for final validation."""
    
    def __init__(self, llm):
        self.llm = llm
        
        # Ensure the agent uses the configured LLM with proper base_url and api_key
        logger.info(f"Creating Validation agent with model: {MODEL}, base_url: {BASE_URL}")
        self.agent = create_react_agent(
            model=self.llm,  # Use the pre-configured LLM
            tools=[],
            prompt="""You are a Validation Expert. Your job is to:

1. Review the complete analysis for consistency
2. Identify any validation issues
3. Provide final recommendations
4. Ensure logical coherence

Always respond with a JSON object containing:
{
    "is_valid": boolean,
    "validation_issues": [list of any issues found],
    "final_recommendation": "comprehensive final recommendation"
}"""
        )
    
    def validate(self, state: MultiAgentState) -> MultiAgentState:
        """Perform final validation."""
        dq_analysis = state.get("dq_analysis")
        evidence_assessment = state.get("evidence_assessment")
        confidence_score = state.get("confidence_score")
        
        validation_issues = []
        
        # Check for consistency
        if dq_analysis and evidence_assessment:
            if dq_analysis.is_data_related and not evidence_assessment.supporting_evidence:
                validation_issues.append("Data issue identified but no supporting evidence found")
            
            if not dq_analysis.is_data_related and evidence_assessment.supporting_evidence:
                validation_issues.append("No data issue identified but supporting evidence exists")
        
        # Check confidence score reasonableness
        if confidence_score:
            if confidence_score.confidence_score > 90 and evidence_assessment and evidence_assessment.contradicting_evidence:
                validation_issues.append("High confidence score despite contradicting evidence")
            
            if confidence_score.confidence_score < 20 and evidence_assessment and evidence_assessment.supporting_evidence:
                validation_issues.append("Low confidence score despite supporting evidence")
        
        # Generate final recommendation
        final_recommendation = self._generate_recommendation(dq_analysis, evidence_assessment, confidence_score, validation_issues)
        
        state["validation_result"] = ValidationResult(
            is_valid=len(validation_issues) == 0,
            validation_issues=validation_issues,
            final_recommendation=final_recommendation
        )
        
        state["current_agent"] = "completed"
        state["workflow_step"] = state.get("workflow_step", 0) + 1
        return state
    
    def _generate_recommendation(self, dq_analysis, evidence_assessment, confidence_score, validation_issues):
        """Generate final recommendation based on all analysis."""
        if not dq_analysis:
            return "Unable to provide recommendation due to missing DQ analysis"
        
        if dq_analysis.is_data_related:
            confidence_level = "high" if confidence_score and confidence_score.confidence_score >= 80 else \
                             "medium" if confidence_score and confidence_score.confidence_score >= 60 else "low"
            
            recommendation = f"This incident appears to be data-related with {confidence_level} confidence"
            
            if dq_analysis.dq_dimensions:
                recommendation += f" affecting {', '.join(dq_analysis.dq_dimensions)} dimensions"
            
            if validation_issues:
                recommendation += f". However, validation identified concerns: {'; '.join(validation_issues)}"
        else:
            recommendation = "This incident does not appear to be primarily data-related"
            
            if evidence_assessment and evidence_assessment.contradicting_evidence:
                recommendation += f". Non-data factors include: {', '.join(evidence_assessment.contradicting_evidence[:3])}"
        
        return recommendation

# Supervisor Agent
# ================

class SupervisorAgent:
    """Main supervisor agent that orchestrates the multi-agent workflow."""
    
    def __init__(self, llm):
        self.llm = llm
        
        # Log configuration details
        logger.info(f"Initializing Supervisor Agent with:")
        logger.info(f"  Model: {MODEL}")
        logger.info(f"  Base URL: {BASE_URL}")
        logger.info(f"  API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else 'CONFIGURED'}")
        
        # Initialize all specialized agents with the configured LLM
        logger.info("Creating specialized agents...")
        self.dq_analyzer = DataQualityAnalyzerAgent(self.llm)
        self.evidence_assessor = EvidenceAssessmentAgent(self.llm)
        self.confidence_scorer = ConfidenceScoringAgent(self.llm)
        self.validator = ValidationAgent(self.llm)
        
        # Build the workflow graph
        logger.info("Building multi-agent workflow graph...")
        self.workflow = self._build_workflow()
        logger.info("Multi-agent supervisor initialized successfully")
    
    def _build_workflow(self) -> StateGraph:
        """Build the multi-agent workflow graph."""
        
        def supervisor_node(state: MultiAgentState) -> Command[Literal["dq_analyzer", "evidence_assessor", "confidence_scorer", "validator", "__end__"]]:
            """Supervisor decision logic."""
            current_agent = state.get("current_agent", "start")
            
            if current_agent == "start":
                return Command(goto="dq_analyzer")
            elif current_agent == "evidence_assessor":
                return Command(goto="evidence_assessor")
            elif current_agent == "confidence_scorer":
                return Command(goto="confidence_scorer")
            elif current_agent == "validator":
                return Command(goto="validator")
            elif current_agent == "completed":
                return Command(goto="__end__")
            else:
                return Command(goto="__end__")
        
        def dq_analyzer_node(state: MultiAgentState) -> MultiAgentState:
            """DQ Analyzer node."""
            return self.dq_analyzer.analyze(state)
        
        def evidence_assessor_node(state: MultiAgentState) -> MultiAgentState:
            """Evidence Assessor node."""
            return self.evidence_assessor.assess(state)
        
        def confidence_scorer_node(state: MultiAgentState) -> MultiAgentState:
            """Confidence Scorer node."""
            return self.confidence_scorer.score(state)
        
        def validator_node(state: MultiAgentState) -> MultiAgentState:
            """Validator node."""
            return self.validator.validate(state)
        
        # Create the workflow graph
        workflow = StateGraph(MultiAgentState)
        
        # Add nodes
        workflow.add_node("supervisor", supervisor_node)
        workflow.add_node("dq_analyzer", dq_analyzer_node)
        workflow.add_node("evidence_assessor", evidence_assessor_node)
        workflow.add_node("confidence_scorer", confidence_scorer_node)
        workflow.add_node("validator", validator_node)
        
        # Add edges
        workflow.add_edge(START, "supervisor")
        workflow.add_conditional_edges("supervisor", lambda x: x["current_agent"])
        workflow.add_edge("dq_analyzer", "supervisor")
        workflow.add_edge("evidence_assessor", "supervisor")
        workflow.add_edge("confidence_scorer", "supervisor")
        workflow.add_edge("validator", "supervisor")
        
        return workflow
    
    def analyze_incident(self, incident_data: Dict[str, Any]) -> FinalAnalysisResult:
        """Analyze a single incident using the multi-agent workflow."""
        # Initialize state
        initial_state = MultiAgentState(
            messages=[],
            incident_data=incident_data,
            dq_analysis=None,
            evidence_assessment=None,
            confidence_score=None,
            validation_result=None,
            current_agent="start",
            workflow_step=0,
            errors=[]
        )
        
        # Add memory for stateful processing
        memory = MemorySaver()
        app = self.workflow.compile(checkpointer=memory)
        
        # Run the workflow
        config = {"configurable": {"thread_id": f"incident_{incident_data.get('INCIDENT_ID', 'unknown')}"}}
        final_state = app.invoke(initial_state, config)
        
        # Convert to final result
        return self._create_final_result(incident_data, final_state)
    
    def _create_final_result(self, incident_data: Dict[str, Any], state: MultiAgentState) -> FinalAnalysisResult:
        """Create final analysis result from workflow state."""
        
        dq_analysis = state.get("dq_analysis")
        evidence_assessment = state.get("evidence_assessment")
        confidence_score = state.get("confidence_score")
        validation_result = state.get("validation_result")
        
        # Determine if it's a data issue
        is_data_issue = "Y" if dq_analysis and dq_analysis.is_data_related else "N"
        
        # Create comprehensive final result
        result = FinalAnalysisResult(
            INCIDENT_ID=str(incident_data.get("INCIDENT_ID", "")),
            IT_INCIDENT_DESC=str(incident_data.get("IT_INCIDENT_DESC", "")),
            IT_INCIDENT_AREA_CATEGORY_NAME=str(incident_data.get("IT_INCIDENT_AREA_CATEGORY_NAME", "")),
            IT_INCIDENT_AREA_SUBCATEGORY_NAME=str(incident_data.get("IT_INCIDENT_AREA_SUBCATEGORY_NAME", "")),
            IT_INCIDENT_CAUSE_TEXT=str(incident_data.get("IT_INCIDENT_CAUSE_TEXT", "")),
            MAJOR_IT_INCIDENT_SUMMARY_TEXT=str(incident_data.get("MAJOR_IT_INCIDENT_SUMMARY_TEXT", "")),
            MAJOR_IT_INCIDENT_FINDINGS_TEXT=str(incident_data.get("MAJOR_IT_INCIDENT_FINDINGS_TEXT", "")),
            IT_INCIDENT_RESOLUTION_DESC=str(incident_data.get("IT_INCIDENT_RESOLUTION_DESC", "")),
            
            IS_DATA_ISSUE=is_data_issue,
            DQ_DIMENSIONS=", ".join(dq_analysis.dq_dimensions) if dq_analysis else "",
            PRIMARY_INDICATORS=", ".join(dq_analysis.primary_indicators) if dq_analysis else "",
            SUPPORTING_EVIDENCE=" | ".join(evidence_assessment.supporting_evidence) if evidence_assessment else "",
            CONTRADICTING_EVIDENCE=" | ".join(evidence_assessment.contradicting_evidence) if evidence_assessment else "",
            EVIDENCE_STRENGTH_SCORES=json.dumps(evidence_assessment.evidence_strength) if evidence_assessment else "",
            CONFIDENCE_SCORE=confidence_score.confidence_score if confidence_score else 0.0,
            CONFIDENCE_CALCULATION=confidence_score.calculation_method if confidence_score else "",
            VALIDATION_STATUS="VALID" if validation_result and validation_result.is_valid else "ISSUES_FOUND",
            FINAL_REASONING=validation_result.final_recommendation if validation_result else "Analysis incomplete",
            PROCESSING_STATUS="SUCCESS",
            WORKFLOW_STEPS=state.get("workflow_step", 0)
        )
        
        # Add extra fields from original data
        for key, value in incident_data.items():
            if not hasattr(result, key) and key not in result.model_fields:
                setattr(result, key, value)
        
        return result

# Streaming Output Manager
# ========================

class StreamingOutputManager:
    """Manages streaming output to disk for large datasets."""
    
    def __init__(self, json_path: str, csv_path: str, progress_log_path: str):
        self.json_path = json_path
        self.csv_path = csv_path
        self.progress_log_path = progress_log_path
        self.csv_writer = None
        self.csv_file = None
        self.json_results = []
        self.headers_written = False
        self.processed_count = 0
        
        # Initialize files
        self._initialize_files()
        
    def _initialize_files(self):
        """Initialize output files."""
        # Initialize CSV file
        self.csv_file = open(self.csv_path, 'w', newline='', encoding='utf-8')
        
        # Initialize progress log
        with open(self.progress_log_path, 'w') as f:
            f.write(f"Analysis started at {datetime.now().isoformat()}\n")
    
    def write_result(self, result: FinalAnalysisResult):
        """Write a single result to files."""
        # Convert to dictionary
        result_dict = result.model_dump()
        
        # Add to JSON buffer
        self.json_results.append(result_dict)
        
        # Write to CSV
        if not self.headers_written:
            self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=result_dict.keys())
            self.csv_writer.writeheader()
            self.headers_written = True
        
        self.csv_writer.writerow(result_dict)
        self.csv_file.flush()  # Ensure data is written to disk
        
        self.processed_count += 1
        
        # Log progress
        if self.processed_count % PROGRESS_INTERVAL == 0:
            self._log_progress()
        
        # Periodically flush JSON buffer to prevent memory issues
        if len(self.json_results) >= BATCH_SIZE:
            self._flush_json_buffer()
    
    def _log_progress(self):
        """Log current progress."""
        timestamp = datetime.now().isoformat()
        progress_msg = f"Processed {self.processed_count} incidents at {timestamp}"
        
        # Log to file
        with open(self.progress_log_path, 'a') as f:
            f.write(f"{progress_msg}\n")
        
        # Log to console
        logger.info(progress_msg)
    
    def _flush_json_buffer(self):
        """Flush JSON buffer to disk."""
        # Write current buffer to JSON file
        with open(self.json_path, 'w', encoding='utf-8') as f:
            json.dump(self.json_results, f, indent=2, ensure_ascii=False)
    
    def finalize(self) -> Dict[str, Any]:
        """Finalize output and return statistics."""
        # Final JSON write
        self._flush_json_buffer()
        
        # Close CSV file
        if self.csv_file:
            self.csv_file.close()
        
        # Final progress log
        completion_time = datetime.now().isoformat()
        with open(self.progress_log_path, 'a') as f:
            f.write(f"Analysis completed at {completion_time}\n")
            f.write(f"Total incidents processed: {self.processed_count}\n")
        
        # Calculate statistics
        data_issues = sum(1 for r in self.json_results if r.get("IS_DATA_ISSUE") == "Y")
        success_count = sum(1 for r in self.json_results if r.get("PROCESSING_STATUS") == "SUCCESS")
        
        stats = {
            "total_processed": self.processed_count,
            "successful_analyses": success_count,
            "data_issues_found": data_issues,
            "success_rate": (success_count / self.processed_count * 100) if self.processed_count > 0 else 0,
            "data_issue_rate": (data_issues / success_count * 100) if success_count > 0 else 0
        }
        
        return stats

# Production Analyzer
# ===================

class ProductionIncidentAnalyzer:
    """Production-ready incident analyzer with streaming output."""
    
    def __init__(self):
        logger.info("Initializing Production Incident Analyzer...")
        logger.info(f"Configuration:")
        logger.info(f"  Model: {MODEL}")
        logger.info(f"  Base URL: {BASE_URL}")
        logger.info(f"  API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else 'CONFIGURED'}")
        
        self.supervisor = SupervisorAgent(llm)
        logger.info("Production analyzer initialized with multi-agent architecture")
    
    def process_csv_streaming(self, input_path: str) -> Dict[str, Any]:
        """Process CSV file with streaming output for large datasets."""
        logger.info(f"Starting streaming analysis of {input_path}")
        
        # Read CSV
        df = pd.read_csv(input_path, encoding='utf-8')
        if df.empty:
            raise ValueError("CSV file is empty")
        
        # Fill NaN values
        df = df.fillna("")
        
        logger.info(f"Loaded {len(df)} incidents for processing")
        
        # Initialize streaming output manager
        output_manager = StreamingOutputManager(
            OUTPUT_JSON_PATH, 
            OUTPUT_CSV_PATH, 
            PROGRESS_LOG_PATH
        )
        
        # Process incidents with streaming output
        for index, row in df.iterrows():
            incident_id = row.get('INCIDENT_ID', f'ROW_{index}')
            
            try:
                # Convert row to dictionary
                incident_data = row.to_dict()
                
                # Analyze with multi-agent system
                result = self.supervisor.analyze_incident(incident_data)
                
                # Stream result to disk immediately
                output_manager.write_result(result)
                
                # Log progress for large datasets
                if (index + 1) % PROGRESS_INTERVAL == 0:
                    logger.info(f"Processed {index + 1}/{len(df)} incidents ({((index + 1)/len(df)*100):.1f}%)")
                    
            except Exception as e:
                logger.error(f"Error processing incident {incident_id}: {e}")
                
                # Create error result and write to disk
                error_result = FinalAnalysisResult(
                    INCIDENT_ID=str(incident_data.get("INCIDENT_ID", "")),
                    IT_INCIDENT_DESC=str(incident_data.get("IT_INCIDENT_DESC", "")),
                    IS_DATA_ISSUE="N",
                    DQ_DIMENSIONS="",
                    PRIMARY_INDICATORS="",
                    SUPPORTING_EVIDENCE="",
                    CONTRADICTING_EVIDENCE="",
                    EVIDENCE_STRENGTH_SCORES="",
                    CONFIDENCE_SCORE=0.0,
                    CONFIDENCE_CALCULATION="",
                    VALIDATION_STATUS="ERROR",
                    FINAL_REASONING=f"Processing error: {str(e)}",
                    PROCESSING_STATUS="ERROR",
                    WORKFLOW_STEPS=0
                )
                
                output_manager.write_result(error_result)
        
        # Finalize output and get statistics
        stats = output_manager.finalize()
        
        logger.info("Streaming analysis completed successfully")
        return stats

# Main Functions
# ==============

def validate_configuration():
    """Validate configuration."""
    logger.info("Validating configuration...")
    
    errors = []
    warnings = []
    
    # Check API key
    if API_KEY == "your-openai-api-key-here" or not API_KEY:
        errors.append("OpenAI API key not configured")
    elif len(API_KEY) < 10:
        warnings.append("API key seems unusually short")
    
    # Check base URL
    if not BASE_URL:
        errors.append("Base URL not configured")
    elif not BASE_URL.startswith(('http://', 'https://')):
        errors.append(f"Invalid base URL format: {BASE_URL}")
    
    # Check model
    if not MODEL:
        errors.append("Model not configured")
    
    # Check input file
    if not os.path.exists(INPUT_CSV_PATH):
        errors.append(f"Input CSV file not found: {INPUT_CSV_PATH}")
    
    # Log configuration details
    logger.info(f"Configuration details:")
    logger.info(f"  Model: {MODEL}")
    logger.info(f"  Base URL: {BASE_URL}")
    logger.info(f"  API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else 'NOT_SET'}")
    logger.info(f"  Input CSV: {INPUT_CSV_PATH}")
    
    # Report warnings
    if warnings:
        for warning in warnings:
            logger.warning(f"‚ö†Ô∏è  {warning}")
    
    # Report errors
    if errors:
        for error in errors:
            logger.error(f"‚ùå {error}")
        return False
    
    logger.info("‚úÖ Configuration validation passed")
    return True

def test_connection():
    """Test LLM connection with explicit configuration."""
    try:
        logger.info("Testing LLM connection with configuration:")
        logger.info(f"  Model: {MODEL}")
        logger.info(f"  Base URL: {BASE_URL}")
        logger.info(f"  API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else 'CONFIGURED'}")
        
        # Test direct OpenAI client first
        logger.info("Testing direct OpenAI client...")
        response = openai_client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": "Test connection. Respond with 'OK'."}],
            max_tokens=5
        )
        
        if response.choices[0].message.content.strip():
            logger.info("‚úÖ Direct OpenAI client test successful")
        else:
            logger.error("‚ùå Direct OpenAI client test failed: Empty response")
            return False
        
        # Test LangChain LLM
        logger.info("Testing LangChain LLM...")
        response = llm.invoke([HumanMessage(content="Test connection. Respond with 'OK'.")])
        
        if response.content.strip():
            logger.info("‚úÖ LangChain LLM test successful")
            logger.info("‚úÖ All connection tests passed")
            return True
        else:
            logger.error("‚ùå LangChain LLM test failed: Empty response")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Connection test failed: {e}")
        logger.error(f"Error type: {type(e).__name__}")
        return False

def main():
    """Main execution function for production analysis."""
    start_time = time.time()
    
    try:
        logger.info("="*80)
        logger.info("PRODUCTION IT INCIDENT ANALYSIS WITH STREAMING OUTPUT")
        logger.info("="*80)
        logger.info("üîß ENDPOINT CONFIGURATION:")
        logger.info(f"   Model: {MODEL}")
        logger.info(f"   Base URL: {BASE_URL}")
        logger.info(f"   API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:] if len(API_KEY) > 4 else 'NOT_SET'}")
        logger.info("üìÅ FILE CONFIGURATION:")
        logger.info(f"   Input: {INPUT_CSV_PATH}")
        logger.info(f"   Output JSON: {OUTPUT_JSON_PATH}")
        logger.info(f"   Output CSV: {OUTPUT_CSV_PATH}")
        logger.info(f"   Progress log: {PROGRESS_LOG_PATH}")
        logger.info("="*80)
        
        # Validation
        if not validate_configuration():
            sys.exit(1)
        
        if not test_connection():
            logger.error("LLM connection failed")
            sys.exit(1)
        
        # Initialize production analyzer
        analyzer = ProductionIncidentAnalyzer()
        
        # Process with streaming output
        logger.info("Starting production incident analysis with streaming output...")
        stats = analyzer.process_csv_streaming(INPUT_CSV_PATH)
        
        # Final summary
        end_time = time.time()
        execution_time = end_time - start_time
        
        logger.info("="*80)
        logger.info("PRODUCTION ANALYSIS COMPLETED! üéâ")
        logger.info("="*80)
        logger.info(f"‚è±Ô∏è  Total execution time: {execution_time:.1f} seconds")
        logger.info(f"üìä Total incidents processed: {stats['total_processed']}")
        logger.info(f"‚úÖ Successful analyses: {stats['successful_analyses']}")
        logger.info(f"üìà Success rate: {stats['success_rate']:.1f}%")
        logger.info(f"üîç Data issues found: {stats['data_issues_found']}")
        logger.info(f"üìä Data issue rate: {stats['data_issue_rate']:.1f}%")
        logger.info(f"üìÅ Results saved to:")
        logger.info(f"   üìÑ JSON: {OUTPUT_JSON_PATH}")
        logger.info(f"   üìä CSV: {OUTPUT_CSV_PATH}")
        logger.info(f"   üìã Progress log: {PROGRESS_LOG_PATH}")
        
    except KeyboardInterrupt:
        logger.info("Analysis interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Production analysis failed: {e}")
        logger.error("Stack trace:", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
