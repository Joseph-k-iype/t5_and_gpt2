"""
Advanced Schema Mapping System with Graph RAG and Sophisticated Reasoning
Uses transformations, relationships, lineage, and advanced prompting techniques
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any, TypedDict, Annotated
from operator import add

from falkordb import FalkorDB
from openai import OpenAI

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
FALKORDB_HOST = os.getenv("FALKORDB_HOST", "localhost")
FALKORDB_PORT = int(os.getenv("FALKORDB_PORT", 6379))

# Configure OpenAI client - FIXED: Use client instance
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# Configure LangChain LLM
llm = ChatOpenAI(
    model="gpt-4o",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

reasoning_llm = ChatOpenAI(
    model="o1",
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# ============================================================================
# EMBEDDING UTILITIES
# ============================================================================

def create_embedding(text: str) -> List[float]:
    """Create embedding using OpenAI text-embedding-3-large model"""
    response = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    )
    return response.data[0].embedding


def batch_create_embeddings(texts: List[str]) -> List[List[float]]:
    """Create embeddings for multiple texts in batch"""
    response = openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=texts
    )
    return [item.embedding for item in response.data]


# ============================================================================
# FALKORDB SCHEMA LOADER
# ============================================================================

class SchemaLoader:
    """Load schema into FalkorDB as property graph with embeddings"""
    
    def __init__(self, host: str = FALKORDB_HOST, port: int = FALKORDB_PORT):
        self.db = FalkorDB(host=host, port=port)
        self.graph = self.db.select_graph('schema_graph')
        
    def load_schema(self, attributes_csv: str, relationships_csv: str):
        """Load schema from CSV files into FalkorDB"""
        print("Loading schema attributes...")
        df_attrs = pd.read_csv(attributes_csv)
        
        print("Loading schema relationships...")
        df_rels = pd.read_csv(relationships_csv)
        
        # Create vector index for tables and columns
        print("Creating vector indices...")
        self._create_vector_indices()
        
        # Load tables and columns
        print("Loading tables and columns...")
        self._load_tables_and_columns(df_attrs)
        
        # Load relationships
        print("Loading relationships...")
        self._load_relationships(df_rels)
        
        print("Schema loaded successfully!")
        
    def _create_vector_indices(self):
        """Create vector indices for semantic search"""
        try:
            self.graph.query("""
                CREATE VECTOR INDEX FOR (t:Table) ON (t.name_embedding) 
                OPTIONS {dimension:3072, similarityFunction:'cosine'}
            """)
            print("Created vector index for Table nodes")
        except Exception as e:
            print(f"Table vector index might already exist: {e}")
        
        try:
            self.graph.query("""
                CREATE VECTOR INDEX FOR (c:Column) ON (c.name_embedding) 
                OPTIONS {dimension:3072, similarityFunction:'cosine'}
            """)
            print("Created vector index for Column nodes")
        except Exception as e:
            print(f"Column vector index might already exist: {e}")
    
    def _load_tables_and_columns(self, df_attrs: pd.DataFrame):
        """Load tables and their columns with embeddings"""
        tables = df_attrs.groupby(['table_id', 'table_name', 'data_model'])
        
        for (table_id, table_name, data_model), columns in tables:
            print(f"Processing table: {table_name}")
            table_embedding = create_embedding(table_name)
            
            # Escape single quotes in strings for Cypher
            table_name_escaped = table_name.replace("'", "\\'")
            data_model_escaped = data_model.replace("'", "\\'")
            
            self.graph.query(f"""
                MERGE (t:Table {{
                    table_id: '{table_id}',
                    table_name: '{table_name_escaped}',
                    data_model: '{data_model_escaped}',
                    name_embedding: vecf32({table_embedding})
                }})
            """)
            
            for _, col in columns.iterrows():
                col_embedding = create_embedding(col['column_name'])
                
                # Escape single quotes for all string values
                col_props = {
                    'column_id': str(col['column_id']).replace("'", "\\'"),
                    'column_name': str(col['column_name']).replace("'", "\\'"),
                    'type': str(col['type']).replace("'", "\\'") if pd.notna(col['type']) else '',
                    'length': str(col['length']).replace("'", "\\'") if pd.notna(col['length']) else '',
                    'scale': str(col['scale']).replace("'", "\\'") if pd.notna(col['scale']) else '',
                    'nullable': str(col['nullable']).replace("'", "\\'") if pd.notna(col['nullable']) else '',
                    'primary_key': str(col['primary_key']).replace("'", "\\'") if pd.notna(col['primary_key']) else '',
                    'unique': str(col['unique']).replace("'", "\\'") if pd.notna(col['unique']) else '',
                    'foreign_key': str(col['foreign_key']).replace("'", "\\'") if pd.notna(col['foreign_key']) else ''
                }
                
                self.graph.query(f"""
                    MATCH (t:Table {{table_id: '{table_id}'}})
                    CREATE (c:Column {{
                        column_id: '{col_props['column_id']}',
                        column_name: '{col_props['column_name']}',
                        type: '{col_props['type']}',
                        length: '{col_props['length']}',
                        scale: '{col_props['scale']}',
                        nullable: '{col_props['nullable']}',
                        primary_key: '{col_props['primary_key']}',
                        unique: '{col_props['unique']}',
                        foreign_key: '{col_props['foreign_key']}',
                        name_embedding: vecf32({col_embedding})
                    }})
                    CREATE (t)-[:HAS_COLUMN]->(c)
                """)
    
    def _load_relationships(self, df_rels: pd.DataFrame):
        """Load relationships between tables"""
        for _, rel in df_rels.iterrows():
            relationship_id = str(rel['relationship_id']).replace("'", "\\'")
            from_table_id = str(rel['from_table_id']).replace("'", "\\'")
            to_table_id = str(rel['to_table_id']).replace("'", "\\'")
            from_multiplicity = str(rel['from_multiplicity']).replace("'", "\\'") if pd.notna(rel['from_multiplicity']) else ''
            to_multiplicity = str(rel['to_multiplicity']).replace("'", "\\'") if pd.notna(rel['to_multiplicity']) else ''
            identifying = str(rel['identifying']).replace("'", "\\'") if pd.notna(rel['identifying']) else ''
            
            self.graph.query(f"""
                MATCH (t1:Table {{table_id: '{from_table_id}'}}),
                      (t2:Table {{table_id: '{to_table_id}'}})
                CREATE (t1)-[r:RELATES_TO {{
                    relationship_id: '{relationship_id}',
                    from_multiplicity: '{from_multiplicity}',
                    to_multiplicity: '{to_multiplicity}',
                    identifying: '{identifying}'
                }}]->(t2)
            """)
            
            print(f"Created relationship: {from_table_id} -> {to_table_id}")


# ============================================================================
# ENHANCED GRAPH RAG RETRIEVER WITH CONTEXT
# ============================================================================

class EnhancedGraphRAGRetriever:
    """Retrieve schema with full context: relationships, lineage, transformations"""
    
    def __init__(self, host: str = FALKORDB_HOST, port: int = FALKORDB_PORT):
        self.db = FalkorDB(host=host, port=port)
        self.graph = self.db.select_graph('schema_graph')
    
    def semantic_search_with_context(self, query_text: str, top_k: int = 10) -> List[Dict]:
        """Search for columns with full contextual information"""
        query_embedding = create_embedding(query_text)
        
        # Get candidates with semantic similarity
        result = self.graph.query(f"""
            CALL db.idx.vector.queryNodes('Column', 'name_embedding', {top_k}, vecf32({query_embedding}))
            YIELD node, score
            MATCH (t:Table)-[:HAS_COLUMN]->(node)
            RETURN 
                node.column_id AS column_id,
                node.column_name AS column_name,
                node.type AS type,
                node.primary_key AS primary_key,
                node.foreign_key AS foreign_key,
                node.nullable AS nullable,
                t.table_name AS table_name,
                t.table_id AS table_id,
                t.data_model AS data_model,
                score
            ORDER BY score DESC
        """)
        
        columns = []
        for record in result.result_set:
            column_info = {
                'column_id': record[0],
                'column_name': record[1],
                'type': record[2],
                'primary_key': record[3],
                'foreign_key': record[4],
                'nullable': record[5],
                'table_name': record[6],
                'table_id': record[7],
                'data_model': record[8],
                'similarity_score': record[9]
            }
            
            # Enrich with context
            column_info['relationships'] = self.get_table_relationships(record[7])
            column_info['related_columns'] = self.get_related_columns_in_table(record[7])
            column_info['lineage'] = self.get_column_lineage(record[7], record[1])
            column_info['table_context'] = self.get_table_context(record[7])
            
            columns.append(column_info)
        
        return columns
    
    def get_table_relationships(self, table_id: str) -> Dict[str, List[Dict]]:
        """Get both incoming and outgoing relationships for a table"""
        table_id_escaped = str(table_id).replace("'", "\\'")
        
        # Outgoing relationships
        outgoing = self.graph.query(f"""
            MATCH (t1:Table {{table_id: '{table_id_escaped}'}})-[r:RELATES_TO]->(t2:Table)
            RETURN 
                'outgoing' AS direction,
                t2.table_id AS related_table_id,
                t2.table_name AS related_table_name,
                r.from_multiplicity AS from_multiplicity,
                r.to_multiplicity AS to_multiplicity,
                r.relationship_id AS relationship_id
        """)
        
        # Incoming relationships
        incoming = self.graph.query(f"""
            MATCH (t1:Table)-[r:RELATES_TO]->(t2:Table {{table_id: '{table_id_escaped}'}})
            RETURN 
                'incoming' AS direction,
                t1.table_id AS related_table_id,
                t1.table_name AS related_table_name,
                r.from_multiplicity AS from_multiplicity,
                r.to_multiplicity AS to_multiplicity,
                r.relationship_id AS relationship_id
        """)
        
        relationships = {
            'outgoing': [],
            'incoming': []
        }
        
        for record in outgoing.result_set:
            relationships['outgoing'].append({
                'related_table': record[2],
                'cardinality': f"{record[3]}:{record[4]}"
            })
        
        for record in incoming.result_set:
            relationships['incoming'].append({
                'related_table': record[2],
                'cardinality': f"{record[3]}:{record[4]}"
            })
        
        return relationships
    
    def get_related_columns_in_table(self, table_id: str) -> List[Dict]:
        """Get all columns in the same table for context"""
        table_id_escaped = str(table_id).replace("'", "\\'")
        
        result = self.graph.query(f"""
            MATCH (t:Table {{table_id: '{table_id_escaped}'}})-[:HAS_COLUMN]->(c:Column)
            RETURN 
                c.column_name AS column_name,
                c.type AS type,
                c.primary_key AS primary_key,
                c.foreign_key AS foreign_key
            ORDER BY c.column_name
        """)
        
        columns = []
        for record in result.result_set:
            columns.append({
                'column_name': record[0],
                'type': record[1],
                'is_pk': record[2] == 'Y',
                'is_fk': record[3] == 'Y'
            })
        
        return columns
    
    def get_column_lineage(self, table_id: str, column_name: str) -> Dict[str, Any]:
        """Get data lineage through foreign key relationships"""
        table_id_escaped = str(table_id).replace("'", "\\'")
        column_name_escaped = str(column_name).replace("'", "\\'")
        
        # Find if this column is a foreign key
        fk_result = self.graph.query(f"""
            MATCH (t:Table {{table_id: '{table_id_escaped}'}})-[:HAS_COLUMN]->(c:Column {{column_name: '{column_name_escaped}'}})
            WHERE c.foreign_key = 'Y'
            RETURN c.foreign_key
        """)
        
        if not fk_result.result_set:
            return {'is_foreign_key': False, 'references': None}
        
        # Trace lineage through relationships
        lineage_result = self.graph.query(f"""
            MATCH (source:Table {{table_id: '{table_id_escaped}'}})-[r:RELATES_TO]->(target:Table)
            MATCH (target)-[:HAS_COLUMN]->(target_col:Column)
            WHERE target_col.primary_key = 'Y'
            RETURN 
                target.table_name AS referenced_table,
                target_col.column_name AS referenced_column,
                r.from_multiplicity AS from_card,
                r.to_multiplicity AS to_card
        """)
        
        references = []
        for record in lineage_result.result_set:
            references.append({
                'referenced_table': record[0],
                'referenced_column': record[1],
                'relationship': f"{record[2]}:{record[3]}"
            })
        
        return {
            'is_foreign_key': True,
            'references': references
        }
    
    def get_table_context(self, table_id: str) -> Dict[str, Any]:
        """Get comprehensive table context"""
        table_id_escaped = str(table_id).replace("'", "\\'")
        
        result = self.graph.query(f"""
            MATCH (t:Table {{table_id: '{table_id_escaped}'}})-[:HAS_COLUMN]->(c:Column)
            WITH t, count(c) AS column_count,
                 sum(CASE WHEN c.primary_key = 'Y' THEN 1 ELSE 0 END) AS pk_count,
                 sum(CASE WHEN c.foreign_key = 'Y' THEN 1 ELSE 0 END) AS fk_count
            OPTIONAL MATCH (t)-[r:RELATES_TO]-()
            WITH t, column_count, pk_count, fk_count, count(DISTINCT r) AS relationship_count
            RETURN 
                t.table_name AS table_name,
                t.data_model AS data_model,
                column_count,
                pk_count,
                fk_count,
                relationship_count
        """)
        
        if result.result_set:
            record = result.result_set[0]
            return {
                'table_name': record[0],
                'data_model': record[1],
                'column_count': record[2],
                'primary_keys': record[3],
                'foreign_keys': record[4],
                'relationships': record[5]
            }
        
        return {}


# ============================================================================
# ADVANCED AGENT STATE
# ============================================================================

class EnhancedMappingState(TypedDict):
    """Enhanced state with full context"""
    transaction_field: Dict[str, Any]
    candidate_columns: List[Dict]
    contextual_analysis: Dict[str, Any]
    expert_analyses: Dict[str, str]  # Multiple expert perspectives
    reasoning_paths: List[Dict]  # Chain of thought reasoning
    mapping_decision: Dict[str, Any]
    verification_result: Dict[str, Any]
    final_mapping: Dict[str, Any]
    messages: Annotated[List, add]


# ============================================================================
# ADVANCED PROMPTING TECHNIQUES
# ============================================================================

CHAIN_OF_THOUGHT_PROMPT = """You are an expert data analyst. Use step-by-step reasoning to solve this mapping problem.

Think through this systematically:

1. UNDERSTAND THE SOURCE FIELD
   - What is the semantic meaning?
   - What is the data type and format?
   - What transformations are being applied?
   - What is the business context?

2. ANALYZE EACH CANDIDATE
   - How well does the name match semantically?
   - Is the data type compatible?
   - Do the transformations align?
   - Are the relationships consistent?

3. CONSIDER CONTEXT
   - What other columns exist in both tables?
   - What are the foreign key relationships?
   - What is the data lineage?
   - How does cardinality affect the mapping?

4. EVALUATE RISKS
   - Data loss in transformation?
   - Type conversion issues?
   - Referential integrity concerns?
   - Performance implications?

5. MAKE RECOMMENDATION
   - Which candidate is the best match?
   - What is your confidence level?
   - What transformations are needed?
   - What should be validated?

Walk through each step explicitly, showing your reasoning."""

MIXTURE_OF_EXPERTS_PROMPTS = {
    'database_expert': """You are a database schema expert specializing in relational database design, 
normalization, foreign keys, and referential integrity. Analyze the mapping from a database structure perspective.

Focus on:
- Primary key / foreign key relationships
- Referential integrity constraints
- Cardinality and multiplicity
- Table normalization levels
- Index implications""",

    'etl_expert': """You are an ETL (Extract, Transform, Load) specialist with expertise in data transformations,
type conversions, and data integration patterns.

Focus on:
- SQL transformation logic compatibility
- Data type conversion safety
- NULL handling and default values
- Data quality and validation rules
- Performance of transformations""",

    'business_analyst': """You are a business analyst who understands data semantics, business rules,
and how data represents real-world entities and relationships.

Focus on:
- Semantic meaning alignment
- Business logic consistency
- Data governance and compliance
- Domain-specific terminology
- Data usage patterns""",

    'data_quality_expert': """You are a data quality specialist focused on data accuracy, completeness,
consistency, and fitness for use.

Focus on:
- Data validation rules
- Quality metrics and thresholds
- Data profiling compatibility
- Anomaly detection considerations
- Master data management implications"""
}


# ============================================================================
# ENHANCED AGENTS WITH ADVANCED REASONING
# ============================================================================

class AdvancedMappingAgents:
    """Agents with sophisticated reasoning capabilities"""
    
    def __init__(self, retriever: EnhancedGraphRAGRetriever):
        self.retriever = retriever
        self.llm = llm
        self.reasoning_llm = reasoning_llm
    
    def retriever_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Retrieve candidates with full contextual information"""
        field = state['transaction_field']
        standardized_name = field['Standardised Name']
        
        print(f"\nüîç Retriever Agent: Enhanced search for '{standardized_name}'")
        
        # Build comprehensive search query
        search_components = [standardized_name]
        
        if pd.notna(field.get('Creation')):
            search_components.append(field['Creation'])
        
        if pd.notna(field.get('Transformation')):
            search_components.append(field['Transformation'])
        
        if pd.notna(field.get('Comment')):
            search_components.append(field['Comment'])
        
        search_query = " ".join(search_components)
        
        # Get candidates with full context
        candidates = self.retriever.semantic_search_with_context(search_query, top_k=10)
        
        state['candidate_columns'] = candidates
        state['messages'].append({
            'role': 'retriever',
            'content': f"Found {len(candidates)} candidates with relationships and lineage"
        })
        
        return state
    
    def context_analyzer_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Analyze contextual information: relationships, lineage, transformations"""
        field = state['transaction_field']
        candidates = state['candidate_columns']
        
        print(f"üìä Context Analyzer: Analyzing relationships and lineage")
        
        contextual_analysis = {
            'transformation_analysis': self._analyze_transformations(field, candidates),
            'relationship_analysis': self._analyze_relationships(candidates),
            'lineage_analysis': self._analyze_lineage(candidates),
            'type_compatibility': self._analyze_type_compatibility(field, candidates)
        }
        
        state['contextual_analysis'] = contextual_analysis
        state['messages'].append({
            'role': 'context_analyzer',
            'content': json.dumps(contextual_analysis, indent=2)
        })
        
        return state
    
    def _analyze_transformations(self, field: Dict, candidates: List[Dict]) -> Dict:
        """Analyze SQL transformation compatibility"""
        creation_sql = str(field.get('Creation', ''))
        transformation_sql = str(field.get('Transformation', ''))
        
        analysis = {
            'has_transformation': bool(transformation_sql),
            'transformation_complexity': 'complex' if 'CASE' in transformation_sql or 'JOIN' in transformation_sql else 'simple',
            'operations': []
        }
        
        # Extract operations from SQL
        if 'CAST' in transformation_sql:
            analysis['operations'].append('type_conversion')
        if 'CONCAT' in transformation_sql:
            analysis['operations'].append('string_concatenation')
        if 'UPPER' in transformation_sql or 'LOWER' in transformation_sql:
            analysis['operations'].append('case_normalization')
        if 'TRIM' in transformation_sql:
            analysis['operations'].append('whitespace_handling')
        
        return analysis
    
    def _analyze_relationships(self, candidates: List[Dict]) -> Dict:
        """Analyze relationship patterns across candidates"""
        analysis = {
            'candidates_with_relationships': 0,
            'common_related_tables': [],
            'relationship_patterns': []
        }
        
        for candidate in candidates:
            rels = candidate.get('relationships', {})
            if rels.get('incoming') or rels.get('outgoing'):
                analysis['candidates_with_relationships'] += 1
                
                # Track relationship patterns
                for rel in rels.get('outgoing', []):
                    analysis['relationship_patterns'].append({
                        'table': candidate['table_name'],
                        'relates_to': rel['related_table'],
                        'type': 'references'
                    })
        
        return analysis
    
    def _analyze_lineage(self, candidates: List[Dict]) -> Dict:
        """Analyze data lineage patterns"""
        analysis = {
            'foreign_key_candidates': [],
            'primary_key_candidates': [],
            'lineage_chains': []
        }
        
        for candidate in candidates:
            lineage = candidate.get('lineage', {})
            
            if lineage.get('is_foreign_key'):
                analysis['foreign_key_candidates'].append({
                    'column': candidate['column_name'],
                    'table': candidate['table_name'],
                    'references': lineage.get('references', [])
                })
            
            if candidate['primary_key'] == 'Y':
                analysis['primary_key_candidates'].append({
                    'column': candidate['column_name'],
                    'table': candidate['table_name']
                })
        
        return analysis
    
    def _analyze_type_compatibility(self, field: Dict, candidates: List[Dict]) -> List[Dict]:
        """Analyze data type compatibility"""
        source_type = str(field.get('DataType', '')).upper()
        
        compatibility = []
        for candidate in candidates:
            target_type = str(candidate.get('type', '')).upper()
            
            compatible = self._check_type_compatibility(source_type, target_type)
            compatibility.append({
                'candidate': f"{candidate['table_name']}.{candidate['column_name']}",
                'source_type': source_type,
                'target_type': target_type,
                'compatible': compatible,
                'requires_conversion': not compatible
            })
        
        return compatibility
    
    def _check_type_compatibility(self, source: str, target: str) -> bool:
        """Check if types are compatible"""
        # Define compatibility matrix
        compatible_types = {
            'INTEGER': ['INT', 'INTEGER', 'BIGINT', 'DECIMAL', 'NUMERIC'],
            'INT': ['INT', 'INTEGER', 'BIGINT', 'DECIMAL', 'NUMERIC'],
            'VARCHAR': ['VARCHAR', 'CHAR', 'TEXT', 'STRING'],
            'DATE': ['DATE', 'TIMESTAMP', 'DATETIME'],
            'DECIMAL': ['DECIMAL', 'NUMERIC', 'FLOAT', 'DOUBLE']
        }
        
        for base_type, compatible_list in compatible_types.items():
            if base_type in source:
                return any(comp in target for comp in compatible_list)
        
        return source == target
    
    def mixture_of_experts_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Get perspectives from multiple expert agents"""
        field = state['transaction_field']
        candidates = state['candidate_columns'][:3]  # Top 3 candidates
        context = state['contextual_analysis']
        
        print(f"üë• Mixture of Experts: Consulting specialists")
        
        expert_analyses = {}
        
        # Prepare context for experts
        expert_context = self._prepare_expert_context(field, candidates, context)
        
        for expert_name, expert_prompt in MIXTURE_OF_EXPERTS_PROMPTS.items():
            print(f"   Consulting {expert_name}...")
            
            messages = [
                SystemMessage(content=expert_prompt),
                HumanMessage(content=expert_context)
            ]
            
            response = self.llm.invoke(messages)
            expert_analyses[expert_name] = response.content
        
        state['expert_analyses'] = expert_analyses
        state['messages'].append({
            'role': 'mixture_of_experts',
            'content': f"Collected {len(expert_analyses)} expert opinions"
        })
        
        return state
    
    def _prepare_expert_context(self, field: Dict, candidates: List[Dict], context: Dict) -> str:
        """Prepare comprehensive context for expert analysis"""
        context_str = f"""
TRANSACTION FIELD TO MAP:
Field Name: {field['Standardised Name']}
Original Name: {field['TT COLUMN/FIELD NAME']}
Data Type: {field.get('DataType', 'N/A')}
Is Primary Key: {field.get('PK', 'N/A')}
Comment: {field.get('Comment', 'N/A')}
Creation SQL: {field.get('Creation', 'N/A')}
Transformation SQL: {field.get('Transformation', 'N/A')}

TOP CANDIDATE COLUMNS:
"""
        
        for i, candidate in enumerate(candidates, 1):
            context_str += f"""
Candidate {i}: {candidate['table_name']}.{candidate['column_name']}
- Type: {candidate['type']}
- Primary Key: {candidate['primary_key']}
- Foreign Key: {candidate['foreign_key']}
- Similarity Score: {candidate['similarity_score']:.4f}
- Relationships: {len(candidate.get('relationships', {}).get('outgoing', []))} outgoing, {len(candidate.get('relationships', {}).get('incoming', []))} incoming
- Related Columns in Table: {len(candidate.get('related_columns', []))}
"""
            
            # Add relationship details
            if candidate.get('relationships'):
                rels = candidate['relationships']
                if rels.get('outgoing'):
                    context_str += f"  References: {', '.join([r['related_table'] for r in rels['outgoing']])}\n"
                if rels.get('incoming'):
                    context_str += f"  Referenced By: {', '.join([r['related_table'] for r in rels['incoming']])}\n"
            
            # Add lineage details
            lineage = candidate.get('lineage', {})
            if lineage.get('is_foreign_key') and lineage.get('references'):
                refs = lineage['references']
                context_str += f"  FK References: {', '.join([f\"{r['referenced_table']}.{r['referenced_column']}\" for r in refs])}\n"
        
        context_str += f"""

CONTEXTUAL ANALYSIS:
Transformation Complexity: {context['transformation_analysis'].get('transformation_complexity', 'N/A')}
Operations: {', '.join(context['transformation_analysis'].get('operations', []))}
Candidates with Relationships: {context['relationship_analysis']['candidates_with_relationships']}
Foreign Key Candidates: {len(context['lineage_analysis']['foreign_key_candidates'])}
Primary Key Candidates: {len(context['lineage_analysis']['primary_key_candidates'])}

Provide your expert analysis focusing on your domain of expertise.
"""
        
        return context_str
    
    def chain_of_thought_reasoning_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Apply chain-of-thought reasoning with o1 model"""
        field = state['transaction_field']
        candidates = state['candidate_columns'][:5]
        context = state['contextual_analysis']
        expert_opinions = state['expert_analyses']
        
        print(f"üß† Chain-of-Thought Reasoning: Deep analysis with o1")
        
        reasoning_context = f"""
FIELD TO MAP: {field['Standardised Name']}

{self._prepare_expert_context(field, candidates, context)}

EXPERT OPINIONS:
"""
        for expert, opinion in expert_opinions.items():
            reasoning_context += f"\n{expert.upper()}:\n{opinion}\n"
        
        reasoning_context += """

Now, use step-by-step reasoning to determine the best mapping.
Consider all expert opinions, relationships, lineage, and transformations.
"""
        
        messages = [
            SystemMessage(content=CHAIN_OF_THOUGHT_PROMPT),
            HumanMessage(content=reasoning_context)
        ]
        
        response = self.reasoning_llm.invoke(messages)
        
        # Parse reasoning into structured paths
        reasoning_paths = [{
            'model': 'o1',
            'reasoning': response.content,
            'timestamp': pd.Timestamp.now().isoformat()
        }]
        
        state['reasoning_paths'] = reasoning_paths
        state['messages'].append({
            'role': 'chain_of_thought',
            'content': 'Completed deep reasoning analysis'
        })
        
        return state
    
    def mapper_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Make final mapping decision based on all analyses"""
        field = state['transaction_field']
        reasoning = state['reasoning_paths'][0]['reasoning']
        candidates = state['candidate_columns']
        
        print(f"üéØ Mapper Agent: Making final decision")
        
        system_prompt = """You are a data mapping specialist.
Based on the comprehensive analysis provided (expert opinions, chain-of-thought reasoning, 
contextual analysis), make a definitive mapping decision.

Return your response as JSON with keys:
{
    "selected_column": "column_name",
    "table_name": "table_name",
    "confidence": 0.0-1.0,
    "reasoning_summary": "brief explanation",
    "transformation_requirements": ["list of required transformations"],
    "relationship_validation": "how relationships validate this mapping",
    "lineage_justification": "how lineage supports this choice",
    "risks": ["list of risks"],
    "recommendations": ["list of recommendations"]
}"""
        
        mapping_context = f"""
Field: {field['Standardised Name']}

Chain-of-Thought Analysis:
{reasoning}

Top Candidates:
{json.dumps([{
    'table': c['table_name'],
    'column': c['column_name'],
    'score': c['similarity_score'],
    'pk': c['primary_key'],
    'fk': c['foreign_key']
} for c in candidates[:3]], indent=2)}

Make the mapping decision.
"""
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=mapping_context)
        ]
        
        response = self.llm.invoke(messages)
        
        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            mapping_decision = json.loads(content)
        except Exception as e:
            print(f"Warning: Could not parse mapper response as JSON: {e}")
            mapping_decision = {
                'selected_column': candidates[0]['column_name'] if candidates else 'None',
                'table_name': candidates[0]['table_name'] if candidates else 'None',
                'confidence': candidates[0]['similarity_score'] if candidates else 0,
                'reasoning_summary': response.content,
                'transformation_requirements': [],
                'relationship_validation': 'See reasoning',
                'lineage_justification': 'See reasoning',
                'risks': [],
                'recommendations': []
            }
        
        state['mapping_decision'] = mapping_decision
        state['messages'].append({
            'role': 'mapper',
            'content': json.dumps(mapping_decision, indent=2)
        })
        
        return state
    
    def verifier_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Verify mapping with relationship and lineage validation"""
        field = state['transaction_field']
        mapping = state['mapping_decision']
        context = state['contextual_analysis']
        
        print(f"‚úÖ Verifier Agent: Validating mapping")
        
        system_prompt = """You are a quality assurance specialist.
Verify the mapping by checking:
1. Data type compatibility
2. Relationship consistency
3. Lineage validity
4. Transformation feasibility
5. Risk assessment

Return JSON with keys: approved (boolean), validation_checks (dict), issues (list), recommendations (list)"""
        
        verification_context = f"""
Proposed Mapping: {mapping['table_name']}.{mapping['selected_column']}
Confidence: {mapping['confidence']}

Contextual Analysis:
{json.dumps(context, indent=2, default=str)}

Relationship Validation: {mapping.get('relationship_validation', 'N/A')}
Lineage Justification: {mapping.get('lineage_justification', 'N/A')}

Verify this mapping comprehensively.
"""
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=verification_context)
        ]
        
        response = self.llm.invoke(messages)
        
        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            verification = json.loads(content)
        except Exception as e:
            print(f"Warning: Could not parse verifier response as JSON: {e}")
            verification = {
                'approved': mapping.get('confidence', 0) > 0.6,
                'validation_checks': {},
                'issues': [],
                'recommendations': []
            }
        
        state['verification_result'] = verification
        state['messages'].append({
            'role': 'verifier',
            'content': json.dumps(verification, indent=2)
        })
        
        return state
    
    def supervisor_agent(self, state: EnhancedMappingState) -> EnhancedMappingState:
        """Supervise and finalize with full traceability"""
        field = state['transaction_field']
        mapping = state['mapping_decision']
        verification = state['verification_result']
        context = state['contextual_analysis']
        expert_analyses = state['expert_analyses']
        
        print(f"üëî Supervisor Agent: Finalizing with traceability")
        
        if verification.get('approved', False) and mapping.get('confidence', 0) > 0.6:
            final_status = 'APPROVED'
        elif mapping.get('confidence', 0) > 0.4:
            final_status = 'REVIEW_REQUIRED'
        else:
            final_status = 'REJECTED'
        
        final_mapping = {
            'transaction_field': field['Standardised Name'],
            'original_field': field['TT COLUMN/FIELD NAME'],
            'mapped_table': mapping.get('table_name'),
            'mapped_column': mapping.get('selected_column'),
            'confidence': mapping.get('confidence'),
            'status': final_status,
            
            # Enhanced context
            'reasoning_summary': mapping.get('reasoning_summary'),
            'transformation_requirements': json.dumps(mapping.get('transformation_requirements', [])),
            'relationship_validation': mapping.get('relationship_validation'),
            'lineage_justification': mapping.get('lineage_justification'),
            
            # Expert opinions
            'database_expert_opinion': expert_analyses.get('database_expert', ''),
            'etl_expert_opinion': expert_analyses.get('etl_expert', ''),
            'business_analyst_opinion': expert_analyses.get('business_analyst', ''),
            'data_quality_expert_opinion': expert_analyses.get('data_quality_expert', ''),
            
            # Contextual metadata
            'has_relationships': context['relationship_analysis']['candidates_with_relationships'] > 0,
            'is_foreign_key': len(context['lineage_analysis']['foreign_key_candidates']) > 0,
            'transformation_complexity': context['transformation_analysis'].get('transformation_complexity'),
            
            # Validation
            'verification_issues': json.dumps(verification.get('issues', [])),
            'risks': json.dumps(mapping.get('risks', [])),
            'recommendations': json.dumps(verification.get('recommendations', []))
        }
        
        state['final_mapping'] = final_mapping
        state['messages'].append({
            'role': 'supervisor',
            'content': f"Final Status: {final_status}"
        })
        
        return state


# ============================================================================
# ENHANCED LANGGRAPH WORKFLOW
# ============================================================================

def create_advanced_mapping_workflow(retriever: EnhancedGraphRAGRetriever) -> StateGraph:
    """Create advanced workflow with all reasoning agents"""
    agents = AdvancedMappingAgents(retriever)
    
    workflow = StateGraph(EnhancedMappingState)
    
    # Add all agent nodes
    workflow.add_node("retriever", agents.retriever_agent)
    workflow.add_node("context_analyzer", agents.context_analyzer_agent)
    workflow.add_node("mixture_of_experts", agents.mixture_of_experts_agent)
    workflow.add_node("chain_of_thought", agents.chain_of_thought_reasoning_agent)
    workflow.add_node("mapper", agents.mapper_agent)
    workflow.add_node("verifier", agents.verifier_agent)
    workflow.add_node("supervisor", agents.supervisor_agent)
    
    # Define sophisticated workflow
    workflow.set_entry_point("retriever")
    workflow.add_edge("retriever", "context_analyzer")
    workflow.add_edge("context_analyzer", "mixture_of_experts")
    workflow.add_edge("mixture_of_experts", "chain_of_thought")
    workflow.add_edge("chain_of_thought", "mapper")
    workflow.add_edge("mapper", "verifier")
    workflow.add_edge("verifier", "supervisor")
    workflow.add_edge("supervisor", END)
    
    return workflow


# ============================================================================
# ENHANCED PIPELINE
# ============================================================================

class AdvancedSchemaMappingPipeline:
    """Complete pipeline with advanced reasoning"""
    
    def __init__(self):
        self.loader = SchemaLoader()
        self.retriever = EnhancedGraphRAGRetriever()
        self.workflow = create_advanced_mapping_workflow(self.retriever).compile(
            checkpointer=MemorySaver()
        )
    
    def load_schema(self, attributes_csv: str, relationships_csv: str):
        """Load schema into FalkorDB"""
        self.loader.load_schema(attributes_csv, relationships_csv)
    
    def map_transaction_fields(self, transaction_csv: str, output_csv: str = "mapping_results.csv"):
        """Map transaction fields using advanced reasoning"""
        print("\n" + "="*80)
        print("ADVANCED SCHEMA MAPPING WITH REASONING")
        print("="*80)
        
        df_trans = pd.read_csv(transaction_csv)
        results = []
        
        for idx, field in df_trans.iterrows():
            print(f"\n{'='*80}")
            print(f"Processing Field {idx + 1}/{len(df_trans)}: {field['Standardised Name']}")
            print(f"{'='*80}")
            
            initial_state = {
                'transaction_field': field.to_dict(),
                'candidate_columns': [],
                'contextual_analysis': {},
                'expert_analyses': {},
                'reasoning_paths': [],
                'mapping_decision': {},
                'verification_result': {},
                'final_mapping': {},
                'messages': []
            }
            
            config = {"configurable": {"thread_id": f"mapping_{idx}"}}
            
            try:
                final_state = self.workflow.invoke(initial_state, config)
                results.append(final_state['final_mapping'])
                
                mapping = final_state['final_mapping']
                print(f"\nüìä MAPPING SUMMARY:")
                print(f"   Status: {mapping['status']}")
                print(f"   Mapped to: {mapping['mapped_table']}.{mapping['mapped_column']}")
                print(f"   Confidence: {mapping['confidence']:.4f}")
                print(f"   Has Relationships: {mapping['has_relationships']}")
                print(f"   Is Foreign Key: {mapping['is_foreign_key']}")
            except Exception as e:
                print(f"‚ùå Error processing field: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        df_results = pd.DataFrame(results)
        df_results.to_csv(output_csv, index=False)
        
        print(f"\n{'='*80}")
        print(f"MAPPING COMPLETE! Results saved to {output_csv}")
        print(f"{'='*80}")
        
        return df_results


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    pipeline = AdvancedSchemaMappingPipeline()
    
    print("Loading schema into FalkorDB...")
    pipeline.load_schema(
        attributes_csv="schema_attributes.csv",
        relationships_csv="schema_relationships.csv"
    )
    
    print("\nMapping transaction fields with advanced reasoning...")
    results = pipeline.map_transaction_fields(
        transaction_csv="transaction_fields.csv",
        output_csv="advanced_mapping_results.csv"
    )
    
    print("\n" + "="*80)
    print("ADVANCED MAPPING STATISTICS")
    print("="*80)
    print(f"Total Fields: {len(results)}")
    print(f"Approved: {len(results[results['status'] == 'APPROVED'])}")
    print(f"Review Required: {len(results[results['status'] == 'REVIEW_REQUIRED'])}")
    print(f"Rejected: {len(results[results['status'] == 'REJECTED'])}")
    print(f"Average Confidence: {results['confidence'].mean():.4f}")
    print(f"Mappings with Relationships: {results['has_relationships'].sum()}")
    print(f"Foreign Key Mappings: {results['is_foreign_key'].sum()}")
