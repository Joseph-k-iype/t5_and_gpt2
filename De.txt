#!/usr/bin/env python3
"""
Complete Legislation to Machine-Readable Rules Converter
Advanced AI system using DSPy, LangGraph React agents, and complex prompting strategies
With OpenAI Python client direct usage and no fallback mechanisms
"""

import os
import asyncio
import json
import uuid
from typing import Dict, List, Optional, Any, Union, Literal
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, field
from pathlib import Path
import logging
import traceback
import yaml

# Core dependencies
import dspy
from pydantic import BaseModel, Field, validator
import openai
from openai import OpenAI
import numpy as np

# LangGraph and LangChain
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

# =============================================================================
# GLOBAL CONFIGURATION
# =============================================================================

class GlobalConfig:
    """Global configuration for the application"""
    
    # API Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    
    # Model Configuration - Only use o3-mini-2025-01-31
    PRIMARY_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    
    # File Paths
    OUTPUT_DIR = Path("./output")
    LOGS_DIR = Path("./logs")
    CONFIG_DIR = Path("./config")
    
    # Processing Configuration
    MAX_CONCURRENT_REQUESTS = 3
    REQUEST_TIMEOUT = 300
    MAX_RETRIES = 3
    
    # Rule Configuration
    DEFAULT_RULE_PRIORITY = 1
    MAX_CONDITIONS_PER_RULE = 10
    
    @classmethod
    def setup_directories(cls):
        """Create necessary directories"""
        for directory in [cls.OUTPUT_DIR, cls.LOGS_DIR, cls.CONFIG_DIR]:
            directory.mkdir(exist_ok=True)
    
    @classmethod
    def validate_config(cls):
        """Validate configuration"""
        if cls.OPENAI_API_KEY == "your-api-key-here":
            raise ValueError("Please set OPENAI_API_KEY environment variable")
        
        # Set OpenAI configuration
        os.environ["OPENAI_API_KEY"] = cls.OPENAI_API_KEY
        openai.api_key = cls.OPENAI_API_KEY

# Initialize global configuration
GlobalConfig.setup_directories()
GlobalConfig.validate_config()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(GlobalConfig.LOGS_DIR / 'legislation_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# SAFE JSON PARSING UTILITIES
# =============================================================================

class SafeJSONParser:
    """Safe JSON parsing utilities with error handling"""
    
    @staticmethod
    def safe_json_loads(json_string: str, default: Any = None) -> Any:
        """Safely parse JSON string with fallback"""
        if not json_string:
            return default
        
        # Clean the JSON string
        cleaned_string = json_string.strip()
        
        # Remove markdown code blocks if present
        if cleaned_string.startswith('```json'):
            cleaned_string = cleaned_string[7:]
        if cleaned_string.endswith('```'):
            cleaned_string = cleaned_string[:-3]
        
        cleaned_string = cleaned_string.strip()
        
        # Parse JSON
        parsed_data = json.loads(cleaned_string)
        return parsed_data
    
    @staticmethod
    def safe_json_dumps(obj: Any, default: str = "{}") -> str:
        """Safely convert object to JSON string"""
        return json.dumps(obj, indent=2, ensure_ascii=False, default=str)
    
    @staticmethod
    def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
        """Extract JSON object from mixed text content"""
        if not text:
            return None
        
        # Look for JSON-like patterns in the text
        import re
        
        # Find potential JSON objects
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        for match in matches:
            result = SafeJSONParser.safe_json_loads(match)
            if result:
                return result
        
        # If no clear JSON object, try the whole text
        return SafeJSONParser.safe_json_loads(text)

# =============================================================================
# STREAMLINED PYDANTIC V2 MODELS
# =============================================================================

class DataDomain(str, Enum):
    """Streamlined data processing domains"""
    DATA_TRANSFER = "data_transfer"
    DATA_USAGE = "data_usage"
    DATA_STORAGE = "data_storage"
    DATA_ACCESS = "data_access"

class DataRole(str, Enum):
    """Streamlined data processing roles"""
    CONTROLLER = "controller"
    PROCESSOR = "processor"
    JOINT_CONTROLLER = "joint_controller"

class ConditionOperator(str, Enum):
    """Valid operators for rule conditions"""
    EQUAL = "equal"
    NOT_EQUAL = "notEqual"
    GREATER_THAN = "greaterThan"
    LESS_THAN = "lessThan"
    GREATER_THAN_INCLUSIVE = "greaterThanInclusive"
    LESS_THAN_INCLUSIVE = "lessThanInclusive"
    IN = "in"
    NOT_IN = "notIn"
    CONTAINS = "contains"
    DOES_NOT_CONTAIN = "doesNotContain"
    REGEX = "regex"
    EXISTS = "exists"
    DOES_NOT_EXIST = "doesNotExist"

class LogicalOperator(str, Enum):
    """Logical operators for combining conditions"""
    ALL = "all"
    ANY = "any"
    NOT = "not"

class EnhancedRuleCondition(BaseModel):
    """Enhanced rule condition with simple English definition and role"""
    fact: str = Field(..., description="The fact name to evaluate")
    operator: ConditionOperator = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    path: Optional[str] = Field(None, description="JSONPath for accessing nested properties")
    params: Optional[Dict[str, Any]] = Field(None, description="Parameters for fact evaluation")
    
    # Enhanced fields
    simple_english_definition: str = Field(..., description="Simple English explanation of this condition")
    data_role: DataRole = Field(..., description="Role of the entity in this condition")
    applicable_domains: List[DataDomain] = Field(default_factory=list, description="Applicable data domains")
    condition_id: str = Field(default_factory=lambda: f"cond_{uuid.uuid4().hex[:8]}", description="Unique condition ID")
    
    class Config:
        use_enum_values = True

class EnhancedRuleConditions(BaseModel):
    """Enhanced conditions container with logical operators"""
    all: Optional[List[Union['EnhancedRuleConditions', EnhancedRuleCondition]]] = None
    any: Optional[List[Union['EnhancedRuleConditions', EnhancedRuleCondition]]] = None
    not_: Optional[Union['EnhancedRuleConditions', EnhancedRuleCondition]] = Field(None, alias="not")
    
    # Enhanced fields
    logic_explanation: Optional[str] = Field(None, description="Simple English explanation of the logical combination")

    @validator('all', 'any', 'not_', pre=True)
    def validate_conditions(cls, v):
        if v is None:
            return v
        return v

class RuleEvent(BaseModel):
    """Event triggered when rule conditions are met"""
    type: str = Field(..., description="Event type identifier")
    params: Optional[Dict[str, Any]] = Field(None, description="Event parameters")
    description: str = Field(..., description="Simple English description of what happens")
    severity: Literal["low", "medium", "high", "critical"] = Field("medium", description="Event severity")

class EnhancedJsonRule(BaseModel):
    """Enhanced JSON rule with detailed definitions and multiple conditions"""
    conditions: EnhancedRuleConditions = Field(..., description="Rule conditions")
    event: RuleEvent = Field(..., description="Event to trigger")
    priority: int = Field(GlobalConfig.DEFAULT_RULE_PRIORITY, description="Rule priority")
    name: Optional[str] = Field(None, description="Rule name")
    
    # Enhanced fields
    simple_english_definition: str = Field(..., description="Simple English explanation of the entire rule")
    applicable_domains: List[DataDomain] = Field(default_factory=list, description="Data domains this rule applies to")
    rule_category: str = Field(..., description="Category of rule (e.g., compliance, penalty, procedure)")
    legal_basis: Optional[str] = Field(None, description="Legal basis or article reference")
    
    @validator('conditions')
    def validate_condition_count(cls, v):
        # Count total conditions recursively
        def count_conditions(conditions):
            count = 0
            if hasattr(conditions, 'all') and conditions.all:
                for cond in conditions.all:
                    if isinstance(cond, EnhancedRuleCondition):
                        count += 1
                    else:
                        count += count_conditions(cond)
            if hasattr(conditions, 'any') and conditions.any:
                for cond in conditions.any:
                    if isinstance(cond, EnhancedRuleCondition):
                        count += 1
                    else:
                        count += count_conditions(cond)
            if hasattr(conditions, 'not_') and conditions.not_:
                if isinstance(conditions.not_, EnhancedRuleCondition):
                    count += 1
                else:
                    count += count_conditions(conditions.not_)
            return count
        
        total_conditions = count_conditions(v)
        if total_conditions > GlobalConfig.MAX_CONDITIONS_PER_RULE:
            raise ValueError(f"Rule has {total_conditions} conditions, maximum allowed is {GlobalConfig.MAX_CONDITIONS_PER_RULE}")
        
        return v

class LegislationSection(BaseModel):
    """Parsed section of legislation"""
    section_id: str = Field(..., description="Unique section identifier")
    title: str = Field(..., description="Section title")
    content: str = Field(..., description="Section content")
    subsections: List['LegislationSection'] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    # Data domains will be inferred from content, not predefined

class EnhancedExtractedRule(BaseModel):
    """Enhanced rule extracted from legislation with detailed metadata"""
    rule_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    source_section: str = Field(..., description="Source legislation section")
    rule_type: str = Field(..., description="Type of rule (compliance, penalty, etc.)")
    natural_language: str = Field(..., description="Original text")
    structured_rule: EnhancedJsonRule = Field(..., description="Machine-readable rule")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Extraction confidence")
    tags: List[str] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.now)
    
    # Enhanced fields
    total_conditions: int = Field(default=0, description="Total number of conditions in this rule")
    condition_summary: List[str] = Field(default_factory=list, description="Summary of all conditions in simple English")
    validation_status: Literal["pending", "valid", "invalid", "needs_review"] = Field("pending")

class ProcessingResult(BaseModel):
    """Result of legislation processing"""
    document_id: str = Field(...)
    extracted_rules: List[EnhancedExtractedRule] = Field(default_factory=list)
    processing_summary: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    processing_errors: List[str] = Field(default_factory=list)
    processing_warnings: List[str] = Field(default_factory=list)

# =============================================================================
# ENHANCED DSPY SIGNATURES AND MODULES
# =============================================================================

# Enhanced DSPy signatures
class EnhancedLegislationAnalysis(dspy.Signature):
    """Analyze legislation text to intelligently infer data domains and identify rule-worthy content"""
    legislation_text = dspy.InputField(desc="Raw legislation text to analyze (e.g., Article 28 of GDPR)")
    context = dspy.InputField(desc="Additional context about the legislation")
    
    analysis = dspy.OutputField(desc="Detailed analysis of rule-worthy content")
    extracted_sections = dspy.OutputField(desc="Key sections containing rules")
    rule_types = dspy.OutputField(desc="Types of rules identified")
    inferred_data_domains = dspy.OutputField(desc="Data domains inferred from content analysis: data_transfer (cross-border data movement), data_usage (how data is processed/used), data_storage (how data is stored/retained), data_access (who can access data and how)")

class EnhancedRuleExtraction(dspy.Signature):
    """Extract specific rules with multiple conditions and intelligently infer data domains from content"""
    section_text = dspy.InputField(desc="Legislation section text")
    section_context = dspy.InputField(desc="Context about this section")
    rule_type = dspy.InputField(desc="Type of rule to extract")
    
    simple_rule_definition = dspy.OutputField(desc="Simple English definition of the rule")
    conditions_list = dspy.OutputField(desc="List of conditions with simple English definitions and roles (controller/processor/joint_controller)")
    inferred_data_domains = dspy.OutputField(desc="Data domains inferred from content: data_transfer (international transfers, cross-border movement), data_usage (processing activities, purposes), data_storage (retention, deletion, backup), data_access (subject rights, authorized access)")
    legal_basis = dspy.OutputField(desc="Legal basis or article reference")
    consequences = dspy.OutputField(desc="What happens when rule is triggered")

class EnhancedJSONRuleGeneration(dspy.Signature):
    """Convert extracted rule to enhanced JSON rules engine format with inferred data domains"""
    simple_rule = dspy.InputField(desc="Simple English rule definition")
    conditions_with_roles = dspy.InputField(desc="Conditions with roles and definitions")
    inferred_data_domains = dspy.InputField(desc="Data domains inferred from content analysis")
    consequences = dspy.InputField(desc="Rule consequences")
    legal_basis = dspy.InputField(desc="Legal basis")
    
    json_rule = dspy.OutputField(desc="Enhanced rule in json-rules-engine format")
    fact_definitions = dspy.OutputField(desc="Required fact definitions")
    confidence_assessment = dspy.OutputField(desc="Confidence in the conversion (0-1)")
    validation_notes = dspy.OutputField(desc="Notes for validation")

class LLMBasedEntityExtraction(dspy.Signature):
    """Use LLM to extract entities and infer data domains from content"""
    text = dspy.InputField(desc="Text to analyze for entity extraction and domain inference")
    extraction_focus = dspy.InputField(desc="What types of entities to focus on")
    
    controllers = dspy.OutputField(desc="Identified data controllers")
    processors = dspy.OutputField(desc="Identified data processors") 
    joint_controllers = dspy.OutputField(desc="Identified joint controllers")
    data_categories = dspy.OutputField(desc="Types of personal data mentioned")
    processing_purposes = dspy.OutputField(desc="Purposes of data processing")
    dates_timeframes = dspy.OutputField(desc="Dates and time frames mentioned")
    amounts_penalties = dspy.OutputField(desc="Financial amounts and penalties")
    locations = dspy.OutputField(desc="Locations and jurisdictions")
    inferred_domains = dspy.OutputField(desc="Data domains inferred from context: data_transfer (if mentions cross-border, international, third countries), data_usage (if mentions processing activities, purposes), data_storage (if mentions retention, deletion, backup), data_access (if mentions access rights, authorized personnel)")

class LLMBasedStructureParsing(dspy.Signature):
    """Use LLM to parse document structure and infer data domains from content"""
    document_text = dspy.InputField(desc="Full document text to parse")
    parsing_instructions = dspy.InputField(desc="Instructions for parsing structure")
    
    sections = dspy.OutputField(desc="Identified sections with titles and content")
    document_type = dspy.OutputField(desc="Type of legal document")
    hierarchical_structure = dspy.OutputField(desc="Hierarchical organization of content")
    key_topics = dspy.OutputField(desc="Main topics covered in the document")
    content_based_domains = dspy.OutputField(desc="Data domains inferred from document content analysis")

# Enhanced DSPy modules
class EnhancedMixtureOfExpertsAnalyzer(dspy.Module):
    """Enhanced Mixture of Experts for legislation analysis"""
    
    def __init__(self, num_experts: int = 4):
        super().__init__()
        self.num_experts = num_experts
        
        # Specialized expert modules
        self.legal_expert = dspy.ChainOfThought(EnhancedLegislationAnalysis)
        self.compliance_expert = dspy.ChainOfThought(EnhancedLegislationAnalysis) 
        self.technical_expert = dspy.ChainOfThought(EnhancedLegislationAnalysis)
        self.privacy_expert = dspy.ChainOfThought(EnhancedLegislationAnalysis)
        
        # Meta-reasoner with enhanced signature
        self.meta_reasoner = dspy.ChainOfThought("expert_analyses, focus_areas -> consolidated_analysis, confidence_score")
    
    def forward(self, legislation_text, context):
        # Each expert analyzes from their specialized perspective
        legal_analysis = self.legal_expert(
            legislation_text=f"As a legal expert focusing on statutory interpretation, analyze: {legislation_text}",
            context=f"Legal statutory perspective: {context}"
        )
        
        compliance_analysis = self.compliance_expert(
            legislation_text=f"As a compliance expert focusing on operational requirements, analyze: {legislation_text}", 
            context=f"Operational compliance perspective: {context}"
        )
        
        technical_analysis = self.technical_expert(
            legislation_text=f"As a technical expert focusing on implementation, analyze: {legislation_text}",
            context=f"Technical implementation perspective: {context}"
        )
        
        privacy_analysis = self.privacy_expert(
            legislation_text=f"As a privacy expert focusing on data protection, analyze: {legislation_text}",
            context=f"Data protection and privacy perspective: {context}"
        )
        
        # Combine expert analyses with weighted consideration
        expert_analyses_text = f"""
        LEGAL EXPERT ANALYSIS:
        {legal_analysis.analysis}
        Inferred Data Domains: {legal_analysis.inferred_data_domains}
        
        COMPLIANCE EXPERT ANALYSIS:
        {compliance_analysis.analysis}
        Inferred Data Domains: {compliance_analysis.inferred_data_domains}
        
        TECHNICAL EXPERT ANALYSIS:
        {technical_analysis.analysis}
        Inferred Data Domains: {technical_analysis.inferred_data_domains}
        
        PRIVACY EXPERT ANALYSIS:
        {privacy_analysis.analysis}
        Inferred Data Domains: {privacy_analysis.inferred_data_domains}
        """
        
        combined_analysis = self.meta_reasoner(
            expert_analyses=expert_analyses_text,
            focus_areas="rule extraction, inferred data domains from content, compliance requirements"
        )
        
        return dspy.Prediction(
            analysis=combined_analysis.consolidated_analysis,
            confidence_score=combined_analysis.confidence_score,
            expert_views={
                "legal": legal_analysis,
                "compliance": compliance_analysis,
                "technical": technical_analysis,
                "privacy": privacy_analysis
            },
            consolidated_domains=f"{legal_analysis.inferred_data_domains}, {compliance_analysis.inferred_data_domains}, {technical_analysis.inferred_data_domains}, {privacy_analysis.inferred_data_domains}"
        )

class EnhancedChainOfThoughtExtractor(dspy.Module):
    """Enhanced Chain of Thought rule extraction with multiple conditions"""
    
    def __init__(self):
        super().__init__()
        self.extractor = dspy.ChainOfThought(EnhancedRuleExtraction)
        self.json_generator = dspy.ChainOfThought(EnhancedJSONRuleGeneration)
    
    def forward(self, section_text, section_context, rule_type):
        # Step-by-step extraction with detailed reasoning
        extraction = self.extractor(
            section_text=section_text,
            section_context=section_context, 
            rule_type=rule_type
        )
        
        # Generate JSON rule with enhanced format
        json_rule = self.json_generator(
            simple_rule=extraction.simple_rule_definition,
            conditions_with_roles=extraction.conditions_list,
            inferred_data_domains=extraction.inferred_data_domains,
            consequences=extraction.consequences,
            legal_basis=extraction.legal_basis
        )
        
        return dspy.Prediction(
            simple_rule_definition=extraction.simple_rule_definition,
            conditions_list=extraction.conditions_list,
            inferred_data_domains=extraction.inferred_data_domains,
            legal_basis=extraction.legal_basis,
            consequences=extraction.consequences,
            json_rule=json_rule.json_rule,
            fact_definitions=json_rule.fact_definitions,
            confidence=json_rule.confidence_assessment,
            validation_notes=json_rule.validation_notes
        )

class LLMBasedEntityExtractor(dspy.Module):
    """LLM-based entity extraction replacing regex patterns"""
    
    def __init__(self):
        super().__init__()
        self.extractor = dspy.ChainOfThought(LLMBasedEntityExtraction)
    
    def forward(self, text, focus="legal_entities_and_data"):
        extraction = self.extractor(
            text=text,
            extraction_focus=f"Extract {focus} from this legal text and infer applicable data domains. Focus on data controllers, processors, joint controllers, data categories, processing purposes, dates/timeframes, amounts/penalties, locations, and determine which data domains apply based on content."
        )
        
        return dspy.Prediction(
            controllers=extraction.controllers,
            processors=extraction.processors,
            joint_controllers=extraction.joint_controllers,
            data_categories=extraction.data_categories,
            processing_purposes=extraction.processing_purposes,
            dates_timeframes=extraction.dates_timeframes,
            amounts_penalties=extraction.amounts_penalties,
            locations=extraction.locations,
            inferred_domains=extraction.inferred_domains
        )

class LLMBasedStructureParser(dspy.Module):
    """LLM-based structure parsing replacing regex patterns"""
    
    def __init__(self):
        super().__init__()
        self.parser = dspy.ChainOfThought(LLMBasedStructureParsing)
    
    def forward(self, document_text):
        parsing = self.parser(
            document_text=document_text,
            parsing_instructions="Parse this legal document into sections, identify titles, extract content for each section, determine the hierarchical structure, and infer applicable data domains based on content analysis. Look for SECTION, ARTICLE, CHAPTER, PART, TITLE, CLAUSE markers and analyze content to determine if it relates to data transfer, data usage, data storage, or data access."
        )
        
        return dspy.Prediction(
            sections=parsing.sections,
            document_type=parsing.document_type,
            hierarchical_structure=parsing.hierarchical_structure,
            key_topics=parsing.key_topics,
            content_based_domains=parsing.content_based_domains
        )

# =============================================================================
# TOOLS FOR LANGGRAPH REACT AGENTS
# =============================================================================

@tool
def semantic_similarity_search(query: str, text_corpus: List[str], top_k: int = 5) -> List[Dict[str, Any]]:
    """Find semantically similar text sections using OpenAI embeddings directly"""
    # Use OpenAI client directly
    client = OpenAI(api_key=GlobalConfig.OPENAI_API_KEY)
    
    # Get embeddings for query
    query_response = client.embeddings.create(
        input=[query],
        model=GlobalConfig.EMBEDDING_MODEL
    )
    query_embedding = np.array(query_response.data[0].embedding)
    
    # Get embeddings for corpus
    corpus_response = client.embeddings.create(
        input=text_corpus,
        model=GlobalConfig.EMBEDDING_MODEL
    )
    corpus_embeddings = np.array([item.embedding for item in corpus_response.data])
    
    # Compute similarities using cosine similarity
    similarities = np.dot(query_embedding, corpus_embeddings.T)
    
    # Get top k results
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    results = []
    for idx in top_indices:
        results.append({
            "text": text_corpus[idx],
            "similarity": float(similarities[idx]),
            "index": int(idx)
        })
    
    return results

@tool
def llm_parse_legislation_structure(text: str) -> Dict[str, Any]:
    """Parse legislation using LLM"""
    # Initialize LLM-based parser
    dspy.settings.configure(lm=dspy.OpenAI(model=GlobalConfig.PRIMARY_MODEL, api_key=GlobalConfig.OPENAI_API_KEY))
    parser = LLMBasedStructureParser()
    
    # Use LLM to parse structure
    result = parser(text)
    
    # Parse the sections from LLM output
    sections_data = SafeJSONParser.safe_json_loads(result.sections, [])
    
    if not isinstance(sections_data, list):
        # If LLM returned a string description, parse it
        sections = []
        if isinstance(sections_data, str) and sections_data:
            # Parse section information from description
            lines = text.split('\n')
            current_section = None
            section_content = []
            
            section_keywords = ['SECTION', 'ARTICLE', 'CHAPTER', 'PART', 'TITLE', 'CLAUSE']
            
            for line_num, line in enumerate(lines):
                line = line.strip()
                if any(line.upper().startswith(keyword) for keyword in section_keywords):
                    if current_section:
                        sections.append({
                            'title': current_section,
                            'content': '\n'.join(section_content).strip(),
                            'section_id': f"section_{len(sections)}",
                            'line_start': line_num - len(section_content),
                            'line_end': line_num - 1
                        })
                    current_section = line
                    section_content = []
                elif line:
                    section_content.append(line)
            
            if current_section:
                sections.append({
                    'title': current_section,
                    'content': '\n'.join(section_content).strip(),
                    'section_id': f"section_{len(sections)}",
                    'line_start': len(lines) - len(section_content),
                    'line_end': len(lines) - 1
                })
    else:
        sections = sections_data
    
    return {
        "sections": sections,
        "total_sections": len(sections),
        "parsing_status": "success",
        "document_type": result.document_type,
        "key_topics": result.key_topics,
        "content_based_domains": result.content_based_domains,
        "total_lines": len(text.split('\n'))
    }

@tool  
def llm_extract_legal_entities(text: str) -> Dict[str, Any]:
    """Extract legal entities using LLM"""
    # Initialize LLM-based extractor
    dspy.settings.configure(lm=dspy.OpenAI(model=GlobalConfig.PRIMARY_MODEL, api_key=GlobalConfig.OPENAI_API_KEY))
    extractor = LLMBasedEntityExtractor()
    
    # Use LLM to extract entities
    result = extractor(text, "legal_entities_and_data_processing_elements")
    
    # Parse LLM outputs safely
    entities = {
        "controllers": SafeJSONParser.safe_json_loads(result.controllers, []),
        "processors": SafeJSONParser.safe_json_loads(result.processors, []),
        "joint_controllers": SafeJSONParser.safe_json_loads(result.joint_controllers, []),
        "data_categories": SafeJSONParser.safe_json_loads(result.data_categories, []),
        "processing_purposes": SafeJSONParser.safe_json_loads(result.processing_purposes, []),
        "dates_timeframes": SafeJSONParser.safe_json_loads(result.dates_timeframes, []),
        "amounts_penalties": SafeJSONParser.safe_json_loads(result.amounts_penalties, []),
        "locations": SafeJSONParser.safe_json_loads(result.locations, [])
    }
    
    # Parse inferred domains
    inferred_domains = SafeJSONParser.safe_json_loads(result.inferred_domains, [])
    if isinstance(inferred_domains, str):
        inferred_domains = [domain.strip() for domain in inferred_domains.split(',') if domain.strip()]
    
    # Convert string outputs to lists if needed
    for key, value in entities.items():
        if isinstance(value, str):
            # Split on common separators and clean
            entities[key] = [item.strip() for item in value.split(',') if item.strip()]
        elif not isinstance(value, list):
            entities[key] = []
    
    return {
        "entities": entities,
        "inferred_domains": inferred_domains,
        "extraction_status": "success",
        "total_entities": sum(len(v) for v in entities.values())
    }

@tool
def validate_enhanced_json_rule(rule_json: str) -> Dict[str, Any]:
    """Validate enhanced JSON rule format"""
    rule_data = SafeJSONParser.safe_json_loads(rule_json, {})
    
    validation_result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "suggestions": [],
        "parsed_rule": rule_data
    }
    
    # Check required fields
    required_fields = ['conditions', 'event', 'simple_english_definition', 'applicable_domains']
    for field in required_fields:
        if field not in rule_data:
            validation_result["errors"].append(f"Missing required field: {field}")
            validation_result["valid"] = False
    
    # Check conditions structure
    if 'conditions' in rule_data:
        conditions = rule_data['conditions']
        if not any(key in conditions for key in ['all', 'any', 'not']):
            validation_result["errors"].append("Conditions must have 'all', 'any', or 'not' operator")
            validation_result["valid"] = False
        
        # Recursively validate condition structure
        def validate_conditions(conds, path=""):
            if isinstance(conds, dict):
                for key, value in conds.items():
                    if key in ['all', 'any'] and isinstance(value, list):
                        for i, item in enumerate(value):
                            validate_conditions(item, f"{path}.{key}[{i}]")
                    elif key == 'not':
                        validate_conditions(value, f"{path}.not")
                    elif key in ['fact', 'operator', 'value']:
                        # Individual condition validation
                        if key == 'fact' and not isinstance(value, str):
                            validation_result["warnings"].append(f"Fact at {path} should be a string")
                        elif key == 'operator' and value not in [op.value for op in ConditionOperator]:
                            validation_result["warnings"].append(f"Unknown operator '{value}' at {path}")
        
        validate_conditions(conditions, "conditions")
    
    # Check event structure
    if 'event' in rule_data:
        event = rule_data['event']
        if not isinstance(event, dict) or 'type' not in event:
            validation_result["errors"].append("Event must be an object with 'type' field")
            validation_result["valid"] = False
    
    # Check data domains
    if 'applicable_domains' in rule_data:
        domains = rule_data['applicable_domains']
        if isinstance(domains, list):
            valid_domains = [domain.value for domain in DataDomain]
            invalid_domains = [d for d in domains if d not in valid_domains]
            if invalid_domains:
                validation_result["warnings"].append(f"Unknown data domains: {invalid_domains}")
        else:
            validation_result["warnings"].append("applicable_domains should be a list")
    
    # Performance suggestions
    if validation_result["valid"]:
        validation_result["suggestions"].append("Rule structure is valid")
        if len(str(rule_json)) > 1000:
            validation_result["suggestions"].append("Consider simplifying rule for better performance")
    
    return validation_result

# =============================================================================
# ENHANCED LANGGRAPH REACT AGENTS
# =============================================================================

class EnhancedLegislationAnalysisAgent:
    """Enhanced React agent for analyzing legislation"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or GlobalConfig.PRIMARY_MODEL
        
        # Only use the primary model
        self.model = ChatOpenAI(
            model=self.model_name,
            timeout=GlobalConfig.REQUEST_TIMEOUT,
            max_retries=GlobalConfig.MAX_RETRIES
        )
            
        self.tools = [llm_parse_legislation_structure, llm_extract_legal_entities, semantic_similarity_search]
        self.checkpointer = MemorySaver()
        
        self.agent = create_react_agent(
            model=self.model,
            tools=self.tools,
            checkpointer=self.checkpointer,
            prompt="""You are an expert legal analysis specialist with deep knowledge of legislation parsing and data protection regulations.

Your role is to analyze legislation text comprehensively and intelligently infer data domains from content rather than relying on predefined categories.

Key responsibilities:
1. Parse legislation into logical sections using available LLM-based tools
2. Identify legal entities, their roles (controller, processor, joint controller), and relationships
3. Intelligently infer applicable data domains from content analysis:
   - data_transfer: if content mentions cross-border transfers, international data flows, third countries, adequacy decisions
   - data_usage: if content mentions processing activities, purposes, lawful basis, consent
   - data_storage: if content mentions retention periods, deletion, backup, archiving
   - data_access: if content mentions subject access rights, authorized personnel, data portability
4. Find sections containing enforceable rules and compliance requirements
5. Provide structured analysis for downstream rule extraction

For example, "Article 28 of GDPR" should be analyzed to understand it relates to processor obligations, which typically involves data_usage and data_storage domains.

Always:
- Think step by step and use chain-of-thought reasoning
- Use available tools to gather comprehensive information
- Infer data domains based on content analysis, not predefined assumptions
- Identify multiple conditions and their roles clearly
- Provide confidence assessments for your analysis
- Never truncate content - analyze the full text provided"""
        )
    
    async def analyze(self, legislation_text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analyze legislation text with enhanced error handling"""
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # Create a proper conversation flow with SystemMessage and HumanMessage
        messages = [
            SystemMessage(content="""You are an expert legal analysis specialist. Your task is to analyze legislation comprehensively and infer data domains from content. Use available tools to parse structure and extract entities. Focus on identifying rules with multiple conditions and clear data processing implications."""),
            HumanMessage(content=f"""
            Analyze the following legislation text and provide a comprehensive analysis:
            
            LEGISLATION TEXT:
            {legislation_text}
            
            CONTEXT: {SafeJSONParser.safe_json_dumps(context or {})}
            
            Please perform the following analysis:
            1. Parse the structure and identify key sections using the LLM-based parsing tool
            2. Extract legal entities, their roles, and relationships using the LLM-based entity extraction tool
            3. Identify sections that contain enforceable rules with multiple conditions
            4. Intelligently infer applicable data domains from content analysis:
               - data_transfer: for cross-border transfers, international flows, third countries
               - data_usage: for processing activities, purposes, consent, lawful basis
               - data_storage: for retention, deletion, backup, archiving requirements
               - data_access: for subject rights, authorized access, data portability
            5. Assess the complexity and provide rule extraction priorities
            6. Identify controller, processor, and joint controller relationships
            
            For example, if analyzing "Article 28 of GDPR", infer that it relates to processor obligations 
            which typically involves data_usage (processing activities) and data_storage (security measures).
            
            Focus on finding rules that have multiple conditions and clear data processing implications.
            Do not truncate or summarize the content - analyze everything in full.
            """)
        ]
        
        result = await asyncio.wait_for(
            self.agent.ainvoke({
                "messages": messages
            }, config),
            timeout=GlobalConfig.REQUEST_TIMEOUT
        )
        
        return {
            "analysis_result": result,
            "status": "completed",
            "thread_id": thread_id,
            "model_used": self.model_name
        }

class EnhancedRuleExtractionAgent:
    """Enhanced React agent for extracting rules with multiple conditions"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or GlobalConfig.PRIMARY_MODEL
        
        # Only use the primary model
        self.model = ChatOpenAI(
            model=self.model_name,
            timeout=GlobalConfig.REQUEST_TIMEOUT,
            max_retries=GlobalConfig.MAX_RETRIES
        )
        
        # Initialize DSPy
        dspy.settings.configure(lm=dspy.OpenAI(model=self.model_name, api_key=GlobalConfig.OPENAI_API_KEY))
        self.moe_analyzer = EnhancedMixtureOfExpertsAnalyzer()
        self.cot_extractor = EnhancedChainOfThoughtExtractor()
            
        self.tools = [llm_extract_legal_entities, validate_enhanced_json_rule, semantic_similarity_search]
        self.checkpointer = MemorySaver()
        
        self.agent = create_react_agent(
            model=self.model,
            tools=self.tools,
            checkpointer=self.checkpointer,
            prompt="""You are an expert rule extraction specialist focusing on converting legal text to machine-readable rules with multiple conditions and clear role assignments.

Your responsibilities:
1. Extract specific, enforceable rules from legislation sections
2. Identify multiple conditions per rule with simple English definitions
3. Assign appropriate roles (controller, processor, joint controller only) to each condition
4. Intelligently infer applicable data domains from content analysis:
   - data_transfer: when content involves cross-border data movement, international transfers
   - data_usage: when content involves processing activities, purposes, consent mechanisms
   - data_storage: when content involves retention, deletion, backup, security measures
   - data_access: when content involves subject rights, authorized access, data portability
5. Convert rules to enhanced json-rules-engine format
6. Validate extracted rules for completeness and accuracy

Key requirements:
- Each rule must have a simple English definition
- Each condition must have a simple English definition and assigned role
- Infer data domains from content analysis (e.g., Article 28 GDPR → data_usage + data_storage)
- Use chain-of-thought reasoning throughout
- Validate all outputs using available tools
- Handle multiple conditions logically (all/any/not operators)
- Never truncate content - process everything in full"""
        )
    
    async def extract_rules(self, section: LegislationSection, context: Dict[str, Any] = None) -> List[EnhancedExtractedRule]:
        """Extract enhanced rules with proper error handling"""
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        extracted_rules = []
        
        # Use DSPy modules
        # Analyze with mixture of experts
        analysis = self.moe_analyzer(
            legislation_text=section.content,
            context=SafeJSONParser.safe_json_dumps(context or {})
        )
        
        logger.info(f"MoE analysis completed for section {section.section_id}")
        
        # Extract different types of rules
        rule_types = ["compliance", "penalty", "procedure", "data_protection", "consent", "retention", "deletion"]
        
        for rule_type in rule_types:
            # Use DSPy Chain of Thought extractor
            extraction = self.cot_extractor(
                section_text=section.content,
                section_context=f"Section: {section.title}, Type: {rule_type}",
                rule_type=rule_type
            )
            
            if extraction.simple_rule_definition.strip():
                # Parse the generated JSON rule safely
                rule_data = SafeJSONParser.safe_json_loads(extraction.json_rule)
                
                if rule_data:
                    # Create enhanced extracted rule
                    enhanced_rule = EnhancedJsonRule(**rule_data)
                    
                    # Extract condition summaries
                    condition_summaries = []
                    conditions_list_data = SafeJSONParser.safe_json_loads(extraction.conditions_list, [])
                    
                    if isinstance(conditions_list_data, list):
                        condition_summaries = [
                            cond.get('simple_english_definition', 'No definition')
                            for cond in conditions_list_data if isinstance(cond, dict)
                        ]
                    elif isinstance(conditions_list_data, str):
                        condition_summaries = [conditions_list_data]
                    
                    # Parse confidence safely
                    confidence = 0.8
                    if hasattr(extraction, 'confidence'):
                        confidence_str = str(extraction.confidence)
                        if confidence_str.replace('.', '').replace('-', '').isdigit():
                            confidence = float(confidence_str)
                    
                    # Parse inferred domains safely
                    inferred_domains = SafeJSONParser.safe_json_loads(extraction.inferred_data_domains, [])
                    if isinstance(inferred_domains, str):
                        inferred_domains = [domain.strip() for domain in inferred_domains.split(',') if domain.strip()]
                    
                    extracted_rule = EnhancedExtractedRule(
                        source_section=section.section_id,
                        rule_type=rule_type,
                        natural_language=extraction.simple_rule_definition,
                        structured_rule=enhanced_rule,
                        confidence_score=confidence,
                        tags=[rule_type, section.section_id] + inferred_domains,
                        condition_summary=condition_summaries,
                        validation_status="pending"
                    )
                    
                    extracted_rules.append(extracted_rule)
                    logger.info(f"Successfully extracted {rule_type} rule from {section.section_id}")
                    
            else:
                logger.debug(f"No valid rule found for {rule_type} in {section.section_id}")
        
        return extracted_rules

# =============================================================================
# ENHANCED MAIN ORCHESTRATOR USING LANGGRAPH
# =============================================================================

class EnhancedAgentState(BaseModel):
    """Enhanced state for the LangGraph workflow"""
    messages: List[Dict[str, Any]] = Field(default_factory=list)
    legislation_text: str = ""
    current_section: Optional[LegislationSection] = None
    extracted_rules: List[EnhancedExtractedRule] = Field(default_factory=list)
    processing_context: Dict[str, Any] = Field(default_factory=dict)
    next_action: str = "analyze"
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    processing_stage: str = "initialized"

class EnhancedLegislationProcessorWorkflow:
    """Enhanced main workflow orchestrator using LangGraph"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or GlobalConfig.PRIMARY_MODEL
        
        # Initialize agents
        self.analysis_agent = EnhancedLegislationAnalysisAgent(model_name)
        self.extraction_agent = EnhancedRuleExtractionAgent(model_name)
        logger.info("Agents initialized successfully")
        
        # Build workflow graph
        self.workflow = self._build_workflow()
        logger.info("Workflow graph built successfully")
    
    def _build_workflow(self) -> StateGraph:
        """Build the enhanced LangGraph workflow"""
        workflow = StateGraph(EnhancedAgentState)
        
        # Add nodes
        workflow.add_node("analyze", self._analyze_node)
        workflow.add_node("extract", self._extract_node)
        workflow.add_node("validate", self._validate_node)
        workflow.add_node("finalize", self._finalize_node)
        
        # Add edges
        workflow.add_edge(START, "analyze")
        workflow.add_edge("analyze", "extract")
        workflow.add_edge("extract", "validate")
        workflow.add_edge("validate", "finalize")
        workflow.add_edge("finalize", END)
        
        return workflow.compile(checkpointer=MemorySaver())
    
    async def _analyze_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced analysis node"""
        logger.info("Starting legislation analysis...")
        state["processing_stage"] = "analyzing"
        
        analysis_result = await self.analysis_agent.analyze(
            state["legislation_text"],
            state.get("processing_context", {})
        )
        
        if analysis_result.get("status") == "completed":
            # Parse sections from analysis using LLM-based parsing
            structure = llm_parse_legislation_structure(state["legislation_text"])
            
            if structure.get("parsing_status") == "success":
                sections = [
                    LegislationSection(
                        section_id=s["section_id"],
                        title=s["title"],
                        content=s["content"],
                        metadata={"line_start": s.get("line_start"), "line_end": s.get("line_end")}
                    )
                    for s in structure["sections"]
                ]
                
                state["processing_context"]["sections"] = [s.model_dump() for s in sections]
                state["processing_context"]["analysis"] = analysis_result
                
                logger.info(f"Successfully parsed {len(sections)} sections")
            else:
                state["errors"].append(f"Structure parsing failed")
        else:
            state["errors"].append(f"Analysis failed: {analysis_result.get('error', 'Unknown error')}")
        
        state["messages"].append({
            "node": "analyze",
            "result": analysis_result,
            "timestamp": datetime.now().isoformat(),
            "status": "completed" if not state.get("errors") else "failed"
        })
        
        return state
    
    async def _extract_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced extraction node"""
        logger.info("Starting rule extraction...")
        state["processing_stage"] = "extracting"
        
        sections_data = state.get("processing_context", {}).get("sections", [])
        all_extracted_rules = []
        
        # Process sections with concurrency control
        semaphore = asyncio.Semaphore(GlobalConfig.MAX_CONCURRENT_REQUESTS)
        
        async def extract_from_section(section_data):
            async with semaphore:
                section = LegislationSection(**section_data)
                rules = await self.extraction_agent.extract_rules(
                    section=section,
                    context=state.get("processing_context", {})
                )
                return rules
        
        # Extract rules concurrently
        tasks = [extract_from_section(section_data) for section_data in sections_data]
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    state["errors"].append(f"Section extraction error: {str(result)}")
                elif isinstance(result, list):
                    all_extracted_rules.extend(result)
        
        state["extracted_rules"] = [rule.model_dump() for rule in all_extracted_rules]
        
        logger.info(f"Extracted {len(all_extracted_rules)} rules from {len(sections_data)} sections")
        
        state["messages"].append({
            "node": "extract",
            "rules_count": len(all_extracted_rules),
            "timestamp": datetime.now().isoformat(),
            "status": "completed"
        })
        
        return state
    
    async def _validate_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced validation node"""
        logger.info("Starting rule validation...")
        state["processing_stage"] = "validating"
        
        extracted_rules_data = state.get("extracted_rules", [])
        validation_results = []
        
        for rule_data in extracted_rules_data:
            # Validate JSON format safely
            structured_rule = rule_data.get("structured_rule", {})
            json_validation = validate_enhanced_json_rule(
                SafeJSONParser.safe_json_dumps(structured_rule)
            )
            
            validation_results.append({
                "rule_id": rule_data.get("rule_id"),
                "validation": json_validation,
                "overall_valid": json_validation.get("valid", False)
            })
            
            # Update rule validation status
            rule_data["validation_status"] = "valid" if json_validation.get("valid") else "invalid"
        
        state["processing_context"]["validation"] = {
            "validation_results": validation_results,
            "total_rules": len(extracted_rules_data),
            "valid_rules": sum(1 for r in validation_results if r.get("overall_valid", False))
        }
        
        logger.info(f"Validated {len(validation_results)} rules")
        
        state["messages"].append({
            "node": "validate",
            "result": state["processing_context"]["validation"],
            "timestamp": datetime.now().isoformat(),
            "status": "completed"
        })
        
        return state
    
    async def _finalize_node(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced finalization node"""
        logger.info("Finalizing processing...")
        state["processing_stage"] = "finalizing"
        
        # Filter valid rules
        validation_data = state.get("processing_context", {}).get("validation", {})
        valid_rule_ids = {
            r["rule_id"] for r in validation_data.get("validation_results", [])
            if r.get("overall_valid", False)
        }
        
        all_rules_data = state.get("extracted_rules", [])
        valid_rules_data = [
            rule_data for rule_data in all_rules_data
            if rule_data.get("rule_id") in valid_rule_ids
        ]
        
        # Create final processing result
        result_data = {
            "document_id": str(uuid.uuid4()),
            "extracted_rules": valid_rules_data,
            "processing_summary": {
                "total_sections": len(state.get("processing_context", {}).get("sections", [])),
                "total_rules_extracted": len(all_rules_data),
                "valid_rules": len(valid_rules_data),
                "processing_time": datetime.now().isoformat(),
                "processing_errors": len(state.get("errors", [])),
                "processing_warnings": len(state.get("warnings", []))
            },
            "metadata": state.get("processing_context", {}),
            "processing_errors": state.get("errors", []),
            "processing_warnings": state.get("warnings", [])
        }
        
        state["processing_context"]["final_result"] = result_data
        
        state["messages"].append({
            "node": "finalize",
            "result": "completed",
            "timestamp": datetime.now().isoformat(),
            "status": "completed"
        })
        
        logger.info(f"Processing completed successfully with {len(valid_rules_data)} valid rules")
        
        return state
    
    async def process_legislation(
        self,
        legislation_text: str,
        context: Dict[str, Any] = None
    ) -> ProcessingResult:
        """Process legislation text"""
        
        initial_state = EnhancedAgentState(
            legislation_text=legislation_text,
            processing_context=context or {}
        )
        
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # Run the workflow with timeout
        final_state = await asyncio.wait_for(
            self.workflow.ainvoke(
                initial_state.model_dump(),
                config
            ),
            timeout=GlobalConfig.REQUEST_TIMEOUT * 3
        )
        
        # Extract final result
        final_result_data = final_state.get("processing_context", {}).get("final_result", {})
        
        if final_result_data:
            return ProcessingResult(**final_result_data)
        else:
            # Create minimal result if no final result generated
            return ProcessingResult(
                document_id=str(uuid.uuid4()),
                extracted_rules=[],
                processing_summary={"error": "No final result generated"},
                metadata=final_state.get("processing_context", {}),
                processing_errors=final_state.get("errors", ["Processing failed - no final result"])
            )

# =============================================================================
# ENHANCED MAIN APPLICATION INTERFACE
# =============================================================================

class CompleteLegislationRulesConverter:
    """Complete main application interface with all features"""
    
    def __init__(
        self,
        model_name: str = None,
        embedding_model: str = None,
        api_key: str = None
    ):
        # Configure API key
        if api_key:
            GlobalConfig.OPENAI_API_KEY = api_key
            os.environ["OPENAI_API_KEY"] = api_key
        
        self.model_name = model_name or GlobalConfig.PRIMARY_MODEL
        self.embedding_model = embedding_model or GlobalConfig.EMBEDDING_MODEL
        
        # Initialize DSPy
        dspy.settings.configure(
            lm=dspy.OpenAI(model=self.model_name, api_key=GlobalConfig.OPENAI_API_KEY),
            trace=[]
        )
        
        # Initialize workflow
        self.processor = EnhancedLegislationProcessorWorkflow(self.model_name)
        
        logger.info(f"CompleteLegislationRulesConverter initialized successfully")
        logger.info(f"Model: {self.model_name}")
        logger.info(f"Embedding model: {self.embedding_model}")
    
    async def convert_legislation(
        self,
        legislation_text: str,
        context: Dict[str, Any] = None,
        output_format: Literal["json", "yaml", "dict"] = "json"
    ) -> Union[str, Dict[str, Any]]:
        """
        Convert legislation text to machine-readable rules with safe JSON handling
        
        Args:
            legislation_text: The legislation text to process (not truncated)
            context: Additional context for processing
            output_format: Output format ("json", "yaml", or "dict")
            
        Returns:
            Processed rules in the specified format
        """
        
        logger.info("Starting complete legislation conversion...")
        
        # Validate input
        if not legislation_text or not legislation_text.strip():
            raise ValueError("Legislation text cannot be empty")
        
        if len(legislation_text) > 50000:
            logger.warning(f"Large document detected ({len(legislation_text)} chars), processing may take longer")
        
        # Process the legislation - no truncation
        result = await self.processor.process_legislation(legislation_text, context)
        
        # Format output with safe JSON handling
        if output_format == "json":
            return SafeJSONParser.safe_json_dumps(result.model_dump())
        elif output_format == "yaml":
            return yaml.dump(result.model_dump(), default_flow_style=False, allow_unicode=True)
        elif output_format == "dict":
            return result.model_dump()
        else:
            raise ValueError(f"Unsupported output format: {output_format}")
    
    async def batch_convert(
        self,
        legislation_documents: List[Dict[str, str]],
        output_dir: str = None,
        max_concurrent: int = None
    ) -> List[str]:
        """
        Batch convert multiple legislation documents with concurrency control
        """
        
        output_dir = output_dir or str(GlobalConfig.OUTPUT_DIR)
        max_concurrent = max_concurrent or GlobalConfig.MAX_CONCURRENT_REQUESTS
        
        output_files = []
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_document(i, doc):
            async with semaphore:
                logger.info(f"Processing document {i+1}/{len(legislation_documents)}: {doc.get('title', 'Untitled')}")
                
                result = await self.convert_legislation(
                    doc["content"],
                    {"title": doc.get("title", f"Document_{i+1}")}
                )
                
                # Save to file with safe JSON
                title = doc.get("title", f"document_{i+1}")
                safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_title = safe_title.replace(' ', '_')
                filename = f"{safe_title}_rules.json"
                filepath = output_path / filename
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(result)
                
                logger.info(f"Saved results to: {filepath}")
                return str(filepath)
        
        # Process documents concurrently
        tasks = [process_document(i, doc) for i, doc in enumerate(legislation_documents)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful results
        output_files = [result for result in results if result and not isinstance(result, Exception)]
        
        logger.info(f"Batch processing completed: {len(output_files)} successful, {len(legislation_documents) - len(output_files)} failed")
        return output_files
    
    def get_supported_rule_types(self) -> List[str]:
        """Get list of supported rule types"""
        return ["compliance", "penalty", "procedure", "data_protection", "consent", "retention", "deletion"]
    
    def get_supported_data_domains(self) -> List[str]:
        """Get list of supported data domains"""
        return [domain.value for domain in DataDomain]
    
    def get_supported_data_roles(self) -> List[str]:
        """Get list of supported data roles"""
        return [role.value for role in DataRole]
    
    def validate_rule_format(self, rule_json: str) -> Dict[str, Any]:
        """Validate rule against enhanced json-rules-engine format with safe parsing"""
        return validate_enhanced_json_rule(rule_json)
    
    def get_configuration(self) -> Dict[str, Any]:
        """Get current configuration"""
        return {
            "primary_model": GlobalConfig.PRIMARY_MODEL,
            "embedding_model": GlobalConfig.EMBEDDING_MODEL,
            "max_concurrent_requests": GlobalConfig.MAX_CONCURRENT_REQUESTS,
            "request_timeout": GlobalConfig.REQUEST_TIMEOUT,
            "max_retries": GlobalConfig.MAX_RETRIES,
            "output_directory": str(GlobalConfig.OUTPUT_DIR),
            "supported_rule_types": self.get_supported_rule_types(),
            "supported_data_domains": self.get_supported_data_domains(),
            "supported_data_roles": self.get_supported_data_roles()
        }

# =============================================================================
# COMPLETE EXAMPLE USAGE AND TESTING
# =============================================================================

async def main():
    """Complete example usage with proper message flow and OpenAI client"""
    
    # Initialize converter with only o3-mini-2025-01-31
    converter = CompleteLegislationRulesConverter(
        model_name=GlobalConfig.PRIMARY_MODEL,
        embedding_model=GlobalConfig.EMBEDDING_MODEL
    )
    
    # Display configuration
    config = converter.get_configuration()
    print("=== CONFIGURATION ===")
    print(SafeJSONParser.safe_json_dumps(config))
    
    # Demonstrate proper conversation flow with SystemMessage and AIMessage
    print("\n=== CONVERSATION FLOW EXAMPLE ===")
    
    # Example of how the system uses different message types
    example_conversation = [
        SystemMessage(content="You are an expert legal analyst specializing in GDPR compliance."),
        HumanMessage(content="Please analyze Article 28 of GDPR for processor obligations."),
        AIMessage(content="I'll analyze Article 28 which covers processor obligations. This article typically involves data_usage (processing activities) and data_storage (security measures) domains based on its content about technical and organizational measures."),
        HumanMessage(content="Extract specific rules with multiple conditions from this article.")
    ]
    
    print("Example message flow:")
    for i, msg in enumerate(example_conversation):
        msg_type = type(msg).__name__
        content_preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
        print(f"  {i+1}. {msg_type}: {content_preview}")
    
    # Test semantic similarity with direct OpenAI client
    print("\n=== TESTING OPENAI CLIENT DIRECTLY ===")
    client = OpenAI(api_key=GlobalConfig.OPENAI_API_KEY)
    
    test_texts = [
        "Data controllers must implement appropriate technical measures",
        "International data transfers require adequate protection",
        "Data subjects have the right to access their personal data"
    ]
    
    # Test embedding generation
    response = client.embeddings.create(
        input=test_texts,
        model=GlobalConfig.EMBEDDING_MODEL
    )
    
    print(f"✅ Successfully generated embeddings using {GlobalConfig.EMBEDDING_MODEL}")
    print(f"   Embedding dimensions: {len(response.data[0].embedding)}")
    print(f"   Total embeddings: {len(response.data)}")
    
    # Test semantic similarity search
    similarity_results = semantic_similarity_search(
        "processor obligations", 
        test_texts, 
        top_k=2
    )
    print(f"✅ Semantic similarity search completed:")
    for result in similarity_results:
        print(f"   Text: {result['text'][:50]}... | Similarity: {result['similarity']:.3f}")
    
    # Example legislation text - Article 28 of GDPR as a concrete example
    legislation_text = """
    Article 28: Processor obligations under GDPR
    
    1. Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing sufficient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject.

    2. The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.

    3. Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller.

    4. The processor shall at the choice of the controller, delete or return all the personal data to the controller after the end of the provision of services relating to processing, and delete existing copies unless Union or Member State law requires storage of the personal data.

    5. The processor shall make available to the controller all information necessary to demonstrate compliance with the obligations laid down in this Article and allow for and contribute to audits, including inspections, conducted by the controller or another auditor mandated by the controller.

    6. Without prejudice to Article 82, the processor shall be liable for damage only where it has not complied with obligations of this Regulation specifically directed to processors or where it has acted outside or contrary to lawful instructions of the controller.
    
    Article 44: General principle for transfers
    
    Any transfer of personal data which are undergoing processing or are intended for processing after transfer to a third country or to an international organisation shall take place only if, subject to the other provisions of this Regulation, the conditions laid down in this Chapter are complied with by the controller and processor, including for onward transfers of personal data from the third country or an international organisation to another third country or to another international organisation.
    
    Article 17: Right to erasure ('right to be forgotten')
    
    1. The data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies:

    a) the personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed;
    b) the data subject withdraws consent on which the processing is based according to point (a) of Article 6(1), or point (a) of Article 9(2), and where there is no other legal ground for the processing;
    c) the data subject objects to the processing pursuant to Article 21(1) and there are no overriding legitimate grounds for the processing;
    
    2. Where the controller has made the personal data public and is obliged pursuant to paragraph 1 to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform controllers which are processing the personal data that the data subject has requested the erasure by such controllers of any links to, or copy or replication of, those personal data.
    """
    
    # Convert legislation - domains will be inferred from content
    print("\n=== STARTING CONVERSION ===")
    result = await converter.convert_legislation(
        legislation_text,
        {
            "legislation_type": "gdpr_articles",
            "jurisdiction": "EU",
            "effective_date": "2018-05-25",
            "regulation_name": "General Data Protection Regulation",
            "note": "System will infer data domains from article content automatically"
        },
        output_format="dict"
    )
    
    print("\n=== CONVERSION RESULT ===")
    print(SafeJSONParser.safe_json_dumps(result))
    
    print("\n=== DOMAIN INFERENCE EXPLANATION ===")
    print("The system analyzed the GDPR articles and should have inferred:")
    print("- Article 28: data_usage (processing activities) + data_storage (security measures)")
    print("- Article 44: data_transfer (cross-border transfers to third countries)")  
    print("- Article 17: data_access (subject rights) + data_storage (erasure/deletion)")
    
    # Display rule summaries
    if isinstance(result, dict) and "extracted_rules" in result:
        print(f"\n=== EXTRACTED RULES SUMMARY ===")
        for i, rule in enumerate(result["extracted_rules"]):
            structured_rule = rule.get('structured_rule', {})
            print(f"\nRule {i+1}:")
            print(f"  Type: {rule.get('rule_type', 'Unknown')}")
            print(f"  Definition: {structured_rule.get('simple_english_definition', 'No definition')}")
            print(f"  Total Conditions: {rule.get('total_conditions', 0)}")
            print(f"  Data Domains: {structured_rule.get('applicable_domains', [])}")
            print(f"  Validation Status: {rule.get('validation_status', 'Unknown')}")
            print(f"  Confidence: {rule.get('confidence_score', 0.0):.2f}")
            
            # Show condition summaries
            condition_summaries = rule.get('condition_summary', [])
            if condition_summaries:
                print(f"  Conditions:")
                for j, condition in enumerate(condition_summaries):
                    print(f"    {j+1}. {condition}")
    
    # Validate extracted rules with safe parsing
    print(f"\n=== RULE VALIDATION ===")
    if isinstance(result, dict) and "extracted_rules" in result:
        for rule in result["extracted_rules"]:
            if "structured_rule" in rule:
                validation = converter.validate_rule_format(
                    SafeJSONParser.safe_json_dumps(rule["structured_rule"])
                )
                print(f"Rule {rule.get('rule_id', 'Unknown')[:8]}... - Valid: {validation.get('valid', False)}")
                if not validation.get('valid', False):
                    print(f"  Errors: {validation.get('errors', [])}")
    
    # Save result to file with safe JSON
    output_file = GlobalConfig.OUTPUT_DIR / "complete_conversion_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(SafeJSONParser.safe_json_dumps(result))
    
    print(f"\n=== RESULT SAVED ===")
    print(f"File: {output_file}")

if __name__ == "__main__":
    # Run the complete example with cross-platform compatibility
    asyncio.run(main())
