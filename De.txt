#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System (Rules-as-Code)
Features:
- ReAct agents with explicit reasoning and competency questions
- Rich ontology with DPV, PROV-O integration
- LangMem for long-term memory across sessions
- o3-mini-2025-01-31 with reasoning effort control
- Web interface for querying the knowledge graph
- SPARQL endpoint for semantic queries
- Comprehensive SHACL validation
- Multi-format exports (TTL, JSON-LD, XML, OWL)

Author: Claude (Anthropic) - Enhanced Rules-as-Code Implementation
Date: July 2025
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import ssl
import threading

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
from rdflib.plugins.stores.sparqlstore import SPARQLStore
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langmem import (
    create_manage_memory_tool, 
    create_search_memory_tool, 
    create_memory_manager,
    create_memory_store_manager
)

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logger.warning("Flask not available - web interface will be disabled")
    class Flask: pass
    class Response: pass
    def jsonify(*args, **kwargs): return {}
    def render_template_string(*args, **kwargs): return ""
    def CORS(*args, **kwargs): pass

# Token counting (offline)
import tiktoken
from tiktoken.load import load_tiktoken_bpe

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the Rules-as-Code system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_as_code_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    TIKTOKEN_MODELS_PATH = os.getenv("TIKTOKEN_MODELS_PATH", "./tiktoken_models")
    
    # o3-mini Model parameters
    REASONING_EFFORT = "high"  # low, medium, high for complex legal analysis
    MAX_COMPLETION_TOKENS = 8000
    
    # Processing parameters
    BATCH_SIZE = 5
    MAX_CONCURRENT = 3
    CHUNK_SIZE = 4000  # Larger chunks for legal documents
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if not cls.ELASTICSEARCH_PASSWORD:
            missing_vars.append("ELASTICSEARCH_PASSWORD")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )

# ====================================
# ENHANCED LEGAL ONTOLOGY NAMESPACES
# ====================================

class LegalRulesNamespaces:
    """Enhanced namespaces for Rules-as-Code with DPV and PROV-O integration"""
    
    # Core W3C vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal domain vocabularies
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Our enhanced Rules-as-Code namespace
    RAC = Namespace("https://rules-as-code.org/ontology#")
    
    # Data management domain-specific namespaces
    STORAGE = Namespace("https://rules-as-code.org/storage#")
    USAGE = Namespace("https://rules-as-code.org/usage#")
    MOVEMENT = Namespace("https://rules-as-code.org/movement#")
    PRIVACY = Namespace("https://rules-as-code.org/privacy#")
    SECURITY = Namespace("https://rules-as-code.org/security#")
    ACCESS = Namespace("https://rules-as-code.org/access#")
    ENTITLEMENTS = Namespace("https://rules-as-code.org/entitlements#")
    
    # Property and relation namespaces
    PROPERTIES = Namespace("https://rules-as-code.org/properties#")
    RELATIONS = Namespace("https://rules-as-code.org/relations#")
    
    # Competency question namespace
    CQ = Namespace("https://rules-as-code.org/competency-questions#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("rac", cls.RAC)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("entitlements", cls.ENTITLEMENTS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("cq", cls.CQ)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# TIKTOKEN MANAGER FOR o3-mini
# ====================================

class TiktokenManager:
    """Manage tiktoken encodings for o3-mini model"""
    
    def __init__(self, models_dir: str = "./tiktoken_models"):
        self.models_dir = Path(models_dir)
        self.encodings = {}
        self._load_encodings()
    
    def _load_encodings(self):
        """Load tiktoken encodings"""
        try:
            # o3-mini uses o200k_base encoding
            self.encoding = tiktoken.get_encoding("o200k_base")
            logger.info("Loaded o200k_base encoding for o3-mini")
        except Exception as e:
            logger.warning(f"Failed to load tiktoken encoding: {e}")
            # Fallback to cl100k_base
            try:
                self.encoding = tiktoken.get_encoding("cl100k_base")
                logger.info("Using cl100k_base as fallback encoding")
            except:
                self.encoding = None
                logger.error("No tiktoken encoding available")
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        if not self.encoding:
            return len(text) // 4  # Rough estimate
        return len(self.encoding.encode(text))
    
    def truncate_text(self, text: str, max_tokens: int) -> str:
        """Truncate text to fit within token limit"""
        if not self.encoding:
            # Character-based fallback
            return text[:max_tokens * 4]
        
        tokens = self.encoding.encode(text)
        if len(tokens) <= max_tokens:
            return text
        
        truncated_tokens = tokens[:max_tokens]
        return self.encoding.decode(truncated_tokens)

# ====================================
# OPENAI CLIENT FOR o3-mini
# ====================================

class OpenAIClient:
    """Enhanced OpenAI client for o3-mini with reasoning effort control"""
    
    def __init__(self):
        Config.validate_config()
        
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        self.tiktoken_manager = TiktokenManager(Config.TIKTOKEN_MODELS_PATH)
        
        # Test the client
        try:
            test_response = self.client.embeddings.create(
                input=["test"],
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            logger.info(f"OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConnectionError(f"OpenAI API connection failed: {e}")
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        logger.info(f"Generating embeddings for {len(texts)} text(s)...")
        
        try:
            # Truncate texts if too long
            truncated_texts = []
            for text in texts:
                token_count = self.tiktoken_manager.count_tokens(text)
                if token_count > 8000:  # Conservative limit for embeddings
                    truncated_text = self.tiktoken_manager.truncate_text(text, 8000)
                    truncated_texts.append(truncated_text)
                else:
                    truncated_texts.append(text)
            
            response = self.client.embeddings.create(
                input=truncated_texts,
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Generate chat completion with o3-mini and reasoning effort"""
        logger.info(f"Starting chat completion with model: {Config.OPENAI_MODEL}")
        
        try:
            # Prepare messages for o3-mini
            prepared_messages = self._prepare_messages_for_o3_mini(messages)
            
            # Set reasoning effort based on complexity
            reasoning_effort = kwargs.get('reasoning_effort', Config.REASONING_EFFORT)
            
            response = self.client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=prepared_messages,
                reasoning_effort=reasoning_effort,
                max_completion_tokens=kwargs.get('max_completion_tokens', Config.MAX_COMPLETION_TOKENS)
            )
            
            content = response.choices[0].message.content
            logger.info(f"Received response with {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            raise
    
    def _prepare_messages_for_o3_mini(self, messages: List[Dict]) -> List[Dict]:
        """Prepare messages for o3-mini model with token management"""
        
        # Convert system messages to developer messages for o3-mini
        prepared_messages = []
        for msg in messages:
            if msg.get('role') == 'system':
                prepared_messages.append({
                    'role': 'developer',  # o3-mini uses developer role instead of system
                    'content': msg.get('content', '')
                })
            else:
                prepared_messages.append(msg)
        
        # Check token count and truncate if necessary
        total_tokens = sum(self.tiktoken_manager.count_tokens(msg.get('content', '')) 
                          for msg in prepared_messages)
        
        if total_tokens > 150000:  # Conservative limit for o3-mini
            # Truncate the longest message
            longest_msg_idx = max(range(len(prepared_messages)), 
                                key=lambda i: len(prepared_messages[i].get('content', '')))
            
            original_content = prepared_messages[longest_msg_idx]['content']
            truncated_content = self.tiktoken_manager.truncate_text(original_content, 100000)
            prepared_messages[longest_msg_idx]['content'] = truncated_content + "\n\n[Content truncated due to length]"
            
            logger.warning("Messages truncated due to token limit")
        
        return prepared_messages

# ====================================
# LANGMEM MEMORY MANAGER
# ====================================

class LegalMemoryManager:
    """Enhanced memory manager for legal knowledge with LangMem"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.openai_client = openai_client
        self.store = None
        self.memory_manager = None
        self._initialize_store()
    
    def _initialize_store(self):
        """Initialize memory store (InMemoryStore for development and testing)"""
        try:
            # Use in-memory store with vector index for embeddings
            self.store = InMemoryStore(
                index={
                    "dims": 3072,  # text-embedding-3-large dimensions
                    "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
                }
            )
            logger.info("Using in-memory store for LangMem (suitable for development and testing)")
            
            # Create memory manager for legal concepts
            from pydantic import BaseModel, Field
            
            class LegalConcept(BaseModel):
                """Schema for legal concepts in memory"""
                concept_name: str = Field(..., description="Name of the legal concept")
                concept_type: str = Field(..., description="Type: rule|definition|entity|relationship")
                jurisdiction: str = Field(..., description="Legal jurisdiction")
                domain: str = Field(..., description="Data management domain")
                description: str = Field(..., description="Description of the concept")
                relationships: List[str] = Field(default=[], description="Related concepts")
            
            class CompetencyQuestion(BaseModel):
                """Schema for competency questions"""
                question: str = Field(..., description="The competency question")
                question_type: str = Field(..., description="Type: scoping|validating|foundational|relationship|metaproperty")
                domain: str = Field(..., description="Data management domain")
                answer: Optional[str] = Field(None, description="Answer to the question")
                sparql_query: Optional[str] = Field(None, description="SPARQL query for this question")
            
            self.memory_manager = create_memory_store_manager(
                f"openai:{Config.OPENAI_MODEL}",
                namespace=("legal_knowledge", "{jurisdiction}", "{organization}"),
                schemas=[LegalConcept, CompetencyQuestion],
                instructions="""Extract and store important legal concepts, rules, definitions, and competency questions from legal documents. 
                Focus on:
                1. Legal rules (obligations, permissions, prohibitions)
                2. Legal entities and their relationships
                3. Data management domains (storage, usage, movement, privacy, security, access, entitlements)
                4. Competency questions that can validate the ontology
                5. Cross-jurisdictional relationships and adequacy determinations
                """,
                store=self.store,
                enable_inserts=True,
                enable_updates=True,
                enable_deletes=False
            )
            
        except Exception as e:
            logger.error(f"Failed to initialize memory manager: {e}")
            # Use basic in-memory store as fallback
            self.store = InMemoryStore()
    
    def get_memory_tools(self, namespace_params: Dict[str, str]):
        """Get memory tools for agents"""
        namespace = ("legal_knowledge", namespace_params.get("jurisdiction", "default"), 
                    namespace_params.get("organization", "default"))
        
        manage_tool = create_manage_memory_tool(namespace=namespace)
        search_tool = create_search_memory_tool(namespace=namespace)
        
        return [manage_tool, search_tool]

# ====================================
# COMPETENCY QUESTION GENERATOR
# ====================================

class CompetencyQuestionAgent:
    """ReAct agent for generating and managing competency questions"""
    
    def __init__(self, openai_client: OpenAIClient, memory_manager: LegalMemoryManager):
        self.openai_client = openai_client
        self.memory_manager = memory_manager
        self.reasoning_steps = []
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "CompetencyQuestionAgent"
        })
        logger.info(f"[CQ Agent] REASONING: {thought}")
    
    async def generate_competency_questions(self, extraction_result: Dict, context: Dict) -> List[Dict]:
        """Generate competency questions for ontology validation"""
        
        self.log_reasoning("Starting competency question generation for legal ontology validation")
        self.log_reasoning(f"Context: {context['country']}/{context['jurisdiction']}")
        
        system_prompt = """You are an expert in ontology engineering and legal knowledge representation. Your task is to generate comprehensive competency questions for validating a legal Rules-as-Code ontology.

COMPETENCY QUESTION TYPES:

1. SCOPING QUESTIONS (SCQ): Define what the ontology should cover
   - "What legal entities are involved in data processing?"
   - "What types of personal data are regulated?"
   - "What are the different legal bases for processing?"

2. VALIDATING QUESTIONS (VCQ): Test if the ontology meets requirements
   - "Can the ontology determine if processing is lawful?"
   - "Does the ontology identify all data subject rights?"
   - "Can the ontology validate consent requirements?"

3. FOUNDATIONAL QUESTIONS (FCQ): Test core concepts
   - "What constitutes personal data under this jurisdiction?"
   - "Who can be a data controller?"
   - "What are the essential elements of consent?"

4. RELATIONSHIP QUESTIONS (RCQ): Test connections between concepts
   - "How are data controllers related to processors?"
   - "What is the relationship between purpose and legal basis?"
   - "How do technical measures relate to security requirements?"

5. METAPROPERTY QUESTIONS (MpCQ): Test ontology structure
   - "How many subclasses does PersonalData have?"
   - "What properties can be applied to ProcessingActivity?"
   - "Which concepts are mandatory vs optional?"

DOMAIN-SPECIFIC FOCUS AREAS:
- Storage: data retention, archiving, deletion
- Usage: purpose limitation, data minimization
- Movement: cross-border transfers, adequacy decisions
- Privacy: consent, legitimate interests, data subject rights
- Security: technical measures, organizational measures
- Access: authorization, authentication, audit trails
- Entitlements: roles, permissions, data access rights

Generate questions that can be answered by querying the ontology and that validate the completeness and correctness of the legal knowledge representation.

CRITICAL: Respond ONLY with valid JSON array of competency questions.

Output format:
[
  {
    "question": "What are the adequacy countries for international data transfers?",
    "type": "foundational",
    "domain": "movement",
    "rationale": "Tests if the ontology includes essential adequacy decisions",
    "expected_answer_type": "list of countries",
    "sparql_pattern": "SELECT ?country WHERE { ?country a rac:AdequacyCountry }"
  }
]"""

        user_prompt = f"""
Based on the following legal rule extraction and context, generate comprehensive competency questions:

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}

EXTRACTED RULES AND CONCEPTS:
{json.dumps(extraction_result, indent=2)}

Generate 25-30 competency questions covering all five types (SCQ, VCQ, FCQ, RCQ, MpCQ) and all data management domains. Include specific questions relevant to {context.get('country', 'the jurisdiction')} legal requirements.

Focus on questions that would be asked by:
1. Compliance officers validating data processing
2. Legal teams assessing regulatory requirements  
3. Data protection officers ensuring GDPR compliance
4. System architects implementing privacy by design
5. Auditors reviewing data governance
"""

        self.log_reasoning("Sending competency question generation request to o3-mini")
        response = await self.openai_client.chat_completion([
            {"role": "developer", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], reasoning_effort="high")
        
        # Parse JSON response
        response_text = response.strip()
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        try:
            questions = json.loads(response_text)
            self.log_reasoning(f"Successfully generated {len(questions)} competency questions")
            return questions
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Returning default questions.")
            return self._get_default_competency_questions(context)
    
    def _get_default_competency_questions(self, context: Dict) -> List[Dict]:
        """Generate default competency questions if LLM fails"""
        return [
            {
                "question": f"What are the main legal entities involved in data processing under {context.get('country', 'this jurisdiction')}?",
                "type": "scoping",
                "domain": "governance",
                "rationale": "Tests coverage of legal entity types",
                "expected_answer_type": "list of entity types",
                "sparql_pattern": "SELECT DISTINCT ?entity WHERE { ?entity rdfs:subClassOf rac:LegalEntity }"
            },
            {
                "question": "What types of personal data are explicitly regulated?",
                "type": "foundational", 
                "domain": "privacy",
                "rationale": "Tests personal data taxonomy completeness",
                "expected_answer_type": "taxonomy of data types",
                "sparql_pattern": "SELECT ?dataType WHERE { ?dataType rdfs:subClassOf dpv:PersonalData }"
            },
            {
                "question": "Can the ontology determine if a processing activity has a valid legal basis?",
                "type": "validating",
                "domain": "usage",
                "rationale": "Tests rule compliance validation capability",
                "expected_answer_type": "boolean with reasoning",
                "sparql_pattern": "ASK { ?activity rac:hasLegalBasis ?basis }"
            }
        ]

# ====================================
# ENHANCED RULE EXTRACTION AGENT
# ====================================

class EnhancedRuleExtractionAgent:
    """Enhanced ReAct agent for comprehensive legal rule extraction"""
    
    def __init__(self, openai_client: OpenAIClient, memory_manager: LegalMemoryManager):
        self.openai_client = openai_client
        self.memory_manager = memory_manager
        self.reasoning_steps = []
        self.action_steps = []
    
    def log_reasoning(self, thought: str):
        """Log a reasoning step"""
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "EnhancedRuleExtractor"
        })
        logger.info(f"[Enhanced RuleExtractor] REASONING: {thought}")
    
    def log_action(self, action: str, result: Any = None):
        """Log an action step"""
        self.action_steps.append({
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "result": str(result)[:200] if result else None,
            "agent": "EnhancedRuleExtractor"
        })
        logger.info(f"[Enhanced RuleExtractor] ACTION: {action}")
    
    async def extract_comprehensive_rules(self, text: str, context: Dict) -> Dict:
        """Extract comprehensive legal rules with DPV and domain alignment"""
        
        self.log_reasoning("Starting comprehensive Rules-as-Code extraction")
        self.log_reasoning(f"Target jurisdiction: {context.get('country', 'Unknown')}")
        self.log_reasoning("Will extract rules, map to DPV concepts, and identify data management domains")
        
        system_prompt = """You are a specialized Rules-as-Code expert converting legislation into machine-readable formats. Your task is to extract comprehensive legal rules and map them to established vocabularies (DPV, PROV-O) and data management domains.

EXTRACTION REQUIREMENTS:

1. LEGAL RULES - Extract with precise semantics:
   - Type: obligation|permission|prohibition|condition|exception
   - Subject (legal entity: data_controller|data_processor|data_subject|supervisory_authority|organization)
   - Predicate (action with DPV mapping: collect|store|use|share|transfer|delete|anonymize)
   - Object (data or system target)
   - Modality (must|may|shall|shall_not|should|could)
   - Conditions (temporal, spatial, circumstantial)
   - Legal_basis (DPV: consent|contract|legal_obligation|vital_interests|public_task|legitimate_interests)
   - Data_management_domain (storage|usage|movement|privacy|security|access|entitlements)

2. ADEQUACY AND THIRD COUNTRY ANALYSIS:
   - Identify adequacy decisions and approved countries
   - Extract third country transfer requirements
   - Map international data transfer rules
   - Identify safeguards and derogations

3. DPV CONCEPT MAPPING - Map to Data Privacy Vocabulary:
   - dpv:PersonalData and subcategories
   - dpv:SensitivePersonalData and special categories
   - dpv:ProcessingActivity and purposes
   - dpv:TechnicalOrganisationalMeasure
   - dpv:DataTransfer and cross-border aspects

4. DOMAIN CLASSIFICATION:
   - STORAGE: retention, archiving, deletion, backup
   - USAGE: purpose limitation, data minimization, profiling
   - MOVEMENT: transfers, cross-border, adequacy, safeguards
   - PRIVACY: consent, transparency, data subject rights
   - SECURITY: encryption, access controls, breach notification
   - ACCESS: authorization, authentication, audit
   - ENTITLEMENTS: roles, permissions, data access rights

5. COMPETENCY QUESTION TRIGGERS:
   - Identify concepts that need validation questions
   - Flag complex relationships requiring testing
   - Note adequacy decisions requiring factual updates

CRITICAL: Respond ONLY with valid JSON. Map concepts to established vocabularies.

Output format:
{
  "rules": [
    {
      "id": "rule_001",
      "type": "obligation",
      "subject": {
        "entity": "Data Controller",
        "dpv_mapping": "dpv:DataController",
        "organization_type": "any"
      },
      "predicate": {
        "action": "obtain_consent",
        "dpv_mapping": "dpv:ObtainConsent",
        "processing_category": "dpv:Collect"
      },
      "object": {
        "entity": "Personal Data",
        "dpv_mapping": "dpv:PersonalData",
        "data_categories": ["identifying", "contact"]
      },
      "modality": {
        "strength": "must",
        "legal_basis": "dpv:Consent"
      },
      "conditions": [
        {
          "type": "temporal",
          "description": "before processing begins",
          "dpv_mapping": "dpv:BeforeProcessing"
        }
      ],
      "data_management_domain": "privacy",
      "original_text": "Controllers must obtain consent before processing personal data",
      "confidence": 0.95,
      "adequacy_related": false
    }
  ],
  "adequacy_decisions": [
    {
      "country": "country_name",
      "status": "adequate|partially_adequate|not_adequate",
      "decision_date": "YYYY-MM-DD",
      "legal_basis": "adequacy_decision_reference",
      "restrictions": ["any restrictions"]
    }
  ],
  "dpv_concept_mappings": [
    {
      "extracted_concept": "Personal Information",
      "dpv_concept": "dpv:PersonalData",
      "relationship": "exact_match|broader|narrower",
      "confidence": 0.9
    }
  ],
  "domain_analysis": {
    "storage": {
      "rule_count": 5,
      "key_concepts": ["retention", "deletion"],
      "dpv_mappings": ["dpv:StorageCondition", "dpv:DataRetention"]
    },
    "usage": {
      "rule_count": 8,
      "key_concepts": ["purpose_limitation", "data_minimization"],
      "dpv_mappings": ["dpv:Purpose", "dpv:DataMinimisation"]
    },
    "movement": {
      "rule_count": 3,
      "key_concepts": ["cross_border_transfer", "adequacy"],
      "dpv_mappings": ["dpv:CrossBorderDataTransfer", "dpv:AdequacyDecision"]
    },
    "privacy": {
      "rule_count": 12,
      "key_concepts": ["consent", "data_subject_rights"],
      "dpv_mappings": ["dpv:Consent", "dpv:DataSubjectRight"]
    },
    "security": {
      "rule_count": 6,
      "key_concepts": ["encryption", "access_control"],
      "dpv_mappings": ["dpv:Encryption", "dpv:AccessControl"]
    },
    "access": {
      "rule_count": 4,
      "key_concepts": ["authorization", "audit"],
      "dpv_mappings": ["dpv:RightOfAccess", "dpv:AuditLog"]
    },
    "entitlements": {
      "rule_count": 2,
      "key_concepts": ["data_access_rights", "role_based_access"],
      "dpv_mappings": ["dpv:DataSubjectRight", "dpv:AccessControl"]
    }
  },
  "competency_question_triggers": [
    {
      "concept": "adequacy_countries",
      "question_type": "factual_validation",
      "suggested_question": "What are the current adequacy countries for data transfer?",
      "requires_update": true
    }
  ]
}"""

        user_prompt = f"""
Analyze the following legal text and extract comprehensive Rules-as-Code with DPV mapping and domain classification:

CONTEXT:
- Country/Jurisdiction: {context.get('country', 'Unknown')} / {context.get('jurisdiction', 'Unknown')}
- Organization: {context.get('organization', 'Unknown')}
- Legal System: {context.get('legal_system', 'Civil Law')}

LEGAL TEXT:
{text}

Extract all rules, map to DPV concepts, classify by data management domains, identify adequacy decisions, and flag concepts requiring competency questions. Focus on creating Rules-as-Code suitable for automated compliance checking.
"""

        self.log_action("Sending comprehensive Rules-as-Code extraction request to o3-mini")
        response = await self.openai_client.chat_completion([
            {"role": "developer", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], reasoning_effort="high")
        
        self.log_action("Processing response and storing in long-term memory")
        
        # Parse JSON response
        response_text = response.strip()
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        try:
            result = json.loads(response_text)
            self.log_action(f"Successfully extracted {len(result.get('rules', []))} rules")
            
            # Store in long-term memory using LangMem
            await self._store_in_memory(result, context)
            
            return result
        except json.JSONDecodeError as e:
            self.log_reasoning(f"JSON parsing failed: {e}. Returning basic structure.")
            return {
                "rules": [],
                "adequacy_decisions": [],
                "dpv_concept_mappings": [],
                "domain_analysis": {},
                "competency_question_triggers": []
            }
    
    async def _store_in_memory(self, extraction_result: Dict, context: Dict):
        """Store extraction results in long-term memory"""
        try:
            namespace_params = {
                "jurisdiction": context.get('jurisdiction', 'default'),
                "organization": context.get('organization', 'default')
            }
            
            # Store key concepts and rules in memory for future reference
            if self.memory_manager.memory_manager:
                memory_data = {
                    "messages": [
                        {
                            "role": "user",
                            "content": f"Extracted {len(extraction_result.get('rules', []))} legal rules from {context.get('country', 'unknown')} legislation"
                        }
                    ]
                }
                
                config = {"configurable": namespace_params}
                await self.memory_manager.memory_manager.ainvoke(memory_data, config=config)
                
                self.log_action("Stored extraction results in long-term memory")
        except Exception as e:
            self.log_reasoning(f"Failed to store in memory: {e}")

# ====================================
# ENHANCED ONTOLOGY BUILDER
# ====================================

class EnhancedOntologyBuilder:
    """Enhanced ontology builder with DPV/PROV-O integration and competency questions"""
    
    def __init__(self):
        self.ns = LegalRulesNamespaces()
        self.reasoning_steps = []
    
    def log_reasoning(self, thought: str):
        self.reasoning_steps.append({
            "timestamp": datetime.now().isoformat(),
            "thought": thought,
            "agent": "EnhancedOntologyBuilder"
        })
        logger.info(f"[Enhanced OntologyBuilder] REASONING: {thought}")
    
    def build_comprehensive_ontology(self, extraction_result: Dict, competency_questions: List[Dict], context: Dict) -> Tuple[Graph, Graph]:
        """Build comprehensive OWL ontology and TTL knowledge graph"""
        
        self.log_reasoning("Building comprehensive Rules-as-Code ontology with DPV integration")
        
        # Create OWL ontology (structure/schema)
        owl_ontology = Graph()
        owl_ontology = self.ns.bind_to_graph(owl_ontology)
        
        # Create TTL knowledge graph (instances/data)
        ttl_graph = Graph()
        ttl_graph = self.ns.bind_to_graph(ttl_graph)
        
        # Build ontology structure
        self._build_ontology_structure(owl_ontology, context)
        self._integrate_dpv_concepts(owl_ontology)
        self._add_data_management_domains(owl_ontology)
        self._add_competency_question_concepts(owl_ontology, competency_questions)
        
        # Populate knowledge graph with instances
        self._populate_knowledge_graph(ttl_graph, extraction_result, competency_questions, context)
        
        self.log_reasoning(f"Built ontology with {len(owl_ontology)} triples and knowledge graph with {len(ttl_graph)} triples")
        
        return owl_ontology, ttl_graph
    
    def _build_ontology_structure(self, graph: Graph, context: Dict):
        """Build the core ontology structure"""
        
        # Ontology metadata
        ontology_uri = URIRef(f"{self.ns.RAC}ontology")
        graph.add((ontology_uri, RDF.type, OWL.Ontology))
        graph.add((ontology_uri, DCTERMS.title, 
                  Literal(f"Rules-as-Code Ontology for {context.get('country', 'Legal Domain')}")))
        graph.add((ontology_uri, DCTERMS.created, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        graph.add((ontology_uri, RDFS.comment, 
                  Literal("Machine-readable legal rules ontology with DPV integration")))
        
        # Import DPV and PROV-O
        graph.add((ontology_uri, OWL.imports, URIRef("https://w3id.org/dpv")))
        graph.add((ontology_uri, OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        
        # Core classes
        core_classes = [
            ("LegalRule", "A machine-readable legal rule"),
            ("LegalEntity", "An entity with legal standing"),
            ("LegalObligation", "A legal requirement or duty"),
            ("LegalPermission", "A legal allowance or right"),
            ("LegalProhibition", "A legal restriction or ban"),
            ("DataManagementDomain", "A domain of data management activity"),
            ("AdequacyDecision", "A decision on adequacy for data transfers"),
            ("CompetencyQuestion", "A question for validating the ontology"),
            ("Country", "A nation state with legal jurisdiction"),
            ("Organization", "A legal organization or entity"),
            ("Jurisdiction", "A legal administrative area")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.RAC}{class_name}")
            graph.add((class_uri, RDF.type, OWL.Class))
            graph.add((class_uri, RDFS.label, Literal(class_name)))
            graph.add((class_uri, RDFS.comment, Literal(definition)))
    
    def _integrate_dpv_concepts(self, graph: Graph):
        """Integrate DPV concepts into the ontology"""
        
        # Map our concepts to DPV
        dpv_mappings = [
            ("LegalEntity", "dpv:LegalEntity"),
            ("PersonalDataProcessing", "dpv:PersonalDataProcessing"),
            ("DataController", "dpv:DataController"),
            ("DataProcessor", "dpv:DataProcessor"),
            ("DataSubject", "dpv:DataSubject"),
            ("LegalBasis", "dpv:LegalBasis"),
            ("Consent", "dpv:Consent"),
            ("LegitimateInterest", "dpv:LegitimateInterest"),
            ("TechnicalMeasure", "dpv:TechnicalMeasure"),
            ("OrganisationalMeasure", "dpv:OrganisationalMeasure")
        ]
        
        for local_concept, dpv_concept in dpv_mappings:
            local_uri = URIRef(f"{self.ns.RAC}{local_concept}")
            dpv_uri = URIRef(f"{self.ns.DPV}{dpv_concept.split(':')[1]}")
            
            graph.add((local_uri, RDF.type, OWL.Class))
            graph.add((local_uri, RDFS.subClassOf, dpv_uri))
            graph.add((local_uri, SKOS.exactMatch, dpv_uri))
    
    def _add_data_management_domains(self, graph: Graph):
        """Add data management domain concepts"""
        
        domains = [
            ("StorageDomain", "Data storage, retention, and archiving"),
            ("UsageDomain", "Data usage, purpose limitation, and processing"),
            ("MovementDomain", "Data transfers, cross-border movement, and adequacy"),
            ("PrivacyDomain", "Privacy rights, consent, and data subject rights"),
            ("SecurityDomain", "Data security, encryption, and access controls"),
            ("AccessDomain", "Data access, authorization, and authentication"),
            ("EntitlementsDomain", "Data entitlements, roles, and permissions")
        ]
        
        for domain_name, definition in domains:
            domain_uri = URIRef(f"{self.ns.RAC}{domain_name}")
            graph.add((domain_uri, RDF.type, OWL.Class))
            graph.add((domain_uri, RDFS.subClassOf, URIRef(f"{self.ns.RAC}DataManagementDomain")))
            graph.add((domain_uri, RDFS.label, Literal(domain_name)))
            graph.add((domain_uri, RDFS.comment, Literal(definition)))
    
    def _add_competency_question_concepts(self, graph: Graph, competency_questions: List[Dict]):
        """Add competency question types as classes"""
        
        cq_types = ["ScopingQuestion", "ValidatingQuestion", "FoundationalQuestion", 
                   "RelationshipQuestion", "MetapropertyQuestion"]
        
        for cq_type in cq_types:
            cq_uri = URIRef(f"{self.ns.CQ}{cq_type}")
            graph.add((cq_uri, RDF.type, OWL.Class))
            graph.add((cq_uri, RDFS.subClassOf, URIRef(f"{self.ns.RAC}CompetencyQuestion")))
            graph.add((cq_uri, RDFS.label, Literal(cq_type)))
    
    def _populate_knowledge_graph(self, graph: Graph, extraction_result: Dict, 
                                competency_questions: List[Dict], context: Dict):
        """Populate the knowledge graph with extracted instances"""
        
        # Add country, jurisdiction, organization as individuals
        country_uri = URIRef(f"{self.ns.RAC}Country_{self._safe_uri_encode(context['country'])}")
        graph.add((country_uri, RDF.type, URIRef(f"{self.ns.RAC}Country")))
        graph.add((country_uri, RDFS.label, Literal(context['country'])))
        
        jurisdiction_uri = URIRef(f"{self.ns.RAC}Jurisdiction_{self._safe_uri_encode(context['jurisdiction'])}")
        graph.add((jurisdiction_uri, RDF.type, URIRef(f"{self.ns.RAC}Jurisdiction")))
        graph.add((jurisdiction_uri, RDFS.label, Literal(context['jurisdiction'])))
        
        org_uri = URIRef(f"{self.ns.RAC}Organization_{self._safe_uri_encode(context['organization'])}")
        graph.add((org_uri, RDF.type, URIRef(f"{self.ns.RAC}Organization")))
        graph.add((org_uri, RDFS.label, Literal(context['organization'])))
        
        # Add extracted rules as individuals
        for rule in extraction_result.get('rules', []):
            self._add_rule_individual(graph, rule, country_uri)
        
        # Add adequacy decisions
        for adequacy in extraction_result.get('adequacy_decisions', []):
            self._add_adequacy_decision(graph, adequacy, country_uri)
        
        # Add competency questions
        for i, cq in enumerate(competency_questions):
            self._add_competency_question(graph, cq, i, country_uri)
    
    def _add_rule_individual(self, graph: Graph, rule: Dict, country_uri: URIRef):
        """Add a legal rule as an individual"""
        
        rule_id = rule.get('id', str(uuid.uuid4()))
        rule_uri = URIRef(f"{self.ns.RAC}Rule_{rule_id}")
        
        graph.add((rule_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalRule")))
        graph.add((rule_uri, RDFS.label, Literal(f"Legal Rule {rule_id}")))
        
        # Add rule properties
        if rule.get('type'):
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasRuleType"), Literal(rule['type'])))
        
        if rule.get('original_text'):
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasOriginalText"), Literal(rule['original_text'])))
        
        if rule.get('data_management_domain'):
            domain_uri = URIRef(f"{self.ns.RAC}{rule['data_management_domain'].title()}Domain")
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}belongsToDomain"), domain_uri))
        
        # Link to country
        graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}appliesInCountry"), country_uri))
        
        # Add provenance
        graph.add((rule_uri, self.ns.PROV.wasGeneratedBy, 
                  URIRef(f"{self.ns.RAC}RulesAsCodeExtraction")))
        graph.add((rule_uri, self.ns.PROV.generatedAtTime, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_adequacy_decision(self, graph: Graph, adequacy: Dict, country_uri: URIRef):
        """Add adequacy decision as individual"""
        
        country_name = adequacy.get('country', '').replace(' ', '_')
        adequacy_uri = URIRef(f"{self.ns.RAC}AdequacyDecision_{country_name}")
        
        graph.add((adequacy_uri, RDF.type, URIRef(f"{self.ns.RAC}AdequacyDecision")))
        graph.add((adequacy_uri, RDFS.label, Literal(f"Adequacy Decision for {adequacy.get('country', 'Unknown')}")))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasAdequacyStatus"), Literal(adequacy.get('status', 'unknown'))))
        
        if adequacy.get('decision_date'):
            graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasDecisionDate"), 
                      Literal(adequacy['decision_date'], datatype=XSD.date)))
    
    def _add_competency_question(self, graph: Graph, cq: Dict, index: int, country_uri: URIRef):
        """Add competency question as individual"""
        
        cq_uri = URIRef(f"{self.ns.CQ}Question_{index}")
        cq_type_uri = URIRef(f"{self.ns.CQ}{cq.get('type', 'foundational').title()}Question")
        
        graph.add((cq_uri, RDF.type, cq_type_uri))
        graph.add((cq_uri, RDFS.label, Literal(f"Competency Question {index}")))
        graph.add((cq_uri, URIRef(f"{self.ns.PROPERTIES}hasQuestionText"), Literal(cq.get('question', ''))))
        
        if cq.get('domain'):
            graph.add((cq_uri, URIRef(f"{self.ns.PROPERTIES}testsDomain"), Literal(cq['domain'])))
        
        if cq.get('sparql_pattern'):
            graph.add((cq_uri, URIRef(f"{self.ns.PROPERTIES}hasSPARQLPattern"), Literal(cq['sparql_pattern'])))
        
        graph.add((cq_uri, URIRef(f"{self.ns.PROPERTIES}appliesInCountry"), country_uri))
    
    def _safe_uri_encode(self, text: str) -> str:
        """Safely encode text for use in URIs"""
        import urllib.parse
        safe_text = text.replace(' ', '_').replace('/', '_').replace('\\', '_')
        return urllib.parse.quote(safe_text, safe='')

# ====================================
# MAIN ORCHESTRATOR
# ====================================

class RulesAsCodeOrchestrator:
    """Main orchestrator for the Rules-as-Code system"""
    
    def __init__(self):
        # Initialize core components
        self.openai_client = self._initialize_openai_client()
        self.es_client = self._initialize_elasticsearch_client()
        self.memory_manager = LegalMemoryManager(self.openai_client)
        
        # Initialize agents
        self.rule_agent = EnhancedRuleExtractionAgent(self.openai_client, self.memory_manager)
        self.cq_agent = CompetencyQuestionAgent(self.openai_client, self.memory_manager)
        self.ontology_builder = EnhancedOntologyBuilder()
        
        # Initialize document processor
        self.doc_processor = DocumentProcessor()
        
        # Initialize SHACL validator
        self.shacl_validator = SHACLValidator()
        
        # Initialize query interface
        self.query_interface = None
        if FLASK_AVAILABLE:
            self.query_interface = LegalKnowledgeQueryInterface(self)
    
    def _initialize_openai_client(self):
        """Initialize OpenAI client"""
        try:
            return OpenAIClient()
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    def _initialize_elasticsearch_client(self):
        """Initialize Elasticsearch client"""
        try:
            client = ElasticsearchClient()
            client.create_index()
            return client
        except Exception as e:
            logger.error(f"Failed to initialize Elasticsearch client: {e}")
            raise
    
    async def process_legal_document(self, document_path: str, metadata: Dict) -> Dict:
        """Process a legal document into Rules-as-Code"""
        
        logger.info(f"Processing legal document: {document_path}")
        logger.info(f"Jurisdiction: {metadata.get('country', 'Unknown')}/{metadata.get('jurisdiction', 'Unknown')}")
        
        try:
            # Extract text from PDF
            text_content = self.doc_processor.extract_text_from_pdf(document_path)
            logger.info(f"Extracted {len(text_content)} characters from document")
            
            # Enhanced rule extraction with DPV mapping
            extraction_result = await self.rule_agent.extract_comprehensive_rules(text_content, metadata)
            
            # Generate competency questions
            competency_questions = await self.cq_agent.generate_competency_questions(extraction_result, metadata)
            
            # Build ontology and knowledge graph
            owl_ontology, ttl_graph = self.ontology_builder.build_comprehensive_ontology(
                extraction_result, competency_questions, metadata
            )
            
            # Validate ontology with SHACL
            validation_results = self._validate_ontology(owl_ontology)
            
            # Store in Elasticsearch for search
            await self._store_in_elasticsearch(extraction_result, competency_questions, metadata, ttl_graph)
            
            # Export in multiple formats
            exports = self._export_ontologies(owl_ontology, ttl_graph, metadata)
            
            # Setup query interface
            if self.query_interface:
                self.query_interface.load_ontology(ttl_graph)
            
            return {
                "success": True,
                "extraction_stats": {
                    "rules_extracted": len(extraction_result.get("rules", [])),
                    "adequacy_decisions": len(extraction_result.get("adequacy_decisions", [])),
                    "competency_questions_generated": len(competency_questions),
                    "dpv_mappings": len(extraction_result.get("dpv_concept_mappings", [])),
                    "domain_coverage": list(extraction_result.get("domain_analysis", {}).keys())
                },
                "validation_results": validation_results,
                "exports": exports,
                "query_interface_ready": self.query_interface is not None
            }
            
        except Exception as e:
            logger.error(f"Failed to process document: {e}")
            return {"success": False, "error": str(e)}
    
    def _validate_ontology(self, ontology: Graph) -> Dict:
        """Validate ontology using SHACL"""
        try:
            conforms, results_graph, results_text = pyshacl.validate(
                data_graph=ontology,
                inference='rdfs',
                serialize_report_graph=True
            )
            
            return {
                "conforms": conforms,
                "validation_performed": True,
                "results_text": results_text if not conforms else "Validation passed",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"SHACL validation failed: {e}")
            return {
                "conforms": False,
                "validation_performed": False,
                "error": str(e)
            }
    
    async def _store_in_elasticsearch(self, extraction_result: Dict, competency_questions: List[Dict], 
                                    metadata: Dict, knowledge_graph: Graph):
        """Store results in Elasticsearch for search and retrieval"""
        
        # Generate embeddings for search
        text_content = " ".join([rule.get('original_text', '') for rule in extraction_result.get('rules', [])])
        if text_content:
            embeddings = await self.openai_client.generate_embeddings([text_content])
            content_vector = embeddings[0] if embeddings else []
        else:
            content_vector = []
        
        # Prepare document for Elasticsearch
        doc = {
            "document_id": str(uuid.uuid4()),
            "country": metadata.get("country", "Unknown"),
            "jurisdiction": metadata.get("jurisdiction", "Unknown"),
            "organization": metadata.get("organization", "Unknown"),
            "title": f"Rules-as-Code for {metadata.get('country', 'Unknown')}",
            
            # Rules-as-Code specific fields
            "rules": extraction_result.get("rules", []),
            "adequacy_decisions": extraction_result.get("adequacy_decisions", []),
            "dpv_concept_mappings": extraction_result.get("dpv_concept_mappings", []),
            "domain_analysis": extraction_result.get("domain_analysis", {}),
            "competency_questions": competency_questions,
            
            # Search fields
            "concepts": self._extract_concepts_for_search(extraction_result),
            "domains": list(extraction_result.get("domain_analysis", {}).keys()),
            "rule_types": list(set([rule.get('type', '') for rule in extraction_result.get('rules', [])])),
            
            # Vector for semantic search
            "content_vector": content_vector,
            
            # Metadata
            "processing_timestamp": datetime.now().isoformat(),
            "ontology_triples_count": len(knowledge_graph),
            "system_version": "rules-as-code-v2.0"
        }
        
        # Store in Elasticsearch
        try:
            self.es_client.client.index(
                index=Config.ELASTICSEARCH_INDEX,
                body=doc
            )
            logger.info("Successfully stored in Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to store in Elasticsearch: {e}")
    
    def _extract_concepts_for_search(self, extraction_result: Dict) -> List[str]:
        """Extract concepts for search indexing"""
        concepts = []
        
        # From rules
        for rule in extraction_result.get('rules', []):
            if rule.get('subject', {}).get('entity'):
                concepts.append(rule['subject']['entity'])
            if rule.get('object', {}).get('entity'):
                concepts.append(rule['object']['entity'])
        
        # From DPV mappings
        for mapping in extraction_result.get('dpv_concept_mappings', []):
            concepts.append(mapping.get('extracted_concept', ''))
        
        return [c for c in concepts if c and isinstance(c, str)]
    
    def _export_ontologies(self, owl_ontology: Graph, ttl_graph: Graph, metadata: Dict) -> Dict:
        """Export ontologies in multiple formats"""
        
        safe_country = metadata['country'].replace(' ', '_').replace('/', '_')
        output_dir = Path(Config.OUTPUT_PATH) / safe_country
        output_dir.mkdir(parents=True, exist_ok=True)
        
        exports = {}
        
        # Export OWL ontology (schema/structure)
        owl_formats = {
            'owl': 'xml',
            'ttl': 'turtle',
            'jsonld': 'json-ld'
        }
        
        for ext, format_name in owl_formats.items():
            filename = output_dir / f"rules_as_code_ontology_{safe_country}.{ext}"
            try:
                owl_ontology.serialize(destination=str(filename), format=format_name)
                exports[f"ontology_{ext}"] = str(filename)
                logger.info(f"Exported OWL ontology to {filename}")
            except Exception as e:
                logger.error(f"Failed to export ontology in {format_name}: {e}")
        
        # Export TTL knowledge graph (instances/data)
        kg_formats = {
            'ttl': 'turtle',
            'jsonld': 'json-ld',
            'xml': 'xml'
        }
        
        for ext, format_name in kg_formats.items():
            filename = output_dir / f"rules_as_code_knowledge_graph_{safe_country}.{ext}"
            try:
                ttl_graph.serialize(destination=str(filename), format=format_name)
                exports[f"knowledge_graph_{ext}"] = str(filename)
                logger.info(f"Exported knowledge graph to {filename}")
            except Exception as e:
                logger.error(f"Failed to export knowledge graph in {format_name}: {e}")
        
        return exports
    
    def start_query_interface(self, host='localhost', port=5000):
        """Start the web-based query interface"""
        if not FLASK_AVAILABLE:
            logger.warning("Flask not available - query interface disabled")
            return None
        
        if not self.query_interface:
            logger.warning("Query interface not initialized")
            return None
        
        def run_server():
            self.query_interface.start_server(host, port)
        
        server_thread = threading.Thread(target=run_server, daemon=True)
        server_thread.start()
        logger.info(f"Query interface started at http://{host}:{port}")
        return server_thread

# ====================================
# SUPPORTING CLASSES
# ====================================

class DocumentProcessor:
    """Document processor for PDF files"""
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF using PyMuPDF"""
        try:
            doc = pymupdf.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text.strip()
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            raise

class ElasticsearchClient:
    """Elasticsearch client for storing and searching rules"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize Elasticsearch client"""
        ssl_context = ssl.create_default_context()
        if os.path.exists(Config.ELASTICSEARCH_CERT_PATH):
            ssl_context.load_verify_locations(Config.ELASTICSEARCH_CERT_PATH)
        else:
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
        
        self.client = Elasticsearch(
            [Config.ELASTICSEARCH_URL],
            basic_auth=(Config.ELASTICSEARCH_USERNAME, Config.ELASTICSEARCH_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True if os.path.exists(Config.ELASTICSEARCH_CERT_PATH) else False
        )
        
        if self.client.ping():
            logger.info("Successfully connected to Elasticsearch")
        else:
            raise ConnectionError("Failed to connect to Elasticsearch")
    
    def create_index(self):
        """Create the legal rules index"""
        mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "country": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "organization": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "rules": {"type": "nested"},
                    "adequacy_decisions": {"type": "nested"},
                    "competency_questions": {"type": "nested"},
                    "concepts": {"type": "keyword"},
                    "domains": {"type": "keyword"},
                    "rule_types": {"type": "keyword"},
                    "content_vector": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "processing_timestamp": {"type": "date"},
                    "ontology_triples_count": {"type": "integer"}
                }
            }
        }
        
        if not self.client.indices.exists(index=Config.ELASTICSEARCH_INDEX):
            self.client.indices.create(index=Config.ELASTICSEARCH_INDEX, body=mapping)
            logger.info(f"Created index: {Config.ELASTICSEARCH_INDEX}")

class SHACLValidator:
    """SHACL validator for ontology validation"""
    
    def __init__(self):
        pass  # Basic validator - can be extended with custom shapes

class LegalKnowledgeQueryInterface:
    """Web interface for querying the Rules-as-Code knowledge graph"""
    
    def __init__(self, orchestrator):
        self.orchestrator = orchestrator
        self.knowledge_graph = Graph()
        
        if FLASK_AVAILABLE:
            self.app = Flask(__name__)
            CORS(self.app)
            self._setup_routes()
        else:
            self.app = None
    
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/')
        def index():
            return render_template_string(self._get_index_template())
        
        @self.app.route('/api/sparql', methods=['POST'])
        def sparql_query():
            try:
                query = request.json.get('query', '')
                results = self.execute_sparql_query(query)
                return jsonify({"success": True, "results": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
        
        @self.app.route('/api/competency-questions')
        def competency_questions():
            try:
                # Get competency questions from knowledge graph
                cq_query = """
                PREFIX rac: <https://rules-as-code.org/ontology#>
                PREFIX properties: <https://rules-as-code.org/properties#>
                
                SELECT ?question ?type ?domain ?sparql WHERE {
                    ?cq a rac:CompetencyQuestion ;
                        properties:hasQuestionText ?question .
                    OPTIONAL { ?cq properties:testsDomain ?domain }
                    OPTIONAL { ?cq properties:hasSPARQLPattern ?sparql }
                    OPTIONAL { ?cq a ?type }
                }
                """
                results = self.execute_sparql_query(cq_query)
                return jsonify({"success": True, "competency_questions": results})
            except Exception as e:
                return jsonify({"success": False, "error": str(e)})
    
    def execute_sparql_query(self, query: str) -> List[Dict]:
        """Execute SPARQL query against the knowledge graph"""
        try:
            results = self.knowledge_graph.query(query)
            result_list = []
            
            for row in results:
                row_dict = {}
                for var in results.vars:
                    value = row[var]
                    if value:
                        row_dict[str(var)] = str(value)
                result_list.append(row_dict)
            
            return result_list
        except Exception as e:
            raise Exception(f"SPARQL query failed: {e}")
    
    def load_ontology(self, knowledge_graph: Graph):
        """Load knowledge graph for querying"""
        self.knowledge_graph = knowledge_graph
        logger.info(f"Loaded knowledge graph with {len(knowledge_graph)} triples")
    
    def _get_index_template(self) -> str:
        """Return enhanced HTML template for Rules-as-Code interface"""
        return """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rules-as-Code Query Interface</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; }
        .container { max-width: 1400px; margin: 0 auto; padding: 20px; }
        .header { text-align: center; color: white; margin-bottom: 30px; }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3); }
        .header p { font-size: 1.2em; opacity: 0.9; }
        .main-panel { background: white; border-radius: 15px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); overflow: hidden; }
        .tabs { display: flex; background: #f8f9fa; border-bottom: 1px solid #dee2e6; }
        .tab { flex: 1; padding: 15px 20px; background: none; border: none; cursor: pointer; font-weight: 500; transition: all 0.3s; }
        .tab.active { background: #007bff; color: white; }
        .tab:hover:not(.active) { background: #e9ecef; }
        .tab-content { display: none; padding: 30px; }
        .tab-content.active { display: block; }
        .query-section { margin-bottom: 20px; }
        .query-area { width: 100%; height: 200px; font-family: 'Courier New', monospace; border: 2px solid #e9ecef; border-radius: 8px; padding: 15px; font-size: 14px; resize: vertical; }
        .query-area:focus { border-color: #007bff; outline: none; box-shadow: 0 0 0 3px rgba(0,123,255,0.25); }
        .button-group { display: flex; gap: 10px; margin: 20px 0; }
        .button { background: linear-gradient(45deg, #007bff, #0056b3); color: white; padding: 12px 24px; border: none; border-radius: 8px; cursor: pointer; font-weight: 500; transition: all 0.3s; }
        .button:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,123,255,0.3); }
        .button.secondary { background: linear-gradient(45deg, #6c757d, #545b62); }
        .results { border: 2px solid #e9ecef; border-radius: 8px; padding: 20px; margin-top: 20px; max-height: 400px; overflow-y: auto; background: #f8f9fa; }
        .results table { width: 100%; border-collapse: collapse; background: white; border-radius: 8px; overflow: hidden; }
        .results th { background: #007bff; color: white; padding: 12px; text-align: left; }
        .results td { padding: 12px; border-bottom: 1px solid #e9ecef; }
        .results tr:hover { background: #f1f3f4; }
        .sample-queries { background: #e7f3ff; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .sample-query { background: white; border-radius: 6px; padding: 15px; margin: 10px 0; cursor: pointer; transition: all 0.3s; border-left: 4px solid #007bff; }
        .sample-query:hover { box-shadow: 0 2px 8px rgba(0,0,0,0.1); transform: translateX(5px); }
        .cq-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
        .cq-card { background: white; border-radius: 8px; padding: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); border-left: 4px solid #28a745; transition: all 0.3s; }
        .cq-card:hover { transform: translateY(-2px); box-shadow: 0 4px 16px rgba(0,0,0,0.15); }
        .cq-type { background: #28a745; color: white; padding: 4px 8px; border-radius: 4px; font-size: 0.8em; margin-bottom: 10px; display: inline-block; }
        .domain-tag { background: #17a2b8; color: white; padding: 2px 6px; border-radius: 3px; font-size: 0.7em; margin-left: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1> Rules-as-Code Query Interface</h1>
            <p>Explore machine-readable legal rules with semantic queries and competency questions</p>
        </div>
        
        <div class="main-panel">
            <div class="tabs">
                <button class="tab active" onclick="showTab('sparql')">SPARQL Queries</button>
                <button class="tab" onclick="showTab('competency')">Competency Questions</button>
                <button class="tab" onclick="showTab('examples')">Sample Queries</button>
            </div>
            
            <div id="sparql" class="tab-content active">
                <h3> SPARQL Query Interface</h3>
                <p>Query the Rules-as-Code knowledge graph using SPARQL. The system includes DPV mappings and comprehensive legal rule structures.</p>
                
                <div class="query-section">
                    <textarea id="sparqlQuery" class="query-area" placeholder="Enter your SPARQL query here...

Example - Find all legal obligations:
PREFIX rac: <https://rules-as-code.org/ontology#>
PREFIX properties: <https://rules-as-code.org/properties#>

SELECT ?rule ?text ?domain WHERE {
  ?rule a rac:LegalRule ;
        properties:hasRuleType 'obligation' ;
        properties:hasOriginalText ?text ;
        properties:belongsToDomain ?domain .
} LIMIT 10"></textarea>
                    
                    <div class="button-group">
                        <button class="button" onclick="executeSparqlQuery()"> Execute Query</button>
                        <button class="button secondary" onclick="clearResults()"> Clear Results</button>
                        <button class="button secondary" onclick="loadSampleQuery()"> Load Sample</button>
                    </div>
                </div>
            </div>
            
            <div id="competency" class="tab-content">
                <h3> Competency Questions</h3>
                <p>Explore competency questions that validate the ontology and test legal knowledge representation.</p>
                
                <div class="button-group">
                    <button class="button" onclick="loadCompetencyQuestions()"> Load Questions</button>
                    <button class="button secondary" onclick="executeAllCQ()"> Execute All</button>
                </div>
                
                <div id="competencyQuestions" class="cq-grid">
                    <!-- Competency questions will be loaded here -->
                </div>
            </div>
            
            <div id="examples" class="tab-content">
                <h3> Sample Queries</h3>
                
                <div class="sample-queries">
                    <div class="sample-query" onclick="loadQuery(this)">
                        <strong>Find Adequacy Countries</strong>
                        <pre>PREFIX rac: <https://rules-as-code.org/ontology#>
SELECT ?country ?status WHERE {
  ?decision a rac:AdequacyDecision ;
            properties:hasAdequacyStatus ?status .
}</pre>
                    </div>
                    
                    <div class="sample-query" onclick="loadQuery(this)">
                        <strong>List Data Management Domains</strong>
                        <pre>PREFIX rac: <https://rules-as-code.org/ontology#>
SELECT DISTINCT ?domain ?ruleCount WHERE {
  ?rule properties:belongsToDomain ?domain .
} GROUP BY ?domain</pre>
                    </div>
                    
                    <div class="sample-query" onclick="loadQuery(this)">
                        <strong>Find DPV Concept Mappings</strong>
                        <pre>PREFIX rac: <https://rules-as-code.org/ontology#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
SELECT ?localConcept ?dpvConcept WHERE {
  ?localConcept skos:exactMatch ?dpvConcept .
  FILTER(STRSTARTS(STR(?dpvConcept), "https://w3id.org/dpv"))
}</pre>
                    </div>
                </div>
            </div>
            
            <div id="results" class="results" style="display: none;">
                <h3> Query Results</h3>
                <div id="resultContent"></div>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            document.querySelectorAll('.tab-content').forEach(tab => tab.classList.remove('active'));
            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
            
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
        
        async function executeSparqlQuery() {
            const query = document.getElementById('sparqlQuery').value;
            if (!query.trim()) {
                alert('Please enter a SPARQL query');
                return;
            }
            
            try {
                const response = await fetch('/api/sparql', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ query })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    displayResults(data.results, 'table');
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Network error: ' + error.message);
            }
        }
        
        async function loadCompetencyQuestions() {
            try {
                const response = await fetch('/api/competency-questions');
                const data = await response.json();
                
                if (data.success) {
                    displayCompetencyQuestions(data.competency_questions);
                } else {
                    showError(data.error);
                }
            } catch (error) {
                showError('Failed to load competency questions: ' + error.message);
            }
        }
        
        function displayResults(results, format) {
            const resultsDiv = document.getElementById('results');
            const contentDiv = document.getElementById('resultContent');
            
            if (results.length === 0) {
                contentDiv.innerHTML = '<p>No results found.</p>';
            } else {
                let html = '<table><thead><tr>';
                
                // Get column headers
                if (results.length > 0) {
                    Object.keys(results[0]).forEach(key => {
                        html += `<th>${key}</th>`;
                    });
                    html += '</tr></thead><tbody>';
                    
                    results.forEach(row => {
                        html += '<tr>';
                        Object.values(row).forEach(value => {
                            html += `<td>${value}</td>`;
                        });
                        html += '</tr>';
                    });
                    html += '</tbody></table>';
                }
                
                contentDiv.innerHTML = html;
            }
            
            resultsDiv.style.display = 'block';
        }
        
        function displayCompetencyQuestions(questions) {
            const container = document.getElementById('competencyQuestions');
            let html = '';
            
            questions.forEach((q, index) => {
                const type = q.type || 'foundational';
                const domain = q.domain || 'general';
                
                html += `
                    <div class="cq-card">
                        <div class="cq-type">${type}</div>
                        <span class="domain-tag">${domain}</span>
                        <h4>${q.question}</h4>
                        ${q.sparql ? `<button class="button" onclick="executeCQ('${q.sparql.replace(/'/g, "\\'")}')">Execute Query</button>` : ''}
                    </div>
                `;
            });
            
            container.innerHTML = html;
        }
        
        function executeCQ(sparql) {
            document.getElementById('sparqlQuery').value = sparql;
            showTab('sparql');
            executeSparqlQuery();
        }
        
        function loadQuery(element) {
            const query = element.querySelector('pre').textContent;
            document.getElementById('sparqlQuery').value = query;
            showTab('sparql');
        }
        
        function loadSampleQuery() {
            const sampleQuery = `PREFIX rac: <https://rules-as-code.org/ontology#>
PREFIX properties: <https://rules-as-code.org/properties#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?rule ?type ?domain ?text WHERE {
  ?rule a rac:LegalRule ;
        properties:hasRuleType ?type ;
        properties:belongsToDomain ?domain ;
        properties:hasOriginalText ?text .
} LIMIT 5`;
            
            document.getElementById('sparqlQuery').value = sampleQuery;
        }
        
        function showError(message) {
            const contentDiv = document.getElementById('resultContent');
            contentDiv.innerHTML = `<div style="color: #dc3545; background: #f8d7da; padding: 15px; border-radius: 8px; border: 1px solid #f5c6cb;">
                <strong>Error:</strong> ${message}
            </div>`;
            document.getElementById('results').style.display = 'block';
        }
        
        function clearResults() {
            document.getElementById('results').style.display = 'none';
            document.getElementById('resultContent').innerHTML = '';
        }
    </script>
</body>
</html>
        """
    
    def start_server(self, host='localhost', port=5000):
        """Start the web server"""
        if not FLASK_AVAILABLE or not self.app:
            logger.error("Flask not available - cannot start web server")
            return
        
        logger.info(f"Starting Rules-as-Code query interface on {host}:{port}")
        self.app.run(host=host, port=port, debug=False)

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file for Rules-as-Code"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission",
                    "legal_system": "Civil Law",
                    "pdf_document": "./documents/gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access"]
                },
                {
                    "country": "United States",
                    "jurisdiction": "Federal",
                    "organization": "Federal Trade Commission",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/ccpa.pdf",
                    "adequacy_focus": False,
                    "data_management_domains": ["privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United Kingdom",
                    "jurisdiction": "UK",
                    "organization": "Information Commissioner's Office",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/uk_gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                }
            ],
            "processing_options": {
                "enable_query_interface": True,
                "interface_host": "localhost",
                "interface_port": 5000,
                "reasoning_effort": "high",
                "generate_competency_questions": True,
                "dpv_integration": True,
                "export_formats": ["ttl", "jsonld", "xml", "owl"]
            },
            "adequacy_countries": [
                "Andorra", "Argentina", "Canada", "Faroe Islands", "Guernsey", "Israel", 
                "Isle of Man", "Japan", "Jersey", "New Zealand", "South Korea", 
                "Switzerland", "United Kingdom", "Uruguay"
            ]
        }

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Enhanced main application for Rules-as-Code"""
    
    load_dotenv()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info(" Starting Enhanced Rules-as-Code System")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info(" Configuration validation passed")
    except ValueError as e:
        logger.error(f" Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info(" Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json with your documents and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize Rules-as-Code orchestrator
    logger.info(" Initializing Rules-as-Code orchestrator...")
    orchestrator = RulesAsCodeOrchestrator()
    
    # Start query interface if enabled
    interface_enabled = config.get('processing_options', {}).get('enable_query_interface', True) and FLASK_AVAILABLE
    if interface_enabled:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        server_thread = orchestrator.start_query_interface(interface_host, interface_port)
    else:
        logger.info(" Query interface disabled or Flask not available")
        server_thread = None
    
    # Process legal documents
    results = []
    total_docs = len(config['documents'])
    
    for i, doc_config in enumerate(config['documents'], 1):
        logger.info(f" Processing document {i}/{total_docs}: {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f" Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_legal_document(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "jurisdiction": doc_config['jurisdiction'],
                "result": result
            })
            
            if result['success']:
                logger.info(f" Successfully processed {doc_config['country']}")
                logger.info(f"    Rules extracted: {result['extraction_stats']['rules_extracted']}")
                logger.info(f"    Competency questions: {result['extraction_stats']['competency_questions_generated']}")
                logger.info(f"    Domain coverage: {', '.join(result['extraction_stats']['domain_coverage'])}")
            else:
                logger.error(f" Failed to process {doc_config['country']}: {result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f" Processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Generate summary
    successful = sum(1 for r in results if r['result']['success'])
    total_rules = sum(r['result'].get('extraction_stats', {}).get('rules_extracted', 0) 
                     for r in results if r['result']['success'])
    total_cqs = sum(r['result'].get('extraction_stats', {}).get('competency_questions_generated', 0) 
                   for r in results if r['result']['success'])
    
    logger.info(" Rules-as-Code processing complete!")
    logger.info(f"    Documents processed: {successful}/{total_docs}")
    logger.info(f"    Total rules extracted: {total_rules}")
    logger.info(f"    Total competency questions: {total_cqs}")
    
    # Save comprehensive results
    summary_path = Path(Config.OUTPUT_PATH) / "rules_as_code_summary.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary = {
        "processing_summary": {
            "timestamp": datetime.now().isoformat(),
            "documents_processed": successful,
            "total_documents": total_docs,
            "total_rules_extracted": total_rules,
            "total_competency_questions": total_cqs,
            "system_version": "rules-as-code-v2.0"
        },
        "results": results
    }
    
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    logger.info(f" Summary saved to: {summary_path}")
    
    if interface_enabled and server_thread:
        interface_host = config.get('processing_options', {}).get('interface_host', Config.WEB_HOST)
        interface_port = config.get('processing_options', {}).get('interface_port', Config.WEB_PORT)
        logger.info(f" Query interface available at http://{interface_host}:{interface_port}")
        logger.info("Press Ctrl+C to stop the system")
        
        try:
            # Keep the main thread alive
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info(" Shutting down Rules-as-Code system...")
    else:
        logger.info(" Rules-as-Code processing completed. Query interface not started.")

if __name__ == "__main__":
    asyncio.run(main())
