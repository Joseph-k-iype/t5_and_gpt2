"""
Document Chunking Utilities - COMPLETE VERSION
Handles INTERNAL watermark and other document artifacts
"""

from typing import List, Dict, Any, Optional
import re


class DocumentChunk:
    """Simple chunk data class"""
    def __init__(self, text: str, chunk_id: int, start_pos: int, end_pos: int, 
                 total_chunks: int, metadata: Dict[str, Any]):
        self.text = text
        self.chunk_id = chunk_id
        self.start_pos = start_pos
        self.end_pos = end_pos
        self.total_chunks = total_chunks
        self.metadata = metadata
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            "text": self.text,
            "chunk_id": self.chunk_id,
            "start_pos": self.start_pos,
            "end_pos": self.end_pos,
            "total_chunks": self.total_chunks,
            "metadata": self.metadata
        }


class DocumentChunker:
    """Intelligently chunks documents without losing information"""
    
    def __init__(self, chunk_size: int = 4000, chunk_overlap: int = 300, 
                 respect_boundaries: bool = True):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_boundaries = respect_boundaries
        
        # Watermarks and classification markers to filter
        self.watermarks = [
            'INTERNAL',
            'CONFIDENTIAL', 
            'RESTRICTED', 
            'DRAFT',
            'PROPRIETARY', 
            'CLASSIFIED',
            'PRIVATE',
            'SENSITIVE'
        ]
    
    def _clean_text(self, text: str) -> str:
        """
        Remove watermarks, classification markers, and document artifacts
        Specifically handles INTERNAL at end of documents
        """
        cleaned = text
        
        # Remove INTERNAL and other watermarks at end of document
        for watermark in self.watermarks:
            # Pattern 1: Watermark at end of document (possibly with page numbers)
            cleaned = re.sub(
                rf'\s*{watermark}\s*\d*\s*$', 
                '', 
                cleaned, 
                flags=re.IGNORECASE
            )
            
            # Pattern 2: Watermark on its own line
            cleaned = re.sub(
                rf'^\s*{watermark}\s*$', 
                '', 
                cleaned, 
                flags=re.IGNORECASE | re.MULTILINE
            )
            
            # Pattern 3: Watermark with page number on line
            cleaned = re.sub(
                rf'\n\s*{watermark}\s*\d+\s*\n', 
                '\n', 
                cleaned, 
                flags=re.IGNORECASE
            )
            
            # Pattern 4: Watermark at start/end of lines
            cleaned = re.sub(
                rf'\n\s*{watermark}\s*\n', 
                '\n', 
                cleaned, 
                flags=re.IGNORECASE
            )
            
            # Pattern 5: Watermark with trailing spaces/tabs
            cleaned = re.sub(
                rf'\s+{watermark}[\s\t]*\n', 
                '\n', 
                cleaned, 
                flags=re.IGNORECASE
            )
        
        # Remove excessive whitespace but preserve paragraph structure
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        
        # Remove trailing/leading whitespace
        cleaned = cleaned.strip()
        
        return cleaned
    
    def chunk_document(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Chunk document into dictionaries - GUARANTEED TO RETURN LIST OF DICTS
        
        Args:
            text: Document text to chunk
            metadata: Optional metadata to attach to chunks
            
        Returns:
            List of dictionaries with consistent structure
        """
        # Clean text first
        text = self._clean_text(text)
        metadata = metadata or {}
        
        # Handle single chunk case
        if len(text) <= self.chunk_size:
            return [{
                "text": text,
                "chunk_id": 0,
                "start_pos": 0,
                "end_pos": len(text),
                "total_chunks": 1,
                "metadata": metadata
            }]
        
        # Chunk based on strategy
        if self.respect_boundaries:
            raw_chunks = self._chunk_with_boundaries(text)
        else:
            raw_chunks = self._chunk_simple(text)
        
        # CRITICAL: Convert all chunks to proper dictionaries with validation
        total_chunks = len(raw_chunks)
        result_chunks = []
        
        for i, chunk in enumerate(raw_chunks):
            # Ensure it's a dict with all required fields
            if isinstance(chunk, dict):
                chunk_dict = {
                    "text": chunk.get("text", ""),
                    "chunk_id": i,
                    "start_pos": chunk.get("start_pos", 0),
                    "end_pos": chunk.get("end_pos", 0),
                    "total_chunks": total_chunks,
                    "metadata": metadata
                }
            else:
                # Fallback if somehow not a dict
                chunk_dict = {
                    "text": str(chunk),
                    "chunk_id": i,
                    "start_pos": 0,
                    "end_pos": len(str(chunk)),
                    "total_chunks": total_chunks,
                    "metadata": metadata
                }
            
            # Validate required fields exist and are correct types
            assert isinstance(chunk_dict, dict), f"Chunk {i} is not a dictionary"
            assert "text" in chunk_dict, f"Chunk {i} missing 'text' field"
            assert isinstance(chunk_dict["text"], str), f"Chunk {i} 'text' is not a string"
            assert "chunk_id" in chunk_dict, f"Chunk {i} missing 'chunk_id' field"
            assert isinstance(chunk_dict["chunk_id"], int), f"Chunk {i} 'chunk_id' is not an int"
            
            result_chunks.append(chunk_dict)
        
        return result_chunks
    
    def _chunk_simple(self, text: str) -> List[Dict[str, Any]]:
        """Simple fixed-size chunking"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            chunk_text = text[start:end]
            
            chunks.append({
                "text": chunk_text,
                "start_pos": start,
                "end_pos": end
            })
            
            start = end - self.chunk_overlap
            if start >= len(text) - 10:
                break
        
        return chunks
    
    def _chunk_with_boundaries(self, text: str) -> List[Dict[str, Any]]:
        """Chunk respecting paragraph boundaries"""
        paragraphs = self._split_paragraphs(text)
        chunks = []
        current_chunk = []
        current_size = 0
        current_start = 0
        
        for para_start, para_end, para_text in paragraphs:
            para_size = len(para_text)
            
            if para_size > self.chunk_size:
                if current_chunk:
                    chunks.append({
                        "text": "".join(current_chunk),
                        "start_pos": current_start,
                        "end_pos": current_start + current_size
                    })
                    current_chunk = []
                    current_size = 0
                
                para_chunks = self._chunk_paragraph_by_sentences(para_text, para_start)
                chunks.extend(para_chunks)
                current_start = para_end
                
            elif current_size + para_size > self.chunk_size and current_chunk:
                chunks.append({
                    "text": "".join(current_chunk),
                    "start_pos": current_start,
                    "end_pos": current_start + current_size
                })
                
                if self.chunk_overlap > 0 and chunks:
                    overlap_text = chunks[-1]["text"][-self.chunk_overlap:]
                    current_chunk = [overlap_text, para_text]
                    current_size = len(overlap_text) + para_size
                else:
                    current_chunk = [para_text]
                    current_size = para_size
                
                current_start = para_start
            else:
                current_chunk.append(para_text)
                current_size += para_size
        
        if current_chunk:
            chunks.append({
                "text": "".join(current_chunk),
                "start_pos": current_start,
                "end_pos": current_start + current_size
            })
        
        return chunks
    
    def _split_paragraphs(self, text: str) -> List[tuple]:
        """Split text into paragraphs"""
        paragraphs = []
        pattern = r'\n\s*\n'
        last_end = 0
        
        for match in re.finditer(pattern, text):
            start = last_end
            end = match.start()
            para_text = text[start:end].strip()
            if para_text:
                paragraphs.append((start, end, para_text + '\n\n'))
            last_end = match.end()
        
        if last_end < len(text):
            para_text = text[last_end:].strip()
            if para_text:
                paragraphs.append((last_end, len(text), para_text))
        
        if not paragraphs:
            paragraphs.append((0, len(text), text))
        
        return paragraphs
    
    def _chunk_paragraph_by_sentences(self, paragraph: str, para_start: int) -> List[Dict[str, Any]]:
        """Chunk large paragraph by sentences"""
        sentences = self._split_sentences(paragraph)
        chunks = []
        current_chunk = []
        current_size = 0
        chunk_start = para_start
        
        for sent_start, sent_end, sent_text in sentences:
            sent_size = len(sent_text)
            
            if sent_size > self.chunk_size:
                if current_chunk:
                    chunks.append({
                        "text": "".join(current_chunk),
                        "start_pos": chunk_start,
                        "end_pos": chunk_start + current_size
                    })
                
                force_chunks = self._force_split(sent_text, para_start + sent_start)
                chunks.extend(force_chunks)
                current_chunk = []
                current_size = 0
                chunk_start = para_start + sent_end
                
            elif current_size + sent_size > self.chunk_size and current_chunk:
                chunks.append({
                    "text": "".join(current_chunk),
                    "start_pos": chunk_start,
                    "end_pos": chunk_start + current_size
                })
                
                if self.chunk_overlap > 0 and chunks:
                    overlap_text = chunks[-1]["text"][-self.chunk_overlap:]
                    current_chunk = [overlap_text, sent_text]
                    current_size = len(overlap_text) + sent_size
                else:
                    current_chunk = [sent_text]
                    current_size = sent_size
                
                chunk_start = para_start + sent_start
            else:
                current_chunk.append(sent_text)
                current_size += sent_size
        
        if current_chunk:
            chunks.append({
                "text": "".join(current_chunk),
                "start_pos": chunk_start,
                "end_pos": chunk_start + current_size
            })
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[tuple]:
        """Split text into sentences"""
        sentences = []
        pattern = r'[.!?]+[\s\n]+'
        last_end = 0
        
        for match in re.finditer(pattern, text):
            start = last_end
            end = match.end()
            sent_text = text[start:end].strip()
            if sent_text:
                sentences.append((start, end, sent_text + ' '))
            last_end = end
        
        if last_end < len(text):
            sent_text = text[last_end:].strip()
            if sent_text:
                sentences.append((last_end, len(text), sent_text))
        
        if not sentences:
            sentences.append((0, len(text), text))
        
        return sentences
    
    def _force_split(self, text: str, start_pos: int) -> List[Dict[str, Any]]:
        """Force split oversized text"""
        chunks = []
        pos = 0
        
        while pos < len(text):
            end = min(pos + self.chunk_size, len(text))
            chunk_text = text[pos:end]
            
            chunks.append({
                "text": chunk_text,
                "start_pos": start_pos + pos,
                "end_pos": start_pos + end
            })
            
            pos = end - self.chunk_overlap
            if pos >= len(text) - 10:
                break
        
        return chunks
    
    def get_chunk_context(self, chunk: Dict[str, Any]) -> str:
        """Generate context string for chunk"""
        chunk_id = chunk.get("chunk_id", 0)
        total_chunks = chunk.get("total_chunks", 1)
        
        if total_chunks == 1:
            return "This is the complete document."
        
        position = "beginning" if chunk_id == 0 else "end" if chunk_id == total_chunks - 1 else "middle"
        context = f"This is chunk {chunk_id + 1} of {total_chunks} (from the {position} of the document)."
        
        if chunk_id > 0:
            context += " Previous chunks have been analyzed."
        if chunk_id < total_chunks - 1:
            context += " More content follows in subsequent chunks."
        
        return context
