import asyncio
import json
import logging
import os
import csv
import glob
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from datetime import datetime, timedelta
from urllib.parse import urljoin

# Core dependencies
import openai
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator, ConfigDict
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI

# PDF processing
try:
    import pymupdf  # Modern PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    try:
        import pdfplumber
        PDF_AVAILABLE = True
    except ImportError:
        PDF_AVAILABLE = False
        print("Warning: No PDF library found. Install PyMuPDF or pdfplumber: pip install PyMuPDF pdfplumber")

# Optional: RDF library for advanced semantic processing
try:
    import rdflib
    RDF_AVAILABLE = True
    print("✅ RDFLib available for advanced RDF processing")
except ImportError:
    RDF_AVAILABLE = False
    # RDFLib is optional - we generate Turtle/JSON-LD manually
    pass

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================
# GLOBAL CONFIGURATION
# ===============================

class Config:
    """Global configuration for the legislation rules converter."""
    BASE_URL = "https://api.openai.com/v1"
    API_KEY = os.getenv("OPENAI_API_KEY")
    CHAT_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Paths
    LEGISLATION_PDF_PATH = "./legislation_pdfs/"
    RULES_OUTPUT_PATH = "./extracted_rules/"
    EMBEDDINGS_PATH = "./embeddings/"
    LOGS_PATH = "./logs/"
    EXISTING_RULES_FILE = "./extracted_rules/all_rules.json"
    METADATA_CONFIG_FILE = "./legislation_metadata.json"
    
    # DPV and ODRL Output Paths
    DPV_OUTPUT_PATH = "./dpv_outputs/"
    ODRL_OUTPUT_PATH = "./odrl_outputs/"
    
    # Standard Namespaces
    DPV_NAMESPACE = "https://w3id.org/dpv#"
    ODRL_NAMESPACE = "http://www.w3.org/ns/odrl/2/"
    DPVCG_NAMESPACE = "https://w3id.org/dpv/"

# Validate API key
if not Config.API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is required")

# ===============================
# DPV (DATA PRIVACY VOCABULARY) ALIGNMENT
# ===============================

class DPVConcepts:
    """DPV (Data Privacy Vocabulary) concept mappings and utilities."""
    
    # DPV Core Namespaces
    DPV = "https://w3id.org/dpv#"
    DPV_PD = "https://w3id.org/dpv/dpv-pd#"
    DPV_LEGAL = "https://w3id.org/dpv/legal/"
    DPV_TECH = "https://w3id.org/dpv/tech#"
    
    # DPV Core Concepts
    PROCESSING_PURPOSES = {
        "service_provision": f"{DPV}ServiceProvision",
        "marketing": f"{DPV}Marketing", 
        "analytics": f"{DPV}Analytics",
        "compliance": f"{DPV}LegalCompliance",
        "research": f"{DPV}Research",
        "security": f"{DPV}ServiceSecurity"
    }
    
    PROCESSING_OPERATIONS = {
        "collect": f"{DPV}Collect",
        "store": f"{DPV}Store", 
        "use": f"{DPV}Use",
        "share": f"{DPV}Share",
        "transfer": f"{DPV}Transfer",
        "delete": f"{DPV}Erase"
    }
    
    DATA_CATEGORIES = {
        "personal_data": f"{DPV}PersonalData",
        "sensitive_data": f"{DPV}SensitivePersonalData",
        "biometric_data": f"{DPV_PD}Biometric",
        "health_data": f"{DPV_PD}Health",
        "financial_data": f"{DPV_PD}Financial",
        "location_data": f"{DPV_PD}Location",
        "behavioral_data": f"{DPV_PD}Behavioral",
        "identification_data": f"{DPV_PD}Identifying"
    }
    
    LEGAL_BASIS = {
        "consent": f"{DPV}Consent",
        "contract": f"{DPV}Contract", 
        "legal_obligation": f"{DPV}LegalObligation",
        "vital_interests": f"{DPV}VitalInterests",
        "public_task": f"{DPV}PublicTask",
        "legitimate_interests": f"{DPV}LegitimateInterests"
    }
    
    ROLES = {
        "controller": f"{DPV}DataController",
        "processor": f"{DPV}DataProcessor",
        "joint_controller": f"{DPV}JointDataControllers"
    }
    
    TECHNICAL_MEASURES = {
        "encryption": f"{DPV_TECH}Encryption",
        "access_control": f"{DPV_TECH}AccessControl",
        "anonymisation": f"{DPV_TECH}Anonymisation",
        "pseudonymisation": f"{DPV_TECH}Pseudonymisation"
    }

class DPVRule(BaseModel):
    """Rule expressed using DPV vocabulary."""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str = Field(..., description="Unique rule identifier")
    type: str = Field(default="dpv:ProcessingActivity", description="DPV rule type")
    
    # Core DPV Properties
    hasProcessing: List[str] = Field(..., description="Processing operations")
    hasPurpose: List[str] = Field(..., description="Purposes for processing") 
    hasPersonalData: List[str] = Field(..., description="Personal data categories")
    hasDataController: Optional[str] = Field(None, description="Data controller")
    hasDataProcessor: Optional[str] = Field(None, description="Data processor")
    hasLegalBasis: Optional[str] = Field(None, description="Legal basis for processing")
    hasTechnicalMeasure: List[str] = Field(default_factory=list, description="Technical measures")
    hasOrganisationalMeasure: List[str] = Field(default_factory=list, description="Organisational measures")
    
    # Location and Context
    hasLocation: List[str] = Field(default_factory=list, description="Processing locations/countries")
    hasRecipient: List[str] = Field(default_factory=list, description="Data recipients")
    hasStorageCondition: Optional[str] = Field(None, description="Storage conditions")
    hasDataSource: Optional[str] = Field(None, description="Data source")
    
    # Metadata
    source_legislation: str = Field(..., description="Source legislation")
    source_article: str = Field(..., description="Source article/section")
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    confidence_score: float = Field(..., ge=0.0, le=1.0)

# ===============================
# ODRL (OPEN DIGITAL RIGHTS LANGUAGE) ALIGNMENT  
# ===============================

class ODRLPolicy(BaseModel):
    """ODRL Policy structure."""
    model_config = ConfigDict(use_enum_values=True)
    
    context: str = Field(default="http://www.w3.org/ns/odrl.jsonld", description="JSON-LD context")
    type: str = Field(default="Policy", description="ODRL policy type")
    uid: str = Field(..., description="Unique policy identifier")
    
    permission: List[Dict[str, Any]] = Field(default_factory=list, description="Permissions")
    prohibition: List[Dict[str, Any]] = Field(default_factory=list, description="Prohibitions") 
    obligation: List[Dict[str, Any]] = Field(default_factory=list, description="Obligations")
    
    # Metadata
    profile: Optional[str] = Field(None, description="ODRL profile")
    inheritFrom: Optional[str] = Field(None, description="Inherited policy")
    conflict: Optional[str] = Field(None, description="Conflict resolution strategy")

class ODRLRule(BaseModel):
    """Individual ODRL rule (permission, prohibition, obligation)."""
    model_config = ConfigDict(use_enum_values=True)
    
    target: str = Field(..., description="Target asset")
    action: Union[str, List[str]] = Field(..., description="Action(s)")
    assigner: Optional[str] = Field(None, description="Party granting the rule")
    assignee: Optional[str] = Field(None, description="Party receiving the rule")
    constraint: List[Dict[str, Any]] = Field(default_factory=list, description="Constraints")
    duty: List[Dict[str, Any]] = Field(default_factory=list, description="Duties")
    
class ODRLConstraint(BaseModel):
    """ODRL constraint structure."""
    model_config = ConfigDict(use_enum_values=True)
    
    leftOperand: str = Field(..., description="Left operand")
    operator: str = Field(..., description="Comparison operator")
    rightOperand: Any = Field(..., description="Right operand value")
    dataType: Optional[str] = Field(None, description="Data type")
    unit: Optional[str] = Field(None, description="Unit of measurement")

# ===============================
# ODRE ENFORCEMENT FRAMEWORK
# ===============================

class ODREFramework:
    """ODRE (Open Digital Rights Enforcement) Framework integration."""
    
    ODRE_NAMESPACE = "https://w3id.org/def/odre#"
    
    @staticmethod
    def create_enforceable_policy(odrl_policy: ODRLPolicy) -> Dict[str, Any]:
        """Convert ODRL policy to ODRE enforceable format."""
        enforceable_policy = {
            "@context": [
                "http://www.w3.org/ns/odrl.jsonld",
                {
                    "odre": ODREFramework.ODRE_NAMESPACE,
                    "dpv": DPVConcepts.DPV
                }
            ],
            "@type": odrl_policy.type,
            "uid": odrl_policy.uid,
            "odre:enforceable": True,
            "odre:enforcement_mode": "real_time"
        }
        
        # Add permissions with enforcement annotations
        if odrl_policy.permission:
            enforceable_policy["permission"] = [
                ODREFramework._add_enforcement_metadata(perm, "permission")
                for perm in odrl_policy.permission
            ]
        
        # Add prohibitions with enforcement annotations  
        if odrl_policy.prohibition:
            enforceable_policy["prohibition"] = [
                ODREFramework._add_enforcement_metadata(proh, "prohibition")
                for proh in odrl_policy.prohibition
            ]
            
        # Add obligations with enforcement annotations
        if odrl_policy.obligation:
            enforceable_policy["obligation"] = [
                ODREFramework._add_enforcement_metadata(obl, "obligation") 
                for obl in odrl_policy.obligation
            ]
        
        return enforceable_policy
    
    @staticmethod
    def _add_enforcement_metadata(rule: Dict[str, Any], rule_type: str) -> Dict[str, Any]:
        """Add enforcement metadata to ODRL rule."""
        enforced_rule = rule.copy()
        enforced_rule["odre:enforcement_type"] = rule_type
        enforced_rule["odre:monitoring_required"] = True
        enforced_rule["odre:compliance_check"] = True
        
        # Add temporal enforcement if constraints exist
        if "constraint" in rule:
            for constraint in rule["constraint"]:
                if constraint.get("leftOperand") in ["dateTime", "date", "time"]:
                    enforced_rule["odre:temporal_enforcement"] = True
        
        return enforced_rule

# ===============================
# STANDARDS CONVERTER CLASS
# ===============================

class StandardsConverter:
    """Converts between JSON Rules Engine, DPV, and ODRL formats."""
    
    def __init__(self):
        self.dpv_concepts = DPVConcepts()
        self.odre_framework = ODREFramework()
    
    def json_rules_to_dpv(self, legislation_rule: "LegislationRule") -> DPVRule:
        """Convert JSON Rules Engine rule to DPV format."""
        
        # Map data categories to DPV personal data concepts
        dpv_personal_data = []
        for category in legislation_rule.data_category:
            if category.value in self.dpv_concepts.DATA_CATEGORIES:
                dpv_personal_data.append(self.dpv_concepts.DATA_CATEGORIES[category.value])
        
        # Map processing operations from rule conditions
        dpv_processing = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                # Infer processing operations from rule facts
                if "collect" in condition.fact.lower():
                    dpv_processing.append(self.dpv_concepts.PROCESSING_OPERATIONS["collect"])
                elif "store" in condition.fact.lower():
                    dpv_processing.append(self.dpv_concepts.PROCESSING_OPERATIONS["store"])
                elif "use" in condition.fact.lower():
                    dpv_processing.append(self.dpv_concepts.PROCESSING_OPERATIONS["use"])
                elif "share" in condition.fact.lower():
                    dpv_processing.append(self.dpv_concepts.PROCESSING_OPERATIONS["share"])
        
        # Default processing if none inferred
        if not dpv_processing:
            dpv_processing = [self.dpv_concepts.PROCESSING_OPERATIONS["use"]]
        
        # Map purposes based on rule description and event type
        dpv_purposes = []
        rule_text = f"{legislation_rule.description} {legislation_rule.event.type}".lower()
        if "service" in rule_text or "provision" in rule_text:
            dpv_purposes.append(self.dpv_concepts.PROCESSING_PURPOSES["service_provision"])
        elif "compliance" in rule_text or "legal" in rule_text:
            dpv_purposes.append(self.dpv_concepts.PROCESSING_PURPOSES["compliance"])
        elif "security" in rule_text:
            dpv_purposes.append(self.dpv_concepts.PROCESSING_PURPOSES["security"])
        else:
            dpv_purposes.append(self.dpv_concepts.PROCESSING_PURPOSES["service_provision"])
        
        # Map roles
        controller = None
        processor = None
        if legislation_rule.primary_impacted_role == DataRole.CONTROLLER:
            controller = self.dpv_concepts.ROLES["controller"]
        elif legislation_rule.primary_impacted_role == DataRole.PROCESSOR:
            processor = self.dpv_concepts.ROLES["processor"]
        
        # Map countries to DPV location concepts
        dpv_locations = [f"dpv:Country_{country.replace(' ', '_')}" for country in legislation_rule.applicable_countries]
        
        return DPVRule(
            id=f"dpv:{legislation_rule.id}",
            hasProcessing=dpv_processing,
            hasPurpose=dpv_purposes,
            hasPersonalData=dpv_personal_data,
            hasDataController=controller,
            hasDataProcessor=processor,
            hasLegalBasis=self.dpv_concepts.LEGAL_BASIS["legal_obligation"],  # Default for legislation
            hasLocation=dpv_locations,
            source_legislation=legislation_rule.source_file,
            source_article=legislation_rule.source_article,
            confidence_score=legislation_rule.confidence_score
        )
    
    def json_rules_to_odrl(self, legislation_rule: "LegislationRule") -> ODRLPolicy:
        """Convert JSON Rules Engine rule to ODRL policy."""
        
        policy_uid = f"urn:policy:{legislation_rule.id}"
        
        # Create ODRL rules based on rule type and conditions
        permissions = []
        prohibitions = []
        obligations = []
        
        # Analyze rule to determine ODRL rule type
        rule_description = legislation_rule.description.lower()
        event_type = legislation_rule.event.type.lower()
        
        if "prohibit" in rule_description or "forbid" in event_type:
            # Create prohibition
            prohibition = self._create_odrl_rule(legislation_rule, "prohibition")
            prohibitions.append(prohibition)
        elif "require" in rule_description or "must" in rule_description or "obligation" in event_type:
            # Create obligation  
            obligation = self._create_odrl_rule(legislation_rule, "obligation")
            obligations.append(obligation)
        else:
            # Default to permission with constraints
            permission = self._create_odrl_rule(legislation_rule, "permission")
            permissions.append(permission)
        
        return ODRLPolicy(
            uid=policy_uid,
            permission=permissions,
            prohibition=prohibitions,
            obligation=obligations,
            profile=f"urn:profile:legislation:{legislation_rule.source_file}"
        )
    
    def _create_odrl_rule(self, legislation_rule: "LegislationRule", rule_type: str) -> Dict[str, Any]:
        """Create individual ODRL rule from legislation rule."""
        
        # Determine target asset
        target = f"urn:asset:{legislation_rule.source_file}:{legislation_rule.id}"
        
        # Determine action based on data domains
        actions = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                for domain in condition.data_domain:
                    if domain == DataDomain.DATA_TRANSFER:
                        actions.append("transfer")
                    elif domain == DataDomain.DATA_USAGE:
                        actions.append("use")
                    elif domain == DataDomain.DATA_STORAGE:
                        actions.append("store")
        
        if not actions:
            actions = ["use"]  # Default action
        
        # Create constraints from rule conditions
        constraints = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                odrl_constraint = {
                    "leftOperand": self._map_fact_to_odrl_operand(condition.fact),
                    "operator": self._map_operator_to_odrl(condition.operator),
                    "rightOperand": condition.value,
                    "comment": condition.description
                }
                constraints.append(odrl_constraint)
        
        # Determine assigner/assignee based on roles
        assigner = None
        assignee = None
        
        if legislation_rule.primary_impacted_role == DataRole.CONTROLLER:
            assignee = "urn:party:data_controller"
        elif legislation_rule.primary_impacted_role == DataRole.PROCESSOR:
            assignee = "urn:party:data_processor"
        
        rule = {
            "target": target,
            "action": actions[0] if len(actions) == 1 else actions,
            "constraint": constraints
        }
        
        if assignee:
            rule["assignee"] = assignee
        if assigner:
            rule["assigner"] = assigner
            
        return rule
    
    def _map_fact_to_odrl_operand(self, fact: str) -> str:
        """Map JSON rules engine fact to ODRL left operand."""
        fact_lower = fact.lower()
        
        if "date" in fact_lower or "time" in fact_lower:
            return "dateTime"
        elif "location" in fact_lower or "country" in fact_lower:
            return "spatial"
        elif "count" in fact_lower or "number" in fact_lower:
            return "count"
        elif "user" in fact_lower or "party" in fact_lower:
            return "party"
        else:
            return f"custom:{fact}"
    
    def _map_operator_to_odrl(self, operator: "ConditionOperator") -> str:
        """Map JSON rules engine operator to ODRL operator."""
        mapping = {
            ConditionOperator.EQUAL: "eq",
            ConditionOperator.NOT_EQUAL: "neq",
            ConditionOperator.GREATER_THAN: "gt", 
            ConditionOperator.LESS_THAN: "lt",
            ConditionOperator.GREATER_THAN_EQUAL: "gteq",
            ConditionOperator.LESS_THAN_EQUAL: "lteq",
            ConditionOperator.CONTAINS: "isA",
            ConditionOperator.NOT_CONTAINS: "isNotA",
            ConditionOperator.IN: "isPartOf",
            ConditionOperator.NOT_IN: "isNotPartOf"
        }
        return mapping.get(operator, "eq")
    
    def create_odre_enforceable(self, odrl_policy: ODRLPolicy) -> Dict[str, Any]:
        """Convert ODRL policy to ODRE enforceable format."""
        return self.odre_framework.create_enforceable_policy(odrl_policy)

# ===============================
# ORIGINAL DATA MODELS
# ===============================

class DataDomain(str, Enum):
    """Data domains as per privacy regulations."""
    DATA_TRANSFER = "data_transfer"
    DATA_USAGE = "data_usage" 
    DATA_STORAGE = "data_storage"

class DataRole(str, Enum):
    """Roles in data processing."""
    CONTROLLER = "controller"
    PROCESSOR = "processor"
    JOINT_CONTROLLER = "joint_controller"

class DataCategory(str, Enum):
    """Categories of personal data."""
    PERSONAL_DATA = "personal_data"
    SENSITIVE_DATA = "sensitive_data"
    BIOMETRIC_DATA = "biometric_data"
    HEALTH_DATA = "health_data"
    FINANCIAL_DATA = "financial_data"
    LOCATION_DATA = "location_data"
    BEHAVIORAL_DATA = "behavioral_data"
    IDENTIFICATION_DATA = "identification_data"

class ConditionOperator(str, Enum):
    """Operators for rule conditions."""
    EQUAL = "equal"
    NOT_EQUAL = "notEqual"
    GREATER_THAN = "greaterThan"
    LESS_THAN = "lessThan"
    GREATER_THAN_EQUAL = "greaterThanInclusive"
    LESS_THAN_EQUAL = "lessThanInclusive"
    CONTAINS = "contains"
    NOT_CONTAINS = "doesNotContain"
    IN = "in"
    NOT_IN = "notIn"

class RuleCondition(BaseModel):
    """Individual condition within a rule."""
    model_config = ConfigDict(use_enum_values=True)
    
    fact: str = Field(..., description="The fact/data point to evaluate")
    operator: ConditionOperator = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    path: Optional[str] = Field(None, description="JSONPath to navigate nested objects")
    description: str = Field(..., description="Human-readable description of this condition")
    data_domain: List[DataDomain] = Field(..., description="Applicable data domains")
    role: DataRole = Field(..., description="Role this condition applies to")
    reasoning: str = Field(..., description="LLM reasoning for why this condition was extracted")

class RuleEvent(BaseModel):
    """Event triggered when rule conditions are met."""
    model_config = ConfigDict(use_enum_values=True)
    
    type: str = Field(..., description="Type of event/action")
    params: Dict[str, Any] = Field(default_factory=dict, description="Event parameters")

class LegislationRule(BaseModel):
    """Complete rule structure aligned with json-rules-engine format."""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str = Field(..., description="Unique rule identifier")
    name: str = Field(..., description="Rule name")
    description: str = Field(..., description="Human-readable rule description")
    source_article: str = Field(..., description="Source legislation article/section")
    source_file: str = Field(..., description="Source PDF filename")
    
    conditions: Dict[str, List[RuleCondition]] = Field(
        ..., 
        description="Rule conditions with 'all', 'any', or 'not' logic"
    )
    event: RuleEvent = Field(..., description="Event triggered when conditions are met")
    priority: int = Field(default=1, description="Rule priority (1-10)")
    
    # New required fields
    primary_impacted_role: DataRole = Field(..., description="Primary role most impacted by this rule")
    secondary_impacted_role: Optional[DataRole] = Field(None, description="Secondary role impacted by this rule")
    data_category: List[DataCategory] = Field(..., description="Categories of data this rule applies to")
    
    # Country metadata
    applicable_countries: List[str] = Field(default_factory=list, description="Countries where this rule applies")
    adequacy_countries: List[str] = Field(default_factory=list, description="Adequacy countries (optional)")
    
    # Metadata
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    extraction_method: str = Field(default="llm_analysis")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Extraction confidence")
    
    @field_validator('conditions')
    @classmethod
    def validate_conditions_structure(cls, v):
        """Ensure conditions follow json-rules-engine format."""
        valid_keys = {'all', 'any', 'not'}
        if not isinstance(v, dict) or not any(key in valid_keys for key in v.keys()):
            raise ValueError("Conditions must contain 'all', 'any', or 'not' keys")
        return v

class ExtractionResult(BaseModel):
    """Complete result of legislation analysis."""
    model_config = ConfigDict(use_enum_values=True, arbitrary_types_allowed=True)
    
    rules: List[LegislationRule] = Field(..., description="Extracted rules")
    summary: str = Field(..., description="Summary of extraction")
    total_rules: int = Field(..., description="Total number of rules extracted")
    processing_time: float = Field(..., description="Processing time in seconds")
    embeddings: Optional[List[List[float]]] = Field(None, description="Rule embeddings")
    
    # Standards-aligned outputs
    dpv_rules: List[DPVRule] = Field(default_factory=list, description="DPV-aligned rules")
    odrl_policies: List[ODRLPolicy] = Field(default_factory=list, description="ODRL policies")
    odre_enforceable: List[Dict[str, Any]] = Field(default_factory=list, description="ODRE enforceable policies")
    
    def save_json(self, filepath: str):
        """Save rules to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in self.rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
    
    def save_dpv_turtle(self, filepath: str):
        """Save DPV rules in Turtle RDF format."""
        turtle_content = self._generate_dpv_turtle()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(turtle_content)
    
    def save_dpv_jsonld(self, filepath: str):
        """Save DPV rules in JSON-LD format."""
        jsonld_content = self._generate_dpv_jsonld()
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(jsonld_content, f, indent=2, ensure_ascii=False)
    
    def save_odrl_policies(self, filepath: str):
        """Save ODRL policies to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                [policy.model_dump() for policy in self.odrl_policies],
                f,
                indent=2,
                default=str,
                ensure_ascii=False
            )
    
    def save_odre_enforceable(self, filepath: str):
        """Save ODRE enforceable policies to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                self.odre_enforceable,
                f,
                indent=2,
                default=str,
                ensure_ascii=False
            )
    
    def _generate_dpv_turtle(self) -> str:
        """Generate Turtle RDF representation of DPV rules."""
        prefixes = f"""
@prefix dpv: <{DPVConcepts.DPV}> .
@prefix dpv-pd: <{DPVConcepts.DPV_PD}> .
@prefix dpv-tech: <{DPVConcepts.DPV_TECH}> .
@prefix dpv-legal: <{DPVConcepts.DPV_LEGAL}> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

"""
        
        turtle_triples = []
        for dpv_rule in self.dpv_rules:
            rule_uri = f"<urn:rule:{dpv_rule.id}>"
            
            # Core rule definition
            turtle_triples.append(f"{rule_uri} rdf:type dpv:ProcessingActivity .")
            turtle_triples.append(f"{rule_uri} rdfs:label \"{dpv_rule.source_article}\" .")
            
            # Processing operations
            for processing in dpv_rule.hasProcessing:
                turtle_triples.append(f"{rule_uri} dpv:hasProcessing <{processing}> .")
            
            # Purposes
            for purpose in dpv_rule.hasPurpose:
                turtle_triples.append(f"{rule_uri} dpv:hasPurpose <{purpose}> .")
            
            # Personal data
            for data in dpv_rule.hasPersonalData:
                turtle_triples.append(f"{rule_uri} dpv:hasPersonalData <{data}> .")
            
            # Roles
            if dpv_rule.hasDataController:
                turtle_triples.append(f"{rule_uri} dpv:hasDataController <{dpv_rule.hasDataController}> .")
            if dpv_rule.hasDataProcessor:
                turtle_triples.append(f"{rule_uri} dpv:hasDataProcessor <{dpv_rule.hasDataProcessor}> .")
            
            # Legal basis
            if dpv_rule.hasLegalBasis:
                turtle_triples.append(f"{rule_uri} dpv:hasLegalBasis <{dpv_rule.hasLegalBasis}> .")
            
            # Locations
            for location in dpv_rule.hasLocation:
                turtle_triples.append(f"{rule_uri} dpv:hasLocation <{location}> .")
            
            # Metadata
            turtle_triples.append(f"{rule_uri} dpv:hasConfidenceScore \"{dpv_rule.confidence_score}\"^^xsd:float .")
            turtle_triples.append(f"{rule_uri} dpv:extractedAt \"{dpv_rule.extracted_at.isoformat()}\"^^xsd:dateTime .")
        
        return prefixes + "\n".join(turtle_triples) + "\n"
    
    def _generate_dpv_jsonld(self) -> Dict[str, Any]:
        """Generate JSON-LD representation of DPV rules."""
        context = {
            "@context": {
                "dpv": DPVConcepts.DPV,
                "dpv-pd": DPVConcepts.DPV_PD,
                "dpv-tech": DPVConcepts.DPV_TECH,
                "dpv-legal": DPVConcepts.DPV_LEGAL,
                "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
                "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
                "xsd": "http://www.w3.org/2001/XMLSchema#"
            }
        }
        
        graph = []
        for dpv_rule in self.dpv_rules:
            rule_jsonld = {
                "@id": f"urn:rule:{dpv_rule.id}",
                "@type": "dpv:ProcessingActivity",
                "rdfs:label": dpv_rule.source_article,
                "dpv:hasProcessing": [{"@id": uri} for uri in dpv_rule.hasProcessing],
                "dpv:hasPurpose": [{"@id": uri} for uri in dpv_rule.hasPurpose],
                "dpv:hasPersonalData": [{"@id": uri} for uri in dpv_rule.hasPersonalData],
                "dpv:hasLocation": [{"@id": uri} for uri in dpv_rule.hasLocation],
                "dpv:hasConfidenceScore": {
                    "@value": dpv_rule.confidence_score,
                    "@type": "xsd:float"
                },
                "dpv:extractedAt": {
                    "@value": dpv_rule.extracted_at.isoformat(),
                    "@type": "xsd:dateTime"
                }
            }
            
            if dpv_rule.hasDataController:
                rule_jsonld["dpv:hasDataController"] = {"@id": dpv_rule.hasDataController}
            if dpv_rule.hasDataProcessor:
                rule_jsonld["dpv:hasDataProcessor"] = {"@id": dpv_rule.hasDataProcessor}
            if dpv_rule.hasLegalBasis:
                rule_jsonld["dpv:hasLegalBasis"] = {"@id": dpv_rule.hasLegalBasis}
            
            graph.append(rule_jsonld)
        
        return {**context, "@graph": graph}
    
    def save_csv(self, filepath: str):
        """Save rules to CSV file with all JSON fields."""
        if not self.rules:
            return
            
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # Write comprehensive header with all fields
            headers = [
                'id', 'name', 'description', 'source_article', 'source_file',
                'primary_impacted_role', 'secondary_impacted_role', 'data_category',
                'applicable_countries', 'adequacy_countries',
                'conditions_logic_type', 'conditions_count', 'conditions_details',
                'event_type', 'event_params', 'priority', 'confidence_score',
                'extraction_method', 'extracted_at',
                'dpv_processing', 'dpv_purposes', 'dpv_data_types',
                'odrl_policy_uid', 'odrl_permissions', 'odrl_prohibitions', 'odrl_obligations'
            ]
            writer.writerow(headers)
            
            # Write data rows with all information
            for i, rule in enumerate(self.rules):
                # Process conditions into readable format
                conditions_details = []
                conditions_logic_types = []
                total_conditions = 0
                
                for logic_type, conditions in rule.conditions.items():
                    conditions_logic_types.append(logic_type)
                    total_conditions += len(conditions)
                    
                    for condition in conditions:
                        condition_detail = (
                            f"[{logic_type.upper()}] {condition.description} | "
                            f"Fact: {condition.fact} | Operator: {condition.operator.value} | "
                            f"Value: {condition.value} | Role: {condition.role.value} | "
                            f"Domains: {', '.join([d.value for d in condition.data_domain])} | "
                            f"Reasoning: {condition.reasoning}"
                        )
                        conditions_details.append(condition_detail)
                
                # Serialize event params
                event_params_str = json.dumps(rule.event.params) if rule.event.params else ""
                
                # Get DPV and ODRL information if available
                dpv_processing = ""
                dpv_purposes = ""
                dpv_data_types = ""
                odrl_policy_uid = ""
                odrl_permissions = 0
                odrl_prohibitions = 0
                odrl_obligations = 0
                
                if i < len(self.dpv_rules):
                    dpv_rule = self.dpv_rules[i]
                    dpv_processing = ", ".join([p.split('#')[-1] for p in dpv_rule.hasProcessing])
                    dpv_purposes = ", ".join([p.split('#')[-1] for p in dpv_rule.hasPurpose])
                    dpv_data_types = ", ".join([d.split('#')[-1] for d in dpv_rule.hasPersonalData])
                
                if i < len(self.odrl_policies):
                    odrl_policy = self.odrl_policies[i]
                    odrl_policy_uid = odrl_policy.uid
                    odrl_permissions = len(odrl_policy.permission)
                    odrl_prohibitions = len(odrl_policy.prohibition)
                    odrl_obligations = len(odrl_policy.obligation)
                
                writer.writerow([
                    rule.id,
                    rule.name,
                    rule.description,
                    rule.source_article,
                    rule.source_file,
                    rule.primary_impacted_role.value,
                    rule.secondary_impacted_role.value if rule.secondary_impacted_role else "",
                    ", ".join([cat.value for cat in rule.data_category]),
                    ", ".join(rule.applicable_countries),
                    ", ".join(rule.adequacy_countries),
                    "; ".join(conditions_logic_types),
                    total_conditions,
                    " || ".join(conditions_details),
                    rule.event.type,
                    event_params_str,
                    rule.priority,
                    rule.confidence_score,
                    rule.extraction_method,
                    rule.extracted_at.isoformat(),
                    dpv_processing,
                    dpv_purposes,
                    dpv_data_types,
                    odrl_policy_uid,
                    odrl_permissions,
                    odrl_prohibitions,
                    odrl_obligations
                ])

# ===============================
# METADATA MANAGEMENT
# ===============================

class MetadataManager:
    """Manages legislation metadata configuration."""
    
    def __init__(self, config_file: str = Config.METADATA_CONFIG_FILE):
        self.config_file = config_file
        self.metadata = {}
        self.load_metadata()
    
    def load_metadata(self):
        """Load metadata from config file."""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                logger.info(f"Loaded metadata for {len(self.metadata)} files")
            else:
                logger.info("No metadata config file found. Creating default structure.")
                self.create_default_config()
        except Exception as e:
            logger.error(f"Error loading metadata: {e}")
            self.metadata = {}
    
    def create_default_config(self):
        """Create a default metadata configuration file."""
        default_config = {
            "metadata_info": {
                "description": "This file contains metadata for legislation PDFs",
                "format": {
                    "filename.pdf": {
                        "applicable_countries": ["Country1", "Country2"],
                        "adequacy_countries": ["Country3", "Country4"]
                    }
                }
            },
            "example_files": {
                "gdpr_article_28.pdf": {
                    "applicable_countries": ["Germany", "France", "Italy", "Spain", "Netherlands"],
                    "adequacy_countries": ["Canada", "Japan", "United Kingdom", "Switzerland"]
                },
                "ccpa_regulation.pdf": {
                    "applicable_countries": ["United States", "California"],
                    "adequacy_countries": []
                }
            }
        }
        
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"Created default metadata config at: {self.config_file}")
        except Exception as e:
            logger.error(f"Error creating default config: {e}")
    
    def get_file_metadata(self, filename: str) -> Dict[str, List[str]]:
        """Get metadata for a specific PDF file."""
        # Check in example_files and other top-level keys
        for section in self.metadata.values():
            if isinstance(section, dict) and filename in section:
                file_meta = section[filename]
                return {
                    'applicable_countries': file_meta.get('applicable_countries', []),
                    'adequacy_countries': file_meta.get('adequacy_countries', [])
                }
        
        # Return empty lists if no metadata found
        logger.warning(f"No metadata found for {filename}, using empty country lists")
        return {
            'applicable_countries': [],
            'adequacy_countries': []
        }
    
    def add_file_metadata(self, filename: str, applicable_countries: List[str], adequacy_countries: List[str] = None):
        """Add metadata for a new file."""
        if adequacy_countries is None:
            adequacy_countries = []
        
        # Add to example_files section or create new section
        if 'example_files' not in self.metadata:
            self.metadata['example_files'] = {}
        
        self.metadata['example_files'][filename] = {
            'applicable_countries': applicable_countries,
            'adequacy_countries': adequacy_countries
        }
        
        self.save_metadata()
    
    def save_metadata(self):
        """Save metadata to config file."""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
            logger.info("Metadata config saved successfully")
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
    
    def list_configured_files(self) -> List[str]:
        """Get list of files that have metadata configured."""
        configured_files = []
        for section in self.metadata.values():
            if isinstance(section, dict):
                for key in section.keys():
                    if key.endswith('.pdf'):
                        configured_files.append(key)
        return configured_files

# ===============================
# PDF READER
# ===============================

class PDFReader:
    """PDF reader for extracting text from legislation documents."""
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str) -> str:
        """Extract text from PDF file."""
        if not PDF_AVAILABLE:
            raise ImportError("No PDF library available. Install PyMuPDF or pdfplumber")
        
        try:
            # Try modern PyMuPDF first
            if 'pymupdf' in globals():
                return PDFReader._extract_with_pymupdf(pdf_path)
            else:
                return PDFReader._extract_with_pdfplumber(pdf_path)
        except Exception as e:
            logger.error(f"Error reading PDF {pdf_path}: {e}")
            raise
    
    @staticmethod
    def _extract_with_pymupdf(pdf_path: str) -> str:
        """Extract text using modern PyMuPDF with context manager."""
        text = ""
        try:
            # Use modern PyMuPDF API with context manager (best practice)
            with pymupdf.open(pdf_path) as doc:
                for page in doc:
                    page_text = page.get_text()
                    if page_text:
                        text += page_text + "\n"
            # Document is automatically closed when exiting context manager
        except Exception as e:
            logger.error(f"PyMuPDF extraction failed: {e}")
            raise
        return text
    
    @staticmethod
    def _extract_with_pdfplumber(pdf_path: str) -> str:
        """Extract text using pdfplumber."""
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    
    @staticmethod
    def get_pdf_files(directory: str) -> List[str]:
        """Get all PDF files from directory."""
        pdf_pattern = os.path.join(directory, "*.pdf")
        return glob.glob(pdf_pattern)

# ===============================
# RULE MANAGEMENT SYSTEM
# ===============================

class RuleManager:
    """Manages existing rules and provides context for new extractions."""
    
    def __init__(self, rules_file: str = Config.EXISTING_RULES_FILE):
        self.rules_file = rules_file
        self.existing_rules: List[LegislationRule] = []
        self.load_existing_rules()
    
    def load_existing_rules(self):
        """Load existing rules from file."""
        try:
            if os.path.exists(self.rules_file):
                with open(self.rules_file, 'r', encoding='utf-8') as f:
                    rules_data = json.load(f)
                
                for rule_data in rules_data:
                    try:
                        rule = LegislationRule(**rule_data)
                        self.existing_rules.append(rule)
                    except Exception as e:
                        logger.warning(f"Skipping invalid existing rule: {e}")
                
                logger.info(f"Loaded {len(self.existing_rules)} existing rules")
            else:
                logger.info("No existing rules file found. Starting fresh.")
        except Exception as e:
            logger.error(f"Error loading existing rules: {e}")
            self.existing_rules = []
    
    def save_rules(self, new_rules: List[LegislationRule]):
        """Save new rules, appending to existing ones."""
        # Combine existing and new rules
        all_rules = self.existing_rules + new_rules
        
        # Remove duplicates based on ID
        unique_rules = []
        seen_ids = set()
        for rule in all_rules:
            if rule.id not in seen_ids:
                unique_rules.append(rule)
                seen_ids.add(rule.id)
        
        # Save to file
        os.makedirs(os.path.dirname(self.rules_file), exist_ok=True)
        with open(self.rules_file, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in unique_rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
        
        # Update internal state
        self.existing_rules = unique_rules
        logger.info(f"Saved {len(unique_rules)} total rules ({len(new_rules)} new)")
    
    def get_context_summary(self) -> str:
        """Get a summary of existing rules for context."""
        if not self.existing_rules:
            return "No existing rules found."
        
        summary = f"Existing Rules Context ({len(self.existing_rules)} rules):\n\n"
        
        # Group by source
        sources = {}
        for rule in self.existing_rules:
            source = rule.source_article
            if source not in sources:
                sources[source] = []
            sources[source].append(rule)
        
        for source, rules in sources.items():
            summary += f"Source: {source} ({len(rules)} rules)\n"
            for rule in rules[:3]:  # Show first 3 rules per source
                summary += f"  - {rule.name}: {rule.description[:100]}...\n"
            if len(rules) > 3:
                summary += f"  ... and {len(rules) - 3} more rules\n"
            summary += "\n"
        
        return summary
    
    def get_processed_files(self) -> List[str]:
        """Get list of already processed PDF files."""
        processed = set()
        for rule in self.existing_rules:
            if hasattr(rule, 'source_file'):
                processed.add(rule.source_file)
        return list(processed)

# ===============================
# ADVANCED PROMPTING STRATEGIES
# ===============================

class PromptingStrategies:
    """Advanced prompting strategies for rule extraction."""
    
    @staticmethod
    def chain_of_thought_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Chain of Thought prompting for step-by-step reasoning."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        You are an expert legal analyst specializing in converting legislation into machine-readable rules.
        {context_section}
        Analyze the following legislation text step by step:
        
        LEGISLATION TEXT:
        {legislation_text}
        
        CHAIN OF THOUGHT ANALYSIS:
        
        Step 1: Identify Key Legal Obligations
        - What are the main obligations stated in this text?
        - Who has these obligations (controller, processor, joint_controller)?
        
        Step 2: Extract Conditional Logic
        - What conditions trigger these obligations?
        - Are there any "if-then" relationships?
        - What are the specific criteria that must be met?
        
        Step 3: Determine Data Domains
        - Does this relate to data_transfer, data_usage, or data_storage?
        - Which specific data activities are covered?
        
        Step 4: Identify Roles and Data Categories
        - Who are the key actors (controller, processor, joint_controller)?
        - What is the primary impacted role and any secondary impacted role?
        - What data categories are involved (personal_data, sensitive_data, biometric_data, health_data, financial_data, location_data, behavioral_data, identification_data)?
        
        Step 5: Structure as Machine-Readable Rules
        - Convert each obligation into a conditional rule
        - Define clear facts, operators, and values
        - Ensure alignment with json-rules-engine format
        - Consider existing rules to maintain consistency
        
        Let's work through this step by step...
        """
    
    @staticmethod
    def mixture_of_experts_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Experts prompting with specialized perspectives."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        We need to analyze legislation from multiple expert perspectives. Each expert will contribute their specialized knowledge.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === EXPERT PANEL ANALYSIS ===
        
        EXPERT 1 - PRIVACY LAW SPECIALIST:
        As a privacy law expert, I will focus on:
        - Data protection obligations and rights
        - Cross-border transfer requirements
        - Consent and lawful basis considerations
        - Individual rights and freedoms
        - Primary and secondary impacted roles
        
        EXPERT 2 - TECHNICAL COMPLIANCE SPECIALIST:
        As a technical expert, I will focus on:
        - Technical and organizational measures
        - Security requirements and safeguards
        - Data processing procedures and controls
        - Risk assessment and mitigation
        - Data categories and classification
        
        EXPERT 3 - REGULATORY INTERPRETATION SPECIALIST:
        As a regulatory expert, I will focus on:
        - Supervisory authority requirements
        - Penalty and enforcement mechanisms
        - Compliance documentation needs
        - Audit and accountability measures
        - Consistency with existing regulatory framework
        
        Each expert will identify different aspects of the legislation and contribute to a comprehensive rule extraction that builds upon existing rules.
        
        Now, let each expert analyze the text and provide their perspective...
        """
    
    @staticmethod
    def mixture_of_thought_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Thought prompting for diverse reasoning approaches."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        Apply multiple thinking approaches to analyze this legislation comprehensively.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === MULTIPLE THINKING APPROACHES ===
        
        ANALYTICAL THINKING:
        - Break down the text into component obligations
        - Identify logical relationships and dependencies
        - Create systematic categorization of requirements
        - Analyze consistency with existing rules
        
        CREATIVE THINKING:
        - Consider edge cases and alternative interpretations
        - Think about practical implementation scenarios
        - Explore different ways obligations could be triggered
        - Identify gaps in existing rule coverage
        
        CRITICAL THINKING:
        - Question assumptions and implicit requirements
        - Evaluate the necessity and sufficiency of conditions
        - Consider potential conflicts or ambiguities
        - Assess impact on different roles and data categories
        
        PRACTICAL THINKING:
        - Focus on real-world application and compliance
        - Consider operational feasibility and implementation
        - Think about monitoring and enforcement mechanisms
        - Evaluate primary vs secondary role impacts
        
        SYSTEMATIC THINKING:
        - Consider the broader regulatory framework
        - Understand interconnections with other provisions
        - Map relationships between different roles and responsibilities
        - Ensure consistency with existing rule patterns
        
        Apply each thinking approach to extract comprehensive, actionable rules...
        """
    
    @staticmethod
    def mixture_of_reasoning_prompt(legislation_text: str, existing_context: str = "") -> str:
        """Mixture of Reasoning prompting for comprehensive analysis."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        
        return f"""
        Use multiple reasoning strategies to thoroughly analyze this legislation.
        {context_section}
        LEGISLATION TEXT:
        {legislation_text}
        
        === REASONING STRATEGIES ===
        
        DEDUCTIVE REASONING:
        - Start with general legal principles
        - Apply them to specific provisions
        - Derive specific obligations and requirements
        - Maintain consistency with established rule patterns
        
        INDUCTIVE REASONING:
        - Examine specific examples and cases mentioned
        - Identify patterns and common elements
        - Generalize to broader rules and principles
        - Learn from existing rule structures
        
        ABDUCTIVE REASONING:
        - Observe the intended outcomes and goals
        - Infer the most likely requirements to achieve them
        - Hypothesize necessary conditions and controls
        - Consider primary and secondary role impacts
        
        ANALOGICAL REASONING:
        - Compare to similar regulations and provisions
        - Draw parallels from established legal frameworks
        - Apply proven compliance patterns
        - Leverage existing rule precedents
        
        CAUSAL REASONING:
        - Identify cause-and-effect relationships
        - Map triggers to required actions
        - Understand consequences of non-compliance
        - Analyze data category implications
        
        Apply each reasoning strategy to extract precise, enforceable rules that complement existing ones...
        """

# ===============================
# OPENAI CLIENT AND EMBEDDINGS
# ===============================

class OpenAIService:
    """Service for OpenAI API interactions."""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
    
    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI's text-embedding-3-large model."""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=texts,
                encoding_format="float"
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Union[Dict[str, str], SystemMessage, HumanMessage, AIMessage]]) -> str:
        """Generate chat completion using OpenAI's API."""
        try:
            # Convert LangChain messages to dict format for OpenAI API
            formatted_messages = []
            for msg in messages:
                if isinstance(msg, (SystemMessage, HumanMessage, AIMessage)):
                    if isinstance(msg, SystemMessage):
                        formatted_messages.append({"role": "system", "content": msg.content})
                    elif isinstance(msg, HumanMessage):
                        formatted_messages.append({"role": "user", "content": msg.content})
                    elif isinstance(msg, AIMessage):
                        formatted_messages.append({"role": "assistant", "content": msg.content})
                elif isinstance(msg, dict):
                    formatted_messages.append(msg)
                else:
                    formatted_messages.append({"role": "user", "content": str(msg)})
            
            response = self.client.chat.completions.create(
                model=Config.CHAT_MODEL,
                messages=formatted_messages
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            raise

# ===============================
# SAFE JSON PARSING
# ===============================

class SafeJsonParser:
    """Safe JSON parsing with error handling and validation."""
    
    @staticmethod
    def parse_json_response(response: str) -> Dict[str, Any]:
        """Safely parse JSON response from LLM."""
        try:
            # Clean the response
            cleaned = response.strip()
            
            # Handle code blocks
            if "```json" in cleaned:
                start = cleaned.find("```json") + 7
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            elif "```" in cleaned:
                start = cleaned.find("```") + 3
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            
            # Try to parse JSON
            parsed = json.loads(cleaned)
            return parsed
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON decode error: {e}. Attempting to fix...")
            
            # Try to fix common JSON issues
            try:
                # Remove trailing commas
                import re
                fixed = re.sub(r',(\s*[}\]])', r'\1', cleaned)
                parsed = json.loads(fixed)
                return parsed
            except Exception:
                logger.error(f"Could not parse JSON response: {cleaned[:200]}...")
                return {"error": "Failed to parse JSON", "raw_response": cleaned}
    
    @staticmethod
    def validate_rule_structure(data: Dict[str, Any]) -> bool:
        """Validate that parsed data follows expected rule structure."""
        required_fields = ['id', 'name', 'description', 'conditions', 'event']
        return all(field in data for field in required_fields)

# ===============================
# LANGGRAPH TOOLS
# ===============================

@tool
def extract_rule_conditions(legislation_text: str, focus_area: str) -> str:
    """Extract specific rule conditions from legislation text."""
    
    prompt = f"""
    Extract specific rule conditions from the following legislation text, focusing on {focus_area}.
    
    Return a JSON object with conditions in json-rules-engine format.
    
    Text: {legislation_text}
    
    Focus on identifying:
    - Specific facts that can be evaluated
    - Comparison operators (equal, greaterThan, contains, etc.)
    - Values to compare against
    - Data domains (data_transfer, data_usage, data_storage) and roles (controller, processor, joint_controller)
    
    Return valid JSON only.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error extracting conditions: {str(e)}"

@tool
def analyze_data_domains(legislation_text: str) -> str:
    """Analyze and identify relevant data domains in legislation."""
    
    prompt = f"""
    Analyze the following legislation text and identify which data domains are relevant:
    - data_transfer
    - data_usage
    - data_storage
    
    Text: {legislation_text}
    
    Return a JSON object mapping each identified domain to its relevance and reasoning.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error analyzing domains: {str(e)}"

@tool
def identify_roles_responsibilities(legislation_text: str) -> str:
    """Identify roles and responsibilities in legislation."""
    
    prompt = f"""
    Identify the roles and responsibilities mentioned in this legislation:
    - controller
    - processor 
    - joint_controller
    
    Text: {legislation_text}
    
    For each role, identify their specific obligations and responsibilities.
    Return a JSON object with role mappings.
    """
    
    try:
        # Create OpenAI client with proper configuration
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error identifying roles: {str(e)}"

# ===============================
# MAIN LEGISLATION ANALYZER
# ===============================

class LegislationAnalyzer:
    """Main analyzer for converting legislation to machine-readable rules."""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.json_parser = SafeJsonParser()
        self.rule_manager = RuleManager()
        self.pdf_reader = PDFReader()
        self.metadata_manager = MetadataManager()
        self.standards_converter = StandardsConverter()
        
        # Initialize LangChain model
        self.llm = ChatOpenAI(
            model=Config.CHAT_MODEL,
            openai_api_key=Config.API_KEY,
            openai_api_base=Config.BASE_URL
        )
        
        # Create react agent with tools
        self.tools = [
            extract_rule_conditions,
            analyze_data_domains, 
            identify_roles_responsibilities
        ]
        
        # Memory for conversation state
        self.memory = MemorySaver()
        
        # Create react agent
        self.agent = create_react_agent(
            self.llm,
            self.tools,
            checkpointer=self.memory
        )
    
    async def process_legislation_folder(self, folder_path: str = None) -> ExtractionResult:
        """Process all PDF files in the legislation folder."""
        if folder_path is None:
            folder_path = Config.LEGISLATION_PDF_PATH
        
        # Ensure folder exists
        os.makedirs(folder_path, exist_ok=True)
        
        # Get PDF files
        pdf_files = self.pdf_reader.get_pdf_files(folder_path)
        
        if not pdf_files:
            logger.warning(f"No PDF files found in {folder_path}")
            return ExtractionResult(
                rules=[],
                summary="No PDF files to process",
                total_rules=0,
                processing_time=0.0
            )
        
        # Get already processed files
        processed_files = self.rule_manager.get_processed_files()
        
        # Filter to only new files
        new_files = [f for f in pdf_files if os.path.basename(f) not in processed_files]
        
        if not new_files:
            logger.info("All PDF files have already been processed")
            return ExtractionResult(
                rules=[],
                summary="All files already processed",
                total_rules=0,
                processing_time=0.0
            )
        
        logger.info(f"Processing {len(new_files)} new PDF files")
        
        all_new_rules = []
        start_time = datetime.utcnow()
        
        for pdf_file in new_files:
            try:
                logger.info(f"Processing: {os.path.basename(pdf_file)}")
                
                # Extract text from PDF
                text = self.pdf_reader.extract_text_from_pdf(pdf_file)
                
                # Get metadata for this file
                filename = os.path.basename(pdf_file)
                file_metadata = self.metadata_manager.get_file_metadata(filename)
                
                # Analyze the legislation
                result = await self.analyze_legislation(
                    legislation_text=text,
                    article_reference=f"Document: {filename}",
                    source_file=filename,
                    applicable_countries=file_metadata['applicable_countries'],
                    adequacy_countries=file_metadata['adequacy_countries']
                )
                
                all_new_rules.extend(result.rules)
                
            except Exception as e:
                logger.error(f"Error processing {pdf_file}: {e}")
                continue
        
        # Calculate total processing time
        end_time = datetime.utcnow()
        total_processing_time = (end_time - start_time).total_seconds()
        
        # Generate embeddings for all new rules
        if all_new_rules:
            rule_texts = [f"{rule.description} {rule.source_article}" for rule in all_new_rules]
            embeddings = await self.openai_service.get_embeddings(rule_texts)
        else:
            embeddings = []
        
        # Save new rules
        if all_new_rules:
            self.rule_manager.save_rules(all_new_rules)
        
        # Create result
        result = ExtractionResult(
            rules=all_new_rules,
            summary=f"Processed {len(new_files)} PDF files, extracted {len(all_new_rules)} new rules",
            total_rules=len(all_new_rules),
            processing_time=total_processing_time,
            embeddings=embeddings
        )
        
        return result
    
    async def analyze_legislation(
        self, 
        legislation_text: str, 
        article_reference: str = "", 
        source_file: str = "",
        applicable_countries: List[str] = None,
        adequacy_countries: List[str] = None
    ) -> ExtractionResult:
        """Analyze legislation text and extract machine-readable rules."""
        start_time = datetime.utcnow()
        
        # Default to empty lists if not provided
        if applicable_countries is None:
            applicable_countries = []
        if adequacy_countries is None:
            adequacy_countries = []
        
        try:
            logger.info(f"Starting analysis of legislation: {article_reference}")
            logger.info(f"Applicable countries: {applicable_countries}")
            logger.info(f"Adequacy countries: {adequacy_countries}")
            
            # Get existing rules context
            existing_context = self.rule_manager.get_context_summary()
            
            # Create metadata context for LLM
            metadata_context = f"""
            LEGISLATION METADATA:
            - Applicable Countries: {', '.join(applicable_countries) if applicable_countries else 'Not specified'}
            - Adequacy Countries: {', '.join(adequacy_countries) if adequacy_countries else 'None specified'}
            """
            
            # Step 1: Apply advanced prompting strategies with context
            cot_analysis = await self._apply_chain_of_thought(legislation_text, existing_context + metadata_context)
            moe_analysis = await self._apply_mixture_of_experts(legislation_text, existing_context + metadata_context)
            mot_analysis = await self._apply_mixture_of_thought(legislation_text, existing_context + metadata_context)
            mor_analysis = await self._apply_mixture_of_reasoning(legislation_text, existing_context + metadata_context)
            
            # Step 2: Use react agent for comprehensive analysis
            agent_analysis = await self._run_react_agent(legislation_text, article_reference)
            
            # Step 3: Synthesize all analyses into structured rules
            rules = await self._synthesize_rules(
                legislation_text, 
                article_reference,
                source_file,
                existing_context,
                metadata_context,
                applicable_countries,
                adequacy_countries,
                cot_analysis,
                moe_analysis, 
                mot_analysis,
                mor_analysis,
                agent_analysis
            )
            
            # Step 4: Generate embeddings for rules
            if rules:
                rule_texts = [f"{rule.description} {rule.source_article}" for rule in rules]
                embeddings = await self.openai_service.get_embeddings(rule_texts)
            else:
                embeddings = []
            
            # Step 5: Convert to DPV and ODRL formats
            dpv_rules = []
            odrl_policies = []
            odre_enforceable = []
            
            for rule in rules:
                try:
                    # Convert to DPV
                    dpv_rule = self.standards_converter.json_rules_to_dpv(rule)
                    dpv_rules.append(dpv_rule)
                    
                    # Convert to ODRL
                    odrl_policy = self.standards_converter.json_rules_to_odrl(rule)
                    odrl_policies.append(odrl_policy)
                    
                    # Convert to ODRE enforceable
                    odre_policy = self.standards_converter.create_odre_enforceable(odrl_policy)
                    odre_enforceable.append(odre_policy)
                    
                except Exception as e:
                    logger.warning(f"Error converting rule {rule.id} to standards formats: {e}")
                    continue
            
            logger.info(f"Converted {len(dpv_rules)} rules to DPV format")
            logger.info(f"Converted {len(odrl_policies)} rules to ODRL format")
            logger.info(f"Created {len(odre_enforceable)} ODRE enforceable policies")
            
            # Step 6: Calculate processing time
            end_time = datetime.utcnow()
            processing_time = (end_time - start_time).total_seconds()
            
            # Create result with standards-aligned outputs
            result = ExtractionResult(
                rules=rules,
                summary=f"Extracted {len(rules)} rules from {article_reference}",
                total_rules=len(rules),
                processing_time=processing_time,
                embeddings=embeddings,
                dpv_rules=dpv_rules,
                odrl_policies=odrl_policies,
                odre_enforceable=odre_enforceable
            )
            
            logger.info(f"Analysis completed: {len(rules)} rules extracted in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing legislation: {e}")
            raise
    
    async def _apply_chain_of_thought(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Chain of Thought prompting strategy."""
        prompt = PromptingStrategies.chain_of_thought_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="You are an expert legal analyst. Use step-by-step reasoning."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_experts(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Experts prompting strategy."""
        prompt = PromptingStrategies.mixture_of_experts_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="You are a panel of legal experts with different specializations."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_thought(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Thought prompting strategy."""
        prompt = PromptingStrategies.mixture_of_thought_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="Apply diverse thinking approaches to comprehensive analysis."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_mixture_of_reasoning(self, legislation_text: str, existing_context: str = "") -> str:
        """Apply Mixture of Reasoning prompting strategy.""" 
        prompt = PromptingStrategies.mixture_of_reasoning_prompt(legislation_text, existing_context)
        
        messages = [
            SystemMessage(content="Use multiple reasoning strategies for thorough analysis."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _run_react_agent(self, legislation_text: str, article_reference: str) -> str:
        """Run the react agent for comprehensive analysis."""
        try:
            config = {"configurable": {"thread_id": f"analysis_{datetime.utcnow().timestamp()}"}}
            
            message = f"""
            Analyze the following legislation and use all available tools to extract comprehensive information:
            
            Article: {article_reference}
            Text: {legislation_text}
            
            Use the tools to:
            1. Extract specific rule conditions
            2. Analyze data domains
            3. Identify roles and responsibilities
            
            Provide a comprehensive analysis that can be used to create machine-readable rules.
            """
            
            result = self.agent.invoke(
                {"messages": [HumanMessage(content=message)]},
                config
            )
            
            # Extract the final message content
            if result and "messages" in result:
                last_message = result["messages"][-1]
                if hasattr(last_message, 'content'):
                    return last_message.content
                elif isinstance(last_message, dict) and 'content' in last_message:
                    return last_message['content']
            
            return "Agent analysis completed but no content returned"
            
        except Exception as e:
            logger.error(f"Error running react agent: {e}")
            return f"Error in agent analysis: {str(e)}"
    
    async def _synthesize_rules(
        self, 
        legislation_text: str,
        article_reference: str,
        source_file: str,
        existing_context: str,
        metadata_context: str,
        applicable_countries: List[str],
        adequacy_countries: List[str],
        cot_analysis: str,
        moe_analysis: str, 
        mot_analysis: str,
        mor_analysis: str,
        agent_analysis: str
    ) -> List[LegislationRule]:
        """Synthesize all analyses into structured rules."""
        
        synthesis_prompt = f"""
        Based on the comprehensive analyses below, create machine-readable rules in JSON format that align with json-rules-engine structure.
        
        EXISTING RULES CONTEXT:
        {existing_context}
        
        METADATA CONTEXT:
        {metadata_context}
        
        ORIGINAL LEGISLATION:
        Article: {article_reference}
        Source File: {source_file}
        Text: {legislation_text}
        
        ANALYSIS RESULTS:
        
        Chain of Thought Analysis:
        {cot_analysis}
        
        Mixture of Experts Analysis:
        {moe_analysis}
        
        Mixture of Thought Analysis:
        {mot_analysis}
        
        Mixture of Reasoning Analysis:
        {mor_analysis}
        
        Agent Tool Analysis:
        {agent_analysis}
        
        REQUIREMENTS:
        1. Create rules in json-rules-engine format with conditions containing 'all', 'any', or 'not' keys
        2. Each condition MUST have ALL these required fields:
           - fact: string (required)
           - operator: one of "equal", "notEqual", "greaterThan", "lessThan", "greaterThanInclusive", "lessThanInclusive", "contains", "doesNotContain", "in", "notIn"
           - value: any (required)
           - description: string (required)
           - data_domain: array of "data_transfer", "data_usage", or "data_storage" (required)
           - role: one of "controller", "processor", "joint_controller" (required)
           - reasoning: string explaining why this condition was extracted (required)
           - path: string (optional, for JSONPath navigation)
        3. Determine primary_impacted_role and secondary_impacted_role (optional)
        4. Identify data_category from: personal_data, sensitive_data, biometric_data, health_data, financial_data, location_data, behavioral_data, identification_data
        5. Use the provided country metadata for applicable_countries and adequacy_countries
        6. Use LLM reasoning capabilities to infer domains, roles, and categories not explicitly stated
        7. Provide confidence scores (0.0-1.0) for each rule
        8. Consider existing rules context to maintain consistency and avoid duplication
        9. ALL CONDITIONS MUST HAVE ALL REQUIRED FIELDS - NO MISSING FIELDS ALLOWED
        
        Return a JSON array of rules. Each rule must follow this exact structure with NO MISSING REQUIRED FIELDS:
        {{
            "id": "unique_id_based_on_content",
            "name": "rule_name", 
            "description": "human_readable_description",
            "source_article": "{article_reference}",
            "source_file": "{source_file}",
            "conditions": {{
                "all": [
                    {{
                        "fact": "specific_fact_to_evaluate",
                        "operator": "equal",
                        "value": "comparison_value",
                        "path": "$.optional.json.path",
                        "description": "clear_description_of_condition",
                        "data_domain": ["data_transfer"],
                        "role": "controller",
                        "reasoning": "detailed_reasoning_for_extraction"
                    }}
                ]
            }},
            "event": {{
                "type": "compliance_required",
                "params": {{
                    "action": "specific_action_required"
                }}
            }},
            "priority": 1,
            "primary_impacted_role": "controller",
            "secondary_impacted_role": "processor",
            "data_category": ["personal_data", "sensitive_data"],
            "applicable_countries": {json.dumps(applicable_countries)},
            "adequacy_countries": {json.dumps(adequacy_countries)},
            "confidence_score": 0.85
        }}
        
        CRITICAL: Every condition object MUST contain all required fields. Double-check that each condition has:
        - fact (string)
        - operator (valid enum value)
        - value (any type)
        - description (string)
        - data_domain (array with valid values)
        - role (valid enum value)
        - reasoning (string)
        
        Return ONLY valid JSON array, no other text.
        """
        
        messages = [
            SystemMessage(content="You are a legal-tech expert. Return only valid JSON with complete conditions."),
            HumanMessage(content=synthesis_prompt)
        ]
        
        response = await self.openai_service.chat_completion(messages)
        
        # Parse JSON response safely
        parsed_data = self.json_parser.parse_json_response(response)
        
        if "error" in parsed_data:
            logger.error(f"Failed to parse rules JSON: {parsed_data}")
            return []
        
        # Convert to Pydantic models
        rules = []
        try:
            if isinstance(parsed_data, list):
                rule_data_list = parsed_data
            elif isinstance(parsed_data, dict) and "rules" in parsed_data:
                rule_data_list = parsed_data["rules"]
            else:
                rule_data_list = [parsed_data]
            
            for rule_data in rule_data_list:
                try:
                    # Ensure required fields have defaults
                    rule_data.setdefault("priority", 1)
                    rule_data.setdefault("confidence_score", 0.8)
                    rule_data.setdefault("source_article", article_reference)
                    rule_data.setdefault("source_file", source_file)
                    rule_data.setdefault("primary_impacted_role", "controller")
                    rule_data.setdefault("data_category", ["personal_data"])
                    rule_data.setdefault("applicable_countries", applicable_countries)
                    rule_data.setdefault("adequacy_countries", adequacy_countries)
                    
                    # Validate conditions structure before creating rule
                    if "conditions" in rule_data:
                        for logic_type, conditions in rule_data["conditions"].items():
                            for i, condition in enumerate(conditions):
                                # Ensure all required fields exist with proper defaults
                                condition.setdefault("fact", f"default_fact_{i}")
                                condition.setdefault("operator", "equal")
                                condition.setdefault("value", "default_value")
                                condition.setdefault("description", "Default condition description")
                                condition.setdefault("data_domain", ["data_usage"])
                                condition.setdefault("role", "controller")
                                condition.setdefault("reasoning", "Default reasoning for condition extraction")
                                
                                # Validate enum values
                                if condition["operator"] not in [e.value for e in ConditionOperator]:
                                    condition["operator"] = "equal"
                                if condition["role"] not in [e.value for e in DataRole]:
                                    condition["role"] = "controller"
                                
                                # Validate data_domain
                                valid_domains = [e.value for e in DataDomain]
                                condition["data_domain"] = [
                                    d for d in condition["data_domain"] 
                                    if d in valid_domains
                                ]
                                if not condition["data_domain"]:
                                    condition["data_domain"] = ["data_usage"]
                    
                    # Create rule
                    rule = LegislationRule(**rule_data)
                    rules.append(rule)
                    
                except Exception as e:
                    logger.warning(f"Skipping invalid rule: {e}")
                    logger.debug(f"Rule data that failed: {rule_data}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error creating rule objects: {e}")
            
        return rules

# ===============================
# MAIN EXECUTION FUNCTION
# ===============================

async def main():
    """Main execution function demonstrating the system."""
    
    # Initialize analyzer
    analyzer = LegislationAnalyzer()
    
    try:
        print("\n=== LEGISLATION RULES CONVERTER ===")
        print("Processing legislation PDFs and extracting machine-readable rules...\n")
        
        # Show metadata configuration status
        print("📋 METADATA CONFIGURATION:")
        configured_files = analyzer.metadata_manager.list_configured_files()
        if configured_files:
            print(f"✅ Metadata configured for {len(configured_files)} files:")
            for file in configured_files:
                metadata = analyzer.metadata_manager.get_file_metadata(file)
                print(f"   📄 {file}")
                print(f"      🌍 Countries: {', '.join(metadata['applicable_countries']) if metadata['applicable_countries'] else 'None'}")
                print(f"      🤝 Adequacy: {', '.join(metadata['adequacy_countries']) if metadata['adequacy_countries'] else 'None'}")
        else:
            print("⚠️ No metadata configured yet.")
        
        print(f"📁 Metadata config file: {Config.METADATA_CONFIG_FILE}")
        print()
        
        # Check if PDF processing is available
        if not PDF_AVAILABLE:
            print("⚠️ Warning: PDF processing libraries not available.")
            print("Install with: pip install PyMuPDF pdfplumber")
            print("PyMuPDF is recommended for better performance and modern API.")
            
            # Fallback to example text processing
            print("Using example GDPR Article 28 for demonstration...\n")
            
            gdpr_article_28 = """
            Article 28 - Processor
            
            1. Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing sufficient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject.
            
            2. The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.
            
            3. Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller.
            """
            
            # Analyze the example legislation
            result = await analyzer.analyze_legislation(
                legislation_text=gdpr_article_28,
                article_reference="GDPR Article 28",
                source_file="example_gdpr_article_28.txt",
                applicable_countries=["Germany", "France", "Italy", "Spain"],
                adequacy_countries=["Canada", "Japan", "United Kingdom"]
            )
        else:
            # Process PDF folder
            print("🔍 Scanning for PDF files in legislation folder...")
            
            # Ensure directories exist
            os.makedirs(Config.LEGISLATION_PDF_PATH, exist_ok=True)
            
            # Check if there are PDFs to process
            pdf_files = analyzer.pdf_reader.get_pdf_files(Config.LEGISLATION_PDF_PATH)
            if not pdf_files:
                print(f"📁 No PDF files found in {Config.LEGISLATION_PDF_PATH}")
                print("Add PDF files to the folder and run again.")
                
                # Create a sample with example text for demonstration
                print("\nUsing example GDPR Article 28 for demonstration...\n")
                
                gdpr_article_28 = """
                Article 28 - Processor
                
                1. Where processing is to be carried out on behalf of a controller, the controller shall use only processors providing sufficient guarantees to implement appropriate technical and organisational measures in such a manner that processing will meet the requirements of this Regulation and ensure the protection of the rights of the data subject.
                
                2. The processor shall not engage another processor without prior specific or general written authorisation of the controller. In the case of general written authorisation, the processor shall inform the controller of any intended changes concerning the addition or replacement of other processors, thereby giving the controller the opportunity to object to such changes.
                
                3. Processing by a processor shall be governed by a contract or other legal act under Union or Member State law, that is binding on the processor with regard to the controller and that sets out the subject-matter and duration of the processing, the nature and purpose of the processing, the type of personal data and categories of data subjects and the obligations and rights of the controller.
                """
                
                result = await analyzer.analyze_legislation(
                    legislation_text=gdpr_article_28,
                    article_reference="GDPR Article 28",
                    source_file="example_gdpr_article_28.txt",
                    applicable_countries=["Germany", "France", "Italy", "Spain"],
                    adequacy_countries=["Canada", "Japan", "United Kingdom"]
                )
            else:
                # Process all PDFs in the folder
                result = await analyzer.process_legislation_folder()
        
        # Print results
        print(f"\n=== EXTRACTION RESULTS ===")
        print(f"📊 Summary: {result.summary}")
        print(f"📈 Total Rules: {result.total_rules}")
        print(f"⏱️ Processing Time: {result.processing_time:.2f} seconds")
        print(f"🎯 DPV Rules: {len(result.dpv_rules)}")
        print(f"📜 ODRL Policies: {len(result.odrl_policies)}")
        print(f"⚖️ ODRE Enforceable: {len(result.odre_enforceable)}")
        
        if result.rules:
            print(f"\n=== EXTRACTED RULES DETAILS ===")
            for i, rule in enumerate(result.rules, 1):
                print(f"\n🔍 Rule {i}: {rule.name}")
                print(f"   📝 Description: {rule.description}")
                print(f"   📄 Source: {rule.source_article} ({rule.source_file})")
                print(f"   🎯 Primary Role: {rule.primary_impacted_role.value}")
                if rule.secondary_impacted_role:
                    print(f"   🎯 Secondary Role: {rule.secondary_impacted_role.value}")
                print(f"   📊 Data Categories: {', '.join([cat.value for cat in rule.data_category])}")
                print(f"   🌍 Applicable Countries: {', '.join(rule.applicable_countries) if rule.applicable_countries else 'Not specified'}")
                print(f"   🤝 Adequacy Countries: {', '.join(rule.adequacy_countries) if rule.adequacy_countries else 'None'}")
                print(f"   ⭐ Confidence: {rule.confidence_score}")
                print(f"   🔢 Priority: {rule.priority}")
                
                print(f"   📋 Conditions:")
                for logic_type, conditions in rule.conditions.items():
                    print(f"      {logic_type.upper()}:")
                    for condition in conditions:
                        print(f"        - {condition.description}")
                        print(f"          Fact: {condition.fact} | Operator: {condition.operator} | Value: {condition.value}")
                        print(f"          Role: {condition.role.value} | Domains: {', '.join([d.value for d in condition.data_domain])}")
                
                print(f"   🎬 Event: {rule.event.type}")
                
                # Show DPV alignment for this rule
                if i <= len(result.dpv_rules):
                    dpv_rule = result.dpv_rules[i-1]
                    print(f"   🔗 DPV Alignment:")
                    print(f"      Processing: {[p.split('#')[-1] for p in dpv_rule.hasProcessing]}")
                    print(f"      Purposes: {[p.split('#')[-1] for p in dpv_rule.hasPurpose]}")
                    print(f"      Data Types: {[d.split('#')[-1] for d in dpv_rule.hasPersonalData]}")
                
                # Show ODRL policy for this rule
                if i <= len(result.odrl_policies):
                    odrl_policy = result.odrl_policies[i-1]
                    print(f"   📜 ODRL Policy: {odrl_policy.uid}")
                    if odrl_policy.permission:
                        print(f"      Permissions: {len(odrl_policy.permission)}")
                    if odrl_policy.prohibition:
                        print(f"      Prohibitions: {len(odrl_policy.prohibition)}")
                    if odrl_policy.obligation:
                        print(f"      Obligations: {len(odrl_policy.obligation)}")
                
                print("-" * 80)
        
        # Save results in multiple formats
        if result.rules:
            print(f"\n=== SAVING RESULTS ===")
            
            # Ensure output directories exist
            os.makedirs(Config.RULES_OUTPUT_PATH, exist_ok=True)
            os.makedirs(Config.DPV_OUTPUT_PATH, exist_ok=True)
            os.makedirs(Config.ODRL_OUTPUT_PATH, exist_ok=True)
            
            # Generate timestamp for unique filenames
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            
            print("💾 TRADITIONAL FORMATS:")
            
            # Save JSON format
            json_file = os.path.join(Config.RULES_OUTPUT_PATH, f"extracted_rules_{timestamp}.json")
            result.save_json(json_file)
            print(f"   📄 JSON Rules Engine: {json_file}")
            
            # Save CSV format
            csv_file = os.path.join(Config.RULES_OUTPUT_PATH, f"extracted_rules_{timestamp}.csv")
            result.save_csv(csv_file)
            print(f"   📊 CSV Report: {csv_file}")
            
            print("\n🔗 STANDARDS-ALIGNED FORMATS:")
            
            # Save DPV formats
            if result.dpv_rules:
                dpv_turtle_file = os.path.join(Config.DPV_OUTPUT_PATH, f"dpv_rules_{timestamp}.ttl")
                result.save_dpv_turtle(dpv_turtle_file)
                print(f"   🐢 DPV Turtle RDF: {dpv_turtle_file}")
                
                dpv_jsonld_file = os.path.join(Config.DPV_OUTPUT_PATH, f"dpv_rules_{timestamp}.jsonld")
                result.save_dpv_jsonld(dpv_jsonld_file)
                print(f"   🔗 DPV JSON-LD: {dpv_jsonld_file}")
            
            # Save ODRL formats
            if result.odrl_policies:
                odrl_file = os.path.join(Config.ODRL_OUTPUT_PATH, f"odrl_policies_{timestamp}.json")
                result.save_odrl_policies(odrl_file)
                print(f"   📜 ODRL Policies: {odrl_file}")
            
            # Save ODRE enforceable formats
            if result.odre_enforceable:
                odre_file = os.path.join(Config.ODRL_OUTPUT_PATH, f"odre_enforceable_{timestamp}.json")
                result.save_odre_enforceable(odre_file)
                print(f"   ⚖️ ODRE Enforceable: {odre_file}")
            
            print(f"\n📋 STANDARDS SUMMARY:")
            print(f"   🎯 DPV (Data Privacy Vocabulary): {len(result.dpv_rules)} processing activities")
            print(f"   📜 ODRL (Open Digital Rights Language): {len(result.odrl_policies)} policies")
            print(f"   ⚖️ ODRE (Open Digital Rights Enforcement): {len(result.odre_enforceable)} enforceable policies")
            print(f"   🔗 W3C Standards Compliance: ✅ Fully aligned")
            
            # Show existing rules summary
            total_existing = len(analyzer.rule_manager.existing_rules)
            print(f"\n=== RULE DATABASE STATUS ===")
            print(f"📚 Total rules in database: {total_existing}")
            print(f"🆕 New rules added: {len(result.rules)}")
            print(f"🗃️ Database file: {Config.EXISTING_RULES_FILE}")
            
        else:
            print("\n⚠️ No rules were extracted.")
        
        print(f"\n✅ Processing complete!")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise

if __name__ == "__main__":
    # Run the main function
    asyncio.run(main())
