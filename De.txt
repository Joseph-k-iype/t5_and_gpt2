#!/usr/bin/env python3
"""
Multi-Agent Legislation Rule Extraction System
Converts legislation into machine-readable rules and conditions using LangGraph
Enhanced with Chain of Thought, Mixture of Experts, and Rule Deduplication
"""

import os
import sys
import json
import csv
import asyncio
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Annotated
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

# Core dependencies
import pymupdf
import openai
import numpy as np
from pydantic import BaseModel, Field

# LangChain and LangGraph dependencies  
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_core.tools import BaseTool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Global Configuration
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = "o3-mini-2025-01-31"
EMBEDDING_MODEL = "text-embedding-3-large"

# Paths
INPUT_PDF_PATH = os.getenv("INPUT_PDF_PATH", "./input_pdfs/")
LEGISLATION_METADATA_PATH = os.getenv("LEGISLATION_METADATA_PATH", "./legislation_metadata.json")
GEOGRAPHY_PATH = os.getenv("GEOGRAPHY_PATH", "./geography.json")
OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output/")

# Ensure output directory exists
Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('legislation_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Validate OpenAI client initialization
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set, OpenAI client initialization will fail at runtime")
    openai_client = None
else:
    try:
        openai_client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}")
        openai_client = None

def _convert_messages_for_openai(messages: List[BaseMessage]) -> List[Dict[str, str]]:
    """Convert LangChain messages to OpenAI format"""
    openai_messages = []
    for msg in messages:
        if msg.type == "human":
            role = "user"
        elif msg.type == "ai":
            role = "assistant"
        elif msg.type == "system":
            role = "system"
        else:
            logger.warning(f"Unknown message type: {msg.type}, defaulting to 'user'")
            role = "user"  # Default fallback
        
        openai_messages.append({
            "role": role,
            "content": msg.content
        })
    
    logger.debug(f"Converted {len(messages)} messages for OpenAI API")
    return openai_messages

class RoleType(Enum):
    CONTROLLER = "Controller"
    PROCESSOR = "Processor" 
    JOINT_CONTROLLER = "Joint Controller"

class RuleCondition(BaseModel):
    """Individual rule condition with logical operators"""
    condition_text: str = Field(description="Clear, atomic condition statement")
    logical_operator: Optional[str] = Field(description="AND, OR, NOT operator", default=None)
    role: Optional[RoleType] = Field(description="Applicable role for this condition", default=None)

class LegislationRule(BaseModel):
    """Complete legislation rule with conditions and metadata"""
    rule_id: str = Field(description="Unique identifier for the rule")
    rule_text: str = Field(description="Main rule statement")
    applies_to_countries: List[str] = Field(description="List of country/region codes")
    conditions: List[RuleCondition] = Field(description="List of conditions for this rule")
    condition_count: int = Field(description="Number of conditions")
    references: List[str] = Field(description="Legal references and citations")
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    confidence_score: float = Field(default=0.0, description="Confidence in rule extraction")
    duplicate_of: Optional[str] = Field(default=None, description="ID of original rule if this is a duplicate")

class AgentState(BaseModel):
    """State object for the multi-agent workflow"""
    messages: List[BaseMessage] = Field(default_factory=list)
    documents: List[Document] = Field(default_factory=list)
    processed_text: str = Field(default="")
    segmented_content: List[Dict[str, Any]] = Field(default_factory=list)
    extracted_entities: List[Dict[str, Any]] = Field(default_factory=list)
    rules: List[LegislationRule] = Field(default_factory=list)
    deduplicated_rules: List[LegislationRule] = Field(default_factory=list)
    current_jurisdiction: str = Field(default="")
    geography_data: Dict[str, Any] = Field(default_factory=dict)
    adequacy_countries: List[str] = Field(default_factory=list)
    vector_store: Optional[Any] = Field(default=None)
    next_agent: str = Field(default="document_processor")
    error_messages: List[str] = Field(default_factory=list)

    class Config:
        arbitrary_types_allowed = True

class CustomEmbeddings(Embeddings):
    """Custom embeddings using OpenAI API directly"""
    
    def __init__(self):
        if not openai_client:
            raise ValueError("OpenAI client not initialized. Please check OPENAI_API_KEY.")
        self.client = openai_client
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text[:8000]  # Limit input size
            )
            embeddings.append(response.data[0].embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        response = self.client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text[:8000]  # Limit input size
        )
        return response.data[0].embedding

class GeographyManager:
    """Manages geography data and country/region mappings"""
    
    def __init__(self, geography_data: Dict[str, Any]):
        self.geography_data = geography_data
        self.country_lookup = self._build_country_lookup()
        self.region_lookup = self._build_region_lookup()
    
    def _build_country_lookup(self) -> Dict[str, Dict[str, Any]]:
        """Build lookup table for countries"""
        lookup = {}
        
        # Add countries from regional groupings
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    for country in continent_data.get("countries", []):
                        lookup[country["iso2"]] = {
                            "name": country["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": country["iso2"]
                        }
                    for territory in continent_data.get("territories", []):
                        lookup[territory["iso2"]] = {
                            "name": territory["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": territory["iso2"],
                            "dependency_of": territory["dependency_of"]
                        }
            else:
                # Regional groupings like EU, EEA, MENAT
                for country in region_data.get("countries", []):
                    lookup[country["iso2"]] = {
                        "name": country["name"],
                        "region": region_key,
                        "iso2": country["iso2"]
                    }
                for territory in region_data.get("territories", []):
                    lookup[territory["iso2"]] = {
                        "name": territory["name"],
                        "region": region_key,
                        "iso2": territory["iso2"],
                        "dependency_of": territory["dependency_of"]
                    }
        
        return lookup
    
    def _build_region_lookup(self) -> Dict[str, List[str]]:
        """Build lookup table for regions to countries"""
        lookup = {}
        
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    country_codes = [c["iso2"] for c in continent_data.get("countries", [])]
                    territory_codes = [t["iso2"] for t in continent_data.get("territories", [])]
                    lookup[f"By_Continent.{continent}"] = country_codes + territory_codes
            else:
                country_codes = [c["iso2"] for c in region_data.get("countries", [])]
                territory_codes = [t["iso2"] for t in region_data.get("territories", [])]
                lookup[region_key] = country_codes + territory_codes
        
        return lookup
    
    def get_country_info(self, iso_code: str) -> Optional[Dict[str, Any]]:
        """Get country information by ISO code"""
        return self.country_lookup.get(iso_code)
    
    def get_region_countries(self, region: str) -> List[str]:
        """Get all countries in a region"""
        return self.region_lookup.get(region, [])
    
    def find_countries_by_name(self, name_pattern: str) -> List[str]:
        """Find countries by name pattern"""
        matches = []
        name_lower = name_pattern.lower()
        for iso_code, info in self.country_lookup.items():
            if name_lower in info["name"].lower():
                matches.append(iso_code)
        return matches

class LegislationProcessor:
    """Main processor for legislation documents"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        self.embeddings = CustomEmbeddings()
        self.geography_manager = None
        
    def load_geography_data(self) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(GEOGRAPHY_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.geography_manager = GeographyManager(data)
                return data
        except FileNotFoundError:
            logger.error(f"Geography file not found: {GEOGRAPHY_PATH}")
            return {}
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing geography JSON: {e}")
            return {}
    
    def load_legislation_metadata(self) -> List[Dict[str, Any]]:
        """Load legislation metadata from JSON file"""
        try:
            with open(LEGISLATION_METADATA_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Ensure we return a list of dictionaries
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    # If it's a single dict, wrap it in a list
                    return [data]
                else:
                    logger.error(f"Unexpected data format in legislation metadata: {type(data)}")
                    return []
        except FileNotFoundError:
            logger.error(f"Legislation metadata file not found: {LEGISLATION_METADATA_PATH}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing legislation metadata JSON: {e}")
            return []
    
    def extract_pdf_content(self, pdf_path: str) -> Tuple[str, List[Document]]:
        """Extract content from PDF using PyMuPDF"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            documents = []
            
            supporting_info_start = False
            legislation_text = ""
            supporting_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += page_text + "\n"
                
                # Check for "Supporting Information" section
                if "supporting information" in page_text.lower():
                    supporting_info_start = True
                
                if not supporting_info_start:
                    legislation_text += page_text + "\n"
                else:
                    supporting_text += page_text + "\n"
                
                # Create document for each page
                documents.append(Document(
                    page_content=page_text,
                    metadata={
                        "page_number": page_num + 1,
                        "source": pdf_path,
                        "is_supporting": supporting_info_start
                    }
                ))
            
            doc.close()
            logger.info(f"Extracted {len(documents)} pages from {pdf_path}")
            return full_text, documents
            
        except Exception as e:
            logger.error(f"Error extracting PDF content from {pdf_path}: {e}")
            return "", []
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """Split documents into smaller chunks"""
        return self.text_splitter.split_documents(documents)

class DocumentProcessorAgent:
    """Agent to understand PDF contents and supporting information"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Process documents and understand legislation vs supporting content"""
        logger.info("DocumentProcessorAgent: Processing documents")
        print("\n📂 DocumentProcessorAgent: Starting document processing...")
        
        # Load geography and metadata
        print("🌍 Loading geography data...")
        state.geography_data = self.processor.load_geography_data()
        print(f"✅ Loaded geography data with {len(state.geography_data)} regions")
        
        print("📋 Loading legislation metadata...")
        legislation_metadata = self.processor.load_legislation_metadata()
        print(f"✅ Loaded metadata for {len(legislation_metadata)} files")
        
        all_documents = []
        
        # Process each PDF file from metadata
        print("\n📄 Processing PDF files from metadata:")
        for i, item in enumerate(legislation_metadata):
            print(f"\n--- Processing item {i+1}/{len(legislation_metadata)} ---")
            pdf_path = item.get("path", "")
            jurisdiction = item.get("jurisdiction", "")
            
            print(f"📁 PDF Path: {pdf_path}")
            print(f"🏛️  Jurisdiction: {jurisdiction}")
            
            if not pdf_path:
                print("❌ No PDF path specified, skipping")
                continue
                
            if not os.path.exists(pdf_path):
                print(f"❌ File not found: {pdf_path}")
                continue
            
            print(f"✅ File exists, extracting content...")
            
            # Extract content
            full_text, documents = self.processor.extract_pdf_content(pdf_path)
            print(f"📄 Extracted {len(documents)} pages from {pdf_path}")
            print(f"📝 Total text length: {len(full_text)} characters")
            
            # Add jurisdiction metadata
            for doc in documents:
                doc.metadata["jurisdiction"] = jurisdiction
            
            all_documents.extend(documents)
            state.current_jurisdiction = jurisdiction
        
        print(f"\n✅ Total documents collected: {len(all_documents)}")
        
        # Chunk documents for better processing
        print("✂️  Chunking documents for processing...")
        chunked_docs = self.processor.chunk_documents(all_documents)
        state.documents = chunked_docs
        print(f"📊 Created {len(chunked_docs)} document chunks")
        
        # Create vector store for semantic search
        print("🔍 Creating vector store...")
        vector_store = InMemoryVectorStore(self.processor.embeddings)
        vector_store.add_documents(chunked_docs)
        state.vector_store = vector_store
        print("✅ Vector store created successfully")
        
        # Chain of Thought + Mixture of Experts Analysis
        print("🤖 Starting LLM analysis...")
        analysis_prompt = self._create_cot_analysis_prompt(state.documents, state.current_jurisdiction)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst with expertise in data protection law. Use systematic reasoning and multiple expert perspectives."),
            HumanMessage(content=analysis_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        state.processed_text = response.choices[0].message.content
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=state.processed_text))
        print("✅ LLM analysis completed")
        
        # Extract adequacy countries from the processed text
        print("🌐 Extracting adequacy countries...")
        state.adequacy_countries = await self._extract_adequacy_countries(state.processed_text, state.geography_data)
        print(f"✅ Found adequacy countries: {state.adequacy_countries}")
        
        state.next_agent = "segmentation"
        print(f"✅ DocumentProcessorAgent completed: {len(state.documents)} chunks processed")
        
        return state
    
    def _create_cot_analysis_prompt(self, documents: List[Document], jurisdiction: str) -> str:
        """Create Chain of Thought analysis prompt with Mixture of Experts"""
        return f"""
        I need you to analyze these legal documents systematically using Chain of Thought reasoning and multiple expert perspectives.

        CHAIN OF THOUGHT ANALYSIS:
        
        Step 1: UNDERSTANDING THE CONTEXT
        Think through what type of legal documents these are and their purpose.
        - Document count: {len(documents)}
        - Primary jurisdiction: {jurisdiction}
        - Document types and structure assessment needed
        
        Step 2: MIXTURE OF EXPERTS CONSULTATION
        
        LEGAL EXPERT PERSPECTIVE:
        - What are the core legal principles and obligations?
        - How do these relate to data protection and privacy?
        - What are the enforcement mechanisms and penalties?
        
        TECHNICAL EXPERT PERSPECTIVE:
        - What technical safeguards and measures are required?
        - How do these rules apply to data processing systems?
        - What are the implementation requirements?
        
        GEOGRAPHIC EXPERT PERSPECTIVE:
        - Which countries and regions are mentioned?
        - What cross-border transfer mechanisms are described?
        - Are there adequacy decisions or special arrangements mentioned?
        
        DATA PROTECTION EXPERT PERSPECTIVE:
        - What roles (Controller, Processor, Joint Controller) are defined?
        - What are the data subject rights and entitlements?
        - How are consent and other legal bases addressed?
        
        Step 3: SYNTHESIS AND CATEGORIZATION
        Now synthesize these expert perspectives to:
        1. Distinguish between actual legislation (binding rules) vs supporting information (guidance, examples)
        2. Identify key concepts related to:
           - Data transfer requirements and restrictions
           - Access rights and procedures
           - Entitlement conditions and criteria
        3. Note any adequacy decisions or countries with special status mentioned
        4. Extract role definitions and responsibilities
        
        Step 4: STRUCTURED OUTPUT
        Provide your analysis in this format:
        
        LEGISLATION CONTENT:
        [Core binding legal requirements]
        
        SUPPORTING INFORMATION:
        [Explanatory content, examples, guidance]
        
        KEY CONCEPTS IDENTIFIED:
        - Data Transfer: [relevant findings]
        - Access Rights: [relevant findings]  
        - Entitlements: [relevant findings]
        - Roles: [Controller/Processor/Joint Controller definitions]
        
        ADEQUACY COUNTRIES/REGIONS MENTIONED:
        [List any countries or regions with adequacy decisions or special transfer status]
        
        CONFIDENCE ASSESSMENT:
        [Rate your confidence in this analysis and note any uncertainties]
        
        Think step by step through each expert perspective before providing your final synthesis.
        """
    
    async def _extract_adequacy_countries(self, processed_text: str, geography_data: Dict[str, Any]) -> List[str]:
        """Extract adequacy countries mentioned in the legislation"""
        geography_summary = json.dumps(geography_data, indent=2)[:1000] + "..." if len(str(geography_data)) > 1000 else json.dumps(geography_data, indent=2)
        
        extraction_prompt = f"""
        Extract all countries and regions mentioned as having adequacy decisions or special data transfer status from this legal analysis:
        
        {processed_text[:2000]}...
        
        Available geography data includes these regions and countries:
        {geography_summary}
        
        Return ONLY a JSON array of ISO2 country codes mentioned as having adequacy status.
        Example: ["US", "CA", "JP", "GB"]
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Extract country codes from legal text. Return only valid JSON array."},
                {"role": "user", "content": extraction_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        adequacy_countries = json.loads(result_text)
        
        # Validate country codes if geography manager is available
        if hasattr(self.processor, 'geography_manager') and self.processor.geography_manager:
            validated_countries = []
            for country in adequacy_countries:
                if isinstance(country, str) and self.processor.geography_manager.get_country_info(country):
                    validated_countries.append(country)
            return validated_countries
        
        return [c for c in adequacy_countries if isinstance(c, str)]

class IntelligentSegmentationAgent:
    """Agent for semantic segmentation with analytical questions"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Perform semantic segmentation using Why/What/When/Where questions"""
        logger.info("IntelligentSegmentationAgent: Starting segmentation")
        
        segmentation_prompt = self._create_cot_segmentation_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal segmentation specialist using systematic analytical reasoning."),
            HumanMessage(content=segmentation_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        segmentation_result = response.choices[0].message.content
        
        # Parse segmentation into structured format using vector search
        segments = await self._create_structured_segments(segmentation_result, state)
        
        state.segmented_content = segments
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=segmentation_result))
        
        state.next_agent = "entity_extraction"
        logger.info(f"IntelligentSegmentationAgent: Completed segmentation with {len(segments)} segments")
        
        return state
    
    def _create_cot_segmentation_prompt(self, state: AgentState) -> str:
        """Create Chain of Thought segmentation prompt"""
        return f"""
        Use Chain of Thought reasoning to systematically segment this legislation using analytical questions.
        
        CHAIN OF THOUGHT PROCESS:
        
        Step 1: QUESTION FRAMEWORK SETUP
        I will apply these analytical questions systematically:
        - WHY: Purpose, rationale, and legal basis
        - WHAT: Specific actions, requirements, prohibitions
        - WHEN: Circumstances, conditions, timing
        - WHERE: Jurisdictions, locations, scope
        - WHO: Roles, entities, responsibilities  
        - HOW: Procedures, mechanisms, methods
        
        Step 2: SYSTEMATIC ANALYSIS
        
        For each section of the legislation, think through:
        
        WHY ANALYSIS:
        - What is the legislative purpose behind each rule?
        - What problems or risks is it addressing?
        - What are the policy objectives?
        
        WHAT ANALYSIS:
        - What specific obligations are created?
        - What actions are required, permitted, or prohibited?
        - What rights are established?
        
        WHEN ANALYSIS:
        - Under what circumstances do rules apply?
        - What are the triggering conditions?
        - Are there temporal requirements or deadlines?
        
        WHERE ANALYSIS:
        - What geographic scope applies?
        - Which jurisdictions are covered?
        - Are there territorial limitations or extensions?
        
        WHO ANALYSIS:
        - Which roles are involved (Controller, Processor, Joint Controller)?
        - What entities have obligations or rights?
        - Who has enforcement authority?
        
        HOW ANALYSIS:
        - What procedures must be followed?
        - What technical or organizational measures are required?
        - How is compliance demonstrated?
        
        Step 3: FOCUS ON KEY CONCEPTS
        Pay special attention to:
        - Data transfer requirements and restrictions
        - Access rights and procedures
        - Entitlement conditions and criteria
        - Role-specific obligations
        
        PROCESSED LEGISLATION:
        {state.processed_text[:3000]}...
        
        CURRENT JURISDICTION: {state.current_jurisdiction}
        ADEQUACY COUNTRIES IDENTIFIED: {state.adequacy_countries}
        
        Step 4: STRUCTURED SEGMENTATION OUTPUT
        Provide segmented analysis organized by analytical questions, focusing on data protection concepts.
        
        Think through each analytical question systematically before providing your segmentation.
        """
    
    async def _create_structured_segments(self, segmentation_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Create structured segments using vector search"""
        segments = []
        
        # Use vector store to find relevant chunks for each analytical dimension
        if state.vector_store:
            analytical_queries = [
                "data transfer requirements cross-border international",
                "access rights data subject entitlements procedures", 
                "controller processor joint controller responsibilities obligations",
                "consent legal basis legitimate interest conditions",
                "adequacy decisions transfer mechanisms safeguards",
                "compliance enforcement penalties supervisory authority"
            ]
            
            for query in analytical_queries:
                relevant_docs = state.vector_store.similarity_search(query, k=3)
                segments.append({
                    "analytical_focus": query.replace(" ", "_"),
                    "relevant_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "segmentation_analysis": segmentation_result
                })
        
        return segments

class ComprehensiveEntityExtractionAgent:
    """Agent for comprehensive entity extraction with expert reasoning"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract entities using mixture of experts approach"""
        logger.info("ComprehensiveEntityExtractionAgent: Extracting entities")
        
        entity_prompt = self._create_cot_entity_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are an expert entity extraction specialist using systematic legal analysis."),
            HumanMessage(content=entity_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        extraction_result = response.choices[0].message.content
        
        # Structure extracted entities with geography integration
        entities = await self._structure_entities_with_geography(extraction_result, state)
        
        state.extracted_entities = entities
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=extraction_result))
        
        state.next_agent = "rule_extraction"
        logger.info(f"ComprehensiveEntityExtractionAgent: Completed entity extraction with {len(entities)} categories")
        
        return state
    
    def _create_cot_entity_extraction_prompt(self, state: AgentState) -> str:
        """Create Chain of Thought entity extraction prompt with experts"""
        geography_summary = self._create_geography_summary(state.geography_data)
        
        return f"""
        Use Chain of Thought reasoning with multiple expert perspectives to extract and resolve entities systematically.
        
        CHAIN OF THOUGHT ENTITY EXTRACTION AND RESOLUTION:
        
        Step 1: EXPERT CONSULTATION FRAMEWORK
        
        LEGAL ENTITIES EXPERT:
        Think through:
        - What legal persons, roles, and entities are defined?
        - Controller vs Processor vs Joint Controller distinctions
        - Data subjects and their categories
        - Supervisory authorities and enforcement bodies
        
        DATA CLASSIFICATION EXPERT:
        Analyze:
        - What types of personal data are referenced?
        - Special categories or sensitive data mentioned
        - Technical data concepts (pseudonymization, encryption)
        - Data processing operations and purposes
        
        GEOGRAPHIC/JURISDICTIONAL EXPERT:
        Examine:
        - Which countries and regions are explicitly mentioned?
        - What adequacy decisions are referenced?
        - Cross-border transfer mechanisms described
        - Territorial scope and applicability
        
        Step 2: SYSTEMATIC ENTITY CATEGORIZATION AND RESOLUTION
        
        For each entity type, extract and resolve:
        
        1. LEGAL ENTITIES: 
           - Controllers, Processors, Joint Controllers
           - Data Subjects, Representatives
           - Supervisory Authorities, Courts
           - RESOLUTION: Identify when different terms refer to same entities
        
        2. DATA ENTITIES:
           - Personal data types and categories
           - Processing operations and purposes
           - Technical measures and safeguards
           - RESOLUTION: Map synonyms and related concepts
        
        3. GEOGRAPHIC ENTITIES:
           - Countries and regions with specific status
           - Adequacy jurisdictions: {state.adequacy_countries}
           - Transfer mechanisms and frameworks
           - RESOLUTION: Standardize country names to ISO codes
        
        4. CONDITIONAL ENTITIES:
           - Legal bases (consent, legitimate interest, etc.)
           - Circumstances and triggering conditions
           - Exceptions and derogations
           - RESOLUTION: Group related conditions and exceptions
        
        Step 3: ENTITY RESOLUTION PROCESS
        
        SEMANTIC RESOLUTION:
        - Identify entities with different names but same meaning
        - Example: "data controller" = "controller" = "data processing controller"
        - Resolve abbreviations and acronyms to full terms
        
        HIERARCHICAL RESOLUTION:
        - Map specific entities to broader categories
        - Example: "biometric data" → "special category personal data"
        - Create parent-child relationships between concepts
        
        GEOGRAPHIC RESOLUTION:
        - Standardize country references to ISO2 codes
        - Resolve regional groupings (EU → individual member states)
        - Map territories to their governing jurisdictions
        
        CONTEXTUAL RESOLUTION:
        - Distinguish between entities with same names in different contexts
        - Example: "processing" (data processing vs legal proceedings)
        - Maintain context-specific entity instances
        
        AVAILABLE GEOGRAPHY DATA:
        {geography_summary}
        
        SEGMENTED CONTENT: {len(state.segmented_content)} analytical segments
        CURRENT JURISDICTION: {state.current_jurisdiction}
        
        Step 4: RELATIONSHIP MAPPING WITH RESOLUTION
        Think through how resolved entities relate to each other:
        - Which entities have obligations to which others?
        - What geographic constraints apply to which entities?
        - How do resolved entity hierarchies affect relationships?
        
        Step 5: CONFIDENCE ASSESSMENT
        Rate your confidence in entity extraction and resolution, noting any ambiguities.
        
        Provide structured entity extraction with clear categorization, resolution mappings, and relationships.
        """
    
    def _create_geography_summary(self, geography_data: Dict[str, Any]) -> str:
        """Create a summary of available geography data"""
        summary = []
        for region, data in geography_data.items():
            if region == "By_Continent":
                summary.append(f"Continental groupings: {list(data.keys())}")
            else:
                country_count = len(data.get("countries", []))
                territory_count = len(data.get("territories", []))
                summary.append(f"{region}: {country_count} countries, {territory_count} territories")
        return "\n".join(summary)
    
    async def _structure_entities_with_geography(self, extraction_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Structure entities with geographic integration and entity resolution"""
        entities = []
        
        # Use vector store to enhance entity extraction with geographic context
        if state.vector_store:
            entity_queries = [
                "controller responsibilities obligations data protection",
                "processor duties requirements data processing", 
                "data subject rights access rectification erasure",
                "cross-border transfer adequacy decisions safeguards",
                "supervisory authority enforcement powers penalties",
                "personal data categories special sensitive biometric"
            ]
            
            for query in entity_queries:
                relevant_docs = state.vector_store.similarity_search(query, k=2)
                
                # Enhance with geography manager if available
                geographic_context = []
                if hasattr(state, 'geography_data') and state.geography_data:
                    geo_manager = GeographyManager(state.geography_data)
                    # Look for country mentions in the relevant docs
                    for doc in relevant_docs:
                        for iso_code in geo_manager.country_lookup.keys():
                            country_info = geo_manager.get_country_info(iso_code)
                            if country_info and country_info["name"].lower() in doc.page_content.lower():
                                geographic_context.append(country_info)
                
                # Perform entity resolution using LLM
                resolved_entities = await self._perform_entity_resolution(
                    [doc.page_content for doc in relevant_docs], 
                    query, 
                    state
                )
                
                entities.append({
                    "entity_category": query.replace(" ", "_"),
                    "extracted_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "geographic_context": geographic_context,
                    "resolved_entities": resolved_entities,
                    "extraction_analysis": extraction_result
                })
        
        return entities
    
    async def _perform_entity_resolution(self, content_chunks: List[str], category: str, state: AgentState) -> Dict[str, Any]:
        """Perform entity resolution using LLM analysis"""
        content_text = "\n".join(content_chunks[:2])  # Limit content for API
        
        resolution_prompt = f"""
        Perform entity resolution for the category: {category}
        
        ENTITY RESOLUTION TASK:
        1. Identify all entities mentioned in the content
        2. Resolve synonyms, abbreviations, and alternative names
        3. Create canonical entity names
        4. Map hierarchical relationships
        5. Standardize geographic references to ISO codes where applicable
        
        CONTENT TO ANALYZE:
        {content_text}
        
        AVAILABLE GEOGRAPHY CODES: {list(state.geography_data.keys()) if state.geography_data else []}
        
        Return JSON with this structure:
        {{
            "canonical_entities": [
                {{
                    "canonical_name": "standardized entity name",
                    "aliases": ["alternative name 1", "abbreviation", "synonym"],
                    "entity_type": "controller/processor/data_type/country/etc",
                    "hierarchy_level": "parent/child/peer",
                    "related_entities": ["entity1", "entity2"],
                    "geographic_codes": ["ISO2 codes if applicable"]
                }}
            ],
            "resolution_mappings": {{
                "original_term": "canonical_name"
            }}
        }}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are an entity resolution specialist. Return only valid JSON."},
                {"role": "user", "content": resolution_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        return json.loads(result_text)

class IntelligentRuleComponentExtractionAgent:
    """Agent to convert complex legal language to atomic logical statements"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract and convert rules to atomic logical statements"""
        logger.info("IntelligentRuleComponentExtractionAgent: Extracting rules")
        
        rule_extraction_prompt = self._create_cot_rule_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal rule extraction specialist using systematic logical analysis."),
            HumanMessage(content=rule_extraction_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        rule_text = response.choices[0].message.content
        
        # Parse and structure the rules with geography integration
        rules = await self._parse_rules_with_geography(rule_text, state)
        
        state.rules = rules
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=rule_text))
        
        state.next_agent = "rule_deduplication"
        logger.info(f"IntelligentRuleComponentExtractionAgent: Extracted {len(rules)} rules")
        
        return state
    
    def _create_cot_rule_extraction_prompt(self, state: AgentState) -> str:
        """Create comprehensive Chain of Thought rule extraction prompt"""
        geo_manager = GeographyManager(state.geography_data) if state.geography_data else None
        available_regions = list(state.geography_data.keys()) if state.geography_data else []
        
        return f"""
        Use Chain of Thought reasoning with Mixture of Experts to convert complex legal language into simple, atomic logical statements.
        
        CHAIN OF THOUGHT RULE EXTRACTION:
        
        Step 1: EXPERT CONSULTATION FOR RULE TYPES
        
        RULE STRUCTURE EXPERT:
        Identify different types of rules:
        - Obligation rules (MUST do X)
        - Prohibition rules (MUST NOT do Y) 
        - Permission rules (MAY do Z under conditions)
        - Conditional rules (IF condition THEN consequence)
        
        LOGICAL STRUCTURE EXPERT:
        Break down complex statements into:
        - Atomic conditions (simple, testable statements)
        - Logical operators (AND, OR, NOT)
        - Clear subject-predicate-object structure
        - Unambiguous language
        
        ROLE ASSIGNMENT EXPERT:
        Determine which roles apply:
        - Controller: Makes decisions about processing
        - Processor: Processes data on behalf of controller
        - Joint Controller: Shares decision-making responsibility
        - Data Subject: Individual whose data is processed
        
        GEOGRAPHIC SCOPE EXPERT:
        Determine territorial application using:
        - Available regions: {available_regions}
        - Adequacy countries: {state.adequacy_countries}
        - Current jurisdiction: {state.current_jurisdiction}
        - Cross-border transfer implications
        
        Step 2: SYSTEMATIC RULE DECOMPOSITION
        
        For each complex legal provision:
        
        2a. IDENTIFY THE CORE OBLIGATION/RIGHT
        What is the fundamental requirement or entitlement?
        
        2b. EXTRACT CONDITIONS
        What circumstances must exist for the rule to apply?
        Use logical operators to connect conditions:
        - AND: All conditions must be true
        - OR: At least one condition must be true  
        - NOT: Condition must not be true
        
        2c. ASSIGN ROLES AND RESPONSIBILITIES
        Which role (Controller/Processor/Joint Controller) has the obligation?
        Which role benefits from the right or protection?
        
        2d. DETERMINE GEOGRAPHIC SCOPE
        Which countries/regions does this rule apply to?
        Consider adequacy decisions and transfer mechanisms.
        
        2e. SIMPLIFY LANGUAGE
        Remove legal jargon and replace with plain English.
        Make statements testable and implementable.
        Replace "this article" or "such provision" with explicit references.
        
        Step 3: FOCUS AREAS FOR DATA PROTECTION
        
        PRIORITY EXTRACTION AREAS:
        1. Data Transfer Rules:
           - Cross-border transfer requirements
           - Adequacy decision implications  
           - Transfer mechanism requirements (SCCs, BCRs, etc.)
        
        2. Access Rights Rules:
           - Data subject access procedures
           - Controller response obligations
           - Information provision requirements
        
        3. Entitlement Rules:
           - Legal basis requirements
           - Consent mechanisms and withdrawal
           - Legitimate interest balancing
        
        4. Role-Specific Obligations:
           - Controller duties and responsibilities
           - Processor obligations and instructions
           - Joint controller arrangements
        
        EXTRACTED ENTITIES: {len(state.extracted_entities)} categories
        SEGMENTED CONTENT: {len(state.segmented_content)} analytical segments
        
        Step 4: RULE FORMULATION REQUIREMENTS
        
        Each rule must have:
        - Unique identifier
        - Clear, unambiguous text
        - Specific country/region application
        - Atomic conditions with logical operators
        - Role assignments
        - Legal references
        - Confidence score
        
        EXAMPLE TRANSFORMATION:
        Complex: "Controllers shall, where personal data are transferred to a third country or international organisation, ensure that the level of protection of natural persons afforded by this Regulation is not undermined."
        
        Simple Rule: "Data controllers must ensure adequate protection when transferring personal data internationally"
        Conditions:
        - "Data involves personal information" (AND)
        - "Transfer destination is outside EU/EEA" (AND) 
        - "Adequate protection level exists OR appropriate safeguards implemented" (OR)
        Role: Controller
        Countries: EU, EEA
        
        Step 5: COMPREHENSIVE RULE EXTRACTION
        Now systematically extract and convert all identifiable rules using this methodology.
        
        Think through each step carefully before providing your structured rule extraction.
        """
    
    async def _parse_rules_with_geography(self, rule_text: str, state: AgentState) -> List[LegislationRule]:
        """Parse LLM response into structured rules with geography integration"""
        rules = []
        geo_manager = GeographyManager(state.geography_data) if state.geography_data else None
        
        # Use another LLM call to structure the extracted rules
        structure_prompt = f"""
        Convert the following rule extraction into structured JSON format with geographic integration:
        
        {rule_text}
        
        GEOGRAPHY INTEGRATION REQUIREMENTS:
        - Use ISO2 country codes from available geography data
        - Consider adequacy countries: {state.adequacy_countries}
        - Default jurisdiction: {state.current_jurisdiction}
        - Available regions: {list(state.geography_data.keys()) if state.geography_data else []}
        
        Return a JSON array of rules with this exact structure:
        [{{
            "rule_id": "unique_identifier",
            "rule_text": "clear, simple rule statement",
            "applies_to_countries": ["ISO2_codes"],
            "conditions": [
                {{
                    "condition_text": "atomic condition statement",
                    "logical_operator": "AND/OR/NOT",
                    "role": "Controller/Processor/Joint Controller"
                }}
            ],
            "condition_count": 0,
            "references": ["legal reference 1", "legal reference 2"],
            "extraction_metadata": {{
                "confidence_score": 0.0,
                "complexity_level": "low/medium/high"
            }}
        }}]
        
        REQUIREMENTS:
        - Use only ISO2 codes that exist in the geography data
        - Make rule text simple and unambiguous
        - Use atomic conditions with clear logical operators
        - Assign appropriate roles to conditions
        - Include confidence assessment
        """
        
        structure_response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a JSON formatter. Return only valid JSON without additional text."},
                {"role": "user", "content": structure_prompt}
            ]
        )
        
        json_text = structure_response.choices[0].message.content
        
        # Clean JSON response
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
        elif "```" in json_text:
            json_text = json_text.split("```")[1]
        
        rules_data = json.loads(json_text.strip())
        
        for rule_data in rules_data:
            conditions = []
            for cond in rule_data.get("conditions", []):
                conditions.append(RuleCondition(
                    condition_text=cond.get("condition_text", ""),
                    logical_operator=cond.get("logical_operator"),
                    role=RoleType(cond.get("role")) if cond.get("role") in [e.value for e in RoleType] else None
                ))
            
            # Validate and clean country codes using geography manager
            applies_to = rule_data.get("applies_to_countries", [state.current_jurisdiction])
            if geo_manager:
                validated_countries = []
                for country_code in applies_to:
                    if geo_manager.get_country_info(country_code):
                        validated_countries.append(country_code)
                    else:
                        # Try to find by name
                        matches = geo_manager.find_countries_by_name(country_code)
                        validated_countries.extend(matches)
                applies_to = validated_countries if validated_countries else [state.current_jurisdiction]
            
            extraction_metadata = rule_data.get("extraction_metadata", {})
            confidence_score = float(extraction_metadata.get("confidence_score", 0.8))
            
            rule = LegislationRule(
                rule_id=rule_data.get("rule_id", f"rule_{len(rules) + 1}"),
                rule_text=rule_data.get("rule_text", ""),
                applies_to_countries=applies_to,
                conditions=conditions,
                condition_count=len(conditions),
                references=rule_data.get("references", []),
                extraction_metadata=extraction_metadata,
                confidence_score=confidence_score
            )
            rules.append(rule)
        
        return rules

class RuleDeduplicationAgent:
    """Agent to identify and handle duplicate rules using LLM semantic analysis"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Identify and deduplicate rules using LLM semantic analysis"""
        logger.info("RuleDeduplicationAgent: Starting rule deduplication")
        
        try:
            if len(state.rules) <= 1:
                state.deduplicated_rules = state.rules
                state.next_agent = "output_generation"
                return state
            
            # Perform pairwise semantic comparison using LLM
            duplicate_pairs = await self._identify_duplicate_pairs(state.rules)
            
            # Create deduplication plan
            deduplication_plan = await self._create_deduplication_plan(state.rules, duplicate_pairs)
            
            # Execute deduplication
            deduplicated_rules = await self._execute_deduplication(state.rules, deduplication_plan)
            
            state.deduplicated_rules = deduplicated_rules
            state.next_agent = "sanity_check"
            
            logger.info(f"RuleDeduplicationAgent: Reduced {len(state.rules)} rules to {len(deduplicated_rules)} after deduplication")
            
        except Exception as e:
            error_msg = f"RuleDeduplicationAgent error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            # Continue with original rules if deduplication fails
            state.deduplicated_rules = state.rules
            state.next_agent = "sanity_check"
        
        return state
    
    async def _identify_duplicate_pairs(self, rules: List[LegislationRule]) -> List[Tuple[int, int, str]]:
        """Identify potentially duplicate rule pairs using LLM analysis"""
        duplicate_pairs = []
        
        # Compare rules pairwise
        for i in range(len(rules)):
            for j in range(i + 1, len(rules)):
                rule1 = rules[i]
                rule2 = rules[j]
                
                comparison_prompt = f"""
                Analyze if these two legal rules are duplicates or substantially similar.
                
                CHAIN OF THOUGHT ANALYSIS:
                
                Step 1: SEMANTIC SIMILARITY ASSESSMENT
                Compare the core meaning and intent of both rules.
                
                Step 2: LOGICAL EQUIVALENCE CHECK
                Do the rules create the same obligations/rights under the same conditions?
                
                Step 3: SCOPE AND APPLICABILITY
                Do they apply to the same roles, countries, and circumstances?
                
                Step 4: CONDITION ANALYSIS
                Are the conditions logically equivalent or overlapping?
                
                RULE 1:
                ID: {rule1.rule_id}
                Text: {rule1.rule_text}
                Countries: {rule1.applies_to_countries}
                Conditions: {[c.condition_text for c in rule1.conditions]}
                
                RULE 2: 
                ID: {rule2.rule_id}
                Text: {rule2.rule_text}
                Countries: {rule2.applies_to_countries}
                Conditions: {[c.condition_text for c in rule2.conditions]}
                
                DECISION CATEGORIES:
                - DUPLICATE: Essentially the same rule (merge recommended)
                - SIMILAR: Related but distinct (keep both, note relationship)
                - DIFFERENT: Clearly distinct rules (no action needed)
                
                Respond with only: DUPLICATE, SIMILAR, or DIFFERENT
                """
                
                response = openai_client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {"role": "system", "content": "You are a legal analysis expert. Analyze rule similarities systematically."},
                        {"role": "user", "content": comparison_prompt}
                    ]
                )
                
                result = response.choices[0].message.content.strip().upper()
                
                if result in ["DUPLICATE", "SIMILAR"]:
                    duplicate_pairs.append((i, j, result))
        
        return duplicate_pairs
    
    async def _create_deduplication_plan(self, rules: List[LegislationRule], duplicate_pairs: List[Tuple[int, int, str]]) -> Dict[str, Any]:
        """Create a plan for handling duplicates"""
        if not duplicate_pairs:
            return {"action": "no_duplicates", "rules_to_keep": list(range(len(rules)))}
        
        plan_prompt = f"""
        Create a deduplication plan for these legal rules based on identified similarities.
        
        RULES: {len(rules)} total rules
        DUPLICATE/SIMILAR PAIRS: {len(duplicate_pairs)} pairs identified
        
        PAIR DETAILS:
        {chr(10).join([f"Rules {pair[0]} and {pair[1]}: {pair[2]}" for pair in duplicate_pairs])}
        
        DEDUPLICATION STRATEGY:
        
        For DUPLICATE pairs:
        - Keep the rule with higher confidence score
        - If confidence is equal, keep the one with more comprehensive conditions
        - Mark the removed rule as duplicate_of the kept rule
        
        For SIMILAR pairs:
        - Keep both rules but note their relationship
        - Add cross-references in metadata
        
        CHAIN OF THOUGHT PLANNING:
        
        Step 1: Identify which rules to keep vs remove
        Step 2: Determine merge strategies for duplicates  
        Step 3: Plan metadata updates for relationships
        Step 4: Ensure no orphaned conditions or important details are lost
        
        Return a JSON plan with this structure:
        {{
            "action": "deduplicate",
            "rules_to_keep": [list of rule indices],
            "rules_to_remove": [list of rule indices], 
            "merge_instructions": [
                {{
                    "keep_rule": index,
                    "remove_rule": index,
                    "merge_conditions": true/false,
                    "merge_countries": true/false
                }}
            ],
            "relationship_notes": [
                {{
                    "rule1": index,
                    "rule2": index, 
                    "relationship": "similar/related"
                }}
            ]
        }}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Create deduplication plans in JSON format only."},
                {"role": "user", "content": plan_prompt}
            ]
        )
        
        plan_text = response.choices[0].message.content.strip()
        
        # Clean JSON
        if "```json" in plan_text:
            plan_text = plan_text.split("```json")[1].split("```")[0]
        elif "```" in plan_text:
            plan_text = plan_text.split("```")[1]
        
        return json.loads(plan_text)
    
    async def _execute_deduplication(self, rules: List[LegislationRule], plan: Dict[str, Any]) -> List[LegislationRule]:
        """Execute the deduplication plan"""
        if plan.get("action") == "no_duplicates":
            return rules
        
        deduplicated_rules = []
        rules_to_keep = plan.get("rules_to_keep", [])
        merge_instructions = plan.get("merge_instructions", [])
        
        # Create a mapping of removed rules to their kept counterparts
        removal_mapping = {}
        for instruction in merge_instructions:
            keep_idx = instruction.get("keep_rule")
            remove_idx = instruction.get("remove_rule")
            if keep_idx is not None and remove_idx is not None:
                removal_mapping[remove_idx] = keep_idx
        
        # Process rules
        for i, rule in enumerate(rules):
            if i in rules_to_keep:
                # Check if this rule should be merged with others
                merged_rule = rule.model_copy()
                
                # Find all rules that should be merged into this one
                for instruction in merge_instructions:
                    if instruction.get("keep_rule") == i:
                        remove_idx = instruction.get("remove_rule")
                        if remove_idx < len(rules):
                            removed_rule = rules[remove_idx]
                            
                            # Merge conditions if requested
                            if instruction.get("merge_conditions", False):
                                for condition in removed_rule.conditions:
                                    if condition not in merged_rule.conditions:
                                        merged_rule.conditions.append(condition)
                                merged_rule.condition_count = len(merged_rule.conditions)
                            
                            # Merge countries if requested
                            if instruction.get("merge_countries", False):
                                for country in removed_rule.applies_to_countries:
                                    if country not in merged_rule.applies_to_countries:
                                        merged_rule.applies_to_countries.append(country)
                            
                            # Merge references
                            for ref in removed_rule.references:
                                if ref not in merged_rule.references:
                                    merged_rule.references.append(ref)
                            
                            # Update metadata
                            merged_rule.extraction_metadata["merged_from"] = merged_rule.extraction_metadata.get("merged_from", [])
                            merged_rule.extraction_metadata["merged_from"].append(removed_rule.rule_id)
                
                deduplicated_rules.append(merged_rule)
            
            elif i in removal_mapping:
                # Mark as duplicate
                duplicate_rule = rule.model_copy()
                duplicate_rule.duplicate_of = rules[removal_mapping[i]].rule_id
                # Don't add to final list, but log the relationship
                logger.info(f"Rule {rule.rule_id} marked as duplicate of {duplicate_rule.duplicate_of}")
        
        return deduplicated_rules

class OutputGenerationAgent:
    """Agent to generate final CSV and JSON output"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Generate output files in CSV and JSON format"""
        logger.info("OutputGenerationAgent: Generating output files")
        print("\n📄 OutputGenerationAgent: Starting output generation...")
        
        # Use deduplicated rules if available, otherwise use original rules
        final_rules = state.deduplicated_rules if state.deduplicated_rules else state.rules
        print(f"📊 Processing {len(final_rules)} final rules for output")
        
        # Debug: Check rule types
        for i, rule in enumerate(final_rules[:3]):  # Check first 3 rules
            print(f"🔍 Rule {i+1} type: {type(rule)}")
            if hasattr(rule, 'rule_id'):
                print(f"   Rule ID: {rule.rule_id}")
            if hasattr(rule, 'conditions'):
                print(f"   Conditions type: {type(rule.conditions)}")
                print(f"   Conditions count: {len(rule.conditions) if rule.conditions else 0}")
        
        # Generate CSV output
        print("📄 Generating CSV output...")
        csv_path = os.path.join(OUTPUT_PATH, "extracted_rules.csv")
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'rule_id', 'applies_to_countries', 'conditions', 
                'condition_count', 'references'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for rule in final_rules:
                # Defensive programming - ensure rule is proper object
                if not hasattr(rule, 'rule_id'):
                    print(f"⚠️  Warning: Invalid rule object: {type(rule)}")
                    continue
                
                # Safe access to conditions
                conditions_list = []
                if hasattr(rule, 'conditions') and rule.conditions:
                    for cond in rule.conditions:
                        if hasattr(cond, 'condition_text'):
                            logical_op = getattr(cond, 'logical_operator', 'N/A') or 'N/A'
                            conditions_list.append(f"{cond.condition_text} ({logical_op})")
                        else:
                            print(f"⚠️  Warning: Invalid condition object: {type(cond)}")
                
                conditions_text = "; ".join(conditions_list)
                
                # Safe access to other attributes
                applies_to = getattr(rule, 'applies_to_countries', [])
                if isinstance(applies_to, list):
                    applies_to_str = ", ".join(applies_to)
                else:
                    applies_to_str = str(applies_to)
                
                references = getattr(rule, 'references', [])
                if isinstance(references, list):
                    references_str = ", ".join(references)
                else:
                    references_str = str(references)
                
                writer.writerow({
                    'rule_id': getattr(rule, 'rule_id', 'unknown'),
                    'applies_to_countries': applies_to_str,
                    'conditions': conditions_text,
                    'condition_count': getattr(rule, 'condition_count', 0),
                    'references': references_str
                })
        
        print(f"✅ CSV output saved to: {csv_path}")
        
        # Generate JSON output
        print("📄 Generating JSON output...")
        json_path = os.path.join(OUTPUT_PATH, "extracted_rules.json")
        rules_dict = []
        
        for rule in final_rules:
            # Defensive programming for JSON generation
            if not hasattr(rule, 'rule_id'):
                print(f"⚠️  Skipping invalid rule object: {type(rule)}")
                continue
            
            # Safe condition processing
            conditions_json = []
            if hasattr(rule, 'conditions') and rule.conditions:
                for cond in rule.conditions:
                    if hasattr(cond, 'condition_text'):
                        cond_dict = {
                            "condition_text": getattr(cond, 'condition_text', ''),
                            "logical_operator": getattr(cond, 'logical_operator', None),
                            "role": getattr(cond, 'role', None)
                        }
                        # Handle role enum
                        if hasattr(cond_dict['role'], 'value'):
                            cond_dict['role'] = cond_dict['role'].value
                        conditions_json.append(cond_dict)
            
            rule_dict = {
                "rule_id": getattr(rule, 'rule_id', ''),
                "rule_text": getattr(rule, 'rule_text', ''),
                "applies_to_countries": getattr(rule, 'applies_to_countries', []),
                "conditions": conditions_json,
                "condition_count": getattr(rule, 'condition_count', 0),
                "references": getattr(rule, 'references', []),
                "extraction_metadata": getattr(rule, 'extraction_metadata', {}),
                "confidence_score": getattr(rule, 'confidence_score', 0.0),
                "duplicate_of": getattr(rule, 'duplicate_of', None)
            }
            rules_dict.append(rule_dict)
        
        with open(json_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(rules_dict, jsonfile, indent=2, ensure_ascii=False)
        
        print(f"✅ JSON output saved to: {json_path}")
        
        # Generate deduplication report
        print("📄 Generating deduplication report...")
        report_path = os.path.join(OUTPUT_PATH, "deduplication_report.json")
        dedup_report = {
            "original_rule_count": len(state.rules),
            "deduplicated_rule_count": len(final_rules),
            "rules_removed": len(state.rules) - len(final_rules),
            "adequacy_countries_identified": getattr(state, 'adequacy_countries', []),
            "geography_regions_available": list(state.geography_data.keys()) if state.geography_data else [],
            "processing_errors": getattr(state, 'error_messages', [])
        }
        
        with open(report_path, 'w', encoding='utf-8') as reportfile:
            json.dump(dedup_report, reportfile, indent=2, ensure_ascii=False)
        
        print(f"✅ Report saved to: {report_path}")
        
        logger.info(f"Generated output files: {csv_path}, {json_path}, {report_path}")
        state.next_agent = "end"
        
        print("✅ OutputGenerationAgent completed successfully")
        return state

class SupervisorAgent:
    """Supervisor agent that orchestrates the multi-agent workflow"""
    
    def __init__(self):
        self.agents = {
            "document_processor": DocumentProcessorAgent(),
            "segmentation": IntelligentSegmentationAgent(),
            "entity_extraction": ComprehensiveEntityExtractionAgent(),
            "rule_extraction": IntelligentRuleComponentExtractionAgent(),
            "rule_deduplication": RuleDeduplicationAgent(),
            "output_generation": OutputGenerationAgent()
        }
        
        # Setup LangGraph workflow with proper typing
        self.workflow = StateGraph(AgentState)
        
        # Add nodes
        self.workflow.add_node("document_processor", self._document_processor_node)
        self.workflow.add_node("segmentation", self._segmentation_node)
        self.workflow.add_node("entity_extraction", self._entity_extraction_node)
        self.workflow.add_node("rule_extraction", self._rule_extraction_node)
        self.workflow.add_node("rule_deduplication", self._rule_deduplication_node)
        self.workflow.add_node("sanity_check", self._sanity_check_node)
        self.workflow.add_node("output_generation", self._output_generation_node)
        self.workflow.add_node("supervisor", self._supervisor_node)
        
        # Define edges
        self.workflow.add_edge(START, "supervisor")
        self.workflow.add_edge("document_processor", "supervisor")
        self.workflow.add_edge("segmentation", "supervisor")
        self.workflow.add_edge("entity_extraction", "supervisor")
        self.workflow.add_edge("rule_extraction", "supervisor")
        self.workflow.add_edge("rule_deduplication", "supervisor")
        self.workflow.add_edge("sanity_check", "supervisor")
        self.workflow.add_edge("output_generation", END)
        
        # Add conditional edges from supervisor
        self.workflow.add_conditional_edges(
            "supervisor",
            self._route_next,
            {
                "document_processor": "document_processor",
                "segmentation": "segmentation",
                "entity_extraction": "entity_extraction",
                "rule_extraction": "rule_extraction",
                "rule_deduplication": "rule_deduplication",
                "sanity_check": "sanity_check",
                "output_generation": "output_generation",
                "end": END
            }
        )
        
        # Setup memory
        self.memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.memory)
    
    async def _document_processor_node(self, state: AgentState) -> AgentState:
        """Document processor node wrapper"""
        return await self.agents["document_processor"].process(state)
    
    async def _segmentation_node(self, state: AgentState) -> AgentState:
        """Segmentation node wrapper"""
        return await self.agents["segmentation"].process(state)
    
    async def _entity_extraction_node(self, state: AgentState) -> AgentState:
        """Entity extraction node wrapper"""
        return await self.agents["entity_extraction"].process(state)
    
    async def _rule_extraction_node(self, state: AgentState) -> AgentState:
        """Rule extraction node wrapper"""
        return await self.agents["rule_extraction"].process(state)
    
    async def _rule_deduplication_node(self, state: AgentState) -> AgentState:
        """Rule deduplication node wrapper"""
        return await self.agents["rule_deduplication"].process(state)
    
    async def _sanity_check_node(self, state: AgentState) -> AgentState:
        """Sanity check node wrapper"""
        return await self._perform_sanity_check_async(state)
    
    async def _output_generation_node(self, state: AgentState) -> AgentState:
        """Output generation node wrapper"""
        return await self.agents["output_generation"].process(state)
    
    def _supervisor_node(self, state: AgentState) -> AgentState:
        """Supervisor node for workflow coordination"""
        logger.info(f"Supervisor: Current agent = {state.next_agent}")
        logger.info(f"Documents processed: {len(state.documents)}")
        logger.info(f"Segments created: {len(state.segmented_content)}")
        logger.info(f"Entities extracted: {len(state.extracted_entities)}")
        logger.info(f"Rules generated: {len(state.rules)}")
        logger.info(f"Rules deduplicated: {len(state.deduplicated_rules)}")
        logger.info(f"Adequacy countries: {state.adequacy_countries}")
        
        if state.error_messages:
            logger.error(f"Errors encountered: {state.error_messages}")
        
        return state
    
    async def _perform_sanity_check_async(self, state: AgentState) -> AgentState:
        """Perform final sanity check on extracted rules"""
        logger.info("SupervisorAgent: Performing final sanity check")
        
        final_rules = state.deduplicated_rules if state.deduplicated_rules else state.rules
        
        # Perform comprehensive validation
        validated_rules = await self._perform_sanity_check(final_rules, state)
        
        # Update state with validated rules
        if state.deduplicated_rules:
            state.deduplicated_rules = validated_rules
        else:
            state.rules = validated_rules
        
        state.next_agent = "output_generation"
        logger.info(f"SupervisorAgent: Sanity check completed. Validated {len(validated_rules)} rules")
        
        return state
    
    async def _perform_sanity_check(self, rules: List[LegislationRule], state: AgentState) -> List[LegislationRule]:
        """Perform comprehensive sanity check on extracted rules"""
        
        # Create comprehensive validation prompt
        sanity_check_prompt = self._create_sanity_check_prompt(rules, state)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst performing final quality assurance on extracted rules."),
            HumanMessage(content=sanity_check_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        validation_result = response.choices[0].message.content
        
        # Parse validation recommendations and apply corrections
        corrected_rules = await self._apply_validation_corrections(rules, validation_result, state)
        
        return corrected_rules
    
    def _create_sanity_check_prompt(self, rules: List[LegislationRule], state: AgentState) -> str:
        """Create comprehensive sanity check prompt"""
        
        rules_summary = []
        for i, rule in enumerate(rules[:10]):  # Limit to first 10 for prompt size
            rules_summary.append(f"""
            Rule {i+1}:
            ID: {rule.rule_id}
            Text: {rule.rule_text}
            Countries: {rule.applies_to_countries}
            Conditions: {len(rule.conditions)} conditions
            Confidence: {rule.confidence_score}
            """)
        
        return f"""
        FINAL SANITY CHECK AND VALIDATION
        
        Please perform a comprehensive validation of these extracted legal rules using systematic analysis.
        
        VALIDATION FRAMEWORK:
        
        Step 1: LOGICAL CONSISTENCY CHECK
        - Do rule texts clearly state obligations, rights, or prohibitions?
        - Are conditions logically connected and non-contradictory?
        - Do logical operators (AND, OR, NOT) make sense in context?
        
        Step 2: GEOGRAPHIC ACCURACY VALIDATION
        - Are country codes valid and appropriate for the rule scope?
        - Do adequacy countries align with actual adequacy decisions?
        - Are regional applications (EU, EEA) correctly specified?
        
        Step 3: ROLE ASSIGNMENT VERIFICATION
        - Are Controller/Processor/Joint Controller roles correctly assigned?
        - Do role assignments match the obligations described?
        - Are there any missing or incorrect role specifications?
        
        Step 4: LEGAL ACCURACY ASSESSMENT
        - Do rules accurately reflect data protection principles?
        - Are legal references appropriate and correctly cited?
        - Do rules align with known legal frameworks (GDPR, etc.)?
        
        Step 5: COMPLETENESS AND COHERENCE
        - Are rules complete and actionable?
        - Do rules cover the main aspects of data transfer, access, and entitlements?
        - Are there any obvious gaps or redundancies?
        
        EXTRACTED RULES TO VALIDATE:
        {chr(10).join(rules_summary)}
        
        CONTEXT:
        - Total rules: {len(rules)}
        - Jurisdiction: {state.current_jurisdiction}
        - Adequacy countries identified: {state.adequacy_countries}
        - Available geography: {list(state.geography_data.keys()) if state.geography_data else []}
        
        VALIDATION OUTPUT REQUIRED:
        
        For each rule, provide:
        1. VALIDATION STATUS: VALID / NEEDS_CORRECTION / INVALID
        2. ISSUES IDENTIFIED: List specific problems if any
        3. RECOMMENDED CORRECTIONS: Specific text or structural changes
        4. CONFIDENCE ADJUSTMENT: Should confidence score be modified?
        5. OVERALL ASSESSMENT: Is the rule set coherent and complete?
        
        Focus on ensuring rules are:
        - Legally accurate and implementable
        - Geographically appropriate
        - Logically consistent
        - Properly structured with clear conditions
        - Aligned with data protection principles
        
        Provide structured validation feedback for rule improvement.
        """
    
    async def _apply_validation_corrections(self, rules: List[LegislationRule], validation_result: str, state: AgentState) -> List[LegislationRule]:
        """Apply validation corrections to rules based on sanity check"""
        
        correction_prompt = f"""
        Based on the validation analysis, apply necessary corrections to the rules.
        
        VALIDATION ANALYSIS:
        {validation_result}
        
        ORIGINAL RULES: {len(rules)} rules
        
        CORRECTION INSTRUCTIONS:
        1. Fix any logical inconsistencies in rule text or conditions
        2. Correct geographic assignments using valid country codes
        3. Adjust role assignments where incorrect
        4. Update confidence scores based on validation assessment
        5. Remove rules marked as INVALID
        6. Apply recommended text corrections
        
        Return corrected rules in the same JSON format as original extraction.
        Only include rules that pass validation or have been successfully corrected.
        
        Available geography codes: {list(state.geography_data.keys()) if state.geography_data else []}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Apply rule corrections based on validation analysis. Return corrected rules in JSON format."},
                {"role": "user", "content": correction_prompt}
            ]
        )
        
        correction_text = response.choices[0].message.content.strip()
        
        # Parse corrected rules if JSON format is returned
        if "```json" in correction_text or "[" in correction_text:
            # Clean JSON response
            if "```json" in correction_text:
                correction_text = correction_text.split("```json")[1].split("```")[0]
            elif "```" in correction_text:
                correction_text = correction_text.split("```")[1]
            
            corrected_data = json.loads(correction_text.strip())
            
            corrected_rules = []
            for rule_data in corrected_data:
                # Rebuild rule with corrections
                conditions = []
                for cond in rule_data.get("conditions", []):
                    conditions.append(RuleCondition(
                        condition_text=cond.get("condition_text", ""),
                        logical_operator=cond.get("logical_operator"),
                        role=RoleType(cond.get("role")) if cond.get("role") in [e.value for e in RoleType] else None
                    ))
                
                corrected_rule = LegislationRule(
                    rule_id=rule_data.get("rule_id", ""),
                    rule_text=rule_data.get("rule_text", ""),
                    applies_to_countries=rule_data.get("applies_to_countries", []),
                    conditions=conditions,
                    condition_count=len(conditions),
                    references=rule_data.get("references", []),
                    extraction_metadata=rule_data.get("extraction_metadata", {}),
                    confidence_score=float(rule_data.get("confidence_score", 0.8)),
                    duplicate_of=rule_data.get("duplicate_of")
                )
                corrected_rules.append(corrected_rule)
            
            return corrected_rules
        else:
            # No structural corrections needed, return original rules
            logger.info("Validation completed without structural corrections needed")
            return rules
    
    def _route_next(self, state: AgentState) -> str:
        """Route to the next agent based on current state"""
        return state.next_agent
    
    async def run(self) -> AgentState:
        """Run the complete multi-agent workflow"""
        logger.info("Starting enhanced multi-agent legislation processing workflow")
        
        initial_state = AgentState()
        thread_config = {"configurable": {"thread_id": "legislation_processing"}}
        
        final_state = await self.app.ainvoke(initial_state, config=thread_config)
        
        logger.info("Workflow completed successfully")
        logger.info(f"Original rules: {len(final_state.rules)}")
        logger.info(f"Final deduplicated rules: {len(final_state.deduplicated_rules)}")
        logger.info(f"Adequacy countries identified: {final_state.adequacy_countries}")
        
        if final_state.error_messages:
            logger.warning(f"Workflow completed with errors: {final_state.error_messages}")
        
        return final_state

async def main():
    """Main entry point for the legislation rule extraction system"""
    
    # Validate OpenAI client
    if not openai_client:
        logger.error("OpenAI client not initialized. Please check OPENAI_API_KEY and network connectivity.")
        print("❌ Error: OpenAI client not initialized. Please check:")
        print("  - OPENAI_API_KEY environment variable is set correctly")
        print("  - Network connectivity to OpenAI API")
        print("  - API key has sufficient permissions and credits")
        sys.exit(1)
    
    # Validate required files exist
    required_files = [LEGISLATION_METADATA_PATH, GEOGRAPHY_PATH]
    for file_path in required_files:
        if not os.path.exists(file_path):
            logger.error(f"Required file not found: {file_path}")
            print(f"❌ Error: Required file not found: {file_path}")
            
            # Create example files if they don't exist
            if file_path == LEGISLATION_METADATA_PATH:
                print(f"Creating example {file_path}...")
                example_metadata = [
                    {
                        "path": "./input_pdfs/example_regulation.pdf",
                        "jurisdiction": "EU"
                    }
                ]
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(example_metadata, f, indent=2)
                    print(f"✅ Created example {file_path}")
                except Exception as e:
                    print(f"❌ Failed to create {file_path}: {e}")
                    sys.exit(1)
            
            elif file_path == GEOGRAPHY_PATH:
                print(f"❌ Please ensure {file_path} exists with proper geography data")
                sys.exit(1)
    
    # Validate that input PDF directory exists
    if not os.path.exists(INPUT_PDF_PATH):
        logger.warning(f"Input PDF directory not found: {INPUT_PDF_PATH}")
        print(f"⚠️  Warning: Input PDF directory not found: {INPUT_PDF_PATH}")
        print("Creating input directory...")
        os.makedirs(INPUT_PDF_PATH, exist_ok=True)
        print(f"✅ Created {INPUT_PDF_PATH}")
    
    logger.info("Starting Enhanced Legislation Rule Extraction System")
    logger.info(f"Model: {MODEL_NAME}")
    logger.info(f"Embedding Model: {EMBEDDING_MODEL}")
    logger.info(f"Output Path: {OUTPUT_PATH}")
    
    print("\n🚀 Starting Enhanced Legislation Rule Extraction System")
    print(f"📄 Model: {MODEL_NAME}")
    print(f"🔍 Embedding Model: {EMBEDDING_MODEL}")
    print(f"📁 Output Path: {OUTPUT_PATH}")
    
    # Initialize and run supervisor
    try:
        supervisor = SupervisorAgent()
        
        final_state = await supervisor.run()
        
        print("\n" + "="*60)
        print("ENHANCED LEGISLATION RULE EXTRACTION COMPLETED")
        print("="*60)
        print(f"Original rules extracted: {len(final_state.rules)}")
        print(f"Deduplicated rules: {len(final_state.deduplicated_rules)}")
        print(f"Duplicates removed: {len(final_state.rules) - len(final_state.deduplicated_rules)}")
        print(f"Documents processed: {len(final_state.documents)}")
        print(f"Entities resolved: {len(final_state.extracted_entities)}")
        print(f"Adequacy countries identified: {final_state.adequacy_countries}")
        print(f"Sanity check completed: ✓")
        print(f"Output files generated in: {OUTPUT_PATH}")
        
        if final_state.error_messages:
            print(f"\n⚠️  Errors encountered: {len(final_state.error_messages)}")
            for error in final_state.error_messages:
                print(f"  - {error}")
        
        # Print sample validated rules
        final_rules = final_state.deduplicated_rules if final_state.deduplicated_rules else final_state.rules
        if final_rules:
            print("\n📋 Sample validated rules:")
            for i, rule in enumerate(final_rules[:3]):
                print(f"\nRule {i+1}:")
                print(f"  ID: {rule.rule_id}")
                print(f"  Text: {rule.rule_text[:100]}...")
                print(f"  Countries: {rule.applies_to_countries}")
                print(f"  Conditions: {rule.condition_count}")
                print(f"  Confidence: {rule.confidence_score:.2f}")
                if rule.duplicate_of:
                    print(f"  Duplicate of: {rule.duplicate_of}")
        else:
            print("\n⚠️  No rules were extracted. Check the error messages above.")
        
    except Exception as e:
        logger.error(f"System failed: {str(e)}")
        print(f"\n❌ System failed: {str(e)}")
        print("Check the logs for more details.")
        sys.exit(1)

async def test_system_components():
    """Test system components before running main workflow"""
    print("🧪 Testing system components...")
    
    # Test OpenAI client
    try:
        if openai_client:
            test_response = openai_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": "Test connection"}]
            )
            print("✅ OpenAI client connection successful")
        else:
            print("❌ OpenAI client not available")
            return False
    except Exception as e:
        print(f"❌ OpenAI client test failed: {e}")
        return False
    
    # Test embeddings
    try:
        embeddings = CustomEmbeddings()
        test_embedding = embeddings.embed_query("test query")
        if len(test_embedding) > 0:
            print("✅ Embeddings working correctly")
        else:
            print("❌ Embeddings test failed")
            return False
    except Exception as e:
        print(f"❌ Embeddings test failed: {e}")
        return False
    
    # Test geography manager
    try:
        processor = LegislationProcessor()
        geography_data = processor.load_geography_data()
        if geography_data:
            geo_manager = GeographyManager(geography_data)
            test_country = geo_manager.get_country_info("US")
            if test_country:
                print("✅ Geography manager working correctly")
            else:
                print("⚠️  Geography manager loaded but no US country found")
        else:
            print("❌ Geography data could not be loaded")
            return False
    except Exception as e:
        print(f"❌ Geography manager test failed: {e}")
        return False
    
    return True


if __name__ == "__main__":
    async def run_with_tests():
        # Run component tests first
        if await test_system_components():
            print("✅ All system components tested successfully\n")
            await main()
        else:
            print("❌ System component tests failed. Please check configuration.")
            sys.exit(1)
    
    asyncio.run(run_with_tests())
