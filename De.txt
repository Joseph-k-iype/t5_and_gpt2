import asyncio
import json
import logging
import os
import csv
import glob
import math
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from enum import Enum
from datetime import datetime, timedelta
from urllib.parse import urljoin

# Core dependencies
import openai
from openai import OpenAI
from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict, ValidationInfo
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI

# PDF processing
try:
    import pymupdf  # Modern PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    try:
        import pdfplumber
        PDF_AVAILABLE = True
    except ImportError:
        PDF_AVAILABLE = False
        print("Warning: No PDF library found. Install PyMuPDF or pdfplumber: pip install PyMuPDF pdfplumber")

# Optional: RDF library for advanced semantic processing
try:
    import rdflib
    from rdflib import Graph, Namespace, URIRef, Literal, BNode
    from rdflib.namespace import RDF, RDFS, XSD
    import urllib.parse
    RDF_AVAILABLE = True
    print("RDFLib available for advanced RDF processing")
except ImportError:
    RDF_AVAILABLE = False
    print("Warning: RDFLib not found. Install with: pip install rdflib")
    pass

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================
# GLOBAL CONFIGURATION
# ===============================

class Config:
    """Global configuration for the legislation rules converter."""
    BASE_URL = "https://api.openai.com/v1"
    API_KEY = os.getenv("OPENAI_API_KEY")
    CHAT_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Paths
    LEGISLATION_PDF_PATH = "./legislation_pdfs/"
    RULES_OUTPUT_PATH = "./extracted_rules/"
    EMBEDDINGS_PATH = "./embeddings/"
    LOGS_PATH = "./logs/"
    EXISTING_RULES_FILE = "./extracted_rules/all_rules.json"
    METADATA_CONFIG_FILE = "./legislation_metadata.json"
    
    # Combined Standards Output Path
    STANDARDS_OUTPUT_PATH = "./standards_output/"
    
    # Updated Standard Namespaces - Latest DPV v2.1
    DPV_NAMESPACE = "https://w3id.org/dpv#"
    DPV_PD_NAMESPACE = "https://w3id.org/dpv/dpv-pd#"
    DPV_TECH_NAMESPACE = "https://w3id.org/dpv/tech#"
    DPV_LEGAL_NAMESPACE = "https://w3id.org/dpv/legal/"
    ODRL_NAMESPACE = "http://www.w3.org/ns/odrl/2/"
    DPVCG_NAMESPACE = "https://w3id.org/dpv/"
    ACTION_NAMESPACE = "https://w3id.org/dpv/actions#"
    
    # PDF Processing Configuration
    CHUNK_SIZE = 4000  # Characters per chunk for large documents
    OVERLAP_SIZE = 200  # Character overlap between chunks
    MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB threshold for chunking

# Validate API key
if not Config.API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is required")

# ===============================
# ENHANCED DATA MODELS WITH PYDANTIC V2
# ===============================

class DataDomain(str, Enum):
    """Data domains as per privacy regulations."""
    DATA_TRANSFER = "data_transfer"
    DATA_USAGE = "data_usage" 
    DATA_STORAGE = "data_storage"
    DATA_COLLECTION = "data_collection"
    DATA_DELETION = "data_deletion"

class DataRole(str, Enum):
    """Roles in data processing - Updated without supervisory_authority."""
    CONTROLLER = "controller"
    PROCESSOR = "processor"
    JOINT_CONTROLLER = "joint_controller"
    DATA_SUBJECT = "data_subject"

class DataCategory(str, Enum):
    """Categories of personal data."""
    PERSONAL_DATA = "personal_data"
    SENSITIVE_DATA = "sensitive_data"
    BIOMETRIC_DATA = "biometric_data"
    HEALTH_DATA = "health_data"
    FINANCIAL_DATA = "financial_data"
    LOCATION_DATA = "location_data"
    BEHAVIORAL_DATA = "behavioral_data"
    IDENTIFICATION_DATA = "identification_data"

class ConditionOperator(str, Enum):
    """Operators for rule conditions."""
    EQUAL = "equal"
    NOT_EQUAL = "notEqual"
    GREATER_THAN = "greaterThan"
    LESS_THAN = "lessThan"
    GREATER_THAN_EQUAL = "greaterThanInclusive"
    LESS_THAN_EQUAL = "lessThanInclusive"
    CONTAINS = "contains"
    NOT_CONTAINS = "doesNotContain"
    IN = "in"
    NOT_IN = "notIn"

class DocumentLevel(str, Enum):
    """Document processing levels."""
    LEVEL_1 = "level_1"  # Actual legislation
    LEVEL_2 = "level_2"  # Regulator guidance
    LEVEL_3 = "level_3"  # Additional guidance

# GDPR Processing Purposes - Updated to match GDPR specifications
class ProcessingPurpose(str, Enum):
    """GDPR-compliant processing purposes."""
    CONSENT = "consent"
    CONTRACTUAL_NECESSITY = "contractual_necessity"
    LEGAL_OBLIGATION = "legal_obligation"
    VITAL_INTERESTS = "vital_interests"
    PUBLIC_TASK = "public_task"
    LEGITIMATE_INTERESTS = "legitimate_interests"

# GDPR Legal Basis - Updated to match GDPR specifications
class LegalBasis(str, Enum):
    """GDPR-compliant legal basis."""
    CONSENT = "consent"
    CONTRACTUAL_OBLIGATION = "contractual_obligation"
    LEGAL_OBLIGATION = "legal_obligation"
    VITAL_INTERESTS = "vital_interests"
    PUBLIC_INTEREST_OFFICIAL_AUTHORITY = "public_interest_official_authority"
    LEGITIMATE_INTERESTS = "legitimate_interests"

# RuleAction class - Updated for better inference
class RuleAction(BaseModel):
    """Action that can be taken based on a rule - inferred from legislation."""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str = Field(..., description="Unique action identifier")
    action_type: str = Field(..., description="Type of action inferred from legislation")
    title: str = Field(..., description="Action title in simple English")
    description: str = Field(..., description="What must be done in simple English")
    priority: str = Field(..., description="Action priority based on legislative language")
    
    # Implementation details
    data_specific_steps: List[str] = Field(..., description="Specific steps for data handling")
    responsible_role: Optional[str] = Field(None, description="Who is responsible for this action")
    
    # Compliance context
    legislative_requirement: str = Field(..., description="Specific legislative requirement")
    data_impact: str = Field(..., description="How this affects data processing")
    verification_method: List[str] = Field(default_factory=list, description="How to verify completion")
    
    # Optional timeline
    timeline: Optional[str] = Field(None, description="Timeline if specified in legislation")
    
    # Metadata
    derived_from_text: str = Field(..., description="Legislative text this action was derived from")
    applicable_countries: List[str] = Field(default_factory=list, description="Countries where action applies")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Confidence in action relevance")

# UserAction class - Updated to remove tools_needed and prerequisites
class UserAction(BaseModel):
    """Specific user action inferred from legislation."""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str = Field(..., description="Unique user action identifier")
    action_type: str = Field(..., description="Type of data action user must perform")
    title: str = Field(..., description="Clear action title in simple English")
    description: str = Field(..., description="What the user must do in simple English")
    priority: str = Field(..., description="Priority level based on legislative context")
    
    # User-specific implementation details
    user_data_steps: List[str] = Field(..., description="Concrete steps for user data handling")
    affected_data_categories: List[str] = Field(default_factory=list, description="Data categories affected")
    user_role_context: Optional[str] = Field(None, description="User's role when performing this action")
    
    # Legislative basis
    legislative_requirement: str = Field(..., description="Specific legal requirement")
    compliance_outcome: str = Field(..., description="What compliance outcome this achieves")
    user_verification_steps: List[str] = Field(default_factory=list, description="How user can verify completion")
    
    # Implementation guidance for users - REMOVED tools_needed and prerequisites
    timeline: Optional[str] = Field(None, description="Timeline if specified in legislation")
    
    # Metadata
    derived_from_text: str = Field(..., description="Legislative text this action was derived from")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Confidence in action inference")

class RuleCondition(BaseModel):
    """Individual condition within a rule."""
    model_config = ConfigDict(use_enum_values=True)
    
    fact: str = Field(..., description="The fact/data point to evaluate")
    operator: ConditionOperator = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    path: Optional[str] = Field(None, description="JSONPath to navigate nested objects")
    description: str = Field(..., description="Human-readable description of this condition")
    data_domain: List[DataDomain] = Field(default_factory=list, description="Applicable data domains")
    role: Optional[DataRole] = Field(None, description="Role this condition applies to")
    reasoning: str = Field(..., description="LLM reasoning for why this condition was extracted")
    document_level: DocumentLevel = Field(..., description="Document level this condition was extracted from")
    chunk_reference: Optional[str] = Field(None, description="Reference to source chunk if document was chunked")

    @field_validator('data_domain', mode='before')
    @classmethod
    def validate_data_domain(cls, v):
        if not v:
            return []
        if isinstance(v, list):
            result = []
            for item in v:
                if isinstance(item, str):
                    try:
                        result.append(DataDomain(item))
                    except ValueError:
                        continue
                elif isinstance(item, DataDomain):
                    result.append(item)
            return result
        return []

    @field_validator('role', mode='before')
    @classmethod
    def validate_role(cls, v):
        if v is None:
            return None
        if isinstance(v, str):
            try:
                return DataRole(v)
            except ValueError:
                return None
        elif isinstance(v, DataRole):
            return v
        return None

    @field_validator('operator', mode='before')
    @classmethod
    def validate_operator(cls, v):
        if isinstance(v, str):
            try:
                return ConditionOperator(v)
            except ValueError:
                return ConditionOperator.EQUAL
        elif isinstance(v, ConditionOperator):
            return v
        return ConditionOperator.EQUAL

    @field_validator('document_level', mode='before')
    @classmethod
    def validate_document_level(cls, v):
        if isinstance(v, str):
            try:
                return DocumentLevel(v)
            except ValueError:
                return DocumentLevel.LEVEL_1
        elif isinstance(v, DocumentLevel):
            return v
        return DocumentLevel.LEVEL_1

class RuleEvent(BaseModel):
    """Event triggered when rule conditions are met."""
    type: str = Field(..., description="Type of event/action")
    params: Dict[str, Any] = Field(default_factory=dict, description="Event parameters")

class LegislationRule(BaseModel):
    """Complete rule structure aligned with json-rules-engine format."""
    model_config = ConfigDict(use_enum_values=True)
    
    id: str = Field(..., description="Unique rule identifier")
    name: str = Field(..., description="Rule name")
    description: str = Field(..., description="Human-readable rule description")
    source_article: str = Field(..., description="Source legislation article/section")
    source_file: str = Field(..., description="Source PDF filename")
    
    conditions: Dict[str, List[RuleCondition]] = Field(
        ..., 
        description="Rule conditions with 'all', 'any', or 'not' logic"
    )
    event: RuleEvent = Field(..., description="Event triggered when conditions are met")
    
    # Actions - Now optional to allow inference
    actions: List[RuleAction] = Field(default_factory=list, description="Actions inferred from legislative text")
    user_actions: List[UserAction] = Field(default_factory=list, description="User-specific actions inferred from legislation")
    
    priority: int = Field(default=1, description="Rule priority (1-10)")
    
    # Required fields with validation
    primary_impacted_role: Optional[DataRole] = Field(None, description="Primary role most impacted by this rule")
    secondary_impacted_role: Optional[DataRole] = Field(None, description="Secondary role impacted by this rule")
    data_category: List[DataCategory] = Field(default_factory=list, description="Categories of data this rule applies to")
    
    # Updated country metadata structure
    applicable_countries: List[str] = Field(..., description="Countries where this rule applies")
    adequacy_countries: List[str] = Field(default_factory=list, description="Adequacy countries")
    
    # Document levels processed
    source_documents: Dict[str, Optional[str]] = Field(default_factory=dict, description="Source documents by level")
    processing_metadata: Dict[str, Any] = Field(default_factory=dict, description="Processing metadata including chunking info")
    
    # Metadata
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    extraction_method: str = Field(default="llm_analysis_with_inferred_actions")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Extraction confidence")

    @field_validator('conditions', mode='after')
    @classmethod
    def validate_conditions_structure(cls, v):
        if not isinstance(v, dict):
            raise ValueError("Conditions must be a dictionary")
        valid_keys = {'all', 'any', 'not'}
        if not any(key in valid_keys for key in v.keys()):
            raise ValueError("Conditions must contain 'all', 'any', or 'not' keys")
        return v

    # Remove the mandatory actions validation
    @field_validator('actions', mode='after')
    @classmethod
    def validate_actions_optional(cls, v):
        # Actions are now optional - they will be inferred if possible
        return v if v is not None else []

    @field_validator('primary_impacted_role', mode='before')
    @classmethod
    def validate_primary_role(cls, v):
        if v is None:
            return None
        if isinstance(v, str):
            try:
                return DataRole(v)
            except ValueError:
                return None
        elif isinstance(v, DataRole):
            return v
        return None

    @field_validator('secondary_impacted_role', mode='before')
    @classmethod
    def validate_secondary_role(cls, v):
        if v is None:
            return None
        if isinstance(v, str):
            try:
                return DataRole(v)
            except ValueError:
                return None
        elif isinstance(v, DataRole):
            return v
        return None

    @field_validator('data_category', mode='before')
    @classmethod
    def validate_data_category(cls, v):
        if not v:
            return []
        if isinstance(v, list):
            result = []
            for item in v:
                if isinstance(item, str):
                    try:
                        result.append(DataCategory(item))
                    except ValueError:
                        continue
                elif isinstance(item, DataCategory):
                    result.append(item)
            return result
        return []

# ===============================
# UPDATED METADATA STRUCTURE
# ===============================

class CountryMetadata(BaseModel):
    """Updated metadata for country configurations."""
    model_config = ConfigDict(validate_assignment=True)
    
    country: List[str] = Field(..., description="List of applicable countries")
    adequacy_country: List[str] = Field(default_factory=list, description="List of adequacy countries")
    file_level_1: Optional[str] = Field(None, description="Level 1 document (actual legislation)")
    file_level_2: Optional[str] = Field(None, description="Level 2 document (regulator guidance)")
    file_level_3: Optional[str] = Field(None, description="Level 3 document (additional guidance)")

    @field_validator('country', mode='after')
    @classmethod
    def validate_country_not_empty(cls, v):
        if not v:
            raise ValueError("At least one country must be specified")
        return v

class MetadataManager:
    """Manages legislation metadata configuration."""
    
    def __init__(self, config_file: str = Config.METADATA_CONFIG_FILE):
        self.config_file = config_file
        self.metadata: Dict[str, Any] = {}
        self.load_metadata()
    
    def load_metadata(self):
        """Load metadata from config file."""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                logger.info(f"Loaded metadata for {len(self.metadata)} configurations")
            else:
                logger.warning(f"Metadata config file not found: {self.config_file}")
                logger.warning("Please create legislation_metadata.json with your legislation configuration")
                self.metadata = {}
        except Exception as e:
            logger.error(f"Error loading metadata: {e}")
            self.metadata = {}
    
    def get_country_metadata(self, entry_id: str) -> Optional[CountryMetadata]:
        """Get metadata for a specific entry."""
        if entry_id in self.metadata:
            try:
                return CountryMetadata(**self.metadata[entry_id])
            except Exception as e:
                logger.error(f"Error parsing metadata for {entry_id}: {e}")
                return None
        return None
    
    def get_all_processing_entries(self) -> List[Tuple[str, CountryMetadata]]:
        """Get all processing entries."""
        entries = []
        for entry_id, data in self.metadata.items():
            try:
                metadata = CountryMetadata(**data)
                entries.append((entry_id, metadata))
            except Exception as e:
                logger.warning(f"Skipping invalid entry {entry_id}: {e}")
        return entries

# ===============================
# PDF PROCESSING WITH DYNAMIC CHUNKING
# ===============================

class DocumentChunk:
    """Represents a chunk of a document."""
    def __init__(self, content: str, chunk_index: int, total_chunks: int, start_pos: int, end_pos: int):
        self.content = content
        self.chunk_index = chunk_index
        self.total_chunks = total_chunks
        self.start_pos = start_pos
        self.end_pos = end_pos
        self.chunk_id = f"chunk_{chunk_index}_{total_chunks}"

class PDFProcessor:
    """Enhanced PDF processor with dynamic chunking for large files."""
    
    @staticmethod
    def get_file_size(filepath: str) -> int:
        """Get file size in bytes."""
        return os.path.getsize(filepath)
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str) -> str:
        """Extract text from PDF file."""
        if not PDF_AVAILABLE:
            raise ImportError("No PDF library available. Install PyMuPDF or pdfplumber")
        
        try:
            if 'pymupdf' in globals():
                return PDFProcessor._extract_with_pymupdf(pdf_path)
            else:
                return PDFProcessor._extract_with_pdfplumber(pdf_path)
        except Exception as e:
            logger.error(f"Error reading PDF {pdf_path}: {e}")
            raise
    
    @staticmethod
    def _extract_with_pymupdf(pdf_path: str) -> str:
        """Extract text using modern PyMuPDF."""
        text = ""
        try:
            with pymupdf.open(pdf_path) as doc:
                for page in doc:
                    page_text = page.get_text()
                    if page_text:
                        text += page_text + "\n"
        except Exception as e:
            logger.error(f"PyMuPDF extraction failed: {e}")
            raise
        return text
    
    @staticmethod
    def _extract_with_pdfplumber(pdf_path: str) -> str:
        """Extract text using pdfplumber."""
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    
    @staticmethod
    def chunk_text(text: str, chunk_size: int = Config.CHUNK_SIZE, overlap_size: int = Config.OVERLAP_SIZE) -> List[DocumentChunk]:
        """Dynamically chunk text based on size with overlaps."""
        if len(text) <= chunk_size:
            return [DocumentChunk(text, 0, 1, 0, len(text))]
        
        chunks = []
        start = 0
        chunk_index = 0
        
        # Calculate total chunks
        total_chunks = math.ceil(len(text) / (chunk_size - overlap_size))
        
        while start < len(text):
            # Calculate end position
            end = min(start + chunk_size, len(text))
            
            # Try to break at sentence boundaries if possible
            if end < len(text):
                # Look for sentence endings within the last 200 characters
                search_start = max(end - 200, start)
                sentence_endings = ['.', '!', '?', '\n\n']
                
                best_break = -1
                for ending in sentence_endings:
                    pos = text.rfind(ending, search_start, end)
                    if pos > best_break:
                        best_break = pos + 1
                
                if best_break > start:
                    end = best_break
            
            chunk_content = text[start:end].strip()
            if chunk_content:
                chunk = DocumentChunk(chunk_content, chunk_index, total_chunks, start, end)
                chunks.append(chunk)
                chunk_index += 1
            
            # Move start position with overlap
            if end >= len(text):
                break
            start = max(end - overlap_size, start + 1)
        
        return chunks
    
    @staticmethod
    def should_chunk_file(filepath: str) -> bool:
        """Determine if file should be chunked based on size."""
        file_size = PDFProcessor.get_file_size(filepath)
        return file_size > Config.MAX_FILE_SIZE

class MultiLevelPDFProcessor:
    """Process PDFs from multiple document levels with chunking support."""
    
    def __init__(self):
        self.pdf_processor = PDFProcessor()
    
    def process_country_documents(self, entry_id: str, metadata: CountryMetadata, base_path: str) -> Dict[str, Union[str, List[DocumentChunk]]]:
        """Process all documents for a country entry with dynamic chunking."""
        documents = {}
        
        # Process all available levels
        level_files = {
            "level_1": metadata.file_level_1,
            "level_2": metadata.file_level_2,
            "level_3": metadata.file_level_3
        }
        
        for level, filename in level_files.items():
            if filename:
                file_path = os.path.join(base_path, filename)
                if os.path.exists(file_path):
                    try:
                        text = self.pdf_processor.extract_text_from_pdf(file_path)
                        
                        # Check if chunking is needed
                        if self.pdf_processor.should_chunk_file(file_path):
                            logger.info(f"Chunking {level} document: {filename}")
                            chunks = self.pdf_processor.chunk_text(text)
                            documents[level] = chunks
                        else:
                            documents[level] = text
                            
                        logger.info(f"Processed {level} document: {filename}")
                    except Exception as e:
                        logger.error(f"Error processing {level} document {filename}: {e}")
                else:
                    logger.warning(f"{level} document not found: {file_path}")
        
        return documents

# ===============================
# ALL ORIGINAL ACTION INFERENCE TOOLS - RESTORED
# ===============================

@tool
def extract_rule_conditions(legislation_text: str, focus_area: str) -> str:
    """Extract specific rule conditions from legislation text."""
    
    prompt = f"""
    Extract rule conditions from the following legislation text, focusing on {focus_area}.
    
    Return conditions in json-rules-engine format based on explicit requirements in the text.
    
    Text: {legislation_text}
    
    Focus on identifying:
    - Specific facts that can be evaluated from the legislation
    - Comparison operators based on legal language
    - Values to compare against as stated in the text
    - Data domains and roles mentioned
    
    Return valid JSON only. Base conditions on explicit legislative language.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error extracting conditions: {str(e)}"

@tool
def analyze_data_domains(legislation_text: str) -> str:
    """Analyze and identify relevant data domains in legislation."""
    
    prompt = f"""
    Analyze the following legislation text and identify which data domains are mentioned:
    - data_transfer
    - data_usage
    - data_storage
    - data_collection
    - data_deletion
    
    Text: {legislation_text}
    
    Return a JSON object mapping each identified domain to its relevance and the specific text that indicates it.
    Include only domains that are mentioned in the legislation text.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error analyzing domains: {str(e)}"

@tool
def identify_roles_responsibilities(legislation_text: str) -> str:
    """Identify roles and responsibilities in legislation."""
    
    prompt = f"""
    Identify the roles and responsibilities mentioned in this legislation:
    - controller
    - processor 
    - joint_controller
    - data_subject
    
    Text: {legislation_text}
    
    For each role mentioned, identify their specific obligations and responsibilities as stated in the text.
    Return a JSON object with role mappings based on what is stated.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error identifying roles: {str(e)}"

# UPDATED TOOLS with better prompts for data operations
@tool
def infer_data_processing_actions(legislation_text: str, data_categories: str, processing_context: str) -> str:
    """Infer specific data processing actions from legislation text."""
    
    prompt = f"""
    Based on the following legislation text, identify specific organizational actions that must be taken regarding data processing.
    
    Legislation Text: {legislation_text}
    Data Categories Mentioned: {data_categories}
    Processing Context: {processing_context}
    
    Extract only actions that are:
    1. Explicitly required by the legislation text
    2. Related to data handling, processing, storage, transfer, or deletion
    3. Actionable by data controllers or processors at organizational level
    4. Have clear data-specific outcomes
    5. Focus on practical data operations like:
       - Data encryption and security measures
       - Data masking and anonymization
       - Access controls and authentication
       - Data retention and deletion procedures
       - Consent management systems
       - Data transfer protocols
       - Audit logging and monitoring
       - Backup and recovery procedures
    6. Described in simple, clear English - avoid legal jargon
    
    For each action, provide:
    - action_type: Brief descriptive name focused on data operations (e.g., "implement_data_encryption", "establish_consent_management")
    - title: Clear action title in simple English
    - description: What must be done with data in simple English
    - priority: Extract from legislative language (urgent/immediate/high/medium/low)
    - data_specific_steps: Concrete steps related to data handling
    - legislative_requirement: Exact requirement from legislation
    - data_impact: How this affects data processing
    - verification_method: How to confirm compliance
    - derived_from_text: Exact legislative text that requires this action
    
    Return valid JSON array. Only include actions explicitly stated or clearly implied by the legislation.
    If no actions can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring data processing actions: {str(e)}"

@tool
def infer_compliance_verification_actions(legislation_text: str, obligations: str, roles: str) -> str:
    """Infer compliance verification actions from legislation."""
    
    prompt = f"""
    Based on the legislation text and identified obligations, extract verification and compliance actions.
    
    Legislation Text: {legislation_text}
    Identified Obligations: {obligations}
    Affected Roles: {roles}
    
    Focus only on actions that:
    1. Are explicitly required for compliance verification
    2. Involve documentation, reporting, or demonstration of data handling
    3. Are mentioned in the legislation text
    4. Can be performed by the specified roles
    5. Focus on practical data verification like:
       - Data audit procedures
       - Compliance monitoring systems
       - Data processing records
       - Privacy impact assessments
       - Data breach notification procedures
       - Regular security reviews
       - Third-party data processor audits
    6. Described in simple, clear English - avoid legal jargon
    
    For each verification action:
    - action_type: Type of verification required (focused on data aspects)
    - title: What needs to be verified in simple English
    - description: How to demonstrate compliance in simple English
    - data_specific_steps: Steps involving data or data processes
    - legislative_requirement: Specific legal requirement
    - verification_method: How compliance is verified
    - derived_from_text: Exact text requiring this verification
    
    Return valid JSON array. Base all actions on explicit legislative requirements only.
    If no actions can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring compliance verification actions: {str(e)}"

@tool
def infer_data_subject_rights_actions(legislation_text: str, rights_mentioned: str, data_domains: str) -> str:
    """Infer actions required to handle data subject rights."""
    
    prompt = f"""
    Analyze the legislation for requirements related to data subject rights and extract required actions.
    
    Legislation Text: {legislation_text}
    Rights Mentioned: {rights_mentioned}
    Data Domains: {data_domains}
    
    Extract actions that:
    1. Are required to facilitate data subject rights
    2. Involve handling, processing, or responding to data subject requests
    3. Are explicitly mentioned in the legislation
    4. Have clear data-handling implications
    5. Focus on practical data rights implementation like:
       - Data access systems and procedures
       - Data rectification workflows
       - Data erasure and deletion procedures
       - Data portability tools and formats
       - Consent withdrawal mechanisms
       - Objection handling processes
       - Automated decision-making controls
    6. Described in simple, clear English - avoid legal jargon
    
    For each rights-related action:
    - action_type: Type of rights handling required (focused on data operations)
    - title: Rights-related obligation in simple English
    - description: What must be done to support data subject rights in simple English
    - data_specific_steps: Specific data handling steps
    - legislative_requirement: Legal basis for the action
    - data_impact: How this affects data and data processing
    - verification_method: How to confirm rights are being respected
    - derived_from_text: Legislative text requiring this action
    
    Return valid JSON array. Only include actions with clear legislative basis.
    If no actions can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring data subject rights actions: {str(e)}"

# UPDATED USER ACTION INFERENCE TOOLS with improved prompts
@tool
def infer_user_actionable_tasks(legislation_text: str, data_context: str, user_roles: str) -> str:
    """Infer practical tasks that users can perform based on legislation."""
    
    prompt = f"""
    Analyze the following legislation text to identify specific tasks that individual users can perform.
    
    Legislation Text: {legislation_text}
    Data Context: {data_context}
    User Roles: {user_roles}
    
    Extract ONLY tasks that are:
    1. Explicitly required by the legislation text
    2. Actionable by individual users with commonly available tools/systems
    3. Related to data operations users can control
    4. Have clear compliance outcomes
    5. Can be practically implemented by users
    6. Focus on individual data actions like:
       - Encrypting personal files and communications
       - Using privacy settings on platforms
       - Managing consent preferences
       - Securing personal data with passwords
       - Backing up important data securely
       - Deleting unnecessary personal data
       - Reviewing data sharing permissions
       - Using secure communication tools
    7. Described in simple, clear English - avoid legal jargon
    
    For each user task, provide:
    - action_type: Specific data operation (e.g., "encrypt_personal_data", "manage_consent_settings")
    - title: Clear task title for users in simple English
    - description: What users must do in simple English
    - priority: Based on legislative urgency
    - user_data_steps: Concrete steps for user data handling
    - affected_data_categories: Types of data involved
    - legislative_requirement: Exact requirement from legislation
    - compliance_outcome: What compliance goal this achieves
    - user_verification_steps: How users can verify completion
    - derived_from_text: Exact text requiring this task
    
    Focus on practical, implementable data tasks. Avoid abstract concepts.
    Return valid JSON array based ONLY on explicit legislative requirements.
    If no user actions can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring user actionable tasks: {str(e)}"

@tool
def infer_user_compliance_tasks(legislation_text: str, compliance_obligations: str, data_domains: str) -> str:
    """Infer compliance-related tasks users can perform."""
    
    prompt = f"""
    Based on the legislation and compliance obligations, identify specific tasks users can perform.
    
    Legislation Text: {legislation_text}
    Compliance Obligations: {compliance_obligations}
    Data Domains: {data_domains}
    
    Extract tasks that users can perform to achieve compliance, focusing on:
    1. Documentation and record-keeping for their own data processing
    2. Implementing data protection measures they can control
    3. Handling data subject requests they receive
    4. Conducting self-assessments and audits
    5. Establishing processes and procedures within their control
    6. Individual data compliance actions like:
       - Maintaining personal data inventories
       - Implementing data retention schedules
       - Setting up automatic data deletion
       - Creating data backup procedures
       - Establishing secure data sharing practices
       - Using privacy-focused tools and services
       - Regular security updates and patches
    7. Described in simple, clear English - avoid legal jargon
    
    For each user compliance task:
    - action_type: Type of compliance task (focused on data)
    - title: What users need to accomplish in simple English
    - description: Specific compliance task for users in simple English
    - user_data_steps: Steps involving actual data
    - legislative_requirement: Legal basis for the task
    - compliance_outcome: Compliance goal achieved
    - user_verification_steps: How to confirm compliance
    - derived_from_text: Legislative text requiring this
    
    Return valid JSON array. Base tasks on explicit legislative requirements.
    If no user compliance tasks can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring user compliance tasks: {str(e)}"

@tool
def infer_user_rights_support_tasks(legislation_text: str, rights_context: str, processing_activities: str) -> str:
    """Infer tasks users can perform to support data subject rights."""
    
    prompt = f"""
    Analyze the legislation for tasks users can perform to facilitate data subject rights.
    
    Legislation Text: {legislation_text}
    Rights Context: {rights_context}
    Processing Activities: {processing_activities}
    
    Identify tasks users can perform to support data subject rights:
    1. Setting up systems they control to handle rights requests
    2. Implementing processes for data access, rectification, erasure they can manage
    3. Ensuring data portability capabilities within their systems
    4. Managing consent and withdrawal mechanisms
    5. Handling objections and restrictions
    6. Individual rights support actions like:
       - Creating personal data access tools
       - Setting up data export capabilities
       - Implementing consent tracking systems
       - Creating data correction workflows
       - Setting up automated deletion triggers
       - Maintaining contact preferences
       - Using privacy-preserving technologies
    7. Described in simple, clear English - avoid legal jargon
    
    For each rights-support task:
    - action_type: Type of rights support task (focused on data)
    - title: User-facing task title in simple English
    - description: What users must implement in simple English
    - user_data_steps: Specific data operations required
    - affected_data_categories: Data types involved
    - legislative_requirement: Rights provision requiring this
    - compliance_outcome: Rights facilitation achieved
    - derived_from_text: Text mandating this task
    
    Return valid JSON array. Focus on practical implementation by users.
    If no user rights support tasks can be inferred, return empty array.
    """
    
    try:
        client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
        
        response = client.chat.completions.create(
            model=Config.CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error inferring user rights support tasks: {str(e)}"

# ===============================
# UPDATED DPV CONCEPTS WITH GDPR-COMPLIANT MAPPINGS
# ===============================

class DPVConcepts:
    """DPV (Data Privacy Vocabulary) concept mappings with GDPR-compliant processing purposes."""
    
    # Updated DPV Core Namespaces v2.1
    DPV = Config.DPV_NAMESPACE
    DPV_PD = Config.DPV_PD_NAMESPACE
    DPV_TECH = Config.DPV_TECH_NAMESPACE
    DPV_LEGAL = Config.DPV_LEGAL_NAMESPACE
    DPV_ACTION = Config.ACTION_NAMESPACE
    
    # GDPR-Compliant Processing Purposes
    PROCESSING_PURPOSES = {
        ProcessingPurpose.CONSENT.value: f"{DPV}Consent",
        ProcessingPurpose.CONTRACTUAL_NECESSITY.value: f"{DPV}ContractualNecessity",
        ProcessingPurpose.LEGAL_OBLIGATION.value: f"{DPV}LegalObligation",
        ProcessingPurpose.VITAL_INTERESTS.value: f"{DPV}VitalInterests",
        ProcessingPurpose.PUBLIC_TASK.value: f"{DPV}PublicTask",
        ProcessingPurpose.LEGITIMATE_INTERESTS.value: f"{DPV}LegitimateInterests"
    }
    
    # GDPR-Compliant Legal Basis
    LEGAL_BASIS = {
        LegalBasis.CONSENT.value: f"{DPV}Consent",
        LegalBasis.CONTRACTUAL_OBLIGATION.value: f"{DPV}ContractualObligation",
        LegalBasis.LEGAL_OBLIGATION.value: f"{DPV}LegalObligation",
        LegalBasis.VITAL_INTERESTS.value: f"{DPV}VitalInterests",
        LegalBasis.PUBLIC_INTEREST_OFFICIAL_AUTHORITY.value: f"{DPV}PublicInterestOfficialAuthority",
        LegalBasis.LEGITIMATE_INTERESTS.value: f"{DPV}LegitimateInterests"
    }
    
    PROCESSING_OPERATIONS = {
        "collect": f"{DPV}Collect",
        "store": f"{DPV}Store", 
        "use": f"{DPV}Use",
        "share": f"{DPV}Share",
        "transfer": f"{DPV}Transfer",
        "delete": f"{DPV}Erase",
        "process": f"{DPV}Process",
        "access": f"{DPV}Access"
    }
    
    DATA_CATEGORIES = {
        "personal_data": f"{DPV}PersonalData",
        "sensitive_data": f"{DPV}SensitivePersonalData",
        "biometric_data": f"{DPV_PD}Biometric",
        "health_data": f"{DPV_PD}Health",
        "financial_data": f"{DPV_PD}Financial",
        "location_data": f"{DPV_PD}Location",
        "behavioral_data": f"{DPV_PD}Behavioral",
        "identification_data": f"{DPV_PD}Identifying"
    }
    
    ROLES = {
        "controller": f"{DPV}DataController",
        "processor": f"{DPV}DataProcessor",
        "joint_controller": f"{DPV}JointDataControllers",
        "data_subject": f"{DPV}DataSubject"
    }
    
    # Dynamic action mapping (no hardcoded actions)
    @classmethod
    def get_action_uri(cls, action_type: str, is_user_action: bool = False) -> str:
        """Generate action URI dynamically based on action type."""
        action_prefix = "User" if is_user_action else ""
        action_name = ''.join(word.capitalize() for word in action_type.replace('_', ' ').split())
        return f"{cls.DPV_ACTION}{action_prefix}{action_name}"

# ===============================
# ENHANCED STANDARDS CONVERTER
# ===============================

class IntegratedRule(BaseModel):
    """Unified rule that combines DPV, ODRL, and ODRE elements."""
    
    id: str = Field(..., description="Unique rule identifier")
    type: str = Field(default="odre:EnforceablePolicy", description="Unified rule type")
    
    # DPV Properties - Updated for v2.1
    dpv_hasProcessing: List[str] = Field(default_factory=list, description="DPV: Processing operations")
    dpv_hasPurpose: List[str] = Field(default_factory=list, description="DPV: Purposes for processing") 
    dpv_hasPersonalData: List[str] = Field(default_factory=list, description="DPV: Personal data categories")
    dpv_hasDataController: Optional[str] = Field(None, description="DPV: Data controller")
    dpv_hasDataProcessor: Optional[str] = Field(None, description="DPV: Data processor")
    dpv_hasLegalBasis: Optional[str] = Field(None, description="DPV: Legal basis for processing")
    dpv_hasLocation: List[str] = Field(default_factory=list, description="DPV: Processing locations/countries")
    
    # DPV Actions - Dynamically inferred
    dpv_hasRuleAction: List[str] = Field(default_factory=list, description="DPV: Rule actions inferred from legislation")
    dpv_hasUserAction: List[str] = Field(default_factory=list, description="DPV: User actions inferred from legislation")
    
    # ODRL Properties
    odrl_permission: List[Dict[str, Any]] = Field(default_factory=list, description="ODRL: Permissions")
    odrl_prohibition: List[Dict[str, Any]] = Field(default_factory=list, description="ODRL: Prohibitions") 
    odrl_obligation: List[Dict[str, Any]] = Field(default_factory=list, description="ODRL: Obligations")
    
    # ODRE Properties
    odre_enforceable: bool = Field(default=True, description="ODRE: Enforceable flag")
    odre_enforcement_mode: str = Field(default="dual_action_based", description="ODRE: Enforcement mode")
    odre_action_inference: bool = Field(default=True, description="ODRE: Action inference enabled")
    odre_user_action_inference: bool = Field(default=True, description="ODRE: User action inference enabled")
    
    # Processing metadata
    source_document_levels: List[str] = Field(default_factory=list, description="Document levels processed")
    chunk_references: List[str] = Field(default_factory=list, description="Chunk references if document was chunked")
    
    # Metadata
    source_legislation: str = Field(..., description="Source legislation")
    source_article: str = Field(..., description="Source article/section")
    extracted_at: datetime = Field(default_factory=datetime.utcnow)
    confidence_score: float = Field(..., ge=0.0, le=1.0)

class StandardsConverter:
    """Converts between JSON Rules Engine and integrated DPV+ODRL+ODRE format."""
    
    def __init__(self):
        self.dpv_concepts = DPVConcepts()
    
    def json_rules_to_integrated(self, legislation_rule: LegislationRule) -> IntegratedRule:
        """Convert JSON Rules Engine rule to integrated format."""
        
        # Extract DPV elements
        dpv_elements = self._extract_dpv_elements(legislation_rule)
        
        # Extract ODRL elements  
        odrl_elements = self._extract_odrl_elements(legislation_rule)
        
        # Create integrated rule
        return self._create_integrated_rule(legislation_rule, dpv_elements, odrl_elements)
    
    def _extract_dpv_elements(self, legislation_rule: LegislationRule) -> Dict[str, Any]:
        """Extract DPV elements from legislation rule with dynamic action mapping."""
        
        dpv_personal_data = []
        for category in legislation_rule.data_category:
            category_value = category.value if hasattr(category, 'value') else str(category)
            if category_value in self.dpv_concepts.DATA_CATEGORIES:
                dpv_personal_data.append(self.dpv_concepts.DATA_CATEGORIES[category_value])
        
        dpv_processing = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                fact_lower = condition.fact.lower()
                for operation, uri in self.dpv_concepts.PROCESSING_OPERATIONS.items():
                    if operation in fact_lower:
                        dpv_processing.append(uri)
        
        # Dynamic purpose mapping based on rule content
        dpv_purposes = []
        rule_text = f"{legislation_rule.description} {legislation_rule.event.type}".lower()
        for purpose_key, purpose_uri in self.dpv_concepts.PROCESSING_PURPOSES.items():
            if purpose_key.replace("_", " ") in rule_text:
                dpv_purposes.append(purpose_uri)
        
        controller = None
        processor = None
        if legislation_rule.primary_impacted_role:
            primary_role_value = legislation_rule.primary_impacted_role.value if hasattr(legislation_rule.primary_impacted_role, 'value') else str(legislation_rule.primary_impacted_role)
            if primary_role_value in self.dpv_concepts.ROLES:
                if primary_role_value == "controller":
                    controller = self.dpv_concepts.ROLES["controller"]
                elif primary_role_value == "processor":
                    processor = self.dpv_concepts.ROLES["processor"]
        
        # Dynamic rule actions mapping
        dpv_rule_actions = []
        for action in legislation_rule.actions:
            action_uri = self.dpv_concepts.get_action_uri(action.action_type, is_user_action=False)
            dpv_rule_actions.append(action_uri)
        
        # Dynamic user actions mapping
        dpv_user_actions = []
        for action in legislation_rule.user_actions:
            action_uri = self.dpv_concepts.get_action_uri(action.action_type, is_user_action=True)
            dpv_user_actions.append(action_uri)
        
        dpv_locations = [f"dpv:Country_{country.replace(' ', '_')}" for country in legislation_rule.applicable_countries]
        
        return {
            "hasProcessing": dpv_processing,
            "hasPurpose": dpv_purposes,
            "hasPersonalData": dpv_personal_data,
            "hasDataController": controller,
            "hasDataProcessor": processor,
            "hasLocation": dpv_locations,
            "hasRuleAction": dpv_rule_actions,
            "hasUserAction": dpv_user_actions
        }
    
    def _extract_odrl_elements(self, legislation_rule: LegislationRule) -> Dict[str, Any]:
        """Extract ODRL elements from legislation rule."""
        
        permissions = []
        prohibitions = []
        obligations = []
        
        rule_description = legislation_rule.description.lower()
        event_type = legislation_rule.event.type.lower()
        
        if "prohibit" in rule_description or "forbid" in event_type:
            prohibition = self._create_odrl_rule(legislation_rule, "prohibition")
            prohibitions.append(prohibition)
        elif "require" in rule_description or "must" in rule_description:
            obligation = self._create_odrl_rule(legislation_rule, "obligation")
            obligations.append(obligation)
        else:
            permission = self._create_odrl_rule(legislation_rule, "permission")
            permissions.append(permission)
        
        return {
            "permission": permissions,
            "prohibition": prohibitions,
            "obligation": obligations
        }
    
    def _create_odrl_rule(self, legislation_rule: LegislationRule, rule_type: str) -> Dict[str, Any]:
        """Create individual ODRL rule from legislation rule."""
        
        target = f"urn:asset:{legislation_rule.source_file}:{legislation_rule.id}"
        
        actions = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                for domain in condition.data_domain:
                    domain_value = domain.value if hasattr(domain, 'value') else str(domain)
                    if domain_value == "data_transfer":
                        actions.append("transfer")
                    elif domain_value == "data_usage":
                        actions.append("use")
                    elif domain_value == "data_storage":
                        actions.append("store")
                    elif domain_value == "data_collection":
                        actions.append("collect")
                    elif domain_value == "data_deletion":
                        actions.append("delete")
        
        if not actions:
            actions = ["use"]
        
        constraints = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                operator_value = condition.operator.value if hasattr(condition.operator, 'value') else str(condition.operator)
                constraint = {
                    "leftOperand": condition.fact,
                    "operator": self._map_operator_to_odrl(operator_value),
                    "rightOperand": condition.value,
                    "comment": condition.description
                }
                constraints.append(constraint)
        
        rule = {
            "target": target,
            "action": actions[0] if len(actions) == 1 else actions,
            "constraint": constraints
        }
        
        return rule
    
    def _map_operator_to_odrl(self, operator: str) -> str:
        """Map operators to ODRL format."""
        mapping = {
            "equal": "eq",
            "notEqual": "neq",
            "greaterThan": "gt", 
            "lessThan": "lt",
            "greaterThanInclusive": "gteq",
            "lessThanInclusive": "lteq",
            "contains": "isA",
            "doesNotContain": "isNotA",
            "in": "isPartOf",
            "notIn": "isNotPartOf"
        }
        return mapping.get(operator, "eq")
    
    def _create_integrated_rule(self, legislation_rule: LegislationRule, dpv_elements: Dict[str, Any], odrl_elements: Dict[str, Any]) -> IntegratedRule:
        """Create integrated rule."""
        
        source_levels = []
        chunk_refs = []
        for logic_type, conditions in legislation_rule.conditions.items():
            for condition in conditions:
                level_value = condition.document_level.value if hasattr(condition.document_level, 'value') else str(condition.document_level)
                if level_value not in source_levels:
                    source_levels.append(level_value)
                if condition.chunk_reference and condition.chunk_reference not in chunk_refs:
                    chunk_refs.append(condition.chunk_reference)
        
        return IntegratedRule(
            id=f"integrated:{legislation_rule.id}",
            dpv_hasProcessing=dpv_elements.get("hasProcessing", []),
            dpv_hasPurpose=dpv_elements.get("hasPurpose", []),
            dpv_hasPersonalData=dpv_elements.get("hasPersonalData", []),
            dpv_hasDataController=dpv_elements.get("hasDataController"),
            dpv_hasDataProcessor=dpv_elements.get("hasDataProcessor"),
            dpv_hasLocation=dpv_elements.get("hasLocation", []),
            dpv_hasRuleAction=dpv_elements.get("hasRuleAction", []),
            dpv_hasUserAction=dpv_elements.get("hasUserAction", []),
            odrl_permission=odrl_elements.get("permission", []),
            odrl_prohibition=odrl_elements.get("prohibition", []),
            odrl_obligation=odrl_elements.get("obligation", []),
            source_document_levels=source_levels,
            chunk_references=chunk_refs,
            source_legislation=legislation_rule.source_file,
            source_article=legislation_rule.source_article,
            confidence_score=legislation_rule.confidence_score
        )

# ===============================
# EXTRACTION RESULT WITH ENHANCED METADATA AND ROBUST CSV EXPORT
# ===============================

class ExtractionResult(BaseModel):
    """Complete result of legislation analysis."""
    
    rules: List[LegislationRule] = Field(..., description="Extracted rules")
    summary: str = Field(..., description="Summary of extraction")
    total_rules: int = Field(..., description="Total number of rules extracted")
    total_actions: int = Field(default=0, description="Total number of rule actions extracted")
    total_user_actions: int = Field(default=0, description="Total number of user actions extracted")
    processing_time: float = Field(..., description="Processing time in seconds")
    embeddings: Optional[List[List[float]]] = Field(None, description="Rule embeddings")
    
    # Integrated standards output
    integrated_rules: List[IntegratedRule] = Field(default_factory=list, description="Integrated standards rules")
    
    # Processing metadata
    documents_processed: Dict[str, List[str]] = Field(default_factory=dict, description="Documents processed by level")
    chunking_metadata: Dict[str, Any] = Field(default_factory=dict, description="Information about document chunking")
    
    def save_json(self, filepath: str):
        """Save rules to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in self.rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
    
    def save_integrated_json(self, filepath: str):
        """Save integrated rules to JSON file."""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in self.integrated_rules],
                f,
                indent=2,
                default=str,
                ensure_ascii=False
            )
    
    def save_integrated_ttl(self, filepath: str):
        """Save integrated rules in TTL format."""
        if not RDF_AVAILABLE:
            print(f"Warning: Cannot generate TTL file - rdflib not available")
            return
            
        turtle_content = self._generate_turtle_with_rdflib()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(turtle_content)
    
    def save_integrated_jsonld(self, filepath: str):
        """Save integrated rules in JSON-LD format."""
        jsonld_content = self._generate_jsonld()
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(jsonld_content, f, indent=2, ensure_ascii=False)
    
    def save_csv(self, filepath: str):
        """Save extraction results to a comprehensive CSV file with detailed reasoning and source section information."""
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            print(f"   Attempting to save CSV to: {filepath}")
            print(f"   Number of rules to save: {len(self.rules)}")
            
            if not self.rules:
                print(f"   Warning: No rules to save to CSV")
                # Create empty CSV with headers
                with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = [
                        'id', 'rule_name', 'rule_description', 'primary_impacted_role', 
                        'secondary_impacted_role', 'data_category', 'applicable_countries', 
                        'adequacy_countries', 'source_section', 'section_title', 'derived_from_section',
                        'extraction_reasoning', 'conditions_logic_type', 'count_of_conditions', 
                        'details_of_conditions', 'conditions_reasoning', 'rule_actions', 'user_actions', 
                        'source_article', 'source_file', 'confidence_score', 'document_level'
                    ]
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                print(f"   Empty CSV file created: {filepath}")
                return
            
            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'id', 'rule_name', 'rule_description', 'primary_impacted_role', 
                    'secondary_impacted_role', 'data_category', 'applicable_countries', 
                    'adequacy_countries', 'source_section', 'section_title', 'derived_from_section',
                    'extraction_reasoning', 'conditions_logic_type', 'count_of_conditions', 
                    'details_of_conditions', 'conditions_reasoning', 'rule_actions', 'user_actions', 
                    'source_article', 'source_file', 'confidence_score', 'document_level'
                ]
                
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                saved_count = 0
                for rule in self.rules:
                    try:
                        # Handle enum values and lists safely with individual error handling
                        primary_role = ''
                        if rule.primary_impacted_role:
                            try:
                                primary_role = rule.primary_impacted_role.value if hasattr(rule.primary_impacted_role, 'value') else str(rule.primary_impacted_role)
                            except:
                                primary_role = str(rule.primary_impacted_role)
                        
                        secondary_role = ''
                        if rule.secondary_impacted_role:
                            try:
                                secondary_role = rule.secondary_impacted_role.value if hasattr(rule.secondary_impacted_role, 'value') else str(rule.secondary_impacted_role)
                            except:
                                secondary_role = str(rule.secondary_impacted_role)
                        
                        data_cats = ''
                        try:
                            data_cats = '; '.join([cat.value if hasattr(cat, 'value') else str(cat) for cat in rule.data_category])
                        except Exception as e:
                            data_cats = f"Error processing data categories: {e}"
                        
                        # Extract source section information from processing metadata
                        source_section = ''
                        section_title = ''
                        derived_from_section = ''
                        extraction_reasoning = ''
                        
                        if rule.processing_metadata:
                            source_section = rule.processing_metadata.get("source_section", "")
                            section_title = rule.processing_metadata.get("section_title", "")
                            derived_from_section = rule.processing_metadata.get("derived_from_section", "")
                            
                            # Create detailed extraction reasoning
                            if source_section:
                                extraction_reasoning = f"Rule extracted from {source_section}"
                                if section_title:
                                    extraction_reasoning += f" ('{section_title}')"
                                extraction_reasoning += f" using {rule.processing_metadata.get('extraction_method', 'unknown method')}"
                            else:
                                extraction_reasoning = f"Rule extracted using {rule.processing_metadata.get('extraction_method', 'unknown method')}"
                        
                        # Process conditions with comprehensive error handling and reasoning
                        conditions_logic = ''
                        total_conditions = 0
                        conditions_detail_text = ''
                        conditions_reasoning = ''
                        
                        try:
                            conditions_logic = '; '.join(rule.conditions.keys())
                            total_conditions = sum(len(conditions) for conditions in rule.conditions.values())
                            
                            # Create detailed conditions description with reasoning
                            condition_details = []
                            reasoning_parts = []
                            
                            for logic_type, conditions in rule.conditions.items():
                                for i, condition in enumerate(conditions, 1):
                                    try:
                                        operator_val = condition.operator.value if hasattr(condition.operator, 'value') else str(condition.operator)
                                        role_val = 'none'
                                        if condition.role:
                                            role_val = condition.role.value if hasattr(condition.role, 'value') else str(condition.role)
                                        
                                        domains = 'none'
                                        try:
                                            domain_list = [d.value if hasattr(d, 'value') else str(d) for d in condition.data_domain]
                                            domains = ', '.join(domain_list) if domain_list else 'none'
                                        except:
                                            domains = 'none'
                                        
                                        # Extract document level and chunk info
                                        level_val = condition.document_level.value if hasattr(condition.document_level, 'value') else str(condition.document_level)
                                        chunk_ref = condition.chunk_reference or 'none'
                                        
                                        detail = f"[{logic_type.upper()}-{i}] {condition.fact} {operator_val} {condition.value} (Role: {role_val}, Domains: {domains}, Level: {level_val}, Chunk: {chunk_ref}) - {condition.description}"
                                        condition_details.append(detail)
                                        
                                        # Add reasoning for this condition
                                        if hasattr(condition, 'reasoning') and condition.reasoning:
                                            reasoning_parts.append(f"Condition {i}: {condition.reasoning}")
                                        
                                    except Exception as e:
                                        condition_details.append(f"[{logic_type.upper()}-{i}] Error processing condition: {e}")
                            
                            conditions_detail_text = ' | '.join(condition_details)
                            conditions_reasoning = ' | '.join(reasoning_parts) if reasoning_parts else 'No specific reasoning provided'
                            
                        except Exception as e:
                            conditions_detail_text = f"Error processing conditions: {e}"
                            conditions_reasoning = f"Error extracting reasoning: {e}"
                        
                        # Process rule actions with error handling and section information
                        rule_actions_text = 'None'
                        try:
                            rule_action_details = []
                            for action in rule.actions:
                                try:
                                    # Include section derivation in action details
                                    section_info = f" [From: {derived_from_section}]" if derived_from_section else ""
                                    detail = f"[{action.action_type}] {action.title}: {action.description}{section_info} (Priority: {action.priority}, Confidence: {action.confidence_score:.2f})"
                                    rule_action_details.append(detail)
                                except Exception as e:
                                    rule_action_details.append(f"Error processing action: {e}")
                            
                            rule_actions_text = ' | '.join(rule_action_details) if rule_action_details else 'None'
                        except Exception as e:
                            rule_actions_text = f"Error processing rule actions: {e}"
                        
                        # Process user actions with error handling and section information
                        user_actions_text = 'None'
                        try:
                            user_action_details = []
                            for action in rule.user_actions:
                                try:
                                    # Include section derivation in action details
                                    section_info = f" [From: {derived_from_section}]" if derived_from_section else ""
                                    detail = f"[{action.action_type}] {action.title}: {action.description}{section_info} (Priority: {action.priority}, Confidence: {action.confidence_score:.2f})"
                                    user_action_details.append(detail)
                                except Exception as e:
                                    user_action_details.append(f"Error processing user action: {e}")
                            
                            user_actions_text = ' | '.join(user_action_details) if user_action_details else 'None'
                        except Exception as e:
                            user_actions_text = f"Error processing user actions: {e}"
                        
                        # Create row with comprehensive information including reasoning
                        row = {
                            'id': str(rule.id),
                            'rule_name': str(rule.name),
                            'rule_description': str(rule.description),
                            'primary_impacted_role': primary_role,
                            'secondary_impacted_role': secondary_role,
                            'data_category': data_cats,
                            'applicable_countries': '; '.join(rule.applicable_countries) if rule.applicable_countries else '',
                            'adequacy_countries': '; '.join(rule.adequacy_countries) if rule.adequacy_countries else '',
                            'source_section': source_section,
                            'section_title': section_title,
                            'derived_from_section': derived_from_section,
                            'extraction_reasoning': extraction_reasoning,
                            'conditions_logic_type': conditions_logic,
                            'count_of_conditions': total_conditions,
                            'details_of_conditions': conditions_detail_text,
                            'conditions_reasoning': conditions_reasoning,
                            'rule_actions': rule_actions_text,
                            'user_actions': user_actions_text,
                            'source_article': str(rule.source_article),
                            'source_file': str(rule.source_file),
                            'confidence_score': str(rule.confidence_score),
                            'document_level': str(getattr(rule.conditions.get('all', [{}])[0], 'document_level', 'unknown') if rule.conditions.get('all') else 'unknown')
                        }
                        writer.writerow(row)
                        saved_count += 1
                        
                    except Exception as e:
                        logger.error(f"Error processing rule {rule.id} for CSV: {e}")
                        print(f"   Error processing rule {rule.id}: {e}")
                        continue
            
            print(f"   CSV Rules with Dual Actions and Detailed Reasoning saved: {filepath}")
            print(f"   Successfully saved {saved_count} out of {len(self.rules)} rules to CSV")
            print(f"   CSV includes: source sections, extraction reasoning, and condition derivation details")
            
        except Exception as e:
            logger.error(f"Error saving CSV file: {e}")
            print(f"   Error saving CSV file: {e}")
            import traceback
            traceback.print_exc()
    
    def _generate_turtle_with_rdflib(self) -> str:
        """Generate Turtle RDF representation with complete rule information and decision trees."""
        if not RDF_AVAILABLE:
            return "# Error: rdflib not available for TTL generation"
        
        g = Graph()
        
        # Define namespaces with updated v2.1 URIs
        DPV = Namespace(Config.DPV_NAMESPACE)
        DPV_PD = Namespace(Config.DPV_PD_NAMESPACE)
        DPV_ACTION = Namespace(Config.ACTION_NAMESPACE)
        ODRL = Namespace(Config.ODRL_NAMESPACE)
        ODRE = Namespace("https://w3id.org/def/odre#")
        RULES = Namespace("https://w3id.org/legislation-rules#")
        
        # Bind namespaces
        g.bind("dpv", DPV)
        g.bind("dpv-pd", DPV_PD)
        g.bind("dpv-action", DPV_ACTION)
        g.bind("odrl", ODRL)
        g.bind("odre", ODRE)
        g.bind("rules", RULES)
        g.bind("rdf", RDF)
        g.bind("rdfs", RDFS)
        g.bind("xsd", XSD)
        
        # Process both original rules and integrated rules for complete coverage
        for rule in self.rules:
            rule_id_encoded = urllib.parse.quote(rule.id, safe=':/')
            rule_uri = URIRef(f"urn:rule:{rule_id_encoded}")
            
            # Core rule information
            g.add((rule_uri, RDF.type, RULES.LegislationRule))
            g.add((rule_uri, RDFS.label, Literal(rule.name)))
            g.add((rule_uri, RULES.description, Literal(rule.description)))
            g.add((rule_uri, RULES.sourceArticle, Literal(rule.source_article)))
            g.add((rule_uri, RULES.sourceFile, Literal(rule.source_file)))
            g.add((rule_uri, RULES.priority, Literal(rule.priority, datatype=XSD.integer)))
            g.add((rule_uri, RULES.confidenceScore, Literal(rule.confidence_score, datatype=XSD.float)))
            
            # Roles
            if rule.primary_impacted_role:
                primary_role_val = rule.primary_impacted_role.value if hasattr(rule.primary_impacted_role, 'value') else str(rule.primary_impacted_role)
                g.add((rule_uri, RULES.primaryImpactedRole, Literal(primary_role_val)))
            
            if rule.secondary_impacted_role:
                secondary_role_val = rule.secondary_impacted_role.value if hasattr(rule.secondary_impacted_role, 'value') else str(rule.secondary_impacted_role)
                g.add((rule_uri, RULES.secondaryImpactedRole, Literal(secondary_role_val)))
            
            # Data categories
            for category in rule.data_category:
                category_val = category.value if hasattr(category, 'value') else str(category)
                g.add((rule_uri, RULES.dataCategory, Literal(category_val)))
            
            # Countries
            for country in rule.applicable_countries:
                g.add((rule_uri, RULES.applicableCountry, Literal(country)))
            
            for country in rule.adequacy_countries:
                g.add((rule_uri, RULES.adequacyCountry, Literal(country)))
            
            # Conditions with full decision tree logic
            for logic_type, conditions in rule.conditions.items():
                # Create a decision tree node for each logic type
                logic_uri = URIRef(f"urn:rule:{rule_id_encoded}:logic:{logic_type}")
                g.add((rule_uri, RULES.hasDecisionLogic, logic_uri))
                g.add((logic_uri, RDF.type, RULES.DecisionLogic))
                g.add((logic_uri, RULES.logicType, Literal(logic_type)))
                
                for i, condition in enumerate(conditions):
                    condition_uri = URIRef(f"urn:rule:{rule_id_encoded}:condition:{logic_type}:{i}")
                    g.add((logic_uri, RULES.hasCondition, condition_uri))
                    g.add((condition_uri, RDF.type, RULES.RuleCondition))
                    g.add((condition_uri, RULES.fact, Literal(condition.fact)))
                    
                    operator_val = condition.operator.value if hasattr(condition.operator, 'value') else str(condition.operator)
                    g.add((condition_uri, RULES.operator, Literal(operator_val)))
                    g.add((condition_uri, RULES.value, Literal(str(condition.value))))
                    g.add((condition_uri, RULES.description, Literal(condition.description)))
                    g.add((condition_uri, RULES.reasoning, Literal(condition.reasoning)))
                    
                    if condition.role:
                        role_val = condition.role.value if hasattr(condition.role, 'value') else str(condition.role)
                        g.add((condition_uri, RULES.role, Literal(role_val)))
                    
                    # Data domains
                    for domain in condition.data_domain:
                        domain_val = domain.value if hasattr(domain, 'value') else str(domain)
                        g.add((condition_uri, RULES.dataDomain, Literal(domain_val)))
                    
                    # Document metadata
                    level_val = condition.document_level.value if hasattr(condition.document_level, 'value') else str(condition.document_level)
                    g.add((condition_uri, RULES.documentLevel, Literal(level_val)))
                    
                    if condition.chunk_reference:
                        g.add((condition_uri, RULES.chunkReference, Literal(condition.chunk_reference)))
            
            # Rule Actions
            for i, action in enumerate(rule.actions):
                action_uri = URIRef(f"urn:rule:{rule_id_encoded}:action:{i}")
                g.add((rule_uri, RULES.hasRuleAction, action_uri))
                g.add((action_uri, RDF.type, RULES.RuleAction))
                g.add((action_uri, RULES.actionType, Literal(action.action_type)))
                g.add((action_uri, RULES.title, Literal(action.title)))
                g.add((action_uri, RULES.description, Literal(action.description)))
                g.add((action_uri, RULES.priority, Literal(action.priority)))
                g.add((action_uri, RULES.legislativeRequirement, Literal(action.legislative_requirement)))
                g.add((action_uri, RULES.dataImpact, Literal(action.data_impact)))
                g.add((action_uri, RULES.confidenceScore, Literal(action.confidence_score, datatype=XSD.float)))
                
                # Data specific steps
                for step in action.data_specific_steps:
                    g.add((action_uri, RULES.dataSpecificStep, Literal(step)))
                
                # Verification methods
                for method in action.verification_method:
                    g.add((action_uri, RULES.verificationMethod, Literal(method)))
                
                if action.responsible_role:
                    g.add((action_uri, RULES.responsibleRole, Literal(action.responsible_role)))
                
                if action.timeline:
                    g.add((action_uri, RULES.timeline, Literal(action.timeline)))
            
            # User Actions
            for i, action in enumerate(rule.user_actions):
                action_uri = URIRef(f"urn:rule:{rule_id_encoded}:userAction:{i}")
                g.add((rule_uri, RULES.hasUserAction, action_uri))
                g.add((action_uri, RDF.type, RULES.UserAction))
                g.add((action_uri, RULES.actionType, Literal(action.action_type)))
                g.add((action_uri, RULES.title, Literal(action.title)))
                g.add((action_uri, RULES.description, Literal(action.description)))
                g.add((action_uri, RULES.priority, Literal(action.priority)))
                g.add((action_uri, RULES.legislativeRequirement, Literal(action.legislative_requirement)))
                g.add((action_uri, RULES.complianceOutcome, Literal(action.compliance_outcome)))
                g.add((action_uri, RULES.confidenceScore, Literal(action.confidence_score, datatype=XSD.float)))
                
                # User data steps
                for step in action.user_data_steps:
                    g.add((action_uri, RULES.userDataStep, Literal(step)))
                
                # Affected data categories
                for category in action.affected_data_categories:
                    g.add((action_uri, RULES.affectedDataCategory, Literal(category)))
                
                # User verification steps
                for step in action.user_verification_steps:
                    g.add((action_uri, RULES.userVerificationStep, Literal(step)))
                
                if action.user_role_context:
                    g.add((action_uri, RULES.userRoleContext, Literal(action.user_role_context)))
                
                if action.timeline:
                    g.add((action_uri, RULES.timeline, Literal(action.timeline)))
            
            # Event information
            g.add((rule_uri, RULES.eventType, Literal(rule.event.type)))
            
            # Metadata
            g.add((rule_uri, RULES.extractedAt, Literal(rule.extracted_at.isoformat(), datatype=XSD.dateTime)))
            g.add((rule_uri, RULES.extractionMethod, Literal(rule.extraction_method)))
        
        # Add integrated rules for semantic web properties
        for integrated_rule in self.integrated_rules:
            rule_id_encoded = urllib.parse.quote(integrated_rule.id, safe=':/')
            rule_uri = URIRef(f"urn:rule:{rule_id_encoded}")
            
            # ODRE Properties
            g.add((rule_uri, RDF.type, ODRE.EnforceablePolicy))
            g.add((rule_uri, RDF.type, DPV.ProcessingActivity))
            g.add((rule_uri, ODRE.enforceable, Literal(integrated_rule.odre_enforceable, datatype=XSD.boolean)))
            g.add((rule_uri, ODRE.enforcement_mode, Literal(integrated_rule.odre_enforcement_mode)))
            g.add((rule_uri, ODRE.action_inference, Literal(integrated_rule.odre_action_inference, datatype=XSD.boolean)))
            g.add((rule_uri, ODRE.user_action_inference, Literal(integrated_rule.odre_user_action_inference, datatype=XSD.boolean)))
            
            # DPV Properties
            for processing in integrated_rule.dpv_hasProcessing:
                g.add((rule_uri, DPV.hasProcessing, URIRef(processing)))
            
            for purpose in integrated_rule.dpv_hasPurpose:
                g.add((rule_uri, DPV.hasPurpose, URIRef(purpose)))
            
            for data in integrated_rule.dpv_hasPersonalData:
                g.add((rule_uri, DPV.hasPersonalData, URIRef(data)))
            
            # Rule actions
            for action in integrated_rule.dpv_hasRuleAction:
                g.add((rule_uri, DPV_ACTION.hasRuleAction, URIRef(action)))
            
            # User actions
            for action in integrated_rule.dpv_hasUserAction:
                g.add((rule_uri, DPV_ACTION.hasUserAction, URIRef(action)))
            
            if integrated_rule.dpv_hasDataController:
                g.add((rule_uri, DPV.hasDataController, URIRef(integrated_rule.dpv_hasDataController)))
            
            if integrated_rule.dpv_hasDataProcessor:
                g.add((rule_uri, DPV.hasDataProcessor, URIRef(integrated_rule.dpv_hasDataProcessor)))
            
            for location in integrated_rule.dpv_hasLocation:
                g.add((rule_uri, DPV.hasLocation, URIRef(location)))
        
        # Serialize to Turtle format
        turtle_output = g.serialize(format='turtle')
        if isinstance(turtle_output, bytes):
            return turtle_output.decode('utf-8')
        return turtle_output
    
    def _generate_jsonld(self) -> Dict[str, Any]:
        """Generate JSON-LD representation with complete rule information and decision trees."""
        context = {
            "@context": {
                "dpv": Config.DPV_NAMESPACE,
                "dpv-pd": Config.DPV_PD_NAMESPACE,
                "dpv-action": Config.ACTION_NAMESPACE,
                "odrl": Config.ODRL_NAMESPACE,
                "odre": "https://w3id.org/def/odre#",
                "rules": "https://w3id.org/legislation-rules#",
                "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
                "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
                "xsd": "http://www.w3.org/2001/XMLSchema#"
            }
        }
        
        graph = []
        
        # Process all original rules with complete information
        for rule in self.rules:
            rule_jsonld = {
                "@id": f"urn:rule:{rule.id}",
                "@type": "rules:LegislationRule",
                "rdfs:label": rule.name,
                "rules:description": rule.description,
                "rules:sourceArticle": rule.source_article,
                "rules:sourceFile": rule.source_file,
                "rules:priority": {
                    "@value": rule.priority,
                    "@type": "xsd:integer"
                },
                "rules:confidenceScore": {
                    "@value": rule.confidence_score,
                    "@type": "xsd:float"
                }
            }
            
            # Roles
            if rule.primary_impacted_role:
                primary_role_val = rule.primary_impacted_role.value if hasattr(rule.primary_impacted_role, 'value') else str(rule.primary_impacted_role)
                rule_jsonld["rules:primaryImpactedRole"] = primary_role_val
            
            if rule.secondary_impacted_role:
                secondary_role_val = rule.secondary_impacted_role.value if hasattr(rule.secondary_impacted_role, 'value') else str(rule.secondary_impacted_role)
                rule_jsonld["rules:secondaryImpactedRole"] = secondary_role_val
            
            # Data categories
            data_categories = []
            for category in rule.data_category:
                category_val = category.value if hasattr(category, 'value') else str(category)
                data_categories.append(category_val)
            if data_categories:
                rule_jsonld["rules:dataCategory"] = data_categories
            
            # Countries
            if rule.applicable_countries:
                rule_jsonld["rules:applicableCountry"] = rule.applicable_countries
            
            if rule.adequacy_countries:
                rule_jsonld["rules:adequacyCountry"] = rule.adequacy_countries
            
            # Decision Logic and Conditions
            decision_logic = []
            for logic_type, condition_list in rule.conditions.items():
                logic_obj = {
                    "@type": "rules:DecisionLogic",
                    "rules:logicType": logic_type,
                    "rules:hasCondition": []
                }
                
                for i, condition in enumerate(condition_list):
                    operator_val = condition.operator.value if hasattr(condition.operator, 'value') else str(condition.operator)
                    role_val = condition.role.value if condition.role and hasattr(condition.role, 'value') else str(condition.role) if condition.role else None
                    level_val = condition.document_level.value if hasattr(condition.document_level, 'value') else str(condition.document_level)
                    
                    condition_obj = {
                        "@type": "rules:RuleCondition",
                        "rules:fact": condition.fact,
                        "rules:operator": operator_val,
                        "rules:value": str(condition.value),
                        "rules:description": condition.description,
                        "rules:reasoning": condition.reasoning,
                        "rules:documentLevel": level_val
                    }
                    
                    if role_val:
                        condition_obj["rules:role"] = role_val
                    
                    if condition.chunk_reference:
                        condition_obj["rules:chunkReference"] = condition.chunk_reference
                    
                    # Data domains
                    domains = []
                    for domain in condition.data_domain:
                        domain_val = domain.value if hasattr(domain, 'value') else str(domain)
                        domains.append(domain_val)
                    if domains:
                        condition_obj["rules:dataDomain"] = domains
                    
                    logic_obj["rules:hasCondition"].append(condition_obj)
                
                decision_logic.append(logic_obj)
            
            if decision_logic:
                rule_jsonld["rules:hasDecisionLogic"] = decision_logic
            
            # Rule Actions
            rule_actions = []
            for action in rule.actions:
                action_obj = {
                    "@type": "rules:RuleAction",
                    "rules:actionType": action.action_type,
                    "rules:title": action.title,
                    "rules:description": action.description,
                    "rules:priority": action.priority,
                    "rules:legislativeRequirement": action.legislative_requirement,
                    "rules:dataImpact": action.data_impact,
                    "rules:confidenceScore": {
                        "@value": action.confidence_score,
                        "@type": "xsd:float"
                    }
                }
                
                if action.data_specific_steps:
                    action_obj["rules:dataSpecificStep"] = action.data_specific_steps
                
                if action.verification_method:
                    action_obj["rules:verificationMethod"] = action.verification_method
                
                if action.responsible_role:
                    action_obj["rules:responsibleRole"] = action.responsible_role
                
                if action.timeline:
                    action_obj["rules:timeline"] = action.timeline
                
                rule_actions.append(action_obj)
            
            if rule_actions:
                rule_jsonld["rules:hasRuleAction"] = rule_actions
            
            # User Actions
            user_actions = []
            for action in rule.user_actions:
                action_obj = {
                    "@type": "rules:UserAction",
                    "rules:actionType": action.action_type,
                    "rules:title": action.title,
                    "rules:description": action.description,
                    "rules:priority": action.priority,
                    "rules:legislativeRequirement": action.legislative_requirement,
                    "rules:complianceOutcome": action.compliance_outcome,
                    "rules:confidenceScore": {
                        "@value": action.confidence_score,
                        "@type": "xsd:float"
                    }
                }
                
                if action.user_data_steps:
                    action_obj["rules:userDataStep"] = action.user_data_steps
                
                if action.affected_data_categories:
                    action_obj["rules:affectedDataCategory"] = action.affected_data_categories
                
                if action.user_verification_steps:
                    action_obj["rules:userVerificationStep"] = action.user_verification_steps
                
                if action.user_role_context:
                    action_obj["rules:userRoleContext"] = action.user_role_context
                
                if action.timeline:
                    action_obj["rules:timeline"] = action.timeline
                
                user_actions.append(action_obj)
            
            if user_actions:
                rule_jsonld["rules:hasUserAction"] = user_actions
            
            # Event information
            rule_jsonld["rules:eventType"] = rule.event.type
            
            # Metadata
            rule_jsonld["rules:extractedAt"] = {
                "@value": rule.extracted_at.isoformat(),
                "@type": "xsd:dateTime"
            }
            rule_jsonld["rules:extractionMethod"] = rule.extraction_method
            
            graph.append(rule_jsonld)
        
        # Add integrated rules for semantic web properties
        for integrated_rule in self.integrated_rules:
            # Find the corresponding original rule
            original_rule_id = integrated_rule.id.replace("integrated:", "")
            
            integrated_jsonld = {
                "@id": f"urn:rule:{integrated_rule.id}",
                "@type": ["odre:EnforceablePolicy", "dpv:ProcessingActivity"],
                "rdfs:label": integrated_rule.source_article,
                
                # ODRE Properties
                "odre:enforceable": integrated_rule.odre_enforceable,
                "odre:enforcement_mode": integrated_rule.odre_enforcement_mode,
                "odre:action_inference": integrated_rule.odre_action_inference,
                "odre:user_action_inference": integrated_rule.odre_user_action_inference,
                
                # DPV Properties
                "dpv:hasProcessing": [{"@id": uri} for uri in integrated_rule.dpv_hasProcessing],
                "dpv:hasPurpose": [{"@id": uri} for uri in integrated_rule.dpv_hasPurpose],
                "dpv:hasPersonalData": [{"@id": uri} for uri in integrated_rule.dpv_hasPersonalData],
                "dpv:hasLocation": [{"@id": uri} for uri in integrated_rule.dpv_hasLocation],
                "dpv-action:hasRuleAction": [{"@id": uri} for uri in integrated_rule.dpv_hasRuleAction],
                "dpv-action:hasUserAction": [{"@id": uri} for uri in integrated_rule.dpv_hasUserAction],
                "dpv-action:hasDocumentLevel": integrated_rule.source_document_levels,
                "dpv-action:hasChunkReference": integrated_rule.chunk_references,
                
                # ODRL Properties
                "odrl:permission": integrated_rule.odrl_permission,
                "odrl:prohibition": integrated_rule.odrl_prohibition,
                "odrl:obligation": integrated_rule.odrl_obligation,
                "odrl:hasPermissionCount": {
                    "@value": len(integrated_rule.odrl_permission),
                    "@type": "xsd:integer"
                },
                "odrl:hasProhibitionCount": {
                    "@value": len(integrated_rule.odrl_prohibition),
                    "@type": "xsd:integer"
                },
                "odrl:hasObligationCount": {
                    "@value": len(integrated_rule.odrl_obligation),
                    "@type": "xsd:integer"
                },
                
                # Link to original rule
                "rules:originalRule": {"@id": f"urn:rule:{original_rule_id}"},
                
                # Metadata
                "dpv:hasConfidenceScore": {
                    "@value": integrated_rule.confidence_score,
                    "@type": "xsd:float"
                },
                "dpv:extractedAt": {
                    "@value": integrated_rule.extracted_at.isoformat(),
                    "@type": "xsd:dateTime"
                },
                "dpv:sourceLegislation": integrated_rule.source_legislation
            }
            
            # Optional properties
            if integrated_rule.dpv_hasDataController:
                integrated_jsonld["dpv:hasDataController"] = {"@id": integrated_rule.dpv_hasDataController}
            if integrated_rule.dpv_hasDataProcessor:
                integrated_jsonld["dpv:hasDataProcessor"] = {"@id": integrated_rule.dpv_hasDataProcessor}
            
            graph.append(integrated_jsonld)
        
        return {**context, "@graph": graph}

# ===============================
# SUPPORT CLASSES
# ===============================

class OpenAIService:
    """Service for OpenAI API interactions."""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=Config.API_KEY,
            base_url=Config.BASE_URL
        )
    
    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings."""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=texts,
                encoding_format="float"
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Union[Dict[str, str], SystemMessage, HumanMessage, AIMessage]]) -> str:
        """Generate chat completion."""
        try:
            formatted_messages = []
            for msg in messages:
                if isinstance(msg, (SystemMessage, HumanMessage, AIMessage)):
                    if isinstance(msg, SystemMessage):
                        formatted_messages.append({"role": "system", "content": msg.content})
                    elif isinstance(msg, HumanMessage):
                        formatted_messages.append({"role": "user", "content": msg.content})
                    elif isinstance(msg, AIMessage):
                        formatted_messages.append({"role": "assistant", "content": msg.content})
                elif isinstance(msg, dict):
                    formatted_messages.append(msg)
                else:
                    formatted_messages.append({"role": "user", "content": str(msg)})
            
            response = self.client.chat.completions.create(
                model=Config.CHAT_MODEL,
                messages=formatted_messages
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in chat completion: {e}")
            raise

class SafeJsonParser:
    """Safe JSON parsing with error handling."""
    
    @staticmethod
    def parse_json_response(response: str) -> Dict[str, Any]:
        """Safely parse JSON response from LLM."""
        try:
            cleaned = response.strip()
            
            if "```json" in cleaned:
                start = cleaned.find("```json") + 7
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            elif "```" in cleaned:
                start = cleaned.find("```") + 3
                end = cleaned.find("```", start)
                if end != -1:
                    cleaned = cleaned[start:end].strip()
            
            parsed = json.loads(cleaned)
            return parsed
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON decode error: {e}. Attempting to fix...")
            
            try:
                import re
                fixed = re.sub(r',(\s*[}\]])', r'\1', cleaned)
                parsed = json.loads(fixed)
                return parsed
            except Exception:
                logger.error(f"Could not parse JSON response: {cleaned[:200]}...")
                return {"error": "Failed to parse JSON", "raw_response": cleaned}

class RuleManager:
    """Manages existing rules."""
    
    def __init__(self, rules_file: str = Config.EXISTING_RULES_FILE):
        self.rules_file = rules_file
        self.existing_rules: List[LegislationRule] = []
        self.load_existing_rules()
    
    def load_existing_rules(self):
        """Load existing rules from file."""
        try:
            if os.path.exists(self.rules_file):
                with open(self.rules_file, 'r', encoding='utf-8') as f:
                    rules_data = json.load(f)
                
                for rule_data in rules_data:
                    try:
                        rule = LegislationRule(**rule_data)
                        self.existing_rules.append(rule)
                    except Exception as e:
                        logger.warning(f"Skipping invalid existing rule: {e}")
                
                logger.info(f"Loaded {len(self.existing_rules)} existing rules")
            else:
                logger.info("No existing rules file found. Starting fresh.")
        except Exception as e:
            logger.error(f"Error loading existing rules: {e}")
            self.existing_rules = []
    
    def save_rules(self, new_rules: List[LegislationRule]):
        """Save new rules, appending to existing ones."""
        all_rules = self.existing_rules + new_rules
        
        unique_rules = []
        seen_ids = set()
        for rule in all_rules:
            if rule.id not in seen_ids:
                unique_rules.append(rule)
                seen_ids.add(rule.id)
        
        os.makedirs(os.path.dirname(self.rules_file), exist_ok=True)
        with open(self.rules_file, 'w', encoding='utf-8') as f:
            json.dump(
                [rule.model_dump() for rule in unique_rules], 
                f, 
                indent=2, 
                default=str,
                ensure_ascii=False
            )
        
        self.existing_rules = unique_rules
        logger.info(f"Saved {len(unique_rules)} total rules ({len(new_rules)} new)")
    
    def get_context_summary(self) -> str:
        """Get a summary of existing rules for context."""
        if not self.existing_rules:
            return "No existing rules found."
        
        summary = f"Existing Rules Context ({len(self.existing_rules)} rules):\n\n"
        
        sources = {}
        for rule in self.existing_rules:
            source = rule.source_article
            if source not in sources:
                sources[source] = []
            sources[source].append(rule)
        
        for source, rules in sources.items():
            summary += f"Source: {source} ({len(rules)} rules)\n"
            for rule in rules[:3]:
                total_actions = len(rule.actions)
                total_user_actions = len(rule.user_actions)
                summary += f"  - {rule.name}: {rule.description[:100]}... ({total_actions} rule actions, {total_user_actions} user actions)\n"
            if len(rules) > 3:
                summary += f"  ... and {len(rules) - 3} more rules\n"
            summary += "\n"
        
        return summary

# ===============================
# ENHANCED PROMPTING STRATEGIES FOR WHOLE DOCUMENT ANALYSIS
# ===============================

class PromptingStrategies:
    """Anti-hallucination prompting strategies focused on dual action inference and whole document analysis."""
    
    @staticmethod
    def comprehensive_document_analysis_prompt(legislation_text: str, existing_context: str = "", level: str = "level_1", chunk_info: str = "") -> str:
        """Comprehensive document analysis prompt that ensures the entire document is understood."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        chunk_section = f"\n\nCHUNK INFORMATION:\n{chunk_info}\n" if chunk_info else ""
        
        return f"""
        Perform a comprehensive analysis of this {level} legislation text with strict adherence to what is stated.
        Read and understand the ENTIRE document before extracting any rules or conditions.
        {context_section}{chunk_section}
        
        LEGISLATION TEXT TO ANALYZE:
        {legislation_text}
        
        COMPREHENSIVE ANALYSIS REQUIREMENTS:
        
        1. FULL DOCUMENT UNDERSTANDING:
        - Read the entire text from beginning to end
        - Identify all sections, articles, paragraphs, and subsections
        - Understand the overall structure and purpose of the legislation
        - Note cross-references between different parts of the document
        - Identify how different sections relate to each other
        
        2. EXTRACT ALL OBLIGATIONS FROM ENTIRE DOCUMENT:
        - Extract every obligation stated anywhere in the text
        - Identify who has each obligation (controller, processor, joint_controller, data_subject)
        - Note the exact text that creates each obligation
        - Link related obligations across different sections
        - Use simple, clear English - avoid legal jargon
        
        3. IDENTIFY ALL CONDITIONS AND TRIGGERS:
        - Find all conditions that trigger obligations throughout the document
        - Note data-related triggers (data types, processing activities, transfers)
        - Extract factual conditions that can be evaluated
        - Identify when conditions in one section affect obligations in another
        
        4. COMPREHENSIVE DATA REQUIREMENTS:
        - Identify all requirements for data handling mentioned anywhere
        - Note all data categories mentioned throughout the text
        - Extract all data processing operations referenced
        - Map data requirements to specific obligations
        
        5. COMPREHENSIVE ROLE AND RESPONSIBILITY MAPPING:
        - Identify every role mentioned in the document
        - Map all responsibilities to each role
        - Note how roles interact with each other
        - Identify role-specific obligations throughout the document
        
        6. COMPREHENSIVE DATA CATEGORY IDENTIFICATION:
        - Find every data category mentioned in the document
        - Note specific data types and classifications
        - Identify sensitive data categories
        - Map data categories to processing requirements
        
        CRITICAL: Analyze the ENTIRE document as a whole. Do not focus on individual sections in isolation.
        Extract information present throughout the complete legislation text. Do not infer beyond what is directly stated.
        Ensure your analysis reflects understanding of the full document context.
        """
    
    @staticmethod
    def focused_analysis_prompt(legislation_text: str, existing_context: str = "", level: str = "level_1", chunk_info: str = "") -> str:
        """Focused analysis prompt that minimizes hallucination."""
        context_section = f"\n\nEXISTING RULES CONTEXT:\n{existing_context}\n" if existing_context else ""
        chunk_section = f"\n\nCHUNK INFORMATION:\n{chunk_info}\n" if chunk_info else ""
        
        return f"""
        Analyze this {level} legislation text with strict adherence to what is stated.
        {context_section}{chunk_section}
        
        LEGISLATION TEXT TO ANALYZE:
        {legislation_text}
        
        ANALYSIS REQUIREMENTS:
        
        1. IDENTIFY EXPLICIT OBLIGATIONS:
        - Extract only obligations stated in the text
        - Identify who has each obligation (controller, processor, joint_controller, data_subject)
        - Note the exact text that creates each obligation
        - Use simple, clear English - avoid legal jargon
        
        2. EXTRACT CONDITIONS AND TRIGGERS:
        - Identify conditions that trigger obligations
        - Note data-related triggers (data types, processing activities, transfers)
        - Extract factual conditions that can be evaluated
        
        3. DETERMINE DATA-SPECIFIC REQUIREMENTS:
        - Identify requirements for data handling
        - Note data categories mentioned in the text
        - Extract data processing operations referenced
        
        4. INFER DUAL ACTIONS (BASED ON TEXT):
        - For each obligation, identify what rule actions must be taken (organizational/policy level)
        - For each obligation, identify what user actions can be taken (individual/practical level)
        - Focus on actions involving data processing, storage, transfer, or deletion
        - Base actions on requirements in the legislation
        - Ensure actions are practical and implementable
        - Use simple, clear English - avoid legal jargon
        
        Extract information present in the legislation text. Do not infer beyond what is directly stated.
        """
    
    @staticmethod
    def expert_verification_prompt(legislation_text: str, preliminary_analysis: str, level: str = "level_1") -> str:
        """Expert verification prompt to validate findings against source text."""
        
        return f"""
        Verify the preliminary analysis against the source legislation.
        
        SOURCE LEGISLATION ({level}):
        {legislation_text}
        
        PRELIMINARY ANALYSIS TO VERIFY:
        {preliminary_analysis}
        
        VERIFICATION TASKS:
        
        1. ACCURACY CHECK:
        - Verify each identified obligation exists in the source text
        - Confirm conditions and triggers are accurately extracted
        - Validate data categories and processing operations mentioned
        - Ensure language is simple and clear
        
        2. COMPLETENESS REVIEW:
        - Identify any explicit obligations that were missed
        - Check for data-specific requirements not captured
        - Verify all relevant roles and responsibilities are identified
        
        3. DUAL ACTION VALIDATION:
        - Confirm each proposed rule action is supported by the legislation text
        - Confirm each proposed user action is practical and based on legislation
        - Verify actions are specific to data handling requirements
        - Ensure actions can be performed by appropriate entities
        - Check that language is in simple English
        
        4. REMOVE UNSUPPORTED ELEMENTS:
        - Flag any elements not supported by the source text
        - Remove elements that cannot be traced to specific legislative language
        
        Provide corrected analysis that adheres to the source legislation text.
        Use simple, clear English throughout.
        """

# ===============================
# MAIN LEGISLATION ANALYZER - ENHANCED FOR WHOLE DOCUMENT PROCESSING
# ===============================

class LegislationAnalyzer:
    """Main analyzer with enhanced dual action inference, chunking support, and whole document analysis."""
    
    def __init__(self):
        self.openai_service = OpenAIService()
        self.json_parser = SafeJsonParser()
        self.rule_manager = RuleManager()
        self.metadata_manager = MetadataManager()
        self.multi_level_processor = MultiLevelPDFProcessor()
        self.standards_converter = StandardsConverter()
        
        # Initialize LangChain model
        self.llm = ChatOpenAI(
            model=Config.CHAT_MODEL,
            openai_api_key=Config.API_KEY,
            openai_api_base=Config.BASE_URL
        )
        
        # Create react agent with ALL ORIGINAL TOOLS - RESTORED
        self.tools = [
            # Original tools - KEPT
            extract_rule_conditions,
            analyze_data_domains, 
            identify_roles_responsibilities,
            infer_data_processing_actions,
            infer_compliance_verification_actions,
            infer_data_subject_rights_actions,
            # NEW user action tools - ADDED
            infer_user_actionable_tasks,
            infer_user_compliance_tasks,
            infer_user_rights_support_tasks
        ]
        
        self.memory = MemorySaver()
        self.agent = create_react_agent(self.llm, self.tools, checkpointer=self.memory)
    
    async def process_legislation_folder(self, folder_path: str = None) -> ExtractionResult:
        """Process all configured legislation entries - NEVER EXIT IF RULES EXIST."""
        if folder_path is None:
            folder_path = Config.LEGISLATION_PDF_PATH
        
        os.makedirs(folder_path, exist_ok=True)
        
        processing_entries = self.metadata_manager.get_all_processing_entries()
        
        if not processing_entries:
            logger.warning("No processing entries found in metadata configuration")
            return ExtractionResult(
                rules=[],
                summary="No configured entries to process",
                total_rules=0,
                total_actions=0,
                total_user_actions=0,
                processing_time=0.0
            )
        
        # REMOVED: Logic to skip processing if rules exist - ALWAYS PROCESS
        
        all_new_rules = []
        documents_processed = {}
        chunking_metadata = {}
        start_time = datetime.utcnow()
        
        for entry_id, metadata in processing_entries:
            try:
                logger.info(f"Processing entry: {entry_id}")
                
                entry_documents = self.multi_level_processor.process_country_documents(
                    entry_id, metadata, folder_path
                )
                
                if not entry_documents:
                    logger.warning(f"No documents found for entry {entry_id}")
                    continue
                
                documents_processed[entry_id] = list(entry_documents.keys())
                
                # Track chunking metadata
                for level, content in entry_documents.items():
                    if isinstance(content, list):  # Chunked document
                        chunking_metadata[f"{entry_id}_{level}"] = {
                            "chunks": len(content),
                            "chunk_size": Config.CHUNK_SIZE,
                            "overlap_size": Config.OVERLAP_SIZE
                        }
                
                result = await self.analyze_legislation_with_levels(
                    entry_documents=entry_documents,
                    entry_id=entry_id,
                    metadata=metadata
                )
                
                # Don't filter out existing rules - process all
                all_new_rules.extend(result.rules)
                
            except Exception as e:
                logger.error(f"Error processing entry {entry_id}: {e}")
                continue
        
        end_time = datetime.utcnow()
        total_processing_time = (end_time - start_time).total_seconds()
        total_actions = sum(len(rule.actions) for rule in all_new_rules)
        total_user_actions = sum(len(rule.user_actions) for rule in all_new_rules)
        
        if all_new_rules:
            rule_texts = [f"{rule.description} {rule.source_article}" for rule in all_new_rules]
            embeddings = await self.openai_service.get_embeddings(rule_texts)
        else:
            embeddings = []
        
        if all_new_rules:
            self.rule_manager.save_rules(all_new_rules)
        
        integrated_rules = []
        for rule in all_new_rules:
            try:
                integrated_rule = self.standards_converter.json_rules_to_integrated(rule)
                integrated_rules.append(integrated_rule)
            except Exception as e:
                logger.warning(f"Error converting rule {rule.id} to integrated format: {e}")
                continue
        
        result = ExtractionResult(
            rules=all_new_rules,
            summary=f"Processed {len(processing_entries)} entries, extracted {len(all_new_rules)} rules with {total_actions} rule actions and {total_user_actions} user actions",
            total_rules=len(all_new_rules),
            total_actions=total_actions,
            total_user_actions=total_user_actions,
            processing_time=total_processing_time,
            embeddings=embeddings,
            integrated_rules=integrated_rules,
            documents_processed=documents_processed,
            chunking_metadata=chunking_metadata
        )
        
        return result
    
    async def analyze_legislation_with_levels(
        self, 
        entry_documents: Dict[str, Union[str, List[DocumentChunk]]],
        entry_id: str,
        metadata: CountryMetadata
    ) -> ExtractionResult:
        """Analyze legislation from multiple document levels with chunking support and whole document analysis."""
        start_time = datetime.utcnow()
        
        try:
            logger.info(f"Starting comprehensive analysis for entry: {entry_id}")
            logger.info(f"Countries: {metadata.country}")
            
            existing_context = self.rule_manager.get_context_summary()
            
            metadata_context = f"""
            ENTRY METADATA:
            - Entry ID: {entry_id}
            - Applicable Countries: {', '.join(metadata.country)}
            - Adequacy Countries: {', '.join(metadata.adequacy_country) if metadata.adequacy_country else 'None specified'}
            - Document Levels Available: {', '.join(entry_documents.keys())}
            """
            
            all_rules = []
            
            # ENHANCED: First pass - comprehensive document understanding
            for level, content in entry_documents.items():
                logger.info(f"Performing comprehensive analysis of {level} document...")
                
                if isinstance(content, list):  # Chunked document
                    # For chunked documents, first get overall understanding
                    full_text = "\n\n".join([chunk.content for chunk in content])
                    comprehensive_analysis = await self._apply_comprehensive_document_analysis(
                        full_text, existing_context + metadata_context, level, f"Full document with {len(content)} chunks"
                    )
                    
                    # Then process each chunk with context of the whole document
                    for chunk in content:
                        chunk_info = f"Chunk {chunk.chunk_index + 1} of {chunk.total_chunks} (positions {chunk.start_pos}-{chunk.end_pos})"
                        
                        chunk_rules = await self._process_text_chunk_with_context(
                            text=chunk.content,
                            chunk_reference=chunk.chunk_id,
                            entry_id=entry_id,
                            level=level,
                            metadata=metadata,
                            existing_context=existing_context,
                            metadata_context=metadata_context,
                            chunk_info=chunk_info,
                            comprehensive_analysis=comprehensive_analysis
                        )
                        
                        all_rules.extend(chunk_rules)
                        logger.info(f"Processed {len(chunk_rules)} rules from chunk {chunk.chunk_index + 1}")
                
                else:  # Single document
                    comprehensive_analysis = await self._apply_comprehensive_document_analysis(
                        content, existing_context + metadata_context, level, ""
                    )
                    
                    level_rules = await self._process_text_chunk_with_context(
                        text=content,
                        chunk_reference=None,
                        entry_id=entry_id,
                        level=level,
                        metadata=metadata,
                        existing_context=existing_context,
                        metadata_context=metadata_context,
                        chunk_info="",
                        comprehensive_analysis=comprehensive_analysis
                    )
                    
                    all_rules.extend(level_rules)
                    logger.info(f"Processed {len(level_rules)} rules from {level} document")
            
            if all_rules:
                rule_texts = [f"{rule.description} {rule.source_article}" for rule in all_rules]
                embeddings = await self.openai_service.get_embeddings(rule_texts)
            else:
                embeddings = []
            
            integrated_rules = []
            for rule in all_rules:
                try:
                    integrated_rule = self.standards_converter.json_rules_to_integrated(rule)
                    integrated_rules.append(integrated_rule)
                except Exception as e:
                    logger.warning(f"Error converting rule {rule.id} to integrated format: {e}")
                    continue
            
            end_time = datetime.utcnow()
            processing_time = (end_time - start_time).total_seconds()
            total_actions = sum(len(rule.actions) for rule in all_rules)
            total_user_actions = sum(len(rule.user_actions) for rule in all_rules)
            
            result = ExtractionResult(
                rules=all_rules,
                summary=f"Extracted {len(all_rules)} rules with {total_actions} rule actions and {total_user_actions} user actions from {entry_id}",
                total_rules=len(all_rules),
                total_actions=total_actions,
                total_user_actions=total_user_actions,
                processing_time=processing_time,
                embeddings=embeddings,
                integrated_rules=integrated_rules,
                documents_processed={entry_id: list(entry_documents.keys())}
            )
            
            logger.info(f"Analysis completed: {len(all_rules)} rules with {total_actions} rule actions and {total_user_actions} user actions extracted in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Error analyzing legislation with levels: {e}")
            raise
    
    async def _apply_comprehensive_document_analysis(self, legislation_text: str, existing_context: str = "", level: str = "level_1", chunk_info: str = "") -> str:
        """Apply comprehensive document analysis to understand the entire document."""
        prompt = PromptingStrategies.comprehensive_document_analysis_prompt(legislation_text, existing_context, level, chunk_info)
        
        messages = [
            SystemMessage(content="You are a legal text analyst. Analyze the ENTIRE document comprehensively. Extract EVERY possible rule, obligation, and condition. Use simple, clear English."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _extract_all_document_sections(self, legislation_text: str) -> Dict[str, str]:
        """Extract and identify all sections, articles, and paragraphs in the document."""
        
        prompt = f"""
        Analyze this legislation document and identify ALL sections, articles, chapters, paragraphs, and subsections.
        
        Document Text:
        {legislation_text}
        
        For each identifiable section, provide:
        1. Section identifier (Article X, Section Y, Paragraph Z, etc.)
        2. Section title/heading if available
        3. Complete text content of that section
        4. Brief description of what the section covers
        
        Return a JSON object mapping section identifiers to their content and metadata.
        Format: {{"section_id": {{"title": "title", "content": "full text", "description": "what it covers"}}}}
        
        Be comprehensive - capture EVERY identifiable section, no matter how small.
        """
        
        messages = [
            SystemMessage(content="You are a document structure analyst. Extract ALL sections comprehensively."),
            HumanMessage(content=prompt)
        ]
        
        response = await self.openai_service.chat_completion(messages)
        parsed_sections = self.json_parser.parse_json_response(response)
        
        if "error" in parsed_sections:
            logger.warning("Could not parse document sections, using full text")
            return {"full_document": {"title": "Full Document", "content": legislation_text, "description": "Complete document text"}}
        
        return parsed_sections
    
    async def _extract_rules_from_each_section(self, sections: Dict[str, Dict[str, str]], entry_id: str, level: str, metadata: CountryMetadata, existing_context: str) -> List[LegislationRule]:
        """Extract rules from each document section separately to ensure comprehensive coverage."""
        all_section_rules = []
        
        for section_id, section_data in sections.items():
            try:
                logger.info(f"Extracting rules from section: {section_id}")
                
                section_content = section_data.get("content", "")
                section_title = section_data.get("title", "")
                section_description = section_data.get("description", "")
                
                if len(section_content.strip()) < 50:  # Skip very short sections
                    continue
                
                # Extract rules specifically from this section
                section_rules = await self._extract_rules_from_single_section(
                    section_content=section_content,
                    section_id=section_id,
                    section_title=section_title,
                    section_description=section_description,
                    entry_id=entry_id,
                    level=level,
                    metadata=metadata,
                    existing_context=existing_context
                )
                
                all_section_rules.extend(section_rules)
                logger.info(f"Extracted {len(section_rules)} rules from section {section_id}")
                
            except Exception as e:
                logger.error(f"Error extracting rules from section {section_id}: {e}")
                continue
        
        return all_section_rules
    
    async def _extract_rules_from_single_section(
        self,
        section_content: str,
        section_id: str,
        section_title: str,
        section_description: str,
        entry_id: str,
        level: str,
        metadata: CountryMetadata,
        existing_context: str
    ) -> List[LegislationRule]:
        """Extract all possible rules from a single document section."""
        
        section_prompt = f"""
        Extract ALL possible rules, obligations, conditions, and requirements from this specific section of legislation.
        Be comprehensive - extract EVERY rule that can be identified, no matter how small or specific.
        
        SECTION INFORMATION:
        - Section ID: {section_id}
        - Section Title: {section_title}
        - Section Description: {section_description}
        - Document Level: {level}
        - Entry ID: {entry_id}
        
        SECTION CONTENT:
        {section_content}
        
        EXTRACTION REQUIREMENTS:
        1. Find EVERY obligation, requirement, prohibition, permission, or condition in this section
        2. Extract rules for ALL roles mentioned (controller, processor, data_subject, joint_controller)
        3. Identify ALL data categories referenced in this section
        4. Create separate rules for different aspects if the section covers multiple topics
        5. Ensure each rule has clear conditions based on what's stated in this section
        6. Include both organizational actions and user actions where applicable
        7. Focus on practical data operations mentioned or implied
        
        For each rule found:
        - Derive primary_impacted_role from the section content
        - Identify all relevant data_category values from the section
        - Create detailed conditions based on section requirements
        - Extract practical actions (both organizational and user-level)
        - Include clear reasoning showing exactly which part of the section supports each rule
        
        Return a comprehensive JSON array with ALL rules found in this section.
        Each rule must include detailed reasoning about its derivation from this specific section.
        """
        
        messages = [
            SystemMessage(content="You are a comprehensive legal rule extractor. Extract EVERY possible rule from the given section. Be thorough and exhaustive."),
            HumanMessage(content=section_prompt)
        ]
        
        response = await self.openai_service.chat_completion(messages)
        parsed_data = self.json_parser.parse_json_response(response)
        
        if "error" in parsed_data:
            logger.warning(f"Could not parse rules from section {section_id}")
            return []
        
        rules = []
        try:
            if isinstance(parsed_data, list):
                rule_data_list = parsed_data
            elif isinstance(parsed_data, dict) and "rules" in parsed_data:
                rule_data_list = parsed_data["rules"]
            else:
                rule_data_list = [parsed_data] if parsed_data else []
            
            for rule_data in rule_data_list:
                try:
                    # Ensure section-specific metadata
                    rule_data["source_section"] = section_id
                    rule_data["section_title"] = section_title
                    rule_data["section_description"] = section_description
                    rule_data["derived_from_section"] = f"{section_id}: {section_title}"
                    
                    # Process and validate the rule
                    processed_rule = await self._process_and_validate_section_rule(
                        rule_data, section_content, section_id, entry_id, level, metadata
                    )
                    
                    if processed_rule:
                        rules.append(processed_rule)
                        
                except Exception as e:
                    logger.warning(f"Error processing rule from section {section_id}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error processing rules from section {section_id}: {e}")
            
        return rules
    
    async def _process_and_validate_section_rule(
        self,
        rule_data: Dict[str, Any],
        section_content: str,
        section_id: str,
        entry_id: str,
        level: str,
        metadata: CountryMetadata
    ) -> Optional[LegislationRule]:
        """Process and validate a rule extracted from a specific section."""
        
        try:
            # Ensure all required fields
            rule_data.setdefault("id", f"rule_{section_id}_{datetime.utcnow().timestamp()}")
            rule_data.setdefault("name", f"Rule from {section_id}")
            rule_data.setdefault("description", f"Requirement from {section_id}")
            rule_data.setdefault("source_article", f"{entry_id} - {level} - {section_id}")
            rule_data.setdefault("source_file", metadata.file_level_1 or "unknown")
            
            # Enhanced role and data category inference
            if not rule_data.get("primary_impacted_role"):
                rule_data["primary_impacted_role"] = self._infer_primary_role(section_content)
            
            if not rule_data.get("data_category") or len(rule_data.get("data_category", [])) == 0:
                rule_data["data_category"] = self._infer_data_categories(section_content)
            
            # Ensure other required fields
            rule_data.setdefault("priority", 1)
            rule_data.setdefault("confidence_score", 0.8)
            rule_data.setdefault("applicable_countries", metadata.country)
            rule_data.setdefault("adequacy_countries", metadata.adequacy_country or [])
            rule_data.setdefault("source_documents", {
                "level_1": metadata.file_level_1,
                "level_2": metadata.file_level_2,
                "level_3": metadata.file_level_3
            })
            
            # Ensure conditions exist
            if "conditions" not in rule_data or not rule_data["conditions"]:
                rule_data["conditions"] = {
                    "all": [
                        {
                            "fact": f"section_{section_id}_requirement",
                            "operator": "equal",
                            "value": True,
                            "description": f"Requirements from {section_id} apply",
                            "data_domain": ["data_usage"],
                            "role": rule_data.get("primary_impacted_role", "controller"),
                            "reasoning": f"Condition derived from section {section_id} requirements",
                            "document_level": level,
                            "chunk_reference": section_id
                        }
                    ]
                }
            
            # Ensure event exists
            if "event" not in rule_data:
                rule_data["event"] = {
                    "type": "compliance_required",
                    "params": {}
                }
            
            # Add section metadata to processing metadata
            processing_metadata = {
                "extraction_method": "section_by_section_comprehensive",
                "source_section": section_id,
                "section_title": rule_data.get("section_title", ""),
                "derived_from_section": rule_data.get("derived_from_section", "")
            }
            rule_data["processing_metadata"] = processing_metadata
            
            # Validate and create rule
            rule = LegislationRule.model_validate(rule_data)
            return rule
            
        except Exception as e:
            logger.warning(f"Failed to process rule from section {section_id}: {e}")
            return None
    
    def _infer_primary_role(self, section_content: str) -> str:
        """Infer primary impacted role from section content."""
        content_lower = section_content.lower()
        
        # Count occurrences and context
        role_scores = {
            "controller": 0,
            "processor": 0,
            "data_subject": 0,
            "joint_controller": 0
        }
        
        # Look for explicit mentions
        if "data controller" in content_lower or "controller" in content_lower:
            role_scores["controller"] += 10
        if "data processor" in content_lower or "processor" in content_lower:
            role_scores["processor"] += 10
        if "data subject" in content_lower:
            role_scores["data_subject"] += 10
        if "joint controller" in content_lower:
            role_scores["joint_controller"] += 10
        
        # Context-based inference
        if any(word in content_lower for word in ["determine", "decide", "purpose", "means"]):
            role_scores["controller"] += 5
        if any(word in content_lower for word in ["process", "behalf", "instruction"]):
            role_scores["processor"] += 5
        if any(word in content_lower for word in ["individual", "person", "consent", "request"]):
            role_scores["data_subject"] += 5
        if any(word in content_lower for word in ["together", "jointly", "common"]):
            role_scores["joint_controller"] += 5
        
        # Return highest scoring role, default to controller
        max_role = max(role_scores.items(), key=lambda x: x[1])
        return max_role[0] if max_role[1] > 0 else "controller"
    
    def _infer_data_categories(self, section_content: str) -> List[str]:
        """Infer data categories from section content."""
        content_lower = section_content.lower()
        categories = []
        
        # Direct mentions
        if "personal data" in content_lower:
            categories.append("personal_data")
        if any(word in content_lower for word in ["sensitive", "special category", "special categories"]):
            categories.append("sensitive_data")
        if "health" in content_lower or "medical" in content_lower:
            categories.append("health_data")
        if any(word in content_lower for word in ["financial", "payment", "banking", "credit"]):
            categories.append("financial_data")
        if any(word in content_lower for word in ["biometric", "fingerprint", "facial", "genetic"]):
            categories.append("biometric_data")
        if any(word in content_lower for word in ["location", "geolocation", "gps", "tracking"]):
            categories.append("location_data")
        if any(word in content_lower for word in ["behavior", "behavioral", "profiling", "tracking"]):
            categories.append("behavioral_data")
        if any(word in content_lower for word in ["identification", "identity", "name", "identifier"]):
            categories.append("identification_data")
        
        # Default if none found
        if not categories:
            categories = ["personal_data"]
        
        return categories
    
    def _deduplicate_rules(self, rules: List[LegislationRule]) -> List[LegislationRule]:
        """Remove duplicate rules while preserving the most comprehensive versions."""
        if not rules:
            return rules
        
        # Group rules by similarity
        unique_rules = []
        processed_descriptions = set()
        
        for rule in rules:
            # Create a normalized description for comparison
            normalized_desc = rule.description.lower().strip()
            
            # Simple deduplication based on description similarity
            is_duplicate = False
            for existing_desc in processed_descriptions:
                # Check for high similarity (simple approach)
                if self._calculate_similarity(normalized_desc, existing_desc) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_rules.append(rule)
                processed_descriptions.add(normalized_desc)
            else:
                logger.info(f"Skipping duplicate rule: {rule.name}")
        
        logger.info(f"Deduplicated {len(rules)} rules to {len(unique_rules)} unique rules")
        return unique_rules
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple similarity between two texts."""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    async def _apply_focused_analysis_with_context(self, legislation_text: str, existing_context: str = "", level: str = "level_1", chunk_info: str = "", comprehensive_analysis: str = "") -> str:
        """Apply focused analysis with comprehensive document context."""
        context_section = f"\n\nCOMPREHENSIVE DOCUMENT ANALYSIS:\n{comprehensive_analysis}\n" if comprehensive_analysis else ""
        prompt = PromptingStrategies.focused_analysis_prompt(legislation_text, existing_context + context_section, level, chunk_info)
        
        messages = [
            SystemMessage(content="You are a legal text analyst. Analyze only what is present in the legislation text using the comprehensive document context. Use simple, clear English."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _apply_expert_verification(self, legislation_text: str, preliminary_analysis: str, level: str = "level_1") -> str:
        """Apply expert verification to validate findings."""
        prompt = PromptingStrategies.expert_verification_prompt(legislation_text, preliminary_analysis, level)
        
        messages = [
            SystemMessage(content="You are a legal compliance expert. Verify analysis accuracy against source text. Use simple, clear English."),
            HumanMessage(content=prompt)
        ]
        
        return await self.openai_service.chat_completion(messages)
    
    async def _run_dual_action_inference_agent_with_context(self, legislation_text: str, article_reference: str, countries: List[str], chunk_reference: Optional[str] = None, comprehensive_analysis: str = "") -> str:
        """Run react agent for DUAL action inference with comprehensive document context."""
        try:
            config = {"configurable": {"thread_id": f"analysis_{datetime.utcnow().timestamp()}"}}
            
            chunk_info = f" (Chunk: {chunk_reference})" if chunk_reference else ""
            context_section = f"\n\nCOMPREHENSIVE DOCUMENT CONTEXT:\n{comprehensive_analysis}\n" if comprehensive_analysis else ""
            
            message = f"""
            Analyze the following legislation text and infer BOTH organizational rule actions AND practical user actions.
            {context_section}
            
            Article: {article_reference}{chunk_info}
            Countries: {', '.join(countries)}
            Text: {legislation_text}
            
            Use the available tools to:
            1. Identify specific rule conditions related to data processing
            2. Analyze data domains and categories involved
            3. Identify roles and responsibilities for data handling
            
            ORGANIZATIONAL ACTION INFERENCE (Rule Actions):
            4. Infer data processing actions required by organizations/controllers/processors
            5. Focus on practical data operations like encryption, masking, access controls
            6. Infer compliance verification actions for organizational compliance
            7. Infer actions related to data subject rights that organizations must implement
            
            USER ACTION INFERENCE (User Actions):
            8. Infer user actionable tasks that individuals can perform with their data
            9. Focus on individual data protection like personal encryption, privacy settings
            10. Infer user compliance tasks for individual compliance
            11. Infer user rights support tasks that individuals can implement
            
            FOCUS CONSTRAINTS:
            - RULE ACTIONS: Organizational, policy-level, systematic data actions
            - USER ACTIONS: Individual, practical, implementable data tasks
            - Base ALL actions on explicit legislative requirements
            - Focus on concrete data operations: encryption, masking, access control, deletion, backup
            - Ensure actions reference specific articles and are in simple English
            - Use the comprehensive document context to understand relationships
            
            Provide analysis that enables creation of machine-readable rules with BOTH organizational rule actions AND practical user actions.
            """
            
            result = self.agent.invoke(
                {"messages": [HumanMessage(content=message)]},
                config
            )
            
            if result and "messages" in result:
                last_message = result["messages"][-1]
                if hasattr(last_message, 'content'):
                    return last_message.content
                elif isinstance(last_message, dict) and 'content' in last_message:
                    return last_message['content']
            
            return "Agent analysis completed but no content returned"
            
        except Exception as e:
            logger.error(f"Error running dual action inference agent: {e}")
            return f"Error in agent analysis: {str(e)}"
    
    async def _synthesize_rules_with_dual_actions_and_context(
        self, 
        legislation_text: str,
        article_reference: str,
        source_files: Dict[str, Optional[str]],
        document_level: str,
        chunk_reference: Optional[str],
        existing_context: str,
        metadata_context: str,
        applicable_countries: List[str],
        adequacy_countries: List[str],
        focused_analysis: str,
        verified_analysis: str,
        agent_analysis: str,
        comprehensive_analysis: str
    ) -> List[LegislationRule]:
        """Synthesize all analyses into comprehensive structured rules with maximum rule extraction."""
        
        applicable_countries_json = json.dumps(applicable_countries)
        adequacy_countries_json = json.dumps(adequacy_countries)
        source_files_json = json.dumps(source_files)
        
        chunk_context = f"\nCHUNK REFERENCE: {chunk_reference}\n" if chunk_reference else ""
        
        synthesis_prompt = f"""
        Based on the analyses below, create machine-readable rules with MAXIMUM COMPREHENSIVENESS.
        Extract EVERY possible rule, obligation, condition, and requirement from the legislation text.
        Create multiple rules if the text covers different aspects or scenarios.
        
        EXISTING RULES CONTEXT:
        {existing_context}
        
        METADATA CONTEXT:
        {metadata_context}{chunk_context}
        
        COMPREHENSIVE DOCUMENT ANALYSIS:
        {comprehensive_analysis}
        
        SOURCE LEGISLATION:
        Article: {article_reference}
        Document Level: {document_level}
        Source Files: {source_files_json}
        Text: {legislation_text}
        
        ANALYSIS RESULTS:
        
        Focused Analysis:
        {focused_analysis}
        
        Expert Verification:
        {verified_analysis}
        
        Agent Dual Action Analysis:
        {agent_analysis}
        
        COMPREHENSIVE EXTRACTION REQUIREMENTS:
        1. Extract EVERY obligation, requirement, prohibition, permission mentioned
        2. Create separate rules for different roles (controller, processor, data_subject, joint_controller)
        3. Create separate rules for different data categories mentioned
        4. Create separate rules for different scenarios or conditions
        5. Create rules for both positive obligations (must do) and negative obligations (must not do)
        6. Extract rules for different timeframes if mentioned (immediate, within X days, etc.)
        7. Create rules for different jurisdictions if multiple countries are mentioned
        8. Extract both explicit and reasonably implied obligations
        9. Focus on practical data operations and create actionable rules
        10. Create comprehensive conditions that capture all requirements
        
        SYNTHESIS REQUIREMENTS:
        1. Create rules in json-rules-engine format
        2. Each condition must reference the document level: "{document_level}"
        3. Each condition must include chunk_reference if applicable: {chunk_reference}
        4. MANDATORY: Each rule MUST have primary_impacted_role and data_category fields populated
        5. Actions must reference specific articles and be in simple English
        6. Actions must focus on practical data operations (encryption, masking, access controls, etc.)
        7. User actions must be practical tasks individuals can perform
        8. Use exact enum values for all structured fields
        9. Timeline is optional - include only if mentioned in legislation
        10. Include detailed reasoning for each rule showing exactly which part of the text supports it
        
        CRITICAL FIELD REQUIREMENTS:
        - primary_impacted_role: MUST be one of: "controller", "processor", "joint_controller", "data_subject"
        - secondary_impacted_role: Optional, same values as above
        - data_category: MUST be array with values like: "personal_data", "sensitive_data", "biometric_data", "health_data", "financial_data", "location_data", "behavioral_data", "identification_data"
        
        REASONING REQUIREMENTS:
        Each rule must include in its description or metadata exactly which part of the legislation text it was derived from and why.
        
        COMPREHENSIVE RULE CREATION:
        Create multiple rules rather than trying to combine everything into one rule.
        If the text mentions different obligations for controllers vs processors, create separate rules.
        If the text mentions different requirements for different data types, create separate rules.
        If the text has both immediate and long-term requirements, create separate rules.
        
        CRITICAL: Return ONLY a valid JSON array of rules. Create as many rules as necessary to comprehensively cover all obligations in the text.
        
        [
          {{
            "id": "unique_rule_id_1",
            "name": "Rule Name for Specific Obligation",
            "description": "Specific rule description derived from [exact text reference]",
            "source_article": "{article_reference}",
            "source_file": "filename",
            "priority": 1,
            "confidence_score": 0.8,
            "primary_impacted_role": "controller",
            "secondary_impacted_role": "processor",
            "data_category": ["personal_data", "sensitive_data"],
            "applicable_countries": {applicable_countries_json},
            "adequacy_countries": {adequacy_countries_json},
            "source_documents": {source_files_json},
            "rule_derivation_reasoning": "This rule was derived from the text that states '[exact quote]' which creates an obligation for [role] to [action]",
            "conditions": {{
              "all": [
                {{
                  "fact": "specific_condition_fact",
                  "operator": "equal",
                  "value": "condition_value",
                  "description": "Condition based on specific legislative requirement",
                  "data_domain": ["data_usage"],
                  "role": "controller",
                  "reasoning": "This condition derived from article text: '[specific text]' which establishes requirement for [specific scenario]",
                  "document_level": "{document_level}",
                  "chunk_reference": "{chunk_reference or 'none'}"
                }}
              ]
            }},
            "event": {{
              "type": "compliance_required",
              "params": {{}}
            }},
            "actions": [
              {{
                "id": "action_id_1",
                "action_type": "specific_data_operation",
                "title": "Specific Data Protection Action",
                "description": "Implement specific data protection measure as required by {article_reference}: [specific requirement]",
                "priority": "medium",
                "data_specific_steps": ["Specific step 1", "Specific step 2", "Specific step 3"],
                "responsible_role": "controller",
                "legislative_requirement": "Exact requirement from {article_reference}: '[quoted text]'",
                "data_impact": "Specific impact on data processing",
                "verification_method": ["Method 1", "Method 2"],
                "timeline": "optional timeline if specified",
                "derived_from_text": "Exact text from {article_reference} that requires this action",
                "applicable_countries": {applicable_countries_json},
                "confidence_score": 0.8
              }}
            ],
            "user_actions": [
              {{
                "id": "user_action_id_1",
                "action_type": "user_specific_data_operation",
                "title": "User Data Protection Task",
                "description": "Specific task users must perform based on {article_reference}: [specific requirement]",
                "priority": "medium",
                "user_data_steps": ["User step 1", "User step 2", "User step 3"],
                "affected_data_categories": ["personal_data", "sensitive_data"],
                "user_role_context": "data_subject",
                "legislative_requirement": "Exact requirement from {article_reference}: '[quoted text]'",
                "compliance_outcome": "Specific compliance outcome achieved",
                "user_verification_steps": ["Verification 1", "Verification 2"],
                "timeline": "optional timeline if specified",
                "derived_from_text": "Exact text from {article_reference} that requires this user action",
                "confidence_score": 0.8
              }}
            ]
          }},
          {{
            "id": "unique_rule_id_2",
            "name": "Another Rule for Different Obligation",
            ...
          }}
        ]
        
        IMPORTANT RULES:
        - Create MULTIPLE rules to comprehensively cover all obligations
        - Each rule should focus on a specific obligation or requirement
        - Include detailed reasoning and text references for each rule
        - primary_impacted_role and data_category fields are MANDATORY
        - ALL list fields MUST be arrays, never strings
        - Actions must reference the specific article: {article_reference}
        - Actions must be in simple English, no legal jargon
        - Focus on practical data operations
        - Return ONLY the JSON array, no other text or markdown
        
        Be comprehensive - it's better to create more specific rules than to miss obligations.
        """
        
        messages = [
            SystemMessage(content="You are a comprehensive legal-tech expert. Extract EVERY possible rule from the legislation. Create multiple specific rules rather than trying to combine everything. Use simple, clear English. Focus on practical data operations."),
            HumanMessage(content=synthesis_prompt)
        ]
        
        response = await self.openai_service.chat_completion(messages)
        
        parsed_data = self.json_parser.parse_json_response(response)
        
        if "error" in parsed_data:
            logger.error(f"Failed to parse rules JSON: {parsed_data}")
            return []
        
        rules = []
        try:
            if isinstance(parsed_data, list):
                rule_data_list = parsed_data
            elif isinstance(parsed_data, dict) and "rules" in parsed_data:
                rule_data_list = parsed_data["rules"]
            else:
                rule_data_list = [parsed_data] if parsed_data else []
            
            for rule_data in rule_data_list:
                try:
                    # ENHANCED: Ensure critical fields are populated with better inference
                    rule_data.setdefault("id", f"synthesis_rule_{datetime.utcnow().timestamp()}")
                    rule_data.setdefault("name", "Legislative Rule")
                    rule_data.setdefault("description", "Rule extracted from legislation")
                    
                    # CRITICAL: Ensure primary_impacted_role is populated with better inference
                    if not rule_data.get("primary_impacted_role"):
                        rule_data["primary_impacted_role"] = self._infer_primary_role(legislation_text)
                    
                    # CRITICAL: Ensure data_category is populated with better inference
                    if not rule_data.get("data_category") or len(rule_data.get("data_category", [])) == 0:
                        rule_data["data_category"] = self._infer_data_categories(legislation_text)
                    
                    # Ensure other required fields
                    priority_value = rule_data.get("priority", 1)
                    if isinstance(priority_value, str):
                        try:
                            rule_data["priority"] = int(priority_value)
                        except ValueError:
                            rule_data["priority"] = 1
                    elif not isinstance(priority_value, int):
                        rule_data["priority"] = 1
                    else:
                        rule_data["priority"] = priority_value
                    
                    confidence_value = rule_data.get("confidence_score", 0.8)
                    if isinstance(confidence_value, str):
                        try:
                            rule_data["confidence_score"] = float(confidence_value)
                        except ValueError:
                            rule_data["confidence_score"] = 0.8
                    elif not isinstance(confidence_value, (int, float)):
                        rule_data["confidence_score"] = 0.8
                    else:
                        rule_data["confidence_score"] = float(confidence_value)
                    
                    rule_data.setdefault("source_article", article_reference)
                    rule_data.setdefault("source_file", source_files.get("level_1", "unknown"))
                    rule_data.setdefault("applicable_countries", applicable_countries)
                    rule_data.setdefault("adequacy_countries", adequacy_countries)
                    rule_data.setdefault("source_documents", source_files)
                    
                    # Ensure event field exists with proper structure
                    if "event" not in rule_data or not isinstance(rule_data["event"], dict):
                        rule_data["event"] = {
                            "type": "compliance_required",
                            "params": {}
                        }
                    
                    # Fix conditions structure - ensure comprehensive conditions
                    if "conditions" not in rule_data:
                        rule_data["conditions"] = {}
                    
                    conditions = rule_data["conditions"]
                    if isinstance(conditions, list):
                        rule_data["conditions"] = {"all": conditions}
                    elif not isinstance(conditions, dict):
                        rule_data["conditions"] = {"all": []}
                    
                    conditions = rule_data["conditions"]
                    if not any(key in conditions for key in ['all', 'any', 'not']):
                        # Create more specific condition based on the rule
                        rule_name = rule_data.get("name", "")
                        condition_fact = f"rule_requirement_{rule_name.lower().replace(' ', '_')}"
                        
                        rule_data["conditions"] = {
                            "all": [
                                {
                                    "fact": condition_fact,
                                    "operator": "equal",
                                    "value": True,
                                    "description": f"When {rule_data.get('description', 'legislative requirement')} applies",
                                    "data_domain": ["data_usage"],
                                    "role": rule_data.get("primary_impacted_role", "controller"),
                                    "reasoning": f"Condition extracted from {article_reference}: {rule_data.get('rule_derivation_reasoning', 'Legislative requirement')}",
                                    "document_level": document_level,
                                    "chunk_reference": chunk_reference or "none"
                                }
                            ]
                        }
                    
                    # Validate and fix each condition
                    for logic_type in list(conditions.keys()):
                        if logic_type in ['all', 'any', 'not']:
                            condition_list = conditions[logic_type]
                            if not isinstance(condition_list, list):
                                conditions[logic_type] = []
                                continue
                            
                            for i, condition in enumerate(condition_list):
                                if isinstance(condition, dict):
                                    condition.setdefault("fact", f"legislative_requirement_{i}")
                                    condition.setdefault("operator", "equal")
                                    condition.setdefault("value", True)
                                    condition.setdefault("description", "Legislative requirement condition")
                                    condition.setdefault("data_domain", ["data_usage"])
                                    condition.setdefault("role", rule_data.get("primary_impacted_role", "controller"))
                                    condition.setdefault("reasoning", f"Extracted from {article_reference}")
                                    condition.setdefault("document_level", document_level)
                                    condition.setdefault("chunk_reference", chunk_reference or "none")
                                else:
                                    # Replace invalid condition with specific one
                                    conditions[logic_type][i] = {
                                        "fact": f"legislative_requirement_{i}",
                                        "operator": "equal",
                                        "value": True,
                                        "description": "Legislative requirement condition",
                                        "data_domain": ["data_usage"],
                                        "role": rule_data.get("primary_impacted_role", "controller"),
                                        "reasoning": f"Default condition from {article_reference}",
                                        "document_level": document_level,
                                        "chunk_reference": chunk_reference or "none"
                                    }
                        else:
                            del conditions[logic_type]
                    
                    # Add comprehensive processing metadata
                    processing_metadata = {
                        "extraction_method": "comprehensive_synthesis_with_dual_action_inference",
                        "source_article": article_reference,
                        "derivation_reasoning": rule_data.get("rule_derivation_reasoning", "Synthesized from comprehensive analysis")
                    }
                    if chunk_reference:
                        processing_metadata["chunk_reference"] = chunk_reference
                    rule_data["processing_metadata"] = processing_metadata
                    
                    # Actions and user_actions remain optional but are enhanced
                    rule_data.setdefault("actions", [])
                    rule_data.setdefault("user_actions", [])
                    
                    # Enhanced action processing
                    if "actions" in rule_data and isinstance(rule_data["actions"], list):
                        for action in rule_data["actions"]:
                            if isinstance(action, dict):
                                action.setdefault("id", f"synthesis_action_{datetime.utcnow().timestamp()}")
                                action.setdefault("action_type", "data_protection_synthesis")
                                action.setdefault("title", "Data Protection Action Required")
                                action.setdefault("description", f"Action based on comprehensive analysis of {article_reference}")
                                action.setdefault("priority", "medium")
                                
                                # Enhanced data-specific steps
                                if "data_specific_steps" not in action:
                                    action["data_specific_steps"] = [
                                        f"Review requirements from {article_reference}",
                                        "Implement necessary data protection measures",
                                        "Verify compliance with legislative requirements"
                                    ]
                                elif isinstance(action["data_specific_steps"], str):
                                    action["data_specific_steps"] = [action["data_specific_steps"]]
                                elif not isinstance(action["data_specific_steps"], list):
                                    action["data_specific_steps"] = []
                                
                                action.setdefault("responsible_role", rule_data.get("primary_impacted_role", "controller"))
                                action.setdefault("legislative_requirement", f"Compliance requirement from {article_reference}")
                                action.setdefault("data_impact", "Affects data processing as per legislative requirements")
                                
                                if "verification_method" not in action:
                                    action["verification_method"] = ["Compliance audit", "Documentation review"]
                                elif isinstance(action["verification_method"], str):
                                    action["verification_method"] = [action["verification_method"]]
                                elif not isinstance(action["verification_method"], list):
                                    action["verification_method"] = []
                                
                                action.setdefault("derived_from_text", legislation_text[:200])
                                action.setdefault("applicable_countries", applicable_countries)
                                action.setdefault("confidence_score", 0.8)
                    
                    # Enhanced user action processing  
                    if "user_actions" in rule_data and isinstance(rule_data["user_actions"], list):
                        for action in rule_data["user_actions"]:
                            if isinstance(action, dict):
                                action.setdefault("id", f"synthesis_user_action_{datetime.utcnow().timestamp()}")
                                action.setdefault("action_type", "user_data_protection_synthesis")
                                action.setdefault("title", "User Data Protection Action")
                                action.setdefault("description", f"User action based on {article_reference}")
                                action.setdefault("priority", "medium")
                                
                                # Enhanced user data steps
                                if "user_data_steps" not in action:
                                    action["user_data_steps"] = [
                                        f"Understand requirements from {article_reference}",
                                        "Implement personal data protection measures",
                                        "Monitor compliance with requirements"
                                    ]
                                elif isinstance(action["user_data_steps"], str):
                                    action["user_data_steps"] = [action["user_data_steps"]]
                                elif not isinstance(action["user_data_steps"], list):
                                    action["user_data_steps"] = []
                                
                                if "affected_data_categories" not in action:
                                    action["affected_data_categories"] = rule_data.get("data_category", ["personal_data"])
                                elif isinstance(action["affected_data_categories"], str):
                                    action["affected_data_categories"] = [action["affected_data_categories"]]
                                elif not isinstance(action["affected_data_categories"], list):
                                    action["affected_data_categories"] = []
                                
                                action.setdefault("user_role_context", "data_subject")
                                action.setdefault("legislative_requirement", f"User compliance requirement from {article_reference}")
                                action.setdefault("compliance_outcome", "Achieves user compliance with legislative requirements")
                                
                                if "user_verification_steps" not in action:
                                    action["user_verification_steps"] = ["Self-assessment", "Documentation check"]
                                elif isinstance(action["user_verification_steps"], str):
                                    action["user_verification_steps"] = [action["user_verification_steps"]]
                                elif not isinstance(action["user_verification_steps"], list):
                                    action["user_verification_steps"] = []
                                
                                action.setdefault("derived_from_text", legislation_text[:200])
                                action.setdefault("confidence_score", 0.8)
                    
                    # Validate and create the rule using Pydantic v2
                    rule = LegislationRule.model_validate(rule_data)
                    rules.append(rule)
                    logger.info(f"Successfully created comprehensive rule: {rule.name} with {len(rule.actions)} actions and {len(rule.user_actions)} user actions")
                    
                except Exception as e:
                    logger.warning(f"Skipping invalid rule due to error: {e}")
                    logger.warning(f"Rule data keys: {list(rule_data.keys()) if isinstance(rule_data, dict) else 'Not a dict'}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error processing comprehensive rule data: {e}")
            
        # If no rules were created, create multiple minimal rules to ensure coverage
        if not rules:
            logger.warning("No rules could be parsed, creating comprehensive minimal rules from legislation")
            minimal_rules = self._create_comprehensive_minimal_rules(
                legislation_text, article_reference, source_files, document_level, 
                chunk_reference, applicable_countries, adequacy_countries
            )
            rules.extend(minimal_rules)
            
        return rules
    
    def _create_comprehensive_minimal_rules(
        self, legislation_text: str, article_reference: str, source_files: Dict[str, Optional[str]],
        document_level: str, chunk_reference: Optional[str], applicable_countries: List[str], 
        adequacy_countries: List[str]
    ) -> List[LegislationRule]:
        """Create multiple minimal rules to ensure comprehensive coverage when parsing fails."""
        minimal_rules = []
        
        # Create rules for different roles
        roles_to_process = ["controller", "processor", "data_subject"]
        data_categories_inferred = self._infer_data_categories(legislation_text)
        
        for i, role in enumerate(roles_to_process):
            try:
                rule_data = {
                    "id": f"minimal_rule_{role}_{datetime.utcnow().timestamp()}_{i}",
                    "name": f"Legislative Requirement for {role.title()}",
                    "description": f"Requirement extracted from {article_reference} for {role}",
                    "source_article": article_reference,
                    "source_file": source_files.get("level_1", "unknown"),
                    "primary_impacted_role": role,
                    "data_category": data_categories_inferred,
                    "conditions": {
                        "all": [
                            {
                                "fact": f"{role}_legislative_requirement",
                                "operator": "equal",
                                "value": True,
                                "description": f"Legislative requirements for {role} apply",
                                "data_domain": ["data_usage"],
                                "role": role,
                                "reasoning": f"Minimal condition for {role} based on {article_reference}",
                                "document_level": document_level,
                                "chunk_reference": chunk_reference or "none"
                            }
                        ]
                    },
                    "event": {
                        "type": "compliance_required",
                        "params": {}
                    },
                    "priority": 1,
                    "actions": [],
                    "user_actions": [],
                    "confidence_score": 0.5,
                    "applicable_countries": applicable_countries,
                    "adequacy_countries": adequacy_countries,
                    "source_documents": source_files,
                    "processing_metadata": {
                        "extraction_method": "comprehensive_minimal_fallback",
                        "chunk_reference": chunk_reference or "none",
                        "role_specific": role
                    }
                }
                
                rule = LegislationRule.model_validate(rule_data)
                minimal_rules.append(rule)
                logger.info(f"Created minimal rule for {role}")
                
            except Exception as e:
                logger.error(f"Failed to create minimal rule for {role}: {e}")
        
        return minimal_rules

# ===============================
# MAIN EXECUTION FUNCTION
# ===============================

async def main():
    """Main execution function with enhanced processing and improved output display."""
    
    analyzer = LegislationAnalyzer()
    
    try:
        print("\n=== ADVANCED LEGISLATION RULES CONVERTER WITH COMPREHENSIVE DOCUMENT ANALYSIS ===")
        print("Processing legislation with dynamic chunking, dual action inference (rule + user), comprehensive document analysis, and anti-hallucination measures...\n")
        
        # Show metadata configuration
        print("📋 METADATA CONFIGURATION:")
        processing_entries = analyzer.metadata_manager.get_all_processing_entries()
        if processing_entries:
            print(f"✅ Configured entries: {len(processing_entries)}")
            for entry_id, metadata in processing_entries:
                print(f"   📂 {entry_id}:")
                print(f"      🌍 Countries: {', '.join(metadata.country)}")
                if metadata.adequacy_country:
                    print(f"      🤝 Adequacy: {', '.join(metadata.adequacy_country)}")
                if metadata.file_level_1:
                    print(f"      📄 Level 1: {metadata.file_level_1}")
                if metadata.file_level_2:
                    print(f"      📖 Level 2: {metadata.file_level_2}")
                if metadata.file_level_3:
                    print(f"      📘 Level 3: {metadata.file_level_3}")
                print()
        else:
            print("⚠️ No configured entries found in legislation_metadata.json")
            print("Please create legislation_metadata.json with your configuration")
            return
        
        print(f"📁 Config file: {Config.METADATA_CONFIG_FILE}")
        print(f"🔧 Chunk size: {Config.CHUNK_SIZE} chars, Overlap: {Config.OVERLAP_SIZE} chars")
        print(f"📏 Chunking threshold: {Config.MAX_FILE_SIZE / (1024*1024):.1f} MB")
        print()
        
        # Check PDF processing availability
        if not PDF_AVAILABLE:
            print("⚠️ Warning: PDF processing libraries not available.")
            print("Install with: pip install PyMuPDF pdfplumber")
            return
        
        # Process configured entries
        print("🔍 Processing configured legislation entries...")
        print("ℹ️ Note: Processing will run regardless of existing rules (no skipping)")
        os.makedirs(Config.LEGISLATION_PDF_PATH, exist_ok=True)
        
        result = await analyzer.process_legislation_folder()
        
        # Print results
        print(f"\n=== PROCESSING RESULTS ===")
        print(f"📊 Summary: {result.summary}")
        print(f"📈 Total Rules: {result.total_rules}")
        print(f"🎯 Total Rule Actions: {result.total_actions}")
        print(f"👤 Total User Actions: {result.total_user_actions}")
        print(f"⏱️ Processing Time: {result.processing_time:.2f} seconds")
        print(f"🔗 Integrated Rules: {len(result.integrated_rules)}")
        print(f"📚 Documents Processed: {result.documents_processed}")
        
        if result.chunking_metadata:
            print(f"🧩 Chunking Applied:")
            for doc_id, chunk_info in result.chunking_metadata.items():
                print(f"   {doc_id}: {chunk_info['chunks']} chunks")
        
        if result.rules:
            print(f"\n=== EXTRACTED RULES WITH DUAL ACTIONS ===")
            for i, rule in enumerate(result.rules, 1):
                print(f"\n🔍 Rule {i}: {rule.name}")
                print(f"   📝 Description: {rule.description}")
                print(f"   📄 Source: {rule.source_article}")
                
                # FIXED: Display roles and data categories properly
                primary_role_display = "not_specified"
                if rule.primary_impacted_role:
                    primary_role_display = rule.primary_impacted_role.value if hasattr(rule.primary_impacted_role, 'value') else str(rule.primary_impacted_role)
                print(f"   🎯 Primary Role: {primary_role_display}")
                
                secondary_role_display = "not_specified"
                if rule.secondary_impacted_role:
                    secondary_role_display = rule.secondary_impacted_role.value if hasattr(rule.secondary_impacted_role, 'value') else str(rule.secondary_impacted_role)
                    print(f"   🎯 Secondary Role: {secondary_role_display}")
                
                data_categories_display = []
                if rule.data_category:
                    data_categories_display = [cat.value if hasattr(cat, 'value') else str(cat) for cat in rule.data_category]
                print(f"   📊 Data Categories: {', '.join(data_categories_display) if data_categories_display else 'not_specified'}")
                
                print(f"   🌍 Countries: {', '.join(rule.applicable_countries) if rule.applicable_countries else 'Not specified'}")
                print(f"   ⭐ Confidence: {rule.confidence_score}")
                
                # Show processing metadata including chunking
                if rule.processing_metadata:
                    if rule.processing_metadata.get("chunk_reference"):
                        print(f"   🧩 Chunk: {rule.processing_metadata['chunk_reference']}")
                
                print(f"   📋 Conditions:")
                for logic_type, conditions in rule.conditions.items():
                    print(f"      {logic_type.upper()}:")
                    for condition in conditions:
                        print(f"        - {condition.description}")
                        
                        operator_display = condition.operator.value if hasattr(condition.operator, 'value') else str(condition.operator)
                        role_display = "not_specified"
                        if condition.role:
                            role_display = condition.role.value if hasattr(condition.role, 'value') else str(condition.role)
                        
                        domain_displays = []
                        if condition.data_domain:
                            domain_displays = [d.value if hasattr(d, 'value') else str(d) for d in condition.data_domain]
                        
                        level_display = condition.document_level.value if hasattr(condition.document_level, 'value') else str(condition.document_level)
                        
                        print(f"          Fact: {condition.fact} | Operator: {operator_display} | Value: {condition.value}")
                        print(f"          Role: {role_display} | Domains: {', '.join(domain_displays) if domain_displays else 'not_specified'}")
                        print(f"          Level: {level_display}")
                        if condition.chunk_reference:
                            print(f"          Chunk: {condition.chunk_reference}")
                
                # Show RULE ACTIONS (Organizational) with article references
                if rule.actions:
                    print(f"   🏢 RULE ACTIONS - Organizational ({len(rule.actions)}):")
                    for action in rule.actions:
                        print(f"      🔧 {action.title} ({action.action_type})")
                        print(f"         Priority: {action.priority}")
                        print(f"         Description: {action.description}")
                        print(f"         Legislative Requirement: {action.legislative_requirement}")
                        print(f"         Data Impact: {action.data_impact}")
                        print(f"         Data-Specific Steps: {', '.join(action.data_specific_steps)}")
                        if action.responsible_role:
                            print(f"         Responsible: {action.responsible_role}")
                        if action.timeline:
                            print(f"         Timeline: {action.timeline}")
                        print(f"         Verification: {', '.join(action.verification_method)}")
                        print(f"         Derived From: {action.derived_from_text[:100]}...")
                        print(f"         Confidence: {action.confidence_score}")
                        print()
                else:
                    print(f"   🏢 RULE ACTIONS - Organizational: None inferred")
                
                # Show USER ACTIONS (Individual) with article references
                if rule.user_actions:
                    print(f"   👤 USER ACTIONS - Individual ({len(rule.user_actions)}):")
                    for action in rule.user_actions:
                        print(f"      🔧 {action.title} ({action.action_type})")
                        print(f"         Priority: {action.priority}")
                        print(f"         Description: {action.description}")
                        print(f"         Legislative Requirement: {action.legislative_requirement}")
                        print(f"         Compliance Outcome: {action.compliance_outcome}")
                        print(f"         User Data Steps: {', '.join(action.user_data_steps)}")
                        if action.affected_data_categories:
                            print(f"         Affected Data: {', '.join(action.affected_data_categories)}")
                        if action.user_role_context:
                            print(f"         User Role: {action.user_role_context}")
                        if action.timeline:
                            print(f"         Timeline: {action.timeline}")
                        print(f"         User Verification: {', '.join(action.user_verification_steps)}")
                        print(f"         Derived From: {action.derived_from_text[:100]}...")
                        print(f"         Confidence: {action.confidence_score}")
                        print()
                else:
                    print(f"   👤 USER ACTIONS - Individual: None inferred")
                
                # Show integrated alignment
                if i <= len(result.integrated_rules):
                    integrated_rule = result.integrated_rules[i-1]
                    print(f"   🔗 Integrated Standards:")
                    print(f"      DPV Processing: {[p.split('#')[-1] for p in integrated_rule.dpv_hasProcessing] if integrated_rule.dpv_hasProcessing else 'none'}")
                    print(f"      DPV Purposes: {[p.split('#')[-1] for p in integrated_rule.dpv_hasPurpose] if integrated_rule.dpv_hasPurpose else 'none'}")
                    print(f"      DPV Data Types: {[d.split('#')[-1] for d in integrated_rule.dpv_hasPersonalData] if integrated_rule.dpv_hasPersonalData else 'none'}")
                    print(f"      DPV Rule Actions: {[a.split('#')[-1] for a in integrated_rule.dpv_hasRuleAction] if integrated_rule.dpv_hasRuleAction else 'none'}")
                    print(f"      DPV User Actions: {[a.split('#')[-1] for a in integrated_rule.dpv_hasUserAction] if integrated_rule.dpv_hasUserAction else 'none'}")
                    print(f"      ODRE Dual Action Inference: Rule={integrated_rule.odre_action_inference}, User={integrated_rule.odre_user_action_inference}")
                    if integrated_rule.chunk_references:
                        print(f"      Chunk References: {integrated_rule.chunk_references}")
                
                print("-" * 80)
        
        # Save results in multiple formats
        if result.rules:
            print(f"\n=== SAVING RESULTS ===")
            
            # Ensure output directories exist
            os.makedirs(Config.RULES_OUTPUT_PATH, exist_ok=True)
            os.makedirs(Config.STANDARDS_OUTPUT_PATH, exist_ok=True)
            
            # Generate timestamp for unique filenames
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            
            print("Enhanced formats with dual actions:")
            
            # Save JSON format with dual actions
            json_file = os.path.join(Config.RULES_OUTPUT_PATH, f"rules_with_dual_actions_{timestamp}.json")
            result.save_json(json_file)
            print(f"   JSON Rules with Dual Actions: {json_file}")
            
            # Save single comprehensive CSV format
            csv_file = os.path.join(Config.RULES_OUTPUT_PATH, f"rules_with_dual_actions_{timestamp}.csv")
            result.save_csv(csv_file)
            
            print("\nIntegrated Standards Formats:")
            
            # Save integrated formats
            if result.integrated_rules:
                integrated_ttl_file = os.path.join(Config.STANDARDS_OUTPUT_PATH, f"integrated_standards_{timestamp}.ttl")
                result.save_integrated_ttl(integrated_ttl_file)
                print(f"   Integrated TTL: {integrated_ttl_file}")
                
                integrated_jsonld_file = os.path.join(Config.STANDARDS_OUTPUT_PATH, f"integrated_standards_{timestamp}.jsonld")
                result.save_integrated_jsonld(integrated_jsonld_file)
                print(f"   Integrated JSON-LD: {integrated_jsonld_file}")
                
                integrated_json_file = os.path.join(Config.STANDARDS_OUTPUT_PATH, f"integrated_rules_{timestamp}.json")
                result.save_integrated_json(integrated_json_file)
                print(f"   Integrated JSON: {integrated_json_file}")
            
            print(f"\nStandards Integration Summary:")
            print(f"   DPV v2.1: Processing activities with dynamic action mappings")
            print(f"   ODRL: Policy expressions with data-specific constraints") 
            print(f"   ODRE: Enforcement framework with dual action inference capability")
            print(f"   Multi-Level Processing: Legislation + guidance docs integration")
            print(f"   Dynamic Chunking: Large document processing support")
            print(f"   Comprehensive Analysis: Whole document understanding")
            print(f"   Anti-Hallucination: Focused analysis with verification")
            
            # Show dual action statistics
            if result.total_actions > 0 or result.total_user_actions > 0:
                rule_action_types = {}
                user_action_types = {}
                priorities = {}
                
                for rule in result.rules:
                    # Rule action statistics
                    for action in rule.actions:
                        rule_action_types[action.action_type] = rule_action_types.get(action.action_type, 0) + 1
                        priorities[action.priority] = priorities.get(action.priority, 0) + 1
                    
                    # User action statistics
                    for action in rule.user_actions:
                        user_action_types[action.action_type] = user_action_types.get(action.action_type, 0) + 1
                        priorities[action.priority] = priorities.get(action.priority, 0) + 1
                
                print(f"\n🎯 DUAL ACTION INFERENCE STATISTICS:")
                print(f"   Total Rule Actions (Organizational): {result.total_actions}")
                print(f"   Total User Actions (Individual): {result.total_user_actions}")
                print(f"   Unique Rule Action Types: {len(rule_action_types)}")
                print(f"   Unique User Action Types: {len(user_action_types)}")
                if rule_action_types:
                    print(f"   Most Common Rule Types: {dict(sorted(rule_action_types.items(), key=lambda x: x[1], reverse=True)[:3])}")
                if user_action_types:
                    print(f"   Most Common User Types: {dict(sorted(user_action_types.items(), key=lambda x: x[1], reverse=True)[:3])}")
                print(f"   Priority Distribution: {dict(priorities)}")
                
                # Calculate average confidence for both action types
                if result.total_actions > 0:
                    total_rule_confidence = sum(action.confidence_score for rule in result.rules for action in rule.actions)
                    avg_rule_confidence = total_rule_confidence / result.total_actions
                    print(f"   Average Rule Action Confidence: {avg_rule_confidence:.2f}")
                
                if result.total_user_actions > 0:
                    total_user_confidence = sum(action.confidence_score for rule in result.rules for action in rule.user_actions)
                    avg_user_confidence = total_user_confidence / result.total_user_actions
                    print(f"   Average User Action Confidence: {avg_user_confidence:.2f}")
            
            # Show database status
            total_existing = len(analyzer.rule_manager.existing_rules)
            existing_rule_actions = sum(len(rule.actions) for rule in analyzer.rule_manager.existing_rules)
            existing_user_actions = sum(len(rule.user_actions) for rule in analyzer.rule_manager.existing_rules)
            print(f"\n=== RULE DATABASE STATUS ===")
            print(f"Total rules in database: {total_existing}")
            print(f"Total rule actions in database: {existing_rule_actions}")
            print(f"Total user actions in database: {existing_user_actions}")
            print(f"New rules added: {len(result.rules)}")
            print(f"New rule actions added: {result.total_actions}")
            print(f"New user actions added: {result.total_user_actions}")
            print(f"Database file: {Config.EXISTING_RULES_FILE}")
            
        else:
            print("\nNo rules were extracted.")
        
        print(f"\nAdvanced processing with comprehensive document analysis and dual action inference complete!")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise

if __name__ == "__main__":
    # Run the enhanced main function
    asyncio.run(main())
