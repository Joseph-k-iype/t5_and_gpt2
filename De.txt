"""
Relevance evaluation node for the Agentic RAG system.

This module implements the relevance evaluation node for the Agentic RAG system,
which uses an LLM to evaluate the relevance of retrieved candidates and calculates
a final relevance score for each candidate.
"""

import json
import logging
import re
import time
from typing import Dict, Any, List, Optional, Tuple

from app.config.settings import get_llm

logger = logging.getLogger(__name__)

async def relevance_evaluation(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Use LLM to evaluate the actual relevance of candidates.
    This helps filter out candidates that are similar in vector space
    but semantically irrelevant.
    
    Args:
        state: Current graph state
        
    Returns:
        Updated state with relevance evaluations
    """
    candidates = state.get("candidates", [])
    logger.info(f"Evaluating relevance of {len(candidates)} candidates")
    
    # If we have no candidates, skip evaluation
    if not candidates:
        logger.warning("No candidates to evaluate")
        state["top_candidates"] = []
        return state
    
    try:
        # Get LLM
        llm = get_llm()
        
        # Prepare candidates for evaluation
        # Limit to top candidates to avoid token limits
        max_candidates = min(10, len(candidates))
        candidates_to_evaluate = sorted(
            candidates, 
            key=lambda x: max(x.get("vector_score", 0), x.get("keyword_score", 0)), 
            reverse=True
        )[:max_candidates]
        
        # Format candidates for prompt
        candidate_text = ""
        for i, candidate in enumerate(candidates_to_evaluate):
            candidate_text += f"""
Candidate {i+1}:
  ID: {candidate.get("id", "")}
  Name: {candidate.get("name", "")}
  Description: {candidate.get("description", "")}
  Vector Score: {candidate.get("vector_score", 0):.2f}
  Keyword Score: {candidate.get("keyword_score", 0):.2f}
"""
        
        # Get context information
        element_name = state.get("element_name", "")
        element_description = state.get("element_description", "")
        context_info = []
        
        if state.get("cdm_context"):
            context_info.append(f"CDM: {state.get('cdm_context')}")
        if state.get("example_context"):
            context_info.append(f"Examples: {state.get('example_context')}")
        if state.get("process_name_context"):
            context_info.append(f"Related Process: {state.get('process_name_context')}")
        if state.get("process_description_context"):
            context_info.append(f"Process Description: {state.get('process_description_context')}")
        
        context_str = "\n".join(context_info) if context_info else "No additional context"
        
        # Create prompt for LLM
        prompt = f"""
You are a data governance expert evaluating the relevance of Preferred Business Terms (PBTs) for a data element.

Data Element:
  Name: {element_name}
  Description: {element_description}
  
Additional Context:
{context_str}

Candidate PBTs:
{candidate_text}

For each candidate, determine how semantically relevant it is to the data element.
Consider:
1. How well the candidate's name matches the element's meaning
2. How well the candidate's description aligns with the element's purpose
3. Whether the concept is the same, even if expressed differently
4. Business term equivalence (e.g., "account number" can match "account identifier")

Rate each candidate on a scale of 0-10 and provide brief reasoning.

Return your evaluation as a JSON array with this structure:
[
  {{
    "id": "candidate_id",
    "score": 8.5,
    "reasoning": "Brief explanation of why this is a good/bad match"
  }}
]

Ensure your response contains ONLY the JSON array with no other text.
"""
        
        # Time the LLM evaluation
        start_time = time.time()
        
        # Get evaluation from LLM
        evaluation_response = await llm.ainvoke(prompt)
        
        llm_time = time.time() - start_time
        logger.debug(f"LLM evaluation took {llm_time:.2f}s")
        
        # Parse evaluation
        evaluations = _extract_evaluations_from_llm(evaluation_response)
        
        # Check if evaluation extraction failed
        if not evaluations:
            logger.warning("Failed to extract evaluations from LLM response. Using fallback scoring.")
            # Create fallback evaluations
            evaluations = []
            for candidate in candidates_to_evaluate:
                evaluations.append({
                    "id": candidate.get("id", ""),
                    "score": max(candidate.get("vector_score", 0), candidate.get("keyword_score", 0)) * 10,  # Scale to 0-10
                    "reasoning": "Score based on vector and keyword similarity (fallback)"
                })
        
        # Create map of candidates by ID for fast lookup
        candidate_map = {c.get("id", ""): c for c in candidates}
        
        # Update candidates with LLM evaluations
        for eval_item in evaluations:
            candidate_id = eval_item.get("id", "")
            if candidate_id in candidate_map:
                # Normalize score to 0-1 range
                semantic_score = eval_item.get("score", 0) / 10.0
                candidate_map[candidate_id]["semantic_score"] = semantic_score
                candidate_map[candidate_id]["reasoning"] = eval_item.get("reasoning", "")
                
                # Calculate final score as a weighted combination
                vector_score = candidate_map[candidate_id].get("vector_score", 0)
                keyword_score = candidate_map[candidate_id].get("keyword_score", 0)
                
                # Weighted average with LLM evaluation getting highest weight
                final_score = (
                    semantic_score * 0.6 + 
                    vector_score * 0.3 + 
                    keyword_score * 0.1
                )
                
                candidate_map[candidate_id]["final_score"] = final_score
        
        # Handle any candidates that weren't evaluated
        for candidate in candidates:
            if candidate.get("semantic_score") is None:
                candidate["semantic_score"] = max(candidate.get("vector_score", 0), candidate.get("keyword_score", 0))
                candidate["final_score"] = candidate["semantic_score"]
                candidate["reasoning"] = "Score based on vector and keyword similarity (not LLM-evaluated)"
        
        logger.info("Completed candidate evaluation")
        
        # Sort candidates by final score
        candidates.sort(key=lambda x: x.get("final_score", 0) or 0.0, reverse=True)
        
        # Select top candidates
        top_k = state.get("top_k", 3)
        min_score = state.get("threshold", 0.5)
        
        # Filter to candidates that meet the threshold
        top_candidates = [c for c in candidates if (c.get("final_score", 0) or 0.0) >= min_score]
        
        # If we have too few results, try to lower the threshold
        if len(top_candidates) < min(2, top_k) and candidates:
            # Use adaptive threshold - take at least 2 if available
            top_candidates = candidates[:min(2, len(candidates))]
        
        # Limit to top_k
        top_candidates = top_candidates[:top_k]
        
        # Update state
        state["top_candidates"] = top_candidates
        
        logger.info(f"Selected {len(top_candidates)} top candidates")
        return state
        
    except Exception as e:
        logger.error(f"Error in relevance evaluation: {e}", exc_info=True)
        state["error"] = f"Relevance evaluation failed: {str(e)}"
        
        # Fallback to basic scoring
        for candidate in candidates:
            candidate["final_score"] = max(candidate.get("vector_score", 0), candidate.get("keyword_score", 0))
        
        # Sort and select top candidates as fallback
        candidates.sort(key=lambda x: x.get("final_score", 0) or 0.0, reverse=True)
        top_k = state.get("top_k", 3)
        state["top_candidates"] = candidates[:top_k]
        
        return state

def _extract_evaluations_from_llm(response: str) -> List[Dict[str, Any]]:
    """
    Extract candidate evaluations from LLM response.
    
    Args:
        response: LLM response text
        
    Returns:
        List of evaluation dictionaries
    """
    try:
        # Try direct JSON parsing first
        return json.loads(response)
    except json.JSONDecodeError:
        # Try to find JSON array in the response
        try:
            # Find text that looks like an array
            list_match = re.search(r'(\[.*\])', response, re.DOTALL)
            if list_match:
                return json.loads(list_match.group(1))
        except (json.JSONDecodeError, AttributeError):
            pass
        
        # More aggressive pattern matching if the above fails
        try:
            # Look for content between triple backticks
            code_block_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if code_block_match:
                return json.loads(code_block_match.group(1))
        except (json.JSONDecodeError, AttributeError):
            pass
    
    # If all extraction methods fail, return empty list
    return []

def calculate_combined_score(
    semantic_score: float, 
    vector_score: float, 
    keyword_score: float,
    semantic_weight: float = 0.6,
    vector_weight: float = 0.3,
    keyword_weight: float = 0.1
) -> float:
    """
    Calculate a combined score from semantic, vector, and keyword scores.
    
    Args:
        semantic_score: Semantic relevance score (0-1)
        vector_score: Vector similarity score (0-1)
        keyword_score: Keyword match score (0-1)
        semantic_weight: Weight for semantic score
        vector_weight: Weight for vector score
        keyword_weight: Weight for keyword score
        
    Returns:
        Combined score (0-1)
    """
    # Ensure weights sum to 1
    total_weight = semantic_weight + vector_weight + keyword_weight
    if total_weight != 1.0:
        semantic_weight /= total_weight
        vector_weight /= total_weight
        keyword_weight /= total_weight
    
    # Calculate weighted average
    combined = (
        semantic_score * semantic_weight +
        vector_score * vector_weight +
        keyword_score * keyword_weight
    )
    
    # Ensure result is in [0, 1]
    return max(0.0, min(1.0, combined))
