#!/usr/bin/env python3
"""
Enhanced Legal Document to Machine-Readable Rules System (Rules-as-Code)
Multi-Agent Architecture with LangGraph, ReAct Agents, and LangMem

Features:
- True ReAct agents with LangGraph orchestration
- Multi-agent chain of experts architecture
- LangMem for long-term memory across sessions
- o3-mini-2025-01-31 with reasoning effort control
- Line-by-line analysis with internal graph reasoning
- Comprehensive ontology generation with validation

Author: Enhanced Rules-as-Code Implementation
Date: July 2025
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple, TypedDict, Annotated
from pathlib import Path
from datetime import datetime
import ssl
import threading

# Core dependencies
import openai
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# Document processing
import pymupdf  # PyMuPDF for PDF processing

# RDF and SPARQL dependencies
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD, DCTERMS
import pyshacl

# LangGraph and LangMem dependencies
from langgraph.graph import StateGraph, MessagesState, START, END, add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_core.language_models.base import BaseLanguageModel

# LangMem imports
from langmem import (
    create_memory_manager,
    create_manage_memory_tool,
    create_search_memory_tool
)

# Web interface dependencies
try:
    from flask import Flask, request, jsonify, render_template_string, Response
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False
    logging.warning("Flask not available - web interface will be disabled")

# Token counting
import tiktoken

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ====================================
# GLOBAL CONFIGURATION
# ====================================

class Config:
    """Global configuration for the Rules-as-Code system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    OPENAI_MODEL = "o3-mini-2025-01-31"
    OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL", "https://localhost:9200")
    ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")
    ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./elasticsearch.crt")
    ELASTICSEARCH_INDEX = "legal_rules_as_code_index"
    
    # Web Interface Configuration
    WEB_HOST = os.getenv("WEB_HOST", "localhost")
    WEB_PORT = int(os.getenv("WEB_PORT", "5000"))
    
    # Data paths
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./config.json")
    OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output")
    ONTOLOGY_PATH = os.getenv("ONTOLOGY_PATH", "./ontologies")
    
    # o3-mini Model parameters
    REASONING_EFFORT = "high"  # low, medium, high for complex legal analysis
    MAX_COMPLETION_TOKENS = 8000
    
    # Processing parameters
    BATCH_SIZE = 5
    MAX_CONCURRENT = 3
    CHUNK_SIZE = 4000
    
    @classmethod
    def validate_config(cls):
        """Validate that all required configuration is present"""
        missing_vars = []
        
        if not cls.OPENAI_API_KEY:
            missing_vars.append("OPENAI_API_KEY")
        
        if missing_vars:
            raise ValueError(
                f"Required environment variables are missing: {', '.join(missing_vars)}\n"
                f"Please set them in your .env file or environment variables."
            )

# ====================================
# ENHANCED LEGAL ONTOLOGY NAMESPACES
# ====================================

class LegalRulesNamespaces:
    """Enhanced namespaces for Rules-as-Code with DPV and PROV-O integration"""
    
    # Core W3C vocabularies
    DPV = Namespace("https://w3id.org/dpv#")
    PROV = Namespace("http://www.w3.org/ns/prov#")
    
    # Legal domain vocabularies
    ELI = Namespace("http://data.europa.eu/eli/ontology#")
    LEX = Namespace("http://www.lexinfo.net/ontology/2.0/lexinfo#")
    
    # Our enhanced Rules-as-Code namespace
    RAC = Namespace("https://rules-as-code.org/ontology#")
    
    # Data management domain-specific namespaces
    STORAGE = Namespace("https://rules-as-code.org/storage#")
    USAGE = Namespace("https://rules-as-code.org/usage#")
    MOVEMENT = Namespace("https://rules-as-code.org/movement#")
    PRIVACY = Namespace("https://rules-as-code.org/privacy#")
    SECURITY = Namespace("https://rules-as-code.org/security#")
    ACCESS = Namespace("https://rules-as-code.org/access#")
    ENTITLEMENTS = Namespace("https://rules-as-code.org/entitlements#")
    
    # Property and relation namespaces
    PROPERTIES = Namespace("https://rules-as-code.org/properties#")
    RELATIONS = Namespace("https://rules-as-code.org/relations#")
    
    @classmethod
    def bind_to_graph(cls, graph: Graph) -> Graph:
        """Bind all namespaces to a graph"""
        graph.bind("dpv", cls.DPV)
        graph.bind("prov", cls.PROV)
        graph.bind("eli", cls.ELI)
        graph.bind("lex", cls.LEX)
        graph.bind("rac", cls.RAC)
        graph.bind("storage", cls.STORAGE)
        graph.bind("usage", cls.USAGE)
        graph.bind("movement", cls.MOVEMENT)
        graph.bind("privacy", cls.PRIVACY)
        graph.bind("security", cls.SECURITY)
        graph.bind("access", cls.ACCESS)
        graph.bind("entitlements", cls.ENTITLEMENTS)
        graph.bind("properties", cls.PROPERTIES)
        graph.bind("relations", cls.RELATIONS)
        graph.bind("skos", SKOS)
        graph.bind("rdf", RDF)
        graph.bind("rdfs", RDFS)
        graph.bind("owl", OWL)
        graph.bind("xsd", XSD)
        graph.bind("dcterms", DCTERMS)
        return graph

# ====================================
# DIRECT OPENAI CLIENT
# ====================================

class DirectOpenAIClient:
    """Direct OpenAI client for o3-mini with reasoning effort control"""
    
    def __init__(self):
        Config.validate_config()
        
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # Initialize tiktoken for token counting
        try:
            self.encoding = tiktoken.get_encoding("o200k_base")
        except:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # Test the client
        try:
            test_response = self.client.embeddings.create(
                input=["test"],
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            logger.info(f"OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConnectionError(f"OpenAI API connection failed: {e}")
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts using OpenAI API directly"""
        logger.info(f"Generating embeddings for {len(texts)} text(s)...")
        
        try:
            # Truncate texts if too long
            truncated_texts = []
            for text in texts:
                token_count = len(self.encoding.encode(text))
                if token_count > 8000:  # Conservative limit for embeddings
                    tokens = self.encoding.encode(text)[:8000]
                    truncated_text = self.encoding.decode(tokens)
                    truncated_texts.append(truncated_text)
                else:
                    truncated_texts.append(text)
            
            response = self.client.embeddings.create(
                input=truncated_texts,
                model=Config.OPENAI_EMBEDDING_MODEL
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Generate chat completion with o3-mini and reasoning effort"""
        logger.info(f"Starting chat completion with model: {Config.OPENAI_MODEL}")
        
        try:
            # Prepare messages for o3-mini
            prepared_messages = self._prepare_messages_for_o3_mini(messages)
            
            # Set reasoning effort based on complexity
            reasoning_effort = kwargs.get('reasoning_effort', Config.REASONING_EFFORT)
            
            response = self.client.chat.completions.create(
                model=Config.OPENAI_MODEL,
                messages=prepared_messages,
                reasoning_effort=reasoning_effort,
                max_completion_tokens=kwargs.get('max_completion_tokens', Config.MAX_COMPLETION_TOKENS)
            )
            
            content = response.choices[0].message.content
            logger.info(f"Received response with {len(content)} characters")
            
            return content
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            raise
    
    def _prepare_messages_for_o3_mini(self, messages: List[Dict]) -> List[Dict]:
        """Prepare messages for o3-mini model with token management"""
        
        # Convert system messages to developer messages for o3-mini
        prepared_messages = []
        for msg in messages:
            if msg.get('role') == 'system':
                prepared_messages.append({
                    'role': 'developer',  # o3-mini uses developer role instead of system
                    'content': msg.get('content', '')
                })
            else:
                prepared_messages.append(msg)
        
        # Check token count and truncate if necessary
        total_tokens = sum(len(self.encoding.encode(msg.get('content', ''))) 
                          for msg in prepared_messages)
        
        if total_tokens > 150000:  # Conservative limit for o3-mini
            # Truncate the longest message
            longest_msg_idx = max(range(len(prepared_messages)), 
                                key=lambda i: len(prepared_messages[i].get('content', '')))
            
            original_content = prepared_messages[longest_msg_idx]['content']
            tokens = self.encoding.encode(original_content)[:100000]
            truncated_content = self.encoding.decode(tokens) + "\n\n[Content truncated due to length]"
            prepared_messages[longest_msg_idx]['content'] = truncated_content
            
            logger.warning("Messages truncated due to token limit")
        
        return prepared_messages

# ====================================
# CUSTOM LANGUAGE MODEL WRAPPER
# ====================================

class O3MiniLanguageModel(BaseLanguageModel):
    """Custom language model wrapper for o3-mini to work with LangGraph"""
    
    def __init__(self, openai_client: DirectOpenAIClient):
        self.openai_client = openai_client
        super().__init__()
    
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """Generate response using o3-mini"""
        # Convert LangChain messages to OpenAI format
        openai_messages = []
        for msg in messages:
            if hasattr(msg, 'content'):
                role = getattr(msg, 'type', 'user')
                if role == 'system':
                    role = 'developer'  # o3-mini uses developer instead of system
                elif role == 'human':
                    role = 'user'
                elif role == 'ai':
                    role = 'assistant'
                
                openai_messages.append({
                    'role': role,
                    'content': msg.content
                })
        
        # Make async call in sync context
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        response = loop.run_until_complete(
            self.openai_client.chat_completion(openai_messages, **kwargs)
        )
        
        # Return in expected format
        from langchain_core.outputs import LLMResult, Generation
        from langchain_core.messages import AIMessage
        
        generation = Generation(text=response, message=AIMessage(content=response))
        return LLMResult(generations=[[generation]])
    
    async def _agenerate(self, messages, stop=None, run_manager=None, **kwargs):
        """Async generate response"""
        openai_messages = []
        for msg in messages:
            if hasattr(msg, 'content'):
                role = getattr(msg, 'type', 'user')
                if role == 'system':
                    role = 'developer'
                elif role == 'human':
                    role = 'user'
                elif role == 'ai':
                    role = 'assistant'
                
                openai_messages.append({
                    'role': role,
                    'content': msg.content
                })
        
        response = await self.openai_client.chat_completion(openai_messages, **kwargs)
        
        from langchain_core.outputs import LLMResult, Generation
        from langchain_core.messages import AIMessage
        
        generation = Generation(text=response, message=AIMessage(content=response))
        return LLMResult(generations=[[generation]])
    
    @property
    def _llm_type(self) -> str:
        return "o3-mini"

# ====================================
# STATE DEFINITIONS FOR LANGGRAPH
# ====================================

class DocumentProcessingState(TypedDict):
    """State for document processing workflow"""
    messages: Annotated[List, add_messages]
    document_path: str
    document_metadata: Dict[str, Any]
    extracted_text: Optional[str]
    processing_status: str
    current_agent: str
    extraction_results: Optional[Dict[str, Any]]
    ontology_graph: Optional[str]  # Serialized graph
    validation_results: Optional[Dict[str, Any]]
    memory_stored: bool
    exports: Optional[Dict[str, str]]
    reasoning_log: List[Dict[str, Any]]

class AgentState(TypedDict):
    """Base state for individual agents"""
    messages: Annotated[List, add_messages]
    current_task: str
    context: Dict[str, Any]
    results: Dict[str, Any]
    reasoning_steps: List[str]
    tool_calls: List[Dict[str, Any]]

# ====================================
# TOOLS FOR REACT AGENTS
# ====================================

# Document processing tools
@tool
def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text content from a PDF document"""
    try:
        doc = pymupdf.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text.strip()
    except Exception as e:
        return f"Error extracting text from PDF: {str(e)}"

@tool
def analyze_document_structure(text: str) -> Dict[str, Any]:
    """Analyze the structure of a legal document"""
    try:
        lines = text.split('\n')
        structure = {
            "total_lines": len(lines),
            "estimated_pages": len(text) // 3000,
            "has_articles": any("Article" in line for line in lines[:100]),
            "has_sections": any("Section" in line for line in lines[:100]),
            "has_definitions": any("definition" in line.lower() for line in lines[:200]),
            "language_indicators": {
                "gdpr_terms": any(term in text.lower() for term in ["personal data", "data controller", "data processor"]),
                "privacy_terms": any(term in text.lower() for term in ["privacy", "consent", "rights"]),
                "data_management": any(term in text.lower() for term in ["storage", "retention", "transfer"])
            }
        }
        return structure
    except Exception as e:
        return {"error": f"Error analyzing structure: {str(e)}"}

# Validation tools
@tool
def validate_extraction_completeness(extraction_data: Dict[str, Any]) -> Dict[str, Any]:
    """Validate the completeness of extraction results"""
    try:
        validation = {
            "subjects_found": len(extraction_data.get("subjects", [])),
            "definitions_found": len([s for s in extraction_data.get("subjects", []) if s.get("definition")]),
            "rules_found": len([s for s in extraction_data.get("subjects", []) if s.get("rules")]),
            "object_properties_found": len(extraction_data.get("object_properties", [])),
            "data_properties_found": len(extraction_data.get("data_properties", [])),
            "domains_covered": list(set([d for s in extraction_data.get("subjects", []) for d in s.get("domains", [])])),
            "cardinality_specified": len([p for p in extraction_data.get("object_properties", []) + extraction_data.get("data_properties", []) if p.get("cardinality")]),
            "completeness_score": 0.0
        }
        
        # Calculate completeness score
        if validation["subjects_found"] > 0:
            definition_ratio = validation["definitions_found"] / validation["subjects_found"]
            domain_coverage = len(validation["domains_covered"]) / 7  # 7 domains max
            property_ratio = (validation["object_properties_found"] + validation["data_properties_found"]) / max(validation["subjects_found"], 1)
            cardinality_ratio = validation["cardinality_specified"] / max(validation["object_properties_found"] + validation["data_properties_found"], 1)
            
            validation["completeness_score"] = (definition_ratio * 0.4) + (domain_coverage * 0.3) + (property_ratio * 0.2) + (cardinality_ratio * 0.1)
        
        validation["is_complete"] = validation["completeness_score"] > 0.7
        return validation
    except Exception as e:
        return {"error": f"Validation failed: {str(e)}"}

# ====================================
# MEMORY SETUP WITH LANGMEM
# ====================================

def setup_memory_store():
    """Setup LangMem memory store for long-term memory"""
    try:
        # Create memory store with vector indexing
        store = InMemoryStore(
            index={
                "dims": 3072,  # text-embedding-3-large dimensions
                "embed": f"openai:{Config.OPENAI_EMBEDDING_MODEL}"
            }
        )
        logger.info("Memory store initialized with vector indexing")
        return store
    except Exception as e:
        logger.error(f"Failed to setup memory store: {e}")
        return InMemoryStore()

def create_memory_tools(namespace_params: Dict[str, str]):
    """Create memory management tools for agents"""
    try:
        namespace = ("legal_rules", namespace_params.get("jurisdiction", "default"), 
                    namespace_params.get("organization", "default"))
        
        manage_tool = create_manage_memory_tool(namespace=namespace)
        search_tool = create_search_memory_tool(namespace=namespace)
        
        return [manage_tool, search_tool]
    except Exception as e:
        logger.error(f"Failed to create memory tools: {e}")
        return []

# ====================================
# REACT AGENTS WITH LANGGRAPH
# ====================================

class DocumentProcessorAgent:
    """ReAct agent for document processing and text extraction"""
    
    def __init__(self, memory_store, openai_client: DirectOpenAIClient):
        self.memory_store = memory_store
        self.openai_client = openai_client
        self.llm = O3MiniLanguageModel(openai_client)
        
        # Define tools for this agent
        self.tools = [
            extract_text_from_pdf,
            analyze_document_structure
        ]
        
        # Create ReAct agent with LangGraph
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt="""You are a legal document processing expert. Your role is to extract and analyze text from legal documents line by line, identify their structure, and prepare them for rule extraction.

CRITICAL INSTRUCTIONS:
- Read the document line by line carefully
- Create an internal mental graph of legal concepts as you read
- Identify legal definitions, rules, obligations, permissions, and prohibitions
- Note relationships between different legal concepts
- Pay attention to cardinality constraints (one-to-one, one-to-many, etc.)
- Prepare comprehensive analysis for ontology building

Always be thorough and precise in your analysis. Your work will feed into the next agent for detailed rule extraction."""
        )
        
        logger.info("DocumentProcessorAgent initialized with ReAct architecture")
    
    async def process_document(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Process a legal document and extract structured information"""
        
        # Create message for the agent
        message = HumanMessage(content=f"""
        Process this legal document for rules extraction with line-by-line analysis:
        
        Document Path: {state['document_path']}
        Jurisdiction: {state['document_metadata'].get('jurisdiction', 'Unknown')}
        Country: {state['document_metadata'].get('country', 'Unknown')}
        Organization: {state['document_metadata'].get('organization', 'Unknown')}
        
        Tasks:
        1. Extract text from the PDF document line by line
        2. Analyze the document structure while building an internal graph
        3. Identify key legal concepts, definitions, and relationships
        4. Note potential cardinality constraints between concepts
        5. Prepare a comprehensive summary for rule extraction
        
        Remember to read carefully line by line and build your internal understanding as you go.
        Return a comprehensive analysis that will help the next agent extract detailed legal rules.
        """)
        
        # Invoke the ReAct agent
        result = await self.agent.ainvoke({"messages": [message]}, config)
        
        # Extract the response
        assistant_message = result["messages"][-1]
        
        # Update state
        state["messages"].extend(result["messages"])
        state["processing_status"] = "text_extracted"
        state["current_agent"] = "DocumentProcessor"
        
        # Try to extract actual text if agent provided instructions
        if state["document_path"] and os.path.exists(state["document_path"]):
            try:
                extracted_text = extract_text_from_pdf(state["document_path"])
                state["extracted_text"] = extracted_text
                logger.info(f"Extracted {len(extracted_text)} characters from document")
            except Exception as e:
                logger.error(f"Text extraction failed: {e}")
                state["extracted_text"] = "Text extraction failed"
        
        return state

class RuleExtractionAgent:
    """ReAct agent for extracting legal rules from text"""
    
    def __init__(self, memory_store, openai_client: DirectOpenAIClient):
        self.memory_store = memory_store
        self.openai_client = openai_client
        self.llm = O3MiniLanguageModel(openai_client)
        
        # Define tools for this agent
        self.tools = [
            validate_extraction_completeness
        ]
        
        # Create ReAct agent
        self.agent = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt="""You are a legal rules extraction expert specializing in converting legislation into machine-readable formats with line-by-line analysis.

CRITICAL INSTRUCTIONS FOR LINE-BY-LINE ANALYSIS:
- Read each line of the legal text carefully and systematically
- Build an internal graph of legal concepts as you process each line
- For each concept you encounter, determine if it's a subject, object property, or data property
- Create detailed definitions for EVERY extracted element
- Specify cardinality constraints (exactly one, at least one, zero or more, etc.)

Your task is to extract comprehensive legal knowledge from legal texts and structure it as Rules-as-Code with:

1. SUBJECTS - Legal entities/concepts with:
   - DEFINITION: Precise legal definition from the text (MANDATORY)
   - RULES: Legal obligations, permissions, prohibitions (OPTIONAL)
   - CONDITIONS: Applicability conditions (OPTIONAL)
   - DOMAINS: Data management domains (MANDATORY)
   - RELATIONSHIPS: Connections to other subjects
   - CARDINALITY: Specify constraints where applicable

2. OBJECT PROPERTIES - Relationships between subjects with:
   - NAME: Property name
   - DEFINITION: Clear definition of the relationship
   - DOMAIN: Subject class that has this property
   - RANGE: Subject class this property points to
   - CARDINALITY: Exactly one, at least one, zero or more, etc.
   - INVERSE_PROPERTY: If applicable

3. DATA PROPERTIES - Attributes with literal values with:
   - NAME: Property name  
   - DEFINITION: Clear definition of the attribute
   - DOMAIN: Subject class that has this property
   - RANGE: Data type (string, date, boolean, integer, etc.)
   - CARDINALITY: Exactly one, at least one, zero or more, etc.
   - CONSTRAINTS: Any value constraints

4. ADEQUACY DECISIONS - Cross-border transfer rules

Focus on these data management domains:
- STORAGE: retention, archiving, deletion
- USAGE: purpose limitation, processing activities  
- MOVEMENT: transfers, cross-border, adequacy
- PRIVACY: consent, transparency, rights
- SECURITY: encryption, access controls, breach notification
- ACCESS: authorization, authentication, audit
- ENTITLEMENTS: roles, permissions, data access rights

Read line by line, build your internal graph, and extract comprehensive structured knowledge."""
        )
        
        logger.info("RuleExtractionAgent initialized with ReAct architecture")
    
    async def extract_rules(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Extract legal rules from the processed text"""
        
        if not state.get("extracted_text"):
            state["extraction_results"] = {"error": "No extracted text available"}
            return state
        
        # Add memory tools to the agent for this specific jurisdiction
        memory_tools = create_memory_tools({
            "jurisdiction": state["document_metadata"].get("jurisdiction", "default"),
            "organization": state["document_metadata"].get("organization", "default")
        })
        
        # Create message for rule extraction with emphasis on line-by-line processing
        text_preview = state["extracted_text"][:8000] + "..." if len(state["extracted_text"]) > 8000 else state["extracted_text"]
        
        message = HumanMessage(content=f"""
        Extract comprehensive legal rules from this legal document using careful line-by-line analysis:
        
        CONTEXT:
        - Country: {state['document_metadata'].get('country', 'Unknown')}
        - Jurisdiction: {state['document_metadata'].get('jurisdiction', 'Unknown')}
        - Organization: {state['document_metadata'].get('organization', 'Unknown')}
        
        LEGAL TEXT:
        {text_preview}
        
        INSTRUCTIONS:
        1. Read each line carefully and build an internal graph of legal concepts
        2. Extract all legal subjects with comprehensive definitions
        3. Define object properties with proper cardinality constraints
        4. Define data properties with appropriate data types and cardinality
        5. Ensure every extracted element has a clear definition
        6. Specify domain classifications for all subjects
        
        Return structured JSON with subjects, object_properties, data_properties, and adequacy_decisions.
        Each element MUST have a definition and appropriate cardinality specification.
        """)
        
        # Invoke the ReAct agent
        result = await self.agent.ainvoke({"messages": [message]}, config)
        
        # Extract and parse the results
        assistant_message = result["messages"][-1].content
        
        # Try to parse JSON from the response
        try:
            # Look for JSON in the response
            json_start = assistant_message.find('{')
            json_end = assistant_message.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = assistant_message[json_start:json_end]
                extraction_results = json.loads(json_str)
            else:
                # Fallback: create basic structure
                extraction_results = {
                    "subjects": [],
                    "object_properties": [],
                    "data_properties": [],
                    "adequacy_decisions": [],
                    "raw_response": assistant_message
                }
        except json.JSONDecodeError:
            extraction_results = {
                "subjects": [],
                "object_properties": [],
                "data_properties": [],
                "adequacy_decisions": [],
                "raw_response": assistant_message,
                "parse_error": "Failed to parse JSON from agent response"
            }
        
        # Update state
        state["messages"].extend(result["messages"])
        state["extraction_results"] = extraction_results
        state["processing_status"] = "rules_extracted"
        state["current_agent"] = "RuleExtractor"
        
        logger.info(f"Extracted {len(extraction_results.get('subjects', []))} subjects")
        
        return state

class OntologyBuilderAgent:
    """ReAct agent for building ontologies from extracted rules"""
    
    def __init__(self, memory_store, openai_client: DirectOpenAIClient):
        self.memory_store = memory_store
        self.openai_client = openai_client
        self.ns = LegalRulesNamespaces()
        self.llm = O3MiniLanguageModel(openai_client)
        
        # Create ReAct agent for ontology building
        self.agent = create_react_agent(
            model=self.llm,
            tools=[validate_extraction_completeness],
            prompt="""You are an ontology engineering expert specializing in building comprehensive OWL ontologies and RDF knowledge graphs from extracted legal rules.

CRITICAL INSTRUCTIONS:
- Build formal ontologies with proper OWL semantics
- Ensure all classes, properties, and individuals have definitions
- Specify cardinality constraints using OWL constructs
- Create both TBox (ontology structure) and ABox (knowledge instances)
- Integrate with DPV (Data Privacy Vocabulary) and PROV-O where applicable
- Generate comprehensive TTL files for both ontology and knowledge graph

Your role is to transform the extracted legal rules into formal semantic structures with proper relationships, cardinality constraints, and comprehensive definitions."""
        )
        
        logger.info("OntologyBuilderAgent initialized")
    
    async def build_ontology(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Build ontology from extraction results"""
        
        if not state.get("extraction_results"):
            state["ontology_graph"] = None
            return state
        
        # Create RDF graphs
        owl_ontology = Graph()
        ttl_graph = Graph()
        
        # Bind namespaces
        owl_ontology = self.ns.bind_to_graph(owl_ontology)
        ttl_graph = self.ns.bind_to_graph(ttl_graph)
        
        # Build ontology structure
        self._build_ontology_structure(owl_ontology, state["document_metadata"])
        self._add_extracted_classes_and_properties(owl_ontology, state["extraction_results"])
        self._populate_knowledge_graph(ttl_graph, state["extraction_results"], state["document_metadata"])
        
        # Serialize graphs
        ontology_ttl = owl_ontology.serialize(format="turtle")
        knowledge_graph_ttl = ttl_graph.serialize(format="turtle")
        
        # Store serialized graphs in state
        state["ontology_graph"] = {
            "owl_ontology": ontology_ttl,
            "knowledge_graph": knowledge_graph_ttl,
            "owl_triples": len(owl_ontology),
            "kg_triples": len(ttl_graph)
        }
        
        state["processing_status"] = "ontology_built"
        state["current_agent"] = "OntologyBuilder"
        
        logger.info(f"Built ontology with {len(owl_ontology)} OWL triples and {len(ttl_graph)} KG triples")
        
        return state
    
    def _build_ontology_structure(self, graph: Graph, metadata: Dict):
        """Build the core ontology structure"""
        
        # Ontology metadata
        ontology_uri = URIRef(f"{self.ns.RAC}ontology")
        graph.add((ontology_uri, RDF.type, OWL.Ontology))
        graph.add((ontology_uri, DCTERMS.title, 
                  Literal(f"Rules-as-Code Ontology for {metadata.get('country', 'Legal Domain')}")))
        graph.add((ontology_uri, DCTERMS.created, 
                  Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
        graph.add((ontology_uri, RDFS.comment, 
                  Literal("Machine-readable legal rules ontology with comprehensive definitions and cardinality constraints")))
        
        # Import DPV and PROV-O
        graph.add((ontology_uri, OWL.imports, URIRef("https://w3id.org/dpv")))
        graph.add((ontology_uri, OWL.imports, URIRef("http://www.w3.org/ns/prov")))
        
        # Core classes
        core_classes = [
            ("LegalSubject", "A legal entity, concept, or subject with defined characteristics"),
            ("LegalRule", "A specific legal obligation, permission, or prohibition"),
            ("LegalDefinition", "A formal legal definition of a concept or subject"),
            ("DataManagementDomain", "A domain of data management activity"),
            ("AdequacyDecision", "A decision on adequacy for data transfers"),
            ("Country", "A nation state with legal jurisdiction"),
            ("Organization", "A legal organization or entity"),
            ("Jurisdiction", "A legal administrative area")
        ]
        
        for class_name, definition in core_classes:
            class_uri = URIRef(f"{self.ns.RAC}{class_name}")
            graph.add((class_uri, RDF.type, OWL.Class))
            graph.add((class_uri, RDFS.label, Literal(class_name)))
            graph.add((class_uri, RDFS.comment, Literal(definition)))
            graph.add((class_uri, SKOS.definition, Literal(definition)))
    
    def _add_extracted_classes_and_properties(self, graph: Graph, extraction_results: Dict):
        """Add extracted classes and properties with definitions and cardinality"""
        
        # Add extracted subjects as classes
        for subject in extraction_results.get("subjects", []):
            if subject.get("name"):
                class_name = self._safe_uri_encode(subject["name"])
                class_uri = URIRef(f"{self.ns.RAC}{class_name}")
                
                graph.add((class_uri, RDF.type, OWL.Class))
                graph.add((class_uri, RDFS.label, Literal(subject["name"])))
                graph.add((class_uri, RDFS.subClassOf, URIRef(f"{self.ns.RAC}LegalSubject")))
                
                if subject.get("definition"):
                    graph.add((class_uri, RDFS.comment, Literal(subject["definition"])))
                    graph.add((class_uri, SKOS.definition, Literal(subject["definition"])))
        
        # Add extracted object properties with cardinality
        for prop in extraction_results.get("object_properties", []):
            if prop.get("name"):
                prop_name = self._safe_uri_encode(prop["name"])
                prop_uri = URIRef(f"{self.ns.PROPERTIES}{prop_name}")
                
                graph.add((prop_uri, RDF.type, OWL.ObjectProperty))
                graph.add((prop_uri, RDFS.label, Literal(prop["name"])))
                
                if prop.get("definition"):
                    graph.add((prop_uri, RDFS.comment, Literal(prop["definition"])))
                    graph.add((prop_uri, SKOS.definition, Literal(prop["definition"])))
                
                # Add domain and range
                if prop.get("domain"):
                    domain_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['domain'])}")
                    graph.add((prop_uri, RDFS.domain, domain_uri))
                
                if prop.get("range"):
                    range_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['range'])}")
                    graph.add((prop_uri, RDFS.range, range_uri))
                
                # Add cardinality constraints
                if prop.get("cardinality"):
                    self._add_cardinality_constraint(graph, prop_uri, prop["cardinality"], prop.get("domain"))
                
                # Add inverse property
                if prop.get("inverse_property"):
                    inverse_uri = URIRef(f"{self.ns.PROPERTIES}{self._safe_uri_encode(prop['inverse_property'])}")
                    graph.add((prop_uri, OWL.inverseOf, inverse_uri))
        
        # Add extracted data properties with cardinality
        for prop in extraction_results.get("data_properties", []):
            if prop.get("name"):
                prop_name = self._safe_uri_encode(prop["name"])
                prop_uri = URIRef(f"{self.ns.PROPERTIES}{prop_name}")
                
                graph.add((prop_uri, RDF.type, OWL.DatatypeProperty))
                graph.add((prop_uri, RDFS.label, Literal(prop["name"])))
                
                if prop.get("definition"):
                    graph.add((prop_uri, RDFS.comment, Literal(prop["definition"])))
                    graph.add((prop_uri, SKOS.definition, Literal(prop["definition"])))
                
                # Add domain
                if prop.get("domain"):
                    domain_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(prop['domain'])}")
                    graph.add((prop_uri, RDFS.domain, domain_uri))
                
                # Add range (datatype)
                if prop.get("range"):
                    range_type = prop["range"]
                    if range_type.startswith("xsd:"):
                        range_uri = getattr(XSD, range_type.split(":")[1])
                        graph.add((prop_uri, RDFS.range, range_uri))
                    else:
                        graph.add((prop_uri, RDFS.range, XSD.string))
                
                # Add cardinality constraints
                if prop.get("cardinality"):
                    self._add_cardinality_constraint(graph, prop_uri, prop["cardinality"], prop.get("domain"))
    
    def _add_cardinality_constraint(self, graph: Graph, property_uri: URIRef, cardinality: str, domain: str):
        """Add cardinality constraints to properties"""
        
        if not domain:
            return
        
        domain_uri = URIRef(f"{self.ns.RAC}{self._safe_uri_encode(domain)}")
        restriction_uri = BNode()
        
        graph.add((restriction_uri, RDF.type, OWL.Restriction))
        graph.add((restriction_uri, OWL.onProperty, property_uri))
        
        # Parse cardinality specification
        cardinality_lower = cardinality.lower()
        
        if "exactly one" in cardinality_lower or "exactly 1" in cardinality_lower:
            graph.add((restriction_uri, OWL.cardinality, Literal(1, datatype=XSD.nonNegativeInteger)))
        elif "at least one" in cardinality_lower or "one or more" in cardinality_lower:
            graph.add((restriction_uri, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))
        elif "at most one" in cardinality_lower or "zero or one" in cardinality_lower:
            graph.add((restriction_uri, OWL.maxCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))
        elif "zero or more" in cardinality_lower:
            graph.add((restriction_uri, OWL.minCardinality, Literal(0, datatype=XSD.nonNegativeInteger)))
        
        # Add restriction to domain class
        graph.add((domain_uri, RDFS.subClassOf, restriction_uri))
    
    def _populate_knowledge_graph(self, graph: Graph, extraction_results: Dict, metadata: Dict):
        """Populate knowledge graph with extracted data"""
        
        # Add country, jurisdiction, organization as individuals
        country_uri = URIRef(f"{self.ns.RAC}Country_{self._safe_uri_encode(metadata['country'])}")
        graph.add((country_uri, RDF.type, URIRef(f"{self.ns.RAC}Country")))
        graph.add((country_uri, RDFS.label, Literal(metadata['country'])))
        
        jurisdiction_uri = URIRef(f"{self.ns.RAC}Jurisdiction_{self._safe_uri_encode(metadata['jurisdiction'])}")
        graph.add((jurisdiction_uri, RDF.type, URIRef(f"{self.ns.RAC}Jurisdiction")))
        graph.add((jurisdiction_uri, RDFS.label, Literal(metadata['jurisdiction'])))
        
        org_uri = URIRef(f"{self.ns.RAC}Organization_{self._safe_uri_encode(metadata['organization'])}")
        graph.add((org_uri, RDF.type, URIRef(f"{self.ns.RAC}Organization")))
        graph.add((org_uri, RDFS.label, Literal(metadata['organization'])))
        
        # Add extracted subjects as individuals
        for subject in extraction_results.get("subjects", []):
            self._add_subject_individual(graph, subject, country_uri)
        
        # Add adequacy decisions
        for adequacy in extraction_results.get("adequacy_decisions", []):
            self._add_adequacy_decision(graph, adequacy, country_uri)
    
    def _add_subject_individual(self, graph: Graph, subject: Dict, country_uri: URIRef):
        """Add a legal subject as an individual"""
        
        subject_name = subject.get("name", "")
        if not subject_name:
            return
        
        uri_suffix = self._safe_uri_encode(subject_name)
        
        # Add as individual instance
        individual_uri = URIRef(f"{self.ns.RAC}{uri_suffix}_Instance")
        class_uri = URIRef(f"{self.ns.RAC}{uri_suffix}")
        
        graph.add((individual_uri, RDF.type, class_uri))
        graph.add((individual_uri, RDFS.label, Literal(f"{subject_name} Instance")))
        
        # Link to country
        graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}appliesInCountry"), country_uri))
        
        # Add domain classifications
        for domain in subject.get("domains", []):
            domain_uri = URIRef(f"{self.ns.RAC}{domain.title()}Domain")
            graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}belongsToDomain"), domain_uri))
        
        # Add definition as literal property
        if subject.get("definition"):
            graph.add((individual_uri, URIRef(f"{self.ns.PROPERTIES}hasDefinition"), Literal(subject["definition"])))
        
        # Add rules if present
        for i, rule in enumerate(subject.get("rules", [])):
            rule_uri = URIRef(f"{self.ns.RAC}Rule_{uri_suffix}_{i}")
            graph.add((rule_uri, RDF.type, URIRef(f"{self.ns.RAC}LegalRule")))
            graph.add((rule_uri, RDFS.label, Literal(f"Rule for {subject_name} #{i+1}")))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasRuleType"), Literal(rule.get("type", "obligation"))))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}hasRuleDescription"), Literal(rule.get("description", ""))))
            graph.add((rule_uri, URIRef(f"{self.ns.PROPERTIES}appliesTo"), individual_uri))
        
        # Add provenance
        graph.add((individual_uri, self.ns.PROV.wasGeneratedBy, URIRef(f"{self.ns.RAC}RulesAsCodeExtraction")))
        graph.add((individual_uri, self.ns.PROV.generatedAtTime, Literal(datetime.now().isoformat(), datatype=XSD.dateTime)))
    
    def _add_adequacy_decision(self, graph: Graph, adequacy: Dict, country_uri: URIRef):
        """Add adequacy decision as individual"""
        
        country_name = adequacy.get("country", "").replace(" ", "_")
        adequacy_uri = URIRef(f"{self.ns.RAC}AdequacyDecision_{country_name}")
        
        graph.add((adequacy_uri, RDF.type, URIRef(f"{self.ns.RAC}AdequacyDecision")))
        graph.add((adequacy_uri, RDFS.label, Literal(f"Adequacy Decision for {adequacy.get('country', 'Unknown')}")))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasAdequacyStatus"), Literal(adequacy.get("status", "unknown"))))
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}forCountry"), Literal(adequacy.get("country", "Unknown"))))
        
        if adequacy.get("decision_date"):
            graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasDecisionDate"), 
                      Literal(adequacy["decision_date"], datatype=XSD.date)))
        
        if adequacy.get("scope"):
            graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}hasScope"), Literal(adequacy["scope"])))
        
        # Link to source country
        graph.add((adequacy_uri, URIRef(f"{self.ns.PROPERTIES}decidedBy"), country_uri))
    
    def _safe_uri_encode(self, text: str) -> str:
        """Safely encode text for use in URIs"""
        import urllib.parse
        safe_text = text.replace(' ', '_').replace('/', '_').replace('\\', '_')
        return urllib.parse.quote(safe_text, safe='')

class MemoryManagerAgent:
    """ReAct agent for managing long-term memory with LangMem"""
    
    def __init__(self, memory_store, openai_client: DirectOpenAIClient):
        self.memory_store = memory_store
        self.openai_client = openai_client
        self.llm = O3MiniLanguageModel(openai_client)
        
        logger.info("MemoryManagerAgent initialized")
    
    async def store_memories(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Store extraction results in long-term memory"""
        
        try:
            # Get memory tools for this jurisdiction
            jurisdiction = state["document_metadata"].get("jurisdiction", "default")
            organization = state["document_metadata"].get("organization", "default")
            
            memory_tools = create_memory_tools({
                "jurisdiction": jurisdiction,
                "organization": organization
            })
            
            if memory_tools and state.get("extraction_results"):
                # Create memory manager for this specific case
                memory_manager = create_memory_manager(
                    f"openai:{Config.OPENAI_MODEL}",
                    namespace=("legal_rules", jurisdiction, organization),
                    instructions=f"""Store important legal concepts and rules from {jurisdiction} legislation.
                    Focus on subjects with definitions, legal rules, property definitions with cardinality, and cross-jurisdictional relationships.""",
                    store=self.memory_store,
                    enable_inserts=True,
                    enable_updates=True
                )
                
                # Prepare memory data
                extraction_summary = {
                    "subjects_count": len(state["extraction_results"].get("subjects", [])),
                    "object_properties_count": len(state["extraction_results"].get("object_properties", [])),
                    "data_properties_count": len(state["extraction_results"].get("data_properties", [])),
                    "adequacy_decisions": state["extraction_results"].get("adequacy_decisions", []),
                    "jurisdiction": jurisdiction,
                    "country": state["document_metadata"].get("country", ""),
                    "processing_date": datetime.now().isoformat()
                }
                
                # Store in memory
                memory_input = {
                    "messages": [
                        HumanMessage(content=f"Store legal knowledge from {jurisdiction}: {json.dumps(extraction_summary)}")
                    ]
                }
                
                config_with_namespace = {
                    "configurable": {
                        "jurisdiction": jurisdiction,
                        "organization": organization
                    }
                }
                
                await memory_manager.ainvoke(memory_input, config=config_with_namespace)
                
                state["memory_stored"] = True
                logger.info(f"Stored memories for {jurisdiction}")
            
        except Exception as e:
            logger.error(f"Failed to store memories: {e}")
            state["memory_stored"] = False
        
        state["processing_status"] = "memory_stored"
        state["current_agent"] = "MemoryManager"
        
        return state

# ====================================
# MULTI-AGENT ORCHESTRATOR WITH LANGGRAPH
# ====================================

class RulesAsCodeOrchestrator:
    """Main orchestrator using LangGraph for multi-agent coordination"""
    
    def __init__(self):
        # Validate configuration
        Config.validate_config()
        
        # Initialize OpenAI client
        self.openai_client = DirectOpenAIClient()
        
        # Setup memory store
        self.memory_store = setup_memory_store()
        
        # Initialize checkpointer for conversation memory
        self.checkpointer = MemorySaver()
        
        # Initialize agents
        self.doc_processor = DocumentProcessorAgent(self.memory_store, self.openai_client)
        self.rule_extractor = RuleExtractionAgent(self.memory_store, self.openai_client)
        self.ontology_builder = OntologyBuilderAgent(self.memory_store, self.openai_client)
        self.memory_manager = MemoryManagerAgent(self.memory_store, self.openai_client)
        
        # Build the agent graph
        self.graph = self._build_agent_graph()
        
        logger.info("RulesAsCodeOrchestrator initialized with multi-agent architecture")
    
    def _build_agent_graph(self) -> StateGraph:
        """Build the LangGraph state graph for agent coordination"""
        
        # Create the state graph
        workflow = StateGraph(DocumentProcessingState)
        
        # Add agent nodes
        workflow.add_node("document_processor", self._document_processor_node)
        workflow.add_node("rule_extractor", self._rule_extractor_node)
        workflow.add_node("ontology_builder", self._ontology_builder_node)
        workflow.add_node("memory_manager", self._memory_manager_node)
        workflow.add_node("validator", self._validator_node)
        workflow.add_node("exporter", self._exporter_node)
        
        # Define the workflow edges
        workflow.add_edge(START, "document_processor")
        workflow.add_edge("document_processor", "rule_extractor")
        workflow.add_edge("rule_extractor", "ontology_builder")
        workflow.add_edge("ontology_builder", "memory_manager")
        workflow.add_edge("memory_manager", "validator")
        workflow.add_edge("validator", "exporter")
        workflow.add_edge("exporter", END)
        
        # Compile the graph with checkpointer
        compiled_graph = workflow.compile(checkpointer=self.checkpointer)
        
        logger.info("Agent workflow graph compiled successfully")
        return compiled_graph
    
    async def _document_processor_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Document processor node"""
        logger.info("Executing document processor agent")
        return await self.doc_processor.process_document(state, config)
    
    async def _rule_extractor_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Rule extractor node"""
        logger.info("Executing rule extraction agent")
        return await self.rule_extractor.extract_rules(state, config)
    
    async def _ontology_builder_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Ontology builder node"""
        logger.info("Executing ontology builder agent")
        return await self.ontology_builder.build_ontology(state, config)
    
    async def _memory_manager_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Memory manager node"""
        logger.info("Executing memory manager agent")
        return await self.memory_manager.store_memories(state, config)
    
    async def _validator_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Validation node"""
        logger.info("Executing validation")
        
        if state.get("extraction_results"):
            validation = validate_extraction_completeness(state["extraction_results"])
            state["validation_results"] = validation
            
            # Add validation message
            validation_msg = AIMessage(content=f"Validation completed. Completeness score: {validation.get('completeness_score', 0):.2f}")
            state["messages"].append(validation_msg)
        
        state["processing_status"] = "validated"
        return state
    
    async def _exporter_node(self, state: DocumentProcessingState, config: RunnableConfig) -> DocumentProcessingState:
        """Export node"""
        logger.info("Executing export")
        
        exports = {}
        
        if state.get("ontology_graph"):
            # Create output directory
            country = state["document_metadata"].get("country", "Unknown").replace(" ", "_")
            output_dir = Path(Config.OUTPUT_PATH) / country
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Export ontology TTL
            owl_path = output_dir / f"ontology_{country}.ttl"
            with open(owl_path, "w", encoding="utf-8") as f:
                f.write(state["ontology_graph"]["owl_ontology"])
            exports["owl_ontology_ttl"] = str(owl_path)
            
            # Export knowledge graph TTL
            kg_path = output_dir / f"knowledge_graph_{country}.ttl"
            with open(kg_path, "w", encoding="utf-8") as f:
                f.write(state["ontology_graph"]["knowledge_graph"])
            exports["knowledge_graph_ttl"] = str(kg_path)
            
            # Export extraction results as JSON
            json_path = output_dir / f"extraction_results_{country}.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(state["extraction_results"], f, indent=2, ensure_ascii=False)
            exports["extraction_json"] = str(json_path)
            
            # Export additional formats
            try:
                # Parse and re-export ontology in different formats
                from rdflib import Graph
                
                # OWL/XML format
                owl_graph = Graph()
                owl_graph.parse(data=state["ontology_graph"]["owl_ontology"], format="turtle")
                
                xml_path = output_dir / f"ontology_{country}.owl"
                with open(xml_path, "w", encoding="utf-8") as f:
                    f.write(owl_graph.serialize(format="xml"))
                exports["owl_ontology_xml"] = str(xml_path)
                
                # JSON-LD format  
                jsonld_path = output_dir / f"ontology_{country}.jsonld"
                with open(jsonld_path, "w", encoding="utf-8") as f:
                    f.write(owl_graph.serialize(format="json-ld"))
                exports["owl_ontology_jsonld"] = str(jsonld_path)
                
            except Exception as e:
                logger.warning(f"Failed to export additional formats: {e}")
        
        state["exports"] = exports
        state["processing_status"] = "completed"
        
        # Add completion message
        completion_msg = AIMessage(content=f"Processing completed. Exported {len(exports)} files.")
        state["messages"].append(completion_msg)
        
        return state
    
    async def process_document(self, document_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process a legal document through the agent workflow"""
        
        logger.info(f"Starting document processing: {document_path}")
        
        # Initialize state
        initial_state = DocumentProcessingState(
            messages=[],
            document_path=document_path,
            document_metadata=metadata,
            extracted_text=None,
            processing_status="initialized",
            current_agent="",
            extraction_results=None,
            ontology_graph=None,
            validation_results=None,
            memory_stored=False,
            exports=None,
            reasoning_log=[]
        )
        
        # Create config with thread ID for conversation memory
        config = RunnableConfig(configurable={"thread_id": str(uuid.uuid4())})
        
        try:
            # Execute the workflow
            final_state = await self.graph.ainvoke(initial_state, config)
            
            # Prepare results summary
            results = {
                "success": True,
                "processing_status": final_state.get("processing_status", "unknown"),
                "extraction_stats": self._calculate_extraction_stats(final_state),
                "ontology_stats": self._calculate_ontology_stats(final_state),
                "validation_results": final_state.get("validation_results", {}),
                "exports": final_state.get("exports", {}),
                "memory_stored": final_state.get("memory_stored", False),
                "message_count": len(final_state.get("messages", [])),
                "reasoning_log": final_state.get("reasoning_log", [])
            }
            
            logger.info(f"Document processing completed successfully")
            return results
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_status": "failed"
            }
    
    def _calculate_extraction_stats(self, state: DocumentProcessingState) -> Dict[str, Any]:
        """Calculate extraction statistics"""
        
        extraction_results = state.get("extraction_results", {})
        subjects = extraction_results.get("subjects", [])
        
        return {
            "subjects_extracted": len(subjects),
            "subjects_with_definitions": len([s for s in subjects if s.get("definition")]),
            "subjects_with_rules": len([s for s in subjects if s.get("rules")]),
            "object_properties": len(extraction_results.get("object_properties", [])),
            "data_properties": len(extraction_results.get("data_properties", [])),
            "adequacy_decisions": len(extraction_results.get("adequacy_decisions", [])),
            "domains_covered": list(set([d for s in subjects for d in s.get("domains", [])])),
            "properties_with_cardinality": len([p for p in extraction_results.get("object_properties", []) + extraction_results.get("data_properties", []) if p.get("cardinality")])
        }
    
    def _calculate_ontology_stats(self, state: DocumentProcessingState) -> Dict[str, Any]:
        """Calculate ontology statistics"""
        
        ontology_graph = state.get("ontology_graph", {})
        
        return {
            "owl_triples": ontology_graph.get("owl_triples", 0),
            "kg_triples": ontology_graph.get("kg_triples", 0),
            "ontology_generated": ontology_graph is not None
        }

# ====================================
# CONFIGURATION MANAGER
# ====================================

class ConfigurationManager:
    """Manage document configuration and processing queue"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            required_fields = ['documents']
            for field in required_fields:
                if field not in config:
                    raise ValueError(f"Missing required field: {field}")
            
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            raise
    
    @staticmethod
    def create_sample_config() -> Dict:
        """Create a sample configuration file"""
        return {
            "documents": [
                {
                    "country": "European Union",
                    "jurisdiction": "EU",
                    "organization": "European Commission", 
                    "legal_system": "Civil Law",
                    "pdf_document": "./documents/gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United States",
                    "jurisdiction": "California",
                    "organization": "California Consumer Privacy Act",
                    "legal_system": "Common Law", 
                    "pdf_document": "./documents/ccpa.pdf",
                    "adequacy_focus": False,
                    "data_management_domains": ["privacy", "security", "access", "entitlements"]
                },
                {
                    "country": "United Kingdom",
                    "jurisdiction": "UK",
                    "organization": "Information Commissioner's Office",
                    "legal_system": "Common Law",
                    "pdf_document": "./documents/uk_gdpr.pdf",
                    "adequacy_focus": True,
                    "data_management_domains": ["storage", "usage", "movement", "privacy", "security", "access", "entitlements"]
                }
            ],
            "processing_options": {
                "enable_memory": True,
                "line_by_line_analysis": True,
                "internal_graph_reasoning": True,
                "reasoning_effort": "high",
                "generate_competency_questions": True,
                "comprehensive_definitions": True,
                "cardinality_constraints": True,
                "export_formats": ["ttl", "jsonld", "xml", "owl"]
            },
            "adequacy_countries": [
                "Andorra", "Argentina", "Canada", "Faroe Islands", "Guernsey", "Israel",
                "Isle of Man", "Japan", "Jersey", "New Zealand", "South Korea", 
                "Switzerland", "United Kingdom", "Uruguay"
            ]
        }

# ====================================
# MAIN APPLICATION
# ====================================

async def main():
    """Main application for Rules-as-Code with LangGraph agents"""
    
    load_dotenv()
    
    logger.info("🏛️ Starting Enhanced Rules-as-Code System with LangGraph Agents and Line-by-Line Analysis")
    
    # Validate configuration
    try:
        Config.validate_config()
        logger.info("✅ Configuration validation passed")
    except ValueError as e:
        logger.error(f"❌ Configuration error: {e}")
        return
    
    # Create sample configuration if needed
    if not os.path.exists(Config.CONFIG_PATH):
        logger.info("📝 Creating sample configuration file")
        sample_config = ConfigurationManager.create_sample_config()
        with open(Config.CONFIG_PATH, 'w') as f:
            json.dump(sample_config, f, indent=2)
        logger.info("Please update config.json with your documents and run again")
        return
    
    # Load configuration
    config = ConfigurationManager.load_config(Config.CONFIG_PATH)
    
    # Initialize orchestrator
    logger.info("🔧 Initializing LangGraph orchestrator with ReAct agents...")
    orchestrator = RulesAsCodeOrchestrator()
    
    # Process documents
    results = []
    total_docs = len(config['documents'])
    
    for i, doc_config in enumerate(config['documents'], 1):
        logger.info(f"📖 Processing document {i}/{total_docs}: {doc_config['country']}")
        
        if not os.path.exists(doc_config['pdf_document']):
            logger.warning(f"⚠️ Document not found: {doc_config['pdf_document']}")
            continue
        
        try:
            result = await orchestrator.process_document(
                doc_config['pdf_document'],
                doc_config
            )
            
            results.append({
                "country": doc_config['country'],
                "jurisdiction": doc_config['jurisdiction'],
                "result": result
            })
            
            if result['success']:
                logger.info(f"✅ Successfully processed {doc_config['country']}")
                logger.info(f"   📊 Subjects extracted: {result['extraction_stats']['subjects_extracted']}")
                logger.info(f"   🔗 Object properties: {result['extraction_stats']['object_properties']}")
                logger.info(f"   📝 Data properties: {result['extraction_stats']['data_properties']}")
                logger.info(f"   ⚖️ Properties with cardinality: {result['extraction_stats']['properties_with_cardinality']}")
                logger.info(f"   🏷️ Domains covered: {', '.join(result['extraction_stats']['domains_covered'])}")
                logger.info(f"   💾 Memory stored: {result['memory_stored']}")
            else:
                logger.error(f"❌ Failed to process {doc_config['country']}: {result.get('error', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f"💥 Processing failed for {doc_config['country']}: {e}")
            results.append({
                "country": doc_config['country'],
                "result": {"success": False, "error": str(e)}
            })
    
    # Generate summary
    successful = sum(1 for r in results if r['result']['success'])
    total_subjects = sum(r['result'].get('extraction_stats', {}).get('subjects_extracted', 0) 
                        for r in results if r['result']['success'])
    total_properties = sum(
        r['result'].get('extraction_stats', {}).get('object_properties', 0) + 
        r['result'].get('extraction_stats', {}).get('data_properties', 0)
        for r in results if r['result']['success']
    )
    
    logger.info("🎉 LangGraph Rules-as-Code processing complete!")
    logger.info(f"   📈 Documents processed: {successful}/{total_docs}")
    logger.info(f"   ⚖️ Total subjects extracted: {total_subjects}")
    logger.info(f"   🔗 Total properties extracted: {total_properties}")
    
    # Save results
    summary_path = Path(Config.OUTPUT_PATH) / "langgraph_rules_as_code_summary.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    
    summary = {
        "processing_summary": {
            "timestamp": datetime.now().isoformat(),
            "documents_processed": successful,
            "total_documents": total_docs,
            "total_subjects_extracted": total_subjects,
            "total_properties_extracted": total_properties,
            "system_version": "rules-as-code-langgraph-v1.0-line-by-line",
            "agents_used": ["DocumentProcessor", "RuleExtractor", "OntologyBuilder", "MemoryManager"],
            "features": ["line-by-line analysis", "internal graph reasoning", "cardinality constraints", "comprehensive definitions"]
        },
        "results": results
    }
    
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    logger.info(f"📄 Summary saved to: {summary_path}")
    logger.info("✨ LangGraph Rules-as-Code processing completed!")

if __name__ == "__main__":
    asyncio.run(main())
