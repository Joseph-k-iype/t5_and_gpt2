# backend/app/utils/knowledge_extractor.py
import re
import asyncio
from typing import Dict, List, Any, Set, Tuple, Optional, Union
import logging
from collections import defaultdict, Counter
import hashlib

logger = logging.getLogger(__name__)

class KnowledgeExtractor:
    """
    Extract knowledge graphs from text using NLP techniques
    """
    
    def __init__(self):
        # Common stop words and noise words to filter out
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you',
            'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'
        }
        
        # Legal/privacy domain specific terms that are important
        self.domain_terms = {
            'gdpr', 'ccpa', 'lgpd', 'privacy', 'data protection', 'consent',
            'personal data', 'processing', 'controller', 'processor', 'breach',
            'notification', 'rights', 'deletion', 'portability', 'rectification',
            'restriction', 'objection', 'automated decision', 'profiling',
            'lawful basis', 'legitimate interest', 'compliance', 'regulation',
            'directive', 'framework', 'jurisdiction', 'cross-border', 'transfer',
            'adequacy', 'safeguards', 'binding corporate rules', 'standard contractual clauses'
        }
    
    async def extract_knowledge_graph(self, text: str, context: Optional[Union[Dict[str, Any], str]] = None,
                                    max_nodes: int = 25, max_edges: int = 50) -> Dict[str, Any]:
        """
        Extract knowledge graph from text
        
        Args:
            text: Text to analyze
            context: Additional context (can be dict, string, or None)
            max_nodes: Maximum number of nodes to include
            max_edges: Maximum number of edges to include
        """
        try:
            # Clean and preprocess text
            cleaned_text = self._clean_text(text)
            
            # Process context if provided
            context_info = self._process_context(context)
            
            # Extract entities and concepts
            entities = await self._extract_entities(cleaned_text)
            
            # Extract relationships
            relationships = await self._extract_relationships(cleaned_text, entities)
            
            # Build graph structure
            nodes, edges = self._build_graph(entities, relationships, max_nodes, max_edges)
            
            # Calculate confidence based on extraction quality
            confidence = self._calculate_confidence(nodes, edges, len(cleaned_text))
            
            return {
                "nodes": nodes,
                "edges": edges,
                "confidence": confidence,
                "context_info": context_info,
                "extraction_stats": {
                    "original_entities": len(entities),
                    "filtered_nodes": len(nodes),
                    "potential_relationships": len(relationships),
                    "final_edges": len(edges)
                }
            }
            
        except Exception as e:
            logger.error(f"Error extracting knowledge graph: {e}")
            return {
                "nodes": [],
                "edges": [],
                "confidence": 0.0,
                "error": str(e)
            }
    
    def _process_context(self, context: Optional[Union[Dict[str, Any], str]]) -> Dict[str, Any]:
        """Process context parameter into a standardized format"""
        if context is None:
            return {}
        elif isinstance(context, str):
            return {"type": "string", "value": context}
        elif isinstance(context, dict):
            return context
        else:
            return {"type": "unknown", "value": str(context)}
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep important punctuation
        text = re.sub(r'[^\w\s\-\.\,\;\:\(\)\/]', ' ', text)
        
        # Normalize whitespace again
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract entities and concepts from text
        """
        entities = []
        
        # Extract capitalized phrases (likely proper nouns/entities)
        capitalized_patterns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        
        # Extract acronyms
        acronym_patterns = re.findall(r'\b[A-Z]{2,}\b', text)
        
        # Extract domain-specific terms
        domain_matches = []
        text_lower = text.lower()
        for term in self.domain_terms:
            if term in text_lower:
                domain_matches.append(term.title())
        
        # Extract quoted terms (likely important concepts)
        quoted_terms = re.findall(r'"([^"]+)"', text)
        quoted_terms.extend(re.findall(r"'([^']+)'", text))
        
        # Combine and score entities
        all_candidates = (
            [(term, 'proper_noun', 0.8) for term in capitalized_patterns] +
            [(term, 'acronym', 0.9) for term in acronym_patterns] +
            [(term, 'domain_term', 1.0) for term in domain_matches] +
            [(term, 'quoted_term', 0.7) for term in quoted_terms]
        )
        
        # Count frequency and filter
        entity_counts = Counter([candidate[0].lower() for candidate in all_candidates])
        
        for term, entity_type, base_score in all_candidates:
            if len(term) < 2 or term.lower() in self.stop_words:
                continue
            
            frequency = entity_counts[term.lower()]
            # Boost score based on frequency
            score = min(base_score + (frequency - 1) * 0.1, 1.0)
            
            entities.append({
                'text': term,
                'type': entity_type,
                'score': score,
                'frequency': frequency
            })
        
        # Remove duplicates and sort by score
        seen = set()
        unique_entities = []
        for entity in sorted(entities, key=lambda x: x['score'], reverse=True):
            key = entity['text'].lower()
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities[:50]  # Limit to top 50 entities
    
    async def _extract_relationships(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract relationships between entities
        """
        relationships = []
        entity_texts = [e['text'] for e in entities]
        
        # Simple co-occurrence based relationships
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
            
            # Find entities in this sentence
            sentence_entities = []
            for entity in entity_texts:
                if entity.lower() in sentence.lower():
                    sentence_entities.append(entity)
            
            # Create relationships between co-occurring entities
            for i, entity1 in enumerate(sentence_entities):
                for entity2 in sentence_entities[i+1:]:
                    relationship_type = self._determine_relationship_type(sentence, entity1, entity2)
                    strength = self._calculate_relationship_strength(sentence, entity1, entity2)
                    
                    relationships.append({
                        'source': entity1,
                        'target': entity2,
                        'type': relationship_type,
                        'strength': strength,
                        'context': sentence[:100] + '...' if len(sentence) > 100 else sentence
                    })
        
        return relationships
    
    def _determine_relationship_type(self, sentence: str, entity1: str, entity2: str) -> str:
        """
        Determine the type of relationship between entities based on context
        """
        sentence_lower = sentence.lower()
        
        # Look for specific relationship indicators
        if any(word in sentence_lower for word in ['requires', 'mandates', 'must', 'shall']):
            return 'requires'
        elif any(word in sentence_lower for word in ['implements', 'provides', 'ensures']):
            return 'implements'
        elif any(word in sentence_lower for word in ['under', 'according to', 'pursuant to']):
            return 'governed_by'
        elif any(word in sentence_lower for word in ['applies to', 'covers', 'includes']):
            return 'applies_to'
        elif any(word in sentence_lower for word in ['differs from', 'unlike', 'whereas']):
            return 'differs_from'
        elif any(word in sentence_lower for word in ['similar to', 'like', 'comparable']):
            return 'similar_to'
        else:
            return 'related_to'
    
    def _calculate_relationship_strength(self, sentence: str, entity1: str, entity2: str) -> float:
        """
        Calculate the strength of relationship between entities
        """
        # Distance between entities in the sentence
        pos1 = sentence.lower().find(entity1.lower())
        pos2 = sentence.lower().find(entity2.lower())
        
        if pos1 == -1 or pos2 == -1:
            return 0.1
        
        distance = abs(pos1 - pos2)
        
        # Closer entities have stronger relationships
        if distance < 20:
            return 1.0
        elif distance < 50:
            return 0.8
        elif distance < 100:
            return 0.6
        else:
            return 0.3
    
    def _build_graph(self, entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]],
                    max_nodes: int, max_edges: int) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        Build the final graph structure
        """
        # Sort entities by score and take top nodes
        top_entities = sorted(entities, key=lambda x: x['score'], reverse=True)[:max_nodes]
        entity_names = {e['text'] for e in top_entities}
        
        # Create nodes
        nodes = []
        for i, entity in enumerate(top_entities):
            node_id = f"node_{i}"
            color = self._get_node_color(entity['type'])
            size = max(15, min(50, int(entity['score'] * 40 + 10)))
            
            nodes.append({
                'id': node_id,
                'label': entity['text'],
                'type': entity['type'],
                'properties': {
                    'score': entity['score'],
                    'frequency': entity['frequency'],
                    'original_text': entity['text']
                },
                'size': size,
                'color': color
            })
        
        # Create mapping from entity text to node ID
        text_to_id = {entity['text']: f"node_{i}" for i, entity in enumerate(top_entities)}
        
        # Filter relationships to only include entities in our node set
        valid_relationships = [
            rel for rel in relationships
            if rel['source'] in entity_names and rel['target'] in entity_names
        ]
        
        # Sort by strength and take top edges
        top_relationships = sorted(valid_relationships, key=lambda x: x['strength'], reverse=True)[:max_edges]
        
        # Create edges
        edges = []
        for i, rel in enumerate(top_relationships):
            edge_id = f"edge_{i}"
            source_id = text_to_id[rel['source']]
            target_id = text_to_id[rel['target']]
            
            edges.append({
                'id': edge_id,
                'source': source_id,
                'target': target_id,
                'label': rel['type'].replace('_', ' ').title(),
                'type': rel['type'],
                'properties': {
                    'strength': rel['strength'],
                    'context': rel['context']
                },
                'weight': rel['strength']
            })
        
        return nodes, edges
    
    def _get_node_color(self, node_type: str) -> str:
        """
        Get color for node based on type
        """
        color_map = {
            'domain_term': '#E74C3C',     # Red for domain-specific terms
            'proper_noun': '#3498DB',     # Blue for proper nouns
            'acronym': '#9B59B6',         # Purple for acronyms
            'quoted_term': '#F39C12',     # Orange for quoted terms
            'concept': '#2ECC71',         # Green for general concepts
        }
        return color_map.get(node_type, '#95A5A6')  # Default gray
    
    def _calculate_confidence(self, nodes: List[Dict], edges: List[Dict], text_length: int) -> float:
        """
        Calculate confidence score for the extracted graph
        """
        if not nodes:
            return 0.0
        
        # Base confidence on number of nodes and edges
        node_score = min(len(nodes) / 20, 1.0)  # Optimal around 20 nodes
        edge_score = min(len(edges) / 30, 1.0)  # Optimal around 30 edges
        
        # Boost confidence for longer, more detailed text
        text_score = min(text_length / 5000, 1.0)  # Optimal around 5000 characters
        
        # Average domain term ratio
        domain_nodes = sum(1 for node in nodes if node.get('type') == 'domain_term')
        domain_ratio = domain_nodes / len(nodes) if nodes else 0
        domain_score = min(domain_ratio * 2, 1.0)  # Boost for domain relevance
        
        # Weighted average
        confidence = (node_score * 0.3 + edge_score * 0.3 + text_score * 0.2 + domain_score * 0.2)
        
        return round(confidence, 2)
