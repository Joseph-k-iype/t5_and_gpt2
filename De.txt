"""
Legal Summary Pipeline
Main orchestrator for generating legal summary Word documents from legislation PDFs
"""

import json
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
import argparse

from src.processors.pdf_processor import PDFProcessor
from src.analyzers.legal_document_analyzer import LegalDocumentAnalyzer
from src.generators.legal_summary_generator import LegalSummaryGenerator
from src.config import Config


class LegalSummaryPipeline:
    """
    End-to-end pipeline for legal document analysis and Word document generation
    """
    
    def __init__(
        self,
        metadata_file: str = "config/legislation_metadata.json",
        output_dir: str = "output/legal_summaries",
        config: Optional[Config] = None
    ):
        self.metadata_file = metadata_file
        self.output_dir = output_dir
        self.config = config or Config()
        
        # Initialize components
        self.pdf_processor = PDFProcessor()
        self.analyzer = LegalDocumentAnalyzer(config=self.config)
        self.generator = LegalSummaryGenerator(output_dir=output_dir)
        
        # Load metadata
        self.metadata = self._load_metadata()
        
        print(f"Initialized Legal Summary Pipeline")
        print(f"Loaded {len(self.metadata)} rules from metadata")
    
    def _load_metadata(self) -> Dict[str, Any]:
        """Load legislation metadata from JSON file"""
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            return metadata
        except FileNotFoundError:
            print(f"Warning: Metadata file not found: {self.metadata_file}")
            return {}
        except json.JSONDecodeError as e:
            print(f"Error parsing metadata JSON: {str(e)}")
            return {}
    
    def _detect_enterprise_context(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Detect enterprise-specific context from document text
        """
        context = {}
        
        # Detect organization
        if "HSBC" in text or "hsbc" in text.lower():
            context["organization"] = "HSBC"
        
        # Detect internal tools
        tools = []
        if "DataVisa" in text or "datavisa" in text.lower():
            tools.append("DataVisa")
        if "DataHub" in text or "datahub" in text.lower():
            tools.append("DataHub")
        if "PrivacyHub" in text or "privacyhub" in text.lower():
            tools.append("PrivacyHub")
        
        if tools:
            context["internal_tools"] = tools
        
        # Detect if this is an enterprise policy
        if context.get("organization"):
            context["is_enterprise_policy"] = True
        
        return context if context else None
    
    def _extract_pdf_text(self, pdf_path: str) -> str:
        """Extract text from a PDF file"""
        try:
            if not os.path.exists(pdf_path):
                print(f"Warning: PDF file not found: {pdf_path}")
                return ""
            
            text = self.pdf_processor.extract_text(pdf_path)
            return text
        except Exception as e:
            print(f"Error extracting text from {pdf_path}: {str(e)}")
            return ""
    
    def process_rule(
        self,
        rule_name: str,
        rule_config: Dict[str, Any],
        generate_doc: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        Process a single rule: extract text, analyze, and optionally generate Word doc
        
        Args:
            rule_name: Name of the rule
            rule_config: Configuration dictionary for the rule
            generate_doc: Whether to generate Word document
            
        Returns:
            Analysis dictionary or None if processing fails
        """
        print(f"\n{'='*80}")
        print(f"Processing Rule: {rule_name}")
        print(f"{'='*80}")
        
        # Extract configuration
        countries = rule_config.get("country", [])
        adequacy_countries = rule_config.get("adequacy_country", [])
        
        jurisdiction = ", ".join(countries) if countries else "General"
        print(f"Jurisdiction: {jurisdiction}")
        
        if adequacy_countries:
            print(f"Adequacy Countries: {', '.join(adequacy_countries)}")
        
        # Get file paths
        level_1_file = rule_config.get("file_level_1")
        level_2_file = rule_config.get("file_level_2")
        level_3_file = rule_config.get("file_level_3")
        
        if not all([level_1_file, level_2_file, level_3_file]):
            print(f"Error: Missing file paths for rule '{rule_name}'")
            return None
        
        # Extract text from all levels
        print(f"\nExtracting text from PDFs...")
        print(f"  Level 1: {level_1_file}")
        level_1_text = self._extract_pdf_text(level_1_file)
        
        print(f"  Level 2: {level_2_file}")
        level_2_text = self._extract_pdf_text(level_2_file)
        
        print(f"  Level 3: {level_3_file}")
        level_3_text = self._extract_pdf_text(level_3_file)
        
        if not (level_1_text or level_2_text or level_3_text):
            print(f"Error: No text could be extracted from any PDF")
            return None
        
        # Detect enterprise context from all texts
        all_text = f"{level_1_text} {level_2_text} {level_3_text}"
        enterprise_context = self._detect_enterprise_context(all_text)
        
        if enterprise_context:
            print(f"\nDetected enterprise context:")
            if enterprise_context.get("organization"):
                print(f"  Organization: {enterprise_context['organization']}")
            if enterprise_context.get("internal_tools"):
                print(f"  Internal Tools: {', '.join(enterprise_context['internal_tools'])}")
        
        # Analyze documents
        print(f"\nAnalyzing documents using ReAct agents...")
        print(f"  This may take several minutes...")
        
        try:
            analysis = self.analyzer.analyze_multi_level(
                rule_name=rule_name,
                jurisdiction=jurisdiction,
                level_1_text=level_1_text,
                level_2_text=level_2_text,
                level_3_text=level_3_text,
                enterprise_context=enterprise_context
            )
            
            # Add adequacy countries to metadata
            if not analysis.get("metadata"):
                analysis["metadata"] = {}
            
            analysis["metadata"]["adequacy_countries"] = adequacy_countries
            analysis["metadata"]["countries"] = countries
            
            print(f"\n✓ Analysis complete")
            print(f"  Rule Type: {analysis.get('rule_type', 'N/A').upper()}")
            print(f"  Confidence: {analysis.get('confidence', 'N/A').upper()}")
            print(f"  User Actions: {len(analysis.get('user_actions', []))}")
            print(f"  System Actions: {len(analysis.get('system_actions', []))}")
            print(f"  Constraints: {len(analysis.get('constraints', []))}")
            
        except Exception as e:
            print(f"✗ Error during analysis: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
        
        # Generate Word document
        if generate_doc:
            print(f"\nGenerating Word document...")
            try:
                doc_path = self.generator.generate_legal_summary(analysis)
                print(f"✓ Document saved: {doc_path}")
                
                # Save JSON analysis as well
                json_path = doc_path.replace('.docx', '.json')
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, indent=2, ensure_ascii=False)
                print(f"✓ JSON analysis saved: {json_path}")
                
            except Exception as e:
                print(f"✗ Error generating document: {str(e)}")
                import traceback
                traceback.print_exc()
        
        return analysis
    
    def process_all_rules(
        self,
        generate_docs: bool = True,
        generate_index: bool = True
    ) -> Dict[str, Dict[str, Any]]:
        """
        Process all rules in the metadata file
        
        Args:
            generate_docs: Whether to generate Word documents
            generate_index: Whether to generate an index document
            
        Returns:
            Dictionary mapping rule names to their analyses
        """
        if not self.metadata:
            print("No metadata loaded. Cannot process rules.")
            return {}
        
        all_analyses = {}
        
        print(f"\n{'#'*80}")
        print(f"# Processing {len(self.metadata)} rules")
        print(f"{'#'*80}")
        
        for i, (rule_name, rule_config) in enumerate(self.metadata.items(), 1):
            print(f"\n[{i}/{len(self.metadata)}] Processing: {rule_name}")
            
            analysis = self.process_rule(
                rule_name=rule_name,
                rule_config=rule_config,
                generate_doc=generate_docs
            )
            
            if analysis:
                all_analyses[rule_name] = analysis
            else:
                print(f"⚠ Skipping {rule_name} due to errors")
        
        # Generate index document
        if generate_index and all_analyses:
            print(f"\n{'='*80}")
            print(f"Generating Index Document")
            print(f"{'='*80}")
            
            try:
                index_path = self.generator.create_index_document(all_analyses)
                print(f"✓ Index document saved: {index_path}")
            except Exception as e:
                print(f"✗ Error generating index: {str(e)}")
        
        # Print summary
        print(f"\n{'#'*80}")
        print(f"# Processing Complete")
        print(f"{'#'*80}")
        print(f"Successfully processed: {len(all_analyses)}/{len(self.metadata)} rules")
        print(f"Output directory: {self.output_dir}")
        
        return all_analyses
    
    def process_single_rule(
        self,
        rule_name: str,
        generate_doc: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        Process a single rule by name
        
        Args:
            rule_name: Name of the rule to process
            generate_doc: Whether to generate Word document
            
        Returns:
            Analysis dictionary or None if rule not found
        """
        if rule_name not in self.metadata:
            print(f"Error: Rule '{rule_name}' not found in metadata")
            print(f"Available rules: {', '.join(self.metadata.keys())}")
            return None
        
        return self.process_rule(
            rule_name=rule_name,
            rule_config=self.metadata[rule_name],
            generate_doc=generate_doc
        )


def main():
    """Main entry point for the pipeline"""
    parser = argparse.ArgumentParser(
        description="Legal Summary Pipeline - Generate Word documents from legislation PDFs"
    )
    parser.add_argument(
        "--metadata",
        type=str,
        default="config/legislation_metadata.json",
        help="Path to legislation metadata JSON file"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/legal_summaries",
        help="Output directory for generated documents"
    )
    parser.add_argument(
        "--rule",
        type=str,
        help="Process only a specific rule (by name)"
    )
    parser.add_argument(
        "--no-docs",
        action="store_true",
        help="Skip Word document generation (analysis only)"
    )
    parser.add_argument(
        "--no-index",
        action="store_true",
        help="Skip index document generation"
    )
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = LegalSummaryPipeline(
        metadata_file=args.metadata,
        output_dir=args.output
    )
    
    # Process rules
    if args.rule:
        # Process single rule
        pipeline.process_single_rule(
            rule_name=args.rule,
            generate_doc=not args.no_docs
        )
    else:
        # Process all rules
        pipeline.process_all_rules(
            generate_docs=not args.no_docs,
            generate_index=not args.no_index
        )


if __name__ == "__main__":
    main()
