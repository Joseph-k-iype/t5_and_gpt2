"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Fixed version with proper tool calling and o3-mini integration
Enhanced with Chain of Thought, Mixture of Thought, and Mixture of Reasoning
Focused on Data Governance Rules (Usage, Transfer, Storage, Access)
"""

import json
import re
import time
import os
from typing import List, Dict, Any, Optional, Annotated, Sequence, TypedDict, Literal
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# ========================= Global Configuration =========================

# Global model configuration for o3-mini
OPENAI_MODEL = "o3-mini"
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
REASONING_EFFORT = "high"  # high, medium, or low

# Global model instance
def get_model():
    """Get configured o3-mini model instance"""
    return ChatOpenAI(
        model=OPENAI_MODEL,
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
        model_kwargs={
            "reasoning_effort": REASONING_EFFORT,
            "max_completion_tokens": 4000  # o3-mini uses max_completion_tokens instead of max_tokens
        }
        # Note: o3-mini doesn't support temperature, top_p, presence_penalty, frequency_penalty
    )


# ========================= State Management =========================

class AgentState(TypedDict):
    """Comprehensive state for the legislation processing agent with reasoning traces"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str
    # Mixture of Thought - Multiple reasoning pathways
    reasoning_pathways: List[Dict[str, Any]]
    # Chain of Thought - Step-by-step reasoning
    reasoning_steps: List[Dict[str, Any]]
    # Mixture of Reasoning - Different reasoning modes
    reasoning_modes: List[str]
    # Extracted data governance rules
    data_usage_rules: List[Dict[str, Any]]
    data_transfer_rules: List[Dict[str, Any]]
    data_storage_rules: List[Dict[str, Any]]
    data_access_rules: List[Dict[str, Any]]
    # Final outputs
    all_extracted_rules: List[Dict[str, Any]]
    json_rules: List[Dict[str, Any]]
    validation_results: Dict[str, Any]
    current_phase: str
    analysis_results: List[Dict[str, Any]]


# ========================= Enums for Reasoning Framework =========================

class ReasoningPathway(Enum):
    """Mixture of Thought - Different pathways to analyze legislation"""
    STRUCTURAL = "structural"  # Analyze document structure and hierarchy
    SEMANTIC = "semantic"  # Understand meaning and intent
    LOGICAL = "logical"  # Extract logical relationships and conditions
    CONTEXTUAL = "contextual"  # Understand context and domain specifics
    COMPLIANCE = "compliance"  # Focus on compliance requirements


class ReasoningMode(Enum):
    """Mixture of Reasoning - Different reasoning approaches"""
    DEDUCTIVE = "deductive"  # General principles to specific rules
    INDUCTIVE = "inductive"  # Specific examples to general patterns
    ABDUCTIVE = "abductive"  # Best explanation for observations
    ANALOGICAL = "analogical"  # Compare with known patterns
    CAUSAL = "causal"  # Cause-effect relationships


class DataDomain(Enum):
    """Data governance domains to focus on"""
    USAGE = "data_usage"
    TRANSFER = "data_transfer"
    STORAGE = "data_storage"
    ACCESS = "data_access"


# ========================= Pydantic Models for Tools =========================

class AnalyzeWithReasoningInput(BaseModel):
    legislation_text: str = Field(..., description="Legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Reasoning pathway to use (structural/semantic/logical/contextual/compliance)")
    reasoning_mode: str = Field(..., description="Reasoning mode to apply (deductive/inductive/abductive/analogical/causal)")
    focus_domain: str = Field(..., description="Data domain to focus on (data_usage/data_transfer/data_storage/data_access)")


class ExtractDataRulesInput(BaseModel):
    analyzed_text: str = Field(..., description="Analyzed text with reasoning context")
    data_domain: str = Field(..., description="Data domain to extract rules for")


class SynthesizeRulesInput(BaseModel):
    usage_rules: List[Dict[str, Any]] = Field(default=[], description="Data usage rules")
    transfer_rules: List[Dict[str, Any]] = Field(default=[], description="Data transfer rules")
    storage_rules: List[Dict[str, Any]] = Field(default=[], description="Data storage rules")
    access_rules: List[Dict[str, Any]] = Field(default=[], description="Data access rules")


class ConvertToJsonRulesInput(BaseModel):
    synthesized_rules: List[Dict[str, Any]] = Field(..., description="Synthesized rules to convert")


class ValidateJsonRulesInput(BaseModel):
    json_rules: List[Dict[str, Any]] = Field(..., description="JSON rules to validate")


# ========================= Advanced Reasoning Tools =========================

@tool(args_schema=AnalyzeWithReasoningInput)
def analyze_with_reasoning(
    legislation_text: str,
    reasoning_pathway: str,
    reasoning_mode: str,
    focus_domain: str
) -> str:
    """
    Analyze legislation using specified reasoning pathway and mode.
    Uses o3-mini capabilities for deep understanding.
    
    This implements:
    - Mixture of Thought: Different analytical pathways
    - Mixture of Reasoning: Different reasoning modes
    - Chain of Thought: Step-by-step analysis
    """
    
    # Create focused prompts based on pathway and mode
    pathway_prompts = {
        "structural": f"""
        Analyze the STRUCTURE of this legislation focusing on {focus_domain}:
        1. Identify main sections and subsections related to {focus_domain}
        2. Map the hierarchical organization of {focus_domain} provisions
        3. Find cross-references and dependencies for {focus_domain} rules
        4. Identify how {focus_domain} rules are organized within the document structure
        """,
        
        "semantic": f"""
        Analyze the MEANING and INTENT for {focus_domain}:
        1. Identify key definitions related to {focus_domain}
        2. Understand the purpose of each {focus_domain} provision
        3. Extract implied meanings and interpretations for {focus_domain}
        4. Identify stakeholders and their {focus_domain} obligations
        """,
        
        "logical": f"""
        Extract LOGICAL RELATIONSHIPS for {focus_domain}:
        1. Identify IF-THEN conditions related to {focus_domain}
        2. Find {focus_domain} requirements (MUST/SHALL statements)
        3. Find {focus_domain} prohibitions (MUST NOT/SHALL NOT)
        4. Map cause-effect relationships in {focus_domain} rules
        """,
        
        "contextual": f"""
        Understand the CONTEXT for {focus_domain}:
        1. Identify the regulatory environment for {focus_domain}
        2. Understand industry-specific {focus_domain} requirements
        3. Find temporal conditions and deadlines for {focus_domain}
        4. Identify exceptions and special cases in {focus_domain} rules
        """,
        
        "compliance": f"""
        Extract COMPLIANCE REQUIREMENTS for {focus_domain}:
        1. Identify mandatory {focus_domain} obligations
        2. Find penalties for non-compliance with {focus_domain} rules
        3. Extract audit and reporting requirements for {focus_domain}
        4. Identify enforcement mechanisms for {focus_domain} violations
        """
    }
    
    mode_prompts = {
        "deductive": "Apply top-down reasoning: Start from general principles and derive specific rules.",
        "inductive": "Apply bottom-up reasoning: Identify specific patterns and infer general rules.",
        "abductive": "Find the best explanation: What rules best explain the legislative intent?",
        "analogical": "Compare with known patterns: How do these rules compare to standard data governance frameworks?",
        "causal": "Identify cause-effect chains: What triggers lead to what consequences?"
    }
    
    # Build the analysis prompt
    analysis_prompt = f"""
    {pathway_prompts.get(reasoning_pathway, pathway_prompts["structural"])}
    
    Reasoning Mode: {mode_prompts.get(reasoning_mode, mode_prompts["deductive"])}
    
    Legislation Text:
    {legislation_text}
    
    Provide a detailed analysis focusing on {focus_domain} rules.
    Include:
    - Key findings for {focus_domain}
    - Identified patterns in {focus_domain} provisions
    - Relevant sections containing {focus_domain} rules
    - Logical relationships in {focus_domain} requirements
    - Compliance requirements for {focus_domain}
    
    Return your analysis as structured text with clear sections.
    """
    
    # Get model and analyze
    model = get_model()
    response = model.invoke(analysis_prompt)
    
    # Return the analysis as a string for tool chaining
    analysis_result = f"""
    PATHWAY: {reasoning_pathway}
    MODE: {reasoning_mode}
    DOMAIN: {focus_domain}
    
    ANALYSIS:
    {response.content}
    
    METADATA:
    - Text length: {len(legislation_text)} characters
    - Analysis timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
    """
    
    return analysis_result


@tool(args_schema=ExtractDataRulesInput)
def extract_data_rules(
    analyzed_text: str,
    data_domain: str
) -> str:
    """
    Extract specific data governance rules using o3-mini capabilities.
    Implements Chain of Thought for step-by-step extraction.
    """
    
    # Domain-specific extraction prompts
    domain_prompts = {
        "data_usage": """
        Extract ALL rules about DATA USAGE from this analyzed text:
        
        Chain of Thought Steps:
        1. First, identify all mentions of how data can be used
        2. Next, find restrictions on data usage
        3. Then, identify purposes for which data can/cannot be used
        4. Extract conditions that must be met for data usage
        5. Find any consent requirements for data usage
        6. Identify data usage retention limits
        
        For each rule found, extract:
        - Rule description (what the rule says)
        - Subject (who must comply)
        - Conditions (when it applies)
        - Requirements (what must be done)
        - Prohibitions (what must not be done)
        - Consequences (what happens if violated)
        """,
        
        "data_transfer": """
        Extract ALL rules about DATA TRANSFER from this analyzed text:
        
        Chain of Thought Steps:
        1. First, identify all mentions of data sharing, transmission, or transfer
        2. Next, find cross-border data transfer rules
        3. Then, identify third-party sharing restrictions
        4. Extract conditions for legitimate data transfers
        5. Find security requirements for data in transit
        6. Identify notification requirements for data transfers
        
        For each rule found, extract:
        - Rule description
        - Transfer type (internal/external/cross-border)
        - Allowed recipients
        - Prohibited recipients
        - Required safeguards
        - Approval requirements
        """,
        
        "data_storage": """
        Extract ALL rules about DATA STORAGE from this analyzed text:
        
        Chain of Thought Steps:
        1. First, identify all data retention requirements
        2. Next, find data deletion/destruction requirements
        3. Then, identify storage location restrictions
        4. Extract encryption and security requirements
        5. Find backup and recovery obligations
        6. Identify data minimization principles
        
        For each rule found, extract:
        - Rule description
        - Storage duration requirements
        - Storage location restrictions
        - Security measures required
        - Deletion requirements
        - Access control requirements
        """,
        
        "data_access": """
        Extract ALL rules about DATA ACCESS from this analyzed text:
        
        Chain of Thought Steps:
        1. First, identify who can access what data
        2. Next, find authentication requirements
        3. Then, identify access control mechanisms
        4. Extract audit trail requirements
        5. Find data subject access rights
        6. Identify breach notification requirements
        
        For each rule found, extract:
        - Rule description
        - Authorized parties
        - Access conditions
        - Authentication requirements
        - Audit requirements
        - Rights and remedies
        """
    }
    
    extraction_prompt = f"""
    {domain_prompts.get(data_domain, domain_prompts["data_usage"])}
    
    Analyzed text to process:
    {analyzed_text}
    
    Extract rules and return them as a JSON array. Each rule should be a JSON object with these exact fields:
    {{
        "rule_id": "unique_identifier",
        "domain": "{data_domain}",
        "description": "clear description of the rule",
        "conditions": ["condition1", "condition2"],
        "requirements": ["requirement1", "requirement2"],
        "prohibitions": ["prohibition1", "prohibition2"],
        "consequences": ["consequence1", "consequence2"],
        "confidence": 0.8
    }}
    
    Return ONLY the JSON array, no other text or markdown formatting.
    """
    
    # Get model and extract
    model = get_model()
    response = model.invoke(extraction_prompt)
    
    # Return the extracted rules as JSON string
    return response.content


@tool(args_schema=SynthesizeRulesInput)
def synthesize_rules(
    usage_rules: List[Dict[str, Any]] = [],
    transfer_rules: List[Dict[str, Any]] = [],
    storage_rules: List[Dict[str, Any]] = [],
    access_rules: List[Dict[str, Any]] = []
) -> str:
    """
    Synthesize rules from multiple domains.
    Implements convergent synthesis from divergent analysis.
    """
    
    all_rules = {
        "data_usage": usage_rules,
        "data_transfer": transfer_rules,
        "data_storage": storage_rules,
        "data_access": access_rules
    }
    
    synthesis_prompt = f"""
    Synthesize the following data governance rules into a coherent set.
    
    Rules by Domain:
    {json.dumps(all_rules, indent=2)}
    
    Synthesis Requirements:
    1. Remove duplicates while preserving unique conditions
    2. Merge related rules that address the same topic
    3. Ensure logical consistency between rules
    4. Resolve any conflicts between rules
    5. Prioritize rules based on clarity and importance
    
    Return ONLY a JSON array of synthesized rules. Each rule should have:
    - rule_id: unique identifier
    - domain: the data domain
    - description: clear description
    - conditions: array of conditions
    - requirements: array of requirements
    - prohibitions: array of prohibitions
    - consequences: array of consequences
    - priority: 1-100
    - confidence: 0.0-1.0
    
    Return ONLY the JSON array, no other text.
    """
    
    model = get_model()
    response = model.invoke(synthesis_prompt)
    
    return response.content


@tool(args_schema=ConvertToJsonRulesInput)
def convert_to_json_rules(synthesized_rules: List[Dict[str, Any]]) -> str:
    """
    Convert synthesized rules to json-rules-engine format.
    Creates complex, multi-condition rules that make logical sense.
    """
    
    conversion_prompt = f"""
    Convert these synthesized rules to json-rules-engine format.
    
    Input Rules:
    {json.dumps(synthesized_rules, indent=2)}
    
    Convert each rule to json-rules-engine format with this structure:
    {{
        "name": "rule_identifier",
        "conditions": {{
            "all": [
                {{
                    "fact": "factName",
                    "operator": "equal",
                    "value": "expectedValue"
                }}
            ]
        }},
        "event": {{
            "type": "rule_triggered",
            "params": {{
                "ruleId": "rule_id",
                "domain": "data_domain",
                "action": "action_to_take",
                "message": "description"
            }}
        }},
        "priority": 50
    }}
    
    Use appropriate facts like:
    - dataOperation (values: "usage", "transfer", "storage", "access")
    - userConsent (values: true, false)
    - dataEncrypted (values: true, false)
    - userAuthorized (values: true, false)
    - crossBorderTransfer (values: true, false)
    - dataType (values: "personal", "sensitive", "public")
    - retentionPeriod (values: numbers for days/months)
    
    Use operators: equal, notEqual, lessThan, greaterThan, in, contains
    
    Return ONLY a JSON array of json-rules-engine formatted rules.
    """
    
    model = get_model()
    response = model.invoke(conversion_prompt)
    
    return response.content


@tool(args_schema=ValidateJsonRulesInput)
def validate_json_rules(json_rules: List[Dict[str, Any]]) -> str:
    """
    Validate JSON rules for json-rules-engine compatibility and logical consistency.
    """
    
    validation_prompt = f"""
    Validate these JSON rules for json-rules-engine compatibility:
    
    Rules to validate:
    {json.dumps(json_rules, indent=2)}
    
    Check for:
    1. Required fields: name, conditions, event
    2. Valid operators: equal, notEqual, lessThan, greaterThan, in, contains, etc.
    3. Proper condition structure with "all" or "any"
    4. Event structure with type and params
    5. Logical consistency between conditions
    6. Priority values (should be numbers)
    
    Return a JSON validation report with this structure:
    {{
        "valid": true/false,
        "total_rules": number,
        "valid_rules": number,
        "invalid_rules": number,
        "errors": ["error1", "error2"],
        "warnings": ["warning1", "warning2"],
        "domain_coverage": {{
            "data_usage": count,
            "data_transfer": count,
            "data_storage": count,
            "data_access": count
        }},
        "quality_score": 0-100
    }}
    
    Return ONLY the JSON validation report.
    """
    
    model = get_model()
    response = model.invoke(validation_prompt)
    
    return response.content


# ========================= Agent Node Functions =========================

def agent_node(state: AgentState) -> Dict[str, Any]:
    """
    Main agent node that orchestrates the reasoning process.
    Uses o3-mini with high reasoning effort for deep analysis.
    """
    messages = state["messages"]
    current_phase = state.get("current_phase", "start")
    
    # Get model with tools
    model = get_model()
    tools = [
        analyze_with_reasoning,
        extract_data_rules,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    model_with_tools = model.bind_tools(tools)
    
    # Create phase-specific prompts
    if current_phase == "start":
        system_prompt = """You are an expert legal analyst specializing in data governance and privacy regulations.

Your task is to process legislation through multiple reasoning phases:

PHASE 1: DIVERGENT ANALYSIS (Mixture of Thought)
Analyze the legislation through multiple pathways. For EACH pathway, call analyze_with_reasoning:

1. analyze_with_reasoning(legislation_text=<full_text>, reasoning_pathway="structural", reasoning_mode="deductive", focus_domain="data_usage")
2. analyze_with_reasoning(legislation_text=<full_text>, reasoning_pathway="semantic", reasoning_mode="inductive", focus_domain="data_transfer") 
3. analyze_with_reasoning(legislation_text=<full_text>, reasoning_pathway="logical", reasoning_mode="abductive", focus_domain="data_storage")
4. analyze_with_reasoning(legislation_text=<full_text>, reasoning_pathway="contextual", reasoning_mode="analogical", focus_domain="data_access")

You MUST call analyze_with_reasoning multiple times to analyze different aspects. Start with the first analysis now."""

        new_messages = [SystemMessage(content=system_prompt)] + list(messages)
        new_phase = "analysis"
        
    elif current_phase == "analysis":
        system_prompt = """Continue with rule extraction. For each data domain, call extract_data_rules:

1. extract_data_rules(analyzed_text=<use the legislation text>, data_domain="data_usage")
2. extract_data_rules(analyzed_text=<use the legislation text>, data_domain="data_transfer")
3. extract_data_rules(analyzed_text=<use the legislation text>, data_domain="data_storage")
4. extract_data_rules(analyzed_text=<use the legislation text>, data_domain="data_access")

Extract rules for each domain systematically."""

        new_messages = [SystemMessage(content=system_prompt)] + list(messages)
        new_phase = "extraction"
        
    elif current_phase == "extraction":
        system_prompt = """Now synthesize all extracted rules by calling:
synthesize_rules(usage_rules=<collected_usage_rules>, transfer_rules=<collected_transfer_rules>, storage_rules=<collected_storage_rules>, access_rules=<collected_access_rules>)

Use the rules that have been extracted in previous steps."""

        new_messages = [SystemMessage(content=system_prompt)] + list(messages)
        new_phase = "synthesis"
        
    elif current_phase == "synthesis":
        system_prompt = """Convert the synthesized rules to json-rules-engine format by calling:
convert_to_json_rules(synthesized_rules=<rules_from_synthesis>)"""

        new_messages = [SystemMessage(content=system_prompt)] + list(messages)
        new_phase = "conversion"
        
    elif current_phase == "conversion":
        system_prompt = """Finally, validate the JSON rules by calling:
validate_json_rules(json_rules=<converted_rules>)"""

        new_messages = [SystemMessage(content=system_prompt)] + list(messages)
        new_phase = "validation"
        
    else:
        new_messages = list(messages)
        new_phase = "complete"
    
    # Get model response
    response = model_with_tools.invoke(new_messages)
    
    return {
        "messages": [response],
        "current_phase": new_phase
    }


def tool_node(state: AgentState) -> Dict[str, Any]:
    """Execute tools and process results"""
    tools = [
        analyze_with_reasoning,
        extract_data_rules,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    
    tool_node_instance = ToolNode(tools)
    result = tool_node_instance.invoke(state)
    
    # Process tool results and update state
    messages = result.get("messages", [])
    updates = {"messages": messages}
    
    # Extract and organize results from tool outputs
    for message in messages:
        if isinstance(message, ToolMessage):
            content = message.content
            
            # Try to parse tool results and update state accordingly
            if message.name == "extract_data_rules":
                try:
                    # Parse extracted rules
                    if content.strip().startswith('['):
                        rules = json.loads(content)
                        # Determine domain and update appropriate list
                        domain = getattr(message, 'tool_call_id', '')
                        for rule in rules:
                            if isinstance(rule, dict):
                                rule_domain = rule.get('domain', '')
                                if 'usage' in rule_domain:
                                    current_usage = state.get('data_usage_rules', [])
                                    current_usage.append(rule)
                                    updates['data_usage_rules'] = current_usage
                                elif 'transfer' in rule_domain:
                                    current_transfer = state.get('data_transfer_rules', [])
                                    current_transfer.append(rule)
                                    updates['data_transfer_rules'] = current_transfer
                                elif 'storage' in rule_domain:
                                    current_storage = state.get('data_storage_rules', [])
                                    current_storage.append(rule)
                                    updates['data_storage_rules'] = current_storage
                                elif 'access' in rule_domain:
                                    current_access = state.get('data_access_rules', [])
                                    current_access.append(rule)
                                    updates['data_access_rules'] = current_access
                except:
                    pass
            
            elif message.name == "convert_to_json_rules":
                try:
                    if content.strip().startswith('['):
                        json_rules = json.loads(content)
                        updates['json_rules'] = json_rules
                except:
                    pass
            
            elif message.name == "validate_json_rules":
                try:
                    if content.strip().startswith('{'):
                        validation = json.loads(content)
                        updates['validation_results'] = validation
                except:
                    pass
    
    return updates


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """Determine whether to continue to tools or end"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # Check if there are tool calls
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # Check if we should move to next phase
    current_phase = state.get("current_phase", "start")
    if current_phase == "complete":
        return "end"
    
    # Continue if we haven't completed all phases
    return "end"


# ========================= Create Agent Graph =========================

def create_legislation_agent():
    """
    Create LangGraph agent with proper tool calling setup.
    """
    
    # Create the graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    
    # Set entry point
    workflow.set_entry_point("agent")
    
    # Add conditional edges
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "end": END
        }
    )
    
    # Add edge from tools back to agent
    workflow.add_edge("tools", "agent")
    
    # Compile with memory
    memory = MemorySaver()
    graph = workflow.compile(checkpointer=memory)
    
    return graph


# ========================= Simple Direct Processing Function =========================

def process_legislation_simple(legislation_text: str) -> Dict[str, Any]:
    """
    Simplified processing function that directly calls tools in sequence.
    This avoids the complexity of the agent loop and ensures tools are called.
    """
    
    print("üöÄ Processing legislation with o3-mini reasoning...")
    results = {
        "analysis_results": [],
        "extracted_rules": {
            "data_usage": [],
            "data_transfer": [],
            "data_storage": [],
            "data_access": []
        },
        "synthesized_rules": [],
        "json_rules": [],
        "validation": {}
    }
    
    # Phase 1: Multiple analyses (Mixture of Thought + Mixture of Reasoning)
    print("\nüìä Phase 1: Divergent Analysis...")
    
    analysis_configs = [
        ("structural", "deductive", "data_usage"),
        ("semantic", "inductive", "data_transfer"), 
        ("logical", "abductive", "data_storage"),
        ("contextual", "analogical", "data_access"),
        ("compliance", "causal", "data_usage")
    ]
    
    for pathway, mode, domain in analysis_configs:
        print(f"   Analyzing {domain} with {pathway} pathway and {mode} reasoning...")
        analysis_result = analyze_with_reasoning.invoke({
            "legislation_text": legislation_text,
            "reasoning_pathway": pathway,
            "reasoning_mode": mode,
            "focus_domain": domain
        })
        results["analysis_results"].append({
            "pathway": pathway,
            "mode": mode,
            "domain": domain,
            "result": analysis_result
        })
        time.sleep(1)  # Brief pause between API calls
    
    # Phase 2: Extract rules for each domain
    print("\nüîç Phase 2: Rule Extraction...")
    
    for domain in ["data_usage", "data_transfer", "data_storage", "data_access"]:
        print(f"   Extracting {domain} rules...")
        extracted_rules_json = extract_data_rules.invoke({
            "analyzed_text": legislation_text,  # Use full text for extraction
            "data_domain": domain
        })
        
        # Parse the extracted rules
        try:
            content = extracted_rules_json.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            if content.startswith('['):
                rules = json.loads(content)
                results["extracted_rules"][domain] = rules
                print(f"     Found {len(rules)} {domain} rules")
            else:
                print(f"     No valid rules found for {domain}")
        except Exception as e:
            print(f"     Error parsing {domain} rules: {e}")
        
        time.sleep(1)
    
    # Phase 3: Synthesize rules
    print("\nüîÑ Phase 3: Rule Synthesis...")
    
    synthesized_json = synthesize_rules.invoke({
        "usage_rules": results["extracted_rules"]["data_usage"],
        "transfer_rules": results["extracted_rules"]["data_transfer"],
        "storage_rules": results["extracted_rules"]["data_storage"],
        "access_rules": results["extracted_rules"]["data_access"]
    })
    
    try:
        content = synthesized_json.strip()
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        if content.startswith('['):
            results["synthesized_rules"] = json.loads(content)
            print(f"   Synthesized {len(results['synthesized_rules'])} rules")
        else:
            print("   No synthesized rules generated")
    except Exception as e:
        print(f"   Error parsing synthesized rules: {e}")
    
    time.sleep(1)
    
    # Phase 4: Convert to JSON rules engine format
    print("\n‚öôÔ∏è Phase 4: JSON Rules Engine Conversion...")
    
    if results["synthesized_rules"]:
        json_rules_content = convert_to_json_rules.invoke({
            "synthesized_rules": results["synthesized_rules"]
        })
        
        try:
            content = json_rules_content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            if content.startswith('['):
                results["json_rules"] = json.loads(content)
                print(f"   Converted {len(results['json_rules'])} JSON rules")
            else:
                print("   No JSON rules generated")
        except Exception as e:
            print(f"   Error parsing JSON rules: {e}")
        
        time.sleep(1)
    
    # Phase 5: Validate rules
    print("\n‚úÖ Phase 5: Validation...")
    
    if results["json_rules"]:
        validation_content = validate_json_rules.invoke({
            "json_rules": results["json_rules"]
        })
        
        try:
            content = validation_content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            if content.startswith('{'):
                results["validation"] = json.loads(content)
                print(f"   Validation complete - Quality Score: {results['validation'].get('quality_score', 'N/A')}")
            else:
                print("   No validation results generated")
        except Exception as e:
            print(f"   Error parsing validation: {e}")
    
    return results


# ========================= Helper Functions =========================

def save_rules_to_file(rules: List[Dict[str, Any]], filename: str = "data_governance_rules.json"):
    """Save JSON rules to file"""
    output = {
        "rules": rules,
        "metadata": {
            "created_by": "advanced_reasoning_agent_o3mini",
            "model": OPENAI_MODEL,
            "reasoning_effort": REASONING_EFFORT,
            "reasoning_framework": "CoT + MoT + MoR",
            "focus": "data_governance",
            "domains": ["data_usage", "data_transfer", "data_storage", "data_access"],
            "engine": "json-rules-engine",
            "rule_count": len(rules),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Saved {len(rules)} rules to {filename}")


def print_results_summary(results: Dict[str, Any]):
    """Print a comprehensive summary of the results"""
    print("\nüß† PROCESSING SUMMARY")
    print("=" * 60)
    
    # Analysis results
    analysis_results = results.get("analysis_results", [])
    print(f"Analysis Pathways Used: {len(analysis_results)}")
    for analysis in analysis_results:
        print(f"  - {analysis['pathway']} + {analysis['mode']} ‚Üí {analysis['domain']}")
    
    # Extraction results
    extracted = results.get("extracted_rules", {})
    print(f"\nüìä RULE EXTRACTION")
    print("-" * 40)
    total_extracted = 0
    for domain, rules in extracted.items():
        count = len(rules)
        total_extracted += count
        print(f"  {domain.replace('_', ' ').title()}: {count} rules")
    print(f"  Total Extracted: {total_extracted} rules")
    
    # Synthesis results
    synthesized = results.get("synthesized_rules", [])
    print(f"\nüîÑ SYNTHESIS RESULTS")
    print("-" * 40)
    print(f"  Synthesized Rules: {len(synthesized)}")
    
    # JSON rules
    json_rules = results.get("json_rules", [])
    print(f"\n‚öôÔ∏è JSON RULES ENGINE FORMAT")
    print("-" * 40)
    print(f"  Final JSON Rules: {len(json_rules)}")
    
    # Validation
    validation = results.get("validation", {})
    if validation:
        print(f"\n‚úÖ VALIDATION RESULTS")
        print("-" * 40)
        print(f"  Valid: {validation.get('valid', 'N/A')}")
        print(f"  Quality Score: {validation.get('quality_score', 'N/A')}%")
        print(f"  Valid Rules: {validation.get('valid_rules', 0)}/{validation.get('total_rules', 0)}")
        
        domain_coverage = validation.get('domain_coverage', {})
        if domain_coverage:
            print(f"  Domain Coverage:")
            for domain, count in domain_coverage.items():
                if count > 0:
                    print(f"    - {domain.replace('_', ' ').title()}: {count}")
    
    # Sample rule
    if json_rules:
        print(f"\nüìã SAMPLE RULE")
        print("-" * 40)
        sample = json_rules[0]
        print(f"  Name: {sample.get('name', 'N/A')}")
        print(f"  Priority: {sample.get('priority', 'N/A')}")
        print(f"  Event Type: {sample.get('event', {}).get('type', 'N/A')}")
        conditions = sample.get('conditions', {}).get('all', [])
        print(f"  Conditions: {len(conditions)}")


# ========================= Main Execution =========================

if __name__ == "__main__":
    # Example data governance legislation
    SAMPLE_LEGISLATION = """
    DATA PROTECTION AND PRIVACY ACT
    
    PART I - GENERAL PROVISIONS
    
    Section 1. Definitions
    For the purposes of this Act:
    (a) "Personal Data" means any information relating to an identified or identifiable natural person.
    (b) "Data Controller" means the entity that determines the purposes and means of processing personal data.
    (c) "Data Processor" means an entity that processes personal data on behalf of the controller.
    (d) "Data Subject" means the individual to whom personal data relates.
    (e) "Cross-border Transfer" means any transfer of personal data to a recipient in a different jurisdiction.
    (f) "Sensitive Data" means personal data revealing racial origin, political opinions, religious beliefs, health data, or biometric data.
    
    PART II - DATA USAGE REQUIREMENTS
    
    Section 2. Lawful Basis for Data Usage
    2.1 Personal data shall only be used when there is a lawful basis, including:
        (a) The data subject has given explicit consent
        (b) Processing is necessary for contract performance
        (c) Processing is required for legal compliance
        (d) Processing is necessary to protect vital interests
    
    2.2 Data controllers must not use personal data for purposes incompatible with those for which it was originally collected.
    
    2.3 Sensitive data shall not be processed unless:
        (a) The data subject has given explicit written consent
        (b) Processing is necessary for employment law obligations
        (c) Processing is necessary for healthcare purposes
    
    Section 3. Data Minimization and Purpose Limitation
    3.1 Data controllers must ensure that personal data usage is:
        (a) Adequate, relevant, and limited to what is necessary
        (b) Collected for specified, explicit, and legitimate purposes
        (c) Not further processed in a manner incompatible with those purposes
    
    PART III - DATA TRANSFER REGULATIONS
    
    Section 4. Intra-organizational Transfers
    4.1 Personal data may be transferred within an organization provided that:
        (a) Appropriate access controls are in place
        (b) The transfer is logged and auditable
        (c) Receiving departments have a legitimate need
    
    Section 5. Third-Party Data Transfers
    5.1 Data controllers shall not transfer personal data to third parties unless:
        (a) A data processing agreement is in place
        (b) The third party provides appropriate security guarantees
        (c) The data subject has been informed of the transfer
    
    5.2 Cross-border transfers of personal data are prohibited unless:
        (a) The recipient country ensures adequate protection
        (b) Appropriate safeguards are implemented, including:
            - Standard contractual clauses
            - Binding corporate rules
            - Approved certification mechanisms
        (c) The data subject has explicitly consented to the transfer
    
    Section 6. Data Transfer Security
    6.1 All data transfers must be encrypted using industry-standard encryption.
    6.2 Data controllers must maintain logs of all data transfers for at least 3 years.
    
    PART IV - DATA STORAGE REQUIREMENTS
    
    Section 7. Storage Duration and Retention
    7.1 Personal data shall not be stored longer than necessary for the purposes for which it was collected.
    7.2 Data retention periods must be defined and documented for each category of personal data.
    7.3 Upon expiration of the retention period, personal data must be securely deleted or anonymized.
    
    Section 8. Storage Security Requirements
    8.1 Data controllers must implement appropriate technical measures including:
        (a) Encryption of personal data at rest
        (b) Regular backups with tested recovery procedures
        (c) Physical security controls for data storage facilities
        (d) Logical access controls and authentication mechanisms
    
    8.2 Sensitive data must be stored:
        (a) In encrypted form using AES-256 or stronger encryption
        (b) With additional access controls and audit logging
        (c) Segregated from other personal data
    
    Section 9. Data Localization
    9.1 Health data and financial data must be stored within national borders.
    9.2 Backup copies may be stored internationally if encrypted and access-controlled.
    
    PART V - DATA ACCESS CONTROLS
    
    Section 10. Access Rights and Permissions
    10.1 Data controllers must implement role-based access controls ensuring:
        (a) Access is granted on a need-to-know basis
        (b) Privileged access is monitored and reviewed quarterly
        (c) Access permissions are revoked upon role change or termination
    
    10.2 Authentication requirements:
        (a) Multi-factor authentication for accessing sensitive data
        (b) Strong password policies enforced
        (c) Session timeouts after 15 minutes of inactivity
    
    Section 11. Data Subject Access Rights
    11.1 Data subjects have the right to:
        (a) Access their personal data within 30 days of request
        (b) Rectify inaccurate personal data
        (c) Request deletion of their personal data
        (d) Object to processing of their personal data
        (e) Request data portability in machine-readable format
    
    Section 12. Audit and Monitoring
    12.1 All access to personal data must be logged including:
        (a) User identity
        (b) Timestamp of access
        (c) Data accessed
        (d) Actions performed
    
    12.2 Access logs must be:
        (a) Retained for minimum 2 years
        (b) Protected from unauthorized modification
        (c) Regularly reviewed for anomalies
    
    PART VI - BREACH NOTIFICATION
    
    Section 13. Breach Response Requirements
    13.1 In case of a data breach, the data controller must:
        (a) Notify the supervisory authority within 72 hours
        (b) Notify affected data subjects without undue delay
        (c) Document the breach and remediation measures
    
    PART VII - PENALTIES
    
    Section 14. Administrative Penalties
    14.1 Violation of data usage requirements: Fine up to $1 million or 2% of annual revenue
    14.2 Unauthorized data transfer: Fine up to $2 million or 4% of annual revenue
    14.3 Inadequate storage security: Fine up to $500,000 per incident
    14.4 Access control violations: Fine up to $750,000 per violation
    14.5 Failure to notify breach: Fine up to $1.5 million
    """
    
    print("üöÄ Advanced Data Governance Rules Extractor")
    print(f"ü§ñ Using OpenAI {OPENAI_MODEL} with {REASONING_EFFORT} reasoning effort")
    print("üìä Framework: Chain of Thought + Mixture of Thought + Mixture of Reasoning")
    print("=" * 80)
    print()
    
    try:
        # Use the simple processing function
        results = process_legislation_simple(SAMPLE_LEGISLATION)
        
        # Phase 3: Synthesize if we have extracted rules
        total_extracted = sum(len(rules) for rules in results["extracted_rules"].values())
        if total_extracted > 0:
            print(f"\nüîÑ Phase 3: Synthesizing {total_extracted} extracted rules...")
            
            synthesized_json = synthesize_rules.invoke({
                "usage_rules": results["extracted_rules"]["data_usage"],
                "transfer_rules": results["extracted_rules"]["data_transfer"],
                "storage_rules": results["extracted_rules"]["data_storage"],
                "access_rules": results["extracted_rules"]["data_access"]
            })
            
            try:
                content = synthesized_json.strip()
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                if content.startswith('['):
                    results["synthesized_rules"] = json.loads(content)
                    print(f"   ‚úÖ Synthesized {len(results['synthesized_rules'])} rules")
                else:
                    print("   ‚ö†Ô∏è No synthesized rules generated")
            except Exception as e:
                print(f"   ‚ùå Error parsing synthesized rules: {e}")
        
        # Phase 4: Convert to JSON rules engine format
        if results["synthesized_rules"]:
            print(f"\n‚öôÔ∏è Phase 4: Converting to JSON Rules Engine format...")
            
            json_rules_content = convert_to_json_rules.invoke({
                "synthesized_rules": results["synthesized_rules"]
            })
            
            try:
                content = json_rules_content.strip()
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                if content.startswith('['):
                    results["json_rules"] = json.loads(content)
                    print(f"   ‚úÖ Converted {len(results['json_rules'])} JSON rules")
                else:
                    print("   ‚ö†Ô∏è No JSON rules generated")
            except Exception as e:
                print(f"   ‚ùå Error parsing JSON rules: {e}")
        
        # Phase 5: Validate
        if results["json_rules"]:
            print(f"\n‚úÖ Phase 5: Validating JSON rules...")
            
            validation_content = validate_json_rules.invoke({
                "json_rules": results["json_rules"]
            })
            
            try:
                content = validation_content.strip()
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                if content.startswith('{'):
                    results["validation"] = json.loads(content)
                    print(f"   ‚úÖ Validation complete")
                else:
                    print("   ‚ö†Ô∏è No validation results generated")
            except Exception as e:
                print(f"   ‚ùå Error parsing validation: {e}")
        
        # Print comprehensive summary
        print_results_summary(results)
        
        # Save results if we have JSON rules
        if results["json_rules"]:
            save_rules_to_file(results["json_rules"])
            
            print(f"\n‚ú® SUCCESS! Generated {len(results['json_rules'])} data governance rules!")
            print("\nüìÑ Sample JSON Rule:")
            sample_json = json.dumps(results["json_rules"][0], indent=2)
            if len(sample_json) > 800:
                print(sample_json[:800] + "...")
            else:
                print(sample_json)
        else:
            print("\n‚ö†Ô∏è No final JSON rules were generated. Check the processing steps above.")
        
        return results
            
    except Exception as e:
        print(f"\n‚ùå Error during processing: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


# ========================= Alternative: Using create_react_agent =========================

def create_simple_react_agent():
    """
    Alternative approach using create_react_agent for simpler tool calling.
    This often works better than custom state graphs.
    """
    from langgraph.prebuilt import create_react_agent
    
    # Get model
    model = get_model()
    
    # Create tools list
    tools = [
        analyze_with_reasoning,
        extract_data_rules,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    
    # Create agent with system prompt
    system_prompt = """You are an expert legal analyst specializing in data governance and privacy regulations.

Your task is to process legislation and extract data governance rules in json-rules-engine format.

Follow this systematic approach:

1. ANALYSIS PHASE: Call analyze_with_reasoning multiple times with different pathways:
   - analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="structural", reasoning_mode="deductive", focus_domain="data_usage")
   - analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="semantic", reasoning_mode="inductive", focus_domain="data_transfer")
   - analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="logical", reasoning_mode="abductive", focus_domain="data_storage")
   - analyze_with_reasoning(legislation_text=<text>, reasoning_pathway="contextual", reasoning_mode="analogical", focus_domain="data_access")

2. EXTRACTION PHASE: Call extract_data_rules for each domain:
   - extract_data_rules(analyzed_text=<legislation_text>, data_domain="data_usage")
   - extract_data_rules(analyzed_text=<legislation_text>, data_domain="data_transfer") 
   - extract_data_rules(analyzed_text=<legislation_text>, data_domain="data_storage")
   - extract_data_rules(analyzed_text=<legislation_text>, data_domain="data_access")

3. SYNTHESIS PHASE: Call synthesize_rules with all extracted rules

4. CONVERSION PHASE: Call convert_to_json_rules with synthesized rules

5. VALIDATION PHASE: Call validate_json_rules with the final JSON rules

Work systematically through each phase and call the tools as specified."""
    
    agent = create_react_agent(
        model=model,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent


def process_with_react_agent(legislation_text: str) -> Dict[str, Any]:
    """Process legislation using the simpler react agent approach"""
    
    agent = create_simple_react_agent()
    
    prompt = f"""
    Process this legislation to extract data governance rules using our systematic approach.
    
    LEGISLATION TEXT:
    {legislation_text}
    
    Follow the 5-phase process described in your system prompt. Start with the analysis phase and work through each step systematically.
    """
    
    # Configure with memory
    config = {"configurable": {"thread_id": "legislation_processing"}}
    
    try:
        # Invoke the agent
        result = agent.invoke(
            {"messages": [{"role": "user", "content": prompt}]},
            config=config
        )
        
        return {
            "status": "completed",
            "messages": result.get("messages", []),
            "agent_type": "react_agent"
        }
        
    except Exception as e:
        print(f"Error in react agent: {e}")
        return {"status": "error", "error": str(e)}


if __name__ == "__main__":
    # Set environment variable for API key if not set
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        exit(1)
    
    print("üöÄ Advanced Data Governance Rules Extractor")
    print(f"ü§ñ Using OpenAI {OPENAI_MODEL} with {REASONING_EFFORT} reasoning effort")
    print("üìä Framework: Chain of Thought + Mixture of Thought + Mixture of Reasoning")
    print("=" * 80)
    
    # Use the simple processing approach
    SAMPLE_LEGISLATION = """
    DATA PROTECTION AND PRIVACY ACT
    
    PART II - DATA USAGE REQUIREMENTS
    
    Section 2. Lawful Basis for Data Usage
    2.1 Personal data shall only be used when there is a lawful basis, including:
        (a) The data subject has given explicit consent
        (b) Processing is necessary for contract performance
        (c) Processing is required for legal compliance
    
    2.2 Data controllers must not use personal data for purposes incompatible with those for which it was originally collected.
    
    PART III - DATA TRANSFER REGULATIONS
    
    Section 5. Third-Party Data Transfers
    5.1 Data controllers shall not transfer personal data to third parties unless:
        (a) A data processing agreement is in place
        (b) The third party provides appropriate security guarantees
        (c) The data subject has been informed of the transfer
    
    5.2 Cross-border transfers of personal data are prohibited unless:
        (a) The recipient country ensures adequate protection
        (b) Appropriate safeguards are implemented
        (c) The data subject has explicitly consented to the transfer
    
    PART IV - DATA STORAGE REQUIREMENTS
    
    Section 7. Storage Duration and Retention
    7.1 Personal data shall not be stored longer than necessary for the purposes for which it was collected.
    7.2 Data retention periods must be defined and documented for each category of personal data.
    
    Section 8. Storage Security Requirements
    8.1 Data controllers must implement appropriate technical measures including:
        (a) Encryption of personal data at rest
        (b) Regular backups with tested recovery procedures
    
    PART V - DATA ACCESS CONTROLS
    
    Section 10. Access Rights and Permissions
    10.1 Data controllers must implement role-based access controls ensuring:
        (a) Access is granted on a need-to-know basis
        (b) Privileged access is monitored and reviewed quarterly
    
    10.2 Authentication requirements:
        (a) Multi-factor authentication for accessing sensitive data
        (b) Strong password policies enforced
    """
    
    try:
        # Try the simple direct processing first
        print("\nüîÑ Method 1: Direct tool calling...")
        results = process_legislation_simple(SAMPLE_LEGISLATION)
        
        if results.get("json_rules"):
            print(f"\n‚úÖ SUCCESS with direct approach!")
        else:
            print(f"\nüîÑ Method 2: Trying React Agent approach...")
            agent_result = process_with_react_agent(SAMPLE_LEGISLATION)
            print("React agent completed. Check messages for tool calls.")
            
            # Print the conversation
            messages = agent_result.get("messages", [])
            for i, msg in enumerate(messages):
                print(f"\nMessage {i+1} ({type(msg).__name__}):")
                if hasattr(msg, 'content'):
                    content = msg.content
                    if len(str(content)) > 500:
                        print(str(content)[:500] + "...")
                    else:
                        print(content)
                
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    print(f"  Tool calls: {len(msg.tool_calls)}")
                    for tool_call in msg.tool_calls:
                        print(f"    - {tool_call.get('name', 'unknown')}")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
