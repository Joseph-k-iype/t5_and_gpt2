"""
Integration Guide - How to integrate token caching throughout the application

This guide shows how to modify the existing code to use token caching for improved
performance across all LLM endpoints.
"""

# ============================================================
# 1. Modify settings.py - token caching in LLM client creation
# ============================================================

"""
Update the get_llm function in app/config/settings.py to use the cached token approach.
"""

def get_llm(proxy_enabled: Optional[bool] = None) -> AzureChatOpenAI:
    """
    Get the language model for the application with cached token authentication.
    
    Args:
        proxy_enabled: Override the PROXY_ENABLED setting in the config file
    
    Returns:
        AzureChatOpenAI: The language model
    """
    # Get environment with proxy setting override if provided
    env = get_os_env(proxy_enabled=proxy_enabled)
    
    logger.info(f"Setting up Azure OpenAI client with proxy enabled: {env.get('PROXY_ENABLED', 'True')}")
    
    try:
        # Get tenant, client and secret info for token acquisition
        tenant_id = env.get("AZURE_TENANT_ID", "")
        client_id = env.get("AZURE_CLIENT_ID", "")
        client_secret = env.get("AZURE_CLIENT_SECRET", "")
        
        # Get cached token (or new token if not in cache)
        from utils.auth_helper import get_azure_token_cached
        token = get_azure_token_cached(
            tenant_id=tenant_id,
            client_id=client_id, 
            client_secret=client_secret,
            scope="https://cognitiveservices.azure.com/.default"
        )
        
        if token:
            logger.info("Successfully obtained Azure token (using cache when available)")
            # Create a simple callable token provider
            token_provider = lambda: token
        else:
            logger.error("Failed to obtain Azure token")
            return MockLLM()
            
    except Exception as e:
        logger.error(f"Token acquisition failed: {e}")
        return MockLLM()
    
    # Get model configuration
    model_name = env.get("MODEL_NAME", "gpt-4o")
    temperature = float(env.get("TEMPERATURE", "0.3"))
    max_tokens = int(env.get("MAX_TOKENS", "2000"))
    api_version = env.get("API_VERSION", "2023-05-15")
    azure_endpoint = env.get("AZURE_ENDPOINT", "")
    
    logger.info(f"Using model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")
    
    # Create and return the LLM
    return AzureChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        azure_ad_token_provider=token_provider
    )


# ============================================================
# 2. Modify embedding.py - token caching in embedding client
# ============================================================

"""
Update the embedding client to use token caching.
Below is the relevant part of the _get_direct_azure_client method to modify.
"""

def _get_direct_azure_client(self):
    """Get the Azure OpenAI client for generating embeddings with token caching."""
    try:
        # Get tenant, client and secret info for token acquisition
        tenant_id = self.env.get("AZURE_TENANT_ID", "")
        client_id = self.env.get("AZURE_CLIENT_ID", "")
        client_secret = self.env.get("AZURE_CLIENT_SECRET", "")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        # Get cached token (or new token if not in cache)
        from utils.auth_helper import get_azure_token_cached
        token = get_azure_token_cached(
            tenant_id=tenant_id,
            client_id=client_id, 
            client_secret=client_secret,
            scope="https://cognitiveservices.azure.com/.default"
        )
        
        if token:
            logger.info("Successfully obtained Azure token for embeddings (using cache when available)")
            
            # Create client with token
            return AzureOpenAI(
                azure_endpoint=azure_endpoint,
                api_version=self.azure_api_version,
                api_key=token  # Use token as API key
            )
        else:
            logger.error("Failed to obtain Azure token for embeddings")
            raise ValueError("Failed to obtain Azure token")
            
    except Exception as e:
        logger.error(f"Error initializing Azure OpenAI client: {e}")
        raise


# ============================================================
# 3. Add token management functions to handle token refreshing
# ============================================================

"""
Add these functions to utils/auth_helper.py to provide more advanced token management.
"""

# Function to proactively refresh tokens before they expire
def refresh_token_if_needed(tenant_id: str, client_id: str, client_secret: str, 
                           scope: str = "https://cognitiveservices.azure.com/.default",
                           min_validity_seconds: int = 600) -> bool:
    """
    Check if a token is about to expire and refresh it if needed.
    
    Args:
        tenant_id: Azure tenant ID
        client_id: Azure client ID
        client_secret: Azure client secret
        scope: OAuth scope
        min_validity_seconds: Minimum seconds of validity required
        
    Returns:
        True if token was refreshed or is valid, False on error
    """
    # Get token from cache to check expiry
    cache_key = token_cache._get_cache_key(tenant_id, client_id, scope)
    
    with token_cache._lock:
        if cache_key in token_cache._tokens:
            _, expiry_time = token_cache._tokens[cache_key]
            time_left = expiry_time - time.time()
            
            # If token expires soon, refresh it
            if time_left < min_validity_seconds:
                logger.info(f"Token for {client_id[:8]}... expires in {time_left:.0f}s, refreshing")
                
                # Remove old token
                del token_cache._tokens[cache_key]
    
    # Get a fresh token (will update cache)
    token = get_azure_token_cached(tenant_id, client_id, client_secret, scope)
    return token is not None

# Function to start a background token refresh thread
def start_token_refresh_service(refresh_interval: int = 300) -> threading.Thread:
    """
    Start a background thread that refreshes tokens periodically.
    
    Args:
        refresh_interval: Interval between refresh checks in seconds
        
    Returns:
        The background thread
    """
    def _token_refresh_worker():
        env = get_os_env()
        tenant_id = env.get("AZURE_TENANT_ID", "")
        client_id = env.get("AZURE_CLIENT_ID", "")
        client_secret = env.get("AZURE_CLIENT_SECRET", "")
        
        while True:
            try:
                # Refresh the token if it's going to expire soon
                refresh_token_if_needed(
                    tenant_id=tenant_id,
                    client_id=client_id,
                    client_secret=client_secret
                )
            except Exception as e:
                logger.error(f"Error in token refresh worker: {e}")
            
            # Sleep for the specified interval
            time.sleep(refresh_interval)
    
    # Create and start the thread
    refresh_thread = threading.Thread(
        target=_token_refresh_worker,
        daemon=True,
        name="TokenRefreshThread"
    )
    refresh_thread.start()
    logger.info(f"Token refresh service started (interval: {refresh_interval}s)")
    
    return refresh_thread

# Global refresh thread reference
_token_refresh_thread = None

# Function to initialize token caching and refreshing
def initialize_token_caching(start_refresh_service: bool = True, 
                            refresh_interval: int = 300) -> None:
    """
    Initialize the token caching system and optionally start the refresh service.
    
    Args:
        start_refresh_service: Whether to start the token refresh service
        refresh_interval: Interval between refresh checks in seconds
    """
    global _token_refresh_thread
    
    # Ensure token cache is initialized
    _ = TokenCache()
    
    # Pre-cache a token for immediate use
    env = get_os_env()
    tenant_id = env.get("AZURE_TENANT_ID", "")
    client_id = env.get("AZURE_CLIENT_ID", "")
    client_secret = env.get("AZURE_CLIENT_SECRET", "")
    
    token = get_azure_token_cached(
        tenant_id=tenant_id,
        client_id=client_id,
        client_secret=client_secret
    )
    
    if token:
        logger.info("Initial token cached successfully")
    else:
        logger.warning("Failed to cache initial token")
    
    # Start the refresh service if requested
    if start_refresh_service and not _token_refresh_thread:
        _token_refresh_thread = start_token_refresh_service(refresh_interval)
