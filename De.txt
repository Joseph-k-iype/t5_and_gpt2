import os
import time
import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dotenv import dotenv_values
from azure.identity import ClientSecretCredential
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chromadb.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert string to boolean."""
    if s.lower() in ['true', '1', 't', 'y', 'yes']:
        return True
    elif s.lower() in ['false', '0', 'f', 'n', 'no']:
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

class OSEnv:
    """Enhanced environment manager with security features"""
    
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        """Initialize environment with configuration files and certificate."""
        self.var_list = []
        self.token = None
        self.token_expiry = None
        
        # Load configurations
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        
        # Set up certificates
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")
        
        # Configure proxy if enabled
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")
            
        # Set up Azure token if secure endpoints enabled
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints")
            self.token = self.get_azure_token()

    def set_certificate_path(self, certificate_path: str) -> None:
        """Set up certificate path for SSL verification."""
        try:
            if not os.path.isabs(certificate_path):
                certificate_path = os.path.abspath(certificate_path)
            
            if not is_file_readable(certificate_path):
                raise Exception("Certificate file missing or not readable")
            
            self.set("REQUESTS_CA_BUNDLE", certificate_path)
            self.set("SSL_CERT_FILE", certificate_path)
            self.set("CURL_CA_BUNDLE", certificate_path)
            
        except Exception as e:
            logger.error(f"Certificate configuration failed: {str(e)}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        """Read and set environment variables from a dotenv file."""
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
                
            if is_file_readable(dotenvfile):
                temp_dict = dotenv_values(dotenvfile)
                for k, v in temp_dict.items():
                    self.set(k, v, print_val)
                del temp_dict
        except Exception as e:
            logger.error(f"Failed to load environment file {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        """Set environment variable securely."""
        try:
            os.environ[var_name] = val
            if var_name not in self.var_list:
                self.var_list.append(var_name)
            if print_val and var_name not in ['AZURE_CLIENT_SECRET', 'AD_USER_PW']:
                logger.info(f"Set {var_name}={val}")
            elif print_val:
                logger.info(f"Set {var_name}=[HIDDEN]")
        except Exception as e:
            logger.error(f"Failed to set environment variable {var_name}: {str(e)}")
            raise

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        """Get environment variable safely."""
        try:
            return os.environ[var_name]
        except KeyError:
            logger.warning(f"Environment variable {var_name} not found")
            return default

    def set_proxy(self) -> None:
        """Set up proxy configuration with authentication."""
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Missing proxy credentials")
            
            proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            
            # Set no_proxy for Azure services
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains))
            logger.info("Proxy settings configured successfully")
            
        except Exception as e:
            logger.error(f"Proxy configuration failed: {str(e)}")
            raise

    def get_azure_token(self, force_refresh: bool = False) -> str:
        """Get Azure authentication token with automatic refresh."""
        try:
            current_time = time.time()
            
            # Check if we need to refresh the token
            if (force_refresh or 
                self.token is None or 
                self.token_expiry is None or 
                current_time >= self.token_expiry - 300):  # Refresh 5 minutes before expiry
                
                credential = ClientSecretCredential(
                    tenant_id=self.get("AZURE_TENANT_ID"),
                    client_id=self.get("AZURE_CLIENT_ID"),
                    client_secret=self.get("AZURE_CLIENT_SECRET")
                )
                
                token = credential.get_token("https://cognitiveservices.azure.com/.default")
                self.token = token.token
                self.token_expiry = current_time + token.expires_on
                self.set("AZURE_TOKEN", self.token, print_val=False)
                logger.info("Azure token refreshed successfully")
                
            return self.token
            
        except Exception as e:
            logger.error(f"Failed to get Azure token: {str(e)}")
            raise

    def list_env_vars(self) -> None:
        """List all environment variables set by this class."""
        sensitive_vars = {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}
        for var in self.var_list:
            if var in sensitive_vars:
                logger.info(f"{var}: [HIDDEN]")
            else:
                logger.info(f"{var}: {self.get(var)}")

class ChromaVectorStore:
    """ChromaDB vector store manager with Azure OpenAI integration"""
    
    def __init__(self, env: OSEnv):
        """Initialize the vector store with environment configuration."""
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None
        logger.info("ChromaVectorStore initialized successfully")

    def _validate_azure_credentials(self) -> None:
        """Validate all required Azure credentials are present."""
        required_vars = [
            "AZURE_TENANT_ID",
            "AZURE_CLIENT_ID",
            "AZURE_CLIENT_SECRET",
            "AZURE_EMBEDDING_DEPLOYMENT",
            "AZURE_EMBEDDING_MODEL",
            "AZURE_API_VERSION",
            "AZURE_OPENAI_ENDPOINT"
        ]
        
        missing = [var for var in required_vars if not self.env.get(var)]
        if missing:
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")
        logger.info("Azure credentials validated successfully")

    def _init_embeddings(self) -> AzureOpenAIEmbeddings:
        """Initialize Azure OpenAI embeddings with Azure AD authentication."""
        try:
            # Ensure we have a fresh token
            token = self.env.get_azure_token(force_refresh=True)
            
            embeddings = AzureOpenAIEmbeddings(
                deployment=str(self.env.get("AZURE_EMBEDDING_DEPLOYMENT")),
                model=str(self.env.get("AZURE_EMBEDDING_MODEL")),
                api_version=str(self.env.get("AZURE_API_VERSION")),
                azure_endpoint=str(self.env.get("AZURE_OPENAI_ENDPOINT")),
                azure_ad_token=token
            )
            logger.info("Azure OpenAI embeddings initialized successfully")
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to initialize Azure OpenAI embeddings: {str(e)}")
            raise

    def create_collection(self, csv_path: Path, text_column: str, 
                        chunk_size: int = 1000, chunk_overlap: int = 100) -> None:
        """Create Chroma collection from CSV data with chunking support."""
        try:
            df = pd.read_csv(csv_path)
            if text_column not in df.columns:
                raise ValueError(f"Text column '{text_column}' not found")
            
            logger.info(f"Processing CSV file: {csv_path}")
            logger.info(f"Total rows in CSV: {len(df)}")
            
            documents, metadatas = self._process_csv(df, text_column, chunk_size, chunk_overlap)
            self.collection_name = csv_path.stem.lower()
            
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                ids=[str(i) for i in range(len(documents))],
                metadatas=metadatas,
                persist_directory=persist_directory
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(documents)} documents")
            self.vector_store.persist()
            logger.info("Collection persisted to disk")
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv(self, df: pd.DataFrame, text_column: str,
                    chunk_size: int = 1000, chunk_overlap: int = 100) -> Tuple[List[Document], List[Dict]]:
        """Process CSV data into Chroma documents and metadata with text chunking."""
        documents = []
        metadatas = []
        
        try:
            for idx, row in df.iterrows():
                text = str(row[text_column])
                if not text.strip():
                    logger.warning(f"Empty text found in row {idx}, skipping")
                    continue
                
                # Create metadata excluding the text column
                metadata = {
                    col: str(row[col]) 
                    for col in df.columns 
                    if col != text_column and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Simple text chunking if text is too long
                if len(text) > chunk_size:
                    chunks = self._chunk_text(text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                        metadatas.append(chunk_metadata)
                else:
                    documents.append(Document(page_content=text, metadata=metadata))
                    metadatas.append(metadata)
                
                if (idx + 1) % 1000 == 0:
                    logger.info(f"Processed {idx + 1} rows")
            
            return documents, metadatas
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Split text into overlapping chunks."""
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            
            # Adjust chunk to not break words
            if end < text_len:
                last_space = chunk.rfind(' ')
                if last_space != -1:
                    end = start + last_space + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
            
        return chunks

    def search(self, query: str, k: int = 5, 
              min_relevance_score: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """Perform similarity search with metadata and relevance filtering."""
        try:
            if self.vector_store is None:
                raise ValueError("No collection loaded. Please create or load a collection first.")
                
            logger.info(f"Performing search with query: {query}")
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            # Process and filter results
            processed_results = []
            for doc, score in results:
                relevance = 1 - score  # Convert distance to similarity score
                if relevance >= min_relevance_score:
                    processed_results.append((doc.page_content, relevance, doc.metadata))
            
            logger.info(f"Found {len(processed_results)} relevant results")
            return processed_results
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            raise

    def load_existing_collection(self, collection_name: str) -> None:
        """Load an existing Chroma collection."""
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            if not os.path.exists(persist_directory):
                raise ValueError(f"Persist directory not found: {persist_directory}")
            
            self.collection_name = collection_name
            self.vector_store = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=persist_directory
            )
            
            collection_count = len(self.vector_store.get()['ids'])
            logger.info(f"Loaded existing collection '{collection_name}' with {collection_count} documents")
            
        except Exception as e:
            logger.error(f"Failed to load collection: {str(e)}")
            raise

    def delete_collection(self, collection_name: str) -> None:
        """Delete a Chroma collection."""
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            
            # Create a temporary client to delete the collection
            temp_store = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=persist_directory
            )
            
            temp_store.delete_collection()
            logger.info(f"Deleted collection '{collection_name}'")
            
            # Reset current store if it was the active collection
            if self.collection_name == collection_name:
                self.vector_store = None
                self.collection_name = None
                
        except Exception as e:
            logger.error(f"Failed to delete collection: {str(e)}")
            raise

    def list_collections(self) -> List[str]:
        """List all available collections in the persist directory."""
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            if not os.path.exists(persist_directory):
                logger.info("No persist directory found")
                return []
            
            # Get all subdirectories that contain a chroma.sqlite3 file
            collections = []
            for item in os.listdir(persist_directory):
                item_path = os.path.join(persist_directory, item)
                if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, 'chroma.sqlite3')):
                    collections.append(item)
            
            logger.info(f"Found {len(collections)} collections: {collections}")
            return collections
            
        except Exception as e:
            logger.error(f"Failed to list collections: {str(e)}")
            raise

    def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about the current collection."""
        try:
            if self.vector_store is None:
                raise ValueError("No collection loaded")
            
            collection_data = self.vector_store.get()
            
            # Calculate statistics
            stats = {
                "collection_name": self.collection_name,
                "document_count": len(collection_data['ids']),
                "metadata_fields": set().union(*(m.keys() for m in collection_data['metadatas'] if m)),
                "embedding_dimension": len(collection_data['embeddings'][0]) if collection_data['embeddings'] else None,
            }
            
            logger.info(f"Collection statistics: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get collection statistics: {str(e)}")
            raise

    def close(self) -> None:
        """Clean up resources and persist data."""
        try:
            if self.vector_store:
                self.vector_store.persist()
                logger.info("Vector store persisted successfully")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")
            raise

def main():
    """Main application entry point."""
    chroma_db = None
    
    try:
        # Get the base directory for environment files
        base_dir = Path(__file__).resolve().parent.parent
        env_dir = base_dir / "env"
        
        # Define paths
        config_path = env_dir / "config.env"
        creds_path = env_dir / "credentials.env"
        cert_path = env_dir / "cacert.pem"
        
        # Validate required files exist
        required_files = {
            'config.env': config_path,
            'credentials.env': creds_path,
            'cacert.pem': cert_path
        }
        
        missing_files = [
            name for name, path in required_files.items() 
            if not path.exists()
        ]
        
        if missing_files:
            raise FileNotFoundError(
                f"Missing required files in {env_dir}:\n" +
                "\n".join(f"- {f}" for f in missing_files)
            )
        
        # Initialize environment and vector store
        logger.info("Initializing environment...")
        env = OSEnv(
            config_file=str(config_path),
            creds_file=str(creds_path),
            certificate_path=str(cert_path)
        )
        
        logger.info("Initializing ChromaDB vector store...")
        chroma_db = ChromaVectorStore(env)
        
        while True:
            print("\nAvailable commands:")
            print("1. Create new collection from CSV")
            print("2. Load existing collection")
            print("3. List all collections")
            print("4. Delete collection")
            print("5. Search current collection")
            print("6. Show collection statistics")
            print("7. Show environment variables")
            print("8. Exit")
            
            choice = input("\nEnter your choice (1-8): ").strip()
            
            try:
                if choice == '1':
                    # Create new collection
                    csv_path = Path(input("Enter CSV file path: ").strip())
                    print("\nAvailable columns:")
                    df = pd.read_csv(csv_path)
                    for idx, col in enumerate(df.columns, 1):
                        print(f"{idx}. {col}")
                    
                    col_idx = int(input("\nEnter text column number: ")) - 1
                    text_col = df.columns[col_idx]
                    
                    chunk_size = int(input("Enter chunk size (default 1000): ") or "1000")
                    chunk_overlap = int(input("Enter chunk overlap (default 100): ") or "100")
                    
                    chroma_db.create_collection(csv_path, text_col, chunk_size, chunk_overlap)
                    
                elif choice == '2':
                    # Load existing collection
                    collections = chroma_db.list_collections()
                    if not collections:
                        print("No collections found")
                        continue
                        
                    print("\nAvailable collections:")
                    for idx, name in enumerate(collections, 1):
                        print(f"{idx}. {name}")
                    
                    col_idx = int(input("\nEnter collection number: ")) - 1
                    chroma_db.load_existing_collection(collections[col_idx])
                    
                elif choice == '3':
                    # List collections
                    collections = chroma_db.list_collections()
                    if collections:
                        print("\nAvailable collections:")
                        for name in collections:
                            print(f"- {name}")
                    else:
                        print("No collections found")
                    
                elif choice == '4':
                    # Delete collection
                    collections = chroma_db.list_collections()
                    if not collections:
                        print("No collections found")
                        continue
                        
                    print("\nAvailable collections:")
                    for idx, name in enumerate(collections, 1):
                        print(f"{idx}. {name}")
                    
                    col_idx = int(input("\nEnter collection number to delete: ")) - 1
                    confirm = input(f"Are you sure you want to delete '{collections[col_idx]}'? (yes/no): ")
                    if confirm.lower() == 'yes':
                        chroma_db.delete_collection(collections[col_idx])
                    
                elif choice == '5':
                    # Search collection
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    query = input("Enter search query: ")
                    k = int(input("Number of results to return (default 5): ") or "5")
                    min_score = float(input("Minimum relevance score (0-1, default 0): ") or "0")
                    
                    results = chroma_db.search(query, k, min_score)
                    print(f"\nFound {len(results)} results:")
                    for i, (text, score, metadata) in enumerate(results, 1):
                        print(f"\n{i}. Relevance Score: {score:.4f}")
                        print(f"Text: {text[:200]}...")
                        print("Metadata:")
                        for key, value in metadata.items():
                            print(f"  {key}: {value}")
                    
                elif choice == '6':
                    # Show collection statistics
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    stats = chroma_db.get_collection_stats()
                    print("\nCollection Statistics:")
                    for key, value in stats.items():
                        print(f"{key}: {value}")
                    
                elif choice == '7':
                    # Show environment variables
                    env.list_env_vars()
                    
                elif choice == '8':
                    print("Exiting...")
                    break
                    
                else:
                    print("Invalid choice")
                
            except Exception as e:
                logger.error(f"Operation failed: {str(e)}")
                print(f"Error: {str(e)}")
                
    except Exception as e:
        logger.error(f"Application error: {str(e)}")
        print(f"Error: {str(e)}")
        
    finally:
        if chroma_db is not None:
            try:
                chroma_db.close()
                logger.info("Application shutdown complete")
            except Exception as e:
                logger.error(f"Error during shutdown: {str(e)}")

if __name__ == "__main__":
    main()
