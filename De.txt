"""
Enhanced Deep Research Agent with Multi-Agent Architecture, LangMem Long-Term Memory, and Advanced Token Management
Fixed and Updated for 2025 - Compatible with latest LangChain, LangGraph, LangMem, and OpenAI APIs

Key Features:
- Multi-agent research architecture with specialized agents
- LangMem SDK for cross-session long-term memory
- LangGraph workflow orchestration
- Elasticsearch integration for document retrieval
- Advanced token management and progressive synthesis
- Domain filtering with LLM-based classification
- Cross-jurisdictional privacy law research capabilities
"""

import asyncio
import json
import logging
import os
import uuid
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Union
from dataclasses import dataclass, field
import re
from collections import defaultdict

# Core imports
import openai
import tiktoken
from elasticsearch import Elasticsearch
import ssl

# LangChain imports (corrected for current version)
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig

# LangChain integration packages
try:
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    LANGCHAIN_OPENAI_AVAILABLE = True
except ImportError:
    LANGCHAIN_OPENAI_AVAILABLE = False
    print("Warning: langchain-openai not available. Install with: pip install langchain-openai")

try:
    from langchain_elasticsearch import ElasticsearchStore
    from langchain_elasticsearch.vectorstores import DenseVectorStrategy
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_LANGCHAIN_AVAILABLE = False
    print("Warning: langchain-elasticsearch not available. Install with: pip install langchain-elasticsearch")

# LangGraph imports (corrected)
try:
    from langgraph.graph import StateGraph, START, END
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.store.memory import InMemoryStore
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("Warning: langgraph not available. Install with: pip install langgraph")

# LangMem imports (corrected)
try:
    from langmem import create_manage_memory_tool, create_search_memory_tool
    LANGMEM_AVAILABLE = True
except ImportError:
    LANGMEM_AVAILABLE = False
    print("Warning: langmem not available. Install with: pip install langmem")

# Pydantic
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================
# CONFIGURATION
# ============================

class Config:
    """Configuration for the enhanced deep research agent"""
    
    def __init__(self):
        # API Configuration
        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
        
        # Elasticsearch Configuration
        self.ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
        self.ES_PASSWORD = os.getenv("ES_PASSWORD", "your-elasticsearch-password-here")
        self.ES_HOST = os.getenv("ES_HOST", "localhost")
        self.ES_PORT = int(os.getenv("ES_PORT", "9200"))
        
        # Model Configuration - ONLY o3-mini and text-embedding-3-large
        self.MODEL = "o3-mini-2025-01-31"
        self.EMBEDDING_MODEL = "text-embedding-3-large"
        self.EMBEDDING_DIMENSIONS = 3072
        
        # Tiktoken Local Configuration
        self.TIKTOKEN_MODEL_PATH = os.getenv("TIKTOKEN_MODEL_PATH", "tiktoken_model")
        
        # Research Configuration
        self.MAX_RESEARCH_ITERATIONS = 3
        self.MAX_SEARCH_RESULTS = 10
        self.CONFIDENCE_THRESHOLD = 0.7
        
        # Token Management
        self.MAX_RESPONSE_TOKENS = 8000
        self.MAX_CONTEXT_TOKENS = 12000
        
        # Memory Configuration
        self.MEMORY_NAMESPACE = ["research", "privacy_law"]
        
        # Setup tiktoken local cache
        self._setup_tiktoken_cache()
    
    def _setup_tiktoken_cache(self):
        """Setup tiktoken to use local cache directory"""
        try:
            # Set tiktoken cache directory to use local models
            if os.path.exists(self.TIKTOKEN_MODEL_PATH):
                os.environ["TIKTOKEN_CACHE_DIR"] = os.path.abspath(self.TIKTOKEN_MODEL_PATH)
                logger.info(f"✓ Tiktoken cache directory set to: {self.TIKTOKEN_MODEL_PATH}")
            else:
                logger.warning(f"⚠️ Tiktoken model directory not found: {self.TIKTOKEN_MODEL_PATH}")
                logger.info("Creating tiktoken_model directory...")
                os.makedirs(self.TIKTOKEN_MODEL_PATH, exist_ok=True)
                os.environ["TIKTOKEN_CACHE_DIR"] = os.path.abspath(self.TIKTOKEN_MODEL_PATH)
        except Exception as e:
            logger.error(f"Error setting up tiktoken cache: {e}")

# Global config instance
config = Config()

# ============================
# UTILITY FUNCTIONS
# ============================

def safe_json_parse(json_string: str, fallback_value: Any = None) -> Any:
    """Safely parse JSON with fallback handling"""
    if not json_string or not isinstance(json_string, str):
        return fallback_value
    
    json_string = json_string.strip()
    
    # Extract JSON from markdown code blocks
    if "```json" in json_string:
        start_idx = json_string.find("```json") + 7
        end_idx = json_string.find("```", start_idx)
        if end_idx > start_idx:
            json_string = json_string[start_idx:end_idx].strip()
    
    try:
        return json.loads(json_string)
    except json.JSONDecodeError:
        try:
            # Try with single quotes replaced
            return json.loads(json_string.replace("'", '"'))
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse JSON: {json_string[:200]}...")
            return fallback_value

# ============================
# TOKEN MANAGEMENT
# ============================

class TokenManager:
    """Enhanced token management using local tiktoken models"""
    
    def __init__(self, model_name: str = "o3-mini-2025-01-31"):
        self.model_name = model_name
        self.encoder = self._get_encoder()
    
    def _get_encoder(self):
        """Get tiktoken encoder from local cache directory"""
        try:
            # Ensure tiktoken cache directory is set
            if "TIKTOKEN_CACHE_DIR" not in os.environ:
                os.environ["TIKTOKEN_CACHE_DIR"] = os.path.abspath(config.TIKTOKEN_MODEL_PATH)
            
            logger.info(f"Loading tiktoken encoder for {self.model_name} from local cache")
            
            # Try to load encoder for o3-mini model (should use o200k_base encoding)
            if "o3" in self.model_name.lower():
                try:
                    encoder = tiktoken.get_encoding("o200k_base")
                    logger.info("✓ Successfully loaded o200k_base encoding for o3-mini")
                    return encoder
                except Exception as e:
                    logger.warning(f"Failed to load o200k_base: {e}")
            
            # Fallback to cl100k_base
            try:
                encoder = tiktoken.get_encoding("cl100k_base")
                logger.info("✓ Successfully loaded cl100k_base encoding as fallback")
                return encoder
            except Exception as e:
                logger.warning(f"Failed to load cl100k_base: {e}")
            
            # Final fallback to encoding_for_model
            try:
                encoder = tiktoken.encoding_for_model("gpt-4")
                logger.info("✓ Successfully loaded encoding for gpt-4 as final fallback")
                return encoder
            except Exception as e:
                logger.error(f"All tiktoken loading methods failed: {e}")
                return self._create_simple_fallback()
                
        except Exception as e:
            logger.error(f"Error loading tiktoken encoder: {e}")
            return self._create_simple_fallback()
    
    def _create_simple_fallback(self):
        """Create simple fallback tokenizer when tiktoken fails"""
        logger.warning("Using simple character-based fallback tokenizer")
        
        class SimpleFallbackTokenizer:
            def encode(self, text):
                if not text:
                    return []
                # Simple approximation: ~4 characters per token for English text
                estimated_tokens = max(1, len(str(text)) // 4)
                return list(range(estimated_tokens))
            
            def decode(self, tokens):
                return f"[{len(tokens)} tokens]"
        
        return SimpleFallbackTokenizer()
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        if not text:
            return 0
        try:
            return len(self.encoder.encode(str(text)))
        except Exception as e:
            logger.warning(f"Token counting error: {e}")
            return len(str(text)) // 4  # Rough estimate
    
    def truncate_to_limit(self, text: str, limit: int) -> str:
        """Truncate text to token limit"""
        if not text:
            return ""
        
        current_tokens = self.count_tokens(text)
        if current_tokens <= limit:
            return text
        
        # Binary search for optimal truncation
        words = text.split()
        left, right = 0, len(words)
        
        while left < right:
            mid = (left + right + 1) // 2
            truncated = " ".join(words[:mid])
            if self.count_tokens(truncated) <= limit:
                left = mid
            else:
                right = mid - 1
        
        return " ".join(words[:left])

# ============================
# OPENAI CLIENT
# ============================

class OpenAIManager:
    """OpenAI API manager using only o3-mini model without max_tokens/temperature"""
    
    def __init__(self):
        self.client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
        self.token_manager = TokenManager()
        self._test_connection()
    
    def _test_connection(self):
        """Test OpenAI connection with o3-mini"""
        try:
            response = self.client.chat.completions.create(
                model=config.MODEL,
                messages=[{"role": "user", "content": "test"}]
            )
            logger.info(f"✓ OpenAI connection successful with {config.MODEL}")
        except Exception as e:
            logger.error(f"OpenAI connection failed with {config.MODEL}: {e}")
            raise
    
    async def chat_completion(self, messages: List[Dict], **kwargs) -> str:
        """Create chat completion with o3-mini (no max_tokens or temperature)"""
        try:
            # Remove any max_tokens or temperature parameters if passed
            clean_kwargs = {k: v for k, v in kwargs.items() 
                          if k not in ['max_tokens', 'temperature']}
            
            response = self.client.chat.completions.create(
                model=config.MODEL,
                messages=messages,
                **clean_kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Chat completion error with {config.MODEL}: {e}")
            raise
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using text-embedding-3-large"""
        try:
            response = self.client.embeddings.create(
                model=config.EMBEDDING_MODEL,
                input=text.strip()[:8000],  # Limit input length
                dimensions=config.EMBEDDING_DIMENSIONS
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding error with {config.EMBEDDING_MODEL}: {e}")
            raise

# ============================
# ELASTICSEARCH MANAGER
# ============================

class ElasticsearchManager:
    """Elasticsearch manager for document retrieval"""
    
    def __init__(self, openai_manager: OpenAIManager):
        self.openai_manager = openai_manager
        self.client = self._create_client()
        self.store = None
        if ELASTICSEARCH_LANGCHAIN_AVAILABLE:
            self._setup_store()
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client"""
        try:
            # Create SSL context
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            client = Elasticsearch(
                [{"host": config.ES_HOST, "port": config.ES_PORT, "scheme": "https"}],
                basic_auth=(config.ES_USERNAME, config.ES_PASSWORD),
                ssl_context=ssl_context,
                verify_certs=False,
                request_timeout=30
            )
            
            # Test connection
            info = client.info()
            logger.info(f"✓ Connected to Elasticsearch: {info.get('version', {}).get('number', 'unknown')}")
            return client
            
        except Exception as e:
            logger.error(f"Failed to create Elasticsearch client: {e}")
            raise
    
    def _setup_store(self):
        """Setup LangChain Elasticsearch store"""
        if not ELASTICSEARCH_LANGCHAIN_AVAILABLE:
            return
        
        try:
            # Create embeddings wrapper
            class DirectOpenAIEmbeddings(Embeddings):
                def __init__(self, openai_manager):
                    self.openai_manager = openai_manager
                
                def embed_documents(self, texts: List[str]) -> List[List[float]]:
                    return asyncio.run(self._aembed_documents(texts))
                
                def embed_query(self, text: str) -> List[float]:
                    return asyncio.run(self.openai_manager.create_embedding(text))
                
                async def _aembed_documents(self, texts: List[str]) -> List[List[float]]:
                    embeddings = []
                    for text in texts:
                        embedding = await self.openai_manager.create_embedding(text)
                        embeddings.append(embedding)
                    return embeddings
            
            embeddings = DirectOpenAIEmbeddings(self.openai_manager)
            
            self.store = ElasticsearchStore(
                es_connection=self.client,
                index_name="privacy_research",
                embedding=embeddings,
                strategy=DenseVectorStrategy()
            )
            
            logger.info("✓ Elasticsearch store initialized")
            
        except Exception as e:
            logger.error(f"Error setting up Elasticsearch store: {e}")
            self.store = None
    
    async def search_documents(self, query: str, limit: int = 10) -> List[Document]:
        """Search for documents"""
        if not self.store:
            logger.warning("Elasticsearch store not available")
            return []
        
        try:
            results = self.store.similarity_search(query, k=limit)
            return results
        except Exception as e:
            logger.error(f"Search error: {e}")
            return []

# ============================
# MEMORY MANAGEMENT (RESTORED)
# ============================

class OptimizedMemoryManager:
    """Advanced memory manager using LangMem for long-term memory with token optimization"""
    
    def __init__(self, openai_manager: OpenAIManager):
        self.openai_manager = openai_manager
        self.memory_manager = None
        self.store = None
        self.memory_tools = []
        
        # Initialize store first
        if LANGGRAPH_AVAILABLE:
            self._setup_store()
        else:
            # Simple dict-based store when LangGraph is not available
            self.store = SimpleMemoryStore()
            
        self.token_manager = TokenManager()
        self.max_memory_tokens = config.MAX_CONTEXT_TOKENS
        
        if LANGMEM_AVAILABLE:
            try:
                self._setup_langmem()
                logger.info("✓ LangMem memory manager initialized")
            except Exception as e:
                logger.warning(f"Failed to setup LangMem: {e}")
        else:
            logger.info("LangMem not available - using basic memory store")
    
    def _setup_store(self):
        """Setup LangGraph memory store"""
        try:
            self.store = InMemoryStore(
                index={
                    "dims": config.EMBEDDING_DIMENSIONS,  # 3072 for text-embedding-3-large
                    "embed": f"openai:{config.EMBEDDING_MODEL}"
                }
            )
            logger.info(f"✓ Memory store initialized with {config.EMBEDDING_DIMENSIONS} dimensions")
        except Exception as e:
            logger.error(f"Error setting up memory store: {e}")
    
    def _setup_langmem(self):
        """Setup LangMem memory manager and tools"""
        if not LANGMEM_AVAILABLE:
            return
        
        try:
            # Create memory tools for agent interaction
            self.manage_memory_tool = create_manage_memory_tool(namespace=config.MEMORY_NAMESPACE)
            self.search_memory_tool = create_search_memory_tool(namespace=config.MEMORY_NAMESPACE)
            
            self.memory_tools = [self.manage_memory_tool, self.search_memory_tool]
            logger.info("✓ LangMem memory tools created")
            
        except Exception as e:
            logger.error(f"Error setting up LangMem: {e}")
            raise
    
    async def store_research_findings(self, user_id: str, session_id: str, 
                                    findings: List[Dict], context: Dict) -> List[str]:
        """Store research findings with token optimization"""
        try:
            if not findings:
                return []
            
            # Prioritize findings
            prioritizer = ContentPrioritizer(self.openai_manager)
            prioritized_findings = await prioritizer.prioritize_findings(findings, context.get('query', ''))
            
            memory_keys = []
            
            # Store top findings with token management
            for i, finding in enumerate(prioritized_findings[:20]):  # Limit to top 20
                try:
                    # Optimize finding content
                    optimized_finding = self._optimize_finding_for_storage(finding)
                    
                    memory_content = {
                        "finding": optimized_finding,
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat(),
                        "priority": finding.get('priority_score', 0.5),
                        "query_context": context.get('query', '')[:200]  # Truncate context
                    }
                    
                    # Check token limit
                    content_tokens = self.token_manager.count_tokens(json.dumps(memory_content))
                    if content_tokens > self.max_memory_tokens:
                        # Truncate content
                        memory_content['finding'] = self.token_manager.truncate_to_limit(
                            str(memory_content['finding']), self.max_memory_tokens - 500
                        )
                    
                    # Store in memory
                    memory_key = f"finding_{session_id}_{i}"
                    namespace = [config.MEMORY_NAMESPACE[0], user_id, "research_findings"]
                    
                    await self.store.aput(namespace, memory_key, memory_content)
                    memory_keys.append(memory_key)
                    
                except Exception as e:
                    logger.error(f"Error storing individual finding: {e}")
                    continue
            
            return memory_keys
            
        except Exception as e:
            logger.error(f"Error storing research findings: {e}")
            return []
    
    def _optimize_finding_for_storage(self, finding: Dict) -> Dict:
        """Optimize finding for storage with token limits"""
        try:
            optimized = {}
            
            # Extract key fields with truncation
            content = finding.get('content', '') or finding.get('insight', '')
            optimized['content'] = self.token_manager.truncate_to_limit(content, 2000)
            
            # Keep essential metadata
            for key in ['jurisdiction', 'confidence', 'regulation', 'implication']:
                if key in finding:
                    value = finding[key]
                    if isinstance(value, str):
                        optimized[key] = self.token_manager.truncate_to_limit(value, 500)
                    else:
                        optimized[key] = value
            
            return optimized
            
        except Exception as e:
            logger.error(f"Error optimizing finding: {e}")
            return finding
    
    async def retrieve_relevant_memories(self, user_id: str, query: str, 
                                       limit: int = 10) -> List[Dict]:
        """Retrieve relevant memories for current research"""
        try:
            namespace = [config.MEMORY_NAMESPACE[0], user_id]
            
            # Basic memory retrieval
            memories = []
            try:
                stored_items = await self.store.asearch(namespace, query=query, limit=limit)
                if hasattr(stored_items[0] if stored_items else None, 'value'):
                    memories = [item.value for item in stored_items]
                else:
                    # For simple memory store
                    memories = stored_items
            except Exception as e:
                logger.warning(f"Memory search failed: {e}")
            
            return memories
            
        except Exception as e:
            logger.error(f"Error retrieving memories: {e}")
            return []
    
    async def store_user_preferences(self, user_id: str, preferences: Dict) -> str:
        """Store user research preferences"""
        try:
            namespace = [config.MEMORY_NAMESPACE[0], user_id, "preferences"]
            memory_key = "user_preferences"
            
            # Optimize preferences for storage
            optimized_preferences = {}
            for key, value in preferences.items():
                if isinstance(value, str):
                    optimized_preferences[key] = self.token_manager.truncate_to_limit(value, 200)
                else:
                    optimized_preferences[key] = value
            
            await self.store.aput(namespace, memory_key, optimized_preferences)
            return memory_key
            
        except Exception as e:
            logger.error(f"Error storing user preferences: {e}")
            return ""
    
    async def store_memory(self, namespace: List[str], key: str, value: Dict) -> bool:
        """Store memory item"""
        if not self.store:
            return False
        
        try:
            await self.store.aput(namespace, key, value)
            return True
        except Exception as e:
            logger.error(f"Memory storage error: {e}")
            return False
    
    async def retrieve_memory(self, namespace: List[str], query: str, limit: int = 5) -> List[Dict]:
        """Retrieve relevant memories"""
        if not self.store:
            return []
        
        try:
            results = await self.store.asearch(namespace, query=query, limit=limit)
            return [item.value for item in results if hasattr(item, 'value')]
        except Exception as e:
            logger.error(f"Memory retrieval error: {e}")
            return []
    
    def get_memory_tools(self) -> List:
        """Get memory tools for agent use"""
        return self.memory_tools


class SimpleMemoryStore:
    """Simple memory store implementation when LangGraph is not available"""
    
    def __init__(self):
        self.data = {}
    
    async def aput(self, namespace, key, value):
        """Store a value"""
        ns_key = "/".join(str(x) for x in namespace) if isinstance(namespace, (list, tuple)) else str(namespace)
        full_key = f"{ns_key}/{key}"
        self.data[full_key] = value
    
    async def aget(self, namespace, key):
        """Get a value"""
        ns_key = "/".join(str(x) for x in namespace) if isinstance(namespace, (list, tuple)) else str(namespace)
        full_key = f"{ns_key}/{key}"
        return self.data.get(full_key)
    
    async def asearch(self, namespace, query=None, limit=10):
        """Search for values"""
        ns_key = "/".join(str(x) for x in namespace) if isinstance(namespace, (list, tuple)) else str(namespace)
        results = []
        
        for key, value in self.data.items():
            if key.startswith(f"{ns_key}/"):
                results.append(value)
                if len(results) >= limit:
                    break
        
        return results


class ContentPrioritizer:
    """Prioritize content based on relevance and quality"""
    
    def __init__(self, openai_manager):
        self.openai_manager = openai_manager
    
    async def prioritize_findings(self, findings: List[Dict], query: str) -> List[Dict]:
        """Prioritize findings by relevance and quality"""
        try:
            prioritized = []
            
            for finding in findings:
                priority_score = await self._calculate_priority(finding, query)
                finding_copy = finding.copy()
                finding_copy['priority_score'] = priority_score
                prioritized.append(finding_copy)
            
            # Sort by priority score (descending)
            prioritized.sort(key=lambda x: x.get('priority_score', 0), reverse=True)
            return prioritized
            
        except Exception as e:
            logger.error(f"Error prioritizing findings: {e}")
            return findings
    
    async def _calculate_priority(self, finding: Dict, query: str) -> float:
        """Calculate priority score for a finding"""
        try:
            # Base score from confidence
            base_score = finding.get('confidence', 0.5)
            
            # Relevance bonus (keyword matching)
            content = finding.get('content', '') or finding.get('insight', '')
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            relevance_bonus = len(query_words.intersection(content_words)) / len(query_words) if query_words else 0
            
            # Length penalty for very long findings
            content_length = len(content)
            length_penalty = 0.1 if content_length > 2000 else 0
            
            # Jurisdiction bonus
            jurisdiction_bonus = 0.1 if finding.get('jurisdiction') else 0
            
            # Regulation specificity bonus
            regulation_bonus = 0.1 if finding.get('regulation') else 0
            
            # Final score
            priority_score = (base_score + (relevance_bonus * 0.3) + 
                            jurisdiction_bonus + regulation_bonus - length_penalty)
            
            return min(max(priority_score, 0), 1.0)
            
        except Exception as e:
            logger.error(f"Error calculating priority: {e}")
            return 0.5


class ProgressiveSynthesizer:
    """Creates progressive summaries to manage token limits"""
    
    def __init__(self, openai_manager, token_manager: TokenManager):
        self.openai_manager = openai_manager
        self.token_manager = token_manager
        self.max_synthesis_tokens = 15000  # Maximum tokens for synthesis input
    
    async def create_progressive_synthesis(self, research_state: MultiAgentResearchState) -> str:
        """Create synthesis in progressive stages to manage token limits"""
        try:
            logger.info("Creating progressive synthesis with token management")
            
            # Stage 1: Extract and prioritize key findings
            all_findings = self._extract_all_findings(research_state)
            prioritized_findings = await self._prioritize_findings(all_findings, research_state.original_query)
            
            # Stage 2: Create hierarchical summary
            hierarchical_summary = await self._create_hierarchical_summary(prioritized_findings)
            
            # Stage 3: Progressive synthesis
            final_synthesis = await self._progressive_synthesis(
                hierarchical_summary, research_state.original_query, research_state
            )
            
            # Stage 4: Final token optimization
            optimized_synthesis = await self._optimize_final_synthesis(final_synthesis)
            
            return optimized_synthesis
            
        except Exception as e:
            logger.error(f"Error in progressive synthesis: {e}")
            return f"Synthesis error: {str(e)}"
    
    def _extract_all_findings(self, research_state: MultiAgentResearchState) -> List[Dict]:
        """Extract all findings from research state"""
        all_findings = []
        
        for agent_id, agent_state in research_state.agent_states.items():
            for finding_result in agent_state.findings:
                if finding_result.get('findings'):
                    for finding in finding_result['findings']:
                        finding_copy = finding.copy()
                        finding_copy['source_agent'] = agent_id
                        finding_copy['agent_confidence'] = finding_result.get('confidence', 0.5)
                        all_findings.append(finding_copy)
        
        return all_findings
    
    async def _prioritize_findings(self, findings: List[Dict], query: str) -> List[Dict]:
        """Prioritize findings by relevance and quality"""
        try:
            prioritizer = ContentPrioritizer(self.openai_manager)
            return await prioritizer.prioritize_findings(findings, query)
        except Exception as e:
            logger.error(f"Error prioritizing findings: {e}")
            return findings
    
    async def _create_hierarchical_summary(self, findings: List[Dict]) -> Dict[str, Any]:
        """Create hierarchical summary of findings"""
        try:
            # Group findings by type and jurisdiction
            grouped_findings = defaultdict(list)
            
            for finding in findings:
                jurisdiction = finding.get('jurisdiction', 'General')
                agent_type = finding.get('source_agent', 'unknown')
                key = f"{jurisdiction}_{agent_type}"
                grouped_findings[key].append(finding)
            
            # Create summaries for each group
            group_summaries = {}
            
            for group_key, group_findings in grouped_findings.items():
                if len(group_findings) <= 3:
                    # Small group, keep detailed
                    group_summaries[group_key] = group_findings
                else:
                    # Large group, summarize
                    summary = await self._summarize_finding_group(group_findings)
                    group_summaries[group_key] = summary
            
            return {
                'grouped_summaries': group_summaries,
                'total_findings': len(findings),
                'top_findings': findings[:10]  # Keep top 10 detailed findings
            }
            
        except Exception as e:
            logger.error(f"Error creating hierarchical summary: {e}")
            return {'grouped_summaries': {}, 'total_findings': 0, 'top_findings': []}
    
    async def _summarize_finding_group(self, findings: List[Dict]) -> List[Dict]:
        """Summarize a group of findings"""
        try:
            # Combine findings into summary prompt
            findings_text = []
            for i, finding in enumerate(findings):
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(f"{i+1}. {content[:300]}...")
            
            combined_text = "\n".join(findings_text)
            
            # Check token limit
            if self.token_manager.count_tokens(combined_text) > self.max_synthesis_tokens:
                # Too long, take only top findings
                return findings[:5]
            
            system_prompt = """
            Summarize the following research findings into 3-5 key points.
            Maintain the most important information and specific details.
            Keep jurisdiction and regulatory information intact.
            
            Return as JSON array:
            [
                {
                    "summary": "Key point summary",
                    "jurisdiction": "Relevant jurisdiction",
                    "confidence": 0.8,
                    "supporting_details": "Important details"
                }
            ]
            """
            
            messages = [{"role": "user", "content": f"Summarize these findings:\n\n{combined_text}"}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                summary = safe_json_parse(response, [])
                return summary if isinstance(summary, list) else [summary]
            except Exception:
                # Fallback to top findings
                return findings[:3]
            
        except Exception as e:
            logger.error(f"Error summarizing finding group: {e}")
            return findings[:3]
    
    async def _progressive_synthesis(self, hierarchical_summary: Dict, query: str, 
                                   research_state: MultiAgentResearchState) -> str:
        """Create final synthesis progressively"""
        try:
            # Stage 1: Create executive summary
            executive_summary = await self._create_executive_summary(hierarchical_summary, query)
            
            # Stage 2: Create detailed analysis
            detailed_analysis = await self._create_detailed_analysis(hierarchical_summary, query)
            
            # Stage 3: Create recommendations
            recommendations = await self._create_recommendations(hierarchical_summary, query)
            
            # Stage 4: Combine with token management
            final_report = await self._combine_synthesis_stages(
                executive_summary, detailed_analysis, recommendations, research_state
            )
            
            return final_report
            
        except Exception as e:
            logger.error(f"Error in progressive synthesis: {e}")
            return f"Unable to complete synthesis: {str(e)}"
    
    async def _create_executive_summary(self, hierarchical_summary: Dict, query: str) -> str:
        """Create executive summary"""
        try:
            top_findings = hierarchical_summary.get('top_findings', [])[:5]
            
            findings_text = []
            for finding in top_findings:
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(content[:200])
            
            system_prompt = """
            Create a concise executive summary (2-3 paragraphs) that answers the research question.
            Focus on the most important findings and their implications.
            Include specific regulatory references where available.
            """
            
            messages = [{"role": "user", "content": f"""
            Query: {query}
            
            Key Findings:
            {chr(10).join(findings_text)}
            
            Create executive summary.
            """}]
            
            return await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
        except Exception as e:
            logger.error(f"Error creating executive summary: {e}")
            return "Executive summary unavailable."
    
    async def _create_detailed_analysis(self, hierarchical_summary: Dict, query: str) -> str:
        """Create detailed analysis section"""
        try:
            grouped_summaries = hierarchical_summary.get('grouped_summaries', {})
            
            analysis_sections = []
            
            for group_key, group_findings in list(grouped_summaries.items())[:5]:  # Limit to 5 groups
                jurisdiction = group_key.split('_')[0]
                
                # Create section for this group
                section_content = []
                
                if isinstance(group_findings, list):
                    for finding in group_findings[:3]:  # Limit to 3 findings per group
                        content = finding.get('content', '') or finding.get('insight', '')
                        section_content.append(content[:300])
                
                if section_content:
                    section_text = f"**{jurisdiction}**: " + " ".join(section_content)
                    analysis_sections.append(section_text)
            
            detailed_analysis = "\n\n".join(analysis_sections)
            
            # Ensure token limit
            if self.token_manager.count_tokens(detailed_analysis) > 8000:
                detailed_analysis = self.token_manager.truncate_to_limit(detailed_analysis, 8000)
            
            return detailed_analysis
            
        except Exception as e:
            logger.error(f"Error creating detailed analysis: {e}")
            return "Detailed analysis unavailable."
    
    async def _create_recommendations(self, hierarchical_summary: Dict, query: str) -> str:
        """Create recommendations section"""
        try:
            top_findings = hierarchical_summary.get('top_findings', [])[:3]
            
            findings_text = []
            for finding in top_findings:
                content = finding.get('content', '') or finding.get('insight', '')
                findings_text.append(content[:150])
            
            system_prompt = """
            Based on the research findings, provide 3-5 specific, actionable recommendations.
            Focus on practical implementation steps and compliance considerations.
            """
            
            messages = [{"role": "user", "content": f"""
            Query: {query}
            
            Key Findings:
            {chr(10).join(findings_text)}
            
            Provide actionable recommendations.
            """}]
            
            return await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
        except Exception as e:
            logger.error(f"Error creating recommendations: {e}")
            return "Recommendations unavailable."
    
    async def _combine_synthesis_stages(self, executive_summary: str, detailed_analysis: str, 
                                      recommendations: str, research_state: MultiAgentResearchState) -> str:
        """Combine synthesis stages with token management"""
        try:
            # Calculate token usage
            exec_tokens = self.token_manager.count_tokens(executive_summary)
            analysis_tokens = self.token_manager.count_tokens(detailed_analysis)
            rec_tokens = self.token_manager.count_tokens(recommendations)
            
            # Add metadata
            metadata = f"""
**Research Summary**
- Query: {research_state.original_query}
- Iterations: {research_state.current_iteration}
- Agents: {', '.join(research_state.agent_states.keys())}
- Confidence: {research_state.overall_confidence:.2f}

"""
            
            metadata_tokens = self.token_manager.count_tokens(metadata)
            total_tokens = exec_tokens + analysis_tokens + rec_tokens + metadata_tokens
            
            # If within limits, combine all
            if total_tokens <= self.token_manager.count_tokens("") + config.MAX_RESPONSE_TOKENS:
                return f"""
{metadata}

**Executive Summary**
{executive_summary}

**Detailed Analysis**
{detailed_analysis}

**Recommendations**
{recommendations}
"""
            
            # If too long, prioritize and truncate
            else:
                # Always include metadata and executive summary
                core_content = f"""
{metadata}

**Executive Summary**
{executive_summary}

**Key Findings**
{self.token_manager.truncate_to_limit(detailed_analysis, 6000)}

**Recommendations**
{self.token_manager.truncate_to_limit(recommendations, 2000)}
"""
                
                return core_content
            
        except Exception as e:
            logger.error(f"Error combining synthesis stages: {e}")
            return f"Synthesis combination error: {str(e)}"
    
    async def _optimize_final_synthesis(self, synthesis: str) -> str:
        """Final optimization of synthesis"""
        try:
            synthesis_tokens = self.token_manager.count_tokens(synthesis)
            
            if synthesis_tokens <= config.MAX_RESPONSE_TOKENS:
                return synthesis
            
            logger.warning(f"Synthesis too long ({synthesis_tokens} tokens), applying final optimization")
            
            # Extract sections
            sections = synthesis.split('\n\n')
            
            # Prioritize sections
            prioritized_sections = []
            for section in sections:
                if any(keyword in section.lower() for keyword in ['executive summary', 'research summary']):
                    prioritized_sections.insert(0, section)  # Highest priority
                elif any(keyword in section.lower() for keyword in ['key findings', 'recommendations']):
                    prioritized_sections.append(section)  # High priority
                else:
                    prioritized_sections.append(section)  # Normal priority
            
            # Build optimized response
            result_sections = []
            current_tokens = 0
            
            for section in prioritized_sections:
                section_tokens = self.token_manager.count_tokens(section)
                if current_tokens + section_tokens <= config.MAX_RESPONSE_TOKENS:
                    result_sections.append(section)
                    current_tokens += section_tokens
                elif current_tokens < config.MAX_RESPONSE_TOKENS * 0.8:
                    # Try truncated version
                    remaining_tokens = config.MAX_RESPONSE_TOKENS - current_tokens - 100
                    truncated = self.token_manager.truncate_to_limit(section, remaining_tokens)
                    result_sections.append(truncated)
                    break
            
            final_synthesis = '\n\n'.join(result_sections)
            
            # Add optimization notice
            if len(result_sections) < len(prioritized_sections):
                final_synthesis += "\n\n*[Response optimized for length - additional details available upon request]*"
            
            return final_synthesis
            
        except Exception as e:
            logger.error(f"Error in final synthesis optimization: {e}")
            return self.token_manager.truncate_to_limit(synthesis, config.MAX_RESPONSE_TOKENS)


# Legacy MemoryManager for compatibility
class MemoryManager(OptimizedMemoryManager):
    """Legacy alias for backward compatibility"""
    pass

# ============================
# DOMAIN FILTER
# ============================

class DomainFilter:
    """LLM-based domain filter for privacy/law related queries"""
    
    def __init__(self, openai_manager: OpenAIManager):
        self.openai_manager = openai_manager
    
    async def is_privacy_law_related(self, query: str) -> Dict[str, Any]:
        """Check if query is privacy/law related using LLM"""
        if not query or len(query.strip()) < 3:
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Query too short"
            }
        
        system_prompt = """
        You are an expert classifier for privacy, data protection, and legal compliance queries.
        
        Determine if the query relates to:
        - Data privacy regulations (GDPR, CCPA, LGPD, etc.)
        - Legal compliance and regulatory requirements
        - Information security and data governance
        - Privacy rights and obligations
        - Legal frameworks and legislation
        
        Return ONLY a JSON response:
        {
            "is_relevant": boolean,
            "confidence": float between 0.0 and 1.0,
            "reasoning": "Brief explanation",
            "domain": "privacy|legal|compliance|other"
        }
        """
        
        try:
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Classify this query: {query}"}
            ]
            
            response = await self.openai_manager.chat_completion(messages)
            result = safe_json_parse(response, {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": "Failed to parse response",
                "domain": "other"
            })
            
            return result
            
        except Exception as e:
            logger.error(f"Domain classification error: {e}")
            return {
                "is_relevant": False,
                "confidence": 0.0,
                "reasoning": f"Error: {str(e)}",
                "domain": "error"
            }

# ============================
# RESEARCH AGENTS & ORCHESTRATION
# ============================

@dataclass
class ResearchTask:
    """Individual research task for specialized agents"""
    task_id: str
    agent_type: str
    query: str
    focus_areas: List[str]
    jurisdictions: Optional[List[str]] = None
    priority: int = 1
    max_results: int = 10
    context: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)

@dataclass
class ResearchPlan:
    """Enhanced research plan with multi-agent coordination"""
    plan_id: str
    main_query: str
    research_objectives: List[str]
    research_tasks: List[ResearchTask]
    agent_assignments: Dict[str, List[str]]
    expected_iterations: int
    coordination_strategy: str

@dataclass
class AgentState:
    """State for individual research agents"""
    agent_id: str
    agent_type: str
    current_tasks: List[str]
    completed_tasks: List[str]
    findings: List[Dict[str, Any]]
    knowledge_gaps: List[str]
    confidence_score: float
    status: str

@dataclass
class MultiAgentResearchState:
    """State for the entire multi-agent research system"""
    session_id: str
    original_query: str
    research_plan: Optional[ResearchPlan]
    agent_states: Dict[str, AgentState]
    shared_knowledge: Dict[str, List[Dict]]
    iteration_history: List[Dict[str, Any]]
    current_iteration: int
    max_iterations: int
    overall_confidence: float
    is_complete: bool = False
    final_synthesis: str = ""
    memory_keys: List[str] = field(default_factory=list)

@dataclass
class ResearchResult:
    """Research result data structure"""
    query: str
    findings: List[Dict[str, Any]]
    confidence: float
    sources: List[str]
    agent_type: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

class SpecializedResearchAgent:
    """Base class for specialized research agents"""
    
    def __init__(self, agent_type: str, openai_manager: OpenAIManager, 
                 es_manager: ElasticsearchManager):
        self.agent_type = agent_type
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.agent_id = f"{agent_type}_{uuid.uuid4().hex[:8]}"
        self.token_manager = TokenManager()
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute a research task - to be implemented by subclasses"""
        raise NotImplementedError

class PlannerAgent(SpecializedResearchAgent):
    """Agent specialized in research planning and coordination"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        super().__init__("planner", openai_manager, es_manager)
    
    async def create_research_plan(self, query: str, context: Dict = None) -> ResearchPlan:
        """Create comprehensive research plan with multi-agent coordination"""
        system_prompt = """
        You are a research planning expert specializing in data privacy and protection regulations.
        Create a detailed research plan that coordinates multiple specialized agents.
        
        Break down the research into specific tasks that can be executed by:
        1. Domain Expert Agents (jurisdiction-specific research)
        2. Concept Analysis Agents (deep concept exploration)
        3. Comparative Analysis Agents (cross-jurisdictional comparison)
        4. Synthesis Agents (knowledge synthesis and gap analysis)
        
        Consider task dependencies, optimal execution order, resource allocation,
        and coordination requirements.
        
        Return a detailed JSON plan with task assignments.
        """
        
        try:
            # Truncate query if too long
            truncated_query = self.token_manager.truncate_to_limit(query, 500)
            truncated_context = self.token_manager.truncate_to_limit(str(context or {}), 1000)
            
            messages = [{"role": "user", "content": f"""
            Create a comprehensive research plan for: {truncated_query}
            
            Context: {truncated_context}
            
            Return a JSON plan with:
            {{
                "main_query": "{truncated_query}",
                "research_objectives": ["objective1", "objective2"],
                "research_tasks": [
                    {{
                        "task_id": "task_1",
                        "agent_type": "domain_expert",
                        "query": "specific research query",
                        "focus_areas": ["area1", "area2"],
                        "jurisdictions": ["EU", "US"],
                        "priority": 1,
                        "dependencies": []
                    }}
                ],
                "agent_assignments": {{
                    "domain_expert": ["task_1", "task_2"],
                    "concept_analyst": ["task_3"],
                    "comparative_analyst": ["task_4"]
                }},
                "expected_iterations": 3,
                "coordination_strategy": "sequential_with_feedback"
            }}
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            plan_data = safe_json_parse(response, {})
            
            # Create ResearchTask objects
            tasks = []
            for task_data in plan_data.get("research_tasks", []):
                task = ResearchTask(
                    task_id=task_data.get("task_id", f"task_{len(tasks)}"),
                    agent_type=task_data.get("agent_type", "domain_expert"),
                    query=task_data.get("query", truncated_query),
                    focus_areas=task_data.get("focus_areas", []),
                    jurisdictions=task_data.get("jurisdictions"),
                    priority=task_data.get("priority", 1),
                    dependencies=task_data.get("dependencies", [])
                )
                tasks.append(task)
            
            return ResearchPlan(
                plan_id=f"plan_{uuid.uuid4().hex[:8]}",
                main_query=truncated_query,
                research_objectives=plan_data.get("research_objectives", []),
                research_tasks=tasks,
                agent_assignments=plan_data.get("agent_assignments", {}),
                expected_iterations=plan_data.get("expected_iterations", 3),
                coordination_strategy=plan_data.get("coordination_strategy", "sequential")
            )
            
        except Exception as e:
            logger.error(f"Error creating research plan: {e}")
            return self._create_fallback_plan(query)
    
    def _create_fallback_plan(self, query: str) -> ResearchPlan:
        """Create a simple fallback plan"""
        task = ResearchTask(
            task_id="fallback_task",
            agent_type="domain_expert",
            query=self.token_manager.truncate_to_limit(query, 500),
            focus_areas=["general_research"],
            priority=1
        )
        
        return ResearchPlan(
            plan_id=f"fallback_plan_{uuid.uuid4().hex[:8]}",
            main_query=self.token_manager.truncate_to_limit(query, 500),
            research_objectives=["Investigate the topic"],
            research_tasks=[task],
            agent_assignments={"domain_expert": ["fallback_task"]},
            expected_iterations=2,
            coordination_strategy="sequential"
        )

class DomainExpertAgent(SpecializedResearchAgent):
    """Agent specialized in jurisdiction-specific domain expertise"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        super().__init__("domain_expert", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute domain-specific research task"""
        try:
            logger.info(f"Domain expert executing task: {task.task_id}")
            
            # Perform specialized search
            docs = await self.es_manager.search_documents(
                query=task.query,
                limit=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "findings": [],
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            # Analyze findings with domain expertise
            findings = await self._analyze_domain_findings(task, docs, shared_context)
            
            # Calculate confidence based on result quality
            confidence = self._calculate_confidence(docs, findings)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "findings": findings,
                "sources": [doc.metadata.get('source', 'unknown') for doc in docs[:5]],
                "confidence": confidence,
                "status": "completed",
                "recommendations": await self._generate_recommendations(task, findings)
            }
            
        except Exception as e:
            logger.error(f"Error in domain expert task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_domain_findings(self, task: ResearchTask, docs: List[Document], 
                                     shared_context: Dict) -> List[Dict]:
        """Analyze findings with domain expertise"""
        try:
            # Prepare context from documents with token management
            doc_context_parts = []
            total_tokens = 0
            max_context_tokens = 8000
            
            for i, doc in enumerate(docs[:10]):
                doc_snippet = f"Document {i+1} ({doc.metadata.get('jurisdiction', 'Unknown')}): {doc.page_content[:500]}..."
                doc_tokens = self.token_manager.count_tokens(doc_snippet)
                
                if total_tokens + doc_tokens <= max_context_tokens:
                    doc_context_parts.append(doc_snippet)
                    total_tokens += doc_tokens
                else:
                    break
            
            doc_context = "\n\n".join(doc_context_parts)
            
            system_prompt = f"""
            You are a domain expert in data privacy and protection regulations.
            Focus on jurisdiction: {', '.join(task.jurisdictions or ['All'])}
            Focus areas: {', '.join(task.focus_areas)}
            
            Analyze the provided documents and extract key domain-specific insights.
            Consider regulatory requirements, compliance obligations, and practical implications.
            
            Return insights as JSON array:
            [
                {{
                    "insight": "Key finding",
                    "jurisdiction": "Specific jurisdiction",
                    "regulation": "Specific regulation/article",
                    "implication": "Practical implication",
                    "confidence": 0.9
                }}
            ]
            """
            
            # Truncate context if needed
            truncated_context = self.token_manager.truncate_to_limit(str(shared_context or {}), 1000)
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            
            Document Context:
            {doc_context}
            
            Shared Research Context:
            {truncated_context}
            
            Extract domain-specific insights following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            # Parse findings
            try:
                findings = safe_json_parse(response, [])
                if not isinstance(findings, list):
                    findings = [findings] if isinstance(findings, dict) else []
                return findings
            except Exception:
                logger.warning("Failed to parse findings as JSON")
                return [{"insight": response[:500], "confidence": 0.5}]
            
        except Exception as e:
            logger.error(f"Error analyzing domain findings: {e}")
            return []
    
    def _calculate_confidence(self, docs: List[Document], findings: List[Dict]) -> float:
        """Calculate confidence score based on result quality"""
        try:
            if not docs or not findings:
                return 0.0
            
            # Factors for confidence calculation
            doc_score = min(len(docs) / 10, 1.0)
            finding_score = min(len(findings) / 5, 1.0)
            
            # Average finding confidence
            finding_confidence = sum(f.get('confidence', 0.5) for f in findings) / len(findings)
            
            # Combined confidence
            confidence = (doc_score + finding_score + finding_confidence) / 3
            return round(confidence, 2)
            
        except Exception as e:
            logger.error(f"Error calculating confidence: {e}")
            return 0.5
    
    async def _generate_recommendations(self, task: ResearchTask, findings: List[Dict]) -> List[str]:
        """Generate recommendations based on findings"""
        if not findings:
            return ["Consider refining the research query for better results"]
        
        try:
            recommendations = []
            
            high_conf_findings = [f for f in findings if f.get('confidence', 0) > 0.7]
            if high_conf_findings:
                recommendations.append(f"Found {len(high_conf_findings)} high-confidence insights")
            
            jurisdictions_covered = set(f.get('jurisdiction', 'Unknown') for f in findings)
            if len(jurisdictions_covered) > 1:
                recommendations.append(f"Multi-jurisdictional coverage: {', '.join(jurisdictions_covered)}")
            
            if len(findings) < 3:
                recommendations.append("Consider broader search terms for more comprehensive coverage")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            return []

class ConceptAnalysisAgent(SpecializedResearchAgent):
    """Agent specialized in deep concept analysis and relationship mapping"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        super().__init__("concept_analyst", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute concept analysis task"""
        try:
            logger.info(f"Concept analyst executing task: {task.task_id}")
            
            docs = await self.es_manager.search_documents(
                query=task.query,
                limit=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "concepts": {},
                    "relationships": {},
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            concept_analysis = await self._analyze_concepts(task, docs, shared_context)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "concepts": concept_analysis.get("concepts", {}),
                "relationships": concept_analysis.get("relationships", {}),
                "definitions": concept_analysis.get("definitions", {}),
                "concept_evolution": concept_analysis.get("evolution", {}),
                "confidence": concept_analysis.get("confidence", 0.5),
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Error in concept analysis task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _analyze_concepts(self, task: ResearchTask, docs: List[Document], 
                              shared_context: Dict) -> Dict[str, Any]:
        """Deep concept analysis with token management"""
        try:
            # Prepare document context with token limits
            doc_parts = []
            total_tokens = 0
            max_tokens = 6000
            
            for i, doc in enumerate(docs[:8]):
                doc_snippet = f"Document {i+1}: {doc.page_content[:400]}..."
                doc_tokens = self.token_manager.count_tokens(doc_snippet)
                
                if total_tokens + doc_tokens <= max_tokens:
                    doc_parts.append(doc_snippet)
                    total_tokens += doc_tokens
                else:
                    break
            
            doc_context = "\n\n".join(doc_parts)
            
            system_prompt = """
            You are a concept analysis expert specializing in data privacy and protection regulations.
            Analyze the provided documents to identify key concepts, their relationships, and evolution.
            
            Focus on:
            1. Core privacy concepts and their definitions
            2. Relationships between concepts
            3. How concepts are interpreted across jurisdictions
            4. Evolution of concepts over time
            
            Return analysis as JSON:
            {
                "concepts": {
                    "concept_name": {
                        "definition": "Definition",
                        "importance": 0.9,
                        "jurisdictions": ["EU", "US"],
                        "related_terms": ["term1", "term2"]
                    }
                },
                "relationships": {
                    "concept1_concept2": {
                        "type": "relationship_type",
                        "strength": 0.8,
                        "description": "Relationship description"
                    }
                },
                "definitions": {
                    "concept": "Clear definition"
                },
                "confidence": 0.85
            }
            """
            
            truncated_context = self.token_manager.truncate_to_limit(str(shared_context or {}), 500)
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            Focus Areas: {', '.join(task.focus_areas)}
            
            Document Context:
            {doc_context}
            
            Shared Context:
            {truncated_context}
            
            Perform deep concept analysis following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                return safe_json_parse(response, {"concepts": {}, "confidence": 0.3})
            except Exception:
                logger.warning("Failed to parse concept analysis as JSON")
                return {"concepts": {}, "confidence": 0.3}
            
        except Exception as e:
            logger.error(f"Error in concept analysis: {e}")
            return {"concepts": {}, "confidence": 0.0}

class ComparativeAnalysisAgent(SpecializedResearchAgent):
    """Agent specialized in cross-jurisdictional comparative analysis"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        super().__init__("comparative_analyst", openai_manager, es_manager)
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute comparative analysis task"""
        try:
            logger.info(f"Comparative analyst executing task: {task.task_id}")
            
            docs = await self.es_manager.search_documents(
                query=task.query,
                limit=task.max_results
            )
            
            if not docs:
                return {
                    "task_id": task.task_id,
                    "comparisons": {},
                    "confidence": 0.0,
                    "status": "no_results"
                }
            
            comparative_analysis = await self._perform_comparative_analysis(task, docs, shared_context)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "comparisons": comparative_analysis.get("comparisons", {}),
                "similarities": comparative_analysis.get("similarities", []),
                "differences": comparative_analysis.get("differences", []),
                "recommendations": comparative_analysis.get("recommendations", []),
                "confidence": comparative_analysis.get("confidence", 0.5),
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Error in comparative analysis task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }
    
    async def _perform_comparative_analysis(self, task: ResearchTask, docs: List[Document], 
                                          shared_context: Dict) -> Dict[str, Any]:
        """Perform cross-jurisdictional comparative analysis"""
        try:
            # Group documents by jurisdiction
            jurisdiction_docs = defaultdict(list)
            for doc in docs:
                jurisdiction = doc.metadata.get('jurisdiction', 'Unknown')
                jurisdiction_docs[jurisdiction].append(doc)
            
            # Prepare comparative context
            comparative_context = []
            for jurisdiction, j_docs in jurisdiction_docs.items():
                doc_summaries = []
                for doc in j_docs[:3]:  # Limit to top 3 docs per jurisdiction
                    doc_summaries.append(doc.page_content[:300])
                
                comparative_context.append(f"{jurisdiction}: {' '.join(doc_summaries)}")
            
            context_text = "\n\n".join(comparative_context)
            context_text = self.token_manager.truncate_to_limit(context_text, 6000)
            
            system_prompt = """
            You are a comparative analysis expert specializing in cross-jurisdictional privacy law comparison.
            Analyze the provided information across different jurisdictions to identify:
            
            1. Key similarities in approaches
            2. Significant differences in implementation
            3. Comparative strengths and weaknesses
            4. Harmonization opportunities
            
            Return analysis as JSON:
            {
                "comparisons": {
                    "jurisdiction1_vs_jurisdiction2": {
                        "similarities": ["similarity1", "similarity2"],
                        "differences": ["difference1", "difference2"],
                        "analysis": "Detailed comparison"
                    }
                },
                "similarities": ["Overall similarities across jurisdictions"],
                "differences": ["Key differences across jurisdictions"],
                "recommendations": ["Comparative recommendations"],
                "confidence": 0.8
            }
            """
            
            messages = [{"role": "user", "content": f"""
            Task: {task.query}
            Jurisdictions to Compare: {', '.join(task.jurisdictions or ['All available'])}
            
            Jurisdictional Context:
            {context_text}
            
            Perform comparative analysis following the JSON format.
            """}]
            
            response = await self.openai_manager.chat_completion(messages, system_prompt=system_prompt)
            
            try:
                return safe_json_parse(response, {"comparisons": {}, "confidence": 0.3})
            except Exception:
                logger.warning("Failed to parse comparative analysis as JSON")
                return {"comparisons": {}, "confidence": 0.3}
            
        except Exception as e:
            logger.error(f"Error in comparative analysis: {e}")
            return {"comparisons": {}, "confidence": 0.0}

class SynthesisAgent(SpecializedResearchAgent):
    """Agent specialized in knowledge synthesis and report generation with progressive synthesis"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        super().__init__("synthesizer", openai_manager, es_manager)
        # Initialize progressive synthesizer with full capabilities
        self.progressive_synthesizer = ProgressiveSynthesizer(openai_manager, self.token_manager)
    
    async def synthesize_research(self, research_state: MultiAgentResearchState) -> str:
        """Synthesize all research findings into comprehensive report using progressive synthesis"""
        try:
            logger.info("Synthesizing research findings with progressive approach")
            
            # Use progressive synthesizer for comprehensive token-optimized results
            return await self.progressive_synthesizer.create_progressive_synthesis(research_state)
            
        except Exception as e:
            logger.error(f"Error in research synthesis: {e}")
            return f"Research synthesis encountered an error: {str(e)}"
    
    async def execute_task(self, task: ResearchTask, shared_context: Dict = None) -> Dict[str, Any]:
        """Execute synthesis task"""
        try:
            logger.info(f"Synthesizer executing task: {task.task_id}")
            
            # For synthesis tasks, we need the full research state from shared context
            if not shared_context or 'research_state' not in shared_context:
                return {
                    "task_id": task.task_id,
                    "status": "error",
                    "error": "No research state provided for synthesis"
                }
            
            research_state = shared_context['research_state']
            synthesis = await self.synthesize_research(research_state)
            
            return {
                "task_id": task.task_id,
                "agent_type": self.agent_type,
                "synthesis": synthesis,
                "confidence": 0.9,  # High confidence in synthesis
                "status": "completed",
                "synthesis_method": "progressive_token_managed"
            }
            
        except Exception as e:
            logger.error(f"Error in synthesis task execution: {e}")
            return {
                "task_id": task.task_id,
                "status": "error",
                "error": str(e)
            }

# ============================
# MULTI-AGENT RESEARCH ORCHESTRATOR
# ============================

class OptimizedMultiAgentOrchestrator:
    """Orchestrates multiple specialized research agents with token management"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager,
                 memory_manager: OptimizedMemoryManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_manager = memory_manager
        self.token_manager = TokenManager()
        
        # Initialize specialized agents
        self.planner = PlannerAgent(openai_manager, es_manager)
        self.domain_expert = DomainExpertAgent(openai_manager, es_manager)
        self.concept_analyst = ConceptAnalysisAgent(openai_manager, es_manager)
        self.comparative_analyst = ComparativeAnalysisAgent(openai_manager, es_manager)
        self.synthesizer = SynthesisAgent(openai_manager, es_manager)
        
        self.agents = {
            "planner": self.planner,
            "domain_expert": self.domain_expert,
            "concept_analyst": self.concept_analyst,
            "comparative_analyst": self.comparative_analyst,
            "synthesizer": self.synthesizer
        }
    
    async def conduct_multi_agent_research(self, query: str, user_id: str = None, 
                                         context: Dict = None) -> MultiAgentResearchState:
        """Conduct comprehensive research using multiple specialized agents with token management"""
        try:
            session_id = f"research_{uuid.uuid4().hex[:8]}"
            logger.info(f"Starting multi-agent research session: {session_id}")
            
            # Check and truncate query if too long
            query_tokens = self.token_manager.count_tokens(query)
            if query_tokens > 1000:
                logger.warning(f"Query too long ({query_tokens} tokens), truncating")
                query = self.token_manager.truncate_to_limit(query, 1000)
            
            # Phase 1: Planning
            logger.info("Phase 1: Research Planning")
            research_plan = await self.planner.create_research_plan(query, context)
            
            # Initialize research state
            research_state = MultiAgentResearchState(
                session_id=session_id,
                original_query=query,
                research_plan=research_plan,
                agent_states={},
                shared_knowledge={},
                iteration_history=[],
                current_iteration=0,
                max_iterations=research_plan.expected_iterations,
                overall_confidence=0.0
            )
            
            # Retrieve relevant memories with enhanced memory management
            if user_id and self.memory_manager:
                # Use the enhanced memory retrieval
                relevant_memories = await self.memory_manager.retrieve_relevant_memories(
                    user_id, query, limit=5
                )
                
                # Also retrieve user preferences
                user_preferences = await self.memory_manager.retrieve_memory(
                    [config.MEMORY_NAMESPACE[0], user_id, "preferences"], 
                    "user_preferences", limit=1
                )
                
                research_state.shared_knowledge["relevant_memories"] = relevant_memories
                research_state.shared_knowledge["user_preferences"] = user_preferences
                
                logger.info(f"Retrieved {len(relevant_memories)} relevant memories and {len(user_preferences)} preferences for user {user_id}")
            
            # Phase 2: Multi-agent research execution
            logger.info("Phase 2: Multi-agent research execution")
            
            for iteration in range(research_plan.expected_iterations):
                research_state.current_iteration = iteration + 1
                logger.info(f"Starting research iteration {research_state.current_iteration}")
                
                iteration_results = await self._execute_research_iteration(
                    research_state, iteration
                )
                
                research_state.iteration_history.append(iteration_results)
                
                # Update overall confidence
                agent_confidences = [
                    state.confidence_score for state in research_state.agent_states.values()
                ]
                research_state.overall_confidence = sum(agent_confidences) / len(agent_confidences) if agent_confidences else 0.0
                
                # Check if we should continue
                if research_state.overall_confidence >= config.CONFIDENCE_THRESHOLD:
                    logger.info(f"Research confidence threshold reached: {research_state.overall_confidence:.2f}")
                    break
            
            # Phase 3: Synthesis with progressive approach
            logger.info("Phase 3: Progressive knowledge synthesis")
            research_state.final_synthesis = await self.synthesizer.synthesize_research(research_state)
            research_state.is_complete = True
            
            # Store findings in long-term memory with token optimization
            if user_id and self.memory_manager:
                await self._store_research_memories(user_id, research_state)
            
            logger.info(f"Multi-agent research completed with confidence: {research_state.overall_confidence:.2f}")
            return research_state
            
        except Exception as e:
            logger.error(f"Error in multi-agent research: {e}")
            # Return error state
            return MultiAgentResearchState(
                session_id="error",
                original_query=query,
                research_plan=None,
                agent_states={},
                shared_knowledge={"error": str(e)},
                iteration_history=[],
                current_iteration=0,
                max_iterations=1,
                overall_confidence=0.0,
                is_complete=True,
                final_synthesis=f"Research failed due to error: {str(e)}"
            )
    
    async def _execute_research_iteration(self, research_state: MultiAgentResearchState, 
                                        iteration: int) -> Dict[str, Any]:
        """Execute a single research iteration with multiple agents"""
        try:
            iteration_results = {
                "iteration": iteration + 1,
                "agent_results": {},
                "shared_updates": {},
                "timestamp": datetime.now().isoformat()
            }
            
            # Get tasks for this iteration
            tasks_to_execute = self._get_iteration_tasks(research_state, iteration)
            
            # Execute tasks with appropriate agents
            for task in tasks_to_execute:
                agent_type = task.agent_type
                if agent_type in self.agents:
                    try:
                        # Prepare shared context with token management
                        shared_context = {
                            "iteration": iteration + 1,
                            "previous_findings": research_state.shared_knowledge,
                            "research_plan": research_state.research_plan.__dict__ if research_state.research_plan else {}
                        }
                        
                        # Truncate shared context if too large
                        context_text = json.dumps(shared_context)
                        if self.token_manager.count_tokens(context_text) > 3000:
                            # Reduce previous findings
                            if "previous_findings" in shared_context:
                                findings = shared_context["previous_findings"]
                                for key in list(findings.keys()):
                                    if isinstance(findings[key], list) and len(findings[key]) > 3:
                                        findings[key] = findings[key][:3]
                        
                        # Execute task
                        result = await self.agents[agent_type].execute_task(task, shared_context)
                        iteration_results["agent_results"][task.task_id] = result
                        
                        # Update agent state
                        if agent_type not in research_state.agent_states:
                            research_state.agent_states[agent_type] = AgentState(
                                agent_id=self.agents[agent_type].agent_id,
                                agent_type=agent_type,
                                current_tasks=[],
                                completed_tasks=[],
                                findings=[],
                                knowledge_gaps=[],
                                confidence_score=0.0,
                                status="idle"
                            )
                        
                        agent_state = research_state.agent_states[agent_type]
                        agent_state.completed_tasks.append(task.task_id)
                        agent_state.findings.append(result)
                        agent_state.confidence_score = result.get("confidence", 0.0)
                        agent_state.status = "completed"
                        
                        # Update shared knowledge with token limits
                        if result.get("findings"):
                            knowledge_key = f"{agent_type}_findings"
                            if knowledge_key not in research_state.shared_knowledge:
                                research_state.shared_knowledge[knowledge_key] = []
                            
                            # Limit findings to prevent token overflow
                            new_findings = result["findings"][:5]  # Limit to top 5 findings
                            research_state.shared_knowledge[knowledge_key].extend(new_findings)
                            
                            # Keep only recent findings if too many
                            if len(research_state.shared_knowledge[knowledge_key]) > 10:
                                research_state.shared_knowledge[knowledge_key] = research_state.shared_knowledge[knowledge_key][-10:]
                        
                    except Exception as e:
                        logger.error(f"Error executing task {task.task_id} with {agent_type}: {e}")
                        iteration_results["agent_results"][task.task_id] = {
                            "task_id": task.task_id,
                            "status": "error",
                            "error": str(e)
                        }
            
            return iteration_results
            
        except Exception as e:
            logger.error(f"Error in research iteration: {e}")
            return {
                "iteration": iteration + 1,
                "error": str(e)
            }
    
    def _get_iteration_tasks(self, research_state: MultiAgentResearchState, 
                           iteration: int) -> List[ResearchTask]:
        """Get tasks to execute for current iteration"""
        if not research_state.research_plan:
            return []
        
        # For simplicity, distribute tasks across iterations
        all_tasks = research_state.research_plan.research_tasks
        tasks_per_iteration = max(1, len(all_tasks) // research_state.max_iterations)
        
        start_idx = iteration * tasks_per_iteration
        end_idx = min(start_idx + tasks_per_iteration, len(all_tasks))
        
        return all_tasks[start_idx:end_idx]
    
    async def _store_research_memories(self, user_id: str, research_state: MultiAgentResearchState):
        """Store research findings in long-term memory with token optimization"""
        try:
            if not self.memory_manager:
                return
            
            # Collect all findings
            all_findings = []
            for agent_state in research_state.agent_states.values():
                for finding in agent_state.findings:
                    if finding.get("findings"):
                        all_findings.extend(finding["findings"])
            
            # Store in memory with token management using the advanced memory manager
            context = {
                "query": research_state.original_query,
                "session_id": research_state.session_id,
                "confidence": research_state.overall_confidence
            }
            
            # Use the advanced memory storage from OptimizedMemoryManager
            memory_keys = await self.memory_manager.store_research_findings(
                user_id, research_state.session_id, all_findings, context
            )
            
            research_state.memory_keys = memory_keys
            logger.info(f"Stored {len(memory_keys)} research memories for session {research_state.session_id}")
            
            # Also store user preferences if this is a new user
            if user_id and not hasattr(research_state, 'preferences_stored'):
                preferences = {
                    "research_domain": "privacy_law",
                    "preferred_jurisdictions": list(set(
                        finding.get('jurisdiction', '') for finding in all_findings 
                        if finding.get('jurisdiction')
                    ))[:5],  # Top 5 jurisdictions
                    "research_patterns": {
                        "typical_query_length": len(research_state.original_query),
                        "preferred_confidence_threshold": research_state.overall_confidence,
                        "common_topics": [research_state.research_plan.coordination_strategy if research_state.research_plan else "general"]
                    }
                }
                
                await self.memory_manager.store_user_preferences(user_id, preferences)
                research_state.preferences_stored = True
            
        except Exception as e:
            logger.error(f"Error storing research memories: {e}")

# Legacy ResearchOrchestrator for compatibility
class ResearchOrchestrator(OptimizedMultiAgentOrchestrator):
    """Legacy alias for backward compatibility"""
    
    async def conduct_research(self, query: str, user_id: str = None) -> Dict[str, Any]:
        """Legacy method that converts to new format"""
        research_state = await self.conduct_multi_agent_research(query, user_id)
        
        return {
            "session_id": research_state.session_id,
            "query": research_state.original_query,
            "agent_results": {name: state.__dict__ for name, state in research_state.agent_states.items()},
            "synthesis": research_state.final_synthesis,
            "overall_confidence": research_state.overall_confidence,
            "agents_used": list(research_state.agent_states.keys())
        }

# ============================
# MAIN CHATBOT CLASS
# ============================

class EnhancedResearchChatbot:
    """Enhanced research chatbot with complex multi-agent architecture"""
    
    def __init__(self):
        logger.info("Initializing Enhanced Research Chatbot with Multi-Agent Architecture and Advanced Memory...")
        
        # Initialize core components
        self.openai_manager = OpenAIManager()
        self.es_manager = ElasticsearchManager(self.openai_manager)
        # Use the advanced OptimizedMemoryManager with full features
        self.memory_manager = OptimizedMemoryManager(self.openai_manager)
        self.domain_filter = DomainFilter(self.openai_manager)
        
        # Initialize complex multi-agent orchestrator with enhanced memory
        self.orchestrator = OptimizedMultiAgentOrchestrator(
            self.openai_manager, self.es_manager, self.memory_manager
        )
        self.token_manager = TokenManager()
        
        # Initialize progressive synthesizer for advanced synthesis
        self.progressive_synthesizer = ProgressiveSynthesizer(
            self.openai_manager, self.token_manager
        )
        
        logger.info("✓ Enhanced Research Chatbot with Complex Multi-Agent Architecture and Advanced Memory initialized successfully!")
    
    async def chat(self, user_query: str, user_id: str = None) -> Dict[str, Any]:
        """Main chat interface with complex multi-agent research"""
        try:
            if not user_query or len(user_query.strip()) < 3:
                return {
                    "answer": "Please provide a more detailed question about data privacy, legal compliance, or regulatory matters.",
                    "confidence": "low",
                    "approach": "validation_error"
                }
            
            # Check query length and truncate if necessary
            if self.token_manager.count_tokens(user_query) > 1000:
                user_query = self.token_manager.truncate_to_limit(user_query, 1000)
                logger.warning("Query truncated due to length")
            
            # Domain filtering
            domain_check = await self.domain_filter.is_privacy_law_related(user_query)
            
            if not domain_check["is_relevant"] or domain_check["confidence"] < 0.3:
                return {
                    "answer": "I specialize in data privacy, data protection, and legal compliance questions. Please ask questions related to privacy regulations, legal frameworks, or compliance requirements.",
                    "confidence": "high",
                    "approach": "domain_filter",
                    "domain_analysis": domain_check
                }
            
            # Conduct complex multi-agent research
            research_state = await self.orchestrator.conduct_multi_agent_research(user_query, user_id)
            
            # Prepare final response
            synthesis = research_state.final_synthesis
            
            # Ensure response is within token limits
            if self.token_manager.count_tokens(synthesis) > config.MAX_RESPONSE_TOKENS:
                synthesis = self.token_manager.truncate_to_limit(synthesis, config.MAX_RESPONSE_TOKENS)
                synthesis += "\n\n*[Response optimized for length]*"
            
            confidence_level = "high" if research_state.overall_confidence > 0.7 else "medium"
            
            return {
                "answer": synthesis,
                "confidence": confidence_level,
                "approach": "complex_multi_agent_research",
                "session_id": research_state.session_id,
                "agents_used": list(research_state.agent_states.keys()),
                "overall_confidence_score": research_state.overall_confidence,
                "domain_analysis": domain_check,
                "research_plan": research_state.research_plan.__dict__ if research_state.research_plan else None,
                "iterations_completed": research_state.current_iteration,
                "agent_details": {
                    name: {
                        "confidence": state.confidence_score,
                        "status": state.status,
                        "findings_count": len(state.findings),
                        "tasks_completed": len(state.completed_tasks)
                    }
                    for name, state in research_state.agent_states.items()
                },
                "memory_info": {
                    "memories_stored": len(research_state.memory_keys) if hasattr(research_state, 'memory_keys') else 0,
                    "memories_retrieved": len(research_state.shared_knowledge.get('relevant_memories', [])),
                    "user_preferences_used": bool(research_state.shared_knowledge.get('user_preferences')),
                    "cross_session_learning": bool(user_id and self.memory_manager)
                },
                "token_optimization": {
                    "progressive_synthesis_used": True,
                    "response_tokens": self.token_manager.count_tokens(synthesis),
                    "token_optimized": self.token_manager.count_tokens(synthesis) > config.MAX_RESPONSE_TOKENS * 0.8
                }
            }
            
        except Exception as e:
            logger.error(f"Chat error: {e}")
            return {
                "answer": f"I encountered an error processing your question: {str(e)}",
                "confidence": "low",
                "approach": "error"
            }
    
    async def conduct_research(self, topic: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct standalone complex multi-agent research"""
        try:
            logger.info(f"Starting complex standalone research: {topic}")
            
            research_state = await self.orchestrator.conduct_multi_agent_research(topic, user_id)
            
            return {
                "topic": topic,
                "session_id": research_state.session_id,
                "research_plan": research_state.research_plan.__dict__ if research_state.research_plan else None,
                "agent_states": {
                    name: state.__dict__ for name, state in research_state.agent_states.items()
                },
                "shared_knowledge": research_state.shared_knowledge,
                "iteration_history": research_state.iteration_history,
                "synthesis": research_state.final_synthesis,
                "overall_confidence": research_state.overall_confidence,
                "agents_used": list(research_state.agent_states.keys()),
                "iterations_completed": research_state.current_iteration,
                "memory_keys": research_state.memory_keys if hasattr(research_state, 'memory_keys') else [],
                "coordination_strategy": research_state.research_plan.coordination_strategy if research_state.research_plan else "unknown",
                "memory_features": {
                    "long_term_memory_used": bool(self.memory_manager),
                    "progressive_synthesis": True,
                    "token_optimization": True,
                    "cross_session_persistence": bool(user_id),
                    "memories_stored": len(research_state.memory_keys) if hasattr(research_state, 'memory_keys') else 0,
                    "user_preferences_tracked": bool(research_state.shared_knowledge.get('user_preferences'))
                },
                "advanced_features": {
                    "multi_agent_coordination": True,
                    "progressive_token_management": True,
                    "hierarchical_synthesis": True,
                    "content_prioritization": True,
                    "semantic_memory": bool(LANGMEM_AVAILABLE),
                    "episodic_memory": bool(LANGMEM_AVAILABLE)
                }
            }
            
        except Exception as e:
            logger.error(f"Research error: {e}")
            return {
                "topic": topic,
                "error": str(e),
                "synthesis": f"Research failed: {str(e)}"
            }

# ============================
# INTERFACE CLASS
# ============================

class ChatbotInterface:
    """Interface for the enhanced research chatbot"""
    
    def __init__(self):
        self.chatbot = None
    
    async def initialize(self):
        """Initialize the chatbot"""
        try:
            logger.info("Initializing Enhanced Research Chatbot Interface...")
            
            if self.chatbot is not None:
                logger.info("Chatbot already initialized")
                return True
            
            self.chatbot = EnhancedResearchChatbot()
            logger.info("✓ Enhanced Research Chatbot Interface initialized successfully!")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize chatbot: {e}")
            self.chatbot = None
            return False
    
    async def ask_question(self, question: str, user_id: str = None) -> Dict[str, Any]:
        """Ask a question"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "answer": "Chatbot failed to initialize. Please check your configuration.",
                    "confidence": "low",
                    "approach": "initialization_error"
                }
        
        return await self.chatbot.chat(question, user_id)
    
    async def conduct_research(self, topic: str, user_id: str = None) -> Dict[str, Any]:
        """Conduct research"""
        if not self.chatbot:
            if not await self.initialize():
                return {
                    "error": "Chatbot failed to initialize",
                    "synthesis": "Unable to conduct research"
                }
        
        return await self.chatbot.conduct_research(topic, user_id)

# ============================
# DEMO AND TESTING
# ============================

async def demo_chatbot():
    """Demonstrate complex multi-agent chatbot capabilities"""
    interface = ChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize chatbot for demo")
        return
    
    print("\n" + "="*80)
    print("ENHANCED DEEP RESEARCH CHATBOT - 2025 VERSION")
    print("="*80)
    print("Features:")
    print("• Complex multi-agent research architecture")
    print("  - PlannerAgent: Creates detailed research plans")
    print("  - DomainExpertAgent: Jurisdiction-specific expertise")
    print("  - ConceptAnalysisAgent: Deep concept mapping")
    print("  - ComparativeAnalysisAgent: Cross-jurisdictional comparison")
    print("  - SynthesisAgent: Progressive knowledge synthesis")
    print("• Advanced Memory Management")
    print("  - LangMem long-term memory across sessions")
    print("  - Semantic and episodic memory storage")
    print("  - User preference tracking and adaptation")
    print("  - Cross-session knowledge persistence")
    print("  - Content prioritization and optimization")
    print("• LangGraph workflow orchestration")
    print("• Elasticsearch document retrieval")
    print("• Progressive token management and synthesis")
    print("• LLM-based domain filtering")
    print("• Privacy law specialization")
    print("• Uses o3-mini-2025-01-31 LLM model")
    print("• Uses text-embedding-3-large for embeddings")
    print("• Local tiktoken models (no internet required)")
    print("="*80)
    
    # Demo questions
    demo_questions = [
        "What are the key differences between GDPR and CCPA consent requirements?",
        "How do data breach notification timelines compare across major privacy frameworks?",
        "What are the compliance challenges for cross-border data transfers?"
    ]
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\n📌 Demo Question {i}: {question}")
        print("-" * 70)
        
        try:
            response = await interface.ask_question(question, user_id="demo_user")
            
            print(f"🎯 Confidence: {response.get('confidence', 'unknown')}")
            print(f"🧠 Approach: {response.get('approach', 'unknown')}")
            
            if response.get('agents_used'):
                print(f"🤖 Agents Used: {', '.join(response['agents_used'])}")
            
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            
            if response.get('overall_confidence_score'):
                print(f"📊 Confidence Score: {response['overall_confidence_score']:.2f}")
            
            # Show agent details if available
            if response.get('agent_details'):
                print("🔍 Agent Details:")
                for agent_name, details in response['agent_details'].items():
                    print(f"   - {agent_name}: {details['status']}, confidence: {details['confidence']:.2f}, findings: {details['findings_count']}")
            
            # Show research plan if available
            if response.get('research_plan'):
                plan = response['research_plan']
                print(f"📋 Research Plan: {len(plan.get('research_tasks', []))} tasks, strategy: {plan.get('coordination_strategy', 'unknown')}")
            
            # Show memory information if available
            if response.get('memory_info'):
                memory_info = response['memory_info']
                print(f"🧠 Memory: {memory_info.get('memories_stored', 0)} stored, {memory_info.get('memories_retrieved', 0)} retrieved")
                if memory_info.get('user_preferences_used'):
                    print("   User preferences applied")
                if memory_info.get('cross_session_learning'):
                    print("   Cross-session learning active")
            
            # Show token optimization info
            if response.get('token_optimization'):
                token_info = response['token_optimization']
                print(f"⚡ Token Optimization: {token_info.get('response_tokens', 'unknown')} tokens")
                if token_info.get('progressive_synthesis_used'):
                    print("   Progressive synthesis applied")
            
            print("\n📋 Answer:")
            answer = response['answer']
            print(answer[:800] + "..." if len(answer) > 800 else answer)
            
        except Exception as e:
            print(f"❌ Error: {e}")
        
        print("\n" + "="*80)

def check_setup():
    """Check system setup"""
    print("\n🔧 SYSTEM SETUP CHECK")
    print("="*50)
    
    setup_issues = []
    
    # Check configuration
    if config.OPENAI_API_KEY == "your-openai-api-key-here":
        print("❌ OPENAI_API_KEY: Not configured")
        setup_issues.append("OpenAI API Key")
    else:
        print("✅ OPENAI_API_KEY: Configured")
    
    if config.ES_PASSWORD == "your-elasticsearch-password-here":
        print("❌ ES_PASSWORD: Not configured")
        setup_issues.append("Elasticsearch Password")
    else:
        print("✅ ES_PASSWORD: Configured")
    
    # Check tiktoken model directory
    print(f"\n📁 Tiktoken Model Directory: {config.TIKTOKEN_MODEL_PATH}")
    if os.path.exists(config.TIKTOKEN_MODEL_PATH):
        print(f"✅ Directory exists: {config.TIKTOKEN_MODEL_PATH}")
        
        # Check for tiktoken cache files
        cache_files = [f for f in os.listdir(config.TIKTOKEN_MODEL_PATH) 
                      if f.endswith('.tiktoken') or len(f) == 40]  # SHA1 hash length
        
        if cache_files:
            print(f"✅ Found {len(cache_files)} tiktoken cache files")
            for file in cache_files[:3]:  # Show first 3
                print(f"   - {file}")
            if len(cache_files) > 3:
                print(f"   ... and {len(cache_files) - 3} more")
        else:
            print("⚠️  No tiktoken cache files found")
            print("   Run setup_tiktoken_models() to download them")
    else:
        print(f"⚠️  Directory not found: {config.TIKTOKEN_MODEL_PATH}")
        print("   Directory will be created automatically")
    
    # Check dependencies
    deps = {
        "openai": "OpenAI client",
        "tiktoken": "Token counting",
        "elasticsearch": "Elasticsearch client"
    }
    
    print(f"\n📦 Core Dependencies:")
    for dep, desc in deps.items():
        try:
            __import__(dep)
            print(f"✅ {dep}: Available ({desc})")
        except ImportError:
            print(f"❌ {dep}: Missing ({desc})")
            setup_issues.append(dep)
    
    # Check optional dependencies
    print(f"\n📦 Advanced Features:")
    print(f"✅ LangChain OpenAI: {'Available' if LANGCHAIN_OPENAI_AVAILABLE else 'Not available'}")
    print(f"✅ Elasticsearch LangChain: {'Available' if ELASTICSEARCH_LANGCHAIN_AVAILABLE else 'Not available'}")
    print(f"✅ LangGraph: {'Available' if LANGGRAPH_AVAILABLE else 'Not available'}")
    print(f"✅ LangMem: {'Available' if LANGMEM_AVAILABLE else 'Not available'}")
    
    # Show what features are enabled
    print(f"\n🧠 Memory & Advanced Features:")
    if LANGMEM_AVAILABLE:
        print("✅ Semantic memory (facts and concepts)")
        print("✅ Episodic memory (research sessions)")
        print("✅ Cross-session learning and adaptation")
        print("✅ User preference tracking")
    else:
        print("⚠️  Basic memory only (LangMem not available)")
    
    if LANGGRAPH_AVAILABLE:
        print("✅ Advanced workflow orchestration")
        print("✅ Multi-agent coordination")
        print("✅ Memory store with embeddings")
    else:
        print("⚠️  Simple orchestration (LangGraph not available)")
    
    if ELASTICSEARCH_LANGCHAIN_AVAILABLE:
        print("✅ Advanced document retrieval")
        print("✅ Semantic search capabilities")
    else:
        print("⚠️  Basic search only (Elasticsearch integration not available)")
    
    print("✅ Progressive token management with hierarchical synthesis")
    print("✅ Content prioritization and optimization")
    print("✅ Local tiktoken models (offline token counting)")
    print("✅ Multi-agent research coordination")
    print("✅ Domain-specific expertise agents")
    
    # Check model configuration
    print(f"\n🤖 Model Configuration:")
    print(f"✅ LLM Model: {config.MODEL}")
    print(f"✅ Embedding Model: {config.EMBEDDING_MODEL}")
    print(f"✅ Embedding Dimensions: {config.EMBEDDING_DIMENSIONS}")
    
    if setup_issues:
        print(f"\n❌ Setup issues found: {', '.join(setup_issues)}")
        print("\n📋 To fix:")
        print("1. Update configuration values in the Config class")
        print("2. Install missing dependencies:")
        print("   pip install openai tiktoken elasticsearch")
        print("   pip install langchain-openai langchain-elasticsearch langgraph langmem")
        print("3. Run setup_tiktoken_models() to download tiktoken files")
        return False
    else:
        print(f"\n✅ Setup verification complete!")
        return True

def setup_tiktoken_models():
    """Setup tiktoken models by triggering downloads to local cache"""
    print("\n🔧 SETTING UP TIKTOKEN MODELS")
    print("="*50)
    
    try:
        # Ensure directory exists
        os.makedirs(config.TIKTOKEN_MODEL_PATH, exist_ok=True)
        
        # Set environment variable for tiktoken cache
        os.environ["TIKTOKEN_CACHE_DIR"] = os.path.abspath(config.TIKTOKEN_MODEL_PATH)
        print(f"✅ Set tiktoken cache directory: {config.TIKTOKEN_MODEL_PATH}")
        
        # List of encodings to download
        encodings_to_download = [
            "o200k_base",  # For o3-mini and newer models
            "cl100k_base", # For GPT-4 and GPT-3.5-turbo
            "p50k_base",   # For Codex models
        ]
        
        print("\n📥 Downloading tiktoken encodings...")
        
        for encoding_name in encodings_to_download:
            try:
                print(f"   Downloading {encoding_name}...")
                encoder = tiktoken.get_encoding(encoding_name)
                # Test the encoder
                test_tokens = encoder.encode("Hello world!")
                print(f"   ✅ {encoding_name}: Downloaded and tested ({len(test_tokens)} tokens for 'Hello world!')")
            except Exception as e:
                print(f"   ❌ {encoding_name}: Failed to download - {e}")
        
        # Also try to download encoding for specific models
        models_to_try = ["gpt-4", "gpt-3.5-turbo"]
        
        print("\n📥 Downloading model-specific encodings...")
        
        for model in models_to_try:
            try:
                print(f"   Downloading encoding for {model}...")
                encoder = tiktoken.encoding_for_model(model)
                test_tokens = encoder.encode("Hello world!")
                print(f"   ✅ {model}: Downloaded and tested")
            except Exception as e:
                print(f"   ❌ {model}: Failed - {e}")
        
        # Check what was downloaded
        print(f"\n📁 Checking cache directory: {config.TIKTOKEN_MODEL_PATH}")
        if os.path.exists(config.TIKTOKEN_MODEL_PATH):
            files = os.listdir(config.TIKTOKEN_MODEL_PATH)
            cache_files = [f for f in files if f.endswith('.tiktoken') or len(f) == 40]
            
            if cache_files:
                print(f"✅ Successfully cached {len(cache_files)} tiktoken files:")
                for file in cache_files:
                    file_path = os.path.join(config.TIKTOKEN_MODEL_PATH, file)
                    file_size = os.path.getsize(file_path)
                    print(f"   - {file} ({file_size:,} bytes)")
            else:
                print("❌ No cache files found")
        
        print("\n✅ Tiktoken model setup complete!")
        print("💡 Tip: These files will now be used for offline token counting")
        
    except Exception as e:
        print(f"\n❌ Error setting up tiktoken models: {e}")
        print("💡 Manual setup:")
        print("   1. Create 'tiktoken_model' directory")
        print("   2. Set TIKTOKEN_CACHE_DIR environment variable")
        print("   3. Run Python once with internet to download encodings")

async def interactive_session():
    """Interactive session"""
    interface = ChatbotInterface()
    
    if not await interface.initialize():
        print("❌ Failed to initialize chatbot")
        return
    
    print("\n🚀 Enhanced Research Chatbot - Interactive Session")
    print("Using o3-mini-2025-01-31 and text-embedding-3-large with local tiktoken")
    print("Features: Multi-Agent Research + Advanced Memory + Progressive Synthesis")
    print("Type 'exit' to quit, 'help' for commands, 'memory' for features status")
    
    user_id = input("Enter your user ID (for memory): ").strip() or "default_user"
    print(f"✅ Session started! User: {user_id}")
    
    while True:
        try:
            question = input("\n💬 Your question: ").strip()
            
            if question.lower() in ['exit', 'quit', 'bye']:
                print("👋 Goodbye!")
                break
            
            if question.lower() == 'help':
                print("\n🔧 Available Commands:")
                print("• 'research [topic]' - Conduct complex multi-agent research")
                print("  Features: Planning → Domain Analysis → Concept Analysis → Comparison → Synthesis")
                print("• 'agents' - Show available specialized agents")
                print("• 'memory' - Show memory and advanced features status")
                print("• 'exit' - Quit session")
                continue
            
            if question.lower() == 'memory':
                print("\n🧠 Memory & Advanced Features Status:")
                print(f"• LangMem Available: {'✅ Yes' if LANGMEM_AVAILABLE else '❌ No'}")
                print(f"• LangGraph Available: {'✅ Yes' if LANGGRAPH_AVAILABLE else '❌ No'}")
                print(f"• Elasticsearch Available: {'✅ Yes' if ELASTICSEARCH_LANGCHAIN_AVAILABLE else '❌ No'}")
                print("• Memory Features:")
                print("  - Cross-session memory persistence")
                print("  - User preference tracking and adaptation")
                print("  - Semantic memory for facts and concepts")
                print("  - Episodic memory for research sessions")
                print("  - Progressive token-optimized synthesis")
                print("  - Content prioritization and optimization")
                print("• Advanced Token Management:")
                print("  - Local tiktoken models (no internet required)")
                print("  - Progressive synthesis with hierarchical summarization")
                print("  - Intelligent content truncation and prioritization")
                print("• Multi-Agent Coordination:")
                print("  - 5 specialized research agents")
                print("  - Cross-agent knowledge sharing")
                print("  - Iterative research refinement")
                continue
            
            if question.lower() == 'agents':
                print("\n🤖 Available Specialized Agents:")
                print("• PlannerAgent: Creates detailed research plans and coordinates agents")
                print("• DomainExpertAgent: Provides jurisdiction-specific domain expertise")
                print("• ConceptAnalysisAgent: Performs deep concept analysis and relationship mapping") 
                print("• ComparativeAnalysisAgent: Conducts cross-jurisdictional comparative analysis")
                print("• SynthesisAgent: Synthesizes knowledge and generates comprehensive reports")
                continue
            
            if question.lower().startswith('research '):
                topic = question[9:].strip()
                if topic:
                    print(f"\n🔬 Conducting complex multi-agent research on: {topic}")
                    result = await interface.conduct_research(topic, user_id)
                    print(f"📊 Research completed with confidence: {result.get('overall_confidence', 0):.2f}")
                    print(f"🤖 Agents used: {', '.join(result.get('agents_used', []))}")
                    print(f"🔄 Iterations: {result.get('iterations_completed', 0)}")
                    
                    if result.get('research_plan'):
                        plan = result['research_plan']
                        print(f"📋 Research Plan: {len(plan.get('research_tasks', []))} tasks")
                        print(f"🎯 Strategy: {plan.get('coordination_strategy', 'unknown')}")
                    
                    if result.get('memory_features'):
                        memory_features = result['memory_features']
                        print("🧠 Memory Features:")
                        print(f"   - Long-term memory: {'Active' if memory_features['long_term_memory_used'] else 'Inactive'}")
                        print(f"   - Memories stored: {memory_features['memories_stored']}")
                        print(f"   - Cross-session: {'Active' if memory_features['cross_session_persistence'] else 'Inactive'}")
                        if memory_features.get('user_preferences_tracked'):
                            print("   - User preferences: Tracked and applied")
                    
                    if result.get('advanced_features'):
                        advanced = result['advanced_features']
                        print("🔬 Advanced Features:")
                        if advanced.get('semantic_memory'):
                            print("   - Semantic memory: Active")
                        if advanced.get('episodic_memory'):
                            print("   - Episodic memory: Active")
                        if advanced.get('progressive_token_management'):
                            print("   - Progressive token management: Active")
                        if advanced.get('hierarchical_synthesis'):
                            print("   - Hierarchical synthesis: Active")
                    
                    print("\n📋 Research Report:")
                    synthesis = result.get('synthesis', 'No synthesis available')
                    print(synthesis[:1000] + "..." if len(synthesis) > 1000 else synthesis)
                continue
            
            if not question:
                print("Please enter a question.")
                continue
            
            print("\n🧠 Processing...")
            response = await interface.ask_question(question, user_id)
            
            print(f"\n📋 Answer (Confidence: {response.get('confidence', 'unknown')}):")
            if response.get('agents_used'):
                print(f"🤖 Agents: {', '.join(response['agents_used'])}")
            
            if response.get('iterations_completed'):
                print(f"🔄 Iterations: {response['iterations_completed']}")
            
            if response.get('overall_confidence_score'):
                print(f"📊 Confidence Score: {response['overall_confidence_score']:.2f}")
            
            # Show agent details if available
            if response.get('agent_details'):
                print("🔍 Agent Performance:")
                for agent_name, details in response['agent_details'].items():
                    print(f"   - {agent_name}: {details['status']}, confidence: {details['confidence']:.2f}")
            
            # Show memory information
            if response.get('memory_info'):
                memory_info = response['memory_info']
                print("🧠 Memory Features:")
                print(f"   - Memories stored: {memory_info.get('memories_stored', 0)}")
                print(f"   - Memories retrieved: {memory_info.get('memories_retrieved', 0)}")
                if memory_info.get('user_preferences_used'):
                    print("   - User preferences: Applied")
                if memory_info.get('cross_session_learning'):
                    print("   - Cross-session learning: Active")
            
            print("="*70)
            print(response['answer'])
            print("="*70)
            
        except KeyboardInterrupt:
            print("\n👋 Goodbye!")
            break
        except Exception as e:
            print(f"\n❌ Error: {e}")

async def main():
    """Main function"""
    print("Enhanced Deep Research Chatbot - 2025 Version")
    print("Using o3-mini and text-embedding-3-large with local tiktoken models")
    print("=" * 70)
    
    if not check_setup():
        print("\n⚠️  Please fix setup issues before continuing.")
        setup_choice = input("\nWould you like to setup tiktoken models now? (y/n): ").strip().lower()
        if setup_choice == 'y':
            setup_tiktoken_models()
        return
    
    mode = input("\nChoose mode:\n1. Interactive Session\n2. Demo\n3. Setup Tiktoken Models\n4. Setup Check Only\nEnter choice (1-4): ").strip()
    
    if mode == "1":
        await interactive_session()
    elif mode == "2":
        await demo_chatbot()
    elif mode == "3":
        setup_tiktoken_models()
    elif mode == "4":
        print("✅ Setup check completed.")
    else:
        print("Invalid choice. Please run again and select 1-4.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n👋 Goodbye!")
    except Exception as e:
        print(f"❌ Fatal error: {e}")
