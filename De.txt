import os
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import List, Dict, Optional
from dotenv import load_dotenv
from azure.identity import ClientSecretCredential
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.schema import TextNode
from llama_index.core.vector_stores import SimpleVectorStore
from llama_index.embeddings.openai import AzureOpenAIEmbedding
from llama_index.core import Settings

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def str_to_bool(s: str) -> bool:
    return s.lower() in ['true', '1', 't', 'y', 'yes']

class OSEnv:
    """Full original environment management class"""
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.token = None
        
        # Load configurations
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        
        # Configure proxy if needed
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            
        # Get Azure token if required
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()

    def set_certificate_path(self, certificate_path: str) -> None:
        if not os.path.isfile(certificate_path):
            raise FileNotFoundError(f"Certificate not found: {certificate_path}")
        
        cert_path = str(Path(certificate_path))
        self.set("REQUESTS_CA_BUNDLE", cert_path, False)
        self.set("SSL_CERT_FILE", cert_path, False)
        self.set("CURL_CA_BUNDLE", cert_path, False)
        logger.info(f"Certificate path set: {cert_path}")

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        try:
            with open(dotenvfile) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        key, val = line.split('=', 1)
                        self.set(key.strip(), val.strip().strip('"\''), print_val)
        except Exception as e:
            logger.error(f"Error loading {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val and var_name not in ['AZURE_CLIENT_SECRET', 'AD_USER_PW']:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        return os.getenv(var_name, default)

    def set_proxy(self) -> None:
        proxy_url = f"http://{self.get('AD_USERNAME')}:{self.get('AD_USER_PW')}@{self.get('HTTPS_PROXY_DOMAIN')}"
        self.set("HTTP_PROXY", proxy_url, False)
        self.set("HTTPS_PROXY", proxy_url, False)
        self.set("NO_PROXY", "localhost,127.0.0.1", False)
        logger.info("Proxy configured")

    def get_azure_token(self) -> str:
        credential = ClientSecretCredential(
            tenant_id=self.get("AZURE_TENANT_ID"),
            client_id=self.get("AZURE_CLIENT_ID"),
            client_secret=self.get("AZURE_CLIENT_SECRET")
        )
        token = credential.get_token("https://cognitiveservices.azure.com/.default")
        self.set("AZURE_TOKEN", token.token, False)
        return token.token

class AzurePBTPredictor:
    def __init__(self, env: OSEnv):
        """Initialize with OSEnv configuration"""
        self.env = env
        self.min_confidence = 0.01
        
        # Configure Azure OpenAI embeddings
        Settings.embed_model = AzureOpenAIEmbedding(
            model=env.get("AZURE_EMBEDDING_MODEL", "text-embedding-3-large"),
            deployment_name=env.get("AZURE_EMBEDDING_DEPLOYMENT"),
            api_key=env.get("AZURE_OPENAI_API_KEY"),
            azure_endpoint=env.get("AZURE_OPENAI_ENDPOINT"),
            api_version=env.get("AZURE_OPENAI_API_VERSION", "2024-02-01")
        )

    def _create_node(self, row: pd.Series, is_source: bool) -> TextNode:
        """Create enriched text nodes"""
        if is_source:
            text = f"Title: {row['name']}\nDescription: {row['description']}"
            metadata = {"type": "source", **row.to_dict()}
        else:
            text = f"Term: {row['pbt-name']}\nDefinition: {row['pbt-definition']}"
            metadata = {"type": "target", "pbt_name": row["pbt-name"], "pbt_definition": row["pbt-definition"]}
        
        return TextNode(
            text=text,
            metadata=metadata,
            embedding=Settings.embed_model.get_text_embedding(text)
        )

    def build_index(self, df: pd.DataFrame, is_source: bool) -> VectorStoreIndex:
        """Build vector index with error handling"""
        nodes = [self._create_node(row, is_source) for _, row in df.iterrows()]
        
        vector_store = SimpleVectorStore()
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        vector_store.add(nodes)
        
        return VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model
        )

    def _ensure_matches(self, matches: List[Dict]) -> List[Dict]:
        """Guarantee 4 matches with fallbacks"""
        while len(matches) < 4:
            matches.append({
                "pbt_name": "No match",
                "pbt_definition": "Insufficient matches",
                "score": max(self.min_confidence, 0.01 * len(matches))
            })
        return sorted(matches, key=lambda x: -x["score"])[:4]

    def get_predictions(self, source_df: pd.DataFrame, target_df: pd.DataFrame) -> pd.DataFrame:
        """Main prediction workflow"""
        target_index = self.build_index(target_df, False)
        results = []
        
        for _, row in source_df.iterrows():
            try:
                source_node = self._create_node(row, True)
                retriever = target_index.as_retriever(similarity_top_k=6)
                matches = [
                    {"pbt_name": m.node.metadata["pbt_name"], 
                     "pbt_definition": m.node.metadata["pbt_definition"],
                     "score": m.score}
                    for m in retriever.retrieve(source_node.text)
                ]
            except Exception as e:
                logger.error(f"Error processing {row['name']}: {str(e)}")
                matches = []
                
            final_matches = self._ensure_matches(matches)
            results.append({
                "name": row["name"],
                "description": row["description"],
                **{f"match_{i+1}": m for i, m in enumerate(final_matches)}
            })
        
        return self._format_output(results)

    def _format_output(self, results: List[Dict]) -> pd.DataFrame:
        """Create structured output"""
        formatted = []
        for entry in results:
            row = {
                "name": entry["name"],
                "description": entry["description"]
            }
            for i in range(4):
                match = entry.get(f"match_{i+1}", {})
                row.update({
                    f"match_{i+1}_pbt_name": match.get("pbt_name", "N/A"),
                    f"match_{i+1}_score": match.get("score", 0.0),
                    f"match_{i+1}_definition": match.get("pbt_definition", "N/A")
                })
            formatted.append(row)
        return pd.DataFrame(formatted)

    def save_results(self, df: pd.DataFrame, output_path: str) -> None:
        """Save with guaranteed columns"""
        required_cols = [f"match_{i}_{field}" for i in range(1,5) for field in ["pbt_name", "score", "definition"]]
        for col in required_cols:
            if col not in df.columns:
                df[col] = "N/A"
        df.to_csv(output_path, index=False)

def main():
    """Full execution flow"""
    try:
        # Initialize environment
        env = OSEnv(
            config_file="env/config.env",
            creds_file="env/credentials.env",
            certificate_path="env/cacert.pem"
        )
        
        # Load data
        data_dir = Path("data")
        source_df = pd.read_csv(data_dir/"source.csv").fillna("").astype(str)
        target_df = pd.read_csv(data_dir/"target.csv").fillna("").astype(str)
        
        # Validate
        for df, cols in [(source_df, {"name", "description"}), 
                       (target_df, {"pbt-name", "pbt-definition"})]:
            if not cols.issubset(df.columns):
                raise ValueError(f"Missing required columns in {df.columns}")
        
        # Process predictions
        predictor = AzurePBTPredictor(env)
        results = predictor.get_predictions(source_df, target_df)
        
        # Save output
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        predictor.save_results(results, output_dir/"pbt_predictions.csv")
        logger.info("Processing completed successfully")
        
    except Exception as e:
        logger.error(f"Critical failure: {str(e)}")
        raise

if __name__ == "__main__":
    main()
