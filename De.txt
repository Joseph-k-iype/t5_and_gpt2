#!/usr/bin/env python3
"""
PII Classification Tool using OpenAI's o3-mini model
Classifies data fields for Personal Data with detailed reasoning and confidence scores
"""

import pandas as pd
import requests
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import argparse
import sys

# ============================================================================
# GLOBAL CONFIGURATION - UPDATE THESE VALUES
# ============================================================================

# OpenAI API Configuration
BASE_URL = "https://api.openai.com/v1"  # Update with your API base URL
API_KEY = "your-api-key-here"           # Update with your API key
MODEL_NAME = "o3-mini-2025-01-31"       # OpenAI model name

# File Configuration
DEFAULT_CSV_PATH = "data.csv"           # Default CSV file path
OUTPUT_PREFIX = "pii_classification_results"  # Output file prefix
API_DELAY = 0.1                         # Delay between API calls (seconds)

# ============================================================================

class PIIClassifier:
    def __init__(self, base_url: str = None, api_key: str = None, model_name: str = None):
        """
        Initialize the PII Classifier
        
        Args:
            base_url (str, optional): OpenAI API base URL. Uses global BASE_URL if None
            api_key (str, optional): OpenAI API key. Uses global API_KEY if None
            model_name (str, optional): Model name. Uses global MODEL_NAME if None
        """
        self.base_url = (base_url or BASE_URL).rstrip('/')
        self.api_key = api_key or API_KEY
        self.model = model_name or MODEL_NAME
        
        # Validate configuration
        if not self.api_key or self.api_key == "your-api-key-here":
            raise ValueError("API key not configured. Please update API_KEY global variable or pass api_key parameter.")
        
        if not self.base_url:
            raise ValueError("Base URL not configured. Please update BASE_URL global variable or pass base_url parameter.")
        
    def classify_field(self, name: str, definition: str) -> Dict:
        """
        Classify a single data field for PII using OpenAI o3-mini
        
        Args:
            name (str): Field name
            definition (str): Field definition
            
        Returns:
            Dict: Classification result with reasoning
        """
        prompt = f"""You are a data privacy expert. Analyze the following data field and classify it across two dimensions: sensitivity level and personal data risk.

Data Field Name: "{name}"
Data Field Definition: "{definition}"

Please provide your analysis in the following JSON format:
{{
  "sensitivity_classification": "string (Highly Sensitive, Sensitive, or Non-sensitive)",
  "contains_personal_data": boolean,
  "personal_data_risk_level": "string (High Risk, Medium Risk, Low Risk, or Not PII)",
  "supporting_reasons": ["reason1", "reason2", "reason3"],
  "contradicting_reasons": ["reason1", "reason2", "reason3"],
  "confidence_score": number (0-100),
  "explanation": "detailed explanation of both classifications"
}}

Classification Guidelines:

SENSITIVITY LEVELS:
- Highly Sensitive: Data that could cause severe harm if disclosed (SSN, financial data, health records, biometrics)
- Sensitive: Data that could cause moderate harm if disclosed (names, contact info, demographics)
- Non-sensitive: Data that poses minimal risk if disclosed (public info, technical metadata, aggregated data)

PERSONAL DATA RISK LEVELS:
- High Risk: Direct identifiers that uniquely identify individuals (SSN, passport, driver's license, credit cards)
- Medium Risk: Quasi-identifiers that can identify when combined (name, email, phone, address, DOB)
- Low Risk: Demographic or behavioral data that may indirectly identify (age range, preferences, usage patterns)
- Not PII: Anonymous, aggregated, or purely technical data with no personal identification capability

The contains_personal_data flag should be true for High Risk, Medium Risk, and Low Risk classifications, and false for Not PII.

Provide exactly 3 supporting reasons why these classifications are correct and 3 contradicting reasons why they might be wrong. Base confidence score on the strength of evidence for both classifications combined."""

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        payload = {
            'model': self.model,
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ]
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # Extract JSON from the response
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("No valid JSON found in response")
                
            json_content = content[start_idx:end_idx]
            result = json.loads(json_content)
            
            # Validate required fields
            required_fields = ['sensitivity_classification', 'contains_personal_data', 'personal_data_risk_level', 
                             'supporting_reasons', 'contradicting_reasons', 'confidence_score', 'explanation']
            
            for field in required_fields:
                if field not in result:
                    result[field] = None
                    
            return result
            
        except requests.exceptions.RequestException as e:
            return {
                'sensitivity_classification': 'Error',
                'contains_personal_data': None,
                'personal_data_risk_level': 'Error',
                'supporting_reasons': [],
                'contradicting_reasons': [],
                'confidence_score': 0,
                'explanation': f'API Error: {str(e)}'
            }
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            return {
                'sensitivity_classification': 'Error',
                'contains_personal_data': None,
                'personal_data_risk_level': 'Error',
                'supporting_reasons': [],
                'contradicting_reasons': [],
                'confidence_score': 0,
                'explanation': f'Response parsing error: {str(e)}'
            }
    
    def process_csv(self, csv_path: str = None, output_prefix: str = None, 
                   delay: float = None) -> Tuple[List[Dict], str, str]:
        """
        Process a CSV file and classify all fields
        
        Args:
            csv_path (str, optional): Path to input CSV file. Uses global DEFAULT_CSV_PATH if None
            output_prefix (str, optional): Prefix for output files. Uses global OUTPUT_PREFIX if None
            delay (float, optional): Delay between API calls in seconds. Uses global API_DELAY if None
            
        Returns:
            Tuple[List[Dict], str, str]: Results list, JSON path, CSV path
        """
        # Use global defaults if not provided
        csv_path = csv_path or DEFAULT_CSV_PATH
        output_prefix = output_prefix or OUTPUT_PREFIX
        delay = delay if delay is not None else API_DELAY
        # Read CSV file
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            raise ValueError(f"Error reading CSV file: {str(e)}")
        
        # Validate required columns
        required_columns = ['Name', 'Definition']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Remove rows with empty Name or Definition
        df = df.dropna(subset=['Name', 'Definition'])
        df = df[df['Name'].str.strip() != '']
        df = df[df['Definition'].str.strip() != '']
        
        if df.empty:
            raise ValueError("No valid data rows found in CSV")
        
        print(f"Processing {len(df)} rows...")
        
        results = []
        for index, row in df.iterrows():
            name = str(row['Name']).strip()
            definition = str(row['Definition']).strip()
            
            print(f"Processing {index + 1}/{len(df)}: {name}")
            
            # Classify the field
            classification = self.classify_field(name, definition)
            
            # Combine with original data
            result = {
                'original_name': name,
                'original_definition': definition,
                **classification,
                'processed_at': datetime.now().isoformat()
            }
            
            results.append(result)
            
            # Add delay to avoid rate limiting
            if delay > 0:
                time.sleep(delay)
        
        # Generate output file paths
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = f"{output_prefix}_{timestamp}.json"
        csv_output_path = f"{output_prefix}_{timestamp}.csv"
        
        # Save as JSON
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # Save as CSV
        results_df = pd.DataFrame(results)
        
        # Convert list columns to string for CSV
        list_columns = ['supporting_reasons', 'contradicting_reasons']
        for col in list_columns:
            if col in results_df.columns:
                results_df[col] = results_df[col].apply(
                    lambda x: '; '.join(x) if isinstance(x, list) else str(x)
                )
        
        results_df.to_csv(csv_output_path, index=False, encoding='utf-8')
        
        print(f"\nProcessing complete!")
        print(f"Results saved to:")
        print(f"  JSON: {json_path}")
        print(f"  CSV:  {csv_output_path}")
        
        return results, json_path, csv_output_path
    
    def print_summary(self, results: List[Dict]):
        """Print a summary of classification results"""
        if not results:
            print("No results to summarize")
            return
        
        # Count classifications
        sensitivity_counts = {}
        personal_data_counts = {}
        confidence_scores = []
        personal_data_true_count = 0
        
        for result in results:
            # Sensitivity classification counts
            sensitivity = result.get('sensitivity_classification', 'Unknown')
            sensitivity_counts[sensitivity] = sensitivity_counts.get(sensitivity, 0) + 1
            
            # Personal data risk level counts
            risk_level = result.get('personal_data_risk_level', 'Unknown')
            personal_data_counts[risk_level] = personal_data_counts.get(risk_level, 0) + 1
            
            # Count personal data flags
            if result.get('contains_personal_data') is True:
                personal_data_true_count += 1
            
            # Collect confidence scores
            if isinstance(result.get('confidence_score'), (int, float)):
                confidence_scores.append(result['confidence_score'])
        
        print("\n" + "="*70)
        print("CLASSIFICATION SUMMARY")
        print("="*70)
        
        print(f"Total fields processed: {len(results)}")
        print(f"Average confidence score: {sum(confidence_scores)/len(confidence_scores):.1f}%" if confidence_scores else "N/A")
        print(f"Fields containing personal data: {personal_data_true_count} ({(personal_data_true_count/len(results)*100):.1f}%)")
        
        print("\nSENSITIVITY CLASSIFICATION:")
        for sensitivity, count in sorted(sensitivity_counts.items()):
            percentage = (count / len(results)) * 100
            print(f"  {sensitivity}: {count} ({percentage:.1f}%)")
        
        print("\nPERSONAL DATA RISK LEVELS:")
        for risk_level, count in sorted(personal_data_counts.items()):
            percentage = (count / len(results)) * 100
            print(f"  {risk_level}: {count} ({percentage:.1f}%)")


def main():
    """Main function for command line usage"""
    parser = argparse.ArgumentParser(description='PII Classification Tool using OpenAI o3-mini')
    parser.add_argument('csv_file', nargs='?', default=None, 
                       help=f'Path to input CSV file with Name and Definition columns (default: {DEFAULT_CSV_PATH})')
    parser.add_argument('--base-url', default=None, 
                       help=f'OpenAI API base URL (default: {BASE_URL})')
    parser.add_argument('--api-key', default=None, 
                       help='OpenAI API key (default: uses global API_KEY)')
    parser.add_argument('--model', default=None, 
                       help=f'Model name (default: {MODEL_NAME})')
    parser.add_argument('--output-prefix', default=None, 
                       help=f'Prefix for output files (default: {OUTPUT_PREFIX})')
    parser.add_argument('--delay', type=float, default=None, 
                       help=f'Delay between API calls in seconds (default: {API_DELAY})')
    parser.add_argument('--summary', action='store_true', 
                       help='Print detailed summary of results')
    
    args = parser.parse_args()
    
    # Use command line argument or global default for CSV file
    csv_file = args.csv_file or DEFAULT_CSV_PATH
    
    # Validate input file
    if not os.path.exists(csv_file):
        print(f"Error: Input file '{csv_file}' not found")
        print(f"Please ensure the file exists or update DEFAULT_CSV_PATH in the script")
        sys.exit(1)
    
    try:
        # Initialize classifier with command line args or global defaults
        classifier = PIIClassifier(
            base_url=args.base_url,
            api_key=args.api_key,
            model_name=args.model
        )
        
        # Process the CSV file
        results, json_path, csv_path = classifier.process_csv(
            csv_path=csv_file,
            output_prefix=args.output_prefix,
            delay=args.delay
        )
        
        # Print summary if requested
        if args.summary:
            classifier.print_summary(results)
            
    except Exception as e:
        print(f"Error: {str(e)}")
        sys.exit(1)


def quick_run():
    """
    Quick run function using global configuration
    Just update the global variables and call this function
    """
    try:
        print("Starting PII Classification with global configuration...")
        print(f"Using CSV: {DEFAULT_CSV_PATH}")
        print(f"Using Model: {MODEL_NAME}")
        print(f"Using Base URL: {BASE_URL}")
        print("-" * 60)
        
        classifier = PIIClassifier()
        results, json_path, csv_path = classifier.process_csv()
        classifier.print_summary(results)
        
        print(f"\nFiles saved:")
        print(f"  JSON: {json_path}")
        print(f"  CSV: {csv_path}")
        
    except Exception as e:
        print(f"Error in quick_run: {str(e)}")
        print("\nPlease check your global configuration:")
        print(f"  BASE_URL: {BASE_URL}")
        print(f"  API_KEY: {'*' * 10 if API_KEY and API_KEY != 'your-api-key-here' else 'NOT SET'}")
        print(f"  DEFAULT_CSV_PATH: {DEFAULT_CSV_PATH}")


if __name__ == "__main__":
    # Check if no command line arguments provided - use quick run
    if len(sys.argv) == 1:
        print("No command line arguments provided. Using global configuration...")
        quick_run()
    else:
        main()


# Example usage as a module:
"""
Method 1: Using global configuration (recommended)
1. Update the global variables at the top of this file:
   - BASE_URL = "https://api.openai.com/v1"
   - API_KEY = "your-actual-api-key"
   - DEFAULT_CSV_PATH = "your_data.csv"

2. Then use simply:
from pii_classifier import PIIClassifier

classifier = PIIClassifier()  # Uses global config
results, json_path, csv_path = classifier.process_csv()  # Uses global CSV path
classifier.print_summary(results)

# Or classify individual fields
result = classifier.classify_field("user_email", "Email address of the user")
print(f"Sensitivity: {result['sensitivity_classification']}")
print(f"Contains Personal Data: {result['contains_personal_data']}")
print(f"Risk Level: {result['personal_data_risk_level']}")

Method 2: Override global configuration
classifier = PIIClassifier(
    base_url="https://custom.api.com/v1",
    api_key="custom-api-key"
)
results, json_path, csv_path = classifier.process_csv("custom_data.csv")

Method 3: Quick start script
# Just update the global variables and run:
if __name__ == "__main__":
    classifier = PIIClassifier()
    results, json_path, csv_path = classifier.process_csv()
    classifier.print_summary(results)
"""
