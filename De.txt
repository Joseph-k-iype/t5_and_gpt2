"""
Advanced ELDM Field Mapping System with Hierarchical Graph RAG
Uses FalkorDB native vector indexes, full-text search, hybrid search, and Graph RAG
Models ELDM hierarchy: Conceptual Entity -> Entity -> Attribute
"""

import os
import json
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, TypedDict, Annotated, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from scipy.stats import zscore
import operator

# OpenAI and LangChain imports
import openai
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# FalkorDB
from falkordb import FalkorDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI API Configuration
BASE_URL = "https://your-openai-gateway.company.com/v1"
API_KEY = "your-api-key-here"
REASONING_MODEL = "o3-mini"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# FalkorDB Configuration
FALKORDB_HOST = "localhost"
FALKORDB_PORT = 6379
GRAPH_NAME = "ELDM_MAPPING"

# File Paths
TRANSACTION_FILE = "transaction_fields.xlsx"
ELDM_FILE = "eldm_attributes.xlsx"
OUTPUT_FILE = "eldm_mapping_results.xlsx"

# Search Configuration
HYBRID_SEARCH_ALPHA = 0.6  # Weight for vector vs full-text (0.6 = 60% vector, 40% text)

# Confidence Thresholds
HIGH_CONFIDENCE_THRESHOLD = 0.85
MEDIUM_CONFIDENCE_THRESHOLD = 0.70
LOW_CONFIDENCE_THRESHOLD = 0.50

# Processing Configuration
BATCH_SIZE = 50
TOP_K_CANDIDATES = 5


# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class TransactionField:
    """Represents a field from the transaction file"""
    tt_file_name: str
    tt_column_name: str
    standardised_name: str
    is_primary_key: bool
    comment: str
    data_type: str
    creation_sql: str
    transformation_sql: str
    
    def get_context(self) -> str:
        """Get rich context for this field"""
        context = f"""
Field: {self.tt_column_name}
Standardised Name: {self.standardised_name}
File: {self.tt_file_name}
Data Type: {self.data_type}
Primary Key: {self.is_primary_key}
Description: {self.comment}

SQL Context:
Creation: {self.creation_sql}
Transformation: {self.transformation_sql}
"""
        return context.strip()
    
    def get_searchable_text(self) -> str:
        """Get text for embedding and full-text search"""
        parts = [
            self.tt_column_name,
            self.standardised_name,
            self.comment,
            self.data_type,
            f"primary_key_{self.is_primary_key}"
        ]
        return " ".join([p for p in parts if p and str(p).strip()])


@dataclass
class ELDMNode:
    """Represents a node in the ELDM hierarchy"""
    name: str
    node_type: str  # 'CONCEPTUAL_ENTITY', 'ENTITY', 'ATTRIBUTE'
    description: str
    parent_name: Optional[str]
    full_path: str  # e.g., "Customer Management > Customer > Customer ID"
    embedding: Optional[np.ndarray] = None
    
    def get_searchable_text(self) -> str:
        """Get text representation for embedding and indexing"""
        return f"{self.name} {self.description} {self.full_path}"


@dataclass
class MappingResult:
    """Represents a mapping result with confidence scores"""
    transaction_field: str
    eldm_attribute: str
    eldm_entity: str
    eldm_conceptual_entity: str
    mapping_level: str  # 'ATTRIBUTE', 'ENTITY', 'CONCEPTUAL_ENTITY'
    confidence_score: float
    vector_similarity: float
    fulltext_score: float
    hybrid_score: float
    graph_rag_score: float
    reasoning_confidence: float
    validation_score: float
    reasoning_explanation: str
    validation_notes: str
    statistical_metrics: Dict[str, float]
    confidence_level: str
    retrieval_method: str
    hierarchy_path: str
    timestamp: str


class GraphState(TypedDict):
    """State for the LangGraph workflow"""
    transaction_field: TransactionField
    vector_candidates: List[Dict[str, Any]]
    fulltext_candidates: List[Dict[str, Any]]
    hybrid_candidates: List[Dict[str, Any]]
    graph_rag_candidates: List[Dict[str, Any]]
    final_candidates: List[Dict[str, Any]]
    mapping_proposals: List[Dict[str, Any]]
    validation_results: List[Dict[str, Any]]
    final_mapping: Optional[MappingResult]
    messages: Annotated[List, operator.add]
    iteration_count: int
    memory_context: Dict[str, Any]


# ============================================================================
# EMBEDDING SERVICE
# ============================================================================

class EmbeddingService:
    """Direct OpenAI API embedding service"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            base_url=BASE_URL,
            api_key=API_KEY
        )
        logger.info(f"Initialized EmbeddingService with model: {EMBEDDING_MODEL}")
    
    def create_embedding(self, text: str) -> np.ndarray:
        """Create embedding for a single text"""
        try:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text,
                dimensions=EMBEDDING_DIMENSIONS
            )
            embedding = np.array(response.data[0].embedding)
            return embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    def create_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """Create embeddings for multiple texts"""
        try:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=texts,
                dimensions=EMBEDDING_DIMENSIONS
            )
            embeddings = [np.array(item.embedding) for item in response.data]
            logger.info(f"Created {len(embeddings)} embeddings in batch")
            return embeddings
        except Exception as e:
            logger.error(f"Error creating batch embeddings: {e}")
            raise


# ============================================================================
# FALKORDB HIERARCHICAL GRAPH RAG
# ============================================================================

class ELDMHierarchicalGraphRAG:
    """FalkorDB-based Graph RAG system with hierarchical ELDM modeling"""
    
    def __init__(self):
        self.db = FalkorDB(
            host=FALKORDB_HOST,
            port=FALKORDB_PORT
        )
        self.graph = self.db.select_graph(GRAPH_NAME)
        logger.info(f"Connected to FalkorDB graph: {GRAPH_NAME}")
    
    def clear_graph(self):
        """Clear all data from the graph"""
        self.graph.query("MATCH (n) DETACH DELETE n")
        logger.info("Cleared FalkorDB graph")
    
    def create_indexes(self):
        """Create vector and full-text indexes using FalkorDB native functions"""
        try:
            # Create vector index for all ELDM nodes with cosine similarity
            vector_index_query = f"""
            CREATE VECTOR INDEX FOR (n:ELDMNode) ON (n.embedding) 
            OPTIONS {{
                dimension: {EMBEDDING_DIMENSIONS}, 
                similarityFunction: 'cosine',
                M: 32,
                efConstruction: 200,
                efRuntime: 10
            }}
            """
            self.graph.query(vector_index_query)
            logger.info(f"Created vector index with cosine similarity")
            
            # Create full-text index using correct syntax
            # FalkorDB full-text index syntax: just pass field names
            fulltext_index_query = """
            CALL db.idx.fulltext.createNodeIndex('ELDMNode', 'name', 'description', 'full_path', 'searchable_text')
            """
            self.graph.query(fulltext_index_query)
            logger.info("Created full-text index on ELDM nodes")
            
            # Create property indexes for efficient lookups
            self.graph.query("CREATE INDEX FOR (n:ELDMNode) ON (n.name)")
            self.graph.query("CREATE INDEX FOR (n:ELDMNode) ON (n.node_type)")
            self.graph.query("CREATE INDEX FOR (t:TransactionField) ON (t.name)")
            self.graph.query("CREATE INDEX FOR (t:TransactionField) ON (t.data_type)")
            
            logger.info("All indexes created successfully")
            
        except Exception as e:
            logger.warning(f"Index creation note: {e}")
    
    def store_eldm_hierarchy(self, nodes: List[ELDMNode]):
        """Store ELDM hierarchy with relationships"""
        # Store all nodes first
        for node in nodes:
            embedding_list = node.embedding.tolist()
            
            query = """
            CREATE (n:ELDMNode {
                name: $name,
                node_type: $node_type,
                description: $description,
                full_path: $full_path,
                searchable_text: $searchable_text,
                embedding: vecf32($embedding),
                created_at: $created_at
            })
            """
            params = {
                'name': node.name,
                'node_type': node.node_type,
                'description': node.description,
                'full_path': node.full_path,
                'searchable_text': node.get_searchable_text(),
                'embedding': embedding_list,
                'created_at': datetime.now().isoformat()
            }
            self.graph.query(query, params)
        
        # Create hierarchical relationships
        for node in nodes:
            if node.parent_name:
                relationship_query = """
                MATCH (child:ELDMNode {name: $child_name})
                MATCH (parent:ELDMNode {name: $parent_name})
                MERGE (parent)-[:HAS_CHILD]->(child)
                MERGE (child)-[:BELONGS_TO]->(parent)
                """
                self.graph.query(relationship_query, {
                    'child_name': node.name,
                    'parent_name': node.parent_name
                })
        
        logger.info(f"Stored {len(nodes)} ELDM nodes with hierarchical relationships")
    
    def vector_search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform native vector similarity search using FalkorDB's vector index"""
        embedding_list = query_embedding.tolist()
        
        # Search and include hierarchy information
        query = f"""
        CALL db.idx.vector.queryNodes('ELDMNode', 'embedding', {top_k * 2}, vecf32($embedding))
        YIELD node, score
        OPTIONAL MATCH (node)-[:BELONGS_TO*]->(parent:ELDMNode)
        WITH node, score, COLLECT(parent.name) as ancestors
        RETURN node.name as name,
               node.description as description,
               node.node_type as node_type,
               node.full_path as full_path,
               score,
               ancestors
        ORDER BY score DESC
        LIMIT {top_k}
        """
        
        result = self.graph.query(query, {'embedding': embedding_list})
        
        candidates = []
        for record in result.result_set:
            candidates.append({
                'name': record[0],
                'description': record[1],
                'node_type': record[2],
                'full_path': record[3],
                'similarity': record[4],
                'ancestors': record[5] if record[5] else [],
                'method': 'VECTOR'
            })
        
        logger.info(f"Vector search returned {len(candidates)} candidates")
        return candidates
    
    def fulltext_search(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform full-text search using FalkorDB's full-text index"""
        query = f"""
        CALL db.idx.fulltext.queryNodes('ELDMNode', $query_text)
        YIELD node, score
        OPTIONAL MATCH (node)-[:BELONGS_TO*]->(parent:ELDMNode)
        WITH node, score, COLLECT(parent.name) as ancestors
        RETURN node.name as name,
               node.description as description,
               node.node_type as node_type,
               node.full_path as full_path,
               score,
               ancestors
        ORDER BY score DESC
        LIMIT {top_k}
        """
        
        result = self.graph.query(query, {'query_text': query_text})
        
        candidates = []
        for record in result.result_set:
            candidates.append({
                'name': record[0],
                'description': record[1],
                'node_type': record[2],
                'full_path': record[3],
                'similarity': record[4],
                'ancestors': record[5] if record[5] else [],
                'method': 'FULLTEXT'
            })
        
        logger.info(f"Full-text search returned {len(candidates)} candidates")
        return candidates
    
    def hybrid_search(self, query_embedding: np.ndarray, query_text: str, 
                     top_k: int = 5, alpha: float = HYBRID_SEARCH_ALPHA) -> List[Dict[str, Any]]:
        """Perform hybrid search combining vector and full-text search"""
        vector_results = self.vector_search(query_embedding, top_k * 2)
        fulltext_results = self.fulltext_search(query_text, top_k * 2)
        
        # Combine and normalize scores
        combined_scores = {}
        metadata = {}
        
        # Normalize vector scores
        vector_scores = {r['name']: r['similarity'] for r in vector_results}
        max_vector = max(vector_scores.values()) if vector_scores else 1.0
        
        # Normalize full-text scores
        fulltext_scores = {r['name']: r['similarity'] for r in fulltext_results}
        max_fulltext = max(fulltext_scores.values()) if fulltext_scores else 1.0
        
        # Collect all unique names and metadata
        for r in vector_results + fulltext_results:
            if r['name'] not in metadata:
                metadata[r['name']] = r
        
        # Calculate hybrid scores
        for name in metadata.keys():
            v_score = vector_scores.get(name, 0.0) / max_vector if max_vector > 0 else 0.0
            f_score = fulltext_scores.get(name, 0.0) / max_fulltext if max_fulltext > 0 else 0.0
            combined_scores[name] = alpha * v_score + (1 - alpha) * f_score
        
        # Get top results
        sorted_names = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        candidates = []
        for name, hybrid_score in sorted_names:
            record = metadata[name]
            candidates.append({
                'name': record['name'],
                'description': record['description'],
                'node_type': record['node_type'],
                'full_path': record['full_path'],
                'similarity': hybrid_score,
                'vector_score': vector_scores.get(name, 0.0),
                'fulltext_score': fulltext_scores.get(name, 0.0),
                'ancestors': record['ancestors'],
                'method': 'HYBRID'
            })
        
        logger.info(f"Hybrid search returned {len(candidates)} candidates")
        return candidates
    
    def graph_rag_search(self, query_embedding: np.ndarray, query_text: str, 
                        transaction_field: TransactionField, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Perform Graph RAG search with hierarchy awareness
        Prioritizes most granular matches (ATTRIBUTE level)
        """
        # Start with hybrid search
        candidates = self.hybrid_search(query_embedding, query_text, top_k * 2)
        
        # Boost scores based on:
        # 1. Granularity (prefer ATTRIBUTE > ENTITY > CONCEPTUAL_ENTITY)
        # 2. Historical mapping patterns
        # 3. Similar data types and contexts
        
        # Check historical mappings
        similar_mappings_query = """
        MATCH (t:TransactionField)-[r:MAPPED_TO]->(n:ELDMNode)
        WHERE t.data_type = $data_type 
           OR t.file_name = $file_name
           OR t.standardised_name CONTAINS $partial_name
        WITH n, COUNT(r) as mapping_frequency, AVG(r.confidence_score) as avg_confidence
        RETURN n.name as name,
               n.node_type as node_type,
               mapping_frequency,
               avg_confidence
        ORDER BY mapping_frequency DESC, avg_confidence DESC
        LIMIT $top_k
        """
        
        params = {
            'data_type': transaction_field.data_type,
            'file_name': transaction_field.tt_file_name,
            'partial_name': transaction_field.standardised_name[:10] if transaction_field.standardised_name else "",
            'top_k': top_k * 2
        }
        
        result = self.graph.query(similar_mappings_query, params)
        
        # Build historical context
        historical_boost = {}
        for record in result.result_set:
            name = record[0]
            node_type = record[1]
            mapping_freq = record[2]
            avg_conf = record[3]
            historical_boost[name] = {
                'boost': mapping_freq * avg_conf * 0.2,
                'frequency': mapping_freq,
                'avg_confidence': avg_conf
            }
        
        # Granularity preference weights
        granularity_weights = {
            'ATTRIBUTE': 1.0,      # Most preferred
            'ENTITY': 0.8,         # Second choice
            'CONCEPTUAL_ENTITY': 0.6  # Last resort
        }
        
        # Apply boosts
        enhanced_candidates = []
        for candidate in candidates:
            name = candidate['name']
            node_type = candidate['node_type']
            base_score = candidate['similarity']
            
            # Granularity boost
            granularity_boost = granularity_weights.get(node_type, 0.5) * 0.1
            
            # Historical boost
            hist_boost = 0.0
            mapping_freq = 0
            if name in historical_boost:
                hist_boost = historical_boost[name]['boost']
                mapping_freq = historical_boost[name]['frequency']
            
            # Combined score
            enhanced_score = min(base_score + granularity_boost + hist_boost, 1.0)
            
            candidate['similarity'] = enhanced_score
            candidate['granularity_boost'] = granularity_boost
            candidate['graph_rag_score'] = hist_boost
            candidate['mapping_frequency'] = mapping_freq
            candidate['method'] = 'GRAPH_RAG'
            
            enhanced_candidates.append(candidate)
        
        # Re-sort by enhanced score, prioritizing ATTRIBUTE level
        enhanced_candidates.sort(
            key=lambda x: (x['similarity'], granularity_weights.get(x['node_type'], 0)),
            reverse=True
        )
        
        logger.info(f"Graph RAG search returned {len(enhanced_candidates[:top_k])} candidates with hierarchy awareness")
        return enhanced_candidates[:top_k]
    
    def get_hierarchy_path(self, node_name: str) -> Tuple[str, str, str, str]:
        """Get the full hierarchy for a node (Conceptual Entity > Entity > Attribute)"""
        query = """
        MATCH (n:ELDMNode {name: $node_name})
        OPTIONAL MATCH path = (n)-[:BELONGS_TO*]->(ancestor:ELDMNode)
        WITH n, 
             COLLECT(DISTINCT ancestor) as ancestors,
             [node IN nodes(path) | node.name] as path_names
        UNWIND ancestors as ancestor
        WITH n, 
             COLLECT(CASE WHEN ancestor.node_type = 'ATTRIBUTE' THEN ancestor.name END) as attrs,
             COLLECT(CASE WHEN ancestor.node_type = 'ENTITY' THEN ancestor.name END) as entities,
             COLLECT(CASE WHEN ancestor.node_type = 'CONCEPTUAL_ENTITY' THEN ancestor.name END) as conceptuals
        RETURN n.node_type as node_type,
               n.full_path as full_path,
               CASE WHEN n.node_type = 'ATTRIBUTE' THEN n.name
                    WHEN SIZE(attrs) > 0 THEN attrs[0]
                    ELSE '' END as attribute,
               CASE WHEN n.node_type = 'ENTITY' THEN n.name
                    WHEN SIZE(entities) > 0 THEN entities[0]
                    WHEN n.node_type = 'ATTRIBUTE' THEN ''
                    ELSE n.name END as entity,
               CASE WHEN n.node_type = 'CONCEPTUAL_ENTITY' THEN n.name
                    WHEN SIZE(conceptuals) > 0 THEN conceptuals[0]
                    ELSE '' END as conceptual
        """
        
        result = self.graph.query(query, {'node_name': node_name})
        
        if result.result_set:
            record = result.result_set[0]
            return (
                record[2] or node_name,  # attribute
                record[3] or '',          # entity
                record[4] or '',          # conceptual
                record[1] or node_name    # full_path
            )
        
        return (node_name, '', '', node_name)
    
    def store_mapping_result(self, result: MappingResult, transaction_field: TransactionField):
        """Store a mapping result with rich hierarchical relationships"""
        query = """
        MATCH (n:ELDMNode {name: $mapped_node_name})
        MERGE (t:TransactionField {
            name: $transaction_field,
            file_name: $file_name,
            data_type: $data_type,
            standardised_name: $standardised_name
        })
        MERGE (t)-[r:MAPPED_TO]->(n)
        SET r.confidence_score = $confidence_score,
            r.vector_similarity = $vector_similarity,
            r.hybrid_score = $hybrid_score,
            r.mapping_level = $mapping_level,
            r.reasoning_confidence = $reasoning_confidence,
            r.validation_score = $validation_score,
            r.confidence_level = $confidence_level,
            r.retrieval_method = $retrieval_method,
            r.reasoning_explanation = $reasoning_explanation,
            r.timestamp = $timestamp
        """
        
        # Determine which node to connect to (attribute is the mapped name)
        mapped_node_name = result.eldm_attribute if result.eldm_attribute else result.eldm_entity
        if not mapped_node_name:
            mapped_node_name = result.eldm_conceptual_entity
        
        params = {
            'mapped_node_name': mapped_node_name,
            'transaction_field': result.transaction_field,
            'file_name': transaction_field.tt_file_name,
            'data_type': transaction_field.data_type,
            'standardised_name': transaction_field.standardised_name,
            'confidence_score': result.confidence_score,
            'vector_similarity': result.vector_similarity,
            'hybrid_score': result.hybrid_score,
            'mapping_level': result.mapping_level,
            'reasoning_confidence': result.reasoning_confidence,
            'validation_score': result.validation_score,
            'confidence_level': result.confidence_level,
            'retrieval_method': result.retrieval_method,
            'reasoning_explanation': result.reasoning_explanation,
            'timestamp': result.timestamp
        }
        self.graph.query(query, params)
        logger.info(f"Stored hierarchical mapping result")


# ============================================================================
# AGENT PROMPTS
# ============================================================================

MAPPING_AGENT_SYSTEM_PROMPT = """You are an expert data architect and semantic mapping specialist with deep knowledge of enterprise data modeling, database schemas, and logical data models.

Your task is to analyze transaction fields and map them to Enterprise Logical Data Model (ELDM) attributes with high precision and confidence.

ELDM HIERARCHY:
The ELDM follows a three-level hierarchy:
1. Conceptual Entity (highest level) - broad business concepts
2. Entity (middle level) - specific business entities
3. Attribute (most granular) - individual data fields

MAPPING PRIORITY:
- ALWAYS prefer mapping to the most granular level (Attribute) when possible
- If no suitable Attribute exists, map to Entity level
- Only map to Conceptual Entity as a last resort
- The full hierarchy path is provided for each candidate

You will receive:
1. A transaction field with rich context
2. Candidates from multiple retrieval methods with their hierarchy levels
3. Historical mapping patterns (Graph RAG)

Your responsibilities:
1. Analyze the semantic meaning and purpose of the transaction field
2. Consider the SQL context for data lineage understanding
3. Evaluate each candidate at its hierarchy level
4. PRIORITIZE ATTRIBUTE-level matches over Entity or Conceptual Entity
5. Consider historical mapping patterns
6. Provide a confidence score (0.0 to 1.0)

Use chain-of-thought reasoning:
- What does this transaction field represent?
- Is there an ATTRIBUTE-level match? (preferred)
- If not, is there an ENTITY-level match?
- What does the hierarchy path tell us?
- Do historical patterns support this mapping?
- What is your confidence level and why?

Be precise and justify your reasoning with evidence from all contexts."""

VALIDATION_AGENT_SYSTEM_PROMPT = """You are a senior data governance specialist and quality assurance expert responsible for validating data mappings in enterprise systems.

Your task is to critically evaluate proposed field-to-attribute mappings to ensure they meet enterprise standards for accuracy, consistency, and semantic correctness.

HIERARCHY VALIDATION:
- Verify that the mapping is at the most granular level possible
- Check if a more granular level exists but wasn't selected
- Ensure the hierarchy path makes logical sense
- Validate that Entity/Conceptual Entity mappings are justified

You will receive:
1. A transaction field with full context
2. A proposed ELDM mapping with its hierarchy level
3. The mapping agent's reasoning
4. Historical patterns from Graph RAG

Your responsibilities:
1. Validate the semantic correctness
2. Verify the hierarchy level is appropriate
3. Check if a more granular mapping exists
4. Evaluate data type compatibility
5. Consider historical patterns
6. Provide a validation score (0.0 to 1.0)

Validation decisions:
- APPROVE (0.8-1.0): Correct mapping at appropriate level
- NEEDS_REVIEW (0.5-0.79): Uncertain or could be more granular
- REJECT (0.0-0.49): Incorrect or wrong hierarchy level

Be critical and thorough. Protect data integrity and ensure granularity."""

SUPERVISOR_AGENT_SYSTEM_PROMPT = """You are the chief data architect overseeing the enterprise data mapping initiative. You have final authority over all mapping decisions.

HIERARCHY DECISION FRAMEWORK:
1. ATTRIBUTE-level mappings are preferred (highest quality)
2. ENTITY-level mappings are acceptable when no suitable attribute exists
3. CONCEPTUAL_ENTITY mappings are last resort only
4. Always document why a more granular level wasn't chosen

You will receive:
1. Transaction field context
2. Mapping proposal with hierarchy level
3. Validation results
4. Statistical confidence metrics
5. Historical mapping patterns

Your responsibilities:
1. Synthesize all evidence and reasoning
2. Verify the hierarchy level is appropriate
3. Ensure mapping is at most granular level possible
4. Make final decisions based on overall confidence
5. Classify mappings into confidence levels:
   - HIGH: Confidence ≥ 0.85 - Auto-approve
   - MEDIUM: 0.70 ≤ Confidence < 0.85 - Flag for review
   - LOW: 0.50 ≤ Confidence < 0.70 - Require human validation
   - REJECTED: Confidence < 0.50 - Reject mapping

Decision principles:
- Prioritize granularity and semantic correctness
- Consider historical patterns but don't blindly follow
- Be conservative with high-confidence approvals
- Document reasoning for all decisions
- Flag when a more granular level might exist

Ensure mapping quality, consistency, and appropriate granularity."""


# ============================================================================
# MULTI-AGENT SYSTEM
# ============================================================================

class MappingAgent:
    """Agent responsible for proposing field-to-attribute mappings"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=REASONING_MODEL,
            base_url=BASE_URL,
            api_key=API_KEY
        )
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", MAPPING_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
        logger.info("Initialized MappingAgent")
    
    def propose_mapping(self, state: GraphState) -> GraphState:
        """Propose a mapping for the transaction field"""
        field = state['transaction_field']
        candidates = state.get('graph_rag_candidates') or state.get('hybrid_candidates') or state.get('vector_candidates', [])
        
        input_text = f"""
Transaction Field Analysis Required:

{field.get_context()}

Candidate ELDM Nodes (with Hierarchy):
"""
        for i, candidate in enumerate(candidates, 1):
            input_text += f"""
{i}. Name: {candidate['name']}
   Hierarchy Level: {candidate['node_type']}
   Full Path: {candidate['full_path']}
   Description: {candidate['description']}
   Similarity Score: {candidate['similarity']:.4f}
   Retrieval Method: {candidate.get('method', 'UNKNOWN')}
"""
            if candidate.get('method') == 'GRAPH_RAG':
                input_text += f"   Historical Mappings: {candidate.get('mapping_frequency', 0)} similar fields\n"
        
        input_text += """
IMPORTANT: Prefer ATTRIBUTE-level matches. Only select ENTITY or CONCEPTUAL_ENTITY if no suitable ATTRIBUTE exists.

Your response must be a JSON object:
{
    "selected_eldm_node": "exact name",
    "selected_hierarchy_level": "ATTRIBUTE|ENTITY|CONCEPTUAL_ENTITY",
    "confidence_score": 0.0-1.0,
    "reasoning": "detailed reasoning including why this hierarchy level",
    "key_factors": ["factor1", "factor2"],
    "concerns": ["any concerns"],
    "granularity_justification": "why this level and not more granular"
}
"""
        
        try:
            messages = state.get('messages', [])
            chain = self.prompt | self.llm
            response = chain.invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            proposal = json.loads(content)
            state['mapping_proposals'].append(proposal)
            state['messages'].append(HumanMessage(content=input_text))
            state['messages'].append(AIMessage(content=response.content))
            
            logger.info(f"Mapping proposed: {proposal['selected_eldm_node']} "
                       f"at {proposal['selected_hierarchy_level']} level "
                       f"(confidence: {proposal['confidence_score']:.3f})")
            
        except Exception as e:
            logger.error(f"Error in propose_mapping: {e}")
            proposal = {
                'selected_eldm_node': candidates[0]['name'],
                'selected_hierarchy_level': candidates[0]['node_type'],
                'confidence_score': candidates[0]['similarity'],
                'reasoning': f"Fallback due to error: {str(e)}",
                'key_factors': ['semantic_similarity'],
                'concerns': ['Error in reasoning'],
                'granularity_justification': 'Top candidate selected'
            }
            state['mapping_proposals'].append(proposal)
        
        return state


class ValidationAgent:
    """Agent responsible for validating proposed mappings"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=REASONING_MODEL,
            base_url=BASE_URL,
            api_key=API_KEY
        )
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", VALIDATION_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
        logger.info("Initialized ValidationAgent")
    
    def validate_mapping(self, state: GraphState) -> GraphState:
        """Validate the proposed mapping"""
        field = state['transaction_field']
        proposals = state['mapping_proposals']
        
        if not proposals:
            logger.warning("No proposals to validate")
            return state
        
        latest_proposal = proposals[-1]
        
        all_candidates = (state.get('graph_rag_candidates', []) + 
                         state.get('hybrid_candidates', []) + 
                         state.get('vector_candidates', []))
        
        selected_candidate = next(
            (c for c in all_candidates if c['name'] == latest_proposal['selected_eldm_node']),
            all_candidates[0] if all_candidates else None
        )
        
        if not selected_candidate:
            logger.error("Could not find selected candidate")
            return state
        
        input_text = f"""
Validation Request:

Transaction Field:
{field.get_context()}

Proposed ELDM Mapping:
- Node Name: {latest_proposal['selected_eldm_node']}
- Hierarchy Level: {latest_proposal['selected_hierarchy_level']}
- Full Path: {selected_candidate.get('full_path', 'N/A')}
- Description: {selected_candidate.get('description', 'N/A')}

Mapping Agent's Reasoning:
{latest_proposal['reasoning']}

Granularity Justification:
{latest_proposal['granularity_justification']}

Agent Confidence: {latest_proposal['confidence_score']:.3f}

Please validate this mapping with focus on hierarchy appropriateness.

Response must be JSON:
{{
    "validation_decision": "APPROVE|NEEDS_REVIEW|REJECT",
    "validation_score": 0.0-1.0,
    "validation_reasoning": "detailed analysis",
    "identified_issues": ["issue1"],
    "strengths": ["strength1"],
    "hierarchy_assessment": "is this the most granular level possible?",
    "recommendations": "improvements"
}}
"""
        
        try:
            messages = state.get('messages', [])
            chain = self.prompt | self.llm
            response = chain.invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            validation = json.loads(content)
            state['validation_results'].append(validation)
            state['messages'].append(HumanMessage(content=input_text))
            state['messages'].append(AIMessage(content=response.content))
            
            logger.info(f"Validation: {validation['validation_decision']} "
                       f"(score: {validation['validation_score']:.3f})")
            
        except Exception as e:
            logger.error(f"Error in validate_mapping: {e}")
            validation = {
                'validation_decision': 'NEEDS_REVIEW',
                'validation_score': 0.6,
                'validation_reasoning': f"Error: {str(e)}",
                'identified_issues': ['Validation error'],
                'strengths': [],
                'hierarchy_assessment': 'Unable to assess',
                'recommendations': 'Manual review required'
            }
            state['validation_results'].append(validation)
        
        return state


class SupervisorAgent:
    """Agent responsible for final mapping decisions"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=REASONING_MODEL,
            base_url=BASE_URL,
            api_key=API_KEY
        )
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SUPERVISOR_AGENT_SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "{input}")
        ])
        logger.info("Initialized SupervisorAgent")
    
    def make_final_decision(self, state: GraphState) -> GraphState:
        """Make the final mapping decision"""
        field = state['transaction_field']
        proposals = state['mapping_proposals']
        validations = state['validation_results']
        
        if not proposals or not validations:
            logger.error("Missing proposals or validations")
            return state
        
        latest_proposal = proposals[-1]
        latest_validation = validations[-1]
        
        all_candidates = (state.get('graph_rag_candidates', []) + 
                         state.get('hybrid_candidates', []) + 
                         state.get('vector_candidates', []))
        
        selected_candidate = next(
            (c for c in all_candidates if c['name'] == latest_proposal['selected_eldm_node']),
            all_candidates[0] if all_candidates else None
        )
        
        if not selected_candidate:
            logger.error("Could not find selected candidate")
            return state
        
        # Calculate statistical metrics
        all_similarities = [c['similarity'] for c in all_candidates]
        z_scores = zscore(all_similarities) if len(all_similarities) > 1 else [0.0]
        top_similarity = selected_candidate['similarity']
        top_z_score = float(z_scores[0]) if all_similarities and all_similarities[0] == top_similarity else 0.0
        gap_to_second = top_similarity - all_similarities[1] if len(all_similarities) > 1 else top_similarity
        
        statistical_metrics = {
            'semantic_similarity': top_similarity,
            'z_score': top_z_score,
            'gap_to_second_best': gap_to_second
        }
        
        input_text = f"""
Final Decision Required:

Transaction Field: {field.tt_column_name}

Mapping Proposal:
- ELDM Node: {latest_proposal['selected_eldm_node']}
- Hierarchy Level: {latest_proposal['selected_hierarchy_level']}
- Full Path: {selected_candidate.get('full_path', 'N/A')}
- Agent Confidence: {latest_proposal['confidence_score']:.3f}

Validation Results:
- Decision: {latest_validation['validation_decision']}
- Score: {latest_validation['validation_score']:.3f}
- Hierarchy Assessment: {latest_validation.get('hierarchy_assessment', 'N/A')}

Statistical Metrics:
- Similarity: {statistical_metrics['semantic_similarity']:.4f}
- Z-Score: {statistical_metrics['z_score']:.4f}

Make your final decision:
{{
    "final_decision": "APPROVE|REJECT",
    "confidence_level": "HIGH|MEDIUM|LOW|REJECTED",
    "overall_confidence_score": 0.0-1.0,
    "reasoning": "comprehensive reasoning",
    "action_required": "any required action"
}}
"""
        
        try:
            messages = state.get('messages', [])
            chain = self.prompt | self.llm
            response = chain.invoke({"messages": messages, "input": input_text})
            
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            decision = json.loads(content)
            
            if decision['final_decision'] == 'APPROVE':
                # Get hierarchy information from graph
                graph_rag = state.get('_graph_rag_instance')
                if graph_rag:
                    attr, entity, conceptual, full_path = graph_rag.get_hierarchy_path(
                        latest_proposal['selected_eldm_node']
                    )
                else:
                    attr = latest_proposal['selected_eldm_node']
                    entity = ''
                    conceptual = ''
                    full_path = selected_candidate.get('full_path', attr)
                
                result = MappingResult(
                    transaction_field=field.tt_column_name,
                    eldm_attribute=attr,
                    eldm_entity=entity,
                    eldm_conceptual_entity=conceptual,
                    mapping_level=latest_proposal['selected_hierarchy_level'],
                    confidence_score=decision['overall_confidence_score'],
                    vector_similarity=selected_candidate.get('vector_score', selected_candidate['similarity']),
                    fulltext_score=selected_candidate.get('fulltext_score', 0.0),
                    hybrid_score=selected_candidate.get('similarity', 0.0),
                    graph_rag_score=selected_candidate.get('graph_rag_score', 0.0),
                    reasoning_confidence=latest_proposal['confidence_score'],
                    validation_score=latest_validation['validation_score'],
                    reasoning_explanation=latest_proposal['reasoning'],
                    validation_notes=latest_validation['validation_reasoning'],
                    statistical_metrics=statistical_metrics,
                    confidence_level=decision['confidence_level'],
                    retrieval_method=selected_candidate.get('method', 'UNKNOWN'),
                    hierarchy_path=full_path,
                    timestamp=datetime.now().isoformat()
                )
                state['final_mapping'] = result
                logger.info(f"APPROVED: {result.eldm_attribute} at {result.mapping_level} level")
            else:
                logger.info(f"REJECTED: {latest_proposal['selected_eldm_node']}")
            
            state['messages'].append(HumanMessage(content=input_text))
            state['messages'].append(AIMessage(content=response.content))
            
        except Exception as e:
            logger.error(f"Error in make_final_decision: {e}")
        
        return state


# ============================================================================
# LANGGRAPH WORKFLOW
# ============================================================================

def create_mapping_workflow() -> StateGraph:
    """Create the LangGraph workflow"""
    mapping_agent = MappingAgent()
    validation_agent = ValidationAgent()
    supervisor_agent = SupervisorAgent()
    
    workflow = StateGraph(GraphState)
    workflow.add_node("propose_mapping", mapping_agent.propose_mapping)
    workflow.add_node("validate_mapping", validation_agent.validate_mapping)
    workflow.add_node("final_decision", supervisor_agent.make_final_decision)
    
    workflow.set_entry_point("propose_mapping")
    workflow.add_edge("propose_mapping", "validate_mapping")
    workflow.add_edge("validate_mapping", "final_decision")
    workflow.add_edge("final_decision", END)
    
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    logger.info("Created LangGraph workflow")
    return app


# ============================================================================
# MAIN MAPPING SYSTEM
# ============================================================================

class ELDMMappingSystem:
    """Complete ELDM hierarchical mapping system"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.graph_rag = ELDMHierarchicalGraphRAG()
        self.workflow = create_mapping_workflow()
        logger.info("Initialized ELDM Hierarchical Mapping System")
    
    def load_transaction_data(self) -> List[TransactionField]:
        """Load transaction data from Excel"""
        logger.info(f"Loading transaction data from {TRANSACTION_FILE}")
        df = pd.read_excel(TRANSACTION_FILE)
        
        fields = []
        for _, row in df.iterrows():
            field = TransactionField(
                tt_file_name=str(row['TT File name']),
                tt_column_name=str(row['TT COLUMN/FIELD Name']),
                standardised_name=str(row.get('Standardised Name', '')),
                is_primary_key=bool(row.get('PK', False)),
                comment=str(row.get('Comment', '')),
                data_type=str(row.get('DataType', '')),
                creation_sql=str(row.get('Creation', '')),
                transformation_sql=str(row.get('Transformation', ''))
            )
            fields.append(field)
        
        logger.info(f"Loaded {len(fields)} transaction fields")
        return fields
    
    def load_eldm_hierarchy(self) -> List[ELDMNode]:
        """Load ELDM hierarchy from Excel and create embeddings"""
        logger.info(f"Loading ELDM hierarchy from {ELDM_FILE}")
        df = pd.read_excel(ELDM_FILE)
        
        # Build hierarchy: Conceptual Entity -> Entity -> Attribute (Name)
        nodes = []
        
        for _, row in df.iterrows():
            name = str(row['Name'])
            entity = str(row.get('Entity', ''))
            conceptual = str(row.get('Conceptual Entity', ''))
            description = str(row.get('Description', ''))
            
            # Build full path
            path_parts = [p for p in [conceptual, entity, name] if p and p != 'nan']
            full_path = ' > '.join(path_parts)
            
            # Determine node type and parent
            if entity and entity != 'nan':
                # This is an attribute with an entity parent
                node_type = 'ATTRIBUTE'
                parent_name = entity
            elif conceptual and conceptual != 'nan':
                # This is an entity with conceptual parent
                node_type = 'ENTITY'
                parent_name = conceptual
            else:
                # This is a top-level conceptual entity or standalone
                node_type = 'CONCEPTUAL_ENTITY' if not entity else 'ENTITY'
                parent_name = None
            
            node = ELDMNode(
                name=name,
                node_type=node_type,
                description=description,
                parent_name=parent_name,
                full_path=full_path
            )
            nodes.append(node)
        
        # Create embeddings
        logger.info(f"Creating embeddings for {len(nodes)} ELDM nodes...")
        texts = [node.get_searchable_text() for node in nodes]
        
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i+BATCH_SIZE]
            batch_embeddings = self.embedding_service.create_embeddings_batch(batch_texts)
            
            for j, embedding in enumerate(batch_embeddings):
                nodes[i+j].embedding = embedding
            
            logger.info(f"Created embeddings for batch {i//BATCH_SIZE + 1}")
        
        logger.info(f"Loaded {len(nodes)} ELDM nodes with hierarchy")
        
        # Log hierarchy statistics
        attr_count = sum(1 for n in nodes if n.node_type == 'ATTRIBUTE')
        entity_count = sum(1 for n in nodes if n.node_type == 'ENTITY')
        conceptual_count = sum(1 for n in nodes if n.node_type == 'CONCEPTUAL_ENTITY')
        logger.info(f"Hierarchy: {conceptual_count} Conceptual Entities, "
                   f"{entity_count} Entities, {attr_count} Attributes")
        
        return nodes
    
    def initialize_knowledge_graph(self):
        """Initialize FalkorDB with hierarchical ELDM structure"""
        logger.info("Initializing hierarchical knowledge graph...")
        
        self.graph_rag.clear_graph()
        
        eldm_nodes = self.load_eldm_hierarchy()
        self.graph_rag.store_eldm_hierarchy(eldm_nodes)
        self.graph_rag.create_indexes()
        
        logger.info("Hierarchical knowledge graph initialized")
    
    def map_single_field(self, field: TransactionField) -> Optional[MappingResult]:
        """Map a single transaction field"""
        logger.info(f"Mapping field: {field.tt_column_name}")
        
        field_text = field.get_searchable_text()
        field_embedding = self.embedding_service.create_embedding(field_text)
        
        # Multi-modal retrieval
        vector_candidates = self.graph_rag.vector_search(field_embedding, TOP_K_CANDIDATES)
        fulltext_candidates = self.graph_rag.fulltext_search(field_text, TOP_K_CANDIDATES)
        hybrid_candidates = self.graph_rag.hybrid_search(
            field_embedding, field_text, TOP_K_CANDIDATES, HYBRID_SEARCH_ALPHA
        )
        graph_rag_candidates = self.graph_rag.graph_rag_search(
            field_embedding, field_text, field, TOP_K_CANDIDATES
        )
        
        # Initialize state
        initial_state = {
            'transaction_field': field,
            'vector_candidates': vector_candidates,
            'fulltext_candidates': fulltext_candidates,
            'hybrid_candidates': hybrid_candidates,
            'graph_rag_candidates': graph_rag_candidates,
            'final_candidates': graph_rag_candidates,
            'mapping_proposals': [],
            'validation_results': [],
            'final_mapping': None,
            'messages': [],
            'iteration_count': 0,
            'memory_context': {
                'sql_context': {
                    'creation': field.creation_sql,
                    'transformation': field.transformation_sql
                },
                'lineage': {
                    'file': field.tt_file_name,
                    'standardised_name': field.standardised_name
                }
            },
            '_graph_rag_instance': self.graph_rag
        }
        
        try:
            config = {"configurable": {"thread_id": field.tt_column_name}}
            result = self.workflow.invoke(initial_state, config)
            
            final_mapping = result.get('final_mapping')
            
            if final_mapping:
                self.graph_rag.store_mapping_result(final_mapping, field)
                logger.info(f"Mapped: {field.tt_column_name} -> {final_mapping.hierarchy_path} "
                           f"(level: {final_mapping.mapping_level})")
                return final_mapping
            else:
                logger.warning(f"No mapping approved for: {field.tt_column_name}")
                return None
                
        except Exception as e:
            logger.error(f"Error mapping field {field.tt_column_name}: {e}")
            return None
    
    def map_all_fields(self) -> List[MappingResult]:
        """Map all transaction fields"""
        logger.info("Starting hierarchical mapping process...")
        
        fields = self.load_transaction_data()
        results = []
        
        for i, field in enumerate(fields, 1):
            logger.info(f"Processing {i}/{len(fields)}: {field.tt_column_name}")
            
            result = self.map_single_field(field)
            if result:
                results.append(result)
            
            if i % 10 == 0:
                logger.info(f"Progress: {i}/{len(fields)} processed, {len(results)} mapped")
        
        logger.info(f"Complete: {len(results)}/{len(fields)} successful mappings")
        return results
    
    def export_results(self, results: List[MappingResult]):
        """Export mapping results to Excel"""
        logger.info(f"Exporting results to {OUTPUT_FILE}")
        
        data = []
        for result in results:
            row = {
                'Transaction Field': result.transaction_field,
                'ELDM Attribute': result.eldm_attribute,
                'ELDM Entity': result.eldm_entity,
                'ELDM Conceptual Entity': result.eldm_conceptual_entity,
                'Mapping Level': result.mapping_level,
                'Hierarchy Path': result.hierarchy_path,
                'Confidence Score': result.confidence_score,
                'Confidence Level': result.confidence_level,
                'Retrieval Method': result.retrieval_method,
                'Vector Similarity': result.vector_similarity,
                'Hybrid Score': result.hybrid_score,
                'Graph RAG Score': result.graph_rag_score,
                'Reasoning Explanation': result.reasoning_explanation,
                'Validation Notes': result.validation_notes,
                'Timestamp': result.timestamp
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Mapping Results', index=False)
            
            # Summary by hierarchy level
            summary = {
                'Total Mappings': len(results),
                'ATTRIBUTE Level': len([r for r in results if r.mapping_level == 'ATTRIBUTE']),
                'ENTITY Level': len([r for r in results if r.mapping_level == 'ENTITY']),
                'CONCEPTUAL_ENTITY Level': len([r for r in results if r.mapping_level == 'CONCEPTUAL_ENTITY']),
                'HIGH Confidence': len([r for r in results if r.confidence_level == 'HIGH']),
                'MEDIUM Confidence': len([r for r in results if r.confidence_level == 'MEDIUM']),
                'LOW Confidence': len([r for r in results if r.confidence_level == 'LOW']),
                'Graph RAG Method': len([r for r in results if r.retrieval_method == 'GRAPH_RAG']),
                'Avg Confidence': np.mean([r.confidence_score for r in results])
            }
            summary_df = pd.DataFrame([summary])
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        logger.info(f"Results exported to {OUTPUT_FILE}")
    
    def run(self):
        """Run the complete mapping pipeline"""
        logger.info("=" * 80)
        logger.info("ELDM Hierarchical Mapping System - Starting")
        logger.info("=" * 80)
        
        try:
            self.initialize_knowledge_graph()
            results = self.map_all_fields()
            
            if results:
                self.export_results(results)
            
            logger.info("=" * 80)
            logger.info("Hierarchical Mapping System - Completed")
            logger.info("=" * 80)
            
            return results
            
        except Exception as e:
            logger.error(f"Error in mapping pipeline: {e}")
            raise


# ============================================================================
# USAGE
# ============================================================================

if __name__ == "__main__":
    system = ELDMMappingSystem()
    results = system.run()
    
    print(f"\n✅ Hierarchical mapping completed!")
    print(f"📊 {len(results)} fields mapped successfully")
    print(f"📈 Results: {OUTPUT_FILE}")
    
    # Hierarchy distribution
    levels = {}
    for r in results:
        levels[r.mapping_level] = levels.get(r.mapping_level, 0) + 1
    
    print("\n🎯 Mapping Level Distribution:")
    for level, count in levels.items():
        pct = count/len(results)*100 if results else 0
        print(f"   {level}: {count} ({pct:.1f}%)")
