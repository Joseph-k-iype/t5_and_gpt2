"""
GDC Record Class Mapping System using OpenAI o3-mini with LangGraph ReAct Agents
FIXED VERSION - Resolves 'unknown' mapping issue
Supports ONE-TO-MANY mappings with RAG using text-embedding-3-large
Uses OpenAI API directly for embeddings
Implements mixture of experts pattern with dynamic chain of thought reasoning
Outputs to Excel with grouping
"""

import json
import os
from typing import List, Dict, Optional, Any
import pandas as pd
from pydantic import BaseModel, Field, field_validator
from langchain_openai import ChatOpenAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import re
from openai import OpenAI
import time

# ==================== GLOBAL CONFIGURATION ====================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_MODEL = "o3-mini"
REASONING_EFFORT = "high"
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Validate API key before initializing clients
if not OPENAI_API_KEY:
    print("WARNING: OPENAI_API_KEY is not set. Please set it before running.")

# Initialize OpenAI client for embeddings
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    timeout=60.0,
    max_retries=3
)

# Initialize global LLM instance
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL,
    reasoning_effort=REASONING_EFFORT
)

# ==================== CUSTOM EMBEDDINGS CLASS ====================

class OpenAIDirectEmbeddings(Embeddings):
    """Custom embeddings class using OpenAI API directly"""
    
    def __init__(self, model: str = EMBEDDING_MODEL, dimensions: int = EMBEDDING_DIMENSIONS):
        self.model = model
        self.dimensions = dimensions
        self.client = openai_client
        self.batch_size = 50
        self.max_chars = 30000
        self.chunk_overlap = 200
    
    def chunk_text(self, text: str) -> List[str]:
        """Dynamically chunk text if it exceeds character limit"""
        if len(text) <= self.max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chars
            
            if end < len(text):
                search_start = max(start, end - 500)
                sentence_ends = ['.', '!', '?', '\n\n']
                
                best_break = None
                for delimiter in sentence_ends:
                    pos = text.rfind(delimiter, search_start, end)
                    if pos != -1:
                        best_break = pos + 1
                        break
                
                if best_break is None:
                    pos = text.rfind(' ', search_start, end)
                    if pos != -1:
                        best_break = pos
                    else:
                        best_break = end
                
                end = best_break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks
    
    def embed_single_text(self, text: str) -> List[float]:
        """Embed a single text with automatic chunking if needed"""
        chunks = self.chunk_text(text)
        
        if len(chunks) == 1:
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[text],
                    dimensions=self.dimensions
                )
                return response.data[0].embedding
            except Exception as e:
                error_msg = str(e).lower()
                if "maximum" in error_msg and "token" in error_msg:
                    print(f"  ⚠ Text still too long, forcing more aggressive chunking...")
                    smaller_chunks = self.chunk_text_aggressive(text)
                    return self.average_embeddings(smaller_chunks)
                raise
        else:
            print(f"  📄 Text chunked into {len(chunks)} parts for embedding")
            return self.average_embeddings(chunks)
    
    def chunk_text_aggressive(self, text: str, max_chars: int = 15000) -> List[str]:
        """More aggressive chunking for very long texts"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + max_chars, len(text))
            
            if end < len(text):
                space_pos = text.rfind(' ', start, end)
                if space_pos > start:
                    end = space_pos
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def average_embeddings(self, chunks: List[str]) -> List[float]:
        """Create embeddings for chunks and return averaged embedding"""
        chunk_embeddings = []
        
        for i, chunk in enumerate(chunks):
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[chunk],
                    dimensions=self.dimensions
                )
                chunk_embeddings.append(response.data[0].embedding)
                time.sleep(0.2)
            except Exception as e:
                print(f"  ✗ Failed to embed chunk {i+1}/{len(chunks)}: {e}")
                continue
        
        if not chunk_embeddings:
            return [0.0] * self.dimensions
        
        avg_embedding = [
            sum(emb[i] for emb in chunk_embeddings) / len(chunk_embeddings)
            for i in range(self.dimensions)
        ]
        
        return avg_embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents"""
        if not texts:
            return []
        
        print(f"Embedding {len(texts)} documents with dynamic chunking...")
        all_embeddings = []
        
        try:
            for i, text in enumerate(texts):
                if (i + 1) % 10 == 0:
                    print(f"  Progress: {i+1}/{len(texts)} documents...")
                
                try:
                    if len(text) > self.max_chars:
                        print(f"  📏 Document {i+1} is long ({len(text)} chars), chunking...")
                    
                    embedding = self.embed_single_text(text)
                    all_embeddings.append(embedding)
                    
                    if (i + 1) % self.batch_size == 0:
                        time.sleep(1.0)
                    else:
                        time.sleep(0.1)
                    
                except Exception as doc_error:
                    print(f"  ✗ Failed to embed document {i+1}: {doc_error}")
                    print(f"    Document length: {len(text)} chars")
                    all_embeddings.append([0.0] * self.dimensions)
            
            print(f"✓ Successfully embedded {len(all_embeddings)}/{len(texts)} documents")
            return all_embeddings
            
        except Exception as e:
            print(f"✗ CRITICAL ERROR embedding documents: {e}")
            print(f"  Returning zero vectors as fallback")
            return [[0.0] * self.dimensions for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """Embed a single query"""
        try:
            print(f"Embedding query ({len(text)} chars): {text[:50]}...")
            embedding = self.embed_single_text(text)
            print(f"✓ Query embedded successfully")
            return embedding
            
        except Exception as e:
            print(f"✗ Error embedding query: {e}")
            print(f"  Returning zero vector as fallback")
            return [0.0] * self.dimensions

# Initialize embeddings
embeddings = OpenAIDirectEmbeddings(model=EMBEDDING_MODEL, dimensions=EMBEDDING_DIMENSIONS)

# Global vector stores
gdc_master_vectorstore: Optional[InMemoryVectorStore] = None
gdc_context_vectorstore: Optional[InMemoryVectorStore] = None

# ==================== PYDANTIC V2 MODELS ====================

class GDCMaster(BaseModel):
    """Pydantic model for GDC Master data"""
    data_domain: str = Field(alias="Data Domain", default="")
    gdc_name: str = Field(alias="GDC Name")
    definition: str = Field(alias="Definition", default="")
    
    class Config:
        populate_by_name = True

class ProcessInfo(BaseModel):
    """Pydantic model for Process information"""
    process_name: str = Field(alias="Process Name", default="")
    process_description: str = Field(alias="Process Description", default="")
    
    class Config:
        populate_by_name = True

class AppInfo(BaseModel):
    """Pydantic model for Application information"""
    app_id: str = Field(alias="App ID", default="")
    app_name: str = Field(alias="App Name", default="")
    app_description: str = Field(alias="App Description", default="")
    processes: List[ProcessInfo] = Field(default_factory=list, alias="Processes")
    
    class Config:
        populate_by_name = True

class PBTInfo(BaseModel):
    """Pydantic model for PBT information"""
    pbt_id: str = Field(alias="PBT ID", default="")
    pbt_name: str = Field(alias="PBT Name", default="")
    pbt_desc: str = Field(alias="PBT Desc", default="")
    apps: List[AppInfo] = Field(default_factory=list, alias="Apps")
    
    class Config:
        populate_by_name = True

class GDCWithContext(BaseModel):
    """Pydantic model for GDC with Context data"""
    gdc_id: str = Field(alias="GDC ID", default="")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    pbts: List[PBTInfo] = Field(default_factory=list, alias="PBTs")
    
    class Config:
        populate_by_name = True

class ValidationEntry(BaseModel):
    """Pydantic model for Validation data"""
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description", default="")
    ilm_category_name: str = Field(alias="ILM Category Name", default="")
    
    class Config:
        populate_by_name = True

class RecordClass(BaseModel):
    """Pydantic model for Record Class data"""
    guid: str = Field(alias="Guid", default="")
    code: str = Field(alias="Code", default="")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description", default="")
    
    class Config:
        populate_by_name = True

class SemanticMatch(BaseModel):
    """Pydantic model for Semantic Match result"""
    gdc_name: str = Field(description="Name of the matched GDC")
    gdc_description: str = Field(description="Description of the matched GDC", default="")
    similarity_score: float = Field(ge=0, le=100, description="Similarity score between 0-100")
    reasoning: str = Field(description="Detailed reasoning for the match")
    
    @field_validator('similarity_score')
    @classmethod
    def validate_score(cls, v: float) -> float:
        if not 0 <= v <= 100:
            raise ValueError('Similarity score must be between 0 and 100')
        return v

class SemanticMatchResponse(BaseModel):
    """Pydantic model for Semantic Matching Expert response"""
    matches: List[SemanticMatch] = Field(description="List of all relevant semantic matches")
    multiple_matches_rationale: str = Field(description="Explanation of why multiple GDCs are relevant")

class ContextEvidence(BaseModel):
    """Pydantic model for Context Evidence"""
    gdc_name: str = Field(description="Name of the GDC")
    context_evidence: List[str] = Field(description="List of contextual evidence", default_factory=list)
    alignment_score: float = Field(ge=0, le=100, description="Alignment score")
    relevance_justification: str = Field(description="Justification for why this GDC is relevant")
    reasoning: str = Field(description="Detailed reasoning for context analysis")

class ContextAnalysisResponse(BaseModel):
    """Pydantic model for Context Analysis Expert response"""
    context_analysis: List[ContextEvidence] = Field(description="List of context analyses for all relevant GDCs")
    all_relevant_gdcs: List[str] = Field(description="List of all GDC names that are relevant")

class ValidationMatchEntry(BaseModel):
    """Pydantic model for a single validation match entry"""
    gdc_name: str = Field(description="GDC name from validation", default="")
    ilm_category_name: str = Field(description="ILM category name from validation", default="")

class ValidationResultItem(BaseModel):
    """Pydantic model for a single validation result"""
    gdc_name: str = Field(description="GDC being validated")
    validation_found: bool = Field(description="Whether validation entry was found")
    matching_entry: Optional[ValidationMatchEntry] = Field(None, description="Matching entry if found")
    validation_status: str = Field(description="Status: confirmed/conflicted/not_found")
    validation_reasoning: str = Field(description="Reasoning for this validation")

class ValidationResponse(BaseModel):
    """Pydantic model for Validation Expert response"""
    validation_results: List[ValidationResultItem] = Field(description="Validation results for each proposed GDC")
    overall_validation_reasoning: str = Field(description="Overall validation reasoning")

class SingleGDCMapping(BaseModel):
    """Pydantic model for a single GDC mapping"""
    gdc_name: str = Field(description="GDC name")
    gdc_description: str = Field(description="GDC description")
    mapping_rank: int = Field(ge=1, description="Rank of this mapping")
    reasoning: str = Field(description="Comprehensive reasoning for this mapping")
    evidence_summary: List[str] = Field(description="Summary of evidence", default_factory=list)

class FinalMappingDecision(BaseModel):
    """Pydantic model for Final Mapping Decision"""
    gdc_mappings: List[SingleGDCMapping] = Field(description="All relevant GDC mappings")
    overall_reasoning: str = Field(description="Overall reasoning for the mapping decisions")

class MappingResult(BaseModel):
    """Pydantic model for final mapping result"""
    guid: str = Field(alias="GUID")
    code: str = Field(alias="Code")
    name: str = Field(alias="Name")
    description: str = Field(alias="Description")
    gdc_name: str = Field(alias="GDC Name")
    gdc_description: str = Field(alias="GDC Description")
    mapping_rank: int = Field(alias="Mapping Rank")
    reasoning: str = Field(alias="Reasoning")
    
    class Config:
        populate_by_name = True

# ==================== TEXT PREPROCESSING ====================

def to_lowercase(text: str) -> str:
    """Convert text to lowercase for consistent processing"""
    return text.lower() if text else ""

def preprocess_text(text: str) -> str:
    """Preprocess text: lowercase and clean"""
    text = to_lowercase(text)
    text = " ".join(text.split())
    return text

# ==================== CONTEXT ENGINEERING ====================

def create_enriched_gdc_master_document(gdc: GDCMaster) -> Document:
    """Create context-enriched document for GDC Master with metadata"""
    gdc_name_lower = preprocess_text(gdc.gdc_name)
    definition_lower = preprocess_text(gdc.definition)
    domain_lower = preprocess_text(gdc.data_domain)
    
    enriched_content = f"""gdc category name: {gdc_name_lower}
data domain: {domain_lower}
definition and description: {definition_lower}
this is a group data category for classification purposes
keywords: {gdc_name_lower} {domain_lower}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "data_domain": domain_lower,
        "definition": definition_lower,
        "type": "gdc_master"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

def create_enriched_gdc_context_document(gdc_ctx: GDCWithContext) -> Document:
    """Create context-enriched document for GDC with hierarchical context"""
    gdc_name_lower = preprocess_text(gdc_ctx.gdc_name)
    gdc_desc_lower = preprocess_text(gdc_ctx.gdc_description)
    
    pbt_names = []
    app_names = []
    process_names = []
    
    for pbt in gdc_ctx.pbts:
        pbt_name_lower = preprocess_text(pbt.pbt_name)
        pbt_desc_lower = preprocess_text(pbt.pbt_desc)
        pbt_names.append(f"{pbt_name_lower} ({pbt_desc_lower})")
        
        for app in pbt.apps:
            app_name_lower = preprocess_text(app.app_name)
            app_desc_lower = preprocess_text(app.app_description)
            app_names.append(f"{app_name_lower} - {app_desc_lower}")
            
            for proc in app.processes:
                proc_name_lower = preprocess_text(proc.process_name)
                proc_desc_lower = preprocess_text(proc.process_description)
                process_names.append(f"{proc_name_lower}: {proc_desc_lower}")
    
    enriched_content = f"""gdc category: {gdc_name_lower}
gdc_description: {gdc_desc_lower}

primary business types (pbts):
{chr(10).join(f"- {pbt}" for pbt in pbt_names) if pbt_names else "- none"}

applications using this gdc:
{chr(10).join(f"- {app}" for app in app_names) if app_names else "- none"}

related processes:
{chr(10).join(f"- {proc}" for proc in process_names) if process_names else "- none"}

contextual keywords: {gdc_name_lower} {' '.join(pbt_names)} {' '.join(app_names)}"""
    
    metadata = {
        "gdc_name": gdc_name_lower,
        "gdc_description": gdc_desc_lower,
        "pbt_count": len(pbt_names),
        "app_count": len(app_names),
        "process_count": len(process_names),
        "type": "gdc_context"
    }
    
    return Document(page_content=enriched_content, metadata=metadata)

# ==================== UTILITY FUNCTIONS ====================

def extract_json_from_text(text: str) -> str:
    """Extract JSON from text with improved robustness - FIXED VERSION"""
    # Remove markdown code blocks
    text = re.sub(r'```(?:json)?\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    
    # Try to find JSON object or array
    # Look for the largest valid JSON structure
    json_patterns = [
        r'\{(?:[^{}]|(?:\{[^{}]*\}))*\}',  # Match nested objects
        r'\[(?:[^\[\]]|(?:\[[^\[\]]*\]))*\]'  # Match nested arrays
    ]
    
    matches = []
    for pattern in json_patterns:
        found = re.finditer(pattern, text, re.DOTALL)
        matches.extend(found)
    
    if matches:
        # Get the longest match (likely to be the complete JSON)
        longest_match = max(matches, key=lambda m: len(m.group(0)))
        return longest_match.group(0)
    
    return text

def load_json_file(filepath: str, model_class: type[BaseModel]) -> List[BaseModel]:
    """Load and validate JSON file using Pydantic model"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        validated_data = []
        for item in data:
            try:
                validated_item = model_class.model_validate(item)
                validated_data.append(validated_item)
            except Exception as e:
                print(f"Validation error for item: {e}")
                continue
        
        return validated_data
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return []

# ==================== RAG VECTOR STORE SETUP ====================

def build_gdc_master_vectorstore(gdc_master_list: List[GDCMaster]) -> InMemoryVectorStore:
    """Build vector store for GDC Master data"""
    print(f"🔧 Building GDC Master vector store with {EMBEDDING_MODEL}...")
    
    documents = [create_enriched_gdc_master_document(gdc) for gdc in gdc_master_list]
    
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Master vector store created with {len(documents)} documents")
    return vectorstore

def build_gdc_context_vectorstore(gdc_context_list: List[GDCWithContext]) -> InMemoryVectorStore:
    """Build vector store for GDC Context data"""
    print(f"🔧 Building GDC Context vector store with {EMBEDDING_MODEL}...")
    
    documents = [create_enriched_gdc_context_document(gdc_ctx) for gdc_ctx in gdc_context_list]
    
    vectorstore = InMemoryVectorStore.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"✓ GDC Context vector store created with {len(documents)} documents")
    return vectorstore

def rag_retrieve_relevant_gdcs(query: str, k: int = 10) -> str:
    """RAG: Retrieve relevant GDCs using semantic search"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    query_lower = preprocess_text(query)
    
    master_results = gdc_master_vectorstore.similarity_search(query_lower, k=k)
    context_results = gdc_context_vectorstore.similarity_search(query_lower, k=k)
    
    retrieved_info = {
        "master_matches": [],
        "context_matches": []
    }
    
    for doc in master_results:
        retrieved_info["master_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "definition": doc.metadata.get("definition", ""),
            "data_domain": doc.metadata.get("data_domain", "")
        })
    
    for doc in context_results:
        retrieved_info["context_matches"].append({
            "gdc_name": doc.metadata.get("gdc_name", ""),
            "gdc_description": doc.metadata.get("gdc_description", ""),
            "pbt_count": doc.metadata.get("pbt_count", 0),
            "app_count": doc.metadata.get("app_count", 0),
            "process_count": doc.metadata.get("process_count", 0),
            "content_preview": doc.page_content[:300]
        })
    
    return json.dumps(retrieved_info, indent=2)

# ==================== EXPERT TOOLS ====================

@tool
def semantic_similarity_expert(record_name: str, record_desc: str) -> str:
    """
    SEMANTIC SIMILARITY EXPERT - Comprehensive Analysis with Chain of Thought
    
    This expert uses advanced RAG retrieval to identify ALL semantically relevant GDCs.
    
    ANALYSIS METHODOLOGY:
    1. Semantic Vector Analysis: Uses text-embedding-3-large to capture deep semantic relationships
    2. Multi-dimensional Matching: Considers name, description, domain, and contextual keywords
    3. Threshold-based Filtering: Identifies all GDCs above relevance threshold
    4. Ranking and Scoring: Provides confidence scores for each match
    
    CHAIN OF THOUGHT PROCESS:
    - Extract key concepts and terminology from record class
    - Perform vector similarity search across GDC master database
    - Analyze semantic relationships beyond keyword matching
    - Identify primary, secondary, and tertiary matches
    - Justify each match with specific evidence
    - Explain why multiple GDCs may be relevant
    
    OUTPUT: Comprehensive list of all semantically relevant GDCs with detailed reasoning
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    query = f"{record_name_lower} {record_desc_lower}"
    retrieved_gdcs = rag_retrieve_relevant_gdcs(query, k=15)
    
    prompt = f"""You are a SEMANTIC SIMILARITY EXPERT with deep expertise in data classification and taxonomy mapping.

CRITICAL INSTRUCTION: A Record Class can map to MULTIPLE GDCs. Your task is to identify ALL relevant GDCs, not just the best match.

RECORD CLASS TO ANALYZE:
Name: {record_name_lower}
Description: {record_desc_lower}

RAG-RETRIEVED RELEVANT GDCs (via vector similarity search using text-embedding-3-large):
{retrieved_gdcs}

COMPREHENSIVE ANALYSIS FRAMEWORK:

STEP 1 - CONCEPTUAL EXTRACTION:
Identify the core concepts, data types, business domains, and functional areas represented in the record class name and description.

STEP 2 - SEMANTIC MATCHING:
For each retrieved GDC, analyze:
- Direct semantic overlap (shared concepts, synonyms, related terms)
- Domain alignment (business area, functional category)
- Definitional correspondence (similar purpose or scope)
- Hierarchical relationships (parent-child, sibling categories)

STEP 3 - RELEVANCE SCORING:
Score each GDC from 0-100 based on:
- 90-100: Primary match (direct alignment, core category)
- 70-89: Strong secondary match (significant overlap, related category)
- 50-69: Moderate match (partial alignment, tangential relationship)
- Below 50: Weak match (minimal relevance)

STEP 4 - MULTI-MATCH JUSTIFICATION:
Explain why multiple GDCs may be needed:
- Different aspects of the record class
- Multiple data domains involved
- Hierarchical categorization needs
- Cross-functional data usage

STEP 5 - EVIDENCE SYNTHESIS:
For each identified GDC, provide specific evidence:
- Which keywords/concepts match
- How definitions align
- What domain overlap exists
- Why this GDC is relevant

OUTPUT REQUIREMENTS:
Return ONLY a valid JSON object in this exact format (no additional text, no markdown):
{{
  "matches": [
    {{
      "gdc_name": "exact gdc name from retrieval",
      "gdc_description": "exact definition from retrieval",
      "similarity_score": 88.5,
      "reasoning": "Detailed explanation: This GDC matches because [specific evidence]. The record class's [specific aspect] aligns with this GDC's [specific characteristic]. Key overlapping concepts include [list concepts]."
    }}
  ],
  "multiple_matches_rationale": "Comprehensive explanation of why multiple GDCs are needed: [explain different aspects, domains, or hierarchical needs that require multiple categories]"
}}

CRITICAL REQUIREMENTS:
- Include ALL GDCs with similarity_score >= 50
- Provide detailed, evidence-based reasoning for each match
- Use exact GDC names from the retrieved data
- Explain multi-match scenarios clearly
- Return ONLY valid JSON, no markdown formatting"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"[ERROR] Semantic Similarity Expert failed: {e}")
        error_response = SemanticMatchResponse(
            matches=[],
            multiple_matches_rationale=f"error during semantic analysis: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def context_analysis_expert(record_name: str, record_desc: str, semantic_matches: str) -> str:
    """
    CONTEXT ANALYSIS EXPERT - Deep Contextual Validation with Chain of Thought
    
    This expert analyzes the contextual appropriateness of GDC matches using hierarchical business context.
    
    ANALYSIS METHODOLOGY:
    1. Contextual Retrieval: Uses RAG to retrieve PBT, Application, and Process context
    2. Hierarchical Analysis: Examines relationships across business taxonomy levels
    3. Evidence Gathering: Collects supporting evidence from multiple context sources
    4. Alignment Scoring: Rates contextual fit for each proposed GDC
    
    CHAIN OF THOUGHT PROCESS:
    - Review semantic matches from previous expert
    - Retrieve detailed context for each GDC (PBTs, Apps, Processes)
    - Analyze how record class usage aligns with GDC's operational context
    - Identify supporting and contradicting evidence
    - Validate cross-functional usage patterns
    - Score contextual alignment
    - Filter out GDCs with poor contextual fit
    
    OUTPUT: Context-validated GDCs with evidence and alignment scores
    """
    record_name_lower = preprocess_text(record_name)
    record_desc_lower = preprocess_text(record_desc)
    
    try:
        matches_data = json.loads(extract_json_from_text(semantic_matches))
        gdc_names = [m.get("gdc_name", "").lower() for m in matches_data.get("matches", [])]
    except Exception as e:
        print(f"[WARNING] Failed to parse semantic matches: {e}")
        gdc_names = []
    
    context_query = f"{record_name_lower} {record_desc_lower} {' '.join(gdc_names)}"
    retrieved_context = rag_retrieve_relevant_gdcs(context_query, k=10)
    
    prompt = f"""You are a CONTEXT ANALYSIS EXPERT specializing in business taxonomy validation and operational context alignment.

RECORD CLASS TO VALIDATE:
Name: {record_name_lower}
Description: {record_desc_lower}

SEMANTIC MATCHES FROM PREVIOUS EXPERT:
{semantic_matches}

RAG-RETRIEVED CONTEXTUAL INFORMATION (PBTs, Applications, Processes):
{retrieved_context}

COMPREHENSIVE CONTEXTUAL ANALYSIS FRAMEWORK:

STEP 1 - CONTEXT EXTRACTION:
For each semantically matched GDC, extract:
- Primary Business Types (PBTs) that use this GDC
- Applications that implement this GDC
- Business processes that generate/consume this data
- Cross-functional usage patterns

STEP 2 - ALIGNMENT ANALYSIS:
Evaluate contextual fit by analyzing:
- Business domain alignment (does the PBT match the record's purpose?)
- Application scope (are the apps relevant to this record type?)
- Process relevance (do the processes align with record usage?)
- Organizational context (cross-functional vs. domain-specific)

STEP 3 - EVIDENCE GATHERING:
For each GDC, collect specific evidence:
- "PBT: [name] - [why this PBT supports the mapping]"
- "Application: [name] - [how this app uses similar data]"
- "Process: [name] - [why this process is relevant]"
- List ALL relevant evidence points

STEP 4 - CONTEXTUAL SCORING:
Score contextual alignment (0-100):
- 90-100: Perfect contextual fit (strong PBT/App/Process alignment)
- 70-89: Good contextual fit (multiple supporting factors)
- 50-69: Moderate fit (some contextual support)
- Below 50: Poor fit (weak or contradicting context)

STEP 5 - RELEVANCE FILTERING:
- KEEP all GDCs with contextual alignment >= 60
- REMOVE GDCs with poor contextual fit
- EXPLAIN why each GDC remains relevant or is filtered out

STEP 6 - CROSS-VALIDATION:
- Verify that retained GDCs have complementary, not redundant, contexts
- Ensure multiple GDCs serve different aspects of the record class
- Explain the unique contextual contribution of each GDC

OUTPUT REQUIREMENTS:
Return ONLY a valid JSON object in this exact format (no additional text, no markdown):
{{
  "context_analysis": [
    {{
      "gdc_name": "exact gdc name",
      "context_evidence": [
        "PBT: [pbt name] - [specific reason why this PBT supports the mapping]",
        "Application: [app name] - [specific reason why this app is relevant]",
        "Process: [process name] - [specific reason why this process uses this data]"
      ],
      "alignment_score": 90.0,
      "relevance_justification": "This GDC is contextually relevant because [specific business context]. The PBTs [list] indicate [business domain]. The applications [list] show [operational usage]. The processes [list] demonstrate [functional alignment].",
      "reasoning": "Comprehensive contextual analysis: [detailed explanation of how context supports or contradicts this mapping]"
    }}
  ],
  "all_relevant_gdcs": ["gdc1", "gdc2", "gdc3"]
}}

CRITICAL REQUIREMENTS:
- Include ALL GDCs with alignment_score >= 60
- Provide specific PBT/App/Process evidence for each GDC
- Use exact GDC names from semantic matches
- Explain contextual contribution clearly
- Return ONLY valid JSON, no markdown formatting"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"[ERROR] Context Analysis Expert failed: {e}")
        error_response = ContextAnalysisResponse(
            context_analysis=[],
            all_relevant_gdcs=[]
        )
        return error_response.model_dump_json()

@tool
def validation_expert(record_name: str, proposed_gdcs: str, validation_data: str) -> str:
    """
    VALIDATION EXPERT - Historical Validation with Chain of Thought
    
    This expert validates proposed GDC mappings against historical validation data.
    
    ANALYSIS METHODOLOGY:
    1. Historical Lookup: Searches validation dataset for matching patterns
    2. Consistency Checking: Identifies confirmations and conflicts
    3. Confidence Assessment: Evaluates strength of validation evidence
    4. Discrepancy Analysis: Investigates and explains conflicts
    
    CHAIN OF THOUGHT PROCESS:
    - Review proposed GDCs from context analysis
    - Search validation dataset for historical mappings
    - Identify exact matches, partial matches, and conflicts
    - Assess validation confidence (strong, moderate, weak)
    - Explain any discrepancies or missing validations
    - Provide recommendations on validation status
    
    OUTPUT: Validation status for each proposed GDC with detailed reasoning
    """
    record_name_lower = preprocess_text(record_name)
    
    try:
        val_data = json.loads(validation_data)
        val_data_lower = []
        for entry in val_data:
            val_data_lower.append({
                "gdc_name": to_lowercase(entry.get("GDC Name", "")),
                "gdc_description": to_lowercase(entry.get("GDC Description", "")),
                "ilm_category_name": to_lowercase(entry.get("ILM Category Name", ""))
            })
        validation_data_lower = json.dumps(val_data_lower, indent=2)
    except Exception as e:
        print(f"[WARNING] Failed to preprocess validation data: {e}")
        validation_data_lower = validation_data
    
    prompt = f"""You are a VALIDATION EXPERT specializing in historical data validation and consistency checking.

RECORD CLASS NAME: {record_name_lower}

PROPOSED GDCs FROM CONTEXT ANALYSIS:
{proposed_gdcs}

HISTORICAL VALIDATION DATASET:
{validation_data_lower}

COMPREHENSIVE VALIDATION FRAMEWORK:

STEP 1 - VALIDATION LOOKUP:
For each proposed GDC, search the validation dataset for:
- Exact GDC name matches
- Similar GDC names (accounting for variations)
- Related ILM category mappings
- Historical usage patterns

STEP 2 - MATCH CLASSIFICATION:
Classify each validation result:
- CONFIRMED: Exact match found in validation set (strong validation)
- PARTIAL: Similar match found with minor variations (moderate validation)
- CONFLICTED: Different GDC suggested in validation set (requires investigation)
- NOT_FOUND: No historical validation available (neutral, not negative)

STEP 3 - CONFIDENCE ASSESSMENT:
Evaluate validation confidence:
- High confidence: Direct confirmation with matching ILM categories
- Moderate confidence: Partial match or related categories
- Low confidence: No validation data or conflicting evidence
- Consider that absence of validation does NOT mean incorrect mapping

STEP 4 - DISCREPANCY ANALYSIS:
For conflicts or missing validations:
- Explain possible reasons (new classification, evolving taxonomy)
- Assess whether conflict is actual error or taxonomy evolution
- Consider that proposed GDC may be more accurate than historical data
- Provide nuanced interpretation

STEP 5 - VALIDATION SYNTHESIS:
For each GDC provide:
- Clear validation status
- Supporting or contradicting evidence from validation set
- Confidence level in the validation
- Recommendations (keep, review, remove)

OUTPUT REQUIREMENTS:
Return ONLY a valid JSON object in this exact format (no additional text, no markdown):
{{
  "validation_results": [
    {{
      "gdc_name": "exact gdc name",
      "validation_found": true,
      "matching_entry": {{
        "gdc_name": "validated gdc name",
        "ilm_category_name": "associated ilm category"
      }},
      "validation_status": "confirmed",
      "validation_reasoning": "This GDC is [confirmed/conflicted/not_found] in the validation set. [Specific evidence from validation data]. [Interpretation and confidence level]. [Recommendation]."
    }}
  ],
  "overall_validation_reasoning": "Summary of validation findings: [how many confirmed, how many conflicted, how many not found]. [Overall confidence in proposed mappings]. [Any patterns or concerns identified]. [Final validation assessment]."
}}

VALIDATION STATUS VALUES:
- "confirmed": Exact or strong match in validation set
- "partial": Similar match with minor variations
- "conflicted": Different GDC suggested in validation
- "not_found": No historical validation data

CRITICAL REQUIREMENTS:
- Validate ALL proposed GDCs
- Provide specific validation evidence when available
- Explain absence of validation (not automatically negative)
- Give nuanced interpretation of conflicts
- Use exact GDC names
- Return ONLY valid JSON, no markdown formatting"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"[ERROR] Validation Expert failed: {e}")
        error_response = ValidationResponse(
            validation_results=[],
            overall_validation_reasoning=f"error during validation: {str(e)}"
        )
        return error_response.model_dump_json()

@tool
def final_decision_expert(record_info: str, all_analyses: str) -> str:
    """
    FINAL DECISION EXPERT - Synthesis and Decision Making with Chain of Thought
    
    This expert synthesizes all previous analyses to make final mapping decisions.
    
    DECISION METHODOLOGY:
    1. Evidence Synthesis: Integrates semantic, contextual, and validation analyses
    2. Ranking Algorithm: Scores and ranks GDCs based on multi-factor assessment
    3. Quality Filtering: Ensures only high-quality mappings are included
    4. Reasoning Documentation: Provides comprehensive justification
    
    CHAIN OF THOUGHT PROCESS:
    - Review semantic similarity scores and reasoning
    - Consider contextual alignment and evidence
    - Factor in validation findings (confirmations/conflicts)
    - Weight different evidence types appropriately
    - Resolve conflicts between different expert opinions
    - Rank GDCs by overall confidence and relevance
    - Provide final mapping decision for each GDC
    - Document comprehensive reasoning with evidence trail
    
    OUTPUT: Final ranked GDC mappings with detailed reasoning
    """
    prompt = f"""You are the FINAL DECISION EXPERT responsible for synthesizing all expert analyses and making final GDC mapping decisions.

RECORD INFORMATION:
{record_info}

ALL EXPERT ANALYSES:
{all_analyses}

COMPREHENSIVE DECISION-MAKING FRAMEWORK:

STEP 1 - EVIDENCE INTEGRATION:
Synthesize findings from all experts:
- Semantic Similarity Expert: Primary relevance scores and semantic matches
- Context Analysis Expert: Contextual fit, PBT/App/Process evidence
- Validation Expert: Historical validation, confirmations, conflicts
Weight each evidence type:
- Semantic similarity: 35% (foundation of relevance)
- Contextual alignment: 40% (operational fit)
- Validation: 25% (historical confirmation)

STEP 2 - COMPREHENSIVE SCORING:
Calculate overall score for each GDC:
- Start with semantic similarity score
- Adjust based on contextual alignment (±15 points)
- Adjust based on validation status:
  * Confirmed: +10 points
  * Partial: +5 points
  * Conflicted: -5 points (but analyze reason)
  * Not found: 0 points (neutral)
- Final score range: 0-100

STEP 3 - RANKING AND FILTERING:
- KEEP all GDCs with overall score >= 65
- RANK by overall score (highest = rank 1)
- EXPLAIN why each GDC is included
- If multiple GDCs have similar scores, explain their distinct contributions

STEP 4 - REASONING SYNTHESIS:
For each GDC mapping, provide comprehensive reasoning that includes:
- Primary reason for the mapping (key semantic/contextual factors)
- Supporting evidence from each expert (specific points)
- How this GDC serves the record class (functional role)
- Why this GDC is distinct from others (if multiple mappings)
- Confidence level (high/medium based on evidence strength)

STEP 5 - EVIDENCE DOCUMENTATION:
For each GDC, create evidence summary with specific points from:
- Semantic analysis: Key matching concepts and similarity score
- Contextual analysis: Specific PBTs/Apps/Processes that support mapping
- Validation: Confirmation status and historical evidence

STEP 6 - MULTI-MAPPING RATIONALE:
If multiple GDCs are mapped:
- Explain why multiple categories are needed
- Describe what distinct aspect each GDC covers
- Ensure GDCs are complementary, not redundant
- Justify the one-to-many relationship

OUTPUT REQUIREMENTS:
Return ONLY a valid JSON object in this exact format (no additional text, no markdown):
{{
  "gdc_mappings": [
    {{
      "gdc_name": "exact gdc name",
      "gdc_description": "exact definition",
      "mapping_rank": 1,
      "reasoning": "COMPREHENSIVE REASONING: This GDC is the [primary/secondary/tertiary] mapping because [synthesis of all expert findings]. SEMANTIC ANALYSIS: [key points from semantic expert]. CONTEXTUAL ANALYSIS: [key points from context expert with specific PBT/App/Process references]. VALIDATION: [validation status and interpretation]. OVERALL CONFIDENCE: [High/Medium] based on [evidence strength summary]. This GDC addresses [specific aspect of record class].",
      "evidence_summary": [
        "Semantic: similarity score [X], matched on [specific concepts]",
        "Context: aligned with PBT '[name]', App '[name]', Process '[name]'",
        "Validation: [status] - [specific validation evidence or reason for absence]"
      ]
    }}
  ],
  "overall_reasoning": "FINAL DECISION RATIONALE: After comprehensive analysis by all experts, [X] GDC(s) are identified as relevant for this record class. [If multiple]: Multiple GDCs are necessary because [explain different aspects/domains/purposes]. [Ranking explanation]: The ranking is based on [criteria]. [Confidence statement]: Overall confidence in these mappings is [High/Medium] because [summary of evidence strength across all experts]."
}}

CRITICAL REQUIREMENTS:
- Include ALL GDCs with overall score >= 65
- Rank by overall confidence (best = rank 1)
- Provide comprehensive reasoning that synthesizes ALL expert opinions
- Include specific evidence from each expert in evidence_summary
- Explain multi-mapping scenarios clearly
- Use exact GDC names from analyses
- Return ONLY valid JSON, no markdown formatting
- Ensure reasoning is detailed and evidence-based (not generic)"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        print(f"[ERROR] Final Decision Expert failed: {e}")
        error_response = FinalMappingDecision(
            gdc_mappings=[],
            overall_reasoning=f"error during final decision: {str(e)}"
        )
        return error_response.model_dump_json()

# ==================== REACT AGENT WORKFLOW ====================

def create_react_agent_workflow():
    """Create LangGraph ReAct agent with comprehensive system prompt"""
    
    tools = [
        semantic_similarity_expert,
        context_analysis_expert,
        validation_expert,
        final_decision_expert
    ]
    
    system_prompt = """You are an EXPERT GDC MAPPING COORDINATOR orchestrating a mixture of experts system for data classification.

MISSION: Map record classes to appropriate Group Data Categories (GDCs) using comprehensive analysis and evidence-based reasoning.

CRITICAL PRINCIPLES:
1. ONE-TO-MANY MAPPING: Record classes can and often should map to MULTIPLE GDCs
2. EVIDENCE-BASED: Every decision must be supported by specific evidence
3. COMPREHENSIVE ANALYSIS: Use ALL available data sources (semantic, contextual, validation)
4. DYNAMIC REASONING: Adapt analysis depth based on record complexity

EXPERT WORKFLOW (MUST FOLLOW IN ORDER):

PHASE 1 - SEMANTIC ANALYSIS:
Call: semantic_similarity_expert(record_name, record_description)
Purpose: Identify ALL semantically relevant GDCs using vector similarity
Expected Output: List of candidate GDCs with similarity scores and reasoning
Key Focus: Cast wide net, include all potentially relevant categories

PHASE 2 - CONTEXTUAL VALIDATION:
Call: context_analysis_expert(record_name, record_description, semantic_matches)
Purpose: Validate semantic matches against operational context (PBTs, Apps, Processes)
Expected Output: Context-filtered GDCs with alignment scores and evidence
Key Focus: Verify operational fit, gather supporting evidence from business context

PHASE 3 - HISTORICAL VALIDATION:
Call: validation_expert(record_name, proposed_gdcs, validation_data)
Purpose: Check proposed mappings against historical validation dataset
Expected Output: Validation status for each GDC (confirmed/conflicted/not_found)
Key Focus: Identify confirmations and investigate conflicts

PHASE 4 - FINAL SYNTHESIS:
Call: final_decision_expert(record_info, all_analyses)
Purpose: Synthesize all analyses and make final mapping decisions with ranking
Expected Output: Ranked list of GDC mappings with comprehensive reasoning
Key Focus: Integrate all evidence, resolve conflicts, provide final recommendations

COORDINATION RESPONSIBILITIES:
- Ensure all experts are called in sequence
- Pass appropriate data between experts
- Handle expert failures gracefully
- Synthesize final output from expert recommendations
- Ensure reasoning is comprehensive and evidence-based

TEXT PREPROCESSING:
- ALL text is converted to lowercase for consistency
- Names, descriptions, and all textual data are preprocessed
- Maintain consistency throughout the analysis chain

QUALITY STANDARDS:
- Never output "unknown" unless absolutely no valid GDC can be identified
- Always prefer evidence-based mappings over defaults
- Provide detailed reasoning with specific references to evidence
- Support multi-GDC mappings when record class spans multiple categories

RAG INTEGRATION:
- All experts use RAG retrieval with OpenAI text-embedding-3-large
- Vector stores contain enriched GDC documents with context
- Semantic search returns top-k most relevant GDCs
- Context includes PBTs, Applications, and Processes"""
    
    agent = create_react_agent(
        model=llm,
        tools=tools,
        prompt=system_prompt
    )
    
    return agent

def process_single_record(
    agent,
    record: RecordClass,
    validation_set: List[ValidationEntry]
) -> List[MappingResult]:
    """Process a single record through the ReAct agent - FIXED VERSION"""
    
    print(f"\n{'='*80}")
    print(f"Processing: {record.name}")
    print(f"{'='*80}")
    
    validation_json = json.dumps([v.model_dump() for v in validation_set], indent=2)
    
    query = f"""Map this Record Class to ALL relevant GDCs using the complete expert workflow:

RECORD CLASS DETAILS:
- GUID: {record.guid}
- Code: {record.code}
- Name: {record.name}
- Description: {record.description}

REQUIRED WORKFLOW:
1. Call semantic_similarity_expert with record name and description
2. Call context_analysis_expert with record info and semantic matches
3. Call validation_expert with record name, proposed GDCs, and validation data
4. Call final_decision_expert with all analyses to synthesize final mappings

VALIDATION DATASET:
{validation_json}

CRITICAL REQUIREMENTS:
- Follow the workflow in exact order
- Pass results from each expert to the next
- Ensure final_decision_expert receives ALL previous analyses
- Return comprehensive mappings with detailed reasoning
- Support multiple GDC mappings when appropriate

Begin analysis now."""
    
    try:
        # Invoke the agent
        result = agent.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        messages = result.get("messages", [])
        print(f"\n[DEBUG] Received {len(messages)} messages from agent")
        
        # Extract final response - look through messages for tool outputs
        final_decision_content = None
        
        # Search through messages for final_decision_expert output
        for msg in reversed(messages):  # Start from end
            if hasattr(msg, 'content') and msg.content:
                # Check if this is a tool response
                if hasattr(msg, 'name') and msg.name == 'final_decision_expert':
                    final_decision_content = msg.content
                    print(f"[DEBUG] Found final_decision_expert output")
                    break
                # Check if content contains JSON that looks like final decision
                elif isinstance(msg.content, str) and 'gdc_mappings' in msg.content.lower():
                    final_decision_content = msg.content
                    print(f"[DEBUG] Found potential final decision in message content")
                    break
        
        # If not found, use last message
        if not final_decision_content:
            final_message = messages[-1] if messages else None
            if final_message and hasattr(final_message, 'content'):
                final_decision_content = final_message.content
                print(f"[DEBUG] Using last message as fallback")
        
        if not final_decision_content:
            raise ValueError("No valid response content from agent")
        
        print(f"[DEBUG] Processing final decision content (length: {len(final_decision_content)})")
        print(f"[DEBUG] Content preview: {final_decision_content[:200]}...")
        
        # Extract and parse final decision
        final_decision = extract_final_decision(final_decision_content)
        
        print(f"[DEBUG] Extracted final decision: {json.dumps(final_decision, indent=2)[:500]}...")
        
        mapping_results = []
        gdc_mappings = final_decision.get("gdc_mappings", [])
        overall_reasoning = final_decision.get("overall_reasoning", "")
        
        print(f"[DEBUG] Found {len(gdc_mappings)} GDC mappings")
        
        if not gdc_mappings:
            print("[WARNING] No GDC mappings found in final decision")
            # Try to extract from raw content as last resort
            try:
                print("[DEBUG] Attempting to extract GDC names from raw content...")
                # Look for GDC names in the content
                import re
                gdc_pattern = r'"gdc_name"\s*:\s*"([^"]+)"'
                found_gdcs = re.findall(gdc_pattern, final_decision_content)
                if found_gdcs:
                    print(f"[DEBUG] Found GDC names in content: {found_gdcs}")
                    for idx, gdc_name in enumerate(found_gdcs, 1):
                        mapping_results.append(MappingResult(
                            guid=record.guid,
                            code=record.code,
                            name=record.name,
                            description=record.description,
                            gdc_name=gdc_name,
                            gdc_description="extracted from response",
                            mapping_rank=idx,
                            reasoning="Extracted from agent response - full reasoning unavailable due to parsing issue"
                        ))
                else:
                    # Truly no mappings found
                    mapping_results.append(MappingResult(
                        guid=record.guid,
                        code=record.code,
                        name=record.name,
                        description=record.description,
                        gdc_name="unknown",
                        gdc_description="",
                        mapping_rank=1,
                        reasoning="No valid GDC mappings could be extracted from agent response"
                    ))
            except Exception as extract_error:
                print(f"[ERROR] Failed to extract GDCs from content: {extract_error}")
                mapping_results.append(MappingResult(
                    guid=record.guid,
                    code=record.code,
                    name=record.name,
                    description=record.description,
                    gdc_name="unknown",
                    gdc_description="",
                    mapping_rank=1,
                    reasoning="Failed to extract GDC mappings from agent response"
                ))
        else:
            # Process valid mappings
            for idx, mapping in enumerate(gdc_mappings, 1):
                gdc_name = mapping.get("gdc_name", "unknown")
                print(f"[DEBUG] Processing mapping {idx}: {gdc_name}")
                
                mapping_results.append(MappingResult(
                    guid=record.guid,
                    code=record.code,
                    name=record.name,
                    description=record.description,
                    gdc_name=gdc_name,
                    gdc_description=mapping.get("gdc_description", ""),
                    mapping_rank=mapping.get("mapping_rank", idx),
                    reasoning=format_mapping_reasoning(mapping, overall_reasoning)
                ))
        
        print(f"[DEBUG] Created {len(mapping_results)} mapping results")
        return mapping_results
        
    except Exception as e:
        print(f"[ERROR] Exception in process_single_record: {type(e).__name__}: {e}")
        import traceback
        print(f"[ERROR] Traceback: {traceback.format_exc()}")
        
        return [MappingResult(
            guid=record.guid,
            code=record.code,
            name=record.name,
            description=record.description,
            gdc_name="error",
            gdc_description="",
            mapping_rank=1,
            reasoning=f"Error during processing: {str(e)}"
        )]

def extract_final_decision(response_text: str) -> Dict[str, Any]:
    """Extract final decision from agent response - IMPROVED VERSION"""
    try:
        print(f"[DEBUG] Extracting JSON from response (length: {len(response_text)})")
        
        # Extract JSON
        json_str = extract_json_from_text(response_text)
        print(f"[DEBUG] Extracted JSON string (length: {len(json_str)})")
        print(f"[DEBUG] JSON preview: {json_str[:300]}...")
        
        # Parse JSON
        decision_data = json.loads(json_str)
        print(f"[DEBUG] Successfully parsed JSON")
        print(f"[DEBUG] JSON keys: {decision_data.keys()}")
        
        # Validate with Pydantic
        final_decision = FinalMappingDecision.model_validate(decision_data)
        print(f"[DEBUG] Successfully validated with Pydantic")
        print(f"[DEBUG] Number of mappings: {len(final_decision.gdc_mappings)}")
        
        result = final_decision.model_dump()
        print(f"[DEBUG] Returning final decision with {len(result.get('gdc_mappings', []))} mappings")
        return result
        
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSON parsing failed: {e}")
        print(f"[ERROR] Attempted to parse: {json_str[:500] if 'json_str' in locals() else 'N/A'}...")
        return {
            "gdc_mappings": [],
            "overall_reasoning": f"JSON parsing error: {str(e)}"
        }
    except Exception as e:
        print(f"[ERROR] Failed to extract final decision: {type(e).__name__}: {e}")
        print(f"[ERROR] Response text preview: {response_text[:500]}...")
        return {
            "gdc_mappings": [],
            "overall_reasoning": f"Extraction error: {str(e)}"
        }

def format_mapping_reasoning(mapping: Dict[str, Any], overall_reasoning: str) -> str:
    """Format reasoning for output"""
    parts = []
    
    reasoning = mapping.get("reasoning", "")
    if reasoning:
        parts.append(f"MAPPING REASONING:\n{reasoning}")
    
    evidence = mapping.get("evidence_summary", [])
    if evidence:
        parts.append(f"\n\nEVIDENCE SUMMARY:\n" + "\n".join(f"• {e}" for e in evidence))
    
    if overall_reasoning and mapping.get("mapping_rank", 1) == 1:
        parts.append(f"\n\nOVERALL CONTEXT:\n{overall_reasoning}")
    
    return "\n".join(parts)

# ==================== EXCEL OUTPUT FUNCTIONS ====================

def save_to_excel_with_grouping(results: List[MappingResult], output_path: str):
    """Save results to Excel with grouping by record class"""
    print("\n💾 Saving results to Excel with grouping...")
    
    # Convert to DataFrame
    results_dict = [r.model_dump(by_alias=True) for r in results]
    df = pd.DataFrame(results_dict)
    
    # Sort by Name (record class) and Mapping Rank
    df = df.sort_values(['Name', 'Mapping Rank'])
    
    # Create Excel writer
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='GDC Mappings', index=False)
        
        # Get workbook and worksheet
        workbook = writer.book
        worksheet = writer.sheets['GDC Mappings']
        
        # Auto-adjust column widths
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 100)
            worksheet.column_dimensions[column_letter].width = adjusted_width
        
        # Apply styling
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        
        # Header styling
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        
        for cell in worksheet[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
        
        # Freeze header row
        worksheet.freeze_panes = 'A2'
        
        # Add borders
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        for row in worksheet.iter_rows(min_row=1, max_row=worksheet.max_row, 
                                       min_col=1, max_col=worksheet.max_column):
            for cell in row:
                cell.border = thin_border
        
        # Alternate row colors for better readability
        light_fill = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")
        
        current_name = None
        use_fill = False
        
        for idx, row in enumerate(worksheet.iter_rows(min_row=2, max_row=worksheet.max_row), start=2):
            name_cell = row[2]  # 'Name' is the 3rd column (index 2)
            
            # Change fill when we encounter a new record class name
            if name_cell.value != current_name:
                current_name = name_cell.value
                use_fill = not use_fill
            
            if use_fill:
                for cell in row:
                    cell.fill = light_fill
    
    print(f"✓ Results saved to {output_path}")
    print(f"✓ Excel file includes grouping and formatting")

# ==================== MAIN EXECUTION ====================

def test_openai_connection():
    """Test OpenAI API connection"""
    print("\n🔍 Testing OpenAI API connection...")
    
    if not OPENAI_API_KEY:
        print("❌ OPENAI_API_KEY is not set!")
        return False
    
    print(f"✓ API Key is set (length: {len(OPENAI_API_KEY)})")
    print(f"✓ Base URL: {OPENAI_BASE_URL}")
    print(f"✓ Embedding Model: {EMBEDDING_MODEL}")
    print(f"✓ Embedding Dimensions: {EMBEDDING_DIMENSIONS}")
    
    try:
        print("\n🧪 Testing embedding with a sample text...")
        test_response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=["test connection"],
            dimensions=EMBEDDING_DIMENSIONS
        )
        
        if test_response.data and len(test_response.data[0].embedding) == EMBEDDING_DIMENSIONS:
            print(f"✅ Successfully created test embedding!")
            print(f"✓ Embedding dimension: {len(test_response.data[0].embedding)}")
            return True
        else:
            print(f"❌ Test embedding failed - unexpected response format")
            return False
            
    except Exception as e:
        print(f"❌ Connection test failed!")
        print(f"Error: {str(e)}")
        return False

def main():
    """Main execution function"""
    global gdc_master_vectorstore, gdc_context_vectorstore
    
    print("=" * 80)
    print("GDC RECORD CLASS MAPPING SYSTEM - FIXED VERSION")
    print("=" * 80)
    print("Features:")
    print("  • RAG with OpenAI text-embedding-3-large")
    print("  • One-to-Many GDC Mappings")
    print("  • LangGraph ReAct Agents")
    print("  • Dynamic Chain of Thought")
    print("  • Mixture of Experts")
    print("  • Comprehensive Prompts")
    print("  • Excel Output with Grouping")
    print("=" * 80)
    
    if not OPENAI_API_KEY:
        print("\n❌ ERROR: OPENAI_API_KEY environment variable not set")
        print("\nPlease set it using:")
        print("  export OPENAI_API_KEY='your-api-key'")
        return
    
    # Test connection
    if not test_openai_connection():
        print("\n❌ OpenAI connection test failed. Please fix the issues above.")
        return
    
    print("\n📁 Loading data files...")
    gdc_master = load_json_file("GDC_master.json", GDCMaster)
    gdc_context = load_json_file("GDC_with_context.json", GDCWithContext)
    validation_set = load_json_file("GDC_MSS_ILM.json", ValidationEntry)
    record_classes = load_json_file("Record_Classes.json", RecordClass)
    
    print(f"✓ Loaded {len(gdc_master)} GDC master entries")
    print(f"✓ Loaded {len(gdc_context)} GDC context entries")
    print(f"✓ Loaded {len(validation_set)} validation entries")
    print(f"✓ Loaded {len(record_classes)} record classes")
    
    if not all([gdc_master, gdc_context, validation_set, record_classes]):
        print("\n❌ ERROR: Failed to load required data files")
        return
    
    print("\n🔍 Building RAG vector stores...")
    gdc_master_vectorstore = build_gdc_master_vectorstore(gdc_master)
    gdc_context_vectorstore = build_gdc_context_vectorstore(gdc_context)
    
    print("\n🤖 Creating ReAct agent with mixture of experts...")
    agent = create_react_agent_workflow()
    print(f"✓ Model: {OPENAI_MODEL}")
    print(f"✓ Reasoning: {REASONING_EFFORT}")
    print(f"✓ Embeddings: {EMBEDDING_MODEL} ({EMBEDDING_DIMENSIONS}d)")
    print(f"✓ Mode: One-to-Many with RAG")
    print(f"✓ Experts: Semantic, Context, Validation, Final Decision")
    
    print("\n🚀 Starting GDC mapping process...\n")
    all_results = []
    
    for i, record in enumerate(record_classes, 1):
        print(f"\n{'='*80}")
        print(f"RECORD {i}/{len(record_classes)}: {record.name}")
        print(f"{'='*80}")
        
        try:
            record_mappings = process_single_record(
                agent=agent,
                record=record,
                validation_set=validation_set
            )
            
            all_results.extend(record_mappings)
            gdc_names = [m.gdc_name for m in record_mappings]
            print(f"\n✓ Mapped to {len(record_mappings)} GDC(s): {', '.join(gdc_names)}")
            
        except Exception as e:
            print(f"\n✗ ERROR processing record: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")
            
            all_results.append(MappingResult(
                guid=record.guid,
                code=record.code,
                name=record.name,
                description=record.description,
                gdc_name="error",
                gdc_description="",
                mapping_rank=1,
                reasoning=f"Processing error: {str(e)}"
            ))
    
    print("\n" + "="*80)
    print("💾 Saving results...")
    save_to_excel_with_grouping(all_results, "GDC_Mapping_Results.xlsx")
    
    print("\n📊 FINAL SUMMARY")
    print("="*80)
    print(f"Total Records Processed: {len(record_classes)}")
    print(f"Total GDC Mappings Created: {len(all_results)}")
    if len(record_classes) > 0:
        avg_mappings = len(all_results) / len(record_classes)
        print(f"Average Mappings per Record: {avg_mappings:.2f}")
    
    # Count mapping distribution
    mapping_dist = {}
    for result in all_results:
        count = sum(1 for r in all_results if r.name == result.name)
        mapping_dist[count] = mapping_dist.get(count, 0) + 1
    
    print(f"\nMapping Distribution:")
    for count in sorted(mapping_dist.keys()):
        num_records = mapping_dist[count] // count  # Each record counted 'count' times
        if num_records > 0:
            print(f"  {count} GDC(s): {num_records} record(s)")
    
    print("="*80)
    print("✅ Process completed successfully!")
    print(f"📄 Results saved to: GDC_Mapping_Results.xlsx")

if __name__ == "__main__":
    main()
