import json
import csv
import sys
import argparse
from typing import Any, Dict, List, Set, Union
from pathlib import Path
import pandas as pd
from collections import defaultdict
import re

class GraphJSONToCSVConverter:
    def __init__(self):
        self.nodes = []
        self.edges = []
        self.node_properties = set()
        self.edge_properties = set()
        self.all_data = []
        
    def flatten_dict(self, d: Dict, parent_key: str = '', sep: str = '_') -> Dict:
        """Recursively flatten a nested dictionary."""
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            
            if isinstance(v, dict):
                items.extend(self.flatten_dict(v, new_key, sep=sep).items())
            elif isinstance(v, list):
                # Handle lists by either joining strings or creating indexed fields
                if all(isinstance(item, (str, int, float, bool)) for item in v):
                    # Simple list - join with semicolon
                    items.append((new_key, '; '.join(str(item) for item in v)))
                else:
                    # Complex list - create indexed fields
                    for i, item in enumerate(v):
                        if isinstance(item, dict):
                            items.extend(self.flatten_dict(item, f"{new_key}_{i}", sep=sep).items())
                        else:
                            items.append((f"{new_key}_{i}", str(item)))
            else:
                items.append((new_key, v))
        
        return dict(items)
    
    def clean_column_name(self, name: str) -> str:
        """Clean column names for CSV compatibility."""
        # Replace special characters with underscores
        name = re.sub(r'[^\w\s-]', '_', str(name))
        # Replace spaces and multiple underscores with single underscore
        name = re.sub(r'[\s_]+', '_', name)
        # Remove leading/trailing underscores
        name = name.strip('_')
        return name
    
    def extract_nodes_and_edges(self, data: Any, path: str = "root") -> None:
        """Extract nodes and edges from the graph data."""
        if isinstance(data, dict):
            # Check if this looks like a node
            if self.is_node(data):
                flattened = self.flatten_dict(data)
                flattened['_source_path'] = path
                flattened['_entity_type'] = 'node'
                self.nodes.append(flattened)
                self.node_properties.update(flattened.keys())
            
            # Check if this looks like an edge/relationship
            elif self.is_edge(data):
                flattened = self.flatten_dict(data)
                flattened['_source_path'] = path
                flattened['_entity_type'] = 'edge'
                self.edges.append(flattened)
                self.edge_properties.update(flattened.keys())
            
            # Recursively process nested structures
            for key, value in data.items():
                if key.lower() in ['nodes', 'vertices', 'entities']:
                    if isinstance(value, list):
                        for i, item in enumerate(value):
                            self.extract_nodes_and_edges(item, f"{path}.{key}[{i}]")
                    else:
                        self.extract_nodes_and_edges(value, f"{path}.{key}")
                
                elif key.lower() in ['edges', 'relationships', 'links', 'connections']:
                    if isinstance(value, list):
                        for i, item in enumerate(value):
                            self.extract_nodes_and_edges(item, f"{path}.{key}[{i}]")
                    else:
                        self.extract_nodes_and_edges(value, f"{path}.{key}")
                
                elif isinstance(value, (dict, list)):
                    self.extract_nodes_and_edges(value, f"{path}.{key}")
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                self.extract_nodes_and_edges(item, f"{path}[{i}]")
    
    def is_node(self, data: Dict) -> bool:
        """Heuristic to determine if a dictionary represents a node."""
        node_indicators = [
            'id', 'nodeId', 'node_id', 'guid', 'uuid',
            'name', 'label', 'title',
            'type', 'nodeType', 'node_type', 'category',
            'properties', 'attributes', 'metadata'
        ]
        
        # Check for common node fields
        keys_lower = [k.lower() for k in data.keys()]
        has_node_indicators = any(indicator in keys_lower for indicator in node_indicators)
        
        # Check if it has edge-specific fields (if so, it's probably not a node)
        edge_indicators = ['source', 'target', 'from', 'to', 'fromNode', 'toNode']
        has_edge_indicators = any(indicator in keys_lower for indicator in edge_indicators)
        
        return has_node_indicators and not has_edge_indicators
    
    def is_edge(self, data: Dict) -> bool:
        """Heuristic to determine if a dictionary represents an edge."""
        edge_indicators = [
            'source', 'target', 'from', 'to',
            'fromNode', 'toNode', 'from_node', 'to_node',
            'sourceId', 'targetId', 'source_id', 'target_id',
            'relationship', 'relationshipType', 'edgeType'
        ]
        
        keys_lower = [k.lower() for k in data.keys()]
        return any(indicator in keys_lower for indicator in edge_indicators)
    
    def extract_flat_records(self, data: Any, prefix: str = "") -> List[Dict]:
        """Extract all records as flat dictionaries (alternative approach)."""
        records = []
        
        if isinstance(data, dict):
            # Flatten this dictionary
            flattened = self.flatten_dict(data)
            if flattened:  # Only add non-empty records
                flattened['_record_path'] = prefix if prefix else 'root'
                records.append(flattened)
            
            # Process nested structures
            for key, value in data.items():
                new_prefix = f"{prefix}.{key}" if prefix else key
                if isinstance(value, (dict, list)):
                    records.extend(self.extract_flat_records(value, new_prefix))
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                new_prefix = f"{prefix}[{i}]" if prefix else f"item_{i}"
                if isinstance(item, dict):
                    records.extend(self.extract_flat_records(item, new_prefix))
                else:
                    # Handle primitive values in lists
                    record = {'value': item, '_record_path': new_prefix}
                    records.append(record)
        
        return records
    
    def save_to_csv(self, data: List[Dict], filename: str, encoding: str = 'utf-8') -> None:
        """Save data to CSV file."""
        if not data:
            print(f"No data to save for {filename}")
            return
        
        # Get all unique columns
        all_columns = set()
        for record in data:
            all_columns.update(record.keys())
        
        # Sort columns for consistent output
        columns = sorted([self.clean_column_name(col) for col in all_columns])
        
        # Create clean data with consistent column names
        clean_data = []
        for record in data:
            clean_record = {}
            for col in all_columns:
                clean_col_name = self.clean_column_name(col)
                value = record.get(col, '')
                # Handle None values
                if value is None:
                    value = ''
                # Convert complex types to strings
                elif isinstance(value, (list, dict)):
                    value = json.dumps(value)
                clean_record[clean_col_name] = value
            clean_data.append(clean_record)
        
        # Write to CSV
        with open(filename, 'w', newline='', encoding=encoding) as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=columns)
            writer.writeheader()
            for record in clean_data:
                writer.writerow(record)
        
        print(f"Saved {len(clean_data)} records to {filename}")
    
    def convert_file(self, input_file: str, output_dir: str = None, 
                    strategy: str = 'auto', encoding: str = 'utf-8') -> None:
        """Convert JSON file to CSV(s)."""
        if output_dir is None:
            output_dir = Path(input_file).parent
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # Load JSON data
        try:
            with open(input_file, 'r', encoding=encoding) as f:
                data = json.load(f)
        except Exception as e:
            print(f"Error loading JSON file: {e}")
            return
        
        base_name = Path(input_file).stem
        
        if strategy == 'graph':
            # Extract nodes and edges separately
            print("Extracting nodes and edges...")
            self.extract_nodes_and_edges(data)
            
            if self.nodes:
                nodes_file = output_dir / f"{base_name}_nodes.csv"
                self.save_to_csv(self.nodes, nodes_file, encoding)
            
            if self.edges:
                edges_file = output_dir / f"{base_name}_edges.csv"
                self.save_to_csv(self.edges, edges_file, encoding)
            
            if not self.nodes and not self.edges:
                print("No nodes or edges detected. Falling back to flat extraction.")
                strategy = 'flat'
        
        if strategy == 'flat' or strategy == 'auto':
            # Extract all records as flat dictionaries
            print("Extracting flat records...")
            records = self.extract_flat_records(data)
            
            if records:
                flat_file = output_dir / f"{base_name}_flat.csv"
                self.save_to_csv(records, flat_file, encoding)
        
        if strategy == 'pandas':
            # Use pandas json_normalize (good for semi-structured data)
            print("Using pandas normalization...")
            try:
                if isinstance(data, list):
                    df = pd.json_normalize(data)
                else:
                    df = pd.json_normalize([data])
                
                pandas_file = output_dir / f"{base_name}_normalized.csv"
                df.to_csv(pandas_file, index=False, encoding=encoding)
                print(f"Saved {len(df)} records to {pandas_file}")
            except Exception as e:
                print(f"Pandas normalization failed: {e}")
    
    def analyze_structure(self, input_file: str) -> None:
        """Analyze the JSON structure to recommend conversion strategy."""
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"Error loading JSON file: {e}")
            return
        
        print("Analyzing JSON structure...")
        
        # Basic structure info
        def analyze_value(value, path="root", depth=0):
            if depth > 10:  # Prevent infinite recursion
                return
            
            if isinstance(value, dict):
                print(f"  {'  ' * depth}Object at {path}: {len(value)} keys")
                if len(value) <= 5:  # Show keys for small objects
                    print(f"  {'  ' * depth}  Keys: {list(value.keys())}")
                
                # Check if it looks like a node or edge
                if self.is_node(value):
                    print(f"  {'  ' * depth}  -> Looks like a NODE")
                elif self.is_edge(value):
                    print(f"  {'  ' * depth}  -> Looks like an EDGE")
                
                for key, val in value.items():
                    if isinstance(val, (dict, list)):
                        analyze_value(val, f"{path}.{key}", depth + 1)
            
            elif isinstance(value, list):
                print(f"  {'  ' * depth}Array at {path}: {len(value)} items")
                if value and len(value) <= 3:  # Analyze first few items
                    for i, item in enumerate(value[:3]):
                        analyze_value(item, f"{path}[{i}]", depth + 1)
        
        analyze_value(data)
        
        print("\nRecommended strategies:")
        print("- 'graph': For node/edge graph data")
        print("- 'flat': For general nested JSON")
        print("- 'pandas': For semi-structured data")
        print("- 'auto': Try graph first, fallback to flat")

def main():
    parser = argparse.ArgumentParser(description="Convert complex JSON (especially graph data) to CSV")
    parser.add_argument("input_file", help="Input JSON file")
    parser.add_argument("--output-dir", "-o", help="Output directory (default: same as input)")
    parser.add_argument("--strategy", "-s", choices=['graph', 'flat', 'pandas', 'auto'], 
                       default='auto', help="Conversion strategy")
    parser.add_argument("--analyze", "-a", action="store_true", 
                       help="Analyze structure without converting")
    parser.add_argument("--encoding", "-e", default="utf-8", help="File encoding")
    
    args = parser.parse_args()
    
    converter = GraphJSONToCSVConverter()
    
    if args.analyze:
        converter.analyze_structure(args.input_file)
    else:
        converter.convert_file(args.input_file, args.output_dir, args.strategy, args.encoding)

if __name__ == "__main__":
    if len(sys.argv) == 1:
        print("Solidatus Graph JSON to CSV Converter")
        print("\nUsage examples:")
        print("  python graph_json_to_csv.py data.json")
        print("  python graph_json_to_csv.py data.json --strategy graph")
        print("  python graph_json_to_csv.py data.json --analyze")
        print("  python graph_json_to_csv.py data.json -o output_folder -s flat")
        sys.exit(0)
    
    main()
