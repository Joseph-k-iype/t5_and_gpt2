#!/usr/bin/env python3
"""
Enhanced Multi-Agent Legal Document Rule Extraction System
Converts PDF regulations into machine-readable JSON rules compatible with json-rules-engine
No truncation, no hardcoding, full LLM-driven extraction with knowledge graph state management
"""

import os
import json
import csv
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import re
import uuid
from functools import wraps

# Core libraries
import pandas as pd
import PyPDF2
from pydantic import BaseModel, Field, validator
import openai
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

# Global Configuration
API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
BASE_URL = "https://api.openai.com/v1"
MODEL_NAME = "gpt-4.1-2025-04-14"
EMBEDDING_MODEL = "text-embedding-3-large"
CHUNK_SIZE = 4000
OVERLAP_SIZE = 200

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Async decorator to ensure proper coroutine handling
def async_method(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)
        if asyncio.iscoroutine(result):
            return await result
        return result
    return wrapper

# Enhanced Pydantic Models
class KnowledgeGraph(BaseModel):
    """Knowledge graph for agent state management"""
    entities: Dict[str, Any] = Field(default_factory=dict, description="Extracted entities")
    relationships: Dict[str, List[str]] = Field(default_factory=dict, description="Entity relationships")
    jurisdictions: List[str] = Field(default_factory=list, description="Discovered jurisdictions")
    adequacy_countries: List[str] = Field(default_factory=list, description="Countries with adequacy decisions")
    legal_actors: List[str] = Field(default_factory=list, description="Legal actors mentioned")
    data_categories: List[str] = Field(default_factory=list, description="Data categories identified")
    obligations: List[Dict[str, Any]] = Field(default_factory=list, description="Legal obligations")
    conditions: List[Dict[str, Any]] = Field(default_factory=list, description="Conditional statements")
    temporal_requirements: List[Dict[str, Any]] = Field(default_factory=list, description="Time-based requirements")
    context: Dict[str, Any] = Field(default_factory=dict, description="Contextual information")

class DocumentSection(BaseModel):
    """Model for document sections with full content preservation"""
    section_type: str = Field(..., description="Type of section (level-1, level-2, level-3)")
    title: str = Field(..., description="Section title")
    content: str = Field(..., description="Complete section content - no truncation")
    regulation_name: Optional[str] = Field(None, description="Name of the regulation")
    knowledge_graph: KnowledgeGraph = Field(default_factory=KnowledgeGraph, description="Section knowledge graph")

class CleanedDocument(BaseModel):
    """Model for cleaned document structure with knowledge graph state"""
    document_id: str = Field(..., description="Unique document identifier")
    sections: List[DocumentSection] = Field(..., description="Document sections with full content")
    global_knowledge_graph: KnowledgeGraph = Field(default_factory=KnowledgeGraph, description="Document-wide knowledge graph")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Processing metadata")

class LogicalStatement(BaseModel):
    """Model for simplified logical statements with context"""
    statement_id: str = Field(..., description="Unique statement identifier")
    original_text: str = Field(..., description="Original complete text - no truncation")
    simplified_text: str = Field(..., description="Simplified logical statement in plain English")
    section_reference: str = Field(..., description="Reference to source section")
    knowledge_graph: KnowledgeGraph = Field(default_factory=KnowledgeGraph, description="Statement knowledge graph")
    context: Dict[str, Any] = Field(default_factory=dict, description="Contextual information for preventing hallucination")

class RuleCondition(BaseModel):
    """Model for rule conditions compatible with json-rules-engine"""
    fact: str = Field(..., description="The fact to evaluate")
    operator: str = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    path: Optional[str] = Field(None, description="JSON path for nested objects")

class RuleEvent(BaseModel):
    """Model for rule events in plain English"""
    type: str = Field(..., description="Event type (require, forbid, allow)")
    params: Dict[str, Any] = Field(default_factory=dict, description="Event parameters in simple language")

class ExtractedRule(BaseModel):
    """Model for extracted rules without hardcoded values"""
    rule_id: str = Field(..., description="Unique rule identifier")
    rule_text: str = Field(..., description="Complete human-readable rule description in plain English")
    applies_to_countries: List[str] = Field(default_factory=list, description="Country ISO codes discovered from document")
    adequacy_countries: List[str] = Field(default_factory=list, description="Adequacy countries discovered from document")
    conditions: Dict[str, List[RuleCondition]] = Field(..., description="Rule conditions (all/any/not)")
    event: RuleEvent = Field(..., description="Event to trigger when conditions are met")
    roles: List[str] = Field(default_factory=list, description="Applicable roles discovered from document")
    data_categories: List[str] = Field(default_factory=list, description="Data categories discovered from document")
    domain: str = Field(..., description="Legal domain discovered from document")
    knowledge_graph: KnowledgeGraph = Field(default_factory=KnowledgeGraph, description="Rule knowledge graph")
    context: Dict[str, Any] = Field(default_factory=dict, description="Full context to prevent hallucination")

class ProcessedDocuments(BaseModel):
    """Model for final processed documents with complete knowledge graph"""
    document_id: str = Field(..., description="Document identifier")
    rules: List[ExtractedRule] = Field(..., description="Extracted rules with full context")
    global_knowledge_graph: KnowledgeGraph = Field(default_factory=KnowledgeGraph, description="Complete document knowledge graph")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Processing metadata")

class MetadataConfig(BaseModel):
    """Model for metadata configuration - minimal initial config"""
    pdf_path: str = Field(..., description="Path to PDF file")
    document_type: Optional[str] = Field("regulation", description="Type of document")

# Geography Data Handler - Enhanced to work with discovered jurisdictions
class GeographyHandler:
    """Handles geography data and country code mappings"""
    
    def __init__(self, geography_file: str):
        self.geography_data = self._load_geography_data(geography_file)
        self.country_code_map = self._build_country_code_map()
        self.jurisdiction_patterns = self._build_jurisdiction_patterns()
    
    def _load_geography_data(self, file_path: str) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load geography data: {e}")
            return {}
    
    def _build_country_code_map(self) -> Dict[str, str]:
        """Build a mapping from country names to ISO codes"""
        country_map = {}
        
        # Process all geographical regions
        for region_key, region_data in self.geography_data.items():
            if isinstance(region_data, dict):
                # Handle direct country lists
                if 'countries' in region_data:
                    for country in region_data['countries']:
                        country_map[country['name'].lower()] = country['iso2']
                
                # Handle nested continent structure
                if region_key == 'By_Continent':
                    for continent, continent_data in region_data.items():
                        for country in continent_data.get('countries', []):
                            country_map[country['name'].lower()] = country['iso2']
        
        return country_map
    
    def _build_jurisdiction_patterns(self) -> Dict[str, List[str]]:
        """Build patterns to identify jurisdictions in text"""
        patterns = {}
        
        # Add EU patterns
        patterns['EU'] = ['european union', 'eu', 'gdpr', 'european economic area']
        patterns['US'] = ['united states', 'usa', 'america', 'ccpa', 'california']
        patterns['UK'] = ['united kingdom', 'uk', 'britain', 'great britain', 'england']
        patterns['CA'] = ['canada', 'canadian', 'pipeda']
        
        return patterns
    
    def get_country_code(self, country_name: str) -> Optional[str]:
        """Get ISO country code for a country name"""
        return self.country_code_map.get(country_name.lower())
    
    def discover_jurisdictions_from_text(self, text: str) -> List[str]:
        """Discover jurisdictions mentioned in text"""
        text_lower = text.lower()
        found_jurisdictions = []
        
        for jurisdiction, patterns in self.jurisdiction_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    found_jurisdictions.append(jurisdiction)
                    break
        
        # Also look for country names directly
        for country_name, iso_code in self.country_code_map.items():
            if country_name in text_lower:
                found_jurisdictions.append(iso_code)
        
        return list(set(found_jurisdictions))

# Enhanced PDF Processing
class PDFProcessor:
    """Handles PDF text extraction with complete content preservation"""
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str) -> str:
        """Extract complete text content from PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text += f"\n[PAGE {page_num + 1}]\n{page_text}\n"
                return text
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            return ""
    
    @staticmethod
    def chunk_text_with_overlap(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP_SIZE) -> List[Dict[str, Any]]:
        """Split text into overlapping chunks with metadata"""
        if not text:
            return []
        
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)
            
            chunks.append({
                'text': chunk_text,
                'start_index': i,
                'end_index': min(i + chunk_size, len(words)),
                'chunk_id': f"chunk_{i}_{uuid.uuid4().hex[:8]}"
            })
            
            if i + chunk_size >= len(words):
                break
        
        return chunks

# Enhanced OpenAI Client with proper async handling and correct API methods
class OpenAIClient:
    """Enhanced OpenAI API wrapper with proper async support and correct API methods"""
    
    def __init__(self):
        # Initialize OpenAI client with proper configuration
        from openai import AsyncOpenAI
        self.client = AsyncOpenAI(
            api_key=API_KEY,
            base_url=BASE_URL
        )
    
    @retry(
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def get_completion(self, messages: List[Dict[str, str]], system_prompt: str = "") -> str:
        """Get completion from OpenAI API with proper async handling"""
        try:
            if system_prompt:
                messages = [{"role": "system", "content": system_prompt}] + messages
            
            # Use the correct async method
            response = await self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise
    
    @retry(
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings from OpenAI API with proper async handling"""
        try:
            response = await self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=texts
            )
            
            return [embedding.embedding for embedding in response.data]
            
        except Exception as e:
            logger.error(f"OpenAI embeddings error: {e}")
            raise

# Enhanced Multi-Agent System with Knowledge Graph State Management

class Agent1DocumentCleaner:
    """Agent 1: Clean and structure documents with complete content preservation"""
    
    def __init__(self, openai_client: OpenAIClient, geography_handler: GeographyHandler):
        self.client = openai_client
        self.geography = geography_handler
    
    @async_method
    async def clean_and_structure(self, raw_text: str, document_metadata: MetadataConfig) -> CleanedDocument:
        """Clean and structure the raw PDF text with knowledge graph creation"""
        
        system_prompt = """You are an expert legal document processing agent. Your task is to clean and structure legal documents without losing ANY information and create comprehensive knowledge graphs.

CRITICAL REQUIREMENTS:
- NEVER truncate or summarize any content
- Preserve ALL legal text, definitions, references, and citations
- Extract ALL jurisdictions, countries, and legal frameworks mentioned
- Build comprehensive knowledge graphs of entities and relationships
- Identify all adequacy decisions and data protection frameworks mentioned

REASONING APPROACH:
1. Analyze complete document structure and identify ALL section patterns
2. Create comprehensive knowledge graph of document relationships, entities, and legal frameworks
3. Extract and organize content by regulatory levels while preserving EVERYTHING
4. Identify ALL jurisdictions, countries, adequacy decisions mentioned in the text
5. Map all legal actors, data categories, and obligations
6. Preserve exact legal language and terminology

DOCUMENT STRUCTURE TO IDENTIFY:
- Level-1-Regulation-{name}: Core regulatory requirements
- Level-2-Regulator-Guidance: Official interpretation and guidance  
- Level-3-Supporting-Information: Additional context and definitions

KNOWLEDGE GRAPH EXTRACTION:
- All countries and jurisdictions mentioned
- All adequacy decisions and frameworks (e.g., "US has adequacy decision")
- All legal actors (controllers, processors, data subjects, etc.)
- All data categories mentioned
- All obligations and requirements
- All temporal requirements (timeframes, deadlines)
- All relationships between entities

OUTPUT FORMAT: Return structured JSON with complete sections and comprehensive knowledge graph."""

        # Process in chunks but maintain complete content
        chunks = PDFProcessor.chunk_text_with_overlap(raw_text)
        global_kg = KnowledgeGraph()
        all_sections = []
        
        for chunk in chunks:
            user_prompt = f"""Analyze this legal document chunk and extract ALL information without any truncation:

DOCUMENT METADATA:
- Document Type: {document_metadata.document_type}
- Source: {document_metadata.pdf_path}

DOCUMENT CHUNK:
{chunk['text']}

REASONING STEPS:
1. Complete document structure analysis
2. Comprehensive knowledge graph creation with ALL entities mentioned:
   - ALL countries and jurisdictions found in text
   - ALL adequacy decisions mentioned (e.g., "Country X has adequacy decision")
   - ALL legal actors identified
   - ALL data categories mentioned
   - ALL obligations and conditions
   - ALL temporal requirements
3. Content organization strategy preserving EVERYTHING
4. Relationship mapping between ALL entities found

Extract ALL sections with COMPLETE content. Build comprehensive knowledge graph of ALL entities, relationships, jurisdictions, adequacy countries, legal actors, data categories, obligations, and temporal requirements found in this text.

CRITICAL: Do not summarize, truncate, or omit ANY information. Extract EVERYTHING mentioned."""

            messages = [{"role": "user", "content": user_prompt}]
            
            try:
                response = await self.client.get_completion(messages, system_prompt)
                
                # Parse response and extract sections with knowledge graph
                chunk_sections, chunk_kg = self._parse_response_with_kg(response, chunk)
                all_sections.extend(chunk_sections)
                
                # Merge knowledge graphs
                global_kg = self._merge_knowledge_graphs(global_kg, chunk_kg)
                
            except Exception as e:
                logger.error(f"Document cleaning failed for chunk {chunk['chunk_id']}: {e}")
                continue
        
        # Discover additional jurisdictions from full text
        discovered_jurisdictions = self.geography.discover_jurisdictions_from_text(raw_text)
        global_kg.jurisdictions.extend(discovered_jurisdictions)
        global_kg.jurisdictions = list(set(global_kg.jurisdictions))  # Remove duplicates
        
        return CleanedDocument(
            document_id=str(uuid.uuid4()),
            sections=all_sections,
            global_knowledge_graph=global_kg,
            metadata={
                "processing_timestamp": datetime.now().isoformat(),
                "original_length": len(raw_text),
                "agent": "DocumentCleaner",
                "chunks_processed": len(chunks),
                "discovered_jurisdictions": discovered_jurisdictions
            }
        )
    
    def _parse_response_with_kg(self, response: str, chunk: Dict[str, Any]) -> tuple[List[DocumentSection], KnowledgeGraph]:
        """Parse AI response into sections with knowledge graph"""
        sections = []
        kg = KnowledgeGraph()
        
        try:
            # Try to parse JSON response
            if '{' in response and '}' in response:
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    data = json.loads(json_match.group())
                    
                    # Extract sections
                    if 'sections' in data:
                        for section_data in data['sections']:
                            sections.append(DocumentSection(**section_data))
                    
                    # Extract knowledge graph
                    if 'knowledge_graph' in data:
                        kg = KnowledgeGraph(**data['knowledge_graph'])
        
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Failed to parse structured response: {e}")
            
            # Fallback: extract from text patterns
            sections = self._extract_sections_from_text(response, chunk)
            kg = self._extract_kg_from_text(response)
        
        return sections, kg
    
    def _extract_sections_from_text(self, response: str, chunk: Dict[str, Any]) -> List[DocumentSection]:
        """Extract sections from unstructured response text"""
        sections = []
        
        # Pattern matching for levels
        level_patterns = [
            (r"Level-1-Regulation-([^:]+):(.*?)(?=Level-[123]|$)", "level-1"),
            (r"Level-2-Regulator-Guidance:(.*?)(?=Level-[123]|$)", "level-2"),
            (r"Level-3-Supporting-Information:(.*?)(?=Level-[123]|$)", "level-3")
        ]
        
        for pattern, level in level_patterns:
            matches = re.finditer(pattern, response, re.DOTALL | re.IGNORECASE)
            for match in matches:
                if level == "level-1":
                    reg_name = match.group(1).strip()
                    content = match.group(2).strip()
                    sections.append(DocumentSection(
                        section_type=level,
                        title=f"Level-1-Regulation-{reg_name}",
                        content=content,
                        regulation_name=reg_name,
                        knowledge_graph=KnowledgeGraph()
                    ))
                else:
                    content = match.group(1).strip()
                    sections.append(DocumentSection(
                        section_type=level,
                        title=f"Level-{level.split('-')[1]}-{'Regulator-Guidance' if level == 'level-2' else 'Supporting-Information'}",
                        content=content,
                        knowledge_graph=KnowledgeGraph()
                    ))
        
        # If no structured sections found, create from chunk
        if not sections:
            sections.append(DocumentSection(
                section_type="general",
                title=f"Document Section - {chunk['chunk_id']}",
                content=chunk['text'],
                knowledge_graph=KnowledgeGraph()
            ))
        
        return sections
    
    def _extract_kg_from_text(self, response: str) -> KnowledgeGraph:
        """Extract knowledge graph from unstructured response"""
        kg = KnowledgeGraph()
        
        # Extract entities using patterns
        text_lower = response.lower()
        
        # Extract jurisdictions
        jurisdiction_patterns = [
            r'jurisdiction[s]?[:\s]*([^.]+)',
            r'applies?\s+to[:\s]*([^.]+)',
            r'in\s+(?:the\s+)?(eu|united\s+states?|uk|canada|germany|france)'
        ]
        
        for pattern in jurisdiction_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                jurisdictions = re.findall(r'\b[A-Z]{2,}\b|\b(?:EU|US|UK|CA)\b', match.upper())
                kg.jurisdictions.extend(jurisdictions)
        
        # Extract adequacy countries
        adequacy_patterns = [
            r'adequacy\s+decision[s]?[^.]*?([A-Z]{2,}(?:\s*,\s*[A-Z]{2,})*)',
            r'adequate\s+protection[^.]*?([A-Z]{2,}(?:\s*,\s*[A-Z]{2,})*)',
            r'([A-Z]{2,})[^.]*?adequacy'
        ]
        
        for pattern in adequacy_patterns:
            matches = re.findall(pattern, response)
            for match in matches:
                countries = re.findall(r'\b[A-Z]{2,}\b', match)
                kg.adequacy_countries.extend(countries)
        
        # Remove duplicates
        kg.jurisdictions = list(set(kg.jurisdictions))
        kg.adequacy_countries = list(set(kg.adequacy_countries))
        
        return kg
    
    def _merge_knowledge_graphs(self, kg1: KnowledgeGraph, kg2: KnowledgeGraph) -> KnowledgeGraph:
        """Merge two knowledge graphs"""
        merged = KnowledgeGraph()
        
        # Merge all lists
        merged.jurisdictions = list(set(kg1.jurisdictions + kg2.jurisdictions))
        merged.adequacy_countries = list(set(kg1.adequacy_countries + kg2.adequacy_countries))
        merged.legal_actors = list(set(kg1.legal_actors + kg2.legal_actors))
        merged.data_categories = list(set(kg1.data_categories + kg2.data_categories))
        merged.obligations.extend(kg1.obligations + kg2.obligations)
        merged.conditions.extend(kg1.conditions + kg2.conditions)
        merged.temporal_requirements.extend(kg1.temporal_requirements + kg2.temporal_requirements)
        
        # Merge dictionaries
        merged.entities.update(kg1.entities)
        merged.entities.update(kg2.entities)
        merged.relationships.update(kg1.relationships)
        merged.relationships.update(kg2.relationships)
        merged.context.update(kg1.context)
        merged.context.update(kg2.context)
        
        return merged

class Agent2SemanticSegmentation:
    """Agent 2: Semantic segmentation with knowledge graph state management"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
    
    @async_method
    async def segment_and_simplify(self, cleaned_doc: CleanedDocument) -> List[LogicalStatement]:
        """Convert complex legal text into simple logical statements with knowledge graph context"""
        
        system_prompt = """You are an expert legal language analyst specializing in converting complex legal text into simple, machine-processable logical statements while maintaining knowledge graph context.

CRITICAL REQUIREMENTS:
- NEVER truncate or lose any legal meaning
- Use knowledge graph context to prevent hallucination
- Convert to simple English without legal jargon
- Preserve all logical relationships and conditions
- Extract ALL conditional statements (if-then logic)
- Identify ALL obligations, permissions, and prohibitions

REASONING APPROACH:
1. Use provided knowledge graph context to understand document scope
2. Parse complex legal sentences into atomic logical components
3. Create knowledge graphs of legal relationships
4. Identify ALL conditional statements (if-then logic)
5. Extract ALL obligations, permissions, and prohibitions
6. Simplify language while preserving ALL legal meaning

FOCUS AREAS FOR ACCESS & ENTITLEMENTS:
- Data subject rights and access requests
- Controller and processor obligations
- Consent requirements and conditions
- Data transfer restrictions and safeguards
- Breach notification requirements
- Retention and deletion obligations

SIMPLIFICATION STRATEGY:
- Convert "shall" and "must" to clear obligations in plain English
- Extract ALL conditional logic (if X then Y)
- Identify ALL actors (controller, processor, data subject)
- Clarify ALL temporal requirements (within X days)
- Specify ALL data categories and processing purposes
- Use simple English instead of legal references

OUTPUT: Atomic logical statements in simple English that preserve ALL legal intent."""

        statements = []
        
        for section in cleaned_doc.sections:
            user_prompt = f"""Using the knowledge graph context, convert this legal text into simple logical statements in plain English:

KNOWLEDGE GRAPH CONTEXT:
- Jurisdictions: {cleaned_doc.global_knowledge_graph.jurisdictions}
- Adequacy Countries: {cleaned_doc.global_knowledge_graph.adequacy_countries}
- Legal Actors: {cleaned_doc.global_knowledge_graph.legal_actors}
- Data Categories: {cleaned_doc.global_knowledge_graph.data_categories}
- Known Obligations: {json.dumps(cleaned_doc.global_knowledge_graph.obligations[:5], indent=2)}

SECTION: {section.title}
SECTION TYPE: {section.section_type}

COMPLETE TEXT (NO TRUNCATION):
{section.content}

REASONING STEPS:
1. Use knowledge graph context to understand scope and prevent hallucination
2. Identify ALL legal actors and roles mentioned
3. Extract ALL conditional logic patterns
4. Map ALL obligations, permissions, prohibitions
5. Create knowledge graph of relationships
6. Generate atomic logical statements in simple English

Focus on access rights, data processing rules, consent requirements, and compliance obligations.

CRITICAL: Extract ALL logical components without truncation. Use simple English instead of legal jargon or article references. Each statement should be understandable without legal training."""

            messages = [{"role": "user", "content": user_prompt}]
            
            try:
                response = await self.client.get_completion(messages, system_prompt)
                section_statements = self._parse_logical_statements(response, section, cleaned_doc.global_knowledge_graph)
                statements.extend(section_statements)
                
            except Exception as e:
                logger.error(f"Semantic segmentation failed for section {section.title}: {e}")
                continue
        
        return statements
    
    def _parse_logical_statements(self, response: str, section: DocumentSection, global_kg: KnowledgeGraph) -> List[LogicalStatement]:
        """Parse AI response into logical statements with context"""
        statements = []
        
        # Split response into individual statements
        statement_blocks = re.split(r'\n\s*[-*•]\s*|\n\s*\d+\.\s*|\n\s*\n', response)
        
        for i, block in enumerate(statement_blocks):
            if block.strip():
                # Create knowledge graph for this statement
                stmt_kg = KnowledgeGraph()
                stmt_kg.jurisdictions = global_kg.jurisdictions.copy()
                stmt_kg.adequacy_countries = global_kg.adequacy_countries.copy()
                stmt_kg.context = {"source_section": section.title, "global_context": global_kg.context}
                
                statements.append(LogicalStatement(
                    statement_id=f"{section.section_type}_{i}_{uuid.uuid4().hex[:8]}",
                    original_text=section.content,  # Full original text
                    simplified_text=block.strip(),
                    section_reference=section.title,
                    knowledge_graph=stmt_kg,
                    context={
                        "section_type": section.section_type,
                        "regulation_name": section.regulation_name,
                        "global_knowledge_graph": global_kg.dict(),
                        "complexity_level": "simplified"
                    }
                ))
        
        return statements

class Agent3RuleExtractor:
    """Agent 3: Extract rules with knowledge graph context"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
    
    @async_method
    async def extract_rules(self, statements: List[LogicalStatement]) -> List[ExtractedRule]:
        """Extract formal rules from logical statements using knowledge graph context"""
        
        system_prompt = """You are an expert rule extraction agent specializing in converting legal logical statements into json-rules-engine compatible rules using knowledge graph context.

CRITICAL REQUIREMENTS:
- Use knowledge graph context to prevent hallucination
- Never reference articles or legal codes - use simple English descriptions
- Extract ALL conditional patterns without truncation
- Create complete, executable rules
- Use discovered jurisdictions and adequacy countries from knowledge graph
- Generate rules in simple English

REASONING APPROACH:
1. Use knowledge graph context to understand scope and entities
2. Analyze logical statements for conditional patterns
3. Identify rule triggers and conditions from context
4. Map legal obligations to rule events in simple language
5. Create knowledge graphs of rule relationships
6. Generate json-rules-engine compatible structures

JSON-RULES-ENGINE FORMAT:
```json
{
  "conditions": {
    "all": [
      {
        "fact": "property_name",
        "operator": "equal|notEqual|in|notIn|greaterThan|lessThan",
        "value": "comparison_value"
      }
    ]
  },
  "event": {
    "type": "require|forbid|allow",
    "params": {
      "action": "specific_action_in_simple_english",
      "description": "what_must_be_done_in_plain_language"
    }
  }
}
```

FOCUS ON ACCESS & ENTITLEMENTS:
- Data subject access requests
- Controller consent obligations  
- Processor instruction requirements
- Transfer safeguard conditions
- Breach notification triggers
- Retention period limits

OUTPUT: Complete json-rules-engine compatible rules with simple English descriptions."""

        rules = []
        
        # Process statements in smaller batches to maintain quality
        batch_size = 3
        for i in range(0, len(statements), batch_size):
            batch = statements[i:i + batch_size]
            
            # Get knowledge graph context from first statement
            context_kg = batch[0].knowledge_graph if batch else KnowledgeGraph()
            
            user_prompt = f"""Extract formal rules from these logical statements using the knowledge graph context:

KNOWLEDGE GRAPH CONTEXT:
- Discovered Jurisdictions: {context_kg.jurisdictions}
- Discovered Adequacy Countries: {context_kg.adequacy_countries}
- Legal Actors: {context_kg.legal_actors}
- Data Categories: {context_kg.data_categories}

LOGICAL STATEMENTS:
{self._format_statements_for_prompt(batch)}

REASONING PROCESS:
1. Use knowledge graph context to understand scope and prevent hallucination
2. Identify conditional logic patterns
3. Map actors to rule facts (user.role, data.category, etc.) using discovered entities
4. Extract operators and comparison values
5. Define rule events in simple English
6. Create knowledge graph of rule dependencies

Generate json-rules-engine compatible rules with simple English descriptions. Use discovered jurisdictions and adequacy countries from knowledge graph context.

CRITICAL: Rules must be complete, executable, and described in simple English without legal jargon or article references."""

            messages = [{"role": "user", "content": user_prompt}]
            
            try:
                response = await self.client.get_completion(messages, system_prompt)
                batch_rules = self._parse_rules_from_response(response, batch, context_kg)
                rules.extend(batch_rules)
                
            except Exception as e:
                logger.error(f"Rule extraction failed for batch {i}: {e}")
                continue
        
        return rules
    
    def _format_statements_for_prompt(self, statements: List[LogicalStatement]) -> str:
        """Format statements for prompt with full context"""
        formatted = []
        for stmt in statements:
            formatted.append(f"ID: {stmt.statement_id}")
            formatted.append(f"SECTION: {stmt.section_reference}")
            formatted.append(f"SIMPLIFIED TEXT: {stmt.simplified_text}")
            formatted.append(f"ORIGINAL CONTEXT: {stmt.original_text[:500]}...")
            formatted.append("---")
        return "\n".join(formatted)
    
    def _parse_rules_from_response(self, response: str, statements: List[LogicalStatement], context_kg: KnowledgeGraph) -> List[ExtractedRule]:
        """Parse rules from AI response with knowledge graph context"""
        rules = []
        
        try:
            # Extract JSON structures from response
            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            
            for i, json_str in enumerate(json_matches):
                try:
                    rule_data = json.loads(json_str)
                    
                    # Create rule with knowledge graph context
                    rule = ExtractedRule(
                        rule_id=f"rule_{datetime.now().strftime('%Y%m%d')}_{uuid.uuid4().hex[:8]}",
                        rule_text=rule_data.get("rule_text", f"Rule extracted from {statements[0].section_reference if statements else 'unknown'}"),
                        conditions=rule_data.get("conditions", {"all": []}),
                        event=RuleEvent(
                            type=rule_data.get("event", {}).get("type", "require"),
                            params=rule_data.get("event", {}).get("params", {})
                        ),
                        applies_to_countries=context_kg.jurisdictions,
                        adequacy_countries=context_kg.adequacy_countries,
                        roles=context_kg.legal_actors,
                        data_categories=context_kg.data_categories,
                        domain="data_protection",
                        knowledge_graph=context_kg,
                        context={
                            "source_statements": [s.statement_id for s in statements],
                            "extraction_context": context_kg.dict()
                        }
                    )
                    
                    rules.append(rule)
                    
                except json.JSONDecodeError as je:
                    logger.warning(f"JSON decode error: {je}")
                    continue
                    
        except Exception as e:
            logger.error(f"Failed to parse rules: {e}")
        
        # If no valid rules found, create default rule with context
        if not rules and statements:
            rules.append(self._create_default_rule(statements[0], context_kg))
        
        return rules
    
    def _create_default_rule(self, statement: LogicalStatement, context_kg: KnowledgeGraph) -> ExtractedRule:
        """Create a default rule from a statement with knowledge graph context"""
        return ExtractedRule(
            rule_id=f"rule_{datetime.now().strftime('%Y%m%d')}_{uuid.uuid4().hex[:8]}",
            rule_text=statement.simplified_text[:200] + "...",
            conditions={"all": [{"fact": "user.role", "operator": "equal", "value": "Controller"}]},
            event=RuleEvent(type="require", params={"action": "compliance_required", "description": "Must ensure compliance with data protection requirements"}),
            applies_to_countries=context_kg.jurisdictions,
            adequacy_countries=context_kg.adequacy_countries,
            roles=context_kg.legal_actors if context_kg.legal_actors else ["Controller"],
            data_categories=context_kg.data_categories if context_kg.data_categories else ["Personal Data"],
            domain="data_protection",
            knowledge_graph=context_kg,
            context={"source_statement": statement.statement_id}
        )

class Agent4RoleAssigner:
    """Agent 4: Enhanced metadata assignment using LLM discovery"""
    
    def __init__(self, openai_client: OpenAIClient, geography_handler: GeographyHandler):
        self.client = openai_client
        self.geography = geography_handler
    
    @async_method
    async def assign_metadata(self, rules: List[ExtractedRule]) -> List[ExtractedRule]:
        """Assign comprehensive metadata using LLM analysis and knowledge graph context"""
        
        system_prompt = """You are an expert legal metadata analyst specializing in discovering and assigning comprehensive metadata to legal rules using knowledge graph context.

CRITICAL REQUIREMENTS:
- Use existing knowledge graph context to prevent hallucination
- Discover ALL roles, categories, and metadata from the rule content and context
- Do NOT use hardcoded values - extract everything from the document context
- Assign appropriate country codes based on discovered jurisdictions
- Use simple English descriptions without legal jargon

REASONING APPROACH:
1. Analyze rule content and knowledge graph context for metadata
2. Identify ALL applicable roles from the rule context
3. Discover ALL relevant data categories from context
4. Map jurisdictional applicability using discovered information
5. Assign appropriate country codes from knowledge graph
6. Create comprehensive metadata knowledge graph

ROLE DISCOVERY FROM CONTEXT:
Analyze the rule content and context to identify roles like:
- Controller: Entity determining purposes and means of processing
- Processor: Entity processing data on behalf of controller
- Joint Controller: Multiple controllers with shared responsibility
- Data Subject: Individual whose personal data is processed

DATA CATEGORY DISCOVERY FROM CONTEXT:
Analyze the rule content and context to identify categories like:
- Personal Data: Basic personal information
- Special Category Data: Sensitive data (health, biometric, etc.)
- Marketing Data: Data used for marketing purposes
- Financial Data: Payment and financial information
- Technical Data: System logs, IP addresses, etc.

JURISDICTIONAL MAPPING FROM KNOWLEDGE GRAPH:
Use the discovered jurisdictions and adequacy countries from knowledge graph context

OUTPUT: Enhanced rules with complete metadata discovered from content and context."""

        enhanced_rules = []
        
        for rule in rules:
            user_prompt = f"""Analyze this rule and assign appropriate metadata using the knowledge graph context:

RULE ID: {rule.rule_id}
RULE TEXT: {rule.rule_text}
CONDITIONS: {json.dumps(rule.conditions, indent=2)}
EVENT: {json.dumps(rule.event.dict(), indent=2)}

EXISTING KNOWLEDGE GRAPH CONTEXT:
- Discovered Jurisdictions: {rule.knowledge_graph.jurisdictions}
- Discovered Adequacy Countries: {rule.knowledge_graph.adequacy_countries}
- Known Legal Actors: {rule.knowledge_graph.legal_actors}
- Known Data Categories: {rule.knowledge_graph.data_categories}
- Rule Context: {json.dumps(rule.context, indent=2)}

REASONING STEPS:
1. Use knowledge graph context to understand scope and prevent hallucination
2. Identify applicable roles from rule conditions, events, and context
3. Determine relevant data categories from rule content and context
4. Map to appropriate countries using discovered jurisdictions
5. Assign adequacy countries using discovered information
6. Create enhanced metadata knowledge graph

Analyze the rule content and knowledge graph context to assign:
- roles: List of applicable roles discovered from content
- data_categories: Relevant data types discovered from context
- applies_to_countries: Countries from discovered jurisdictions
- adequacy_countries: Countries from discovered adequacy decisions
- domain: Legal domain from context

CRITICAL: Use ONLY information from the rule content and knowledge graph context. Do NOT use hardcoded values."""

            messages = [{"role": "user", "content": user_prompt}]
            
            try:
                response = await self.client.get_completion(messages, system_prompt)
                enhanced_rule = self._enhance_rule_with_discovered_metadata(rule, response)
                enhanced_rules.append(enhanced_rule)
                
            except Exception as e:
                logger.error(f"Metadata assignment failed for rule {rule.rule_id}: {e}")
                enhanced_rules.append(rule)  # Add original rule if enhancement fails
        
        return enhanced_rules
    
    def _enhance_rule_with_discovered_metadata(self, rule: ExtractedRule, response: str) -> ExtractedRule:
        """Enhance rule with metadata discovered by LLM"""
        
        # Create enhanced rule with discovered metadata
        enhanced_rule = rule.copy(deep=True)
        
        # Extract discovered metadata from response
        discovered_roles = self._extract_from_response(response, "roles", ["Controller"])
        discovered_categories = self._extract_from_response(response, "data_categories", ["Personal Data"])
        discovered_domain = self._extract_single_from_response(response, "domain", "data_protection")
        
        # Update rule with discovered information
        enhanced_rule.roles = discovered_roles
        enhanced_rule.data_categories = discovered_categories
        enhanced_rule.domain = discovered_domain
        
        # Map discovered jurisdictions to country codes
        if rule.knowledge_graph.jurisdictions:
            country_codes = []
            for jurisdiction in rule.knowledge_graph.jurisdictions:
                if len(jurisdiction) == 2:  # Already a country code
                    country_codes.append(jurisdiction)
                else:
                    # Try to map to country code
                    code = self.geography.get_country_code(jurisdiction)
                    if code:
                        country_codes.append(code)
            
            enhanced_rule.applies_to_countries = list(set(country_codes))
        
        # Use discovered adequacy countries
        enhanced_rule.adequacy_countries = rule.knowledge_graph.adequacy_countries
        
        return enhanced_rule
    
    def _extract_from_response(self, response: str, key: str, default: List[str]) -> List[str]:
        """Extract list values from LLM response"""
        response_lower = response.lower()
        
        # Look for key in response
        pattern = rf"{key}[:\s]*\[([^\]]+)\]"
        match = re.search(pattern, response_lower)
        
        if match:
            items_text = match.group(1)
            items = [item.strip().strip('"\'') for item in items_text.split(',')]
            return [item for item in items if item]
        
        # Fallback: look for key followed by values
        pattern = rf"{key}[:\s]*([^.\n]+)"
        match = re.search(pattern, response_lower)
        
        if match:
            values_text = match.group(1)
            values = [v.strip().strip('"\'') for v in values_text.split(',')]
            return [v for v in values if v and not v.startswith('list')]
        
        return default
    
    def _extract_single_from_response(self, response: str, key: str, default: str) -> str:
        """Extract single value from LLM response"""
        response_lower = response.lower()
        
        pattern = rf"{key}[:\s]*([^.\n]+)"
        match = re.search(pattern, response_lower)
        
        if match:
            value = match.group(1).strip().strip('"\'')
            return value if value else default
        
        return default

# Enhanced Main Processing Pipeline
class LegalRuleExtractionPipeline:
    """Enhanced main pipeline with knowledge graph state management"""
    
    def __init__(self, geography_file: str):
        self.openai_client = OpenAIClient()
        self.geography_handler = GeographyHandler(geography_file)
        self.pdf_processor = PDFProcessor()
        
        # Initialize agents with proper dependencies
        self.agent1 = Agent1DocumentCleaner(self.openai_client, self.geography_handler)
        self.agent2 = Agent2SemanticSegmentation(self.openai_client)
        self.agent3 = Agent3RuleExtractor(self.openai_client)
        self.agent4 = Agent4RoleAssigner(self.openai_client, self.geography_handler)
    
    async def process_document(self, metadata_config: MetadataConfig) -> ProcessedDocuments:
        """Process a single document through the complete pipeline"""
        logger.info(f"Starting processing of document: {metadata_config.pdf_path}")
        
        try:
            # Step 1: Extract complete text from PDF
            logger.info("Step 1: Extracting complete text from PDF")
            raw_text = self.pdf_processor.extract_text_from_pdf(metadata_config.pdf_path)
            
            if not raw_text:
                raise ValueError("No text extracted from PDF")
            
            logger.info(f"Extracted {len(raw_text)} characters from PDF")
            
            # Step 2: Clean and structure document with knowledge graph creation
            logger.info("Step 2: Cleaning and structuring document with knowledge graph")
            cleaned_doc = await self.agent1.clean_and_structure(raw_text, metadata_config)
            
            logger.info(f"Created document with {len(cleaned_doc.sections)} sections")
            logger.info(f"Discovered jurisdictions: {cleaned_doc.global_knowledge_graph.jurisdictions}")
            logger.info(f"Discovered adequacy countries: {cleaned_doc.global_knowledge_graph.adequacy_countries}")
            
            # Step 3: Semantic segmentation with knowledge graph context
            logger.info("Step 3: Performing semantic segmentation with context")
            logical_statements = await self.agent2.segment_and_simplify(cleaned_doc)
            
            logger.info(f"Generated {len(logical_statements)} logical statements")
            
            # Step 4: Extract rules with knowledge graph context
            logger.info("Step 4: Extracting rules with knowledge graph context")
            extracted_rules = await self.agent3.extract_rules(logical_statements)
            
            logger.info(f"Extracted {len(extracted_rules)} rules")
            
            # Step 5: Assign metadata using discovered information
            logger.info("Step 5: Assigning metadata using discovered information")
            final_rules = await self.agent4.assign_metadata(extracted_rules)
            
            logger.info(f"Enhanced {len(final_rules)} rules with metadata")
            
            # Create final processed document with complete knowledge graph
            processed_doc = ProcessedDocuments(
                document_id=cleaned_doc.document_id,
                rules=final_rules,
                global_knowledge_graph=cleaned_doc.global_knowledge_graph,
                metadata={
                    "processing_timestamp": datetime.now().isoformat(),
                    "original_file": metadata_config.pdf_path,
                    "total_rules": len(final_rules),
                    "processing_stages": ["clean", "segment", "extract", "enhance"],
                    "discovered_metadata": {
                        "jurisdictions": cleaned_doc.global_knowledge_graph.jurisdictions,
                        "adequacy_countries": cleaned_doc.global_knowledge_graph.adequacy_countries,
                        "legal_actors": cleaned_doc.global_knowledge_graph.legal_actors,
                        "data_categories": cleaned_doc.global_knowledge_graph.data_categories
                    }
                }
            )
            
            logger.info(f"Processing completed successfully. Extracted {len(final_rules)} rules with full context.")
            return processed_doc
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            raise
    
    async def process_multiple_documents(self, config_file: str) -> List[ProcessedDocuments]:
        """Process multiple documents from configuration file"""
        logger.info(f"Processing multiple documents from config: {config_file}")
        
        # Load configuration
        with open(config_file, 'r', encoding='utf-8') as f:
            configs = json.load(f)
        
        processed_docs = []
        
        for config_data in configs:
            try:
                metadata_config = MetadataConfig(**config_data)
                processed_doc = await self.process_document(metadata_config)
                processed_docs.append(processed_doc)
                
            except Exception as e:
                logger.error(f"Failed to process document {config_data.get('pdf_path', 'unknown')}: {e}")
                continue
        
        return processed_docs
    
    def save_results(self, processed_docs: List[ProcessedDocuments], output_dir: str):
        """Save results in JSON and CSV formats with complete information"""
        logger.info(f"Saving results to {output_dir}")
        
        # Create output directory
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Prepare data for output
        all_rules = []
        for doc in processed_docs:
            for rule in doc.rules:
                rule_dict = rule.dict()
                rule_dict['document_id'] = doc.document_id
                
                # Add global knowledge graph information
                rule_dict['global_knowledge_graph'] = doc.global_knowledge_graph.dict()
                
                all_rules.append(rule_dict)
        
        # Save JSON format with complete information
        json_file = Path(output_dir) / "extracted_rules.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(all_rules, f, indent=2, ensure_ascii=False)
        
        # Save CSV format
        csv_file = Path(output_dir) / "extracted_rules.csv"
        if all_rules:
            # Flatten nested dictionaries for CSV
            flattened_rules = []
            for rule in all_rules:
                flat_rule = {}
                for key, value in rule.items():
                    if isinstance(value, (dict, list)):
                        flat_rule[key] = json.dumps(value, ensure_ascii=False)
                    else:
                        flat_rule[key] = value
                flattened_rules.append(flat_rule)
            
            df = pd.DataFrame(flattened_rules)
            df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # Save knowledge graph summary
        kg_file = Path(output_dir) / "knowledge_graph_summary.json"
        kg_summary = {}
        for doc in processed_docs:
            kg_summary[doc.document_id] = doc.global_knowledge_graph.dict()
        
        with open(kg_file, 'w', encoding='utf-8') as f:
            json.dump(kg_summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved: {json_file}, {csv_file}, {kg_file}")

# CLI Interface with proper async handling
async def main():
    """Main function for CLI execution with proper async handling"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced Legal Document Rule Extraction System")
    parser.add_argument("--config", required=True, help="Path to metadata configuration JSON file")
    parser.add_argument("--geography", required=True, help="Path to geography JSON file")
    parser.add_argument("--output", default="./output", help="Output directory for results")
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = LegalRuleExtractionPipeline(args.geography)
    
    try:
        # Process documents
        processed_docs = await pipeline.process_multiple_documents(args.config)
        
        # Save results
        pipeline.save_results(processed_docs, args.output)
        
        logger.info("Processing completed successfully!")
        
        # Print summary
        total_rules = sum(len(doc.rules) for doc in processed_docs)
        logger.info(f"Total rules extracted: {total_rules}")
        
        for doc in processed_docs:
            logger.info(f"Document {doc.document_id}: {len(doc.rules)} rules")
            logger.info(f"  - Jurisdictions: {doc.global_knowledge_graph.jurisdictions}")
            logger.info(f"  - Adequacy countries: {doc.global_knowledge_graph.adequacy_countries}")
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        return 1
    
    return 0

# Example configuration creator
def create_example_files():
    """Create example configuration files"""
    
    # Example metadata_config.json with minimal required fields
    example_config = [
        {
            "pdf_path": "./documents/gdpr_regulation.pdf",
            "document_type": "regulation"
        },
        {
            "pdf_path": "./documents/ccpa_regulation.pdf", 
            "document_type": "regulation"
        }
    ]
    
    with open("metadata_config.json", 'w') as f:
        json.dump(example_config, f, indent=2)
    
    logger.info("Example configuration files created!")

# Main execution
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 1:
        create_example_files()
        print("Example configuration created. Run with --help for usage instructions.")
    else:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
