#!/usr/bin/env python3
"""
GDPR Comprehensive Compliance System - Multi-Agent Architecture with Full GDPR Inference
Article 30 RoPA foundation with complete GDPR regulatory framework integration

This system combines:
- Article 30 RoPA as the foundation for compliance documentation
- Comprehensive inference across the entire GDPR regulatory framework
- Multi-agent discovery with o3-mini reasoning for regulatory analysis
- Vector embeddings stored in Elasticsearch for semantic search
- Knowledge graphs in FalkorDB for relationship discovery
- Dynamic discovery of compliance requirements without hardcoded elements

The system performs legitimate regulatory inference by connecting explicit document
content to the complete GDPR compliance framework, including:
- Article 5 (Principles) compliance requirements
- Articles 6-10 (Legal basis) conditions and obligations  
- Articles 13-14 (Transparency) information requirements
- Articles 15-22 (Data subject rights) facilitation needs
- Articles 24-31 (Controller/processor responsibilities)
- Articles 32-35 (Security and impact assessment)
- Articles 44-49 (International transfers)

Author: AI Assistant
Date: 2025
Version: 3.2.0 - Comprehensive GDPR Inference with Vector & Graph RAG
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import argparse
import pickle
from pathlib import Path

# Core dependencies
import PyMuPDF as pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph for multi-agent architecture
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Global Configuration
GLOBAL_CONFIG = {
    # OpenAI Configuration - Only o3-mini
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "your_openai_api_key"),
    "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", None),
    
    # Elasticsearch Configuration
    "ELASTICSEARCH_HOST": os.getenv("ELASTICSEARCH_HOST", "https://localhost:9200"),
    "ELASTICSEARCH_USERNAME": os.getenv("ELASTICSEARCH_USERNAME", None),
    "ELASTICSEARCH_PASSWORD": os.getenv("ELASTICSEARCH_PASSWORD", None),
    "ELASTICSEARCH_CA_CERTS": os.getenv("ELASTICSEARCH_CA_CERTS", None),
    "ELASTICSEARCH_VERIFY_CERTS": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "true").lower() == "true",
    
    # FalkorDB Configuration
    "FALKORDB_HOST": os.getenv("FALKORDB_HOST", "localhost"),
    "FALKORDB_PORT": int(os.getenv("FALKORDB_PORT", 6379)),
    "FALKORDB_PASSWORD": os.getenv("FALKORDB_PASSWORD", None),
    
    # Document Paths
    "PDF_DOCUMENTS_PATH": os.getenv("PDF_DOCUMENTS_PATH", "./documents"),
    "OUTPUT_PATH": os.getenv("OUTPUT_PATH", "./output"),
    "MEMORY_PATH": os.getenv("MEMORY_PATH", "./memory"),
    
    # System Configuration
    "INDEX_NAME": "gdpr_article30_ropa_discovery",
    "GRAPH_NAME": "gdpr_ropa_knowledge_graph",
    "MEMORY_FILE": "ropa_agent_memory.pkl",
    
    # Embedding Configuration
    "EMBEDDING_MODEL": "text-embedding-3-large",
    "EMBEDDING_DIMENSIONS": 3072
}

# Configure logging
os.makedirs(GLOBAL_CONFIG["OUTPUT_PATH"], exist_ok=True)
os.makedirs(GLOBAL_CONFIG["MEMORY_PATH"], exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{GLOBAL_CONFIG['OUTPUT_PATH']}/gdpr_ropa_system.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RopaAgentState(TypedDict):
    """Shared state for all RoPA discovery agents"""
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document Processing State
    processed_documents: List[Dict[str, Any]]
    current_document: Optional[Dict[str, Any]]
    
    # Article 30 Discoveries (Built Dynamically)
    discovered_controllers: List[Dict[str, Any]]
    discovered_processors: List[Dict[str, Any]]
    discovered_processing_activities: List[Dict[str, Any]]
    discovered_data_categories: List[Dict[str, Any]]
    discovered_data_subjects: List[Dict[str, Any]]
    discovered_recipients: List[Dict[str, Any]]
    discovered_legal_bases: List[Dict[str, Any]]
    discovered_retention_periods: List[Dict[str, Any]]
    discovered_security_measures: List[Dict[str, Any]]
    discovered_transfers: List[Dict[str, Any]]
    discovered_territorial_scope: List[Dict[str, Any]]
    
    # Vector & Graph RAG State
    vector_search_results: List[Dict[str, Any]]
    graph_search_results: List[Dict[str, Any]]
    similar_documents: List[Dict[str, Any]]
    related_concepts: List[Dict[str, Any]]
    
    # Agent Reasoning Memory
    agent_memories: Dict[str, List[Dict[str, Any]]]
    reasoning_chains: List[Dict[str, Any]]
    
    # Progressive Learning
    learned_patterns: Dict[str, List[str]]
    confidence_scores: Dict[str, float]
    
    # Final Metamodel
    metamodel_structure: Optional[Dict[str, Any]]
    compliance_assessment: Optional[Dict[str, Any]]

class OpenAIEmbeddingGenerator:
    """Direct OpenAI API embedding generation"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"]
        )
        self.model = GLOBAL_CONFIG["EMBEDDING_MODEL"]
        self.dimensions = GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"]
        self.max_chars = 8000  # Character-based limit instead of tokens
        
        logger.info(f"Initialized OpenAI embedding generator with {self.model}")
    
    def chunk_text_by_chars(self, text: str) -> List[str]:
        """Chunk text by character count for embedding"""
        if len(text) <= self.max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.max_chars, len(text))
            
            # Try to break at sentence boundaries
            if end < len(text):
                # Find last sentence boundary
                last_period = text.rfind('.', start, end)
                last_newline = text.rfind('\n', start, end)
                last_boundary = max(last_period, last_newline)
                
                if last_boundary > start + len(text[start:end]) * 0.7:
                    end = last_boundary + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            start = end
        
        return chunks
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for single text"""
        try:
            # Ensure text is within character limits
            if len(text) > self.max_chars:
                logger.warning(f"Text has {len(text)} characters, using first chunk")
                chunks = self.chunk_text_by_chars(text)
                text = chunks[0]
            
            response = self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        embeddings = []
        
        for text in texts:
            embedding = self.generate_embedding(text)
            embeddings.append(embedding)
        
        return embeddings
    
    def generate_weighted_embedding(self, text: str, regulatory_weight: float = 2.0) -> List[float]:
        """Generate embedding with regulatory term weighting"""
        # Split text into chunks
        chunks = self.chunk_text_by_chars(text)
        
        if len(chunks) == 1:
            return self.generate_embedding(chunks[0])
        
        # Generate embeddings for each chunk
        chunk_embeddings = []
        weights = []
        
        regulatory_terms = [
            'gdpr', 'article 30', 'record of processing', 'controller', 'processor',
            'personal data', 'data subject', 'processing activity', 'legal basis',
            'retention', 'security measures', 'transfer', 'ropa'
        ]
        
        for chunk in chunks:
            embedding = self.generate_embedding(chunk)
            chunk_embeddings.append(embedding)
            
            # Calculate weight based on regulatory content
            chunk_lower = chunk.lower()
            regulatory_count = sum(1 for term in regulatory_terms if term in chunk_lower)
            weight = 1.0 + (regulatory_count * 0.1 * regulatory_weight)
            weights.append(weight)
        
        # Calculate weighted average
        total_weight = sum(weights)
        weighted_embedding = [
            sum(emb[i] * weights[j] for j, emb in enumerate(chunk_embeddings)) / total_weight
            for i in range(len(chunk_embeddings[0]))
        ]
        
        return weighted_embedding

class VectorEngine:
    """Elasticsearch-based vector storage and search engine"""
    
    def __init__(self):
        self.embedding_generator = OpenAIEmbeddingGenerator()
        self.client = self._create_elasticsearch_client()
        self.index_name = GLOBAL_CONFIG["INDEX_NAME"]
        self._create_vector_index()
    
    def _create_elasticsearch_client(self):
        """Create Elasticsearch client with configuration"""
        client_config = {
            "hosts": [GLOBAL_CONFIG["ELASTICSEARCH_HOST"]],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # SSL/TLS configuration
        if GLOBAL_CONFIG["ELASTICSEARCH_HOST"].startswith('https://'):
            client_config["use_ssl"] = True
            client_config["verify_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]
            
            if GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]:
                client_config["ca_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]
            
            if not GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]:
                client_config["ssl_show_warn"] = False
        
        # Authentication
        if GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"] and GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]:
            client_config["basic_auth"] = (
                GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"],
                GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]
            )
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_vector_index(self):
        """Create vector index for RoPA documents"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                        "index": True,
                        "similarity": "cosine"
                    },
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "token_count": {"type": "integer"},
                    "regulatory_density": {"type": "float"},
                    
                    # Discovered RoPA elements
                    "discovered_elements": {
                        "type": "nested",
                        "properties": {
                            "element_type": {"type": "keyword"},
                            "element_name": {"type": "text"},
                            "confidence": {"type": "float"},
                            "agent_source": {"type": "keyword"}
                        }
                    },
                    
                    # Agent analysis
                    "agent_analysis": {
                        "type": "nested",
                        "properties": {
                            "agent_name": {"type": "keyword"},
                            "analysis_result": {"type": "text"},
                            "confidence_score": {"type": "float"}
                        }
                    },
                    
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created vector index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create vector index: {e}")
            raise
    
    def index_document_chunk(self, chunk: Dict[str, Any], agent_analysis: Dict[str, Any] = None):
        """Index document chunk with embedding and agent analysis"""
        try:
            # Generate embedding
            embedding = self.embedding_generator.generate_weighted_embedding(chunk["text"])
            
            # Calculate regulatory density
            regulatory_terms = [
                'gdpr', 'article 30', 'controller', 'processor', 'personal data',
                'processing activity', 'legal basis', 'data subject', 'retention',
                'security', 'transfer', 'ropa'
            ]
            
            text_lower = chunk["text"].lower()
            regulatory_count = sum(1 for term in regulatory_terms if term in text_lower)
            word_count = len(chunk["text"].split())
            regulatory_density = regulatory_count / word_count if word_count > 0 else 0.0
            
            # Prepare document for indexing
            doc = {
                "text": chunk["text"],
                "embedding": embedding,
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "document_type": chunk.get("document_type", "regulatory"),
                "token_count": len(chunk["text"].split()),  # Word count approximation
                "regulatory_density": regulatory_density,
                "timestamp": datetime.now()
            }
            
            # Add agent analysis if provided
            if agent_analysis:
                doc["agent_analysis"] = []
                doc["discovered_elements"] = []
                
                for agent_name, analysis in agent_analysis.items():
                    doc["agent_analysis"].append({
                        "agent_name": agent_name,
                        "analysis_result": str(analysis.get("result", "")),
                        "confidence_score": analysis.get("confidence", 0.0)
                    })
                    
                    # Extract discovered elements
                    if "result" in analysis and isinstance(analysis["result"], dict):
                        for element_type, elements in analysis["result"].items():
                            if isinstance(elements, list):
                                for element in elements:
                                    if isinstance(element, dict) and element.get("name"):
                                        doc["discovered_elements"].append({
                                            "element_type": element_type,
                                            "element_name": element["name"],
                                            "confidence": element.get("confidence", 0.0),
                                            "agent_source": agent_name
                                        })
            
            # Index document
            self.client.index(index=self.index_name, id=chunk["chunk_id"], document=doc)
            
        except Exception as e:
            logger.error(f"Failed to index chunk {chunk['chunk_id']}: {e}")
            raise
    
    def vector_search(self, query: str, top_k: int = 10, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Perform vector similarity search"""
        try:
            # Generate query embedding
            query_embedding = self.embedding_generator.generate_embedding(query)
            
            # Build search query
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["text^2", "discovered_elements.element_name^3"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            }
                        ]
                    }
                },
                "size": top_k,
                "_source": {"excludes": ["embedding"]}
            }
            
            # Add filters
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    filter_clauses.append({"term": {key: value}})
                search_body["query"]["bool"]["filter"] = filter_clauses
            
            response = self.client.search(index=self.index_name, **search_body)
            
            # Format results
            results = []
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "chunk_id": source["chunk_id"],
                    "text": source["text"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "regulatory_density": source.get("regulatory_density", 0.0),
                    "discovered_elements": source.get("discovered_elements", []),
                    "agent_analysis": source.get("agent_analysis", [])
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def find_similar_documents(self, document_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Find documents similar to given document"""
        try:
            # Get the document
            doc_response = self.client.get(index=self.index_name, id=document_id)
            doc_text = doc_response["_source"]["text"]
            
            # Perform similarity search
            return self.vector_search(doc_text, top_k)
            
        except Exception as e:
            logger.error(f"Failed to find similar documents: {e}")
            return []

class GraphEngine:
    """FalkorDB-based knowledge graph for RoPA relationships"""
    
    def __init__(self):
        try:
            connection_kwargs = {
                "host": GLOBAL_CONFIG["FALKORDB_HOST"],
                "port": GLOBAL_CONFIG["FALKORDB_PORT"]
            }
            
            if GLOBAL_CONFIG["FALKORDB_PASSWORD"]:
                connection_kwargs["password"] = GLOBAL_CONFIG["FALKORDB_PASSWORD"]
            
            self.db = FalkorDB(**connection_kwargs)
            self.graph = self.db.select_graph(GLOBAL_CONFIG["GRAPH_NAME"])
            
            logger.info(f"Connected to FalkorDB at {GLOBAL_CONFIG['FALKORDB_HOST']}:{GLOBAL_CONFIG['FALKORDB_PORT']}")
            
            # Initialize graph structure
            self._initialize_graph_schema()
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def _initialize_graph_schema(self):
        """Initialize graph schema for RoPA concepts"""
        try:
            # Create indexes for better performance
            indexes = [
                "CREATE INDEX FOR (n:Controller) ON (n.name)",
                "CREATE INDEX FOR (n:Processor) ON (n.name)",
                "CREATE INDEX FOR (n:ProcessingActivity) ON (n.name)",
                "CREATE INDEX FOR (n:DataCategory) ON (n.name)",
                "CREATE INDEX FOR (n:LegalBasis) ON (n.basis)",
                "CREATE INDEX FOR (n:DataSubject) ON (n.category)",
                "CREATE INDEX FOR (n:Document) ON (n.chunk_id)"
            ]
            
            for index_query in indexes:
                try:
                    self.graph.query(index_query)
                except:
                    # Index might already exist
                    pass
            
            logger.info("Graph schema initialized")
            
        except Exception as e:
            logger.warning(f"Could not initialize all graph indexes: {e}")
    
    def add_discovered_element(self, element_type: str, element_data: Dict[str, Any], source_chunk_id: str):
        """Add discovered RoPA element to knowledge graph"""
        try:
            element_name = element_data.get("name", "")
            if not element_name:
                return
                
            # Escape single quotes to prevent query injection
            element_name_escaped = element_name.replace("'", "\\'")
            confidence = element_data.get("confidence", 0.0)
            
            # Create node based on element type
            if element_type == "controllers":
                contact_details = element_data.get("contact_details", "").replace("'", "\\'")
                representative = element_data.get("representative", "").replace("'", "\\'")
                dpo = element_data.get("dpo", "").replace("'", "\\'")
                
                query = f"""
                MERGE (c:Controller {{name: '{element_name_escaped}'}})
                SET c.confidence = {confidence},
                    c.contact_details = '{contact_details}',
                    c.representative = '{representative}',
                    c.dpo = '{dpo}'
                """
                
            elif element_type == "processors":
                contact_details = element_data.get("contact_details", "").replace("'", "\\'")
                services_json = json.dumps(element_data.get("services", [])).replace("'", "\\'")
                
                query = f"""
                MERGE (p:Processor {{name: '{element_name_escaped}'}})
                SET p.confidence = {confidence},
                    p.contact_details = '{contact_details}',
                    p.services = '{services_json}'
                """
                
            elif element_type == "processing_activities":
                purpose = element_data.get("purpose", "").replace("'", "\\'")
                legal_basis = element_data.get("legal_basis", "").replace("'", "\\'")
                retention_period = element_data.get("retention_period", "").replace("'", "\\'")
                data_categories_json = json.dumps(element_data.get("data_categories", [])).replace("'", "\\'")
                data_subjects_json = json.dumps(element_data.get("data_subjects", [])).replace("'", "\\'")
                
                query = f"""
                MERGE (pa:ProcessingActivity {{name: '{element_name_escaped}'}})
                SET pa.confidence = {confidence},
                    pa.purpose = '{purpose}',
                    pa.legal_basis = '{legal_basis}',
                    pa.retention_period = '{retention_period}',
                    pa.data_categories = '{data_categories_json}',
                    pa.data_subjects = '{data_subjects_json}'
                """
                
            elif element_type == "data_categories":
                sensitivity = element_data.get("sensitivity", "normal")
                special_category = str(element_data.get("special_category", False)).lower()
                examples_json = json.dumps(element_data.get("examples", [])).replace("'", "\\'")
                
                query = f"""
                MERGE (dc:DataCategory {{name: '{element_name_escaped}'}})
                SET dc.confidence = {confidence},
                    dc.sensitivity = '{sensitivity}',
                    dc.special_category = {special_category},
                    dc.examples = '{examples_json}'
                """
                
            elif element_type == "legal_bases":
                article = element_data.get("article", "").replace("'", "\\'")
                description = element_data.get("description", "").replace("'", "\\'")
                requirements_json = json.dumps(element_data.get("requirements", [])).replace("'", "\\'")
                
                query = f"""
                MERGE (lb:LegalBasis {{basis: '{element_name_escaped}'}})
                SET lb.confidence = {confidence},
                    lb.article = '{article}',
                    lb.description = '{description}',
                    lb.requirements = '{requirements_json}'
                """
                
            elif element_type == "data_subjects":
                description = element_data.get("description", "").replace("'", "\\'")
                examples_json = json.dumps(element_data.get("examples", [])).replace("'", "\\'")
                
                query = f"""
                MERGE (ds:DataSubject {{category: '{element_name_escaped}'}})
                SET ds.confidence = {confidence},
                    ds.description = '{description}',
                    ds.examples = '{examples_json}'
                """
                
            else:
                # Generic node for other types
                data_json = json.dumps(element_data).replace("'", "\\'")
                
                query = f"""
                MERGE (n:RopaElement {{name: '{element_name_escaped}', type: '{element_type}'}})
                SET n.confidence = {confidence},
                    n.data = '{data_json}'
                """
            
            self.graph.query(query)
            
            # Link to source document with escaped chunk_id
            chunk_id_escaped = source_chunk_id.replace("'", "\\'")
            doc_link_query = f"""
            MERGE (d:Document {{chunk_id: '{chunk_id_escaped}'}})
            MATCH (e) WHERE e.name = '{element_name_escaped}' OR e.basis = '{element_name_escaped}' OR e.category = '{element_name_escaped}'
            MERGE (d)-[:CONTAINS]->(e)
            """
            self.graph.query(doc_link_query)
            
        except Exception as e:
            logger.error(f"Failed to add element {element_type}:{element_name}: {e}")
    
    def create_relationships(self, relationships: List[Dict[str, Any]]):
        """Create relationships between RoPA elements"""
        try:
            for rel in relationships:
                source = rel.get("source", "").replace("'", "\\'")
                target = rel.get("target", "").replace("'", "\\'")
                rel_type = rel.get("type", "RELATED_TO")
                confidence = rel.get("confidence", 0.0)
                
                if source and target:
                    query = f"""
                    MATCH (a) WHERE a.name = '{source}' OR a.basis = '{source}' OR a.category = '{source}'
                    MATCH (b) WHERE b.name = '{target}' OR b.basis = '{target}' OR b.category = '{target}'
                    MERGE (a)-[r:{rel_type}]->(b)
                    SET r.confidence = {confidence}
                    """
                    self.graph.query(query)
                    
        except Exception as e:
            logger.error(f"Failed to create relationships: {e}")
    
    def graph_search(self, query_concept: str, max_depth: int = 3, limit: int = 20) -> List[Dict[str, Any]]:
        """Perform graph search for related concepts"""
        try:
            concept_escaped = query_concept.replace("'", "\\'")
            
            graph_query = f"""
            MATCH path = (start)-[*1..{max_depth}]-(related)
            WHERE (
                start.name CONTAINS '{concept_escaped}' OR 
                start.basis CONTAINS '{concept_escaped}' OR 
                start.category CONTAINS '{concept_escaped}' OR
                start.purpose CONTAINS '{concept_escaped}'
            )
            RETURN DISTINCT start, related, relationships(path), length(path) as distance
            ORDER BY distance, start.confidence DESC
            LIMIT {limit}
            """
            
            result = self.graph.query(graph_query)
            
            formatted_results = []
            for record in result.result_set:
                start_node = self._format_node(record[0])
                related_node = self._format_node(record[1])
                relationships = [str(rel) for rel in record[2]]
                distance = record[3]
                
                formatted_results.append({
                    "start_node": start_node,
                    "related_node": related_node,
                    "relationships": relationships,
                    "distance": distance,
                    "relevance_score": 1.0 / (distance + 1)
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Graph search failed: {e}")
            return []
    
    def find_compliance_paths(self, activity_name: str) -> List[Dict[str, Any]]:
        """Find compliance paths for a processing activity"""
        try:
            activity_escaped = activity_name.replace("'", "\\'")
            
            query = f"""
            MATCH (pa:ProcessingActivity {{name: '{activity_escaped}'}})
            OPTIONAL MATCH (pa)-[:BASED_ON]->(lb:LegalBasis)
            OPTIONAL MATCH (pa)-[:PROCESSES]->(dc:DataCategory)
            OPTIONAL MATCH (pa)-[:INVOLVES]->(ds:DataSubject)
            OPTIONAL MATCH (pa)-[:CONTROLLED_BY]->(c:Controller)
            RETURN pa, lb, dc, ds, c
            """
            
            result = self.graph.query(query)
            
            compliance_paths = []
            for record in result.result_set:
                path = {
                    "activity": self._format_node(record[0]),
                    "legal_basis": self._format_node(record[1]) if record[1] else None,
                    "data_category": self._format_node(record[2]) if record[2] else None,
                    "data_subject": self._format_node(record[3]) if record[3] else None,
                    "controller": self._format_node(record[4]) if record[4] else None
                }
                compliance_paths.append(path)
            
            return compliance_paths
            
        except Exception as e:
            logger.error(f"Failed to find compliance paths: {e}")
            return []
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if not node:
            return None
            
        try:
            if hasattr(node, 'properties'):
                properties = dict(node.properties)
                properties['labels'] = list(node.labels) if hasattr(node, 'labels') else []
                return properties
            else:
                return {"id": str(node)}
        except:
            return {"id": str(node)}

class MemoryManager:
    """Manages persistent memory for the multi-agent system"""
    
    def __init__(self):
        self.memory_file = os.path.join(GLOBAL_CONFIG["MEMORY_PATH"], GLOBAL_CONFIG["MEMORY_FILE"])
        self.state_history = []
    
    def save_state(self, state: RopaAgentState):
        """Save current state to persistent memory"""
        try:
            # Convert state to serializable format
            serializable_state = {}
            for key, value in state.items():
                if key == "messages":
                    serializable_state[key] = [
                        {"type": type(msg).__name__, "content": msg.content} 
                        for msg in value
                    ]
                else:
                    serializable_state[key] = value
            
            # Add timestamp
            serializable_state["timestamp"] = datetime.now().isoformat()
            
            # Save to file
            with open(self.memory_file, 'wb') as f:
                pickle.dump(serializable_state, f)
            
            logger.info(f"State saved to {self.memory_file}")
            
        except Exception as e:
            logger.error(f"Failed to save state: {e}")
    
    def load_state(self) -> Optional[RopaAgentState]:
        """Load state from persistent memory"""
        try:
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'rb') as f:
                    loaded_state = pickle.load(f)
                
                # Convert back to proper state format
                state = RopaAgentState()
                for key, value in loaded_state.items():
                    if key == "messages" and value:
                        state[key] = [
                            HumanMessage(content=msg["content"]) if msg["type"] == "HumanMessage"
                            else AIMessage(content=msg["content"])
                            for msg in value
                        ]
                    elif key != "timestamp":
                        state[key] = value
                
                logger.info(f"State loaded from {self.memory_file}")
                return state
            
        except Exception as e:
            logger.error(f"Failed to load state: {e}")
        
        return None
    
    def clear_memory(self):
        """Clear persistent memory"""
        try:
            if os.path.exists(self.memory_file):
                os.remove(self.memory_file)
                logger.info("Memory cleared")
        except Exception as e:
            logger.error(f"Failed to clear memory: {e}")

class O3MiniAgent:
    """Base agent using o3-mini with high reasoning effort"""
    
    def __init__(self, agent_name: str, system_prompt: str):
        self.agent_name = agent_name
        self.system_prompt = system_prompt
        
        # Initialize o3-mini client
        client_kwargs = {"api_key": GLOBAL_CONFIG["OPENAI_API_KEY"]}
        if GLOBAL_CONFIG["OPENAI_BASE_URL"]:
            client_kwargs["base_url"] = GLOBAL_CONFIG["OPENAI_BASE_URL"]
        
        self.client = OpenAI(**client_kwargs)
        
    def reason_with_context(self, prompt: str, vector_context: List[Dict[str, Any]] = None, 
                           graph_context: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Perform reasoning using o3-mini with vector and graph context"""
        try:
            # Prepare context
            context_str = ""
            
            if vector_context:
                context_str += "\n\nVector Search Context (Similar Content):\n"
                for i, ctx in enumerate(vector_context[:3]):  # Top 3 results
                    context_str += f"[Vector-{i+1}] Score: {ctx.get('score', 0):.3f}\n"
                    context_str += f"Text: {ctx.get('text', '')[:500]}...\n"
                    context_str += f"Elements: {ctx.get('discovered_elements', [])}\n\n"
            
            if graph_context:
                context_str += "\n\nGraph Search Context (Related Concepts):\n"
                for i, ctx in enumerate(graph_context[:3]):  # Top 3 results
                    context_str += f"[Graph-{i+1}] Distance: {ctx.get('distance', 0)}\n"
                    context_str += f"Start: {ctx.get('start_node', {})}\n"
                    context_str += f"Related: {ctx.get('related_node', {})}\n"
                    context_str += f"Relationships: {ctx.get('relationships', [])}\n\n"
            
            full_prompt = f"{self.system_prompt}\n\n{prompt}{context_str}"
            
            # Use o3-mini with high reasoning effort
            response = self.client.chat.completions.create(
                model="o3-mini",
                messages=[{"role": "user", "content": full_prompt}],
                reasoning_effort="high"
            )
            
            content = response.choices[0].message.content
            
            # Try to extract JSON from response
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    json_str = content[json_start:json_end]
                    result = json.loads(json_str)
                    return {
                        "agent": self.agent_name,
                        "reasoning": content[:json_start].strip() if json_start > 0 else "",
                        "result": result,
                        "vector_context_used": len(vector_context) if vector_context else 0,
                        "graph_context_used": len(graph_context) if graph_context else 0,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            # If no JSON found, return as text response
            return {
                "agent": self.agent_name,
                "reasoning": content,
                "result": {"analysis": content},
                "vector_context_used": len(vector_context) if vector_context else 0,
                "graph_context_used": len(graph_context) if graph_context else 0,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Agent {self.agent_name} reasoning failed: {e}")
            return {
                "agent": self.agent_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    # Alias methods for backward compatibility
    def analyze_with_rag(self, text: str, vector_engine, graph_engine, state=None):
        vector_context = vector_engine.vector_search(text[:1000], top_k=5) if vector_engine else None
        graph_context = graph_engine.graph_search("processing", max_depth=2, limit=5) if graph_engine else None
        return self.reason_with_context(text, vector_context, graph_context)
    
    def discover_with_rag(self, text: str, vector_engine, graph_engine):
        vector_context = vector_engine.vector_search(text[:1000], top_k=5) if vector_engine else None
        graph_context = graph_engine.graph_search("data", max_depth=2, limit=5) if graph_engine else None
        return self.reason_with_context(text, vector_context, graph_context)
    
    def assess_with_rag(self, data: Dict[str, Any], vector_engine, graph_engine):
        text = str(data)[:1000]
        vector_context = vector_engine.vector_search("compliance", top_k=5) if vector_engine else None
        graph_context = graph_engine.graph_search("compliance", max_depth=2, limit=5) if graph_engine else None
        return self.reason_with_context(text, vector_context, graph_context)
    
    def build_with_rag(self, data: Dict[str, Any], vector_engine, graph_engine):
        text = str(data)[:1000]
        vector_context = vector_engine.vector_search("metamodel", top_k=5) if vector_engine else None
        graph_context = graph_engine.graph_search("structure", max_depth=2, limit=5) if graph_engine else None
        return self.reason_with_context(text, vector_context, graph_context)

class DocumentAnalysisAgent(O3MiniAgent):
    """Agent for initial document analysis with vector and graph RAG"""
    
    def __init__(self):
        system_prompt = """You are a certified GDPR compliance expert analyzing regulatory documents to build comprehensive Record of Processing Activities (RoPA) with full GDPR context integration.

While Article 30 provides the RoPA framework, you must perform legitimate regulatory inference by connecting document content to the entire GDPR structure:

CORE ARTICLE 30 ELEMENTS:
- Controllers, processors, and their details
- Processing activities and purposes
- Data categories and data subjects
- Legal bases and recipients
- International transfers and safeguards
- Retention periods and security measures

GDPR CONTEXTUAL INFERENCE (based on document content):

When identifying PROCESSING ACTIVITIES, infer connections to:
- Article 5 (Principles): Purpose limitation, data minimization, accuracy requirements
- Article 24 (Responsibility): Controller accountability measures needed
- Article 25 (Data protection by design): Technical and organizational measures

When identifying DATA CATEGORIES, infer requirements from:
- Article 9 (Special categories): Additional protections and legal conditions
- Article 13-14 (Transparency): Information that must be provided to data subjects
- Article 15-22 (Data subject rights): Rights that must be facilitated for each category

When identifying LEGAL BASES, infer implications from:
- Article 6 (Lawfulness): Specific conditions and requirements
- Article 7 (Consent): Consent management requirements when applicable
- Article 9 (Special categories): Additional conditions for sensitive data

When identifying SECURITY MEASURES, connect to:
- Article 32 (Security): Technical and organizational measures required
- Article 33-34 (Breach notification): Incident response procedures
- Article 35 (DPIA): Risk assessment requirements

When identifying TRANSFERS, infer from:
- Articles 44-49 (International transfers): Safeguards and mechanisms required
- Adequacy decisions and transfer impact assessments

For each discovery, provide:
1. Direct evidence from the document
2. Regulatory inference based on GDPR requirements
3. Compliance implications and necessary actions
4. Cross-references to relevant GDPR articles

This inference must be based on legitimate regulatory analysis of document content, connecting explicit information to comprehensive GDPR compliance requirements."""
        
        super().__init__("DocumentAnalysisAgent", system_prompt)
    
    def analyze_with_rag(self, document_text: str, vector_engine: VectorEngine, 
                        graph_engine: GraphEngine, state: RopaAgentState) -> Dict[str, Any]:
        """Analyze document with vector and graph RAG context"""
        
        # Get vector context
        vector_context = vector_engine.vector_search(document_text[:1000], top_k=5)
        
        # Get graph context for key terms
        key_terms = ["processing activity", "data category", "legal basis", "controller"]
        graph_context = []
        for term in key_terms:
            graph_results = graph_engine.graph_search(term, max_depth=2, limit=5)
            graph_context.extend(graph_results)
        
        prompt = f"""Analyze this document text for comprehensive GDPR compliance with Article 30 RoPA as the foundation:

Document Text: {document_text[:8000]}...

Previous discoveries count:
- Processing activities: {len(state.get('discovered_processing_activities', []))}
- Data categories: {len(state.get('discovered_data_categories', []))}
- Legal bases: {len(state.get('discovered_legal_bases', []))}

Using vector and graph context, perform comprehensive GDPR analysis:

1. ARTICLE 30 FOUNDATION ELEMENTS:
   - Controllers and processors with complete details
   - Processing activities with purposes and scope
   - Data categories including special categories
   - Data subjects with category definitions
   - Legal bases with article references
   - Recipients and international transfers
   - Retention periods and security measures

2. COMPREHENSIVE GDPR INFERENCE from document content:

   For each PROCESSING ACTIVITY identified, infer:
   - Article 5 principles compliance requirements (purpose limitation, data minimization, etc.)
   - Article 24 accountability measures needed
   - Article 25 data protection by design requirements
   - Articles 13-14 transparency obligations
   - Articles 15-22 data subject rights facilitation needs
   - Article 32 security measures appropriate to risk
   - Article 35 DPIA requirements if high-risk processing

   For each DATA CATEGORY identified, infer:
   - Article 9 special category additional protections if applicable
   - Article 13-14 specific information to be provided to data subjects
   - Articles 15-22 rights exercise procedures needed
   - Article 32 risk-appropriate security measures
   - Retention requirements under Article 5(1)(e)

   For each LEGAL BASIS identified, infer:
   - Article 7 consent requirements if consent basis
   - Balancing test requirements for legitimate interests
   - Article 9 additional conditions for special categories
   - Transparency obligations about legal basis
   - Data subject rights limitations or enhancements

   For each INTERNATIONAL TRANSFER identified, infer:
   - Articles 44-49 transfer mechanism requirements
   - Transfer impact assessment needs
   - Additional safeguards beyond standard measures
   - Ongoing monitoring and review obligations
   - Data subject information requirements

3. COMPLIANCE IMPLICATIONS AND ACTIONS:
   - Immediate compliance gaps requiring attention
   - Documentation and evidence requirements
   - Procedural implementations needed
   - Technical measures to be implemented
   - Training and awareness requirements
   - Ongoing monitoring and review needs

Return comprehensive JSON with:
- Direct discoveries from document content
- Legitimate regulatory inference based on GDPR requirements
- Compliance implications and necessary actions
- Cross-references to relevant GDPR articles and requirements"""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class ProcessingActivityAgent(O3MiniAgent):
    """Specialized agent for discovering processing activities with RAG"""
    
    def __init__(self):
        system_prompt = """You are a data processing specialist with comprehensive GDPR knowledge, focusing on identifying processing activities and their full regulatory context.

Your expertise encompasses connecting processing activities to the complete GDPR compliance framework:

PROCESSING ACTIVITIES IDENTIFICATION:
Identify any operation performed on personal data including:
- Collection, recording, organization, structuring, storage
- Adaptation, alteration, retrieval, consultation, use
- Disclosure, transmission, dissemination, alignment
- Restriction, erasure, destruction

COMPREHENSIVE GDPR INFERENCE for each processing activity:

ARTICLE 5 PRINCIPLES ANALYSIS:
- Lawfulness, fairness, transparency implications
- Purpose limitation requirements and compatible purposes
- Data minimization needs and proportionality
- Accuracy obligations and correction procedures
- Storage limitation and retention requirements
- Integrity and confidentiality measures needed

ARTICLE 6 LEGAL BASIS ASSESSMENT:
- Most appropriate legal basis for each activity
- Conditions and requirements for chosen basis
- Documentation and evidence requirements
- Balancing test needs for legitimate interests

ARTICLE 24 CONTROLLER RESPONSIBILITIES:
- Accountability measures required for each activity
- Technical and organizational measures needed
- Staff training and awareness requirements
- Documentation and record-keeping obligations

ARTICLE 25 DATA PROTECTION BY DESIGN:
- Privacy by design requirements for each activity
- Default settings and configurations needed
- Technical measures for data protection
- Organizational procedures required

ARTICLE 32 SECURITY IMPLICATIONS:
- Security measures appropriate to risk level
- Pseudonymization and encryption requirements
- Confidentiality, integrity, availability needs
- Incident detection and response procedures

DATA SUBJECT RIGHTS FACILITATION (Articles 15-22):
- Rights applicable to each processing activity
- Procedures needed to respond to requests
- Technical measures to enable rights exercise
- Documentation of rights handling procedures

Based on document content, infer the complete regulatory compliance framework needed for each identified processing activity, providing both explicit evidence and legitimate regulatory inference."""
        
        super().__init__("ProcessingActivityAgent", system_prompt)
    
    def discover_with_rag(self, text: str, vector_engine: VectorEngine, 
                         graph_engine: GraphEngine) -> Dict[str, Any]:
        """Discover processing activities with RAG context"""
        
        # Search for similar processing activities
        vector_context = vector_engine.vector_search(f"processing activity {text[:500]}", top_k=5)
        
        # Find related processing activities in graph
        graph_context = graph_engine.graph_search("ProcessingActivity", max_depth=2, limit=10)
        
        prompt = f"""Analyze this text to discover specific data processing activities with RAG context:

Text: {text}

Using the provided context, identify:

1. Specific processing activities (not generic categories)
2. For each activity:
   - Exact purpose and business justification
   - Data types involved
   - Data subject categories
   - Applicable legal basis
   - Retention requirements
   - Security considerations
   - Cross-border implications

3. Activity relationships and dependencies
4. Workflow and process integration points

Return detailed JSON with discovered activities and their complete specifications."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class DataCategoryAgent(O3MiniAgent):
    """Specialized agent for data category discovery with RAG"""
    
    def __init__(self):
        system_prompt = """You are a data classification expert with comprehensive GDPR knowledge, specializing in identifying data categories and their complete regulatory implications.

Your role extends beyond simple data identification to include inference of full GDPR compliance requirements for each data category:

DATA CATEGORY IDENTIFICATION:
Identify all categories of personal data as defined by GDPR Article 4(1), including:

REGULAR PERSONAL DATA:
- Identity and contact information
- Financial and transaction data
- Professional and employment data
- Technical and usage data
- Behavioral and preference data

SPECIAL CATEGORIES (Article 9):
- Racial or ethnic origin
- Political opinions
- Religious or philosophical beliefs
- Trade union membership
- Genetic and biometric data
- Health data
- Sex life or sexual orientation

CRIMINAL CONVICTION DATA (Article 10):
- Criminal convictions and related security measures

COMPREHENSIVE GDPR INFERENCE for each data category:

ARTICLE 9 SPECIAL CATEGORY ANALYSIS:
- Additional legal conditions required (explicit consent, employment law, vital interests, etc.)
- Enhanced security and protection measures needed
- Specific documentation and consent management requirements
- Cross-border transfer restrictions and safeguards

TRANSPARENCY OBLIGATIONS (Articles 13-14):
- Specific information that must be provided to data subjects for each category
- Source disclosure requirements for indirectly collected data
- Purpose and legal basis communication needs
- Retention period communication requirements

DATA SUBJECT RIGHTS IMPLICATIONS (Articles 15-22):
- Right of access: What information must be provided for each category
- Right to rectification: Correction procedures needed
- Right to erasure: Deletion requirements and exceptions
- Right to portability: Technical requirements for data provision
- Right to restriction: Limitation procedures needed
- Right to object: Opt-out mechanisms required

ARTICLE 32 SECURITY REQUIREMENTS:
- Risk-appropriate security measures for each data category
- Encryption and pseudonymization requirements
- Access control and authorization needs
- Data breach notification thresholds

RETENTION AND DELETION (Article 5):
- Storage limitation principles application
- Retention period justification for each category
- Deletion procedures and technical implementation
- Archiving requirements and legal basis

Based on document content, infer the complete data protection framework required for each identified data category, connecting explicit information to comprehensive GDPR obligations."""
        
        super().__init__("DataCategoryAgent", system_prompt)
    
    def discover_with_rag(self, text: str, vector_engine: VectorEngine, 
                         graph_engine: GraphEngine) -> Dict[str, Any]:
        """Discover data categories with RAG context"""
        
        vector_context = vector_engine.vector_search(f"personal data category {text[:500]}", top_k=5)
        graph_context = graph_engine.graph_search("DataCategory", max_depth=2, limit=10)
        
        prompt = f"""Analyze this text to discover and classify personal data categories with RAG context:

Text: {text}

Identify:

1. All types of personal data mentioned or implied
2. Classification for each type:
   - Regular personal data vs. special categories (Article 9)
   - Criminal conviction data (Article 10)
   - Sensitivity level and protection requirements
   - Source and collection method
   - Processing purposes and usage

3. Data relationships and combinations
4. Cross-border transfer implications

Return detailed JSON with discovered data categories and classifications."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class LegalBasisAgent(O3MiniAgent):
    """Specialized agent for legal basis identification with RAG"""
    
    def __init__(self):
        system_prompt = """You are a legal compliance expert with comprehensive GDPR knowledge, specializing in legal basis identification and their complete regulatory implications.

Your expertise encompasses connecting legal bases to the full spectrum of GDPR compliance requirements:

LEGAL BASIS IDENTIFICATION (Article 6):
(a) CONSENT: Specific, informed, unambiguous indication of wishes
(b) CONTRACT: Performance of contract or pre-contractual steps
(c) LEGAL OBLIGATION: Compliance with legal obligations under EU or Member State law
(d) VITAL INTERESTS: Protection of vital interests of data subject or others
(e) PUBLIC TASK: Performance of task in public interest or official authority
(f) LEGITIMATE INTERESTS: Legitimate interests pursued by controller or third party

COMPREHENSIVE GDPR INFERENCE for each legal basis:

CONSENT (Article 7) REQUIREMENTS when applicable:
- Demonstrable consent with clear affirmative action
- Specific consent for each purpose
- Informed consent with clear information provided
- Freely given consent without coercion
- Withdrawable consent with easy withdrawal mechanisms
- Separate consent for different processing operations
- Enhanced consent requirements for children (Article 8)

CONTRACT BASIS IMPLICATIONS:
- Necessity test: Processing must be necessary for contract performance
- Pre-contractual processing scope and limitations
- Contract modification impact on processing
- Data subject rights limitations under contract basis

LEGAL OBLIGATION BASIS ANALYSIS:
- Specific legal obligation identification and reference
- EU or Member State law requirements
- Scope of processing under legal obligation
- Documentation of legal requirement source

LEGITIMATE INTERESTS (Article 6(1)(f)) ASSESSMENT:
- Balancing test between interests and data subject rights
- Impact assessment on data subjects
- Reasonable expectations of data subjects
- Fundamental rights and freedoms assessment
- Documentation of balancing test results

SPECIAL CATEGORIES LEGAL BASIS (Article 9):
- Additional conditions beyond Article 6 basis
- Explicit consent requirements and standards
- Employment, social security, and social protection law
- Vital interests with inability to consent
- Non-profit body processing with appropriate safeguards
- Manifestly public data processing
- Legal claims and judicial acts
- Substantial public interest with proportionality
- Health and social care with professional secrecy
- Public health with suitable measures
- Archiving, research, and statistics with safeguards

CRIMINAL CONVICTION DATA (Article 10):
- Official authority control requirements
- Specific legal authorization needs
- Appropriate safeguards implementation

TRANSPARENCY OBLIGATIONS (Articles 13-14):
- Information to be provided about legal basis
- Legitimate interests disclosure requirements
- Changes to legal basis communication needs

Based on document content, infer the complete legal framework and compliance obligations associated with each identified legal basis, connecting explicit information to comprehensive GDPR requirements."""
        
        super().__init__("LegalBasisAgent", system_prompt)
    
    def discover_with_rag(self, text: str, vector_engine: VectorEngine, 
                         graph_engine: GraphEngine) -> Dict[str, Any]:
        """Discover legal bases with RAG context"""
        
        vector_context = vector_engine.vector_search(f"legal basis article 6 {text[:500]}", top_k=5)
        graph_context = graph_engine.graph_search("LegalBasis", max_depth=2, limit=10)
        
        prompt = f"""Analyze the processing context to identify applicable legal bases with RAG context:

Text: {text}

For each processing scenario:

1. Identify applicable Article 6 legal basis
2. Determine if Article 9 conditions are needed
3. Check if Article 10 applies
4. Evaluate multiple legal bases scenarios
5. Assess territorial implications for legal bases

Return detailed JSON with legal basis analysis and justifications."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class TerritorialScopeAgent(O3MiniAgent):
    """Specialized agent for territorial scope analysis with RAG"""
    
    def __init__(self):
        system_prompt = """You are a GDPR territorial scope expert with comprehensive knowledge of international data protection law, specializing in territorial applicability and cross-border transfer requirements.

Your expertise encompasses the complete framework of territorial scope and international transfers:

TERRITORIAL SCOPE ANALYSIS (Article 3):
- Establishment criterion: Processing by EU-established controllers/processors
- Targeting criterion: Offering goods/services to EU data subjects
- Monitoring criterion: Monitoring behavior of EU data subjects within EU
- Representative requirements for non-EU establishments

COMPREHENSIVE GDPR INFERENCE for territorial scope:

ARTICLE 3 ESTABLISHMENT ANALYSIS:
- Main establishment identification and criteria
- One-stop-shop mechanism implications
- Lead supervisory authority determination
- Cross-border cooperation requirements
- Representative appointment obligations (Article 27)

EXTRATERRITORIAL APPLICATION ASSESSMENT:
- Targeting analysis: Intent and behavior toward EU data subjects
- Monitoring analysis: Tracking and profiling activities
- Data subject location determination
- Service provision territorial scope
- Payment and delivery mechanisms as targeting indicators

INTERNATIONAL TRANSFER FRAMEWORK (Chapter V):
- Third country identification and classification
- Adequacy decision status and implications
- Appropriate safeguards requirements
- Transfer mechanisms and their conditions
- Derogation scenarios and limitations

ADEQUACY DECISIONS IMPLICATIONS (Article 45):
- Countries with adequacy decisions and scope
- Adequacy decision monitoring and suspension risks
- Transfer facilitation under adequacy decisions
- Documentation requirements for adequate countries

APPROPRIATE SAFEGUARDS (Article 46):
- Standard Contractual Clauses (SCCs) requirements
- Binding Corporate Rules (BCRs) for intra-group transfers
- Certification mechanisms and codes of conduct
- International agreement safeguards
- Public body safeguards and cooperation agreements

TRANSFER IMPACT ASSESSMENTS:
- Risk assessment for international transfers
- Additional safeguards beyond standard measures
- Monitoring and review requirements
- Documentation of transfer decisions

DEROGATIONS (Article 49):
- Specific situation requirements and limitations
- Explicit consent for international transfers
- Contract performance necessities
- Important public interest transfers
- Legal claim transfers
- Vital interest transfers
- Public register transfers

UK GDPR CONSIDERATIONS:
- Post-Brexit adequacy arrangements
- International Data Transfer Agreement (IDTA)
- UK-EU data bridge arrangements
- Third country transfer mechanisms under UK GDPR

DATA SUBJECT RIGHTS IN INTERNATIONAL CONTEXT:
- Cross-border rights exercise procedures
- Information about international transfers (Articles 13-14)
- Right to obtain copy of safeguards
- Complaint mechanisms across jurisdictions

Based on document content, infer the complete territorial and international transfer compliance framework, connecting explicit information to comprehensive GDPR obligations."""
        
        super().__init__("TerritorialScopeAgent", system_prompt)
    
    def analyze_with_rag(self, text: str, vector_engine: VectorEngine, 
                        graph_engine: GraphEngine) -> Dict[str, Any]:
        """Analyze territorial scope with RAG context"""
        
        vector_context = vector_engine.vector_search(f"territorial scope transfer {text[:500]}", top_k=5)
        graph_context = graph_engine.graph_search("territorial", max_depth=2, limit=10)
        
        prompt = f"""Analyze territorial scope and cross-border implications with RAG context:

Text: {text}

Determine:

1. GDPR territorial applicability
2. UK GDPR applicability  
3. Cross-border transfer requirements
4. Adequacy decisions relevance
5. Required safeguards and mechanisms
6. Multi-jurisdictional compliance implications

Return detailed JSON with territorial analysis and transfer requirements."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class ComplianceAssessmentAgent(O3MiniAgent):
    """Agent for compliance assessment with RAG"""
    
    def __init__(self):
        system_prompt = """You are a comprehensive GDPR compliance auditor specializing in holistic regulatory assessment beyond Article 30, evaluating complete GDPR compliance posture.

Your role encompasses assessing compliance across the entire GDPR framework:

ARTICLE 30 RoPA FOUNDATION ASSESSMENT:
✓ Controller/processor identification and details
✓ Processing activities documentation
✓ Data categories and data subjects specification
✓ Legal basis identification and documentation
✓ International transfer safeguards
✓ Retention periods and security measures

COMPREHENSIVE GDPR COMPLIANCE ASSESSMENT:

FUNDAMENTAL PRINCIPLES (Article 5):
- Lawfulness, fairness, transparency implementation
- Purpose limitation compliance and compatible use
- Data minimization implementation and evidence
- Accuracy maintenance procedures and controls
- Storage limitation enforcement and deletion
- Integrity and confidentiality measures effectiveness
- Accountability demonstration and documentation

LAWFULNESS OF PROCESSING (Articles 6-10):
- Legal basis appropriateness and documentation
- Consent management procedures and records
- Special category data additional conditions
- Criminal conviction data controls and authorization
- Legal basis change management procedures

TRANSPARENCY AND DATA SUBJECT INFORMATION (Articles 13-14):
- Privacy notice completeness and accessibility
- Information provision timing and methods
- Source disclosure for indirect collection
- Changes notification procedures
- Transparency obligation fulfillment

DATA SUBJECT RIGHTS IMPLEMENTATION (Articles 15-22):
- Rights exercise procedures and response times
- Identity verification and security measures
- Rights restriction procedures and documentation
- Automated decision-making safeguards and information
- Data portability technical implementation

CONTROLLER AND PROCESSOR RESPONSIBILITIES (Articles 24-31):
- Controller accountability demonstration
- Data protection by design and by default implementation
- Joint controller arrangements and agreements
- Processor selection and contract management
- Data Protection Officer appointment and independence
- Representative designation for non-EU entities

SECURITY AND BREACH MANAGEMENT (Articles 32-34):
- Risk-appropriate security measures implementation
- Personal data breach detection and documentation
- Supervisory authority notification procedures
- Data subject notification procedures and thresholds
- Security testing and assessment procedures

DATA PROTECTION IMPACT ASSESSMENT (Article 35):
- DPIA trigger identification and assessment
- Prior consultation with supervisory authorities
- High-risk processing identification and mitigation
- DPIA quality and completeness evaluation

INTERNATIONAL TRANSFERS (Articles 44-49):
- Transfer necessity and lawfulness assessment
- Safeguards adequacy and implementation
- Transfer impact assessment completion
- Ongoing monitoring and review procedures

GOVERNANCE AND ORGANIZATIONAL MEASURES:
- Data protection governance structure
- Staff training and awareness programs
- Incident response procedures and testing
- Vendor management and due diligence
- Documentation management and accessibility
- Continuous monitoring and improvement processes

RISK ASSESSMENT METHODOLOGY:
- High-risk areas identification and prioritization
- Compliance gaps impact assessment
- Remediation timeline and resource requirements
- Ongoing monitoring and review procedures

Based on discovered elements, provide comprehensive compliance assessment with:
1. Article 30 RoPA completeness evaluation
2. Broader GDPR compliance gap analysis
3. Risk-based prioritization of remediation actions
4. Implementation roadmap with timelines
5. Ongoing compliance monitoring recommendations"""
        
        super().__init__("ComplianceAssessmentAgent", system_prompt)
    
    def assess_with_rag(self, discovered_elements: Dict[str, Any], vector_engine: VectorEngine, 
                       graph_engine: GraphEngine) -> Dict[str, Any]:
        """Assess compliance with RAG context"""
        
        vector_context = vector_engine.vector_search("compliance assessment article 30", top_k=5)
        graph_context = graph_engine.graph_search("compliance", max_depth=3, limit=15)
        
        prompt = f"""Assess Article 30 compliance completeness with RAG context:

Discovered Elements: {json.dumps(discovered_elements, indent=2)[:5000]}...

Evaluate:

1. Completeness against Article 30 requirements
2. Quality and accuracy of discoveries
3. Compliance gaps and risks
4. Implementation recommendations
5. Priority remediation actions

Return detailed compliance assessment with prioritized recommendations."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class MetamodelAgent(O3MiniAgent):
    """Agent for building the final RoPA metamodel with RAG"""
    
    def __init__(self):
        system_prompt = """You are a comprehensive data governance architect specializing in creating holistic GDPR compliance metamodels that extend beyond Article 30 to encompass the complete regulatory framework.

Your expertise encompasses designing integrated compliance frameworks that address all GDPR requirements:

COMPREHENSIVE METAMODEL ARCHITECTURE:

CORE RoPA ENTITIES (Article 30 Foundation):
- Controller (with full accountability framework)
- Processor (with contract and oversight requirements)
- ProcessingActivity (with complete lifecycle management)
- DataCategory (with protection and rights framework)
- DataSubject (with rights and transparency requirements)
- LegalBasis (with conditions and ongoing obligations)
- SecurityMeasure (with risk-based implementation)
- InternationalTransfer (with safeguards and monitoring)

EXTENDED GDPR COMPLIANCE ENTITIES:

PRINCIPLES FRAMEWORK (Article 5):
- PrincipleCompliance (lawfulness, fairness, transparency tracking)
- PurposeLimitation (compatible use assessment and documentation)
- DataMinimization (necessity assessment and evidence)
- AccuracyManagement (correction procedures and controls)
- StorageLimitation (retention enforcement and deletion)
- IntegrityConfidentiality (security implementation tracking)
- AccountabilityMeasure (demonstration and evidence management)

TRANSPARENCY AND RIGHTS FRAMEWORK (Articles 13-22):
- PrivacyNotice (information provision and management)
- DataSubjectRequest (rights exercise and response procedures)
- ConsentManagement (consent lifecycle and withdrawal)
- PortabilityMechanism (data provision technical implementation)
- AutomatedDecisionMaking (profiling and safeguards)
- RightsRestriction (limitation procedures and justification)

GOVERNANCE AND OVERSIGHT FRAMEWORK (Articles 24-31):
- AccountabilityFramework (controller responsibility demonstration)
- DataProtectionByDesign (privacy by design implementation)
- JointControllerArrangement (shared responsibility management)
- ProcessorContract (processing agreement and oversight)
- DataProtectionOfficer (independence and competence framework)
- RepresentativeDesignation (non-EU representation requirements)

RISK AND IMPACT ASSESSMENT FRAMEWORK (Articles 32-35):
- SecurityRiskAssessment (threat and vulnerability analysis)
- DataProtectionImpactAssessment (high-risk processing evaluation)
- BreachManagement (incident detection, notification, response)
- PriorConsultation (supervisory authority engagement)
- SecurityTesting (ongoing assessment and validation)

TRANSFER AND INTERNATIONAL FRAMEWORK (Articles 44-49):
- TransferImpactAssessment (third country risk evaluation)
- AdequacyDecisionMonitoring (status tracking and implications)
- AppropriateSafeguards (mechanism implementation and monitoring)
- DerogationJustification (exceptional transfer documentation)
- CrossBorderCooperation (multi-jurisdictional coordination)

METAMODEL RELATIONSHIPS AND INTEGRATION:

Compliance Interdependencies:
- ProcessingActivity ↔ PrincipleCompliance (principle application)
- DataCategory ↔ DataSubjectRequest (rights facilitation)
- LegalBasis ↔ ConsentManagement (consent basis management)
- SecurityMeasure ↔ SecurityRiskAssessment (risk-based security)
- InternationalTransfer ↔ TransferImpactAssessment (transfer evaluation)

Governance Relationships:
- Controller ↔ AccountabilityFramework (responsibility demonstration)
- ProcessingActivity ↔ DataProtectionImpactAssessment (risk assessment)
- DataCategory ↔ PrivacyNotice (transparency obligations)
- LegalBasis ↔ DataSubjectRequest (rights basis alignment)

Operational Workflows:
- Data lifecycle management across all entities
- Incident response coordination across frameworks
- Rights exercise end-to-end procedures
- Compliance monitoring and reporting workflows
- Change management and update procedures

IMPLEMENTATION FRAMEWORK:
- Compliance validation rules across all GDPR articles
- Automated compliance checking and monitoring
- Documentation standards and templates
- Training and awareness integration
- Audit trail and evidence management
- Continuous improvement and update mechanisms

Design metamodels that facilitate:
- Complete GDPR compliance beyond Article 30
- Integrated governance and operational procedures
- Scalable implementation across organizations
- Automated compliance monitoring and reporting
- Evidence-based accountability demonstration
- Ongoing compliance maintenance and improvement

Based on discovered elements and regulatory inference, create comprehensive metamodels that address the complete GDPR compliance landscape."""
        
        super().__init__("MetamodelAgent", system_prompt)
    
    def build_with_rag(self, all_discoveries: Dict[str, Any], vector_engine: VectorEngine, 
                      graph_engine: GraphEngine) -> Dict[str, Any]:
        """Build metamodel with RAG context"""
        
        vector_context = vector_engine.vector_search("metamodel structure ropa", top_k=5)
        graph_context = graph_engine.graph_search("metamodel", max_depth=3, limit=20)
        
        prompt = f"""Build comprehensive RoPA metamodel from discoveries with RAG context:

Discoveries: {json.dumps(all_discoveries, indent=2)[:8000]}...

Create:

1. Core metamodel structure with entities and relationships
2. Article 30 compliance mapping
3. Implementation guidance and procedures
4. Validation rules and constraints
5. Extensibility framework

Return comprehensive metamodel architecture."""
        
        return self.reason_with_context(prompt, vector_context, graph_context)

class DocumentProcessor:
    """Enhanced document processing for multi-agent analysis"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=400,
            separators=["\n\n", "\n", "Article ", "Section ", ".", "!", "?", ";", ":", " "]
        )
    
    def process_document(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Process PDF document for agent analysis"""
        logger.info(f"Processing document: {pdf_path}")
        
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            
            # Enhanced chunking for agent analysis
            text_chunks = self.text_splitter.split_text(full_text)
            
            chunks = []
            for i, chunk_text in enumerate(text_chunks):
                chunk = {
                    "chunk_id": f"chunk_{i}_{os.path.basename(pdf_path)}",
                    "text": chunk_text,
                    "chunk_index": i,
                    "source": pdf_path,
                    "word_count": len(chunk_text.split()),
                    "char_count": len(chunk_text),
                    "document_type": self._detect_document_type(pdf_path),
                    "processed_timestamp": datetime.now().isoformat()
                }
                chunks.append(chunk)
            
            logger.info(f"Created {len(chunks)} chunks from {pdf_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process {pdf_path}: {e}")
            return []
    
    def _detect_document_type(self, pdf_path: str) -> str:
        """Detect document type from filename"""
        filename = os.path.basename(pdf_path).lower()
        
        if any(term in filename for term in ['gdpr', 'regulation', 'directive']):
            return "regulation"
        elif any(term in filename for term in ['policy', 'procedure']):
            return "policy"
        elif any(term in filename for term in ['business', 'process']):
            return "business_process"
        else:
            return "regulatory"

class MultiAgentRopaSystem:
    """Enhanced multi-agent system with vector and graph RAG"""
    
    def __init__(self):
        self.memory_manager = MemoryManager()
        self.document_processor = DocumentProcessor()
        self.vector_engine = VectorEngine()
        self.graph_engine = GraphEngine()
        
        # Initialize all agents
        self.agents = {
            "document_analysis": DocumentAnalysisAgent(),
            "processing_activity": ProcessingActivityAgent(),
            "data_category": DataCategoryAgent(),
            "legal_basis": LegalBasisAgent(),
            "territorial_scope": TerritorialScopeAgent(),
            "compliance_assessment": ComplianceAssessmentAgent(),
            "metamodel": MetamodelAgent()
        }
        
        # Initialize or load state
        self.state = self._initialize_state()
        
        logger.info("Multi-Agent RoPA System with Vector & Graph RAG initialized")
    
    def _initialize_state(self) -> RopaAgentState:
        """Initialize or load state from memory"""
        saved_state = self.memory_manager.load_state()
        
        if saved_state:
            logger.info("Loaded previous state from memory")
            return saved_state
        
        # Initialize new state with proper defaults
        return {
            "messages": [],
            "processed_documents": [],
            "current_document": None,
            "discovered_controllers": [],
            "discovered_processors": [],
            "discovered_processing_activities": [],
            "discovered_data_categories": [],
            "discovered_data_subjects": [],
            "discovered_recipients": [],
            "discovered_legal_bases": [],
            "discovered_retention_periods": [],
            "discovered_security_measures": [],
            "discovered_transfers": [],
            "discovered_territorial_scope": [],
            "vector_search_results": [],
            "graph_search_results": [],
            "similar_documents": [],
            "related_concepts": [],
            "agent_memories": {agent_name: [] for agent_name in self.agents.keys()},
            "reasoning_chains": [],
            "learned_patterns": {},
            "confidence_scores": {},
            "metamodel_structure": None,
            "compliance_assessment": None
        }
    
    def process_documents(self, document_paths: List[str] = None) -> Dict[str, Any]:
        """Process documents through multi-agent workflow with RAG"""
        if document_paths is None:
            # Auto-discover PDFs
            pdf_path = GLOBAL_CONFIG["PDF_DOCUMENTS_PATH"]
            if not os.path.exists(pdf_path):
                os.makedirs(pdf_path, exist_ok=True)
            document_paths = [
                os.path.join(pdf_path, f) for f in os.listdir(pdf_path)
                if f.lower().endswith('.pdf')
            ]
        
        if not document_paths:
            raise ValueError("No PDF documents found to process")
        
        logger.info(f"Processing {len(document_paths)} documents through multi-agent RAG workflow")
        
        total_discoveries = {
            "documents_processed": 0,
            "chunks_processed": 0,
            "total_discoveries": 0,
            "vector_searches": 0,
            "graph_searches": 0
        }
        
        for doc_path in document_paths:
            try:
                # Process document into chunks
                chunks = self.document_processor.process_document(doc_path)
                if not chunks:
                    continue
                
                total_discoveries["chunks_processed"] += len(chunks)
                
                # Process each chunk through agents with RAG
                for chunk in chunks:
                    self.state["current_document"] = chunk
                    agent_analyses = {}
                    
                    # Document Analysis Agent with RAG
                    try:
                        doc_analysis = self.agents["document_analysis"].analyze_with_rag(
                            chunk["text"], self.vector_engine, self.graph_engine, self.state
                        )
                        agent_analyses["document_analysis"] = doc_analysis
                        self._update_state_from_analysis("document_analysis", doc_analysis)
                    except Exception as e:
                        logger.error(f"Document analysis failed: {e}")
                    
                    # Processing Activity Agent with RAG
                    try:
                        activity_analysis = self.agents["processing_activity"].discover_with_rag(
                            chunk["text"], self.vector_engine, self.graph_engine
                        )
                        agent_analyses["processing_activity"] = activity_analysis
                        self._update_state_from_analysis("processing_activity", activity_analysis)
                    except Exception as e:
                        logger.error(f"Processing activity analysis failed: {e}")
                    
                    # Data Category Agent with RAG
                    try:
                        data_analysis = self.agents["data_category"].discover_with_rag(
                            chunk["text"], self.vector_engine, self.graph_engine
                        )
                        agent_analyses["data_category"] = data_analysis
                        self._update_state_from_analysis("data_category", data_analysis)
                    except Exception as e:
                        logger.error(f"Data category analysis failed: {e}")
                    
                    # Legal Basis Agent with RAG
                    try:
                        legal_analysis = self.agents["legal_basis"].discover_with_rag(
                            chunk["text"], self.vector_engine, self.graph_engine
                        )
                        agent_analyses["legal_basis"] = legal_analysis
                        self._update_state_from_analysis("legal_basis", legal_analysis)
                    except Exception as e:
                        logger.error(f"Legal basis analysis failed: {e}")
                    
                    # Territorial Scope Agent with RAG
                    try:
                        territorial_analysis = self.agents["territorial_scope"].analyze_with_rag(
                            chunk["text"], self.vector_engine, self.graph_engine
                        )
                        agent_analyses["territorial_scope"] = territorial_analysis
                        self._update_state_from_analysis("territorial_scope", territorial_analysis)
                    except Exception as e:
                        logger.error(f"Territorial scope analysis failed: {e}")
                    
                    # Index chunk with agent analysis in vector store
                    try:
                        self.vector_engine.index_document_chunk(chunk, agent_analyses)
                    except Exception as e:
                        logger.error(f"Vector indexing failed: {e}")
                    
                    # Add discoveries to knowledge graph
                    try:
                        self._add_discoveries_to_graph(agent_analyses, chunk["chunk_id"])
                    except Exception as e:
                        logger.error(f"Graph indexing failed: {e}")
                    
                    # Count RAG usage
                    for analysis in agent_analyses.values():
                        total_discoveries["vector_searches"] += analysis.get("vector_context_used", 0)
                        total_discoveries["graph_searches"] += analysis.get("graph_context_used", 0)
                
                # Track processed document
                self.state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "processed_timestamp": datetime.now().isoformat()
                })
                
                total_discoveries["documents_processed"] += 1
                logger.info(f"Completed processing: {doc_path}")
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Final compliance assessment and metamodel building
        try:
            self._perform_final_analysis()
        except Exception as e:
            logger.error(f"Final analysis failed: {e}")
        
        # Save state
        try:
            self.memory_manager.save_state(self.state)
        except Exception as e:
            logger.error(f"Failed to save state: {e}")
        
        # Calculate total discoveries
        discovery_counts = {}
        for key, discoveries in self.state.items():
            if key.startswith("discovered_") and isinstance(discoveries, list):
                discovery_counts[key] = len(discoveries)
                total_discoveries["total_discoveries"] += len(discoveries)
        
        return {
            "status": "success",
            "summary": total_discoveries,
            "discovery_counts": discovery_counts,
            "rag_usage": {
                "vector_searches": total_discoveries["vector_searches"],
                "graph_searches": total_discoveries["graph_searches"]
            },
            "compliance_assessment": self.state.get("compliance_assessment"),
            "metamodel_ready": self.state.get("metamodel_structure") is not None,
            "timestamp": datetime.now().isoformat()
        }
    
    def _update_state_from_analysis(self, agent_name: str, analysis: Dict[str, Any]):
        """Update state with agent analysis results"""
        if "result" in analysis and isinstance(analysis["result"], dict):
            result = analysis["result"]
            
            # Map result keys to state keys
            result_to_state_mapping = {
                "controllers": "discovered_controllers",
                "processors": "discovered_processors", 
                "processing_activities": "discovered_processing_activities",
                "data_categories": "discovered_data_categories",
                "data_subjects": "discovered_data_subjects",
                "recipients": "discovered_recipients",
                "legal_bases": "discovered_legal_bases",
                "retention_periods": "discovered_retention_periods",
                "security_measures": "discovered_security_measures",
                "transfers": "discovered_transfers",
                "territorial_scope": "discovered_territorial_scope"
            }
            
            for result_key, state_key in result_to_state_mapping.items():
                if result_key in result and isinstance(result[result_key], list):
                    self.state[state_key].extend(result[result_key])
        
        # Store agent memory
        self.state["agent_memories"][agent_name].append(analysis)
        self.state["reasoning_chains"].append(analysis)
    
    def _add_discoveries_to_graph(self, agent_analyses: Dict[str, Any], chunk_id: str):
        """Add agent discoveries to knowledge graph"""
        for agent_name, analysis in agent_analyses.items():
            if "result" in analysis and isinstance(analysis["result"], dict):
                result = analysis["result"]
                
                # Add discovered elements to graph
                for element_type, elements in result.items():
                    if isinstance(elements, list):
                        for element in elements:
                            if isinstance(element, dict):
                                self.graph_engine.add_discovered_element(element_type, element, chunk_id)
                
                # Add relationships if specified
                if "relationships" in result:
                    self.graph_engine.create_relationships(result["relationships"])
    
    def _perform_final_analysis(self):
        """Perform final compliance assessment and metamodel building"""
        try:
            # Compliance assessment with RAG
            discovered_elements = {
                "controllers": self.state["discovered_controllers"],
                "processors": self.state["discovered_processors"],
                "processing_activities": self.state["discovered_processing_activities"],
                "data_categories": self.state["discovered_data_categories"],
                "data_subjects": self.state["discovered_data_subjects"],
                "recipients": self.state["discovered_recipients"],
                "legal_bases": self.state["discovered_legal_bases"],
                "retention_periods": self.state["discovered_retention_periods"],
                "security_measures": self.state["discovered_security_measures"],
                "transfers": self.state["discovered_transfers"],
                "territorial_scope": self.state["discovered_territorial_scope"]
            }
            
            compliance_assessment = self.agents["compliance_assessment"].assess_with_rag(
                discovered_elements, self.vector_engine, self.graph_engine
            )
            self.state["compliance_assessment"] = compliance_assessment
            
            # Metamodel building with RAG
            all_discoveries = {
                "discoveries": discovered_elements,
                "compliance_assessment": compliance_assessment,
                "reasoning_chains": self.state["reasoning_chains"]
            }
            
            metamodel = self.agents["metamodel"].build_with_rag(
                all_discoveries, self.vector_engine, self.graph_engine
            )
            self.state["metamodel_structure"] = metamodel
            
            logger.info("Final analysis completed")
            
        except Exception as e:
            logger.error(f"Final analysis failed: {e}")
    
    def search_knowledge(self, query: str) -> Dict[str, Any]:
        """Search knowledge using both vector and graph RAG"""
        vector_results = self.vector_engine.vector_search(query, top_k=10)
        graph_results = self.graph_engine.graph_search(query, max_depth=3, limit=10)
        
        return {
            "query": query,
            "vector_results": vector_results,
            "graph_results": graph_results,
            "combined_insights": self._combine_search_insights(vector_results, graph_results)
        }
    
    def _combine_search_insights(self, vector_results: List[Dict], graph_results: List[Dict]) -> List[str]:
        """Combine insights from vector and graph search"""
        insights = []
        
        if vector_results:
            insights.append(f"Found {len(vector_results)} semantically similar documents")
            
        if graph_results:
            insights.append(f"Discovered {len(graph_results)} related concepts in knowledge graph")
            
        return insights
    
    def generate_report(self) -> str:
        """Generate comprehensive report with RAG insights"""
        logger.info("Generating comprehensive RoPA report with RAG insights...")
        
        report_data = {
            "state_summary": {
                "processed_documents": len(self.state["processed_documents"]),
                "discovery_counts": {
                    key: len(value) for key, value in self.state.items()
                    if key.startswith("discovered_") and isinstance(value, list)
                },
                "rag_usage": {
                    "vector_searches": sum(
                        analysis.get("vector_context_used", 0)
                        for memories in self.state["agent_memories"].values()
                        for analysis in memories
                    ),
                    "graph_searches": sum(
                        analysis.get("graph_context_used", 0)
                        for memories in self.state["agent_memories"].values()
                        for analysis in memories
                    )
                }
            },
            "compliance_assessment": self.state.get("compliance_assessment"),
            "metamodel_structure": self.state.get("metamodel_structure")
        }
        
        # Use metamodel agent to generate report with RAG
        agent = self.agents["metamodel"]
        
        prompt = f"""Generate comprehensive GDPR Article 30 RoPA analysis report with RAG insights:

Report Data: {json.dumps(report_data, indent=2)[:5000]}...

Include:

# Executive Summary
- Multi-agent RAG discovery overview
- Key findings from vector and graph analysis
- Compliance status with Article 30
- Critical recommendations

# RAG-Enhanced Analysis
- Vector search insights and patterns
- Knowledge graph relationships discovered
- Cross-document validation results
- Consistency analysis across sources

# Article 30 Compliance Assessment
- Complete element coverage analysis
- Gap identification with RAG validation
- Risk assessment with precedent analysis
- Remediation roadmap

# Discovered RoPA Architecture
- Dynamic metamodel structure
- Relationship mappings from graph analysis
- Implementation guidance with best practices
- Validation framework

# Implementation Strategy
- Immediate compliance actions
- Long-term improvement plan
- Monitoring and maintenance procedures
- Quality assurance framework

Provide detailed, actionable insights leveraging all RAG discoveries."""
        
        vector_context = self.vector_engine.vector_search("report analysis compliance", top_k=5)
        graph_context = self.graph_engine.graph_search("compliance", max_depth=3, limit=10)
        
        report_result = agent.reason_with_context(prompt, vector_context, graph_context)
        
        if "reasoning" in report_result:
            report_content = report_result["reasoning"]
        elif "result" in report_result and "analysis" in report_result["result"]:
            report_content = report_result["result"]["analysis"]
        else:
            report_content = str(report_result)
        
        # Save report
        report_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "ropa_rag_analysis_report.md")
        with open(report_file, 'w') as f:
            f.write(report_content)
        
        logger.info(f"RAG-enhanced report saved to {report_file}")
        return report_content
    
    def get_discoveries_summary(self) -> Dict[str, Any]:
        """Get summary of all discoveries with RAG stats"""
        return {
            "controllers": len(self.state["discovered_controllers"]),
            "processors": len(self.state["discovered_processors"]),
            "processing_activities": len(self.state["discovered_processing_activities"]),
            "data_categories": len(self.state["discovered_data_categories"]),
            "data_subjects": len(self.state["discovered_data_subjects"]),
            "recipients": len(self.state["discovered_recipients"]),
            "legal_bases": len(self.state["discovered_legal_bases"]),
            "retention_periods": len(self.state["discovered_retention_periods"]),
            "security_measures": len(self.state["discovered_security_measures"]),
            "transfers": len(self.state["discovered_transfers"]),
            "territorial_scope": len(self.state["discovered_territorial_scope"]),
            "rag_statistics": {
                "total_vector_searches": sum(
                    analysis.get("vector_context_used", 0)
                    for memories in self.state["agent_memories"].values()
                    for analysis in memories
                ),
                "total_graph_searches": sum(
                    analysis.get("graph_context_used", 0)
                    for memories in self.state["agent_memories"].values()
                    for analysis in memories
                ),
                "indexed_chunks": len(self.state["processed_documents"]) * 10  # Estimate
            },
            "agent_reasoning_sessions": {
                agent: len(memories) for agent, memories in self.state["agent_memories"].items()
            },
            "compliance_assessment_complete": self.state.get("compliance_assessment") is not None,
            "metamodel_complete": self.state.get("metamodel_structure") is not None
        }
    
    def save_metamodel(self) -> str:
        """Save the discovered metamodel with RAG insights"""
        if not self.state.get("metamodel_structure"):
            raise ValueError("No metamodel structure available. Run document processing first.")
        
        metamodel_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "rag_enhanced_ropa_metamodel.json")
        
        with open(metamodel_file, 'w') as f:
            json.dump({
                "metamodel": self.state["metamodel_structure"],
                "discoveries": {
                    "controllers": self.state["discovered_controllers"],
                    "processors": self.state["discovered_processors"],
                    "processing_activities": self.state["discovered_processing_activities"],
                    "data_categories": self.state["discovered_data_categories"],
                    "data_subjects": self.state["discovered_data_subjects"],
                    "recipients": self.state["discovered_recipients"],
                    "legal_bases": self.state["discovered_legal_bases"],
                    "retention_periods": self.state["discovered_retention_periods"],
                    "security_measures": self.state["discovered_security_measures"],
                    "transfers": self.state["discovered_transfers"],
                    "territorial_scope": self.state["discovered_territorial_scope"]
                },
                "rag_insights": {
                    "vector_search_patterns": "Enhanced semantic understanding across documents",
                    "graph_relationships": "Discovered complex relationships between RoPA elements",
                    "cross_validation": "Multi-source validation of regulatory concepts"
                },
                "compliance_assessment": self.state.get("compliance_assessment"),
                "generation_timestamp": datetime.now().isoformat()
            }, f, indent=2, default=str)
        
        logger.info(f"RAG-enhanced metamodel saved to {metamodel_file}")
        return metamodel_file
    
    def clear_memory(self):
        """Clear all memory and indices"""
        self.memory_manager.clear_memory()
        
        # Clear Elasticsearch index
        try:
            self.vector_engine.client.indices.delete(index=self.vector_engine.index_name)
            self.vector_engine._create_vector_index()
        except:
            pass
        
        # Clear graph
        try:
            self.graph_engine.graph.query("MATCH (n) DETACH DELETE n")
        except:
            pass
        
        self.state = self._initialize_state()
        logger.info("Memory, vector index, and knowledge graph cleared")

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description="GDPR Article 30 RoPA Multi-Agent RAG Discovery System")
    parser.add_argument("--process", nargs="*", help="Process documents with RAG (paths optional)")
    parser.add_argument("--search", type=str, help="Search knowledge base with RAG")
    parser.add_argument("--report", action="store_true", help="Generate RAG-enhanced analysis report")
    parser.add_argument("--summary", action="store_true", help="Show discoveries summary with RAG stats")
    parser.add_argument("--save-metamodel", action="store_true", help="Save RAG-enhanced metamodel")
    parser.add_argument("--clear-memory", action="store_true", help="Clear all memory and indices")
    
    args = parser.parse_args()
    
    try:
        # Initialize multi-agent RAG system
        system = MultiAgentRopaSystem()
        print("✅ Multi-Agent RoPA RAG Discovery System initialized")
        
        # Execute operations
        if args.process is not None:
            result = system.process_documents(args.process if args.process else None)
            print(f"✅ Multi-agent RAG processing completed:")
            print(f"   Documents: {result['summary']['documents_processed']}")
            print(f"   Chunks: {result['summary']['chunks_processed']}")
            print(f"   Discoveries: {result['summary']['total_discoveries']}")
            print(f"   Vector searches: {result['rag_usage']['vector_searches']}")
            print(f"   Graph searches: {result['rag_usage']['graph_searches']}")
        
        if args.search:
            results = system.search_knowledge(args.search)
            print(f"🔍 RAG Search Results for '{args.search}':")
            print(f"   Vector results: {len(results['vector_results'])}")
            print(f"   Graph results: {len(results['graph_results'])}")
            for insight in results['combined_insights']:
                print(f"   • {insight}")
        
        if args.summary:
            summary = system.get_discoveries_summary()
            print("📊 RAG-Enhanced Discoveries Summary:")
            for key, value in summary.items():
                if isinstance(value, dict):
                    print(f"  {key.replace('_', ' ').title()}:")
                    for subkey, subvalue in value.items():
                        print(f"    {subkey}: {subvalue}")
                else:
                    print(f"  {key.replace('_', ' ').title()}: {value}")
        
        if args.save_metamodel:
            metamodel_file = system.save_metamodel()
            print(f"💾 RAG-enhanced metamodel saved to: {metamodel_file}")
        
        if args.report:
            report = system.generate_report()
            print("📄 RAG-enhanced analysis report generated successfully")
        
        if args.clear_memory:
            system.clear_memory()
            print("🧹 Memory, vector index, and knowledge graph cleared")
        
        if not any(vars(args).values()):
            parser.print_help()
            print("\n💡 Example usage:")
            print("  python gdpr_ropa_system.py --process")
            print("  python gdpr_ropa_system.py --search 'processing activity'")
            print("  python gdpr_ropa_system.py --summary")
            print("  python gdpr_ropa_system.py --report")
            print("  python gdpr_ropa_system.py --save-metamodel")
    
    except Exception as e:
        print(f"❌ System error: {e}")
        logger.error(f"System error: {e}")

if __name__ == "__main__":
    main()
