import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
import io
from typing import List, Dict, Optional, Union, Any, Tuple
import warnings
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt
import base64
from pathlib import Path

# Suppress warnings
warnings.filterwarnings('ignore')

# Configure Streamlit page
st.set_page_config(
    page_title="Excel Analysis Dashboard",
    page_icon="📊",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .stButton > button {
        width: 100%;
    }
    .main > div {
        padding-top: 1rem;
    }
    .block-container {
        padding-top: 2rem;
    }
    .stProgress > div > div > div > div {
        background-color: #00cc00;
    }
    .styled-metric {
        padding: 10px;
        border-radius: 5px;
        background-color: #f0f2f6;
        margin: 5px;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

class DataValidator:
    """Class to handle data validation and error checking."""
    
    @staticmethod
    def validate_file(file) -> bool:
        """Validate uploaded file."""
        try:
            if file is None:
                return False
            
            # Check file size (max 200MB)
            if file.size > 200 * 1024 * 1024:
                st.error(f"File {file.name} is too large. Maximum size is 200MB.")
                return False
            
            # Check file extension
            if not file.name.lower().endswith(('.xlsx', '.xls')):
                st.error(f"File {file.name} is not an Excel file.")
                return False
            
            return True
        except Exception as e:
            st.error(f"Error validating file {file.name}: {str(e)}")
            return False
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        """Validate DataFrame structure."""
        try:
            if df is None or df.empty:
                st.error("DataFrame is empty or None.")
                return False
            
            if len(df.columns) == 0:
                st.error("DataFrame has no columns.")
                return False
            
            if len(df) == 0:
                st.error("DataFrame has no rows.")
                return False
            
            return True
        except Exception as e:
            st.error(f"Error validating DataFrame: {str(e)}")
            return False

class DataLoader:
    """Class to handle data loading and initial processing."""
    
    def __init__(self):
        self.validator = DataValidator()
    
    def load_excel(self, file) -> Optional[pd.DataFrame]:
        """Load Excel file with comprehensive error handling."""
        try:
            if not self.validator.validate_file(file):
                return None
            
            # Read Excel file with error handling
            df = pd.read_excel(
                file,
                engine='openpyxl',
                na_values=['NA', 'N/A', '', ' ']  # Handle common NA values
            )
            
            if not self.validator.validate_dataframe(df):
                return None
            
            # Clean and process DataFrame
            df = self._process_dataframe(df)
            
            return df
            
        except Exception as e:
            st.error(f"Error loading {file.name}: {str(e)}")
            return None
    
    def _process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process and clean DataFrame."""
        try:
            # Clean column names
            df.columns = df.columns.str.strip().str.replace(' ', '_')
            
            # Handle each column
            for col in df.columns:
                # Handle datetime columns
                if df[col].dtype == 'object':
                    try:
                        # Try to convert to datetime
                        df[col] = pd.to_datetime(df[col], errors='ignore')
                    except:
                        pass
                
                # Try converting string numbers to numeric
                if df[col].dtype == 'object':
                    try:
                        numeric_col = pd.to_numeric(df[col], errors='coerce')
                        if numeric_col.notna().mean() > 0.8:  # If >80% are valid numbers
                            df[col] = numeric_col
                    except:
                        pass
                
                # Convert boolean columns to int
                if df[col].dtype == bool:
                    df[col] = df[col].astype(int)
            
            return df
            
        except Exception as e:
            st.error(f"Error processing DataFrame: {str(e)}")
            return df

class DataProcessor:
    """Class to handle data merging, transformations, and cleaning with comprehensive reporting."""
    
    def __init__(self):
        self.cleaning_report = {
            'missing_values': {},
            'outliers': {},
            'duplicates': {},
            'summary': {}
        }

    @staticmethod
    def validate_merge_keys(dataframes: List[pd.DataFrame], merge_keys: List[str]) -> bool:
        """Validate merge keys exist in all dataframes and have compatible types."""
        try:
            if not dataframes or not merge_keys:
                st.error("No dataframes or merge keys provided.")
                return False
            
            # Check keys exist in all dataframes
            for idx, df in enumerate(dataframes):
                missing_keys = [key for key in merge_keys if key not in df.columns]
                if missing_keys:
                    st.error(f"Keys {missing_keys} not found in DataFrame {idx + 1}")
                    return False
            
            # Check key types are compatible
            base_df = dataframes[0]
            for key in merge_keys:
                base_type = base_df[key].dtype
                for idx, df in enumerate(dataframes[1:], 1):
                    curr_type = df[key].dtype
                    if not pd.api.types.is_dtype_equal(base_type, curr_type):
                        st.warning(f"Column '{key}' has different types: {base_type} vs {curr_type} in DataFrame {idx + 1}")
                        # Try converting to string if types don't match
                        for df in dataframes:
                            df[key] = df[key].astype(str)
            
            return True
            
        except Exception as e:
            st.error(f"Error validating merge keys: {str(e)}")
            return False

    @staticmethod
    def clean_merge_data(df: pd.DataFrame, keys: List[str]) -> pd.DataFrame:
        """Clean data before merging."""
        try:
            result = df.copy()
            
            # Clean merge key columns
            for key in keys:
                # Strip whitespace if string
                if pd.api.types.is_string_dtype(result[key]):
                    result[key] = result[key].str.strip()
                
                # Handle missing values in key columns
                if result[key].isna().any():
                    st.warning(f"Found missing values in merge key '{key}'. Filling with appropriate values.")
                    if pd.api.types.is_numeric_dtype(result[key]):
                        result[key] = result[key].fillna(-999999)  # Special value for missing numerics
                    else:
                        result[key] = result[key].fillna("MISSING")
            
            return result
            
        except Exception as e:
            st.error(f"Error cleaning merge data: {str(e)}")
            return df

    @staticmethod
    def merge_dataframes(dataframes: List[pd.DataFrame], 
                        merge_keys: List[str], 
                        merge_type: str = 'left') -> Optional[pd.DataFrame]:
        """Merge multiple DataFrames with validation and error handling."""
        try:
            if not dataframes:
                st.error("No DataFrames to merge.")
                return None
            
            # Validate merge keys
            if not DataProcessor.validate_merge_keys(dataframes, merge_keys):
                return None
            
            # Clean data for merging
            cleaned_dfs = [DataProcessor.clean_merge_data(df, merge_keys) for df in dataframes]
            
            # Initialize progress
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Perform merge
            result = cleaned_dfs[0]
            total_rows = len(result)
            
            for idx, df in enumerate(cleaned_dfs[1:], 1):
                # Update progress
                progress = idx / len(cleaned_dfs[1:])
                progress_bar.progress(progress)
                status_text.text(f"Merging DataFrame {idx + 1}...")
                
                # Store column counts for validation
                pre_merge_cols = len(result.columns) + len(df.columns) - len(merge_keys)
                
                # Add suffixes to avoid column name conflicts
                suffix_a = f"_1_{idx}" if idx > 1 else ""
                suffix_b = f"_2_{idx}"
                
                # Perform merge
                result = result.merge(
                    df,
                    on=merge_keys,
                    how=merge_type,
                    suffixes=(suffix_a, suffix_b),
                    validate='m:m'  # Validate many-to-many relationships
                )
                
                # Log merge results and validate
                new_total = len(result)
                post_merge_cols = len(result.columns)
                
                # Validate no columns were accidentally dropped
                if post_merge_cols < pre_merge_cols - len(merge_keys):
                    st.warning(f"Some columns may have been lost in merge step {idx}. Please check the results.")
                
                # Log row changes
                if merge_type in ['left', 'right'] and new_total != total_rows:
                    st.warning(f"Row count changed in {merge_type} merge: {total_rows} → {new_total}")
                elif merge_type == 'inner' and new_total < min(total_rows, len(df)):
                    st.info(f"Inner merge reduced rows: {total_rows} → {new_total}")
                
                st.success(f"Merge step {idx}: {total_rows} → {new_total} rows")
                total_rows = new_total
            
            progress_bar.progress(1.0)
            status_text.text("Merge completed successfully!")
            
            # Final validation
            if result.empty:
                st.error("Merge resulted in empty DataFrame. Please check your merge keys and data.")
                return None
            
            # Report duplicate keys if any
            for key in merge_keys:
                dupe_count = result[key].duplicated().sum()
                if dupe_count > 0:
                    st.info(f"Found {dupe_count} duplicate values in key '{key}' after merge.")
            
            return result
            
        except pd.errors.MergeError as me:
            st.error(f"Merge error: {str(me)}")
            return None
        except Exception as e:
            st.error(f"Error during merge: {str(e)}")
            return None

    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Handle missing values and report details."""
        result = df.copy()
        missing_report = {}
        total_cells = len(df) * len(df.columns)
        total_missing = 0
        
        # Check each column for missing values
        for column in result.columns:
            missing_mask = result[column].isna()
            missing_count = missing_mask.sum()
            total_missing += missing_count
            
            if missing_count > 0:
                missing_report[column] = {
                    'count': missing_count,
                    'percentage': (missing_count / len(df)) * 100,
                    'missing_rows': df.index[missing_mask].tolist()
                }
                # Replace ALL missing values with "Data Not Available"
                result[column] = result[column].fillna("Data Not Available")
        
        self.cleaning_report['missing_values'] = {
            'columns_with_missing': len(missing_report),
            'total_missing_cells': total_missing,
            'total_cells': total_cells,
            'overall_missing_percentage': (total_missing / total_cells * 100) if total_cells > 0 else 0,
            'details_by_column': missing_report
        }
        
        return result

    def handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Handle outliers using IQR method for numeric columns."""
        result = df.copy()
        outlier_report = {}
        total_outliers = 0
        
        # Process only numeric columns
        numeric_columns = result.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            # Calculate IQR boundaries
            Q1 = result[column].quantile(0.25)
            Q3 = result[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Identify outliers
            outliers_mask = (result[column] < lower_bound) | (result[column] > upper_bound)
            outlier_count = outliers_mask.sum()
            total_outliers += outlier_count
            
            if outlier_count > 0:
                # Store outlier values before clipping
                outlier_values = result.loc[outliers_mask, column].tolist()
                
                outlier_report[column] = {
                    'count': outlier_count,
                    'percentage': (outlier_count / len(df)) * 100,
                    'boundaries': {
                        'lower': lower_bound,
                        'upper': upper_bound
                    },
                    'statistics': {
                        'Q1': Q1,
                        'Q3': Q3,
                        'IQR': IQR
                    },
                    'outlier_values': outlier_values,
                    'outlier_rows': df.index[outliers_mask].tolist()
                }
                
                # Clip values to boundaries
                result[column] = result[column].clip(lower=lower_bound, upper=upper_bound)
        
        self.cleaning_report['outliers'] = {
            'columns_processed': len(numeric_columns),
            'columns_with_outliers': len(outlier_report),
            'total_outliers': total_outliers,
            'details_by_column': outlier_report
        }
        
        return result

    def handle_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove exact duplicate rows and report details."""
        # Identify duplicates
        duplicate_mask = df.duplicated()
        duplicate_count = duplicate_mask.sum()
        duplicate_percentage = (duplicate_count / len(df)) * 100
        
        # Store duplicate rows for reporting
        duplicate_rows = df[duplicate_mask].copy()
        
        # Remove duplicates
        result = df.drop_duplicates()
        
        self.cleaning_report['duplicates'] = {
            'rows_before': len(df),
            'rows_after': len(result),
            'duplicates_removed': duplicate_count,
            'percentage_duplicate': duplicate_percentage,
            'duplicate_examples': duplicate_rows.head(5).to_dict('records') if not duplicate_rows.empty else []
        }
        
        return result

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Main method to clean data with comprehensive reporting."""
        # Reset cleaning report
        self.cleaning_report = {
            'missing_values': {},
            'outliers': {},
            'duplicates': {},
            'summary': {}
        }
        
        # Store initial state
        initial_state = {
            'rows': len(df),
            'columns': len(df.columns),
            'total_cells': len(df) * len(df.columns),
            'memory_usage': df.memory_usage(deep=True).sum() / (1024 * 1024)  # In MB
        }
        
        # Step 1: Handle Missing Values
        st.write("Step 1: Handling Missing Values...")
        df_cleaned = self.handle_missing_values(df)
        
        # Step 2: Handle Outliers
        st.write("Step 2: Processing Outliers...")
        df_cleaned = self.handle_outliers(df_cleaned)
        
        # Step 3: Remove Duplicates
        st.write("Step 3: Removing Duplicates...")
        df_cleaned = self.handle_duplicates(df_cleaned)
        
        # Final state
        final_state = {
            'rows': len(df_cleaned),
            'columns': len(df_cleaned.columns),
            'total_cells': len(df_cleaned) * len(df_cleaned.columns),
            'memory_usage': df_cleaned.memory_usage(deep=True).sum() / (1024 * 1024)  # In MB
        }
        
        # Generate final summary
        self.cleaning_report['summary'] = {
            'initial_state': initial_state,
            'final_state': final_state,
            'changes': {
                'rows_removed': initial_state['rows'] - final_state['rows'],
                'memory_reduction': initial_state['memory_usage'] - final_state['memory_usage'],
                'missing_values_handled': self.cleaning_report['missing_values'].get('total_missing_cells', 0),
                'outliers_handled': self.cleaning_report['outliers'].get('total_outliers', 0),
                'duplicates_removed': self.cleaning_report['duplicates'].get('duplicates_removed', 0)
            },
            'performance_metrics': {
                'data_reduction_percentage': ((initial_state['rows'] - final_state['rows']) / initial_state['rows'] * 100) if initial_state['rows'] > 0 else 0,
                'memory_reduction_percentage': ((initial_state['memory_usage'] - final_state['memory_usage']) / initial_state['memory_usage'] * 100) if initial_state['memory_usage'] > 0 else 0
            }
        }
        
        return df_cleaned

    def display_cleaning_report(self):
        """Display detailed cleaning report."""
        st.header("🧹 Data Cleaning Report")
        
        # Overall Summary
        st.subheader("📊 Summary of Changes")
        summary = self.cleaning_report['summary']
        
        # Create summary metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="Total Rows",
                value=f"{summary['final_state']['rows']:,}",
                delta=f"-{summary['changes']['rows_removed']:,}",
                delta_color="inverse"
            )
        
        with col2:
            st.metric(
                label="Memory Usage (MB)",
                value=f"{summary['final_state']['memory_usage']:.2f}",
                delta=f"-{summary['changes']['memory_reduction']:.2f}",
                delta_color="inverse"
            )
        
        with col3:
            st.metric(
                label="Data Reduction",
                value=f"{summary['performance_metrics']['data_reduction_percentage']:.1f}%",
                delta="reduction" if summary['performance_metrics']['data_reduction_percentage'] > 0 else "no change"
            )
        
        # Missing Values Section
        st.subheader("🔍 Missing Values")
        missing = self.cleaning_report['missing_values']
        if missing:
            st.write(f"Total Missing Cells: {missing['total_missing_cells']:,} ({missing['overall_missing_percentage']:.2f}%)")
            st.write(f"Columns with Missing Values: {missing['columns_with_missing']}")
            
            if missing.get('details_by_column'):
                # Create a DataFrame for missing values information
                missing_df = pd.DataFrame.from_dict(
                    missing['details_by_column'],
                    orient='index'
                )
                if not missing_df.empty:
                    missing_df = missing_df.sort_values('count', ascending=False)
                    st.write("Details by Column:")
                    
                    # Display as a styled table
                    st.dataframe(
                        missing_df[['count', 'percentage']].style.format({
                            'count': '{:,.0f}',
                            'percentage': '{:.2f}%'
                        })
                    )
        
        # Outliers Section
        st.subheader("📊 Outliers")
        outliers = self.cleaning_report['outliers']
        if outliers:
            st.write(f"Numeric Columns Processed: {outliers['columns_processed']}")
            st.write(f"Columns with Outliers: {outliers['columns_with_outliers']}")
            st.write(f"Total Outliers Found: {outliers['total_outliers']:,}")
            
            if outliers.get('details_by_column'):
                st.write("Details by Column:")
                for col, info in outliers['details_by_column'].items():
                    with st.expander(f"{col} - {info['count']:,} outliers ({info['percentage']:.2f}%)"):
                        st.write("### Boundaries")
                        st.write(f"- Lower: {info['boundaries']['lower']:.2f}")
                        st.write(f"- Upper: {info['boundaries']['upper']:.2f}")
                        
                        st.write("### Statistics")
                        st.write(f"- Q1: {info['statistics']['Q1']:.2f}")
                        st.write(f"- Q3: {info['statistics']['Q3']:.2f}")
                        st.write(f"- IQR: {info['statistics']['IQR']:.2f}")
                        
                        if info.get('outlier_values'):
                            st.write("### Sample Outlier Values")
                            st.write(info['outlier_values'][:5])
        
        # Duplicates Section
        st.subheader("🔄 Duplicates")
        dupes = self.cleaning_report['duplicates']
        if dupes:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    label="Duplicates Removed",
                    value=f"{dupes['duplicates_removed']:,}",
                    delta=f"-{dupes['duplicates_removed']:,}",
                    delta_color="inverse"
                )
            
            with col2:
                st.metric(
                    label="Duplicate Percentage",
                    value=f"{dupes['percentage_duplicate']:.2f}%"
                )
            
            with col3:
                st.metric(
                    label="Final Row Count",
                    value=f"{dupes['rows_after']:,}",
                    delta=f"-{dupes['rows_before'] - dupes['rows_after']:,}",
                    delta_color="inverse"
                )
            
            if dupes.get('duplicate_examples'):
                with st.expander("View Sample Duplicate Rows"):
                    st.dataframe(pd.DataFrame(dupes['duplicate_examples']))
        
        # Final Data Quality Metrics
        st.subheader("📈 Final Data Quality Metrics")
        
        quality_metrics = {
            'Completeness': (1 - missing['total_missing_cells'] / summary['final_state']['total_cells']) * 100 if missing else 100,
            'Uniqueness': (1 - dupes['percentage_duplicate'] / 100) * 100 if dupes else 100,
            'Validity': (1 - outliers['total_outliers'] / summary['final_state']['total_cells']) * 100 if outliers else 100
        }
        
        # Display metrics in columns
        cols = st.columns(3)
        for idx, (metric, value) in enumerate(quality_metrics.items()):
            with cols[idx]:
                st.metric(
                    label=f"{metric} Score",
                    value=f"{value:.2f}%"
                )
                
    def get_report_summary(self) -> Dict:
        """Get a dictionary summary of the cleaning report."""
        return {
            'total_rows_processed': self.cleaning_report['summary']['initial_state']['rows'],
            'final_rows': self.cleaning_report['summary']['final_state']['rows'],
            'missing_values_handled': self.cleaning_report['summary']['changes']['missing_values_handled'],
            'outliers_handled': self.cleaning_report['summary']['changes']['outliers_handled'],
            'duplicates_removed': self.cleaning_report['summary']['changes']['duplicates_removed'],
            'data_reduction_percentage': self.cleaning_report['summary']['performance_metrics']['data_reduction_percentage']
        }
        
class Analytics:
    """Class to handle all analytical operations."""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
    
    def create_pivot(self, 
                index_cols: List[str],
                value_cols: List[str],
                agg_funcs: List[str],
                filters: Dict = None) -> Optional[pd.DataFrame]:
        """Create pivot table with comprehensive error handling."""
        try:
            # Validate inputs
            if not index_cols or not value_cols or not agg_funcs:
                st.error("Missing required parameters for pivot table.")
                return None
            
            # Apply filters
            filtered_df = self.df.copy()
            if filters:
                for col, values in filters.items():
                    if values:
                        filtered_df = filtered_df[
                            filtered_df[col].astype(str).isin([str(v) for v in values])
                        ]
            
            # Create pivot tables for each combination
            pivot_tables = []
            for col in value_cols:
                # Get column type
                is_numeric = pd.api.types.is_numeric_dtype(filtered_df[col])
                is_datetime = pd.api.types.is_datetime64_any_dtype(filtered_df[col])
                
                for func in agg_funcs:
                    try:
                        # Handle different aggregation functions based on data type
                        if func == 'distinct_count':
                            agg_func = lambda x: len(x.unique())
                            display_name = 'Unique Count'
                        elif func == 'count':
                            agg_func = 'count'
                            display_name = 'Count'
                        elif is_numeric and func in ['sum', 'mean', 'max', 'min']:
                            agg_func = func
                            display_name = func.capitalize()
                        elif func in ['first', 'last']:
                            agg_func = func
                            display_name = func.capitalize()
                        elif func == 'list':
                            agg_func = lambda x: ', '.join(x.unique().astype(str))
                            display_name = 'Unique Values'
                        else:
                            # Skip invalid combinations
                            continue
                        
                        pivot = pd.pivot_table(
                            filtered_df,
                            values=col,
                            index=index_cols,
                            aggfunc=agg_func,
                            fill_value=0 if is_numeric else ''
                        )
                        
                        # Format column name
                        pivot.columns = [f"{col} ({display_name})"]
                        pivot_tables.append(pivot)
                        
                    except Exception as e:
                        st.warning(f"Error calculating {func} for {col}: {str(e)}")
                        continue
            
            if not pivot_tables:
                st.warning("No valid pivot tables could be created.")
                return None
            
            # Combine results
            result = pd.concat(pivot_tables, axis=1)
            
            # Add totals row if any numeric columns exist
            try:
                totals = pd.DataFrame(index=['Total'])
                for col in result.columns:
                    try:
                        if 'Count' in col:
                            totals[col] = result[col].sum()
                        elif any(x in col for x in ['Sum', 'Mean']):
                            totals[col] = result[col].sum()
                        elif 'Max' in col:
                            totals[col] = result[col].max()
                        elif 'Min' in col:
                            totals[col] = result[col].min()
                        else:
                            totals[col] = ''  # For non-numeric aggregations
                    except:
                        totals[col] = ''
                
                result = pd.concat([result, totals])
            except Exception as e:
                st.warning(f"Error calculating totals: {str(e)}")
            
            return result
            
        except Exception as e:
            st.error(f"Error creating pivot table: {str(e)}")
            return None
    
    def calculate_statistics(self, column: str) -> Dict:
        """Calculate comprehensive statistics for a column."""
        try:
            series = self.df[column]
            
            # Convert boolean to int
            if series.dtype == bool:
                series = series.astype(int)
            
            # Basic stats for all types
            stats = {
                'count': int(len(series)),
                'unique': int(series.nunique()),
                'missing': int(series.isna().sum()),
                'missing_pct': float(series.isna().mean() * 100)
            }
            
            # Additional stats for numeric columns
            if pd.api.types.is_numeric_dtype(series):
                numeric_series = pd.to_numeric(series, errors='coerce')
                stats.update({
                    'mean': float(numeric_series.mean()) if not pd.isna(numeric_series.mean()) else None,
                    'median': float(numeric_series.median()) if not pd.isna(numeric_series.median()) else None,
                    'std': float(numeric_series.std()) if not pd.isna(numeric_series.std()) else None,
                    'min': float(numeric_series.min()) if not pd.isna(numeric_series.min()) else None,
                    'max': float(numeric_series.max()) if not pd.isna(numeric_series.max()) else None,
                    'skew': float(numeric_series.skew()) if not pd.isna(numeric_series.skew()) else None,
                    'kurtosis': float(numeric_series.kurtosis()) if not pd.isna(numeric_series.kurtosis()) else None,
                    'percentiles': {
                        '25%': float(numeric_series.quantile(0.25)),
                        '50%': float(numeric_series.quantile(0.50)),
                        '75%': float(numeric_series.quantile(0.75))
                    }
                })
            
            return stats
            
        except Exception as e:
            st.error(f"Error calculating statistics for {column}: {str(e)}")
            return {}

class Visualizer:
    """Class to handle all visualization functionality."""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
    
    def create_pivot_chart(self, 
                          pivot_df: pd.DataFrame,
                          chart_type: str = 'bar') -> Optional[go.Figure]:
        """Create visualization for pivot table results."""
        try:
            if pivot_df is None or pivot_df.empty:
                st.error("No data available for visualization.")
                return None
            
            # Remove total row for visualization
            plot_df = pivot_df[pivot_df.index != 'Total'].copy()
            
            # Reset index to make it a column
            plot_df = plot_df.reset_index()
            
            # Get index column name
            index_col = pivot_df.index.name if pivot_df.index.name else 'index'
            
            fig = go.Figure()
            
            if chart_type == 'bar':
                # Add a trace for each value column
                for col in plot_df.columns[1:]:  # Skip index column
                    fig.add_trace(
                        go.Bar(
                            name=col,
                            x=plot_df[index_col],
                            y=plot_df[col],
                            text=plot_df[col].round(2),
                            textposition='auto',
                        )
                    )
                fig.update_layout(barmode='group')
                
            elif chart_type == 'line':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Scatter(
                            name=col,
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='lines+markers',
                            text=plot_df[col].round(2),
                            textposition='top center',
                        )
                    )
                    
            elif chart_type == 'scatter':
                for col in plot_df.columns[1:]:
                    fig.add_trace(
                        go.Scatter(
                            name=col,
                            x=plot_df[index_col],
                            y=plot_df[col],
                            mode='markers',
                            marker=dict(size=10),
                            text=plot_df[col].round(2),
                            textposition='top center',
                        )
                    )
            
            # Update layout
            fig.update_layout(
                title='Pivot Table Visualization',
                title_x=0.5,
                height=600,
                showlegend=True,
                template='plotly_white',
                xaxis_title=index_col,
                yaxis_title="Values",
                hovermode='x unified'
            )
            
            return fig
            
        except Exception as e:
            st.error(f"Error creating pivot visualization: {str(e)}")
            return None
    
    def create_correlation_analysis(self) -> Tuple[Optional[go.Figure], Optional[pd.DataFrame]]:
        """Create correlation analysis with both heatmap and matrix."""
        try:
            # Get numeric columns
            numeric_df = self.df.select_dtypes(include=[np.number])
            
            if numeric_df.empty:
                st.warning("No numeric columns available for correlation analysis.")
                return None, None
            
            # Calculate correlation matrix
            corr_matrix = numeric_df.corr().round(3)
            
            # Create heatmap
            fig = go.Figure(data=go.Heatmap(
                z=corr_matrix.values,
                x=corr_matrix.columns,
                y=corr_matrix.columns,
                text=corr_matrix.values.round(3),
                texttemplate='%{text:.3f}',
                textfont={"size": 10},
                hoverongaps=False,
                colorscale='RdBu_r',
                zmid=0
            ))
            
            fig.update_layout(
                title='Correlation Heatmap',
                title_x=0.5,
                height=700,
                width=800,
                xaxis_title="Features",
                yaxis_title="Features",
                xaxis={'side': 'bottom'}
            )
            
            return fig, corr_matrix
            
        except Exception as e:
            st.error(f"Error creating correlation analysis: {str(e)}")
            return None, None
class ExportManager:
    """Class to handle all export functionality."""
    
    @staticmethod
    def to_excel(df: pd.DataFrame,
                 pivot_df: Optional[pd.DataFrame] = None,
                 stats: Optional[Dict] = None) -> Optional[bytes]:
        """Export data to Excel with formatting."""
        try:
            output = io.BytesIO()
            
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                # Write original data
                df.to_excel(writer, sheet_name='Data', index=False)
                
                # Write pivot results if available
                if pivot_df is not None and not pivot_df.empty:
                    pivot_df.to_excel(writer, sheet_name='Pivot Analysis')
                
                # Write statistics if available
                if stats:
                    pd.DataFrame(stats).to_excel(writer, sheet_name='Statistics')
                
                # Get workbook and add formats
                workbook = writer.book
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#D3D3D3',
                    'border': 1,
                    'text_wrap': True
                })
                
                number_format = workbook.add_format({
                    'num_format': '#,##0.00',
                    'border': 1
                })
                
                # Format each worksheet
                for sheet in writer.sheets.values():
                    # Format headers
                    sheet.set_row(0, None, header_format)
                    
                    # Set column widths
                    sheet.set_column(0, 50, 15)
                    
                    # Freeze panes
                    sheet.freeze_panes(1, 0)
            
            return output.getvalue()
            
        except Exception as e:
            st.error(f"Error exporting to Excel: {str(e)}")
            return None

def main():
    """Main application function with enhanced data processing and analysis capabilities."""
    
    # Initialize session state
    if 'merged_data' not in st.session_state:
        st.session_state.merged_data = None
    if 'pivot_results' not in st.session_state:
        st.session_state.pivot_results = None
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'data_processor' not in st.session_state:
        st.session_state.data_processor = None
    if 'cleaning_report' not in st.session_state:
        st.session_state.cleaning_report = None

    st.title("📊 Advanced Excel Analysis Dashboard")
    
    # File Upload Section
    with st.sidebar:
        st.header("📁 File Upload")
        uploaded_files = st.file_uploader(
            "Upload Excel Files",
            type=['xlsx', 'xls'],
            accept_multiple_files=True,
            help="Upload one or more Excel files for analysis"
        )
        
        # Display upload instructions if no files
        if not uploaded_files:
            st.info("👆 Please upload Excel files to begin analysis.")
            return
        
        # Data Cleaning Options
        st.header("⚙️ Data Cleaning Settings")
        handle_missing = st.checkbox("Handle Missing Values", value=True,
                                   help="Replace ALL missing values with 'Data Not Available'")
        handle_outliers = st.checkbox("Handle Outliers", value=True,
                                    help="Detect and clip outliers using IQR method (numeric columns only)")
        handle_duplicates = st.checkbox("Remove Duplicates", value=True,
                                      help="Remove exact duplicate rows")
    
    # Load and process files
    data_loader = DataLoader()
    dataframes = []
    
    with st.spinner('Processing uploaded files...'):
        for file in uploaded_files:
            df = data_loader.load_excel(file)
            if df is not None:
                dataframes.append(df)
                st.success(f"✓ {file.name} loaded successfully")
                with st.expander(f"Preview: {file.name}"):
                    st.dataframe(df.head())
                    st.write(f"Shape: {df.shape}")
    
    if not dataframes:
        st.error("❌ No valid data loaded.")
        return
    
    # Get common columns
    common_columns = list(set.intersection(*[set(df.columns) for df in dataframes]))
    
    if not common_columns:
        st.error("❌ No common columns found between files!")
        for idx, df in enumerate(dataframes):
            st.write(f"Columns in file {idx + 1}:", df.columns.tolist())
        return
    
    # Main Analysis Tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "🔄 Merge & Clean",
        "📊 Analysis",
        "📈 Visualization",
        "💾 Export"
    ])
    
    # Tab 1: Merge & Clean
    with tab1:
        st.header("Merge Settings")
        
        merge_keys = st.multiselect(
            "Select columns to merge on:",
            common_columns,
            help="Select one or more columns to use as merge keys"
        )
        
        merge_type = st.selectbox(
            "Select merge type:",
            ['left', 'right', 'inner', 'outer'],
            help="Choose how to merge the files"
        )
        
        if st.button("🔄 Apply Merge & Clean"):
            if not merge_keys:
                st.warning("⚠️ Please select merge columns.")
                return
            
            with st.spinner("Merging and cleaning data..."):
                # Initialize data processor
                data_processor = DataProcessor()
                st.session_state.data_processor = data_processor
                
                # Perform merge
                merged_df = data_processor.merge_dataframes(dataframes, merge_keys, merge_type)
                
                if merged_df is not None:
                    st.write("Step 1: Merging Data...")
                    st.write(f"- Initial shape: {merged_df.shape}")
                    
                    # Apply data cleaning if selected
                    if handle_missing or handle_outliers or handle_duplicates:
                        st.write("Step 2: Cleaning Data...")
                        merged_df = data_processor.clean_data(merged_df)
                        
                        # Display cleaning report
                        data_processor.display_cleaning_report()
                        st.session_state.cleaning_report = data_processor.cleaning_report
                    
                    st.session_state.merged_data = merged_df
                    st.success("✅ Data processing completed!")
                    
                    with st.expander("👀 Preview Processed Data"):
                        st.dataframe(merged_df.head())
                        st.write(f"Final shape: {merged_df.shape}")
    
    # Only show other tabs if we have processed data
    if st.session_state.merged_data is not None:
        merged_df = st.session_state.merged_data
        
        # Tab 2: Analysis
        with tab2:
            st.header("Analysis Options")
            
            analysis_type = st.multiselect(
                "Select Analysis Types:",
                ["Pivot Table", "Statistical Analysis", "Correlation Analysis"]
            )
            
            if "Pivot Table" in analysis_type:
                st.subheader("Pivot Table Settings")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    group_cols = st.multiselect(
                        "Group By:",
                        merged_df.columns,
                        help="Select columns to group by"
                    )
                
                with col2:
                    value_cols = st.multiselect(
                        "Values:",
                        merged_df.columns,  # Allow all columns, not just numeric ones
                        help="Select columns to analyze"
                    )
                
                with col3:
                    # Show different aggregation options based on selected column types
                    available_aggs = ['count', 'distinct_count']  # These work for all types
                    
                    # Add numeric aggregations if any numeric column is selected
                    if any(pd.api.types.is_numeric_dtype(merged_df[col]) for col in value_cols):
                        available_aggs.extend(['sum', 'mean', 'max', 'min'])
                    
                    # Add text-specific aggregations if any text column is selected
                    if any(pd.api.types.is_string_dtype(merged_df[col]) for col in value_cols):
                        available_aggs.extend(['first', 'last', 'list'])
                    
                    agg_funcs = st.multiselect(
                        "Aggregations:",
                        available_aggs,
                        help="Choose aggregation functions (options vary based on selected column types)"
                    )
                
                # Filters
                if group_cols:
                    st.subheader("🔍 Filters")
                    filters = {}
                    filter_cols = st.columns(min(len(group_cols), 4))
                    
                    for idx, col in enumerate(group_cols):
                        with filter_cols[idx % 4]:
                            unique_vals = sorted(merged_df[col].unique())
                            selected = st.multiselect(
                                f"Filter {col}:",
                                unique_vals,
                                help=f"Select values to include for {col}"
                            )
                            if selected:
                                filters[col] = selected
                    
                    if st.button("Generate Pivot Table"):
                        with st.spinner("Creating pivot table..."):
                            analytics = Analytics(merged_df)
                            pivot_result = analytics.create_pivot(
                                group_cols,
                                value_cols,
                                agg_funcs,
                                filters
                            )
                            
                            if pivot_result is not None:
                                st.session_state.pivot_results = pivot_result
                                st.success("✅ Pivot table created!")
                                st.dataframe(pivot_result)
            
            if "Statistical Analysis" in analysis_type:
                st.subheader("Statistical Analysis")
                
                # Select columns for analysis
                stat_columns = st.multiselect(
                    "Select columns for statistical analysis:",
                    merged_df.columns
                )
                
                if stat_columns:
                    analytics = Analytics(merged_df)
                    stats_results = {}
                    
                    for col in stat_columns:
                        stats_results[col] = analytics.calculate_statistics(col)
                    
                    st.session_state.analysis_results = stats_results
                    
                    # Display results
                    for col, stats in stats_results.items():
                        with st.expander(f"📊 {col} Statistics"):
                            st.write(stats)
            
            if "Correlation Analysis" in analysis_type:
                st.subheader("Correlation Analysis")
                
                numeric_cols = merged_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    corr_cols = st.multiselect(
                        "Select columns for correlation:",
                        numeric_cols,
                        default=list(numeric_cols)[:5]
                    )
                    
                    if corr_cols and len(corr_cols) > 1:
                        corr_df = merged_df[corr_cols]
                        viz = Visualizer(corr_df)
                        
                        # Create correlation analysis
                        corr_fig, corr_matrix = viz.create_correlation_analysis()
                        
                        if corr_fig and corr_matrix is not None:
                            # Show correlation matrix
                            st.write("Correlation Matrix:")
                            st.dataframe(
                                corr_matrix.style.background_gradient(
                                    cmap='RdBu_r',
                                    vmin=-1,
                                    vmax=1
                                )
                            )
                            
                            # Show correlation heatmap
                            st.write("Correlation Heatmap:")
                            st.plotly_chart(corr_fig, use_container_width=True)
                else:
                    st.warning("Need at least 2 numeric columns for correlation analysis.")
        
        # Tab 3: Visualization
        with tab3:
            st.header("Visualization Options")
            
            if st.session_state.pivot_results is not None:
                chart_type = st.selectbox(
                    "Select Chart Type:",
                    ['bar', 'line', 'scatter']
                )
                
                viz = Visualizer(merged_df)
                fig = viz.create_pivot_chart(
                    st.session_state.pivot_results,
                    chart_type
                )
                
                if fig:
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Create a pivot table first to see visualizations.")
        
        # Tab 4: Export
        with tab4:
            st.header("Export Options")
            
            export_format = st.selectbox(
                "Select Export Format:",
                ["Excel Report", "CSV Data"]
            )
            
            if export_format == "Excel Report":
                if st.button("Generate Excel Report"):
                    with st.spinner("Generating Excel report..."):
                        export_manager = ExportManager()
                        excel_data = export_manager.to_excel(
                            merged_df,
                            st.session_state.pivot_results,
                            st.session_state.analysis_results
                        )
                        
                        if excel_data:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                label="📥 Download Excel Report",
                                data=excel_data,
                                file_name=f"analysis_report_{timestamp}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
            
            elif export_format == "CSV Data":
                if st.button("Export to CSV"):
                    csv = merged_df.to_csv(index=False)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    st.download_button(
                        label="📥 Download CSV",
                        data=csv,
                        file_name=f"analysis_data_{timestamp}.csv",
                        mime="text/csv"
                    )

if __name__ == "__main__":
    main()
