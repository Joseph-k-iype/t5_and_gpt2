import os
import logging
import csv
import uuid
import PyPDF2
import httpx
import requests
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential, ClientSecretCredential
# Use the Azure-specific OpenAI client.
from openai import AzureOpenAI
import chromadb
from chromadb.config import Settings

# --- Load configuration files from the 'config' folder ---
load_dotenv("config/dev")         # loads variables from config/dev
load_dotenv("config/dev.creds")    # loads variables from config/dev.creds

# --- Set up Proxy URL and CA bundle (as before) ---
ad_username = os.getenv("AD_USERNAME")
ad_user_id = os.getenv("AD_USER_ID")
http_proxy_config = os.getenv("HTTP_PROXY")  # e.g. "@abc.uk.systems:80"
proxy_url = f"http://{ad_username}:{ad_user_id}{http_proxy_config}"

os.environ['HTTP_PROXY'] = proxy_url
os.environ['HTTPS_PROXY'] = proxy_url
os.environ["REQUESTS_CA_BUNDLE"] = os.getenv("CONF_PEM_PATH", "cacert.pem")

custom_client = httpx.Client(
    verify=os.getenv("CONF_PEM_PATH", "cacert.pem"), 
    proxy=proxy_url
)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Configure NO_PROXY if not already set.
if not os.getenv("NO_PROXY"):
    NO_PROXY_DOMAINS = [
        '.cognitiveservices.azure.com',
        '.search.windows.net',
        '.openai.azure.com',
        '.core.windows.net',
        '.azurewebsites.net'
    ]
    os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

session = requests.Session()
session.verify = os.getenv("CONF_PEM_PATH", "cacert.pem")
session.proxies = {'http': None, 'https': None}

# --- Embeddings Functionality ---
# Set your embeddings endpoint (you can use AZURE_OPENAI_EMBEDDINGS_ENDPOINT or fallback to AZURE_OPENAI_ENDPOINT)
embeddings_endpoint = os.getenv("AZURE_OPENAI_EMBEDDINGS_ENDPOINT", os.getenv("AZURE_OPENAI_ENDPOINT"))

try:
    credential = DefaultAzureCredential()
    embeddings_token = credential.get_token('https://cognitiveservices.azure.com/.default')

    def get_embeddings(texts, endpoint, deployment_name="text-embedding-3-large", batch_size=100):
        headers = {
            'Authorization': f'Bearer {embeddings_token.token}',
            'Content-Type': 'application/json'
        }
        openai_api_version = os.getenv("OPENAI_API_VERSION", "2023-05-15")
        api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version={openai_api_version}"
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            try:
                payload = {"input": batch}
                response = session.post(api_url, headers=headers, json=payload)
                if response.status_code == 200:
                    response_data = response.json()
                    batch_embeddings = [item['embedding'] for item in response_data['data']]
                    embeddings.extend(batch_embeddings)
                    logger.info(f"Received embeddings for batch {i+1}-{min(i+batch_size, len(texts))}")
                else:
                    logger.error(f"Failed to receive embeddings for batch {i+1}-{min(i+batch_size, len(texts))}, status code: {response.status_code}")
                    embeddings.extend([None] * len(batch))
            except Exception as e:
                logger.error(f"Error processing batch {i+1}-{min(i+batch_size, len(texts))}: {str(e)}")
                embeddings.extend([None] * len(batch))
        return embeddings

except Exception as e:
    logger.error(f"Failed to initialize Azure SDK for embeddings: {str(e)}")
    raise

def test_connection(endpoint):
    try:
        test_texts = ['Hello World']
        embeddings = get_embeddings(test_texts, endpoint)
        if embeddings and embeddings[0]:
            print(f"Embedding Dimension: {len(embeddings[0])}")
            return True
        else:
            print("Failed to retrieve embeddings")
            return False
    except Exception as e:
        print(f"Failed to connect to {endpoint}: {str(e)}")
        return False

# --- Configure Azure OpenAI for LLM using Azure AD Authentication ---
try:
    llm_credential = ClientSecretCredential(
        tenant_id=os.environ["AZURE_TENANT_ID"],
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"]
    )
    llm_token = llm_credential.get_token('https://cognitiveservices.azure.com/.default')
    
    # Instantiate the AzureOpenAI client.
    llm_client = AzureOpenAI(
        api_key=llm_token.token,
        base_url=os.getenv("AZURE_OPENAI_ENDPOINT"),  # e.g. "https://abc-multi-wdac-nonprod-use.openai.azure.com"
        azure_deployment="gpt-4o-mini",  # your deployment name
        api_version="2023-03-15-preview"
    )
    
    def run_llm_agent(prompt: str) -> str:
        """
        Uses AzureOpenAI's Chat Completion endpoint (via the AzureOpenAI client)
        to generate a response for the given prompt.
        """
        try:
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",  # required: specify your deployment name
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
            )
            answer = response.choices[0].message.content
            logger.info(f"LLM agent response: {answer}")
            return answer
        except Exception as e:
            logger.error(f"Error in LLM agent: {str(e)}")
            return "Error generating response from the LLM agent."
            
except Exception as e:
    logger.error(f"Failed to configure Azure OpenAI for LLM: {str(e)}")
    raise

# --- File Processing Functions ---
def process_pdf(file_path):
    """Extract text from a PDF file, one document per page."""
    documents = []
    try:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    documents.append(text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing PDF file: {str(e)}")
        return []

def process_txt(file_path):
    """Read the entire text from a TXT file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        return [text.strip()]
    except Exception as e:
        logger.error(f"Error processing TXT file: {str(e)}")
        return []

def process_csv(file_path):
    """
    Read a CSV file, ask the user for the main column and supporting columns,
    then construct a document per row by combining those columns.
    """
    documents = []
    try:
        with open(file_path, "r", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            fieldnames = reader.fieldnames
            if not fieldnames:
                logger.error("CSV file has no header")
                return []
            print(f"CSV Columns: {fieldnames}")
            main_col = input("Enter the name of the main column: ").strip()
            support_cols = input("Enter supporting column names (comma-separated), or leave blank: ").split(",")
            support_cols = [col.strip() for col in support_cols if col.strip()]
            for row in reader:
                parts = []
                if main_col in row:
                    parts.append(row[main_col])
                for col in support_cols:
                    if col in row:
                        parts.append(row[col])
                doc_text = " ".join(parts)
                documents.append(doc_text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing CSV file: {str(e)}")
        return []

# --- ChromaDB Integration ---
# Instantiate a ChromaDB client with telemetry disabled.
chroma_settings = Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db",
    anonymized_telemetry=False  # Ensure no telemetry is sent
)
chroma_client = chromadb.Client(chroma_settings)
collection = chroma_client.get_or_create_collection("documents_collection")

def store_documents_in_vector_db(documents, source_filename=""):
    """
    Given a list of document texts, compute embeddings and add them to ChromaDB.
    """
    if not documents:
        print("No documents to store.")
        return
    embeddings = get_embeddings(documents, embeddings_endpoint, deployment_name="text-embedding-3-large")
    ids = [str(uuid.uuid4()) for _ in documents]
    metadatas = [{"source": source_filename, "doc_index": i} for i in range(len(documents))]
    collection.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)
    print(f"Stored {len(documents)} documents in the vector database.")

# --- Querying the Vector DB and Answering Questions ---
def answer_question(query: str) -> str:
    """
    Given a query, compute its embedding, retrieve the top documents from ChromaDB,
    then construct a prompt with the retrieved context and ask the LLM agent.
    """
    try:
        query_embedding = get_embeddings([query], embeddings_endpoint, deployment_name="text-embedding-3-large")[0]
        results = collection.query(query_embedding, n_results=5)
        context = "\n\n".join(results.get("documents", []))
        combined_prompt = f"Use the following context to answer the question:\n\nContext:\n{context}\n\nQuestion: {query}"
        answer = run_llm_agent(combined_prompt)
        return answer
    except Exception as e:
        logger.error(f"Error in answering question: {str(e)}")
        return "Error generating answer."

# --- Main Execution ---
if __name__ == "__main__":
    # Test embeddings connection.
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-azure-endpoint.openai.azure.com")
    if azure_endpoint.startswith("https:") and not azure_endpoint.startswith("https://"):
        azure_endpoint = azure_endpoint.replace("https:", "https://", 1)
    
    if test_connection(azure_endpoint):
        print("Embeddings connection successful")
    else:
        print("Embeddings connection failed")
    
    # File upload section.
    current_file = input("Enter the path of the file to upload (pdf, txt, csv) or press Enter to skip: ").strip()
    docs = []
    if current_file:
        if current_file.lower().endswith(".pdf"):
            docs = process_pdf(current_file)
        elif current_file.lower().endswith(".txt"):
            docs = process_txt(current_file)
        elif current_file.lower().endswith(".csv"):
            docs = process_csv(current_file)
        else:
            print("Unsupported file type.")
        if docs:
            store_documents_in_vector_db(docs, source_filename=os.path.basename(current_file))
    
    # Query loop.
    print("\nNow you can ask questions based on the stored data. Type 'exit' to quit.")
    while True:
        user_query = input("Your question: ").strip()
        if user_query.lower() in ["exit", "quit"]:
            break
        answer = answer_question(user_query)
        print("Answer:", answer)
