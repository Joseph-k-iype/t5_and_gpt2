import os
import re
import json
import logging
import csv
import uuid
import shutil
import time
import PyPDF2
import httpx
import requests
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from openai import AzureOpenAI
import chromadb

# ------------------------------
# Configuration and Setup
# ------------------------------
load_dotenv("config/dev")         # load general settings
load_dotenv("config/dev.creds")    # load credentials

# Set up proxy and CA bundle
ad_username = os.getenv("AD_USERNAME")
ad_user_id = os.getenv("AD_USER_ID")
http_proxy_config = os.getenv("HTTP_PROXY")  # e.g. "@abc.uk.systems:80"
proxy_url = f"http://{ad_username}:{ad_user_id}{http_proxy_config}"

os.environ['HTTP_PROXY'] = proxy_url
os.environ['HTTPS_PROXY'] = proxy_url
os.environ["REQUESTS_CA_BUNDLE"] = os.getenv("CONF_PEM_PATH", "cacert.pem")

custom_client = httpx.Client(verify=os.getenv("CONF_PEM_PATH", "cacert.pem"), proxy=proxy_url)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

if not os.getenv("NO_PROXY"):
    NO_PROXY_DOMAINS = [
        '.cognitiveservices.azure.com',
        '.search.windows.net',
        '.openai.azure.com',
        '.core.windows.net',
        '.azurewebsites.net'
    ]
    os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

session = requests.Session()
session.verify = os.getenv("CONF_PEM_PATH", "cacert.pem")
session.proxies = {'http': None, 'https': None}

# Maximum characters allowed per API call
MAX_TOTAL_CHARS = 5461

# Toggle agentic chunking (if True, use LLM agent for chunking)
AGENTIC_CHUNKING = True

# ------------------------------
# Chunking Functions
# ------------------------------
def stable_chunk_text(text, max_chars=MAX_TOTAL_CHARS):
    """
    Splits text into chunks at sentence boundaries (via regex).
    If a single sentence exceeds max_chars, it is split arbitrarily.
    """
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(sentence) > max_chars:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = ""
            # Split long sentence arbitrarily
            for i in range(0, len(sentence), max_chars):
                chunks.append(sentence[i:i+max_chars])
        else:
            if len(current_chunk) + len(sentence) + 1 <= max_chars:
                current_chunk = current_chunk + " " + sentence if current_chunk else sentence
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

def agentic_chunk_text(text, max_chars=MAX_TOTAL_CHARS):
    """
    Uses the LLM agent to split the given text into coherent chunks.
    The prompt instructs the agent to output a JSON array of text chunks, each not exceeding max_chars.
    If the response cannot be parsed as JSON, falls back to the stable_chunk_text method.
    """
    prompt = (
        f"Please split the following text into a JSON array of coherent, logically separated chunks, "
        f"each with no more than {max_chars} characters. Only return the JSON array and nothing else.\n\nText:\n{text}"
    )
    response_text = run_llm_agent(prompt)
    try:
        chunks = json.loads(response_text)
        if isinstance(chunks, list) and all(isinstance(item, str) for item in chunks):
            logger.info(f"Agentic chunking produced {len(chunks)} chunks.")
            return chunks
        else:
            logger.warning("Agentic chunking response not a valid list; falling back to stable chunking.")
            return stable_chunk_text(text, max_chars)
    except Exception as e:
        logger.error(f"Error parsing agentic chunking response: {str(e)}; falling back to stable chunking.")
        return stable_chunk_text(text, max_chars)

def average_embeddings(embeddings):
    """Averages a list of embedding vectors element-wise."""
    if not embeddings:
        return None
    n = len(embeddings)
    dim = len(embeddings[0])
    avg = [0.0] * dim
    for emb in embeddings:
        for i, val in enumerate(emb):
            avg[i] += val
    return [x / n for x in avg]

# ------------------------------
# Embeddings API Call
# ------------------------------
try:
    credential = DefaultAzureCredential()
    embeddings_token = credential.get_token('https://cognitiveservices.azure.com/.default')
except Exception as e:
    logger.error(f"Failed to initialize Azure SDK for embeddings: {str(e)}")
    raise

def get_embedding_for_text(text, endpoint, deployment_name="text-embedding-3-large", max_chars=MAX_TOTAL_CHARS, agentic=False):
    """
    Computes an embedding for a single document.
    If agentic is True, uses the LLM agent to determine chunk boundaries; otherwise, uses stable_chunk_text.
    Embeddings for each chunk are computed and then averaged.
    """
    if agentic:
        chunks = agentic_chunk_text(text, max_chars)
    else:
        chunks = stable_chunk_text(text, max_chars)
    chunk_embeddings = []
    headers = {
        'Authorization': f'Bearer {embeddings_token.token}',
        'Content-Type': 'application/json'
    }
    openai_api_version = os.getenv("OPENAI_API_VERSION", "2023-05-15")
    for chunk in chunks:
        payload = {"input": [chunk]}
        api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version={openai_api_version}"
        try:
            response = session.post(api_url, headers=headers, json=payload)
            if response.status_code == 200:
                data = response.json()
                emb = data['data'][0]['embedding']
                chunk_embeddings.append(emb)
                logger.info(f"Processed chunk (length {len(chunk)})")
            else:
                logger.error(f"API call failed with status code {response.status_code} for chunk starting with '{chunk[:30]}...'")
                chunk_embeddings.append(None)
            time.sleep(0.1)
        except Exception as e:
            logger.error(f"Error processing chunk: {str(e)}")
            chunk_embeddings.append(None)
    valid_embeddings = [emb for emb in chunk_embeddings if emb is not None]
    if not valid_embeddings:
        return None
    return average_embeddings(valid_embeddings)

def get_embeddings_for_documents(documents, endpoint, deployment_name="text-embedding-3-large", max_chars=MAX_TOTAL_CHARS, agentic=False):
    return [get_embedding_for_text(doc, endpoint, deployment_name, max_chars, agentic) for doc in documents]

def test_connection(endpoint):
    try:
        test_text = "Hello World"
        emb = get_embedding_for_text(test_text, endpoint, agentic=AGENTIC_CHUNKING)
        if emb:
            print(f"Test embedding dimension: {len(emb)}")
            return True
        else:
            print("Failed to retrieve embedding")
            return False
    except Exception as e:
        print(f"Connection test error: {str(e)}")
        return False

# ------------------------------
# Configure Azure OpenAI LLM
# ------------------------------
try:
    llm_credential = ClientSecretCredential(
        tenant_id=os.environ["AZURE_TENANT_ID"],
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"]
    )
    llm_token = llm_credential.get_token('https://cognitiveservices.azure.com/.default')
    
    llm_client = AzureOpenAI(
        api_key=llm_token.token,
        base_url=os.getenv("AZURE_OPENAI_ENDPOINT"),
        azure_deployment="gpt-4o-mini",
        api_version="2023-03-15-preview"
    )
    
    def run_llm_agent(prompt: str) -> str:
        try:
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
            )
            answer = response.choices[0].message.content
            logger.info(f"LLM agent response: {answer}")
            return answer
        except Exception as e:
            logger.error(f"Error in LLM agent: {str(e)}")
            return "Error generating response."
            
except Exception as e:
    logger.error(f"Failed to configure Azure OpenAI for LLM: {str(e)}")
    raise

# ------------------------------
# File Processing Functions
# ------------------------------
def process_pdf(file_path):
    documents = []
    try:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    documents.append(text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing PDF file: {str(e)}")
        return []

def process_txt(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        return [text.strip()]
    except Exception as e:
        logger.error(f"Error processing TXT file: {str(e)}")
        return []

def process_csv(file_path):
    documents = []
    try:
        with open(file_path, "r", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            fieldnames = reader.fieldnames
            if not fieldnames:
                logger.error("CSV file has no header")
                return []
            print(f"CSV Columns: {fieldnames}")
            main_col = input("Enter the name of the main column: ").strip()
            support_cols = input("Enter supporting column names (comma-separated), or leave blank: ").split(",")
            support_cols = [col.strip() for col in support_cols if col.strip()]
            for row in reader:
                parts = []
                if main_col in row:
                    parts.append(row[main_col])
                for col in support_cols:
                    if col in row:
                        parts.append(row[col])
                doc_text = " ".join(parts)
                documents.append(doc_text.strip())
        return documents
    except Exception as e:
        logger.error(f"Error processing CSV file: {str(e)}")
        return []

# ------------------------------
# ChromaDB Integration for chromadb==0.5.3
# ------------------------------
persist_dir = "./chroma_db"
try:
    chroma_client = chromadb.PersistentClient(path=persist_dir)
except ValueError as e:
    print("Migration error encountered. Removing old persistent data and retrying.")
    shutil.rmtree(persist_dir)
    chroma_client = chromadb.PersistentClient(path=persist_dir)

collection = chroma_client.get_or_create_collection(name="documents_collection")

def store_documents_in_vector_db(documents, source_filename=""):
    if not documents:
        print("No documents to store.")
        return
    doc_embeddings = get_embeddings_for_documents(documents, embeddings_endpoint, deployment_name="text-embedding-3-large", agentic=AGENTIC_CHUNKING)
    ids = [str(uuid.uuid4()) for _ in documents]
    metadatas = [{"source": source_filename, "doc_index": i} for i in range(len(documents))]
    collection.add(ids=ids, documents=documents, embeddings=doc_embeddings, metadatas=metadatas)
    print(f"Stored {len(documents)} documents in the vector database.")

def answer_question(query: str) -> str:
    try:
        query_emb = get_embedding_for_text(query, embeddings_endpoint, deployment_name="text-embedding-3-large", agentic=AGENTIC_CHUNKING)
        results = collection.query(query_emb, n_results=5)
        context = "\n\n".join(results.get("documents", []))
        combined_prompt = f"Use the following context to answer the question:\n\nContext:\n{context}\n\nQuestion: {query}"
        return run_llm_agent(combined_prompt)
    except Exception as e:
        logger.error(f"Error in answering question: {str(e)}")
        return "Error generating answer."

# ------------------------------
# Main Execution
# ------------------------------
if __name__ == "__main__":
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-azure-endpoint.openai.azure.com")
    if azure_endpoint.startswith("https:") and not azure_endpoint.startswith("https://"):
        azure_endpoint = azure_endpoint.replace("https:", "https://", 1)
    
    if test_connection(azure_endpoint):
        print("Embeddings connection successful")
    else:
        print("Embeddings connection failed")
    
    current_file = input("Enter the path of the file to upload (pdf, txt, csv) or press Enter to skip: ").strip()
    docs = []
    if current_file:
        if current_file.lower().endswith(".pdf"):
            docs = process_pdf(current_file)
        elif current_file.lower().endswith(".txt"):
            docs = process_txt(current_file)
        elif current_file.lower().endswith(".csv"):
            docs = process_csv(current_file)
        else:
            print("Unsupported file type.")
        if docs:
            store_documents_in_vector_db(docs, source_filename=os.path.basename(current_file))
    
    print("\nNow you can ask questions based on the stored data. Type 'exit' to quit.")
    while True:
        user_query = input("Your question: ").strip()
        if user_query.lower() in ["exit", "quit"]:
            break
        print("Answer:", answer_question(user_query))
