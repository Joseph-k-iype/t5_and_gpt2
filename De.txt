"""
Legal Document Processing System
Converts legislation and laws into machine-readable rules using multi-agent orchestration.

Key Features:
- PDF document ingestion and legal rule extraction
- SKOS ontology generation (JSON-LD, TTL, XML)
- Multi-agent system with ReAct agents using LangGraph
- Chain of experts approach with specialist agents
- LangMem for long-term memory
- Dynamic domain classification using LLM capabilities
- Elasticsearch integration with OpenAI embeddings
- Directed acyclic graph support for querying
"""

import os
import json
import asyncio
import logging
import re
import hashlib
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

# Core dependencies
import PyPDF2
import numpy as np
from rdflib import Graph, Namespace, URIRef, Literal
from rdflib.namespace import RDF, RDFS, SKOS, XSD, OWL
from openai import OpenAI

# LangChain/LangGraph dependencies
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.tools import tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# LangMem dependencies
from langmem import create_manage_memory_tool, create_search_memory_tool, create_memory_manager

# Data processing
import spacy
import networkx as nx

# Elasticsearch integration
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import ssl

# =============================================================================
# GLOBAL CONFIGURATION
# =============================================================================

# =============================================================================
# CONFIGURATION - Update these values with your actual credentials
# =============================================================================

# QUICK SETUP: Run "python config_helper.py" to set up your credentials interactively

# Option 1: Set your API keys directly here (for development)
# OPENAI_API_KEY = "sk-your-actual-openai-api-key-here"
# ELASTICSEARCH_USERNAME = "your-elasticsearch-username"
# ELASTICSEARCH_PASSWORD = "your-elasticsearch-password"

# Option 2: Use environment variables (recommended for production)
# The getenv() calls below will use the direct values above if env vars aren't set

# API Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your_openai_api_key_here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_MODEL = "o3-mini-2025-01-31"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
OPENAI_EMBEDDING_DIMENSIONS = 3072
REASONING_EFFORT = "high"

# Elasticsearch Configuration
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost")
ELASTICSEARCH_PORT = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "changeme")
ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "./certs/elasticsearch.crt")
ELASTICSEARCH_INDEX_PREFIX = "legal_documents"

# File Paths
TIKTOKEN_MODELS_PATH = "./tiktoken_models"
SPACY_MODEL_PATH = "./nlp_model/en_core_web_sm-3.8.0.tar.gz"
CONFIG_FILE_PATH = "./legal_config.json"
OUTPUT_DIRECTORY = "./ontology_output"
DOCUMENTS_DIRECTORY = "./documents"

# Set tiktoken cache directory early to use local models and prevent downloads
if Path(TIKTOKEN_MODELS_PATH).exists():
    os.environ["TIKTOKEN_CACHE_DIR"] = str(Path(TIKTOKEN_MODELS_PATH).absolute())
    os.environ["TIKTOKEN_OFFLINE"] = "1"  # Force offline mode - no downloads

# Processing Configuration
MAX_TOKENS_PER_CHUNK = 6000
OVERLAP_TOKENS = 200
EMBEDDING_BATCH_SIZE = 100
MAX_CHUNKS_TO_PROCESS = None  # None for unlimited

# =============================================================================

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def validate_configuration():
    """Validate that all required configuration is properly set."""
    
    issues = []
    
    # Check OpenAI configuration
    if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
        issues.append("❌ OPENAI_API_KEY not set or using placeholder value")
        issues.append("   Edit the global variables at the top of the script with your actual API key")
    elif OPENAI_API_KEY.startswith("sk-"):
        print("✅ OpenAI API key configured")
    else:
        issues.append("⚠️  OPENAI_API_KEY doesn't start with 'sk-' - may be incorrect")
    
    # Show API configuration
    print(f"✅ OpenAI Model: {OPENAI_MODEL}")
    print(f"✅ OpenAI Base URL: {OPENAI_BASE_URL}")
    
    # Check Elasticsearch configuration (optional warnings only)
    if ELASTICSEARCH_USERNAME == "elastic" and ELASTICSEARCH_PASSWORD == "changeme":
        print("⚠️  Using default Elasticsearch credentials")
    else:
        print("✅ Elasticsearch credentials configured")
    
    # Check tiktoken models - prevent downloads
    tiktoken_path = Path(TIKTOKEN_MODELS_PATH)
    if not tiktoken_path.exists():
        issues.append(f"❌ Tiktoken models directory doesn't exist: {TIKTOKEN_MODELS_PATH}")
    else:
        required_models = ["cl100k_base.tiktoken", "p50k_base.tiktoken", "r50k_base.tiktoken"]
        found_models = []
        missing_models = []
        
        for model in required_models:
            if (tiktoken_path / model).exists():
                found_models.append(model)
            else:
                missing_models.append(model)
        
        if found_models:
            print(f"✅ Found tiktoken models: {', '.join(found_models)}")
        
        if missing_models:
            issues.append(f"⚠️  Missing tiktoken models: {', '.join(missing_models)}")
            issues.append(f"   Place these files in {TIKTOKEN_MODELS_PATH}/")
            
        # Test tiktoken loading with offline mode
        try:
            os.environ["TIKTOKEN_CACHE_DIR"] = str(tiktoken_path.absolute())
            os.environ["TIKTOKEN_OFFLINE"] = "1"
            import tiktoken
            
            if found_models:
                # Try to load one of the available models
                if "cl100k_base.tiktoken" in found_models:
                    encoding = tiktoken.get_encoding("cl100k_base")
                    test_tokens = encoding.encode("test")
                    print(f"✅ Tiktoken cl100k_base working: {len(test_tokens)} tokens for 'test'")
                elif "p50k_base.tiktoken" in found_models:
                    encoding = tiktoken.get_encoding("p50k_base") 
                    test_tokens = encoding.encode("test")
                    print(f"✅ Tiktoken p50k_base working: {len(test_tokens)} tokens for 'test'")
                
        except Exception as e:
            issues.append(f"❌ Tiktoken offline loading failed: {e}")
    
    # Check spaCy model
    try:
        import spacy
        nlp = spacy.load("en_core_web_sm")
        print("✅ spaCy model loaded successfully")
    except OSError:
        issues.append("❌ spaCy model 'en_core_web_sm' not found")
        issues.append("   Install with: python -m spacy download en_core_web_sm")
    except Exception as e:
        issues.append(f"❌ spaCy model loading failed: {e}")
    
    return issues

class ElasticsearchManager:
    """Manages Elasticsearch operations with OpenAI embeddings integration."""
    
    def __init__(self):
        """Initialize Elasticsearch connection with SSL and authentication."""
        self.client = self._create_elasticsearch_client()
        self.indices = {
            "documents": f"{ELASTICSEARCH_INDEX_PREFIX}_documents",
            "chunks": f"{ELASTICSEARCH_INDEX_PREFIX}_chunks", 
            "rules": f"{ELASTICSEARCH_INDEX_PREFIX}_rules",
            "concepts": f"{ELASTICSEARCH_INDEX_PREFIX}_concepts"
        }
        self._create_indices()
    
    def _create_elasticsearch_client(self) -> Elasticsearch:
        """Create Elasticsearch client with proper authentication and SSL."""
        try:
            # SSL context configuration
            ssl_context = ssl.create_default_context()
            
            if Path(ELASTICSEARCH_CERT_PATH).exists():
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_REQUIRED
                ssl_context.load_verify_locations(ELASTICSEARCH_CERT_PATH)
                logger.info(f"Using SSL certificate: {ELASTICSEARCH_CERT_PATH}")
            else:
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
                logger.warning(f"SSL certificate not found at {ELASTICSEARCH_CERT_PATH}, using insecure connection")
            
            # Create client with authentication
            client = Elasticsearch(
                [f"https://{ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}"],
                basic_auth=(ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD),
                ssl_context=ssl_context,
                verify_certs=True if Path(ELASTICSEARCH_CERT_PATH).exists() else False,
                timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            # Test connection
            if client.ping():
                logger.info(f"Successfully connected to Elasticsearch at {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
                
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_indices(self):
        """Create Elasticsearch indices with proper mappings for embeddings."""
        
        # Document index mapping
        document_mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "jurisdiction": {"type": "keyword"},
                    "organization": {"type": "keyword"},
                    "country": {"type": "keyword"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "rules_count": {"type": "integer"},
                    "processing_metadata": {"type": "object"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # Chunk index mapping with dense vector for embeddings
        chunk_mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "chunk_id": {"type": "integer"},
                    "text": {"type": "text", "analyzer": "standard"},
                    "section": {"type": "text"},
                    "tokens": {"type": "integer"},
                    "start_char": {"type": "integer"},
                    "end_char": {"type": "integer"},
                    "embedding": {
                        "type": "dense_vector",
                        "dims": OPENAI_EMBEDDING_DIMENSIONS,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "embedding_model": {"type": "keyword"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # Rules index mapping
        rules_mapping = {
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "rule_id": {"type": "keyword"},
                    "subject": {"type": "text", "analyzer": "standard"},
                    "predicate": {"type": "text", "analyzer": "standard"},
                    "object": {"type": "text", "analyzer": "standard"},
                    "modality": {"type": "keyword"},
                    "conditions": {"type": "text"},
                    "exceptions": {"type": "text"},
                    "source_chunk": {"type": "integer"},
                    "chunk_section": {"type": "text"},
                    "domains": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # Concepts index mapping
        concepts_mapping = {
            "mappings": {
                "properties": {
                    "concept_id": {"type": "keyword"},
                    "pref_label": {"type": "text", "analyzer": "standard"},
                    "definition": {"type": "text", "analyzer": "standard"},
                    "concept_type": {"type": "keyword"},
                    "modality": {"type": "keyword"},
                    "broader": {"type": "keyword"},
                    "narrower": {"type": "keyword"},
                    "related": {"type": "keyword"},
                    "domains": {"type": "keyword"},
                    "in_scheme": {"type": "keyword"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # Create indices
        mappings = {
            self.indices["documents"]: document_mapping,
            self.indices["chunks"]: chunk_mapping,
            self.indices["rules"]: rules_mapping,
            self.indices["concepts"]: concepts_mapping
        }
        
        for index_name, mapping in mappings.items():
            try:
                if not self.client.indices.exists(index=index_name):
                    self.client.indices.create(index=index_name, body=mapping)
                    logger.info(f"Created Elasticsearch index: {index_name}")
                else:
                    logger.info(f"Elasticsearch index already exists: {index_name}")
            except Exception as e:
                logger.error(f"Failed to create index {index_name}: {e}")
    
    def store_document(self, document_data: Dict[str, Any]):
        """Store document metadata in Elasticsearch."""
        try:
            self.client.index(
                index=self.indices["documents"],
                id=document_data["document_id"],
                document=document_data
            )
            logger.info(f"Stored document: {document_data['document_id']}")
        except Exception as e:
            logger.error(f"Failed to store document: {e}")
    
    def store_chunks_with_embeddings(self, chunks: List[Dict[str, Any]], document_id: str):
        """Store document chunks with embeddings in Elasticsearch."""
        try:
            actions = []
            for chunk in chunks:
                action = {
                    "_index": self.indices["chunks"],
                    "_id": f"{document_id}_chunk_{chunk['chunk_id']}",
                    "_source": {
                        "document_id": document_id,
                        **chunk,
                        "timestamp": datetime.now().isoformat()
                    }
                }
                actions.append(action)
            
            if actions:
                bulk(self.client, actions)
                logger.info(f"Stored {len(actions)} chunks for document {document_id}")
        except Exception as e:
            logger.error(f"Failed to store chunks: {e}")
    
    def store_rules(self, rules: List[Dict[str, Any]], document_id: str, jurisdiction: str):
        """Store extracted rules in Elasticsearch."""
        try:
            actions = []
            for i, rule in enumerate(rules):
                rule_id = f"{document_id}_rule_{i}"
                action = {
                    "_index": self.indices["rules"],
                    "_id": rule_id,
                    "_source": {
                        "document_id": document_id,
                        "rule_id": rule_id,
                        "jurisdiction": jurisdiction,
                        **rule,
                        "timestamp": datetime.now().isoformat()
                    }
                }
                actions.append(action)
            
            if actions:
                bulk(self.client, actions)
                logger.info(f"Stored {len(actions)} rules for document {document_id}")
        except Exception as e:
            logger.error(f"Failed to store rules: {e}")
    
    def store_concepts(self, concepts: List[Dict[str, Any]]):
        """Store SKOS concepts in Elasticsearch."""
        try:
            actions = []
            for concept in concepts:
                action = {
                    "_index": self.indices["concepts"],
                    "_id": concept["id"],
                    "_source": {
                        "concept_id": concept["id"],
                        "pref_label": concept.get("prefLabel", ""),
                        "definition": concept.get("definition", ""),
                        "concept_type": concept.get("type", ""),
                        "modality": concept.get("modality", ""),
                        "broader": concept.get("broader", []),
                        "narrower": concept.get("narrower", []),
                        "related": concept.get("related", []),
                        "domains": concept.get("domains", []),
                        "in_scheme": concept.get("inScheme", ""),
                        "timestamp": datetime.now().isoformat()
                    }
                }
                actions.append(action)
            
            if actions:
                bulk(self.client, actions)
                logger.info(f"Stored {len(actions)} concepts")
        except Exception as e:
            logger.error(f"Failed to store concepts: {e}")
    
    def semantic_search_chunks(self, query_embedding: List[float], document_id: str = None, 
                              top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform semantic search on chunks using embeddings."""
        try:
            query_body = {
                "knn": {
                    "field": "embedding",
                    "query_vector": query_embedding,
                    "k": top_k,
                    "num_candidates": top_k * 2
                },
                "_source": ["document_id", "chunk_id", "text", "section", "tokens"]
            }
            
            # Add document filter if specified
            if document_id:
                query_body["filter"] = {
                    "term": {"document_id": document_id}
                }
            
            response = self.client.search(
                index=self.indices["chunks"],
                body=query_body
            )
            
            results = []
            for hit in response["hits"]["hits"]:
                result = hit["_source"]
                result["similarity_score"] = hit["_score"]
                results.append(result)
            
            logger.info(f"Semantic search returned {len(results)} chunks")
            return results
            
        except Exception as e:
            logger.error(f"Failed to perform semantic search: {e}")
            return []
    
    def search_rules_by_domain(self, domain: str, jurisdiction: str = None) -> List[Dict[str, Any]]:
        """Search rules by domain and optional jurisdiction."""
        try:
            query_body = {
                "query": {
                    "bool": {
                        "must": []
                    }
                }
            }
            
            # Add domain filter if specified
            if domain:
                query_body["query"]["bool"]["must"].append(
                    {"term": {"domains": domain}}
                )
            else:
                # If no domain specified, match all
                query_body["query"] = {"match_all": {}}
            
            if jurisdiction:
                query_body["query"]["bool"]["must"].append(
                    {"term": {"jurisdiction": jurisdiction}}
                )
            
            response = self.client.search(
                index=self.indices["rules"],
                body=query_body,
                size=100
            )
            
            results = [hit["_source"] for hit in response["hits"]["hits"]]
            logger.info(f"Found {len(results)} rules for domain: {domain}")
            return results
            
        except Exception as e:
            logger.error(f"Failed to search rules by domain: {e}")
            return []
    
    def get_document_statistics(self) -> Dict[str, Any]:
        """Get statistics about stored documents and rules."""
        try:
            stats = {}
            
            for index_type, index_name in self.indices.items():
                count_response = self.client.count(index=index_name)
                stats[f"{index_type}_count"] = count_response["count"]
            
            # Get jurisdiction distribution
            try:
                jurisdiction_agg = self.client.search(
                    index=self.indices["documents"],
                    body={
                        "size": 0,
                        "aggs": {
                            "jurisdictions": {
                                "terms": {"field": "jurisdiction"}
                            }
                        }
                    }
                )
                
                stats["jurisdictions"] = {
                    bucket["key"]: bucket["doc_count"] 
                    for bucket in jurisdiction_agg["aggregations"]["jurisdictions"]["buckets"]
                }
            except Exception as e:
                logger.warning(f"Failed to get jurisdiction statistics: {e}")
                stats["jurisdictions"] = {}
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get statistics: {e}")
            return {}

class LegalDocumentProcessor:
    """Main orchestrator for legal document processing system."""
    
    def __init__(self, config_path: str = None):
        """
        Initialize the legal document processor.
        
        Args:
            config_path: Path to JSON configuration file (optional, uses global default)
        """
        self.config_path = config_path or CONFIG_FILE_PATH
        self.config = self._load_config(self.config_path)
        
        # Initialize OpenAI client using global configuration
        self.openai_client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        # Initialize models using global configuration - ONLY use o3-mini via direct API
        # Skip LangChain entirely since o3-mini is not supported
        self.llm = None  # Force direct API usage
        logger.info(f"Using direct OpenAI API with {OPENAI_MODEL} model")
        
        # Initialize local tiktoken encoding
        self.encoding = self._load_local_tiktoken_encoding()
        
        # Initialize local spaCy model
        self.nlp = self._load_local_spacy_model()
        
        # Initialize Elasticsearch manager
        self.elasticsearch = ElasticsearchManager()
        
        # Initialize memory manager using global configuration
        self.memory_manager = create_memory_manager(
            OPENAI_MODEL,
            instructions="Extract legal rules, obligations, permissions, prohibitions, and domain classifications for data management compliance."
        )
        
        # Initialize knowledge graph
        self.knowledge_graph = nx.DiGraph()
        
        # Initialize RDF graph for SKOS ontology
        self.rdf_graph = Graph()
        self._setup_namespaces()
        
        # Skip multi-agent LangGraph setup - use ONLY direct o3-mini API
        self.workflow = None
        logger.info("Using direct o3-mini API processing (no LangChain agents)")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from JSON file."""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"Config file not found: {config_path}")
            return {"documents": []}
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            return {"documents": []}
    
    def _load_local_tiktoken_encoding(self):
        """Load tiktoken encoding from local models folder - NO DOWNLOADS."""
        try:
            # Disable tiktoken downloads completely
            os.environ["TIKTOKEN_CACHE_DIR"] = str(Path(TIKTOKEN_MODELS_PATH).absolute())
            os.environ["TIKTOKEN_OFFLINE"] = "1"  # Prevent any downloads
            
            import tiktoken
            
            tiktoken_models_path = Path(TIKTOKEN_MODELS_PATH)
            if tiktoken_models_path.exists():
                logger.info(f"Using ONLY local tiktoken models from: {tiktoken_models_path.absolute()}")
                
                # Check for available tiktoken models in the local directory
                available_models = list(tiktoken_models_path.glob("*.tiktoken"))
                logger.info(f"Available tiktoken models: {[m.name for m in available_models]}")
                
                # Try to use cl100k_base locally first
                if (tiktoken_models_path / "cl100k_base.tiktoken").exists():
                    try:
                        logger.info("Loading cl100k_base encoding from local file")
                        encoding = tiktoken.get_encoding("cl100k_base")
                        # Test it works
                        test_tokens = encoding.encode("test")
                        logger.info(f"✅ cl100k_base loaded successfully: {len(test_tokens)} tokens for 'test'")
                        return encoding
                    except Exception as e:
                        logger.warning(f"Failed to load cl100k_base: {e}")
                
                # Try p50k_base as fallback
                if (tiktoken_models_path / "p50k_base.tiktoken").exists():
                    try:
                        logger.info("Loading p50k_base encoding from local file")
                        encoding = tiktoken.get_encoding("p50k_base")
                        test_tokens = encoding.encode("test")
                        logger.info(f"✅ p50k_base loaded successfully: {len(test_tokens)} tokens for 'test'")
                        return encoding
                    except Exception as e:
                        logger.warning(f"Failed to load p50k_base: {e}")
                
                # Try r50k_base as last resort
                if (tiktoken_models_path / "r50k_base.tiktoken").exists():
                    try:
                        logger.info("Loading r50k_base encoding from local file")
                        encoding = tiktoken.get_encoding("r50k_base")
                        test_tokens = encoding.encode("test")
                        logger.info(f"✅ r50k_base loaded successfully: {len(test_tokens)} tokens for 'test'")
                        return encoding
                    except Exception as e:
                        logger.warning(f"Failed to load r50k_base: {e}")
                
                logger.error("None of the local tiktoken models could be loaded")
                return self._create_fallback_tokenizer()
            else:
                logger.error(f"Tiktoken models directory not found: {tiktoken_models_path}")
                return self._create_fallback_tokenizer()
            
        except Exception as e:
            logger.error(f"Failed to load tiktoken encoding: {e}")
            # Fallback: create a simple token counter
            return self._create_fallback_tokenizer()
    
    def _load_local_spacy_model(self):
        """Load spaCy model from installed package or local tar.gz file."""
        try:
            # First try to load from installed package (preferred)
            try:
                import spacy
                nlp = spacy.load("en_core_web_sm")
                logger.info("Successfully loaded spaCy model from installed package")
                return nlp
            except OSError:
                logger.info("Installed spaCy model not found, trying local file...")
            
            # Fallback to local tar.gz file if specified
            spacy_model_path = Path(SPACY_MODEL_PATH)
            if spacy_model_path.exists():
                logger.info(f"Loading spaCy model from: {spacy_model_path}")
                nlp = spacy.load(str(spacy_model_path))
                logger.info("Successfully loaded local spaCy model from tar.gz")
                return nlp
            else:
                logger.error("No spaCy model available. Please install with: python -m spacy download en_core_web_sm")
                raise OSError("No spaCy model available")
                    
        except Exception as e:
            logger.error(f"Failed to load spaCy model: {e}")
            raise
    
    def _create_fallback_tokenizer(self):
        """Create a fallback tokenizer if tiktoken fails."""
        class FallbackTokenizer:
            def encode(self, text: str) -> list:
                # Approximate token count (rough estimate: 1 token ≈ 4 characters)
                return list(range(len(text) // 4))
        
        logger.warning("Using fallback tokenizer - token counts will be approximate")
        return FallbackTokenizer()
    
    def _setup_namespaces(self):
        """Setup RDF namespaces for SKOS ontology with mandatory classes."""
        self.LEG = Namespace("http://legal-ontology.org/legal#")
        self.DATA_MGT = Namespace("http://legal-ontology.org/data-management#")
        self.JURISDICTION = Namespace("http://legal-ontology.org/jurisdiction#")
        self.ORGANIZATION = Namespace("http://legal-ontology.org/organization#")
        self.COUNTRY = Namespace("http://legal-ontology.org/country#")
        self.RULE = Namespace("http://legal-ontology.org/rule#")
        self.DOMAIN = Namespace("http://legal-ontology.org/domain#")
        
        # Bind namespaces
        self.rdf_graph.bind("leg", self.LEG)
        self.rdf_graph.bind("data_mgt", self.DATA_MGT)
        self.rdf_graph.bind("jurisdiction", self.JURISDICTION)
        self.rdf_graph.bind("organization", self.ORGANIZATION)
        self.rdf_graph.bind("country", self.COUNTRY)
        self.rdf_graph.bind("rule", self.RULE)
        self.rdf_graph.bind("domain", self.DOMAIN)
        self.rdf_graph.bind("skos", SKOS)
        self.rdf_graph.bind("rdfs", RDFS)
        self.rdf_graph.bind("owl", OWL)
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI client directly."""
        try:
            response = self.openai_client.embeddings.create(
                model=OPENAI_EMBEDDING_MODEL,
                input=texts,
                dimensions=OPENAI_EMBEDDING_DIMENSIONS
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return []
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF document with improved handling for large files."""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_parts = []
                
                total_pages = len(pdf_reader.pages)
                logger.info(f"Extracting text from {total_pages} pages")
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():  # Only add non-empty pages
                        # Clean up text
                        page_text = self._clean_extracted_text(page_text)
                        text_parts.append(f"[PAGE {page_num + 1}]\n{page_text}")
                    
                    if (page_num + 1) % 10 == 0:
                        logger.info(f"Processed {page_num + 1}/{total_pages} pages")
                
                full_text = "\n\n".join(text_parts)
                logger.info(f"Extracted {len(full_text)} characters from PDF")
                return full_text
                
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    
    def _clean_extracted_text(self, text: str) -> str:
        """Clean extracted PDF text for better processing."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Fix common PDF extraction issues
        text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)  # Fix hyphenated words across lines
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Normalize paragraph breaks
        
        # Remove page headers/footers patterns
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'\d{1,3}\s*$', '', text, flags=re.MULTILINE)  # Remove page numbers at end of line
        
        return text.strip()
    
    def chunk_document_semantically(self, text: str, max_tokens: int = None, overlap_tokens: int = None) -> List[Dict[str, Any]]:
        """
        Chunk document using semantic boundaries (sections, articles, paragraphs).
        
        Args:
            text: Full document text
            max_tokens: Maximum tokens per chunk (uses global default if None)
            overlap_tokens: Overlap between chunks for context preservation (uses global default if None)
            
        Returns:
            List of chunk dictionaries with metadata
        """
        # Use global configuration if not specified
        max_tokens = max_tokens or MAX_TOKENS_PER_CHUNK
        overlap_tokens = overlap_tokens or OVERLAP_TOKENS
        
        # Parse document with spaCy
        doc = self.nlp(text)
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_metadata = {
            "section": None,
            "start_char": 0,
            "end_char": 0,
            "chunk_id": 0
        }
        
        # Legal document section patterns
        section_patterns = [
            r'(?i)^(SECTION|ARTICLE|CHAPTER|PART|CLAUSE|PARAGRAPH)\s+\d+',
            r'(?i)^(\d+\.)+\s',  # Numbered sections like 1.1, 1.2.3
            r'(?i)^[A-Z][A-Z\s]{10,}$',  # ALL CAPS headers
        ]
        
        sentences = list(doc.sents)
        overlap_text = ""
        
        for i, sent in enumerate(sentences):
            sent_text = sent.text.strip()
            sent_tokens = len(self.encoding.encode(sent_text))
            
            # Check if this is a section header
            is_section_header = any(
                re.match(pattern, sent_text) 
                for pattern in section_patterns
            )
            
            # If we hit a section header and have content, finalize current chunk
            if is_section_header and current_chunk:
                self._finalize_chunk(chunks, current_chunk, chunk_metadata, overlap_text)
                
                # Start new chunk with overlap
                current_chunk = overlap_text
                current_tokens = len(self.encoding.encode(current_chunk))
                chunk_metadata = {
                    "section": sent_text,
                    "start_char": sent.start_char,
                    "end_char": sent.end_char,
                    "chunk_id": len(chunks)
                }
            
            # Check if adding this sentence would exceed token limit
            if current_tokens + sent_tokens > max_tokens and current_chunk:
                self._finalize_chunk(chunks, current_chunk, chunk_metadata, overlap_text)
                
                # Start new chunk with overlap
                current_chunk = overlap_text + " " + sent_text
                current_tokens = len(self.encoding.encode(current_chunk))
                chunk_metadata = {
                    "section": chunk_metadata.get("section"),
                    "start_char": sent.start_char,
                    "end_char": sent.end_char,
                    "chunk_id": len(chunks)
                }
            else:
                # Add sentence to current chunk
                if current_chunk:
                    current_chunk += " " + sent_text
                else:
                    current_chunk = sent_text
                    chunk_metadata["start_char"] = sent.start_char
                
                current_tokens += sent_tokens
                chunk_metadata["end_char"] = sent.end_char
            
            # Prepare overlap text (last few sentences)
            if i >= len(sentences) - 3:  # Last 3 sentences for overlap
                overlap_sentences = sentences[max(0, i-2):i+1]
                overlap_text = " ".join([s.text.strip() for s in overlap_sentences])
                overlap_token_count = len(self.encoding.encode(overlap_text))
                if overlap_token_count > overlap_tokens:
                    overlap_text = " ".join([s.text.strip() for s in overlap_sentences[-2:]])
        
        # Finalize last chunk
        if current_chunk:
            self._finalize_chunk(chunks, current_chunk, chunk_metadata, "")
        
        logger.info(f"Document chunked into {len(chunks)} semantic chunks")
        return chunks
    
    def _finalize_chunk(self, chunks: List[Dict[str, Any]], chunk_text: str, 
                       metadata: Dict[str, Any], overlap_text: str):
        """Finalize and add chunk to chunks list."""
        chunk_data = {
            "text": chunk_text.strip(),
            "tokens": len(self.encoding.encode(chunk_text)),
            "chunk_id": metadata["chunk_id"],
            "section": metadata.get("section"),
            "start_char": metadata["start_char"],
            "end_char": metadata["end_char"],
            "overlap": overlap_text.strip() if overlap_text else None
        }
        chunks.append(chunk_data)
    
    def create_chunk_embeddings(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Create embeddings for document chunks using global configuration.
        
        Args:
            chunks: List of chunk dictionaries
            
        Returns:
            Chunks with embeddings added
        """
        # Extract texts for embedding
        chunk_texts = [chunk["text"] for chunk in chunks]
        
        # Generate embeddings in batches using global configuration
        batch_size = EMBEDDING_BATCH_SIZE
        all_embeddings = []
        
        for i in range(0, len(chunk_texts), batch_size):
            batch_texts = chunk_texts[i:i + batch_size]
            batch_embeddings = self.get_embeddings(batch_texts)
            all_embeddings.extend(batch_embeddings)
            
            logger.info(f"Generated embeddings for batch {i//batch_size + 1}/{(len(chunk_texts)-1)//batch_size + 1}")
        
        # Add embeddings to chunks
        for i, chunk in enumerate(chunks):
            if i < len(all_embeddings):
                chunk["embedding"] = all_embeddings[i]
                chunk["embedding_model"] = OPENAI_EMBEDDING_MODEL
                chunk["embedding_dimensions"] = OPENAI_EMBEDDING_DIMENSIONS
        
        return chunks
    
    def find_similar_chunks(self, query_text: str, document_id: str = None, 
                           top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Find chunks most similar to query using Elasticsearch and embeddings.
        
        Args:
            query_text: Query text
            document_id: Specific document to search (optional)
            top_k: Number of top similar chunks to return
            
        Returns:
            Top similar chunks with similarity scores
        """
        # Get query embedding
        query_embeddings = self.get_embeddings([query_text])
        if not query_embeddings:
            return []
        
        query_embedding = query_embeddings[0]
        
        # Use Elasticsearch for semantic search
        return self.elasticsearch.semantic_search_chunks(
            query_embedding=query_embedding,
            document_id=document_id,
            top_k=top_k
        )
    
    def _setup_agents(self):
        """Setup the multi-agent system using LangGraph with global configuration."""
        
        # Define agent state
        class LegalAgentState(MessagesState):
            document_id: str
            jurisdiction: str
            organization: str
            extracted_rules: List[Dict[str, Any]]
            domain_classifications: Dict[str, List[str]]
            ontology_concepts: List[Dict[str, Any]]
            processing_stage: str
            chunk_id: Optional[int] = None
            chunk_context: Optional[Dict[str, Any]] = None
            
        # Create memory tools
        memory_tools = [
            create_manage_memory_tool(namespace=("legal_memory",)),
            create_search_memory_tool(namespace=("legal_memory",))
        ]
        
        # Document Analysis Agent
        @tool
        def analyze_document_structure(text: str) -> Dict[str, Any]:
            """Analyze document structure and extract sections."""
            doc = self.nlp(text)
            
            sections = []
            current_section = None
            
            for sent in doc.sents:
                # Identify section headers using linguistic patterns
                if any(pattern in sent.text.lower() for pattern in 
                      ['section', 'article', 'chapter', 'part', 'clause']):
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        "title": sent.text.strip(),
                        "content": [],
                        "start_pos": sent.start_char
                    }
                elif current_section:
                    current_section["content"].append(sent.text.strip())
            
            if current_section:
                sections.append(current_section)
            
            return {
                "sections": sections,
                "total_sentences": len(list(doc.sents)),
                "entities": [(ent.text, ent.label_) for ent in doc.ents]
            }
        
        # Rule Extraction Agent
        @tool
        def extract_legal_rules(text: str) -> List[Dict[str, Any]]:
            """Extract legal rules using ReAct reasoning with global configuration."""
            
            prompt = f"""
            Analyze the following legal text and extract structured rules, obligations, permissions, and prohibitions.
            
            For each rule found, provide:
            1. Subject (who the rule applies to)
            2. Predicate (the action or state)
            3. Object (what is affected)
            4. Modality (obligation/permission/prohibition)
            5. Conditions (when it applies)
            6. Exceptions (when it doesn't apply)
            
            Text: {text[:3000]}...
            
            Return a JSON list of rules with the structure above.
            """
            
            try:
                response = self.openai_client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "You are a legal expert specializing in extracting structured rules from legal documents."},
                        {"role": "user", "content": prompt}
                    ],
                    reasoning_effort=REASONING_EFFORT
                )
                
                rules_text = response.choices[0].message.content
                if "```json" in rules_text:
                    rules_text = rules_text.split("```json")[1].split("```")[0]
                
                rules = json.loads(rules_text)
                return rules
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from rule extraction")
                return []
            except Exception as e:
                logger.error(f"Error in rule extraction: {e}")
                return []
        
        # Domain Classification Agent
        @tool
        def classify_data_management_domains(rules: List[Dict[str, Any]]) -> Dict[str, List[int]]:
            """Classify rules into data management domains using LLM capabilities with global configuration."""
            
            prompt = f"""
            Classify the following legal rules into data management domains. Analyze the content and dynamically determine relevant domains.
            
            Consider areas such as:
            - Data Storage and Retention
            - Data Usage and Processing
            - Data Movement and Transfer
            - Privacy and Consent
            - Security and Protection
            - Access and Disclosure
            - Data Quality and Integrity
            - Governance and Compliance
            
            For each rule, determine which domains it relates to and explain why.
            
            Rules: {json.dumps(rules, indent=2)}
            
            Return a JSON object mapping domain names to lists of rule indices (0-based).
            """
            
            try:
                response = self.openai_client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "You are a data management expert specializing in regulatory compliance classification."},
                        {"role": "user", "content": prompt}
                    ],
                    reasoning_effort=REASONING_EFFORT
                )
                
                classification_text = response.choices[0].message.content
                if "```json" in classification_text:
                    classification_text = classification_text.split("```json")[1].split("```")[0]
                
                domains = json.loads(classification_text)
                return domains
            except json.JSONDecodeError:
                logger.warning("Failed to parse JSON from domain classification")
                return {}
            except Exception as e:
                logger.error(f"Error in domain classification: {e}")
                return {}
        
        # Ontology Generation Agent
        @tool
        def generate_skos_concepts(rules: List[Dict[str, Any]], domains: Dict[str, List[int]]) -> List[Dict[str, Any]]:
            """Generate SKOS concepts from extracted rules."""
            
            concepts = []
            
            # Generate concepts for each rule
            for i, rule in enumerate(rules):
                concept_id = f"rule_{i}_{hashlib.md5(str(rule).encode()).hexdigest()[:8]}"
                
                concept = {
                    "id": concept_id,
                    "type": "skos:Concept",
                    "prefLabel": f"{rule.get('subject', '')} {rule.get('predicate', '')}",
                    "definition": self._generate_rule_definition(rule),
                    "broader": [],
                    "narrower": [],
                    "related": [],
                    "inScheme": "legal_compliance_scheme",
                    "modality": rule.get("modality", ""),
                    "conditions": rule.get("conditions", []),
                    "exceptions": rule.get("exceptions", []),
                    "domains": [domain for domain, rule_indices in domains.items() if i in rule_indices]
                }
                
                concepts.append(concept)
            
            # Generate domain concepts
            for domain_name in domains.keys():
                domain_concept = {
                    "id": f"domain_{domain_name.lower().replace(' ', '_')}",
                    "type": "skos:Concept",
                    "prefLabel": domain_name,
                    "definition": f"Legal rules and requirements related to {domain_name.lower()}",
                    "broader": ["data_management"],
                    "narrower": [c["id"] for c in concepts if domain_name in c.get("domains", [])],
                    "topConceptOf": "legal_compliance_scheme"
                }
                concepts.append(domain_concept)
            
            return concepts
        
        # Create supervisor agent
        def supervisor_node(state: LegalAgentState):
            """Supervisor agent that orchestrates the workflow."""
            
            stage = state.get("processing_stage", "start")
            
            if stage == "start":
                return {
                    "messages": [AIMessage(content="Starting document analysis")],
                    "processing_stage": "document_analysis"
                }
            elif stage == "document_analysis":
                return {
                    "messages": [AIMessage(content="Moving to rule extraction")],
                    "processing_stage": "rule_extraction"
                }
            elif stage == "rule_extraction":
                return {
                    "messages": [AIMessage(content="Moving to domain classification")],
                    "processing_stage": "domain_classification"
                }
            elif stage == "domain_classification":
                return {
                    "messages": [AIMessage(content="Moving to ontology generation")],
                    "processing_stage": "ontology_generation"
                }
            else:
                return {
                    "messages": [AIMessage(content="Processing complete")],
                    "processing_stage": "complete"
                }

        # Create specialized agents with fallback for o3-mini
        if self.llm:
            self.document_analyzer = create_react_agent(
                self.llm,
                tools=[analyze_document_structure] + memory_tools
            )
            
            self.rule_extractor = create_react_agent(
                self.llm,
                tools=[extract_legal_rules] + memory_tools
            )
            
            self.domain_classifier = create_react_agent(
                self.llm,
                tools=[classify_data_management_domains] + memory_tools
            )
            
            self.ontology_generator = create_react_agent(
                self.llm,
                tools=[generate_skos_concepts] + memory_tools
            )
            
            # Build the workflow graph
            workflow = StateGraph(LegalAgentState)
            
            # Add nodes
            workflow.add_node("supervisor", supervisor_node)
            workflow.add_node("document_analyzer", self.document_analyzer)
            workflow.add_node("rule_extractor", self.rule_extractor)
            workflow.add_node("domain_classifier", self.domain_classifier)
            workflow.add_node("ontology_generator", self.ontology_generator)
            
            # Add edges
            workflow.add_edge(START, "supervisor")
            workflow.add_edge("supervisor", "document_analyzer")
            workflow.add_edge("document_analyzer", "rule_extractor")
            workflow.add_edge("rule_extractor", "domain_classifier")
            workflow.add_edge("domain_classifier", "ontology_generator")
            workflow.add_edge("ontology_generator", END)
            
            # Compile the workflow
            self.workflow = workflow.compile(checkpointer=MemorySaver())
            logger.info("Successfully initialized multi-agent workflow")
        else:
            logger.warning("LangChain agents not available, using direct OpenAI API processing")
            self.workflow = None
    
    def _generate_rule_definition(self, rule: Dict[str, Any]) -> str:
        """Generate a human-readable definition for a rule."""
        modality = rule.get("modality", "").lower()
        subject = rule.get("subject", "")
        predicate = rule.get("predicate", "")
        obj = rule.get("object", "")
        
        if modality == "obligation":
            return f"{subject} must {predicate} {obj}"
        elif modality == "permission":
            return f"{subject} may {predicate} {obj}"
        elif modality == "prohibition":
            return f"{subject} must not {predicate} {obj}"
        else:
            return f"{subject} {predicate} {obj}"
    
    async def process_document(self, pdf_path: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process a legal document through the multi-agent system with Elasticsearch integration."""
        
        # Extract text from PDF
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            raise ValueError("Failed to extract text from PDF")
        
        logger.info(f"Processing document: {document_metadata.get('id', 'unknown')}")
        logger.info(f"Document length: {len(text)} characters")
        
        # Create semantic chunks for large documents using global configuration
        chunks = self.chunk_document_semantically(text)
        logger.info(f"Document split into {len(chunks)} chunks")
        
        # Limit chunks if configured
        if MAX_CHUNKS_TO_PROCESS and len(chunks) > MAX_CHUNKS_TO_PROCESS:
            chunks = chunks[:MAX_CHUNKS_TO_PROCESS]
            logger.info(f"Limited to {MAX_CHUNKS_TO_PROCESS} chunks for processing")
        
        # Create embeddings for chunks
        chunks_with_embeddings = self.create_chunk_embeddings(chunks)
        
        # Store document in Elasticsearch
        document_data = {
            "document_id": document_metadata.get("id", "unknown"),
            "title": document_metadata.get("title", ""),
            "jurisdiction": document_metadata.get("jurisdiction", "unknown"),
            "organization": document_metadata.get("organization", "unknown"),
            "country": document_metadata.get("country", ""),
            "content": text[:10000],  # Store first 10k characters
            "rules_count": 0,  # Will be updated after rule extraction
            "processing_metadata": {
                "total_chunks": len(chunks_with_embeddings),
                "total_characters": len(text),
                "chunks_processed": len([c for c in chunks_with_embeddings if "embedding" in c])
            },
            "timestamp": datetime.now().isoformat()
        }
        self.elasticsearch.store_document(document_data)
        
        # Store chunks with embeddings in Elasticsearch
        self.elasticsearch.store_chunks_with_embeddings(
            chunks_with_embeddings, 
            document_metadata.get("id", "unknown")
        )
        
        # Process chunks using direct o3-mini API (no LangChain)
        result = await self._process_with_direct_api(chunks_with_embeddings, document_metadata)
        
        # Store results in Elasticsearch
        self.elasticsearch.store_rules(
            result["extracted_rules"], 
            document_metadata.get("id", "unknown"),
            document_metadata.get("jurisdiction", "unknown")
        )
        self.elasticsearch.store_concepts(result["ontology_concepts"])
        
        # Update document with final rule count
        document_data["rules_count"] = len(result["extracted_rules"])
        self.elasticsearch.store_document(document_data)
        
        # Generate ontology outputs
        ontology_outputs = self._generate_ontology_outputs(result)
        
        # Update knowledge graph
        self._update_knowledge_graph(result, chunks_with_embeddings)
        
        # Store in long-term memory
        await self._store_in_memory(result, chunks_with_embeddings)
        
        return {
            "document_id": result["document_id"],
            "jurisdiction": result["jurisdiction"],
            "organization": result["organization"],
            "rules": result["extracted_rules"],
            "domains": result["domain_classifications"],
            "concepts": result["ontology_concepts"],
            "ontology": ontology_outputs,
            "chunks": chunks_with_embeddings,
            "processing_metadata": result["processing_metadata"],
            "knowledge_graph_nodes": len(self.knowledge_graph.nodes),
            "knowledge_graph_edges": len(self.knowledge_graph.edges),
            "elasticsearch_stats": self.elasticsearch.get_document_statistics()
        }
    
    async def _process_with_direct_api(self, chunks_with_embeddings: List[Dict[str, Any]], document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process chunks using direct OpenAI API calls."""
        
        all_rules = []
        all_domain_classifications = {}
        all_concepts = []
        
        logger.info("Using direct OpenAI API processing (o3-mini compatible)")
        
        # Process each chunk
        for i, chunk in enumerate(chunks_with_embeddings):
            logger.info(f"Processing chunk {i+1}/{len(chunks_with_embeddings)} with direct API")
            
            try:
                # Extract rules from chunk
                rules = await self._extract_rules_direct(chunk["text"])
                
                # Add chunk metadata to rules
                for rule in rules:
                    rule["source_chunk"] = i
                    rule["chunk_section"] = chunk.get("section")
                
                all_rules.extend(rules)
                
                # Classify domains for these rules
                if rules:
                    domains = await self._classify_domains_direct(rules)
                    
                    # Merge domain classifications
                    for domain, rule_indices in domains.items():
                        if domain not in all_domain_classifications:
                            all_domain_classifications[domain] = []
                        # Adjust rule indices to account for previous chunks
                        adjusted_indices = [idx + len(all_rules) - len(rules) for idx in rule_indices]
                        all_domain_classifications[domain].extend(adjusted_indices)
                
            except Exception as e:
                logger.error(f"Error processing chunk {i} with direct API: {e}")
                continue
        
        # Generate concepts from all rules
        try:
            all_concepts = await self._generate_concepts_direct(all_rules, all_domain_classifications)
        except Exception as e:
            logger.error(f"Error generating concepts: {e}")
            all_concepts = []
        
        return {
            "document_id": document_metadata.get("id", "unknown"),
            "jurisdiction": document_metadata.get("jurisdiction", "unknown"),
            "organization": document_metadata.get("organization", "unknown"),
            "extracted_rules": all_rules,
            "domain_classifications": all_domain_classifications,
            "ontology_concepts": all_concepts,
            "processing_metadata": {
                "total_chunks": len(chunks_with_embeddings),
                "processing_method": "direct_openai_api"
            }
        }
    
    async def _extract_rules_direct(self, text: str) -> List[Dict[str, Any]]:
        """Extract legal rules using direct OpenAI API."""
        
        prompt = f"""
        Analyze the following legal text and extract structured rules, obligations, permissions, and prohibitions.
        
        For each rule found, provide:
        1. Subject (who the rule applies to)
        2. Predicate (the action or state)
        3. Object (what is affected)
        4. Modality (obligation/permission/prohibition)
        5. Conditions (when it applies)
        6. Exceptions (when it doesn't apply)
        
        Text: {text[:3000]}...
        
        Return a JSON list of rules with the structure above.
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "You are a legal expert specializing in extracting structured rules from legal documents. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                reasoning_effort=REASONING_EFFORT
            )
            
            rules_text = response.choices[0].message.content
            if "```json" in rules_text:
                rules_text = rules_text.split("```json")[1].split("```")[0]
            
            rules = json.loads(rules_text)
            return rules if isinstance(rules, list) else []
            
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON from rule extraction: {e}")
            return []
        except Exception as e:
            logger.error(f"Error in direct rule extraction: {e}")
            return []
    
    async def _classify_domains_direct(self, rules: List[Dict[str, Any]]) -> Dict[str, List[int]]:
        """Classify rules into domains using direct OpenAI API."""
        
        prompt = f"""
        Classify the following legal rules into data management domains. Analyze the content and dynamically determine relevant domains.
        
        Consider areas such as:
        - Data Storage and Retention
        - Data Usage and Processing
        - Data Movement and Transfer
        - Privacy and Consent
        - Security and Protection
        - Access and Disclosure
        - Data Quality and Integrity
        - Governance and Compliance
        
        Rules: {json.dumps(rules, indent=2)}
        
        Return a JSON object mapping domain names to lists of rule indices (0-based).
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "You are a data management expert specializing in regulatory compliance classification. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                reasoning_effort=REASONING_EFFORT
            )
            
            classification_text = response.choices[0].message.content
            if "```json" in classification_text:
                classification_text = classification_text.split("```json")[1].split("```")[0]
            
            domains = json.loads(classification_text)
            return domains if isinstance(domains, dict) else {}
            
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON from domain classification: {e}")
            return {}
        except Exception as e:
            logger.error(f"Error in direct domain classification: {e}")
            return {}
    
    async def _generate_concepts_direct(self, rules: List[Dict[str, Any]], domains: Dict[str, List[int]]) -> List[Dict[str, Any]]:
        """Generate SKOS concepts using direct OpenAI API."""
        
        try:
            concepts = []
            
            # Generate concepts for each rule
            for i, rule in enumerate(rules):
                concept_id = f"rule_{i}_{hashlib.md5(str(rule).encode()).hexdigest()[:8]}"
                
                concept = {
                    "id": concept_id,
                    "type": "skos:Concept",
                    "prefLabel": f"{rule.get('subject', '')} {rule.get('predicate', '')}",
                    "definition": self._generate_rule_definition(rule),
                    "broader": [],
                    "narrower": [],
                    "related": [],
                    "inScheme": "legal_compliance_scheme",
                    "modality": rule.get("modality", ""),
                    "conditions": rule.get("conditions", []),
                    "exceptions": rule.get("exceptions", []),
                    "domains": [domain for domain, rule_indices in domains.items() if i in rule_indices]
                }
                
                concepts.append(concept)
            
            # Generate domain concepts
            for domain_name in domains.keys():
                domain_concept = {
                    "id": f"domain_{domain_name.lower().replace(' ', '_')}",
                    "type": "skos:Concept",
                    "prefLabel": domain_name,
                    "definition": f"Legal rules and requirements related to {domain_name.lower()}",
                    "broader": ["data_management"],
                    "narrower": [c["id"] for c in concepts if domain_name in c.get("domains", [])],
                    "topConceptOf": "legal_compliance_scheme"
                }
                concepts.append(domain_concept)
            
            return concepts
            
        except Exception as e:
            logger.error(f"Error generating concepts: {e}")
            return []
    
    def process_all_documents(self) -> List[Dict[str, Any]]:
        """Process all documents defined in the configuration."""
        results = []
        
        for doc_config in self.config["documents"]:
            try:
                logger.info(f"Processing document: {doc_config['id']}")
                
                # Check if file exists
                if not Path(doc_config["path"]).exists():
                    logger.warning(f"File not found: {doc_config['path']}")
                    results.append({
                        "document_id": doc_config['id'],
                        "error": f"File not found: {doc_config['path']}",
                        "success": False
                    })
                    continue
                
                # Run async processing
                result = asyncio.run(self.process_document(
                    doc_config["path"],
                    doc_config
                ))
                
                result["success"] = True
                results.append(result)
                logger.info(f"Successfully processed {doc_config['id']}: {len(result['rules'])} rules extracted")
                
            except Exception as e:
                logger.error(f"Failed to process document {doc_config['id']}: {e}")
                results.append({
                    "document_id": doc_config['id'],
                    "error": str(e),
                    "success": False
                })
        
        return results
    
    def _generate_ontology_outputs(self, processing_result: Dict[str, Any]) -> Dict[str, str]:
        """Generate SKOS ontology in multiple formats with mandatory jurisdiction/country/organization classes."""
        
        # Clear existing graph
        self.rdf_graph = Graph()
        self._setup_namespaces()
        
        # Create main concept scheme
        scheme_uri = self.LEG["legal_compliance_scheme"]
        self.rdf_graph.add((scheme_uri, RDF.type, SKOS.ConceptScheme))
        self.rdf_graph.add((scheme_uri, SKOS.prefLabel, Literal("Legal Compliance Ontology", lang="en")))
        self.rdf_graph.add((scheme_uri, RDFS.comment, Literal("Machine-readable SKOS ontology for legal compliance rules, jurisdictions, organizations, and data management requirements", lang="en")))
        self.rdf_graph.add((scheme_uri, SKOS.definition, Literal("A comprehensive ontology for legal document processing that enables querying from any starting point including jurisdiction, country, organization, domain, or rule level", lang="en")))
        
        # Create mandatory top-level concept classes
        top_level_concepts = {
            "jurisdiction_class": {
                "uri": self.LEG["JurisdictionClass"],
                "label": "Legal Jurisdiction",
                "definition": "Top-level class for all legal jurisdictions and regulatory authorities"
            },
            "country_class": {
                "uri": self.LEG["CountryClass"], 
                "label": "Country",
                "definition": "Top-level class for all countries and geographical regions"
            },
            "organization_class": {
                "uri": self.LEG["OrganizationClass"],
                "label": "Organization",
                "definition": "Top-level class for all organizations, regulatory bodies, and institutions"
            },
            "domain_class": {
                "uri": self.LEG["DomainClass"],
                "label": "Data Management Domain",
                "definition": "Top-level class for all data management and compliance domains"
            },
            "rule_class": {
                "uri": self.LEG["RuleClass"],
                "label": "Legal Rule",
                "definition": "Top-level class for all legal rules, obligations, permissions, and prohibitions"
            },
            "document_class": {
                "uri": self.LEG["DocumentClass"],
                "label": "Legal Document",
                "definition": "Top-level class for all legal documents and legislation"
            }
        }
        
        # Add top-level concept classes
        for concept_id, concept_data in top_level_concepts.items():
            concept_uri = concept_data["uri"]
            self.rdf_graph.add((concept_uri, RDF.type, SKOS.Concept))
            self.rdf_graph.add((concept_uri, SKOS.prefLabel, Literal(concept_data["label"], lang="en")))
            self.rdf_graph.add((concept_uri, SKOS.definition, Literal(concept_data["definition"], lang="en")))
            self.rdf_graph.add((concept_uri, SKOS.inScheme, scheme_uri))
            self.rdf_graph.add((concept_uri, SKOS.topConceptOf, scheme_uri))
            # Add custom properties for machine readability
            self.rdf_graph.add((concept_uri, self.LEG.conceptLevel, Literal("top_level")))
            self.rdf_graph.add((concept_uri, self.LEG.queryable, Literal(True)))
        
        # Create jurisdiction instances
        jurisdiction = processing_result.get("jurisdiction", "unknown")
        organization = processing_result.get("organization", "unknown")
        
        jurisdiction_uri = self.JURISDICTION[self._safe_uri_name(jurisdiction)]
        self.rdf_graph.add((jurisdiction_uri, RDF.type, SKOS.Concept))
        self.rdf_graph.add((jurisdiction_uri, SKOS.prefLabel, Literal(jurisdiction, lang="en")))
        self.rdf_graph.add((jurisdiction_uri, SKOS.definition, Literal(f"Legal jurisdiction: {jurisdiction}", lang="en")))
        self.rdf_graph.add((jurisdiction_uri, SKOS.inScheme, scheme_uri))
        self.rdf_graph.add((jurisdiction_uri, SKOS.broader, self.LEG["JurisdictionClass"]))
        self.rdf_graph.add((jurisdiction_uri, self.LEG.conceptLevel, Literal("jurisdiction")))
        self.rdf_graph.add((jurisdiction_uri, self.LEG.queryable, Literal(True)))
        
        # Create organization instance
        organization_uri = self.ORGANIZATION[self._safe_uri_name(organization)]
        self.rdf_graph.add((organization_uri, RDF.type, SKOS.Concept))
        self.rdf_graph.add((organization_uri, SKOS.prefLabel, Literal(organization, lang="en")))
        self.rdf_graph.add((organization_uri, SKOS.definition, Literal(f"Regulatory organization: {organization}", lang="en")))
        self.rdf_graph.add((organization_uri, SKOS.inScheme, scheme_uri))
        self.rdf_graph.add((organization_uri, SKOS.broader, self.LEG["OrganizationClass"]))
        self.rdf_graph.add((organization_uri, self.LEG.conceptLevel, Literal("organization")))
        self.rdf_graph.add((organization_uri, self.LEG.queryable, Literal(True)))
        
        # Link jurisdiction to organization
        self.rdf_graph.add((jurisdiction_uri, self.LEG.governedBy, organization_uri))
        self.rdf_graph.add((organization_uri, self.LEG.governs, jurisdiction_uri))
        
        # Create country instance if different from jurisdiction
        country = processing_result.get("country", jurisdiction)
        if country != jurisdiction:
            country_uri = self.COUNTRY[self._safe_uri_name(country)]
            self.rdf_graph.add((country_uri, RDF.type, SKOS.Concept))
            self.rdf_graph.add((country_uri, SKOS.prefLabel, Literal(country, lang="en")))
            self.rdf_graph.add((country_uri, SKOS.definition, Literal(f"Country or region: {country}", lang="en")))
            self.rdf_graph.add((country_uri, SKOS.inScheme, scheme_uri))
            self.rdf_graph.add((country_uri, SKOS.broader, self.LEG["CountryClass"]))
            self.rdf_graph.add((country_uri, self.LEG.conceptLevel, Literal("country")))
            self.rdf_graph.add((country_uri, self.LEG.queryable, Literal(True)))
            
            # Link jurisdiction to country
            self.rdf_graph.add((jurisdiction_uri, self.LEG.withinCountry, country_uri))
            self.rdf_graph.add((country_uri, self.LEG.hasJurisdiction, jurisdiction_uri))
        
        # Create document instance
        document_id = processing_result.get("document_id", "unknown")
        document_uri = self.LEG[self._safe_uri_name(f"document_{document_id}")]
        self.rdf_graph.add((document_uri, RDF.type, SKOS.Concept))
        self.rdf_graph.add((document_uri, SKOS.prefLabel, Literal(f"Document: {document_id}", lang="en")))
        self.rdf_graph.add((document_uri, SKOS.definition, Literal(f"Legal document processed from {document_id}", lang="en")))
        self.rdf_graph.add((document_uri, SKOS.inScheme, scheme_uri))
        self.rdf_graph.add((document_uri, SKOS.broader, self.LEG["DocumentClass"]))
        self.rdf_graph.add((document_uri, self.LEG.conceptLevel, Literal("document")))
        self.rdf_graph.add((document_uri, self.LEG.queryable, Literal(True)))
        
        # Link document to jurisdiction and organization
        self.rdf_graph.add((document_uri, self.LEG.fromJurisdiction, jurisdiction_uri))
        self.rdf_graph.add((document_uri, self.LEG.issuedBy, organization_uri))
        self.rdf_graph.add((jurisdiction_uri, self.LEG.hasDocument, document_uri))
        self.rdf_graph.add((organization_uri, self.LEG.issues, document_uri))
        
        # Create domain concepts with proper hierarchy
        domains = processing_result.get("domain_classifications", {})
        domain_uris = {}
        
        for domain_name in domains.keys():
            domain_uri = self.DOMAIN[self._safe_uri_name(domain_name)]
            domain_uris[domain_name] = domain_uri
            
            self.rdf_graph.add((domain_uri, RDF.type, SKOS.Concept))
            self.rdf_graph.add((domain_uri, SKOS.prefLabel, Literal(domain_name, lang="en")))
            self.rdf_graph.add((domain_uri, SKOS.definition, Literal(f"Data management domain: {domain_name}", lang="en")))
            self.rdf_graph.add((domain_uri, SKOS.inScheme, scheme_uri))
            self.rdf_graph.add((domain_uri, SKOS.broader, self.LEG["DomainClass"]))
            self.rdf_graph.add((domain_uri, self.LEG.conceptLevel, Literal("domain")))
            self.rdf_graph.add((domain_uri, self.LEG.queryable, Literal(True)))
            
            # Link domain to jurisdiction
            self.rdf_graph.add((domain_uri, self.LEG.appliesInJurisdiction, jurisdiction_uri))
            self.rdf_graph.add((jurisdiction_uri, self.LEG.hasApplicableDomain, domain_uri))
        
        # Add rule concepts with comprehensive metadata
        rules = processing_result.get("extracted_rules", [])
        for i, rule in enumerate(rules):
            rule_id = f"rule_{document_id}_{i}"
            rule_uri = self.RULE[self._safe_uri_name(rule_id)]
            
            # Basic concept properties
            self.rdf_graph.add((rule_uri, RDF.type, SKOS.Concept))
            
            # Create readable label
            subject = rule.get("subject", "Unknown")
            predicate = rule.get("predicate", "action")
            rule_label = f"{subject} {predicate}"
            
            self.rdf_graph.add((rule_uri, SKOS.prefLabel, Literal(rule_label, lang="en")))
            self.rdf_graph.add((rule_uri, SKOS.definition, Literal(self._generate_rule_definition(rule), lang="en")))
            self.rdf_graph.add((rule_uri, SKOS.inScheme, scheme_uri))
            self.rdf_graph.add((rule_uri, SKOS.broader, self.LEG["RuleClass"]))
            
            # Rule-specific properties for machine readability
            self.rdf_graph.add((rule_uri, self.LEG.conceptLevel, Literal("rule")))
            self.rdf_graph.add((rule_uri, self.LEG.queryable, Literal(True)))
            self.rdf_graph.add((rule_uri, self.LEG.ruleId, Literal(rule_id)))
            self.rdf_graph.add((rule_uri, self.LEG.ruleIndex, Literal(i)))
            
            # Add detailed rule components
            if rule.get("subject"):
                self.rdf_graph.add((rule_uri, self.LEG.subject, Literal(rule["subject"], lang="en")))
            if rule.get("predicate"):
                self.rdf_graph.add((rule_uri, self.LEG.predicate, Literal(rule["predicate"], lang="en")))
            if rule.get("object"):
                self.rdf_graph.add((rule_uri, self.LEG.object, Literal(rule["object"], lang="en")))
            if rule.get("modality"):
                self.rdf_graph.add((rule_uri, self.LEG.modality, Literal(rule["modality"])))
            
            # Add conditions and exceptions as structured data
            conditions = rule.get("conditions", [])
            if conditions:
                if isinstance(conditions, list):
                    for condition in conditions:
                        self.rdf_graph.add((rule_uri, self.LEG.hasCondition, Literal(str(condition), lang="en")))
                else:
                    self.rdf_graph.add((rule_uri, self.LEG.hasCondition, Literal(str(conditions), lang="en")))
            
            exceptions = rule.get("exceptions", [])
            if exceptions:
                if isinstance(exceptions, list):
                    for exception in exceptions:
                        self.rdf_graph.add((rule_uri, self.LEG.hasException, Literal(str(exception), lang="en")))
                else:
                    self.rdf_graph.add((rule_uri, self.LEG.hasException, Literal(str(exceptions), lang="en")))
            
            # Link rule to document, jurisdiction, and organization
            self.rdf_graph.add((rule_uri, self.LEG.fromDocument, document_uri))
            self.rdf_graph.add((rule_uri, self.LEG.governedByJurisdiction, jurisdiction_uri))
            self.rdf_graph.add((rule_uri, self.LEG.enforcedBy, organization_uri))
            
            # Reverse links for queryability
            self.rdf_graph.add((document_uri, self.LEG.containsRule, rule_uri))
            self.rdf_graph.add((jurisdiction_uri, self.LEG.governsRule, rule_uri))
            self.rdf_graph.add((organization_uri, self.LEG.enforcesRule, rule_uri))
            
            # Link rules to domains
            for domain_name, rule_indices in domains.items():
                if i in rule_indices:
                    domain_uri = domain_uris[domain_name]
                    self.rdf_graph.add((rule_uri, self.LEG.appliesToDomain, domain_uri))
                    self.rdf_graph.add((domain_uri, self.LEG.hasApplicableRule, rule_uri))
            
            # Add chunk-level metadata for traceability
            if rule.get("source_chunk") is not None:
                self.rdf_graph.add((rule_uri, self.LEG.sourceChunk, Literal(rule["source_chunk"])))
            if rule.get("chunk_section"):
                self.rdf_graph.add((rule_uri, self.LEG.chunkSection, Literal(rule["chunk_section"], lang="en")))
        
        # Add cross-references and related concepts for enhanced queryability
        self._add_cross_references(jurisdiction_uri, organization_uri, domain_uris, document_uri)
        
        # Generate machine-readable output formats
        outputs = {
            "json_ld": self.rdf_graph.serialize(format="json-ld"),
            "turtle": self.rdf_graph.serialize(format="turtle"),
            "xml": self.rdf_graph.serialize(format="xml"),
            "n3": self.rdf_graph.serialize(format="n3"),
            "nt": self.rdf_graph.serialize(format="nt")
        }
        
        return outputs
    
    def _safe_uri_name(self, name: str) -> str:
        """Convert a name to a safe URI component."""
        # Replace spaces and special characters with underscores
        safe_name = re.sub(r'[^\w\-_]', '_', str(name))
        # Remove multiple underscores
        safe_name = re.sub(r'_+', '_', safe_name)
        # Remove leading/trailing underscores
        safe_name = safe_name.strip('_')
        return safe_name or "unknown"
    
    def _add_cross_references(self, jurisdiction_uri, organization_uri, domain_uris, document_uri):
        """Add cross-references between concepts for enhanced queryability."""
        
        # Add semantic relationships between domains
        domain_relationships = {
            "Privacy and Consent": ["Data Usage and Processing", "Data Storage and Retention"],
            "Security and Protection": ["Data Storage and Retention", "Access and Disclosure"],
            "Governance and Compliance": ["Audit and Monitoring", "Data Quality and Integrity"],
            "Data Movement and Transfer": ["Security and Protection", "Privacy and Consent"]
        }
        
        for source_domain, related_domains in domain_relationships.items():
            if source_domain in domain_uris:
                source_uri = domain_uris[source_domain]
                for related_domain in related_domains:
                    if related_domain in domain_uris:
                        related_uri = domain_uris[related_domain]
                        self.rdf_graph.add((source_uri, SKOS.related, related_uri))
        
        # Add temporal and hierarchical metadata
        self.rdf_graph.add((document_uri, self.LEG.processedDate, Literal(datetime.now().isoformat())))
        self.rdf_graph.add((jurisdiction_uri, self.LEG.lastUpdated, Literal(datetime.now().isoformat())))
        
        # Add counts for machine processing
        rule_count = len([s for s, p, o in self.rdf_graph.triples((None, SKOS.broader, self.LEG["RuleClass"]))])
        domain_count = len([s for s, p, o in self.rdf_graph.triples((None, SKOS.broader, self.LEG["DomainClass"]))])
        
        self.rdf_graph.add((document_uri, self.LEG.ruleCount, Literal(rule_count)))
        self.rdf_graph.add((jurisdiction_uri, self.LEG.domainCount, Literal(domain_count)))
    
    def _update_knowledge_graph(self, result: Dict[str, Any], chunks: List[Dict[str, Any]] = None):
        """Update the knowledge graph with extracted information including chunks."""
        
        document_id = result["document_id"]
        jurisdiction = result["jurisdiction"]
        
        # Add document node
        self.knowledge_graph.add_node(
            document_id,
            type="document",
            jurisdiction=jurisdiction,
            organization=result["organization"],
            rules_count=len(result.get("extracted_rules", [])),
            processing_metadata=result.get("processing_metadata", {})
        )
        
        # Add chunk nodes if available
        if chunks:
            for chunk in chunks:
                chunk_id = f"{document_id}_chunk_{chunk['chunk_id']}"
                
                self.knowledge_graph.add_node(
                    chunk_id,
                    type="chunk",
                    document_id=document_id,
                    chunk_id=chunk["chunk_id"],
                    text=chunk["text"][:500] + "..." if len(chunk["text"]) > 500 else chunk["text"],
                    section=chunk.get("section"),
                    tokens=chunk["tokens"],
                    has_embedding="embedding" in chunk,
                    start_char=chunk.get("start_char"),
                    end_char=chunk.get("end_char")
                )
                
                # Connect document to chunk
                self.knowledge_graph.add_edge(document_id, chunk_id, relationship="contains_chunk")
        
        # Add rule nodes and relationships
        for i, rule in enumerate(result.get("extracted_rules", [])):
            rule_id = f"{document_id}_rule_{i}"
            
            self.knowledge_graph.add_node(
                rule_id,
                type="rule",
                modality=rule.get("modality"),
                subject=rule.get("subject"),
                predicate=rule.get("predicate"),
                object=rule.get("object"),
                conditions=rule.get("conditions", []),
                exceptions=rule.get("exceptions", []),
                source_chunk=rule.get("source_chunk"),
                chunk_section=rule.get("chunk_section")
            )
            
            # Connect document to rule
            self.knowledge_graph.add_edge(document_id, rule_id, relationship="contains_rule")
            
            # Connect chunk to rule if chunk information available
            if "source_chunk" in rule and chunks:
                chunk_id = f"{document_id}_chunk_{rule['source_chunk']}"
                if self.knowledge_graph.has_node(chunk_id):
                    self.knowledge_graph.add_edge(chunk_id, rule_id, relationship="contains_rule")
            
            # Connect to domains
            for domain, rule_indices in result.get("domain_classifications", {}).items():
                if i in rule_indices:
                    domain_node = f"domain_{domain.lower().replace(' ', '_')}"
                    if not self.knowledge_graph.has_node(domain_node):
                        self.knowledge_graph.add_node(domain_node, type="domain", name=domain)
                    
                    self.knowledge_graph.add_edge(rule_id, domain_node, relationship="belongs_to_domain")
        
        logger.info(f"Updated knowledge graph: {len(self.knowledge_graph.nodes)} nodes, {len(self.knowledge_graph.edges)} edges")
    
    async def _store_in_memory(self, result: Dict[str, Any], chunks: List[Dict[str, Any]] = None):
        """Store processing results in long-term memory with chunk support."""
        
        # Store document-level information
        document_memory = {
            "type": "processed_document",
            "document_id": result["document_id"],
            "jurisdiction": result["jurisdiction"],
            "organization": result["organization"],
            "rules_count": len(result.get("extracted_rules", [])),
            "domains": list(result.get("domain_classifications", {}).keys()),
            "processing_metadata": result.get("processing_metadata", {}),
            "timestamp": datetime.now().isoformat()
        }
        
        # Store using LangMem
        try:
            await self.memory_manager.add_memory(
                namespace=("legal_memory", result["jurisdiction"], "documents"),
                key=result["document_id"],
                value=document_memory
            )
        except Exception as e:
            logger.warning(f"Failed to store document memory: {e}")
        
        # Store individual rules with enhanced context
        for i, rule in enumerate(result.get("extracted_rules", [])):
            rule_memory = {
                "type": "legal_rule",
                "content": rule,
                "document_id": result["document_id"],
                "jurisdiction": result["jurisdiction"],
                "rule_index": i,
                "source_chunk": rule.get("source_chunk"),
                "chunk_section": rule.get("chunk_section"),
                "timestamp": datetime.now().isoformat()
            }
            
            rule_id = f"rule_{result['document_id']}_{i}"
            try:
                await self.memory_manager.add_memory(
                    namespace=("legal_memory", result["jurisdiction"], "rules"),
                    key=rule_id,
                    value=rule_memory
                )
            except Exception as e:
                logger.warning(f"Failed to store rule memory {rule_id}: {e}")
        
        logger.info(f"Stored memory for document {result['document_id']} with {len(result.get('extracted_rules', []))} rules")
    
    def query_knowledge_graph(self, query: str, use_embeddings: bool = True, document_id: str = None) -> Dict[str, Any]:
        """
        Query the system using Elasticsearch with optional embedding-based search.
        
        Args:
            query: Natural language query
            use_embeddings: Whether to use embedding-based semantic search
            document_id: Specific document to search (optional)
            
        Returns:
            Query results with relevant chunks and rules
        """
        if use_embeddings:
            # Use Elasticsearch for semantic search
            similar_chunks = self.find_similar_chunks(query, document_id=document_id, top_k=10)
            
            # Get related rules from similar chunks
            related_rules = []
            for chunk in similar_chunks:
                # Search for rules from the same document and chunk
                all_rules = self.elasticsearch.search_rules_by_domain(
                    domain="",  # Search all domains
                    jurisdiction=None
                )
                # Filter rules that come from this chunk
                chunk_id = chunk.get("chunk_id")
                document_chunk_rules = [
                    rule for rule in all_rules 
                    if rule.get("document_id") == chunk.get("document_id") and 
                       rule.get("source_chunk") == chunk_id
                ]
                related_rules.extend(document_chunk_rules)
            
            return {
                "query": query,
                "similar_chunks": similar_chunks,
                "related_rules": related_rules,
                "search_method": "elasticsearch_semantic",
                "elasticsearch_stats": self.elasticsearch.get_document_statistics()
            }
        else:
            # Fallback to keyword search in knowledge graph
            return self._keyword_search(query)
    
    def query_by_jurisdiction(self, jurisdiction: str, query_type: str = "all") -> Dict[str, Any]:
        """
        Query all information related to a specific jurisdiction.
        
        Args:
            jurisdiction: The jurisdiction to query
            query_type: Type of query ("all", "rules", "domains", "documents")
            
        Returns:
            All information related to the jurisdiction
        """
        try:
            # Search documents by jurisdiction
            doc_query = {
                "query": {
                    "term": {"jurisdiction": jurisdiction}
                }
            }
            
            doc_response = self.elasticsearch.client.search(
                index=self.elasticsearch.indices["documents"],
                body=doc_query,
                size=100
            )
            
            documents = [hit["_source"] for hit in doc_response["hits"]["hits"]]
            
            # Search rules by jurisdiction
            rules_query = {
                "query": {
                    "term": {"jurisdiction": jurisdiction}
                }
            }
            
            rules_response = self.elasticsearch.client.search(
                index=self.elasticsearch.indices["rules"],
                body=rules_query,
                size=1000
            )
            
            rules = [hit["_source"] for hit in rules_response["hits"]["hits"]]
            
            # Get domains from rules
            domains = set()
            for rule in rules:
                rule_domains = rule.get("domains", [])
                if isinstance(rule_domains, list):
                    domains.update(rule_domains)
                elif rule_domains:
                    domains.add(rule_domains)
            
            result = {
                "jurisdiction": jurisdiction,
                "query_type": query_type,
                "documents": documents,
                "rules": rules,
                "domains": list(domains),
                "total_documents": len(documents),
                "total_rules": len(rules),
                "total_domains": len(domains),
                "search_method": "jurisdiction_based"
            }
            
            if query_type == "rules":
                return {"jurisdiction": jurisdiction, "rules": rules, "count": len(rules)}
            elif query_type == "domains":
                return {"jurisdiction": jurisdiction, "domains": list(domains), "count": len(domains)}
            elif query_type == "documents":
                return {"jurisdiction": jurisdiction, "documents": documents, "count": len(documents)}
            else:
                return result
                
        except Exception as e:
            logger.error(f"Failed to query by jurisdiction: {e}")
            return {"error": str(e), "jurisdiction": jurisdiction}
    
    def query_by_organization(self, organization: str, query_type: str = "all") -> Dict[str, Any]:
        """
        Query all information related to a specific organization.
        
        Args:
            organization: The organization to query
            query_type: Type of query ("all", "rules", "domains", "documents", "jurisdictions")
            
        Returns:
            All information related to the organization
        """
        try:
            # Search documents by organization
            doc_query = {
                "query": {
                    "term": {"organization": organization}
                }
            }
            
            doc_response = self.elasticsearch.client.search(
                index=self.elasticsearch.indices["documents"],
                body=doc_query,
                size=100
            )
            
            documents = [hit["_source"] for hit in doc_response["hits"]["hits"]]
            
            # Get jurisdictions from documents
            jurisdictions = set()
            for doc in documents:
                if doc.get("jurisdiction"):
                    jurisdictions.add(doc["jurisdiction"])
            
            # Search rules by organization (through documents)
            document_ids = [doc["document_id"] for doc in documents]
            rules = []
            
            if document_ids:
                rules_query = {
                    "query": {
                        "terms": {"document_id": document_ids}
                    }
                }
                
                rules_response = self.elasticsearch.client.search(
                    index=self.elasticsearch.indices["rules"],
                    body=rules_query,
                    size=1000
                )
                
                rules = [hit["_source"] for hit in rules_response["hits"]["hits"]]
            
            # Get domains from rules
            domains = set()
            for rule in rules:
                rule_domains = rule.get("domains", [])
                if isinstance(rule_domains, list):
                    domains.update(rule_domains)
                elif rule_domains:
                    domains.add(rule_domains)
            
            result = {
                "organization": organization,
                "query_type": query_type,
                "documents": documents,
                "rules": rules,
                "domains": list(domains),
                "jurisdictions": list(jurisdictions),
                "total_documents": len(documents),
                "total_rules": len(rules),
                "total_domains": len(domains),
                "total_jurisdictions": len(jurisdictions),
                "search_method": "organization_based"
            }
            
            if query_type == "rules":
                return {"organization": organization, "rules": rules, "count": len(rules)}
            elif query_type == "domains":
                return {"organization": organization, "domains": list(domains), "count": len(domains)}
            elif query_type == "documents":
                return {"organization": organization, "documents": documents, "count": len(documents)}
            elif query_type == "jurisdictions":
                return {"organization": organization, "jurisdictions": list(jurisdictions), "count": len(jurisdictions)}
            else:
                return result
                
        except Exception as e:
            logger.error(f"Failed to query by organization: {e}")
            return {"error": str(e), "organization": organization}
    
    def query_by_country(self, country: str, query_type: str = "all") -> Dict[str, Any]:
        """
        Query all information related to a specific country.
        
        Args:
            country: The country to query
            query_type: Type of query ("all", "rules", "domains", "documents", "jurisdictions", "organizations")
            
        Returns:
            All information related to the country
        """
        try:
            # Search documents by country
            doc_query = {
                "query": {
                    "term": {"country": country}
                }
            }
            
            doc_response = self.elasticsearch.client.search(
                index=self.elasticsearch.indices["documents"],
                body=doc_query,
                size=100
            )
            
            documents = [hit["_source"] for hit in doc_response["hits"]["hits"]]
            
            # Get jurisdictions and organizations from documents
            jurisdictions = set()
            organizations = set()
            for doc in documents:
                if doc.get("jurisdiction"):
                    jurisdictions.add(doc["jurisdiction"])
                if doc.get("organization"):
                    organizations.add(doc["organization"])
            
            # Get rules through documents
            document_ids = [doc["document_id"] for doc in documents]
            rules = []
            
            if document_ids:
                rules_query = {
                    "query": {
                        "terms": {"document_id": document_ids}
                    }
                }
                
                rules_response = self.elasticsearch.client.search(
                    index=self.elasticsearch.indices["rules"],
                    body=rules_query,
                    size=1000
                )
                
                rules = [hit["_source"] for hit in rules_response["hits"]["hits"]]
            
            # Get domains from rules
            domains = set()
            for rule in rules:
                rule_domains = rule.get("domains", [])
                if isinstance(rule_domains, list):
                    domains.update(rule_domains)
                elif rule_domains:
                    domains.add(rule_domains)
            
            result = {
                "country": country,
                "query_type": query_type,
                "documents": documents,
                "rules": rules,
                "domains": list(domains),
                "jurisdictions": list(jurisdictions),
                "organizations": list(organizations),
                "total_documents": len(documents),
                "total_rules": len(rules),
                "total_domains": len(domains),
                "total_jurisdictions": len(jurisdictions),
                "total_organizations": len(organizations),
                "search_method": "country_based"
            }
            
            if query_type == "rules":
                return {"country": country, "rules": rules, "count": len(rules)}
            elif query_type == "domains":
                return {"country": country, "domains": list(domains), "count": len(domains)}
            elif query_type == "documents":
                return {"country": country, "documents": documents, "count": len(documents)}
            elif query_type == "jurisdictions":
                return {"country": country, "jurisdictions": list(jurisdictions), "count": len(jurisdictions)}
            elif query_type == "organizations":
                return {"country": country, "organizations": list(organizations), "count": len(organizations)}
            else:
                return result
                
        except Exception as e:
            logger.error(f"Failed to query by country: {e}")
            return {"error": str(e), "country": country}
    
    def query_by_rule_id(self, rule_id: str) -> Dict[str, Any]:
        """
        Query detailed information about a specific rule.
        
        Args:
            rule_id: The rule ID to query
            
        Returns:
            Detailed information about the rule and its relationships
        """
        try:
            # Search for the specific rule
            rule_query = {
                "query": {
                    "term": {"rule_id": rule_id}
                }
            }
            
            rule_response = self.elasticsearch.client.search(
                index=self.elasticsearch.indices["rules"],
                body=rule_query,
                size=1
            )
            
            if not rule_response["hits"]["hits"]:
                return {"error": "Rule not found", "rule_id": rule_id}
            
            rule = rule_response["hits"]["hits"][0]["_source"]
            
            # Get related document
            document_id = rule.get("document_id")
            document = None
            if document_id:
                doc_response = self.elasticsearch.client.get(
                    index=self.elasticsearch.indices["documents"],
                    id=document_id
                )
                document = doc_response["_source"] if doc_response.get("found") else None
            
            # Get related chunk if available
            chunk = None
            source_chunk = rule.get("source_chunk")
            if document_id and source_chunk is not None:
                chunk_query = {
                    "query": {
                        "bool": {
                            "must": [
                                {"term": {"document_id": document_id}},
                                {"term": {"chunk_id": source_chunk}}
                            ]
                        }
                    }
                }
                
                chunk_response = self.elasticsearch.client.search(
                    index=self.elasticsearch.indices["chunks"],
                    body=chunk_query,
                    size=1
                )
                
                if chunk_response["hits"]["hits"]:
                    chunk = chunk_response["hits"]["hits"][0]["_source"]
            
            # Get related rules in the same domain
            related_rules = []
            rule_domains = rule.get("domains", [])
            if rule_domains:
                if not isinstance(rule_domains, list):
                    rule_domains = [rule_domains]
                
                for domain in rule_domains:
                    domain_rules = self.elasticsearch.search_rules_by_domain(
                        domain=domain,
                        jurisdiction=rule.get("jurisdiction")
                    )
                    # Filter out the current rule
                    domain_rules = [r for r in domain_rules if r.get("rule_id") != rule_id]
                    related_rules.extend(domain_rules[:5])  # Limit to 5 per domain
            
            return {
                "rule_id": rule_id,
                "rule": rule,
                "document": document,
                "chunk": chunk,
                "related_rules": related_rules[:10],  # Limit total related rules
                "search_method": "rule_based"
            }
            
        except Exception as e:
            logger.error(f"Failed to query by rule ID: {e}")
            return {"error": str(e), "rule_id": rule_id}
    
    def query_ontology_sparql(self, sparql_query: str) -> Dict[str, Any]:
        """
        Execute SPARQL queries on the generated ontology.
        
        Args:
            sparql_query: SPARQL query string
            
        Returns:
            Query results
        """
        try:
            if not self.rdf_graph:
                return {"error": "No ontology loaded. Process documents first."}
            
            results = self.rdf_graph.query(sparql_query)
            
            # Convert results to a more usable format
            formatted_results = []
            for row in results:
                result_row = {}
                for var_name in results.vars:
                    value = row[var_name]
                    if value:
                        result_row[str(var_name)] = str(value)
                formatted_results.append(result_row)
            
            return {
                "sparql_query": sparql_query,
                "results": formatted_results,
                "total_results": len(formatted_results),
                "search_method": "sparql"
            }
            
        except Exception as e:
            logger.error(f"Failed to execute SPARQL query: {e}")
            return {"error": str(e), "sparql_query": sparql_query}
    
    def get_ontology_hierarchy(self, start_point: str = "top") -> Dict[str, Any]:
        """
        Get the hierarchical structure of the ontology for navigation.
        
        Args:
            start_point: Starting point ("top", "jurisdiction", "organization", "country", "domain", "rule")
            
        Returns:
            Hierarchical structure of the ontology
        """
        try:
            if not self.rdf_graph:
                return {"error": "No ontology loaded. Process documents first."}
            
            hierarchy = {}
            
            if start_point == "top":
                # Get all top-level concepts
                sparql_query = """
                PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
                PREFIX leg: <http://legal-ontology.org/legal#>
                
                SELECT ?concept ?label ?definition WHERE {
                    ?concept skos:topConceptOf ?scheme .
                    ?concept skos:prefLabel ?label .
                    OPTIONAL { ?concept skos:definition ?definition }
                }
                """
                
                results = self.rdf_graph.query(sparql_query)
                hierarchy["top_concepts"] = [
                    {
                        "uri": str(row.concept),
                        "label": str(row.label),
                        "definition": str(row.definition) if row.definition else ""
                    }
                    for row in results
                ]
            
            elif start_point == "jurisdiction":
                # Get all jurisdictions
                sparql_query = """
                PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
                PREFIX leg: <http://legal-ontology.org/legal#>
                
                SELECT ?jurisdiction ?label ?definition WHERE {
                    ?jurisdiction skos:broader leg:JurisdictionClass .
                    ?jurisdiction skos:prefLabel ?label .
                    OPTIONAL { ?jurisdiction skos:definition ?definition }
                }
                """
                
                results = self.rdf_graph.query(sparql_query)
                hierarchy["jurisdictions"] = [
                    {
                        "uri": str(row.jurisdiction),
                        "label": str(row.label),
                        "definition": str(row.definition) if row.definition else ""
                    }
                    for row in results
                ]
            
            # Add more hierarchy types as needed
            
            return {
                "start_point": start_point,
                "hierarchy": hierarchy,
                "search_method": "ontology_hierarchy"
            }
            
        except Exception as e:
            logger.error(f"Failed to get ontology hierarchy: {e}")
            return {"error": str(e), "start_point": start_point}
    
    def search_by_domain(self, domain: str, jurisdiction: str = None) -> Dict[str, Any]:
        """
        Search rules by domain using Elasticsearch.
        
        Args:
            domain: Data management domain to search
            jurisdiction: Optional jurisdiction filter
            
        Returns:
            Rules matching the domain criteria
        """
        rules = self.elasticsearch.search_rules_by_domain(domain, jurisdiction)
        
        return {
            "domain": domain,
            "jurisdiction": jurisdiction,
            "rules": rules,
            "count": len(rules),
            "search_method": "elasticsearch_domain"
        }
    
    def semantic_search(self, query: str, top_k: int = 5, document_id: str = None) -> Dict[str, Any]:
        """
        Perform semantic search across all documents using Elasticsearch.
        
        Args:
            query: Natural language query
            top_k: Number of top results to return
            document_id: Optional document filter
            
        Returns:
            Semantic search results
        """
        similar_chunks = self.find_similar_chunks(query, document_id=document_id, top_k=top_k)
        
        return {
            "query": query,
            "results": similar_chunks,
            "count": len(similar_chunks),
            "search_method": "elasticsearch_semantic"
        }
    
    def _keyword_search(self, query: str) -> Dict[str, Any]:
        """Fallback keyword-based search."""
        nodes = []
        edges = []
        
        # Simple keyword matching
        query_terms = query.lower().split()
        
        for node_id, node_data in self.knowledge_graph.nodes(data=True):
            node_text = str(node_data).lower()
            if any(term in node_text for term in query_terms):
                nodes.append({"id": node_id, "data": node_data})
        
        # Get edges for relevant nodes
        for node in nodes:
            for edge in self.knowledge_graph.edges(node["id"], data=True):
                edges.append({
                    "source": edge[0],
                    "target": edge[1],
                    "data": edge[2]
                })
        
        return {
            "query": query,
            "nodes": nodes,
            "edges": edges,
            "search_method": "keyword_based",
            "graph_stats": {
                "total_nodes": len(self.knowledge_graph.nodes),
                "total_edges": len(self.knowledge_graph.edges)
            }
        }
    
    def export_ontology(self, output_dir: str = None):
        """Export the complete ontology in multiple formats using global configuration."""
        
        output_path = Path(output_dir or OUTPUT_DIRECTORY)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Export SKOS ontology
        ontology_outputs = self._generate_ontology_outputs({"ontology_concepts": []})
        
        for format_name, content in ontology_outputs.items():
            file_extension = {
                "json_ld": "jsonld",
                "turtle": "ttl", 
                "xml": "xml"
            }[format_name]
            
            with open(output_path / f"legal_ontology.{file_extension}", 'w') as f:
                f.write(content)
        
        # Export knowledge graph
        try:
            nx.write_gexf(self.knowledge_graph, output_path / "knowledge_graph.gexf")
        except Exception as e:
            logger.warning(f"Failed to export knowledge graph: {e}")
        
        # Export Elasticsearch statistics
        try:
            stats = self.elasticsearch.get_document_statistics()
            with open(output_path / "elasticsearch_stats.json", 'w') as f:
                json.dump(stats, f, indent=2)
        except Exception as e:
            logger.warning(f"Failed to export Elasticsearch stats: {e}")
        
        logger.info(f"Ontology exported to {output_path}")

# Example usage and configuration with global settings
if __name__ == "__main__":
    # Configuration using global paths
    config = {
        "documents": [
            {
                "id": "gdpr_2016_679",
                "title": "General Data Protection Regulation",
                "path": f"{DOCUMENTS_DIRECTORY}/gdpr_regulation_2016_679.pdf",
                "jurisdiction": "EU",
                "organization": "European_Union",
                "country": "European Union"
            },
            {
                "id": "ccpa_2018",
                "title": "California Consumer Privacy Act",
                "path": f"{DOCUMENTS_DIRECTORY}/ccpa_2018.pdf",
                "jurisdiction": "California",
                "organization": "California_State",
                "country": "United States"
            },
            {
                "id": "uk_dpa_2018",
                "title": "Data Protection Act 2018",
                "path": f"{DOCUMENTS_DIRECTORY}/uk_dpa_2018.pdf",
                "jurisdiction": "UK",
                "organization": "UK_Government",
                "country": "United Kingdom"
            },
            {
                "id": "singapore_pdpa_2012",
                "title": "Personal Data Protection Act",
                "path": f"{DOCUMENTS_DIRECTORY}/singapore_pdpa_2012.pdf",
                "jurisdiction": "Singapore",
                "organization": "Singapore_Government",
                "country": "Singapore"
            },
            {
                "id": "basel_iii_framework",
                "title": "Basel III International Regulatory Framework",
                "path": f"{DOCUMENTS_DIRECTORY}/basel_iii_framework.pdf",
                "jurisdiction": "International",
                "organization": "Basel_Committee",
                "country": "International"
            }
        ]
    }
    
    # Save configuration using global path
    with open(CONFIG_FILE_PATH, 'w') as f:
        json.dump(config, f, indent=2)
    
    # Initialize and run processor
    async def main():
        # Check required directories and files
        required_paths = [
            (Path(TIKTOKEN_MODELS_PATH), "tiktoken models"),
            (Path(SPACY_MODEL_PATH), "spaCy model"),
            (Path(DOCUMENTS_DIRECTORY), "documents directory"),
            (Path(OUTPUT_DIRECTORY), "output directory")
        ]
        
        for path, description in required_paths:
            if not path.exists():
                if "directory" in description:
                    path.mkdir(parents=True, exist_ok=True)
                    logger.info(f"Created {description}: {path}")
                else:
                    logger.warning(f"{description} not found at: {path}")
        
        # Validate configuration
        print("🔧 Checking configuration...")
        config_issues = validate_configuration()
        
        if config_issues:
            print("\n⚠️  Configuration Issues Found:")
            for issue in config_issues:
                print(f"  {issue}")
            
            print("\n💡 To fix configuration issues:")
            print("1. Edit the global variables at the top of legal_document_processor.py:")
            print('   OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-your-actual-key-here")')
            print("2. Place tiktoken models in ./tiktoken_models/ directory")
            print("3. Install spaCy model: python -m spacy download en_core_web_sm")
            
            # Only stop for critical API key issues
            critical_issues = [i for i in config_issues if "OPENAI_API_KEY not set" in i]
            if critical_issues:
                print("\n❌ Cannot proceed without OpenAI API key.")
                return
            else:
                print("\n⚠️  Some warnings found but continuing with available resources...")
        else:
            print("✅ Configuration validated successfully")
        
        # Initialize processor with global configuration
        processor = LegalDocumentProcessor()
        
        # Display configuration summary
        print("\n" + "="*60)
        print("LEGAL DOCUMENT PROCESSING SYSTEM")
        print("="*60)
        print(f"OpenAI Model: {OPENAI_MODEL} (Direct API)")
        print(f"OpenAI Base URL: {OPENAI_BASE_URL}")
        print(f"Embedding Model: {OPENAI_EMBEDDING_MODEL}")
        print(f"Reasoning Effort: {REASONING_EFFORT}")
        print(f"Processing Method: Direct o3-mini API (No LangChain)")
        print(f"Elasticsearch: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
        print(f"Tiktoken Models: {TIKTOKEN_MODELS_PATH} (Offline)")
        print(f"spaCy Model: Installed package")
        print(f"Max Tokens per Chunk: {MAX_TOKENS_PER_CHUNK}")
        print(f"Output Directory: {OUTPUT_DIRECTORY}")
        print("="*60)
        
        # Process all documents from config
        results = processor.process_all_documents()
        
        # Print summary
        successful_docs = [r for r in results if r.get("success", True)]
        failed_docs = [r for r in results if not r.get("success", True)]
        
        print(f"\n📊 PROCESSING SUMMARY:")
        print(f"✅ Successfully processed: {len(successful_docs)}/{len(results)} documents")
        
        if failed_docs:
            print(f"❌ Failed documents:")
            for doc in failed_docs:
                print(f"   - {doc['document_id']}: {doc.get('error', 'Unknown error')}")
        
        for result in successful_docs:
            print(f"📄 {result['document_id']}: {len(result.get('rules', []))} rules extracted")
        
        # Export ontology
        processor.export_ontology()
        
        # Example queries using Elasticsearch and new query methods
        if successful_docs:
            print(f"\n🔍 EXAMPLE QUERIES - Starting from Any Point:")
            
            # Semantic search example
            semantic_result = processor.semantic_search("data storage requirements", top_k=3)
            print(f"Semantic search for 'data storage requirements': {semantic_result['count']} results")
            
            # Domain search example
            domain_result = processor.search_by_domain("Data Storage and Retention")
            print(f"Domain search for 'Data Storage and Retention': {domain_result['count']} rules")
            
            # Jurisdiction-based queries
            first_doc = successful_docs[0]
            jurisdiction = first_doc.get("jurisdiction", "EU")
            
            jurisdiction_result = processor.query_by_jurisdiction(jurisdiction, "all")
            print(f"Jurisdiction query for '{jurisdiction}': {jurisdiction_result.get('total_rules', 0)} rules, {jurisdiction_result.get('total_documents', 0)} documents")
            
            # Organization-based queries
            organization = first_doc.get("organization", "European_Union")
            org_result = processor.query_by_organization(organization, "all")
            print(f"Organization query for '{organization}': {org_result.get('total_rules', 0)} rules across {org_result.get('total_jurisdictions', 0)} jurisdictions")
            
            # Country-based queries
            country = first_doc.get("country", "European Union")
            country_result = processor.query_by_country(country, "all")
            print(f"Country query for '{country}': {country_result.get('total_rules', 0)} rules from {country_result.get('total_organizations', 0)} organizations")
            
            # Rule-specific query
            if jurisdiction_result.get("rules"):
                sample_rule = jurisdiction_result["rules"][0]
                rule_id = sample_rule.get("rule_id")
                if rule_id:
                    rule_result = processor.query_by_rule_id(rule_id)
                    print(f"Rule query for '{rule_id}': Found rule with {len(rule_result.get('related_rules', []))} related rules")
            
            # Ontology hierarchy
            hierarchy_result = processor.get_ontology_hierarchy("top")
            if "hierarchy" in hierarchy_result:
                top_concepts = len(hierarchy_result["hierarchy"].get("top_concepts", []))
                print(f"Ontology hierarchy: {top_concepts} top-level concepts available for navigation")
            
            # SPARQL query example
            sparql_query = """
            PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
            PREFIX leg: <http://legal-ontology.org/legal#>
            
            SELECT ?jurisdiction ?ruleCount WHERE {
                ?jurisdiction skos:broader leg:JurisdictionClass .
                ?jurisdiction leg:domainCount ?ruleCount .
            }
            LIMIT 5
            """
            
            sparql_result = processor.query_ontology_sparql(sparql_query)
            if "results" in sparql_result:
                print(f"SPARQL query: Found {sparql_result.get('total_results', 0)} jurisdiction-rule mappings")
            
            # Knowledge graph query example
            kg_result = processor.query_knowledge_graph("privacy consent requirements")
            print(f"Knowledge graph query: {len(kg_result.get('similar_chunks', []))} relevant chunks")
            
            print(f"\n📋 QUERY ENTRY POINTS AVAILABLE:")
            print(f"   🏛️  Jurisdiction: query_by_jurisdiction(jurisdiction)")
            print(f"   🏢 Organization: query_by_organization(organization)")
            print(f"   🌍 Country: query_by_country(country)")
            print(f"   📊 Domain: search_by_domain(domain)")
            print(f"   📄 Rule: query_by_rule_id(rule_id)")
            print(f"   🔍 Semantic: semantic_search(query)")
            print(f"   🕸️  SPARQL: query_ontology_sparql(sparql)")
            print(f"   🌳 Hierarchy: get_ontology_hierarchy(start_point)")
        
        # Show Elasticsearch statistics
        try:
            stats = processor.elasticsearch.get_document_statistics()
            print(f"\n📈 ELASTICSEARCH STATISTICS:")
            for key, value in stats.items():
                if isinstance(value, dict):
                    print(f"   {key}:")
                    for k, v in value.items():
                        print(f"     - {k}: {v}")
                else:
                    print(f"   {key}: {value}")
        except Exception as e:
            logger.warning(f"Could not retrieve Elasticsearch statistics: {e}")
        
        print(f"\n✅ Processing complete! Results saved to: {OUTPUT_DIRECTORY}")
    
    # Run the system
    asyncio.run(main())
