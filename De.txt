import os
import pandas as pd
import numpy as np
import chromadb
from chromadb.utils import embedding_functions
from typing import List, Dict, Tuple
import logging
from dotenv import load_dotenv
from pathlib import Path
from azure.identity import ClientSecretCredential
from tqdm import tqdm
import json

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OSEnv:
    """Environment and certificate management class."""
    
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self._load_env_files(config_file, creds_file)
        self._setup_certificates(certificate_path)
        self._setup_azure_auth()

    def _load_env_files(self, config_file: str, creds_file: str) -> None:
        """Load environment variables from config files."""
        for file in [config_file, creds_file]:
            if not os.path.isfile(file):
                raise FileNotFoundError(f"Environment file not found: {file}")
            load_dotenv(file)
            logger.info(f"Loaded environment from {file}")

    def _setup_certificates(self, cert_path: str) -> None:
        """Configure SSL certificates."""
        if not os.path.isfile(cert_path):
            raise FileNotFoundError(f"Certificate file not found: {cert_path}")
        os.environ['SSL_CERT_FILE'] = cert_path
        os.environ['REQUESTS_CA_BUNDLE'] = cert_path
        logger.info(f"Configured certificates from {cert_path}")

    def _setup_azure_auth(self) -> None:
        """Set up Azure authentication."""
        try:
            credential = ClientSecretCredential(
                tenant_id=os.getenv("AZURE_TENANT_ID"),
                client_id=os.getenv("AZURE_CLIENT_ID"),
                client_secret=os.getenv("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            os.environ["AZURE_TOKEN"] = token.token
            logger.info("Azure authentication configured")
        except Exception as e:
            logger.error(f"Azure authentication failed: {str(e)}")
            raise

class SemanticMatcher:
    """Handles semantic matching between CSV entries using embeddings."""
    
    def __init__(self, env_setup: OSEnv):
        """Initialize with environment setup."""
        self.env = env_setup
        self._setup_embedding_function()
        self._setup_chroma()
        
    def _setup_embedding_function(self) -> None:
        """Configure Azure OpenAI embedding function."""
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key=os.getenv("AZURE_TOKEN"),
            api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_type="azure",
            api_version=os.getenv("API_VERSION", "2024-02-01"),
            model_name="text-embedding-ada-002"
        )
        logger.info("Embedding function configured")

    def _setup_chroma(self) -> None:
        """Set up ChromaDB client."""
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name="semantic_matches",
            embedding_function=self.embedding_function
        )
        logger.info("ChromaDB initialized")

    def _prepare_text(self, name: str, description: str) -> str:
        """Combine name and description for embedding."""
        return f"{name}: {description}".strip()

    def load_csv_data(self, source_csv: str, target_csv: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load and validate CSV files."""
        try:
            source_df = pd.read_csv(source_csv)
            target_df = pd.read_csv(target_csv)
            
            # Validate columns
            if not {'name', 'description'}.issubset(source_df.columns):
                raise ValueError("Source CSV must have 'name' and 'description' columns")
            if not {'pbt-name', 'pbt-description'}.issubset(target_df.columns):
                raise ValueError("Target CSV must have 'pbt-name' and 'pbt-description' columns")
            
            # Clean data
            for df in [source_df, target_df]:
                for col in df.columns:
                    df[col] = df[col].fillna('').astype(str).str.strip()
            
            logger.info(f"Loaded {len(source_df)} source entries and {len(target_df)} target entries")
            return source_df, target_df
            
        except Exception as e:
            logger.error(f"Failed to load CSV data: {str(e)}")
            raise

    def process_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame, 
                       similarity_threshold: float = 0.75) -> List[Dict]:
        """Process and find matches between dataframes."""
        try:
            matches = []
            
            # Add target entries to ChromaDB
            logger.info("Adding target entries to ChromaDB...")
            target_texts = [
                self._prepare_text(row['pbt-name'], row['pbt-description'])
                for _, row in target_df.iterrows()
            ]
            self.collection.add(
                documents=target_texts,
                ids=[f"target_{i}" for i in range(len(target_df))]
            )
            
            # Process each source entry
            logger.info("Finding matches for source entries...")
            for idx, source_row in tqdm(source_df.iterrows(), total=len(source_df)):
                source_text = self._prepare_text(
                    source_row['name'], 
                    source_row['description']
                )
                
                # Query ChromaDB for matches
                results = self.collection.query(
                    query_texts=[source_text],
                    n_results=3
                )
                
                # Process results
                for i, (target_idx, distance) in enumerate(zip(
                    results['ids'][0],
                    results['distances'][0]
                )):
                    similarity = 1 - distance  # Convert distance to similarity
                    if similarity >= similarity_threshold:
                        target_idx = int(target_idx.split('_')[1])
                        matches.append({
                            'source_name': source_row['name'],
                            'source_description': source_row['description'],
                            'target_name': target_df.iloc[target_idx]['pbt-name'],
                            'target_description': target_df.iloc[target_idx]['pbt-description'],
                            'similarity_score': round(similarity, 4)
                        })
            
            # Sort matches by similarity score
            matches.sort(key=lambda x: x['similarity_score'], reverse=True)
            logger.info(f"Found {len(matches)} matches above threshold")
            return matches
            
        except Exception as e:
            logger.error(f"Failed to process matches: {str(e)}")
            raise

    def save_results(self, matches: List[Dict], output_file: str) -> None:
        """Save matches to output file."""
        try:
            # Convert to DataFrame for easier viewing
            results_df = pd.DataFrame(matches)
            
            # Save as CSV
            results_df.to_csv(output_file, index=False)
            
            # Also save detailed JSON
            json_file = output_file.rsplit('.', 1)[0] + '.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(matches, f, indent=2)
            
            logger.info(f"Results saved to {output_file} and {json_file}")
            
        except Exception as e:
            logger.error(f"Failed to save results: {str(e)}")
            raise

def main():
    """Main function to run the semantic matching process."""
    try:
        # Setup paths
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / 'env'
        data_dir = base_dir / 'data'
        output_dir = base_dir / 'output'
        
        # Create directories if they don't exist
        for directory in [data_dir, output_dir]:
            directory.mkdir(exist_ok=True)
        
        # Initialize environment
        env_setup = OSEnv(
            config_file=str(env_dir / 'config.env'),
            creds_file=str(env_dir / 'credentials.env'),
            certificate_path=str(env_dir / 'cacert.pem')
        )
        
        # Initialize matcher
        matcher = SemanticMatcher(env_setup)
        
        # Process files
        source_csv = str(data_dir / 'source.csv')
        target_csv = str(data_dir / 'target.csv')
        output_file = str(output_dir / 'matches.csv')
        
        # Load data
        print("\nLoading CSV files...")
        source_df, target_df = matcher.load_csv_data(source_csv, target_csv)
        
        # Process matches
        print("\nProcessing semantic matches...")
        matches = matcher.process_matches(source_df, target_df)
        
        # Save results
        print("\nSaving results...")
        matcher.save_results(matches, output_file)
        
        print(f"\nProcess completed successfully!")
        print(f"Results saved to: {output_file}")
        print(f"Total matches found: {len(matches)}")
        
    except FileNotFoundError as e:
        print(f"\nFile Error: {str(e)}")
        print("Please check your file paths and try again.")
    except ValueError as e:
        print(f"\nValidation Error: {str(e)}")
        print("Please check your CSV files and try again.")
    except Exception as e:
        print(f"\nUnexpected Error: {str(e)}")
        print("Please check the logs for more details.")
        logger.exception("Unexpected error occurred")

if __name__ == "__main__":
    main()
