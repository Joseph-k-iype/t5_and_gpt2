#!/usr/bin/env python3
"""
Enhanced Multi-Agent Legislation Rule Extraction System
Converts legislation into machine-readable rules and conditions using LangGraph
Enhanced with Chain of Thought, Mixture of Experts, Rule Deduplication, and Machine-Readable Format
Preserves all original features while adding JSON rules engine compatibility
"""

import os
import sys
import json
import csv
import asyncio
import logging
import hashlib
import time
import uuid
from typing import List, Dict, Any, Optional, Tuple, Annotated, Union, Literal
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

# Core dependencies
import pymupdf
import openai
import numpy as np
from pydantic import BaseModel, Field, ConfigDict

# LangChain and LangGraph dependencies  
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_core.tools import BaseTool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Global Configuration
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = "o3-mini-2025-01-31"
EMBEDDING_MODEL = "text-embedding-3-large"

# Paths
INPUT_PDF_PATH = os.getenv("INPUT_PDF_PATH", "./input_pdfs/")
LEGISLATION_METADATA_PATH = os.getenv("LEGISLATION_METADATA_PATH", "./legislation_metadata.json")
GEOGRAPHY_PATH = os.getenv("GEOGRAPHY_PATH", "./geography.json")
OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output/")

# Ensure output directory exists
Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('legislation_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Validate OpenAI client initialization
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set, OpenAI client initialization will fail at runtime")
    openai_client = None
else:
    try:
        openai_client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}")
        openai_client = None

def _convert_messages_for_openai(messages: List[BaseMessage]) -> List[Dict[str, str]]:
    """Convert LangChain messages to OpenAI format"""
    openai_messages = []
    for msg in messages:
        if msg.type == "human":
            role = "user"
        elif msg.type == "ai":
            role = "assistant"
        elif msg.type == "system":
            role = "system"
        else:
            logger.warning(f"Unknown message type: {msg.type}, defaulting to 'user'")
            role = "user"  # Default fallback
        
        openai_messages.append({
            "role": role,
            "content": msg.content
        })
    
    logger.debug(f"Converted {len(messages)} messages for OpenAI API")
    return openai_messages

class RoleType(Enum):
    CONTROLLER = "Controller"
    PROCESSOR = "Processor" 
    JOINT_CONTROLLER = "Joint Controller"
    DATA_SUBJECT = "Data Subject"

class OperatorType(Enum):
    EQUAL = "equal"
    NOT_EQUAL = "notEqual"
    GREATER_THAN = "greaterThan"
    LESS_THAN = "lessThan"
    GREATER_THAN_INCLUSIVE = "greaterThanInclusive"
    LESS_THAN_INCLUSIVE = "lessThanInclusive"
    CONTAINS = "contains"
    NOT_CONTAINS = "notContains"
    IN = "in"
    NOT_IN = "notIn"
    EXISTS = "exists"
    NOT_EXISTS = "notExists"
    AND = "and"
    OR = "or"
    NOT = "not"

class ActionType(Enum):
    REQUIRE = "require"
    FORBID = "forbid"
    PERMIT = "permit"
    NOTIFY = "notify"
    LOG = "log"
    VALIDATE = "validate"
    TRANSFORM = "transform"
    ESCALATE = "escalate"
    OBTAIN_CONSENT = "obtain_consent"
    PROVIDE_NOTICE = "provide_notice"
    IMPLEMENT_SAFEGUARDS = "implement_safeguards"
    CONDUCT_ASSESSMENT = "conduct_assessment"

class DataCategoryType(Enum):
    PERSONAL_DATA = "Personal Data"
    SPECIAL_CATEGORY_DATA = "Special Category Data"
    BIOMETRIC_DATA = "Biometric Data"
    HEALTH_DATA = "Health Data"
    GENETIC_DATA = "Genetic Data"
    FINANCIAL_DATA = "Financial Data"
    LOCATION_DATA = "Location Data"
    COMMUNICATION_DATA = "Communication Data"
    BEHAVIORAL_DATA = "Behavioral Data"
    IDENTIFICATION_DATA = "Identification Data"
    CRIMINAL_DATA = "Criminal Data"
    PROFESSIONAL_DATA = "Professional Data"

# Original RuleCondition preserved from original code
class RuleCondition(BaseModel):
    """Individual rule condition with logical operators"""
    model_config = ConfigDict(use_enum_values=True)
    
    condition_text: str = Field(description="Clear, atomic condition statement")
    logical_operator: Optional[str] = Field(description="AND, OR, NOT operator", default=None)
    roles: List[RoleType] = Field(description="Applicable roles for this condition", default_factory=list)
    is_negation: bool = Field(description="Whether this is a negation (must not)", default=False)

# Enhanced machine-readable condition for JSON rules engine
class MachineReadableCondition(BaseModel):
    """Machine-readable condition for JSON rules engine"""
    model_config = ConfigDict(use_enum_values=True)
    
    condition_id: str = Field(description="Unique identifier for this condition")
    fact: str = Field(description="The fact/variable to evaluate")
    operator: OperatorType = Field(description="The comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(description="The value to compare against")
    path: Optional[str] = Field(default=None, description="JSONPath for nested object access")
    logical_operator: Optional[OperatorType] = Field(default=None, description="Logical operator for combining conditions")
    roles: List[RoleType] = Field(default_factory=list, description="Applicable roles for this condition")
    is_negation: bool = Field(default=False, description="Whether this is a negation condition")
    variables: Dict[str, Any] = Field(default_factory=dict, description="Variables used in condition evaluation")
    nested_conditions: List['MachineReadableCondition'] = Field(default_factory=list, description="Nested sub-conditions")
    # Link to original condition
    original_condition_text: Optional[str] = Field(default=None, description="Original human-readable condition text")

class MachineReadableAction(BaseModel):
    """Machine-readable action for JSON rules engine"""
    model_config = ConfigDict(use_enum_values=True)
    
    action_id: str = Field(description="Unique identifier for this action")
    type: ActionType = Field(description="Type of action to perform")
    params: Dict[str, Any] = Field(default_factory=dict, description="Parameters for the action")
    target_roles: List[RoleType] = Field(default_factory=list, description="Roles this action targets")
    conditions: List[str] = Field(default_factory=list, description="Condition IDs that trigger this action")
    message: Optional[str] = Field(default=None, description="Human-readable message for the action")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional action metadata")
    variables: Dict[str, Any] = Field(default_factory=dict, description="Variables used in action execution")
    if_else_logic: Dict[str, Any] = Field(default_factory=dict, description="If-else logic for conditional actions")

# Enhanced LegislationRule that preserves all original fields
class LegislationRule(BaseModel):
    """Complete legislation rule with conditions and metadata - preserving original structure"""
    model_config = ConfigDict(use_enum_values=True)
    
    rule_id: str = Field(description="Unique identifier for the rule")
    rule_text: str = Field(description="Main rule statement")
    rule_definition: str = Field(description="Detailed rule definition", default="")
    applies_to_countries: List[str] = Field(description="List of country/region codes")
    roles: List[RoleType] = Field(description="Primary roles this rule applies to", default_factory=list)
    data_categories: List[str] = Field(description="Data categories this rule relates to", default_factory=list)
    
    # Original conditions preserved
    conditions: List[RuleCondition] = Field(description="List of conditions for this rule")
    condition_count: int = Field(description="Number of conditions")
    
    # Enhanced machine-readable components
    machine_readable_conditions: List[MachineReadableCondition] = Field(
        description="Machine-readable conditions for JSON rules engine", default_factory=list
    )
    machine_readable_actions: List[MachineReadableAction] = Field(
        description="Machine-readable actions for JSON rules engine", default_factory=list
    )
    
    # JSON Rules Engine format
    json_rules_engine_format: Dict[str, Any] = Field(
        default_factory=dict, 
        description="Complete JSON rules engine format"
    )
    
    references: List[str] = Field(description="Legal references and citations")
    adequacy_countries: List[str] = Field(description="Countries with adequacy decisions mentioned", default_factory=list)
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    confidence_score: float = Field(default=0.0, description="Confidence in rule extraction")
    duplicate_of: Optional[str] = Field(default=None, description="ID of original rule if this is a duplicate")
    
    # Enhanced fields
    priority: int = Field(default=50, description="Rule priority for execution order")
    nested_rules: List['LegislationRule'] = Field(default_factory=list, description="Nested sub-rules")
    parent_rule_id: Optional[str] = Field(default=None, description="Parent rule ID if this is a nested rule")
    variables: Dict[str, Any] = Field(default_factory=dict, description="Variables used across the rule")
    if_else_logic: Dict[str, Any] = Field(default_factory=dict, description="If-else logic for complex rule scenarios")

class AgentState(BaseModel):
    """State object for the multi-agent workflow - preserving all original fields"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    messages: List[BaseMessage] = Field(default_factory=list)
    documents: List[Document] = Field(default_factory=list)
    processed_text: str = Field(default="")
    legislation_content: str = Field(default="")
    supporting_content: str = Field(default="")
    segmented_content: List[Dict[str, Any]] = Field(default_factory=list)
    extracted_entities: List[Dict[str, Any]] = Field(default_factory=list)
    rules: List[LegislationRule] = Field(default_factory=list)
    deduplicated_rules: List[LegislationRule] = Field(default_factory=list)
    current_jurisdiction: str = Field(default="")
    geography_data: Dict[str, Any] = Field(default_factory=dict)
    adequacy_countries: List[str] = Field(default_factory=list)
    # Remove vector_store from state to avoid serialization issues
    vector_documents: List[Document] = Field(default_factory=list)
    next_agent: str = Field(default="document_processor")
    error_messages: List[str] = Field(default_factory=list)
    react_reasoning: List[Dict[str, str]] = Field(default_factory=list)

class CustomEmbeddings(Embeddings):
    """Custom embeddings using OpenAI API directly"""
    
    def __init__(self):
        if not openai_client:
            raise ValueError("OpenAI client not initialized. Please check OPENAI_API_KEY.")
        self.client = openai_client
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text[:8000]  # Limit input size
            )
            embeddings.append(response.data[0].embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        response = self.client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text[:8000]  # Limit input size
        )
        return response.data[0].embedding

class GeographyManager:
    """Manages geography data and country/region mappings"""
    
    def __init__(self, geography_data: Dict[str, Any]):
        self.geography_data = geography_data
        self.country_lookup = self._build_country_lookup()
        self.region_lookup = self._build_region_lookup()
    
    def _build_country_lookup(self) -> Dict[str, Dict[str, Any]]:
        """Build lookup table for countries"""
        lookup = {}
        
        # Add countries from regional groupings
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    for country in continent_data.get("countries", []):
                        lookup[country["iso2"]] = {
                            "name": country["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": country["iso2"]
                        }
                    for territory in continent_data.get("territories", []):
                        lookup[territory["iso2"]] = {
                            "name": territory["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": territory["iso2"],
                            "dependency_of": territory["dependency_of"]
                        }
            else:
                # Regional groupings like EU, EEA, MENAT
                for country in region_data.get("countries", []):
                    lookup[country["iso2"]] = {
                        "name": country["name"],
                        "region": region_key,
                        "iso2": country["iso2"]
                    }
                for territory in region_data.get("territories", []):
                    lookup[territory["iso2"]] = {
                        "name": territory["name"],
                        "region": region_key,
                        "iso2": territory["iso2"],
                        "dependency_of": territory["dependency_of"]
                    }
        
        return lookup
    
    def _build_region_lookup(self) -> Dict[str, List[str]]:
        """Build lookup table for regions to countries"""
        lookup = {}
        
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    country_codes = [c["iso2"] for c in continent_data.get("countries", [])]
                    territory_codes = [t["iso2"] for t in continent_data.get("territories", [])]
                    lookup[f"By_Continent.{continent}"] = country_codes + territory_codes
            else:
                country_codes = [c["iso2"] for c in region_data.get("countries", [])]
                territory_codes = [t["iso2"] for t in region_data.get("territories", [])]
                lookup[region_key] = country_codes + territory_codes
        
        return lookup
    
    def get_country_info(self, iso_code: str) -> Optional[Dict[str, Any]]:
        """Get country information by ISO code"""
        return self.country_lookup.get(iso_code)
    
    def get_region_countries(self, region: str) -> List[str]:
        """Get all countries in a region"""
        return self.region_lookup.get(region, [])
    
    def find_countries_by_name(self, name_pattern: str) -> List[str]:
        """Find countries by name pattern"""
        matches = []
        name_lower = name_pattern.lower()
        for iso_code, info in self.country_lookup.items():
            if name_lower in info["name"].lower():
                matches.append(iso_code)
        return matches

class LegislationProcessor:
    """Main processor for legislation documents"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        self.embeddings = CustomEmbeddings()
        self.geography_manager = None
        
    def load_geography_data(self) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(GEOGRAPHY_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.geography_manager = GeographyManager(data)
                return data
        except FileNotFoundError:
            logger.error(f"Geography file not found: {GEOGRAPHY_PATH}")
            return {}
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing geography JSON: {e}")
            return {}
    
    def load_legislation_metadata(self) -> List[Dict[str, Any]]:
        """Load legislation metadata from JSON file"""
        try:
            with open(LEGISLATION_METADATA_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Ensure we return a list of dictionaries
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    # If it's a single dict, wrap it in a list
                    return [data]
                else:
                    logger.error(f"Unexpected data format in legislation metadata: {type(data)}")
                    return []
        except FileNotFoundError:
            logger.error(f"Legislation metadata file not found: {LEGISLATION_METADATA_PATH}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing legislation metadata JSON: {e}")
            return []
    
    def extract_pdf_content(self, pdf_path: str) -> Tuple[str, List[Document], str, str]:
        """Extract content from PDF using PyMuPDF and separate legislation from supporting info"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            documents = []
            
            supporting_info_start = False
            legislation_text = ""
            supporting_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += page_text + "\n"
                
                # Check for "Supporting Information" section
                if "supporting information" in page_text.lower():
                    supporting_info_start = True
                
                if not supporting_info_start:
                    legislation_text += page_text + "\n"
                else:
                    supporting_text += page_text + "\n"
                
                # Create document for each page
                documents.append(Document(
                    page_content=page_text,
                    metadata={
                        "page_number": page_num + 1,
                        "source": pdf_path,
                        "is_supporting": supporting_info_start,
                        "content_type": "supporting" if supporting_info_start else "legislation"
                    }
                ))
            
            doc.close()
            logger.info(f"Extracted {len(documents)} pages from {pdf_path}")
            logger.info(f"Legislation text: {len(legislation_text)} chars, Supporting text: {len(supporting_text)} chars")
            return full_text, documents, legislation_text, supporting_text
            
        except Exception as e:
            logger.error(f"Error extracting PDF content from {pdf_path}: {e}")
            return "", [], "", ""
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """Split documents into smaller chunks"""
        return self.text_splitter.split_documents(documents)
    
    def create_vector_store(self, documents: List[Document]) -> InMemoryVectorStore:
        """Create vector store for semantic search"""
        vector_store = InMemoryVectorStore(self.embeddings)
        vector_store.add_documents(documents)
        return vector_store

class ReactDocumentProcessorAgent:
    """Enhanced React Agent for document processing with reasoning - preserving all original functionality"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Process documents using React reasoning pattern"""
        logger.info("ReactDocumentProcessorAgent: Starting document processing")
        print("\n📂 ReactDocumentProcessorAgent: Starting document processing...")
        
        # THOUGHT: Plan the document processing approach
        thought = """
        THOUGHT: I need to systematically process PDF documents to extract legislation rules.
        My approach will be:
        1. Load geography and metadata configurations
        2. Process each PDF file and separate legislation from supporting information
        3. Create document chunks for better processing
        4. Analyze content using multiple expert perspectives
        5. Extract adequacy countries from both legislation and supporting sections
        """
        state.react_reasoning.append({"step": "document_processing_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Load configurations
        print("\n🎬 ACTION: Loading configurations...")
        state.geography_data = self.processor.load_geography_data()
        legislation_metadata = self.processor.load_legislation_metadata()
        
        # OBSERVATION: Assess what was loaded
        observation = f"""
        OBSERVATION: Successfully loaded configurations:
        - Geography data: {len(state.geography_data)} regions
        - Legislation metadata: {len(legislation_metadata)} files
        Available regions: {list(state.geography_data.keys())}
        """
        state.react_reasoning.append({"step": "document_processing_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        all_documents = []
        all_legislation_text = ""
        all_supporting_text = ""
        
        # ACTION: Process each PDF file
        print("\n🎬 ACTION: Processing PDF files...")
        for i, item in enumerate(legislation_metadata):
            pdf_path = item.get("path", "")
            jurisdiction = item.get("jurisdiction", "")
            
            print(f"\n--- Processing item {i+1}/{len(legislation_metadata)} ---")
            print(f"📁 PDF Path: {pdf_path}")
            print(f"🏛️  Jurisdiction: {jurisdiction}")
            
            if not pdf_path or not os.path.exists(pdf_path):
                print("❌ File not found, skipping")
                continue
            
            # Extract content with separation
            full_text, documents, legislation_text, supporting_text = self.processor.extract_pdf_content(pdf_path)
            
            # Add jurisdiction metadata
            for doc in documents:
                doc.metadata["jurisdiction"] = jurisdiction
            
            all_documents.extend(documents)
            all_legislation_text += legislation_text + "\n"
            all_supporting_text += supporting_text + "\n"
            state.current_jurisdiction = jurisdiction
            
            print(f"📄 Extracted {len(documents)} pages")
            print(f"📝 Legislation: {len(legislation_text)} chars, Supporting: {len(supporting_text)} chars")
        
        # OBSERVATION: Document extraction results
        observation = f"""
        OBSERVATION: Document extraction completed:
        - Total documents: {len(all_documents)}
        - Legislation content: {len(all_legislation_text)} characters
        - Supporting content: {len(all_supporting_text)} characters
        - Current jurisdiction: {state.current_jurisdiction}
        """
        state.react_reasoning.append({"step": "extraction_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        # Store separated content
        state.legislation_content = all_legislation_text
        state.supporting_content = all_supporting_text
        
        # ACTION: Create document chunks
        print("\n🎬 ACTION: Creating document chunks...")
        chunked_docs = self.processor.chunk_documents(all_documents)
        state.documents = chunked_docs
        state.vector_documents = chunked_docs  # Store for later vector operations
        
        # THOUGHT: Plan expert analysis
        thought = """
        THOUGHT: Now I need to analyze the content using multiple expert perspectives.
        I'll focus on:
        1. Legal structure and obligations
        2. Geographic scope and adequacy decisions
        3. Data protection roles and responsibilities
        4. Technical requirements and safeguards
        """
        state.react_reasoning.append({"step": "analysis_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Perform expert analysis
        print("\n🎬 ACTION: Performing multi-expert analysis...")
        analysis_prompt = self._create_react_analysis_prompt(state.legislation_content, state.supporting_content, state.current_jurisdiction)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst with expertise in data protection law using React reasoning."),
            HumanMessage(content=analysis_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        state.processed_text = response.choices[0].message.content
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=state.processed_text))
        
        # ACTION: Extract adequacy countries from both sections
        print("\n🎬 ACTION: Extracting adequacy countries...")
        state.adequacy_countries = await self._extract_adequacy_countries_comprehensive(
            state.legislation_content, state.supporting_content, state.geography_data
        )
        
        # OBSERVATION: Final analysis results
        observation = f"""
        OBSERVATION: Analysis completed successfully:
        - Expert analysis generated: {len(state.processed_text)} characters
        - Adequacy countries identified: {state.adequacy_countries}
        - Document chunks created: {len(chunked_docs)}
        - Ready for next phase: semantic segmentation
        """
        state.react_reasoning.append({"step": "final_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        state.next_agent = "segmentation"
        print("✅ ReactDocumentProcessorAgent completed successfully")
        
        return state
    
    def _create_react_analysis_prompt(self, legislation_text: str, supporting_text: str, jurisdiction: str) -> str:
        """Create React-style analysis prompt with enhanced supporting information analysis"""
        return f"""
        Use React reasoning pattern (Thought-Action-Observation) to analyze this legal content systematically with special focus on supporting information.

        THOUGHT: I need to analyze this legal document to understand its structure, obligations, and geographic scope, paying special attention to adequacy countries and conditions mentioned in supporting information.
        
        CONTENT TO ANALYZE:
        
        LEGISLATION SECTION ({len(legislation_text)} characters):
        {legislation_text[:4000]}...
        
        SUPPORTING INFORMATION SECTION ({len(supporting_text)} characters):
        {supporting_text[:4000]}...
        
        JURISDICTION: {jurisdiction}
        
        ACTION: Apply multiple expert perspectives systematically with enhanced supporting information analysis:
        
        1. LEGAL STRUCTURE EXPERT ANALYSIS:
        THOUGHT: I need to identify the precise legal framework and obligations.
        ACTION: 
        - Identify specific articles, sections, and legal provisions in LEGISLATION
        - Determine what articles are actually present (be precise - don't reference articles not in the text)
        - Extract core legal obligations and prohibitions from LEGISLATION
        - Identify rights and entitlements from LEGISLATION
        - Extract implementation guidance and examples from SUPPORTING INFORMATION
        OBSERVATION: Document legal structure findings with precise article references.
        
        2. ENHANCED SUPPORTING INFORMATION EXPERT ANALYSIS:
        THOUGHT: Supporting information often contains crucial adequacy countries, implementation conditions, and practical examples.
        ACTION:
        - Extract ALL adequacy countries mentioned in supporting information
        - Identify implementation conditions and requirements from supporting examples
        - Find country-specific guidance and case studies
        - Extract transfer mechanism examples and their applicable countries
        - Identify regulatory precedents and their jurisdictions
        - Look for compliance scenarios mentioning specific countries
        OBSERVATION: Document all supporting information findings that affect rule implementation.
        
        3. COMPREHENSIVE GEOGRAPHIC SCOPE EXPERT ANALYSIS:
        THOUGHT: Adequacy decisions and country-specific rules are often detailed in supporting information.
        ACTION:
        - Identify specific countries and regions mentioned in LEGISLATION
        - Extract ALL adequacy countries from SUPPORTING INFORMATION (this is crucial)
        - Identify cross-border transfer mechanisms from both sections
        - Determine territorial application from both sections
        - Map country-specific implementation requirements from supporting examples
        - Extract regional adequacy frameworks mentioned in either section
        OBSERVATION: Provide comprehensive geographic analysis including ALL countries found in either section.
        
        4. DATA PROTECTION ROLES EXPERT ANALYSIS:
        THOUGHT: Role definitions and obligations may be clarified in supporting information.
        ACTION:
        - Identify Controller, Processor, Joint Controller definitions from LEGISLATION
        - Extract role-specific examples and scenarios from SUPPORTING INFORMATION
        - Determine Data Subject rights from both sections
        - Identify role-specific compliance requirements from supporting examples
        OBSERVATION: Document comprehensive role analysis with supporting examples.
        
        5. CONDITIONS AND REQUIREMENTS EXPERT ANALYSIS:
        THOUGHT: Supporting information often contains detailed conditions and implementation requirements.
        ACTION:
        - Extract mandatory conditions from LEGISLATION
        - Identify implementation conditions from SUPPORTING INFORMATION
        - Extract compliance scenarios and their requirements from supporting examples
        - Identify exception conditions from both sections
        - Map procedural requirements from supporting guidance
        OBSERVATION: Document all conditions including those from supporting information.
        
        6. TECHNICAL SAFEGUARDS EXPERT ANALYSIS:
        THOUGHT: Technical implementation details are often in supporting information.
        ACTION:
        - Identify required technical measures from LEGISLATION
        - Extract implementation examples from SUPPORTING INFORMATION
        - Determine security standards from both sections
        - Identify approved mechanisms from supporting guidance
        OBSERVATION: Document technical requirements with implementation examples.
        
        COMPREHENSIVE SYNTHESIS REQUIREMENTS:
        
        OBSERVATION: Provide structured findings for each expert analysis with special emphasis on:
        
        LEGISLATION CONTENT:
        [Core binding legal requirements with precise article references]
        
        SUPPORTING INFORMATION FINDINGS:
        [All adequacy countries, implementation conditions, examples, and guidance]
        
        COMBINED KEY CONCEPTS:
        - Data Transfer: [findings from both legislation and supporting sections]
        - Access Rights: [findings from both sections]  
        - Entitlements: [findings from both sections]
        - Roles: [definitions from legislation + examples from supporting]
        - Adequacy Countries: [ALL countries found in either section]
        - Implementation Conditions: [conditions from both sections]
        
        COMPREHENSIVE ADEQUACY COUNTRIES/REGIONS:
        [Complete list of ALL countries with adequacy decisions or special transfer status mentioned in EITHER section]
        
        SUPPORTING INFORMATION CONDITIONS:
        [All conditions, requirements, and implementation guidance from supporting section]
        
        CONFIDENCE ASSESSMENT:
        [Rate confidence in analysis and note any uncertainties]
        
        CRITICAL: Pay special attention to supporting information as it often contains:
        - Additional adequacy countries not mentioned in legislation
        - Implementation conditions and requirements
        - Country-specific examples and case studies
        - Practical guidance for compliance
        - Transfer mechanism details
        
        Think step by step through each expert perspective, ensuring comprehensive analysis of BOTH sections.
        """
    
    async def _extract_adequacy_countries_comprehensive(self, legislation_text: str, supporting_text: str, geography_data: Dict[str, Any]) -> List[str]:
        """Extract adequacy countries comprehensively from both legislation and supporting sections"""
        
        extraction_prompt = f"""
        Use React reasoning to comprehensively extract ALL adequacy countries from both legislation and supporting information.
        
        THOUGHT: I need to systematically search for all mentions of adequacy decisions, adequate protection levels, and countries with special data transfer status in BOTH sections.
        
        ACTION: Perform comprehensive analysis of both sections:
        
        LEGISLATION SECTION ({len(legislation_text)} characters):
        {legislation_text[:4000]}...
        
        SUPPORTING INFORMATION SECTION ({len(supporting_text)} characters):
        {supporting_text[:4000]}...
        
        COMPREHENSIVE SEARCH CRITERIA:
        
        1. EXPLICIT ADEQUACY MENTIONS:
        - "adequacy decision"
        - "adequate level of protection"
        - "Commission has decided"
        - "adequate protection"
        - "adequacy status"
        
        2. TRANSFER MECHANISM MENTIONS:
        - Countries with approved transfer frameworks
        - Standard Contractual Clauses (SCC) exemptions
        - Binding Corporate Rules (BCR) contexts
        - Certification mechanisms
        
        3. SPECIFIC COUNTRY REFERENCES:
        - Countries explicitly named in transfer contexts
        - Regional adequacy decisions (EEA, etc.)
        - Third countries with special status
        - Countries mentioned in examples or case studies
        
        4. SUPPORTING INFORMATION ANALYSIS:
        - Country examples in transfer scenarios
        - Case studies mentioning specific countries
        - Implementation guidance referencing countries
        - Regulatory precedents from specific jurisdictions
        
        OBSERVATION: Look for ALL country mentions in BOTH sections that relate to data transfer, adequacy, or special protection status.
        
        THOUGHT: Match found countries with available geography data and provide comprehensive list.
        
        Available geography regions: {list(geography_data.keys()) if geography_data else []}
        
        ACTION: Return COMPREHENSIVE JSON array of ALL ISO2 country codes found with adequacy or special transfer status.
        
        Include countries mentioned in:
        - Direct adequacy decisions
        - Transfer mechanism contexts
        - Supporting information examples
        - Implementation guidance
        - Case studies and scenarios
        
        OBSERVATION: Provide complete list without filtering - include ALL countries with any form of adequacy or special transfer status mentioned.
        
        Example format: ["US", "CA", "JP", "GB", "CH", "NZ", "KR", "IL", "AD", "AR", "UY", "FO", "GG", "IM", "JE"]
        
        Return ONLY the JSON array with ALL adequacy countries found in either section.
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Extract ALL adequacy countries comprehensively using React reasoning. Return only complete JSON array."},
                {"role": "user", "content": extraction_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        # Extract JSON array from the text
        import re
        json_match = re.search(r'\[.*?\]', result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group()
        
        try:
            adequacy_countries = json.loads(result_text)
            logger.info(f"Extracted {len(adequacy_countries)} adequacy countries from comprehensive analysis")
        except json.JSONDecodeError:
            logger.warning("Failed to parse adequacy countries JSON, using enhanced fallback extraction")
            # Enhanced fallback: systematic text analysis
            adequacy_countries = []
            combined_text = (legislation_text + " " + supporting_text).lower()
            
            # Enhanced country detection patterns
            country_patterns = {
                "US": ["united states", "usa", "u.s.", "america"],
                "CA": ["canada", "canadian"],
                "JP": ["japan", "japanese"],
                "GB": ["united kingdom", "uk", "britain", "british"],
                "CH": ["switzerland", "swiss"],
                "NZ": ["new zealand"],
                "KR": ["south korea", "korea", "korean"],
                "IL": ["israel", "israeli"],
                "AD": ["andorra"],
                "AR": ["argentina", "argentinian"],
                "UY": ["uruguay", "uruguayan"],
                "FO": ["faroe islands", "faroese"],
                "GG": ["guernsey"],
                "IM": ["isle of man"],
                "JE": ["jersey"],
                "AU": ["australia", "australian"],
                "SG": ["singapore"],
                "NO": ["norway", "norwegian"],
                "IS": ["iceland", "icelandic"],
                "LI": ["liechtenstein"]
            }
            
            for iso_code, patterns in country_patterns.items():
                for pattern in patterns:
                    if pattern in combined_text and ("adequacy" in combined_text or "adequate" in combined_text or "transfer" in combined_text):
                        if iso_code not in adequacy_countries:
                            adequacy_countries.append(iso_code)
                            logger.info(f"Found adequacy country via pattern matching: {iso_code} ({pattern})")
        
        # Validate country codes if geography manager is available
        if hasattr(self.processor, 'geography_manager') and self.processor.geography_manager:
            validated_countries = []
            for country in adequacy_countries:
                if isinstance(country, str) and self.processor.geography_manager.get_country_info(country):
                    validated_countries.append(country)
                else:
                    logger.warning(f"Invalid country code found: {country}")
            return validated_countries
        
        return [c for c in adequacy_countries if isinstance(c, str)]

class ReactIntelligentSegmentationAgent:
    """Enhanced React Agent for semantic segmentation - preserving all original functionality"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Perform segmentation using React reasoning"""
        logger.info("ReactIntelligentSegmentationAgent: Starting segmentation")
        print("\n🔄 ReactIntelligentSegmentationAgent: Starting segmentation...")
        
        # THOUGHT: Plan segmentation approach
        thought = """
        THOUGHT: I need to segment this legislation using analytical questions to break down complex legal language.
        My approach:
        1. Apply Why/What/When/Where/Who/How framework systematically
        2. Focus on data transfer, access rights, and entitlements
        3. Create structured segments for better rule extraction
        4. Use vector search to enhance segmentation with semantic context
        """
        state.react_reasoning.append({"step": "segmentation_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Perform analytical segmentation
        print("\n🎬 ACTION: Performing analytical segmentation...")
        segmentation_prompt = self._create_react_segmentation_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal segmentation specialist using React reasoning and analytical questions."),
            HumanMessage(content=segmentation_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        segmentation_result = response.choices[0].message.content
        
        # OBSERVATION: Segmentation results
        observation = f"""
        OBSERVATION: Segmentation analysis completed:
        - Generated comprehensive analytical breakdown
        - Applied Why/What/When/Where/Who/How framework
        - Focused on data protection key concepts
        - Ready for structured segment creation
        """
        state.react_reasoning.append({"step": "segmentation_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        # ACTION: Create structured segments with vector search
        print("\n🎬 ACTION: Creating structured segments with semantic context...")
        segments = await self._create_react_structured_segments(segmentation_result, state)
        
        state.segmented_content = segments
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=segmentation_result))
        
        # OBSERVATION: Final segmentation results
        observation = f"""
        OBSERVATION: Structured segmentation completed:
        - Created {len(segments)} analytical segments
        - Each segment enhanced with semantic context
        - Segments cover key data protection concepts
        - Ready for entity extraction phase
        """
        state.react_reasoning.append({"step": "final_segmentation_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        state.next_agent = "entity_extraction"
        logger.info(f"ReactIntelligentSegmentationAgent: Completed with {len(segments)} segments")
        print("✅ ReactIntelligentSegmentationAgent completed successfully")
        
        return state
    
    def _create_react_segmentation_prompt(self, state: AgentState) -> str:
        """Create React-style segmentation prompt"""
        return f"""
        Use React reasoning to systematically segment this legislation using analytical questions.
        
        THOUGHT: I need to break down complex legal language into understandable segments using structured analytical questions.
        
        ACTION: Apply analytical framework systematically:
        
        LEGISLATION CONTENT TO SEGMENT:
        {state.legislation_content[:3000]}...
        
        SUPPORTING INFORMATION CONTEXT:
        {state.supporting_content[:1500]}...
        
        ADEQUACY COUNTRIES IDENTIFIED: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        
        ANALYTICAL QUESTIONS FRAMEWORK:
        
        1. WHY ANALYSIS:
        - What is the legislative purpose behind each provision?
        - What risks or problems are being addressed?
        - What policy objectives are being achieved?
        
        2. WHAT ANALYSIS:
        - What specific obligations are created?
        - What actions are required, prohibited, or permitted?
        - What rights and entitlements are established?
        - What are the precise article numbers mentioned?
        
        3. WHEN ANALYSIS:
        - Under what circumstances do rules apply?
        - What are the triggering conditions?
        - Are there temporal requirements or deadlines?
        - What are the exception conditions?
        
        4. WHERE ANALYSIS:
        - What geographic scope applies?
        - Which countries and jurisdictions are covered?
        - What cross-border implications exist?
        - Which adequacy countries are referenced?
        
        5. WHO ANALYSIS:
        - Which roles are involved (Controller, Processor, Joint Controller, Data Subject)?
        - What entities have obligations or rights?
        - Who has enforcement authority?
        - What role combinations are possible?
        
        6. HOW ANALYSIS:
        - What procedures must be followed?
        - What technical/organizational measures are required?
        - How is compliance demonstrated?
        - What are the implementation mechanisms?
        
        OBSERVATION: Document findings for each analytical dimension.
        
        THOUGHT: Focus especially on data transfer, access rights, and entitlement concepts.
        
        ACTION: Provide structured analysis organized by analytical questions, with specific attention to:
        - Data transfer requirements and restrictions
        - Access rights and procedures
        - Entitlement conditions and criteria
        - Role-specific obligations
        - Edge cases and negations
        
        OBSERVATION: Ensure precise references to actual article numbers present in the text.
        """
    
    async def _create_react_structured_segments(self, segmentation_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Create structured segments using React reasoning and vector search"""
        segments = []
        
        # Create vector store temporarily for this operation
        if state.vector_documents:
            vector_store = self.processor.create_vector_store(state.vector_documents)
            
            # THOUGHT: Plan vector search queries
            thought = """
            THOUGHT: I need to use semantic search to find relevant content for each analytical dimension.
            This will help create more precise and contextual segments.
            """
            state.react_reasoning.append({"step": "vector_search_thought", "content": thought})
            
            # ACTION: Perform vector searches for key concepts
            analytical_queries = [
                "data transfer requirements cross-border international adequacy",
                "access rights data subject procedures requests", 
                "controller processor joint controller responsibilities duties",
                "consent legal basis legitimate interest conditions",
                "adequacy decisions transfer mechanisms safeguards standards",
                "compliance enforcement penalties supervisory authority powers",
                "entitlements rights obligations negations exceptions"
            ]
            
            for query in analytical_queries:
                if vector_store:
                    relevant_docs = vector_store.similarity_search(query, k=3)
                else:
                    # Fallback to using all documents
                    relevant_docs = state.vector_documents[:3]
                
                segments.append({
                    "analytical_focus": query.replace(" ", "_"),
                    "query_used": query,
                    "relevant_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "segmentation_analysis": segmentation_result,
                    "adequacy_countries_context": state.adequacy_countries
                })
            
            # OBSERVATION: Vector search results
            observation = f"""
            OBSERVATION: Vector search enhanced segmentation:
            - Performed {len(analytical_queries)} semantic queries
            - Found relevant content for each analytical dimension
            - Enhanced segments with contextual information
            - Maintained adequacy countries context
            """
            state.react_reasoning.append({"step": "vector_search_observation", "content": observation})
        
        return segments

class ReactComprehensiveEntityExtractionAgent:
    """Enhanced React Agent for entity extraction with resolution - preserving all original functionality"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract and resolve entities using React reasoning"""
        logger.info("ReactComprehensiveEntityExtractionAgent: Starting entity extraction")
        print("\n🔍 ReactComprehensiveEntityExtractionAgent: Starting entity extraction...")
        
        # THOUGHT: Plan entity extraction and resolution approach
        thought = """
        THOUGHT: I need to extract and resolve entities comprehensively using multiple expert perspectives.
        My approach:
        1. Apply mixture of experts for different entity types
        2. Perform semantic and hierarchical entity resolution
        3. Integrate geographic context with adequacy decisions
        4. Handle role combinations and edge cases
        5. Create canonical entity mappings
        """
        state.react_reasoning.append({"step": "entity_extraction_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Perform expert-based entity extraction
        print("\n🎬 ACTION: Performing multi-expert entity extraction...")
        entity_prompt = self._create_react_entity_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are an expert entity extraction and resolution specialist using React reasoning."),
            HumanMessage(content=entity_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        extraction_result = response.choices[0].message.content
        
        # OBSERVATION: Extraction analysis
        observation = f"""
        OBSERVATION: Entity extraction analysis completed:
        - Applied multiple expert perspectives
        - Generated comprehensive entity categorization
        - Identified resolution requirements
        - Ready for structured entity processing
        """
        state.react_reasoning.append({"step": "entity_extraction_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        # ACTION: Structure entities with geography integration and resolution
        print("\n🎬 ACTION: Structuring entities with geographic integration...")
        entities = await self._structure_entities_with_react_resolution(extraction_result, state)
        
        state.extracted_entities = entities
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=extraction_result))
        
        # OBSERVATION: Final entity extraction results
        observation = f"""
        OBSERVATION: Entity extraction and resolution completed:
        - Created {len(entities)} entity categories
        - Applied semantic and hierarchical resolution
        - Integrated geographic context and adequacy decisions
        - Resolved role combinations and relationships
        - Ready for rule component extraction
        """
        state.react_reasoning.append({"step": "final_entity_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        state.next_agent = "rule_extraction"
        logger.info(f"ReactComprehensiveEntityExtractionAgent: Completed with {len(entities)} entity categories")
        print("✅ ReactComprehensiveEntityExtractionAgent completed successfully")
        
        return state
    
    def _create_react_entity_extraction_prompt(self, state: AgentState) -> str:
        """Create React-style entity extraction prompt with resolution"""
        geography_summary = self._create_geography_summary(state.geography_data)
        
        return f"""
        Use React reasoning with multiple expert perspectives to extract and resolve entities systematically.
        
        THOUGHT: I need to identify all entities and resolve them into canonical forms using expert knowledge.
        
        ACTION: Apply expert consultation framework:
        
        CONTENT FOR ENTITY EXTRACTION:
        
        LEGISLATION CONTENT:
        {state.legislation_content[:2000]}...
        
        SUPPORTING INFORMATION:
        {state.supporting_content[:1500]}...
        
        ADEQUACY COUNTRIES: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        
        EXPERT PERSPECTIVES:
        
        1. LEGAL ENTITIES EXPERT:
        THOUGHT: Identify all legal persons, roles, and entities.
        ACTION: Extract and categorize:
        - Controllers, Processors, Joint Controllers (with role combinations)
        - Data Subjects and their categories  
        - Supervisory authorities and enforcement bodies
        - Legal representatives and designated contacts
        OBSERVATION: Document role definitions and relationships.
        
        2. DATA CLASSIFICATION EXPERT:
        THOUGHT: Categorize all data-related entities and concepts.
        ACTION: Extract and classify:
        - Personal data types and special categories
        - Processing operations and purposes
        - Technical measures (pseudonymization, encryption, etc.)
        - Data protection principles and legal bases
        OBSERVATION: Create data type hierarchies and relationships.
        
        3. GEOGRAPHIC/JURISDICTIONAL EXPERT:
        THOUGHT: Map all geographic entities and adequacy relationships.
        ACTION: Extract and standardize:
        - Countries and regions with specific legal status
        - Adequacy jurisdictions and their implications
        - Cross-border transfer mechanisms and frameworks
        - Territorial scope and applicability rules
        OBSERVATION: Standardize to ISO codes and create regional mappings.
        
        4. CONDITIONAL/PROCEDURAL EXPERT:
        THOUGHT: Identify all conditions, procedures, and exceptions.
        ACTION: Extract and organize:
        - Legal bases (consent, legitimate interest, etc.)
        - Triggering conditions and circumstances
        - Exceptions, derogations, and edge cases
        - Procedural requirements and timelines
        OBSERVATION: Group related conditions and map exception hierarchies.
        
        ENTITY RESOLUTION REQUIREMENTS:
        
        SEMANTIC RESOLUTION:
        - Identify synonyms: "data controller" = "controller" = "person responsible for processing"
        - Resolve abbreviations: "DPA" = "Data Protection Authority"
        - Handle variations: "personal data" = "personal information"
        
        HIERARCHICAL RESOLUTION:
        - Map specific to general: "biometric data" → "special category personal data" → "personal data"
        - Create parent-child relationships between concepts
        - Establish role hierarchies and combinations
        
        GEOGRAPHIC RESOLUTION:
        - Standardize country names to ISO2 codes
        - Resolve regional groupings (EU → member states)
        - Map adequacy relationships and implications
        
        CONTEXTUAL RESOLUTION:
        - Distinguish context-specific meanings
        - Handle multiple role assignments (Controller AND Processor)
        - Resolve temporal and conditional variations
        
        AVAILABLE GEOGRAPHY DATA:
        {geography_summary}
        
        THOUGHT: Now I need to synthesize all expert findings into structured entity categories.
        
        ACTION: Create comprehensive entity extraction with:
        - Clear categorization by expert domain
        - Resolution mappings for all identified entities
        - Geographic integration with adequacy context
        - Role relationship mappings
        - Confidence assessments for each entity type
        
        OBSERVATION: Provide structured entity analysis with resolution details.
        """
    
    def _create_geography_summary(self, geography_data: Dict[str, Any]) -> str:
        """Create a summary of available geography data"""
        summary = []
        for region, data in geography_data.items():
            if region == "By_Continent":
                summary.append(f"Continental groupings: {list(data.keys())}")
            else:
                country_count = len(data.get("countries", []))
                territory_count = len(data.get("territories", []))
                summary.append(f"{region}: {country_count} countries, {territory_count} territories")
        return "\n".join(summary)
    
    async def _structure_entities_with_react_resolution(self, extraction_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Structure entities with React reasoning and comprehensive resolution"""
        entities = []
        
        # THOUGHT: Plan entity structuring approach
        thought = """
        THOUGHT: I need to structure the extracted entities using semantic search and perform comprehensive resolution.
        This will create canonical entity mappings that support accurate rule extraction.
        """
        state.react_reasoning.append({"step": "entity_structuring_thought", "content": thought})
        
        # ACTION: Use vector search to enhance entity extraction
        if state.vector_documents:
            # Create temporary vector store
            vector_store = self.processor.create_vector_store(state.vector_documents)
            
            entity_queries = [
                "controller responsibilities obligations data protection duties",
                "processor requirements instructions data processing activities", 
                "data subject rights access rectification erasure portability",
                "cross-border transfer adequacy decisions safeguards mechanisms",
                "supervisory authority enforcement powers penalties investigations",
                "personal data categories special sensitive biometric health",
                "joint controller shared responsibilities decision making"
            ]
            
            for query in entity_queries:
                relevant_docs = vector_store.similarity_search(query, k=2)
                
                # Enhance with geography and adequacy context
                geographic_context = []
                if state.geography_data:
                    geo_manager = GeographyManager(state.geography_data)
                    # Look for country mentions in relevant docs
                    for doc in relevant_docs:
                        for iso_code in geo_manager.country_lookup.keys():
                            country_info = geo_manager.get_country_info(iso_code)
                            if country_info and country_info["name"].lower() in doc.page_content.lower():
                                geographic_context.append(country_info)
                
                # ACTION: Perform entity resolution using LLM
                resolved_entities = await self._perform_react_entity_resolution(
                    [doc.page_content for doc in relevant_docs], 
                    query, 
                    state
                )
                
                entities.append({
                    "entity_category": query.replace(" ", "_"),
                    "query_used": query,
                    "extracted_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "geographic_context": geographic_context,
                    "adequacy_countries_context": state.adequacy_countries,
                    "resolved_entities": resolved_entities,
                    "extraction_analysis": extraction_result
                })
        
        # OBSERVATION: Entity structuring results
        observation = f"""
        OBSERVATION: Entity structuring completed:
        - Created {len(entities)} entity categories with semantic enhancement
        - Applied comprehensive entity resolution for each category
        - Integrated geographic context and adequacy decisions
        - Mapped role relationships and hierarchies
        """
        state.react_reasoning.append({"step": "entity_structuring_observation", "content": observation})
        
        return entities
    
    async def _perform_react_entity_resolution(self, content_chunks: List[str], category: str, state: AgentState) -> Dict[str, Any]:
        """Perform entity resolution using React reasoning"""
        content_text = "\n".join(content_chunks[:2])  # Limit content for API
        
        resolution_prompt = f"""
        Use React reasoning to perform comprehensive entity resolution for category: {category}
        
        THOUGHT: I need to identify all entities in this content and resolve them into canonical forms.
        
        ACTION: Perform systematic entity resolution:
        
        CONTENT TO ANALYZE:
        {content_text}
        
        ADEQUACY COUNTRIES CONTEXT: {state.adequacy_countries}
        AVAILABLE GEOGRAPHY: {list(state.geography_data.keys()) if state.geography_data else []}
        
        RESOLUTION TASKS:
        1. Identify all entities mentioned in the content
        2. Resolve synonyms, abbreviations, and alternative names
        3. Create canonical entity names with clear definitions
        4. Map hierarchical relationships (parent/child/peer)
        5. Standardize geographic references to ISO codes
        6. Handle role combinations and multiple assignments
        7. Identify negations and edge cases
        
        OBSERVATION: Document all resolution mappings and relationships.
        
        THOUGHT: Create structured output that supports accurate rule extraction.
        
        ACTION: Return comprehensive resolution data in JSON format:
        {{
            "canonical_entities": [
                {{
                    "canonical_name": "standardized entity name",
                    "definition": "clear definition of the entity",
                    "aliases": ["alternative name 1", "abbreviation", "synonym"],
                    "entity_type": "controller/processor/data_type/country/condition/etc",
                    "hierarchy_level": "parent/child/peer",
                    "parent_entities": ["broader category entities"],
                    "child_entities": ["more specific entities"],
                    "related_entities": ["associated entities"],
                    "geographic_codes": ["ISO2 codes if applicable"],
                    "role_combinations": ["possible role combinations"],
                    "adequacy_status": "adequacy decision status if applicable",
                    "negation_forms": ["negative forms or exceptions"],
                    "confidence_score": 0.0
                }}
            ],
            "resolution_mappings": {{
                "original_term": "canonical_name",
                "abbreviation": "canonical_name"
            }},
            "role_relationships": {{
                "controller_processor": "relationship description",
                "joint_controller": "shared responsibility description"
            }},
            "geographic_mappings": {{
                "country_name": "ISO2_code",
                "region_name": ["list_of_country_codes"]
            }}
        }}
        
        OBSERVATION: Ensure all entities support multiple role assignments and handle edge cases.
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are an entity resolution specialist using React reasoning. Return only valid JSON."},
                {"role": "user", "content": resolution_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        try:
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse entity resolution JSON, returning empty structure")
            return {
                "canonical_entities": [],
                "resolution_mappings": {},
                "role_relationships": {},
                "geographic_mappings": {}
            }

class ReactIntelligentRuleComponentExtractionAgent:
    """Enhanced React Agent for precise rule extraction with machine-readable format - preserving all original functionality"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract rules using React reasoning with precision and machine-readable format"""
        logger.info("ReactIntelligentRuleComponentExtractionAgent: Starting rule extraction")
        print("\n⚙️ ReactIntelligentRuleComponentExtractionAgent: Starting rule extraction...")
        
        # THOUGHT: Plan comprehensive rule extraction approach
        thought = """
        THOUGHT: I need to extract rules with precision, handling multiple roles, edge cases, and negations.
        Additionally, I need to create machine-readable format for JSON rules engines.
        My approach:
        1. Convert complex legal language to atomic logical statements
        2. Handle multiple role assignments (Controller AND Processor)
        3. Extract precise article references from actual content
        4. Handle negations and edge cases (MUST NOT, exceptions)
        5. Integrate adequacy countries and geographic scope
        6. Create clear rule definitions and conditions
        7. Generate machine-readable conditions and actions for JSON rules engines
        """
        state.react_reasoning.append({"step": "rule_extraction_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Perform comprehensive rule extraction
        print("\n🎬 ACTION: Performing comprehensive rule extraction with machine-readable format...")
        rule_extraction_prompt = self._create_react_rule_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal rule extraction specialist using React reasoning with precision for roles, edge cases, and machine-readable format."),
            HumanMessage(content=rule_extraction_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        rule_text = response.choices[0].message.content
        
        # OBSERVATION: Rule extraction analysis
        observation = f"""
        OBSERVATION: Rule extraction analysis completed:
        - Generated comprehensive rule breakdown
        - Applied logical decomposition with role handling
        - Identified precise article references
        - Handled edge cases and negations
        - Created machine-readable format specifications
        - Ready for structured rule parsing
        """
        state.react_reasoning.append({"step": "rule_extraction_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        # ACTION: Parse and structure rules with enhanced handling
        print("\n🎬 ACTION: Parsing and structuring rules with geographic integration and machine-readable format...")
        rules = await self._parse_rules_with_react_precision(rule_text, state)
        
        state.rules = rules
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=rule_text))
        
        # OBSERVATION: Final rule extraction results
        observation = f"""
        OBSERVATION: Rule extraction and structuring completed:
        - Extracted {len(rules)} precise rules
        - Handled multiple role assignments and combinations
        - Integrated adequacy countries and geographic scope
        - Processed edge cases and negations
        - Created atomic conditions with logical operators
        - Generated machine-readable conditions and actions
        - Created JSON rules engine compatible format
        - Ready for deduplication phase
        """
        state.react_reasoning.append({"step": "final_rule_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        state.next_agent = "rule_deduplication"
        logger.info(f"ReactIntelligentRuleComponentExtractionAgent: Extracted {len(rules)} rules")
        print("✅ ReactIntelligentRuleComponentExtractionAgent completed successfully")
        
        return state
    
    def _create_react_rule_extraction_prompt(self, state: AgentState) -> str:
        """Create comprehensive React-style rule extraction prompt with enhanced supporting information integration and machine-readable format"""
        available_regions = list(state.geography_data.keys()) if state.geography_data else []
        
        return f"""
        Use React reasoning with expert consultation to convert complex legal language into precise, atomic logical statements with comprehensive supporting information integration and machine-readable format for JSON rules engines.
        
        THOUGHT: I need to extract rules with maximum precision, incorporating ALL information from both legislation and supporting sections, especially adequacy countries and implementation conditions, while creating machine-readable format.
        
        CONTENT FOR COMPREHENSIVE RULE EXTRACTION:
        
        LEGISLATION CONTENT:
        {state.legislation_content[:4000]}...
        
        SUPPORTING INFORMATION (CRITICAL FOR ADEQUACY COUNTRIES AND CONDITIONS):
        {state.supporting_content[:3000]}...
        
        PROCESSED ANALYSIS CONTEXT:
        {state.processed_text[:2000] if state.processed_text else "No processed analysis available"}...
        
        EXTRACTED ENTITIES: {len(state.extracted_entities)} categories
        ADEQUACY COUNTRIES IDENTIFIED: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        AVAILABLE REGIONS: {available_regions}
        
        ACTION: Apply comprehensive expert consultation framework:
        
        1. ENHANCED RULE STRUCTURE EXPERT:
        THOUGHT: I need to identify all types of rules from both legislation and supporting guidance.
        ACTION: Categorize rules as:
        - Obligation rules (MUST do X) from legislation
        - Prohibition rules (MUST NOT do Y) from legislation  
        - Permission rules (MAY do Z under conditions) from both sections
        - Conditional rules (IF condition THEN consequence) from both sections
        - Exception rules (EXCEPT when, UNLESS) from both sections
        - Implementation rules (HOW to comply) from supporting information
        OBSERVATION: Document comprehensive rule types with their sources.
        
        2. MACHINE-READABLE FORMAT EXPERT:
        THOUGHT: I need to convert each rule into JSON rules engine compatible format.
        ACTION: For each rule, create:
        - Facts: Variables that can be evaluated (user.role, data.category, transfer.destination_country, etc.)
        - Operators: Comparison operators (equal, notEqual, in, notIn, contains, greaterThan, etc.)
        - Values: Expected values to compare against
        - Logical operators: AND, OR, NOT for combining conditions
        - Actions: What should happen when conditions are met (require, forbid, permit, notify, etc.)
        - Variables: Dynamic variables for complex evaluations
        - If-else logic: Conditional decision trees
        OBSERVATION: Ensure compatibility with JSON rules engine specifications.
        
        3. SUPPORTING INFORMATION INTEGRATION EXPERT:
        THOUGHT: Supporting information contains crucial implementation details, adequacy countries, and conditions.
        ACTION: Extract from supporting information:
        - Additional adequacy countries not in legislation
        - Implementation conditions and requirements
        - Country-specific compliance scenarios
        - Transfer mechanism details and applicable countries
        - Practical examples with specific conditions
        - Regulatory precedents and their implications
        OBSERVATION: Ensure all supporting information findings are integrated into rules.
        
        4. COMPREHENSIVE ADEQUACY INTEGRATION EXPERT:
        THOUGHT: Adequacy countries from supporting information must be integrated into relevant rules.
        ACTION: For each rule involving data transfer:
        - Include ALL adequacy countries from BOTH sections
        - Map adequacy countries to specific transfer rules
        - Include country-specific conditions from supporting information
        - Consider regional adequacy frameworks
        - Integrate transfer mechanism requirements per country
        OBSERVATION: Ensure comprehensive adequacy country integration in all relevant rules.
        
        5. ENHANCED LOGICAL STRUCTURE EXPERT:
        THOUGHT: Conditions from supporting information must be converted to atomic statements and machine-readable format.
        ACTION: Create:
        - Atomic conditions from both legislation and supporting information
        - Implementation conditions from supporting examples
        - Country-specific conditions from supporting guidance
        - Logical operators with proper precedence for complex conditions
        - Clear subject-predicate-object structure for all conditions
        - Negation handling for exceptions and prohibitions
        - Machine-readable fact-operator-value format
        - Variables for dynamic evaluation
        OBSERVATION: Ensure all conditions are testable, implementable, and machine-readable.
        
        6. COMPREHENSIVE ROLE ASSIGNMENT EXPERT:
        THOUGHT: Role assignments must consider examples and scenarios from supporting information.
        ACTION: Assign roles with enhanced precision:
        - Controller obligations from legislation + supporting examples
        - Processor requirements from legislation + supporting scenarios  
        - Joint Controller arrangements from both sections
        - Data Subject rights with supporting implementation guidance
        - Multiple role combinations based on supporting examples
        OBSERVATION: Ensure role assignments reflect comprehensive analysis.
        
        7. ENHANCED GEOGRAPHIC SCOPE EXPERT:
        THOUGHT: Geographic scope must integrate ALL countries from both sections.
        ACTION: Map comprehensive geographic scope:
        - Base jurisdiction requirements from legislation
        - ALL adequacy countries from supporting information: {state.adequacy_countries}
        - Country-specific requirements from supporting examples
        - Regional grouping applications with supporting context
        - Cross-border transfer implications with adequacy considerations
        OBSERVATION: Ensure geographic scope includes all relevant jurisdictions.
        
        8. MACHINE-READABLE ACTION EXPERT:
        THOUGHT: Each rule needs corresponding actions in machine-readable format.
        ACTION: Create actions for each rule:
        - Determine action type (require, forbid, permit, notify, log, validate, etc.)
        - Define action parameters and target roles
        - Include human-readable messages
        - Add variables for dynamic action execution
        - Create if-else logic for conditional actions
        OBSERVATION: Ensure actions are executable and properly linked to conditions.
        
        COMPREHENSIVE RULE FORMULATION PROCESS:
        
        For each legal provision in BOTH sections:
        
        Step 1: IDENTIFY CORE OBLIGATION/RIGHT/PROHIBITION
        THOUGHT: What is the fundamental requirement from legislation + supporting guidance?
        ACTION: Extract primary legal obligation with supporting implementation details.
        OBSERVATION: Document whether it's mandatory, prohibited, or permissive with supporting context.
        
        Step 2: EXTRACT AND INTEGRATE ALL CONDITIONS
        THOUGHT: What conditions exist in BOTH legislation and supporting information?
        ACTION: Create comprehensive atomic conditions:
        - Legislative conditions with logical operators
        - Supporting information conditions and requirements
        - Country-specific conditions from supporting examples
        - Implementation conditions from supporting guidance
        - Adequacy-related conditions for transfer rules
        - Machine-readable fact-operator-value format
        OBSERVATION: Ensure all conditions are independently testable with clear sources and machine-readable.
        
        Step 3: CREATE MACHINE-READABLE CONDITIONS
        THOUGHT: How can I convert these conditions to JSON rules engine format?
        ACTION: For each condition, create:
        - Fact: Variable name (e.g., "user.role", "data.category", "transfer.destination_country")
        - Operator: Comparison operator (e.g., "equal", "in", "contains")
        - Value: Expected value (e.g., "Controller", ["Personal Data", "Special Category Data"])
        - Variables: Dynamic variables for complex evaluations
        - Nested conditions: Hierarchical condition structures
        OBSERVATION: Ensure compatibility with JSON rules engine specifications.
        
        Step 4: CREATE CORRESPONDING ACTIONS
        THOUGHT: What should happen when conditions are met?
        ACTION: Create machine-readable actions:
        - Type: Action type (require, forbid, permit, notify, etc.)
        - Params: Action parameters
        - Target roles: Which roles the action applies to
        - Message: Human-readable description
        - Variables: Dynamic variables for action execution
        - If-else logic: Conditional action execution
        OBSERVATION: Ensure actions are properly linked to conditions.
        
        Step 5: COMPREHENSIVE ROLE AND ADEQUACY ASSIGNMENT
        THOUGHT: Which roles and countries are affected based on BOTH sections?
        ACTION: Assign comprehensive scope:
        - Primary role responsibilities from legislation
        - Supporting role examples and scenarios
        - ALL adequacy countries where relevant: {state.adequacy_countries}
        - Country-specific role requirements from supporting information
        OBSERVATION: Document all applicable role and geographic assignments.
        
        Step 6: SUPPORTING INFORMATION CONDITION INTEGRATION
        THOUGHT: How do supporting information conditions enhance or clarify legislative rules?
        ACTION: Integrate supporting conditions:
        - Implementation requirements from supporting guidance
        - Country-specific conditions from supporting examples
        - Adequacy-related conditions for transfer mechanisms
        - Practical compliance conditions from supporting scenarios
        OBSERVATION: Ensure supporting conditions are properly integrated without duplication.
        
        Step 7: JSON RULES ENGINE FORMAT CREATION
        THOUGHT: How do I create complete JSON rules engine structure?
        ACTION: Create JSON rules engine format:
        - Conditions: All conditions in "all"/"any"/"not" structure
        - Event: Action to trigger when conditions are met
        - Priority: Rule execution priority
        - Variables: Any variables used in the rule
        OBSERVATION: Ensure complete JSON rules engine compatibility.
        
        PRIORITY EXTRACTION WITH SUPPORTING INTEGRATION:
        
        1. ENHANCED DATA TRANSFER RULES:
        - Legislative transfer requirements + supporting implementation guidance
        - ALL adequacy countries from supporting information: {state.adequacy_countries}
        - Transfer mechanism requirements with supporting details
        - Country-specific transfer conditions from supporting examples
        - Machine-readable conditions: fact="transfer.destination_country", operator="in", value={state.adequacy_countries}
        
        2. COMPREHENSIVE ACCESS RIGHTS RULES:
        - Legislative access procedures + supporting implementation examples
        - Controller response obligations with supporting practical guidance
        - Country-specific access requirements from supporting information
        - Implementation timelines and procedures from supporting guidance
        - Machine-readable conditions: fact="request.type", operator="equal", value="Access"
        
        3. ENHANCED ENTITLEMENT RULES:
        - Legislative legal basis requirements + supporting implementation scenarios
        - Consent mechanisms with supporting practical examples
        - Country-specific entitlement conditions from supporting information
        - Implementation requirements from supporting guidance
        - Machine-readable conditions: fact="processing.legal_basis", operator="equal", value="Consent"
        
        RULE FORMULATION REQUIREMENTS WITH SUPPORTING INTEGRATION:
        
        Each extracted rule MUST include:
        - Unique timestamp-based identifier
        - Clear rule text integrating legislative and supporting information
        - Detailed rule definition with supporting context
        - ALL adequacy countries from supporting information where relevant
        - Comprehensive conditions from both sections (original RuleCondition format)
        - Machine-readable conditions in JSON rules engine format
        - Machine-readable actions with parameters and variables
        - Multiple role assignments based on complete analysis
        - Precise legal references to actual articles present
        - Data categories affected by the rule
        - Implementation guidance from supporting information
        - Confidence score reflecting completeness of integration
        - JSON rules engine compatible format
        - Variables and if-else logic for complex scenarios
        - Priority level for rule execution order
        
        MACHINE-READABLE EXAMPLES:
        
        Data Transfer Rule Example:
        {{
            "conditions": {{
                "all": [
                    {{
                        "fact": "user.role",
                        "operator": "equal",
                        "value": "Controller"
                    }},
                    {{
                        "fact": "transfer.destination_country",
                        "operator": "notIn",
                        "value": {state.adequacy_countries}
                    }}
                ]
            }},
            "event": {{
                "type": "require",
                "params": {{
                    "action": "implement_safeguards",
                    "safeguards": ["Standard Contractual Clauses", "Binding Corporate Rules"]
                }}
            }},
            "priority": 100
        }}
        
        Access Rights Rule Example:
        {{
            "conditions": {{
                "all": [
                    {{
                        "fact": "request.type",
                        "operator": "equal",
                        "value": "Access"
                    }},
                    {{
                        "fact": "user.role",
                        "operator": "equal",
                        "value": "Controller"
                    }}
                ]
            }},
            "event": {{
                "type": "require",
                "params": {{
                    "action": "respond_within_timeframe",
                    "timeframe": "30 days"
                }}
            }},
            "priority": 80
        }}
        
        CRITICAL SUPPORTING INFORMATION INTEGRATION:
        - Include ALL adequacy countries found in supporting information
        - Integrate implementation conditions from supporting examples
        - Consider country-specific scenarios from supporting guidance
        - Include practical compliance requirements from supporting information
        - Ensure transfer rules reflect adequacy countries from supporting context
        
        THOUGHT: Now I need to systematically extract rules ensuring complete integration of supporting information and machine-readable format.
        
        ACTION: Perform comprehensive rule extraction with full supporting information integration and machine-readable format creation.
        
        OBSERVATION: Ensure each rule comprehensively reflects both legislative requirements and supporting implementation guidance, especially adequacy countries and conditions, while being compatible with JSON rules engines.
        
        Be extremely precise about article references and ensure ALL adequacy countries from supporting information are integrated into relevant rules with machine-readable format.
        """
    
    async def _parse_rules_with_react_precision(self, rule_text: str, state: AgentState) -> List[LegislationRule]:
        """Parse LLM response into structured rules with React precision, enhanced handling, and machine-readable format"""
        
        # THOUGHT: Plan rule parsing approach
        thought = """
        THOUGHT: I need to convert the rule extraction into structured LegislationRule objects with enhanced precision and machine-readable format.
        This requires careful handling of multiple roles, adequacy countries, geographic scope, and JSON rules engine compatibility.
        """
        state.react_reasoning.append({"step": "rule_parsing_thought", "content": thought})
        
        rules = []
        geo_manager = GeographyManager(state.geography_data) if state.geography_data else None
        
        # ACTION: Structure the extracted rules using LLM
        structure_prompt = f"""
        Use React reasoning to convert rule extraction into precise structured JSON format with machine-readable components.
        
        THOUGHT: I need to create structured rule objects that preserve all extracted information with enhanced precision and machine-readable format.
        
        ACTION: Convert the following rule extraction into structured JSON:
        
        {rule_text}
        
        CONTEXT FOR STRUCTURING:
        - Adequacy countries identified: {state.adequacy_countries}
        - Default jurisdiction: {state.current_jurisdiction}
        - Available regions: {list(state.geography_data.keys()) if state.geography_data else []}
        - Geography integration required for ISO2 codes
        
        OBSERVATION: Structure must include all role combinations, edge cases, and machine-readable format.
        
        THOUGHT: Create comprehensive rule objects with enhanced fields and machine-readable components.
        
        ACTION: Return JSON array with this exact enhanced structure:
        [{{
            "rule_id": "unique_identifier_with_timestamp",
            "rule_text": "clear, simple rule statement",
            "rule_definition": "detailed explanation of the rule obligation/right",
            "applies_to_countries": ["ISO2_codes_for_territorial_scope"],
            "roles": ["Controller", "Processor", "Joint Controller", "Data Subject"],
            "data_categories": ["Personal Data", "Special Category Data", "Biometric Data", "Health Data", "etc"],
            "conditions": [
                {{
                    "condition_text": "atomic condition statement",
                    "logical_operator": "AND/OR/NOT",
                    "roles": ["Controller", "Processor"],
                    "is_negation": false
                }}
            ],
            "machine_readable_conditions": [
                {{
                    "condition_id": "unique_condition_id",
                    "fact": "user.role",
                    "operator": "equal",
                    "value": "Controller",
                    "path": "$.nested.property",
                    "logical_operator": "and",
                    "roles": ["Controller"],
                    "is_negation": false,
                    "variables": {{"var1": "value1"}},
                    "nested_conditions": [],
                    "original_condition_text": "original human-readable condition"
                }}
            ],
            "machine_readable_actions": [
                {{
                    "action_id": "unique_action_id",
                    "type": "require",
                    "params": {{"field": "consent", "timeframe": "30 days"}},
                    "target_roles": ["Controller"],
                    "conditions": ["condition_id"],
                    "message": "Controller must obtain consent",
                    "metadata": {{}},
                    "variables": {{"dynamic_var": "value"}},
                    "if_else_logic": {{}}
                }}
            ],
            "json_rules_engine_format": {{
                "conditions": {{
                    "all": [
                        {{
                            "fact": "user.role",
                            "operator": "equal",
                            "value": "Controller"
                        }}
                    ]
                }},
                "event": {{
                    "type": "require",
                    "params": {{"action": "obtain_consent"}}
                }},
                "priority": 50
            }},
            "condition_count": 0,
            "references": ["Article 44", "Section 2.1"],
            "adequacy_countries": ["ISO2_codes_mentioned_in_rule_context"],
            "extraction_metadata": {{
                "confidence_score": 0.0,
                "complexity_level": "low/medium/high",
                "article_numbers_present": ["44", "45"],
                "role_combinations": ["Controller+Processor"],
                "edge_cases_handled": ["negations", "exceptions"],
                "data_categories_identified": ["Personal Data", "Special Category Data"]
            }},
            "confidence_score": 0.0,
            "duplicate_of": null,
            "priority": 50,
            "nested_rules": [],
            "parent_rule_id": null,
            "variables": {{"rule_var": "value"}},
            "if_else_logic": {{}}
        }}]
        
        DATA CATEGORIES TO IDENTIFY:
        - Personal Data (general personal information)
        - Special Category Data (sensitive personal data)
        - Biometric Data (fingerprints, DNA, etc.)
        - Health Data (medical information)
        - Genetic Data (genetic information)
        - Financial Data (payment, banking information)
        - Location Data (geographic position data)
        - Communication Data (emails, messages)
        - Behavioral Data (online behavior, preferences)
        - Identification Data (names, IDs, numbers)
        - Criminal Data (criminal convictions, offenses)
        - Professional Data (employment, qualifications)
        
        MACHINE-READABLE FACTS TO USE:
        - user.role (Controller, Processor, Joint Controller, Data Subject)
        - data.category (Personal Data, Special Category Data, etc.)
        - transfer.destination_country (ISO2 country code)
        - processing.legal_basis (Consent, Legitimate Interest, etc.)
        - request.type (Access, Rectification, Erasure, Portability)
        - adequacy.status (true/false for adequacy decision)
        - processing.purpose (Marketing, Analytics, etc.)
        - data.retention_period (number of days/months/years)
        
        MACHINE-READABLE OPERATORS TO USE:
        - equal, notEqual, greaterThan, lessThan, greaterThanInclusive, lessThanInclusive
        - contains, notContains, in, notIn
        - exists, notExists
        - and, or, not (for logical combinations)
        
        ACTION TYPES TO USE:
        - require, forbid, permit, notify, log, validate, transform, escalate
        - obtain_consent, provide_notice, implement_safeguards, conduct_assessment
        
        UNIQUE RULE ID FORMAT:
        - Use format: "rule_[article]_[category]_[timestamp]"
        - Example: "rule_art44_transfer_20250101_001"
        - Ensure each ID is completely unique
        
        REQUIREMENTS:
        - Generate unique rule IDs with timestamp-based suffixes
        - Identify and link relevant data categories for each rule
        - Use only ISO2 codes that exist in geography data
        - Include multiple roles where applicable (Controller AND Processor)
        - Handle negations with is_negation flag
        - Reference only articles actually present in source text
        - Include rule_definition field with detailed explanation
        - Add adequacy_countries field with context from rule
        - Ensure atomic conditions with clear logical operators
        - Assign multiple roles to conditions where applicable
        - Create machine-readable conditions and actions
        - Generate complete JSON rules engine format
        - Include variables and if-else logic for complex scenarios
        - Set appropriate priority levels (1-100, higher = more important)
        
        OBSERVATION: Ensure comprehensive rule coverage with precision and machine-readable compatibility.
        """
        
        structure_response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a JSON formatter using React reasoning. Return only valid JSON with enhanced rule structure and machine-readable format."},
                {"role": "user", "content": structure_prompt}
            ]
        )
        
        json_text = structure_response.choices[0].message.content
        
        # Clean JSON response
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
        elif "```" in json_text:
            json_text = json_text.split("```")[1]
        
        try:
            rules_data = json.loads(json_text.strip())
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse rules JSON: {e}, returning empty list")
            state.react_reasoning.append({"step": "rule_parsing_error", "content": f"JSON parsing failed: {e}"})
            return []
        
        # ACTION: Convert parsed data to LegislationRule objects with unique IDs
        timestamp = int(time.time())
        
        for idx, rule_data in enumerate(rules_data):
            # Handle original conditions (preserving original structure)
            conditions = []
            for cond in rule_data.get("conditions", []):
                # Handle multiple roles in conditions
                cond_roles = []
                for role_str in cond.get("roles", []):
                    if role_str in [e.value for e in RoleType]:
                        cond_roles.append(RoleType(role_str))
                
                conditions.append(RuleCondition(
                    condition_text=cond.get("condition_text", ""),
                    logical_operator=cond.get("logical_operator"),
                    roles=cond_roles,
                    is_negation=cond.get("is_negation", False)
                ))
            
            # Handle machine-readable conditions
            mr_conditions = []
            for mr_cond in rule_data.get("machine_readable_conditions", []):
                # Handle roles
                mr_cond_roles = []
                for role_str in mr_cond.get("roles", []):
                    if role_str in [e.value for e in RoleType]:
                        mr_cond_roles.append(RoleType(role_str))
                
                # Handle operator
                try:
                    operator = OperatorType(mr_cond.get("operator", "equal"))
                except ValueError:
                    operator = OperatorType.EQUAL
                
                # Handle logical operator
                logical_op = None
                if mr_cond.get("logical_operator"):
                    try:
                        logical_op = OperatorType(mr_cond.get("logical_operator"))
                    except ValueError:
                        pass
                
                mr_condition = MachineReadableCondition(
                    condition_id=mr_cond.get("condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    fact=mr_cond.get("fact", "unknown"),
                    operator=operator,
                    value=mr_cond.get("value", ""),
                    path=mr_cond.get("path"),
                    logical_operator=logical_op,
                    roles=mr_cond_roles,
                    is_negation=mr_cond.get("is_negation", False),
                    variables=mr_cond.get("variables", {}),
                    nested_conditions=[],  # Simplified for now
                    original_condition_text=mr_cond.get("original_condition_text")
                )
                mr_conditions.append(mr_condition)
            
            # Handle machine-readable actions
            mr_actions = []
            for mr_action in rule_data.get("machine_readable_actions", []):
                # Handle target roles
                target_roles = []
                for role_str in mr_action.get("target_roles", []):
                    if role_str in [e.value for e in RoleType]:
                        target_roles.append(RoleType(role_str))
                
                # Handle action type
                try:
                    action_type = ActionType(mr_action.get("type", "require"))
                except ValueError:
                    action_type = ActionType.REQUIRE
                
                mr_action_obj = MachineReadableAction(
                    action_id=mr_action.get("action_id", f"action_{uuid.uuid4().hex[:8]}"),
                    type=action_type,
                    params=mr_action.get("params", {}),
                    target_roles=target_roles,
                    conditions=mr_action.get("conditions", []),
                    message=mr_action.get("message"),
                    metadata=mr_action.get("metadata", {}),
                    variables=mr_action.get("variables", {}),
                    if_else_logic=mr_action.get("if_else_logic", {})
                )
                mr_actions.append(mr_action_obj)
            
            # Handle multiple roles for the rule
            rule_roles = []
            for role_str in rule_data.get("roles", []):
                if role_str in [e.value for e in RoleType]:
                    rule_roles.append(RoleType(role_str))
            
            # Generate unique rule ID
            base_id = rule_data.get("rule_id", f"rule_unknown_{idx}")
            unique_id = f"{base_id}_{timestamp}_{idx:03d}"
            
            # Validate and clean country codes
            applies_to = rule_data.get("applies_to_countries", [state.current_jurisdiction])
            adequacy_countries = rule_data.get("adequacy_countries", [])
            data_categories = rule_data.get("data_categories", [])
            
            if geo_manager:
                # Validate applies_to countries
                validated_countries = []
                for country_code in applies_to:
                    if geo_manager.get_country_info(country_code):
                        validated_countries.append(country_code)
                    else:
                        # Try to find by name
                        matches = geo_manager.find_countries_by_name(country_code)
                        validated_countries.extend(matches)
                applies_to = validated_countries if validated_countries else [state.current_jurisdiction]
                
                # Validate adequacy countries
                validated_adequacy = []
                for country_code in adequacy_countries:
                    if geo_manager.get_country_info(country_code):
                        validated_adequacy.append(country_code)
                adequacy_countries = validated_adequacy
            
            extraction_metadata = rule_data.get("extraction_metadata", {})
            confidence_score = float(extraction_metadata.get("confidence_score", 0.8))
            
            rule = LegislationRule(
                rule_id=unique_id,
                rule_text=rule_data.get("rule_text", ""),
                rule_definition=rule_data.get("rule_definition", rule_data.get("rule_text", "")),
                applies_to_countries=applies_to,
                roles=rule_roles,
                data_categories=data_categories,
                conditions=conditions,
                condition_count=len(conditions),
                machine_readable_conditions=mr_conditions,
                machine_readable_actions=mr_actions,
                json_rules_engine_format=rule_data.get("json_rules_engine_format", {}),
                references=rule_data.get("references", []),
                adequacy_countries=adequacy_countries,
                extraction_metadata=extraction_metadata,
                confidence_score=confidence_score,
                duplicate_of=rule_data.get("duplicate_of"),
                priority=rule_data.get("priority", 50),
                nested_rules=[],  # Will be populated in post-processing if needed
                parent_rule_id=rule_data.get("parent_rule_id"),
                variables=rule_data.get("variables", {}),
                if_else_logic=rule_data.get("if_else_logic", {})
            )
            rules.append(rule)
        
        # OBSERVATION: Rule parsing results
        observation = f"""
        OBSERVATION: Rule parsing completed successfully:
        - Converted {len(rules)} rule extractions to structured objects
        - Handled multiple role assignments and combinations
        - Integrated adequacy countries and geographic scope
        - Processed edge cases and negations
        - Validated country codes against geography data
        - Created machine-readable conditions and actions
        - Generated JSON rules engine compatible format
        """
        state.react_reasoning.append({"step": "rule_parsing_observation", "content": observation})
        
        return rules

class RuleDeduplicationAgent:
    """Agent to identify and handle duplicate rules using LLM semantic analysis - preserving all original functionality"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Identify and deduplicate rules using LLM semantic analysis"""
        logger.info("RuleDeduplicationAgent: Starting rule deduplication")
        
        try:
            if len(state.rules) <= 1:
                state.deduplicated_rules = state.rules
                state.next_agent = "sanity_check"
                return state
            
            # Perform pairwise semantic comparison using LLM
            duplicate_pairs = await self._identify_duplicate_pairs(state.rules)
            
            # Create deduplication plan
            deduplication_plan = await self._create_deduplication_plan(state.rules, duplicate_pairs)
            
            # Execute deduplication
            deduplicated_rules = await self._execute_deduplication(state.rules, deduplication_plan)
            
            state.deduplicated_rules = deduplicated_rules
            state.next_agent = "sanity_check"
            
            logger.info(f"RuleDeduplicationAgent: Reduced {len(state.rules)} rules to {len(deduplicated_rules)} after deduplication")
            
        except Exception as e:
            error_msg = f"RuleDeduplicationAgent error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            # Continue with original rules if deduplication fails
            state.deduplicated_rules = state.rules
            state.next_agent = "sanity_check"
        
        return state
    
    async def _identify_duplicate_pairs(self, rules: List[LegislationRule]) -> List[Tuple[int, int, str]]:
        """Identify potentially duplicate rule pairs using LLM analysis"""
        duplicate_pairs = []
        
        # Validate rules are proper LegislationRule objects
        validated_rules = []
        for i, rule in enumerate(rules):
            if isinstance(rule, LegislationRule):
                validated_rules.append(rule)
            elif isinstance(rule, dict):
                logger.warning(f"Converting dict to LegislationRule at index {i}")
                try:
                    # Convert dict to LegislationRule
                    conditions = []
                    for cond_data in rule.get("conditions", []):
                        cond_roles = []
                        for role_str in cond_data.get("roles", []):
                            if role_str in [e.value for e in RoleType]:
                                cond_roles.append(RoleType(role_str))
                        conditions.append(RuleCondition(
                            condition_text=cond_data.get("condition_text", ""),
                            logical_operator=cond_data.get("logical_operator"),
                            roles=cond_roles,
                            is_negation=cond_data.get("is_negation", False)
                        ))
                    
                    rule_roles = []
                    for role_str in rule.get("roles", []):
                        if role_str in [e.value for e in RoleType]:
                            rule_roles.append(RoleType(role_str))
                    
                    validated_rule = LegislationRule(
                        rule_id=rule.get("rule_id", f"rule_{i+1}"),
                        rule_text=rule.get("rule_text", ""),
                        rule_definition=rule.get("rule_definition", ""),
                        applies_to_countries=rule.get("applies_to_countries", []),
                        roles=rule_roles,
                        data_categories=rule.get("data_categories", []),
                        conditions=conditions,
                        condition_count=len(conditions),
                        references=rule.get("references", []),
                        adequacy_countries=rule.get("adequacy_countries", []),
                        extraction_metadata=rule.get("extraction_metadata", {}),
                        confidence_score=float(rule.get("confidence_score", 0.8)),
                        duplicate_of=rule.get("duplicate_of")
                    )
                    validated_rules.append(validated_rule)
                except Exception as e:
                    logger.error(f"Failed to convert rule dict to LegislationRule: {e}")
                    continue
            else:
                logger.warning(f"Skipping invalid rule type: {type(rule)}")
                continue
        
        # Use validated rules for comparison
        rules = validated_rules
        
        # Compare rules pairwise
        for i in range(len(rules)):
            for j in range(i + 1, len(rules)):
                rule1 = rules[i]
                rule2 = rules[j]
                
                comparison_prompt = f"""
                Analyze if these two legal rules are duplicates or substantially similar.
                
                CHAIN OF THOUGHT ANALYSIS:
                
                Step 1: SEMANTIC SIMILARITY ASSESSMENT
                Compare the core meaning and intent of both rules.
                
                Step 2: LOGICAL EQUIVALENCE CHECK
                Do the rules create the same obligations/rights under the same conditions?
                
                Step 3: SCOPE AND APPLICABILITY
                Do they apply to the same roles, countries, and circumstances?
                
                Step 4: CONDITION ANALYSIS
                Are the conditions logically equivalent or overlapping?
                
                RULE 1:
                ID: {rule1.rule_id}
                Text: {rule1.rule_text}
                Definition: {rule1.rule_definition}
                Countries: {rule1.applies_to_countries}
                Roles: {[r.value for r in rule1.roles]}
                Conditions: {[c.condition_text for c in rule1.conditions]}
                
                RULE 2: 
                ID: {rule2.rule_id}
                Text: {rule2.rule_text}
                Definition: {rule2.rule_definition}
                Countries: {rule2.applies_to_countries}
                Roles: {[r.value for r in rule2.roles]}
                Conditions: {[c.condition_text for c in rule2.conditions]}
                
                DECISION CATEGORIES:
                - DUPLICATE: Essentially the same rule (merge recommended)
                - SIMILAR: Related but distinct (keep both, note relationship)
                - DIFFERENT: Clearly distinct rules (no action needed)
                
                Respond with only: DUPLICATE, SIMILAR, or DIFFERENT
                """
                
                response = openai_client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {"role": "system", "content": "You are a legal analysis expert. Analyze rule similarities systematically."},
                        {"role": "user", "content": comparison_prompt}
                    ]
                )
                
                result = response.choices[0].message.content.strip().upper()
                
                if result in ["DUPLICATE", "SIMILAR"]:
                    duplicate_pairs.append((i, j, result))
        
        return duplicate_pairs
    
    async def _create_deduplication_plan(self, rules: List[LegislationRule], duplicate_pairs: List[Tuple[int, int, str]]) -> Dict[str, Any]:
        """Create a plan for handling duplicates"""
        if not duplicate_pairs:
            return {"action": "no_duplicates", "rules_to_keep": list(range(len(rules)))}
        
        plan_prompt = f"""
        Create a deduplication plan for these legal rules based on identified similarities.
        
        RULES: {len(rules)} total rules
        DUPLICATE/SIMILAR PAIRS: {len(duplicate_pairs)} pairs identified
        
        PAIR DETAILS:
        {chr(10).join([f"Rules {pair[0]} and {pair[1]}: {pair[2]}" for pair in duplicate_pairs])}
        
        DEDUPLICATION STRATEGY:
        
        For DUPLICATE pairs:
        - Keep the rule with higher confidence score
        - If confidence is equal, keep the one with more comprehensive conditions
        - Mark the removed rule as duplicate_of the kept rule
        
        For SIMILAR pairs:
        - Keep both rules but note their relationship
        - Add cross-references in metadata
        
        CHAIN OF THOUGHT PLANNING:
        
        Step 1: Identify which rules to keep vs remove
        Step 2: Determine merge strategies for duplicates  
        Step 3: Plan metadata updates for relationships
        Step 4: Ensure no orphaned conditions or important details are lost
        
        Return a JSON plan with this structure:
        {{
            "action": "deduplicate",
            "rules_to_keep": [list of rule indices],
            "rules_to_remove": [list of rule indices], 
            "merge_instructions": [
                {{
                    "keep_rule": index,
                    "remove_rule": index,
                    "merge_conditions": true/false,
                    "merge_countries": true/false
                }}
            ],
            "relationship_notes": [
                {{
                    "rule1": index,
                    "rule2": index, 
                    "relationship": "similar/related"
                }}
            ]
        }}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Create deduplication plans in JSON format only."},
                {"role": "user", "content": plan_prompt}
            ]
        )
        
        plan_text = response.choices[0].message.content.strip()
        
        # Clean JSON
        if "```json" in plan_text:
            plan_text = plan_text.split("```json")[1].split("```")[0]
        elif "```" in plan_text:
            plan_text = plan_text.split("```")[1]
        
        try:
            return json.loads(plan_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse deduplication plan JSON, returning no action plan")
            return {"action": "no_duplicates", "rules_to_keep": list(range(len(rules)))}
    
    async def _execute_deduplication(self, rules: List[LegislationRule], plan: Dict[str, Any]) -> List[LegislationRule]:
        """Execute the deduplication plan"""
        if plan.get("action") == "no_duplicates":
            return rules
        
        deduplicated_rules = []
        rules_to_keep = plan.get("rules_to_keep", [])
        merge_instructions = plan.get("merge_instructions", [])
        
        # Create a mapping of removed rules to their kept counterparts
        removal_mapping = {}
        for instruction in merge_instructions:
            keep_idx = instruction.get("keep_rule")
            remove_idx = instruction.get("remove_rule")
            if keep_idx is not None and remove_idx is not None:
                removal_mapping[remove_idx] = keep_idx
        
        # Process rules
        for i, rule in enumerate(rules):
            if i in rules_to_keep:
                # Check if this rule should be merged with others
                merged_rule = rule.model_copy()
                
                # Find all rules that should be merged into this one
                for instruction in merge_instructions:
                    if instruction.get("keep_rule") == i:
                        remove_idx = instruction.get("remove_rule")
                        if remove_idx < len(rules):
                            removed_rule = rules[remove_idx]
                            
                            # Merge conditions if requested
                            if instruction.get("merge_conditions", False):
                                for condition in removed_rule.conditions:
                                    if condition not in merged_rule.conditions:
                                        merged_rule.conditions.append(condition)
                                merged_rule.condition_count = len(merged_rule.conditions)
                                
                                # Merge machine-readable conditions
                                for mr_condition in removed_rule.machine_readable_conditions:
                                    if mr_condition not in merged_rule.machine_readable_conditions:
                                        merged_rule.machine_readable_conditions.append(mr_condition)
                                
                                # Merge machine-readable actions
                                for mr_action in removed_rule.machine_readable_actions:
                                    if mr_action not in merged_rule.machine_readable_actions:
                                        merged_rule.machine_readable_actions.append(mr_action)
                            
                            # Merge countries if requested
                            if instruction.get("merge_countries", False):
                                for country in removed_rule.applies_to_countries:
                                    if country not in merged_rule.applies_to_countries:
                                        merged_rule.applies_to_countries.append(country)
                            
                            # Merge references
                            for ref in removed_rule.references:
                                if ref not in merged_rule.references:
                                    merged_rule.references.append(ref)
                            
                            # Update metadata
                            merged_rule.extraction_metadata["merged_from"] = merged_rule.extraction_metadata.get("merged_from", [])
                            merged_rule.extraction_metadata["merged_from"].append(removed_rule.rule_id)
                
                deduplicated_rules.append(merged_rule)
            
            elif i in removal_mapping:
                # Mark as duplicate
                duplicate_rule = rule.model_copy()
                duplicate_rule.duplicate_of = rules[removal_mapping[i]].rule_id
                # Don't add to final list, but log the relationship
                logger.info(f"Rule {rule.rule_id} marked as duplicate of {duplicate_rule.duplicate_of}")
        
        return deduplicated_rules

class OutputGenerationAgent:
    """Agent to generate final CSV and JSON output with enhanced fields and machine-readable format"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Generate output files in CSV and JSON format with enhanced rule structure and machine-readable format"""
        logger.info("OutputGenerationAgent: Generating output files")
        print("\n📄 OutputGenerationAgent: Starting output generation...")
        
        # Use deduplicated rules if available, otherwise use original rules
        final_rules_raw = []
        if hasattr(state, 'deduplicated_rules') and state.deduplicated_rules:
            final_rules_raw = state.deduplicated_rules
        elif hasattr(state, 'rules') and state.rules:
            final_rules_raw = state.rules
        else:
            logger.error("No rules found in state")
            print("❌ No rules found in state")
            state.next_agent = "end"
            return state
        
        print(f"📊 Processing {len(final_rules_raw)} final rules for output")
        
        # Validate that we have LegislationRule objects
        validated_rules = []
        for i, rule in enumerate(final_rules_raw):
            if isinstance(rule, LegislationRule):
                validated_rules.append(rule)
            elif isinstance(rule, dict):
                # Convert dict back to LegislationRule
                try:
                    conditions = []
                    for cond_data in rule.get("conditions", []):
                        if isinstance(cond_data, dict):
                            cond_roles = []
                            for role_str in cond_data.get("roles", []):
                                if role_str in [e.value for e in RoleType]:
                                    cond_roles.append(RoleType(role_str))
                            
                            conditions.append(RuleCondition(
                                condition_text=cond_data.get("condition_text", ""),
                                logical_operator=cond_data.get("logical_operator"),
                                roles=cond_roles,
                                is_negation=cond_data.get("is_negation", False)
                            ))
                    
                    rule_roles = []
                    for role_str in rule.get("roles", []):
                        if role_str in [e.value for e in RoleType]:
                            rule_roles.append(RoleType(role_str))
                    
                    validated_rule = LegislationRule(
                        rule_id=rule.get("rule_id", f"rule_{i+1}"),
                        rule_text=rule.get("rule_text", ""),
                        rule_definition=rule.get("rule_definition", rule.get("rule_text", "")),
                        applies_to_countries=rule.get("applies_to_countries", []),
                        roles=rule_roles,
                        data_categories=rule.get("data_categories", []),
                        conditions=conditions,
                        condition_count=len(conditions),
                        references=rule.get("references", []),
                        adequacy_countries=rule.get("adequacy_countries", []),
                        extraction_metadata=rule.get("extraction_metadata", {}),
                        confidence_score=float(rule.get("confidence_score", 0.8)),
                        duplicate_of=rule.get("duplicate_of")
                    )
                    validated_rules.append(validated_rule)
                except Exception as e:
                    logger.warning(f"Failed to convert rule dict to LegislationRule: {e}")
                    continue
            else:
                logger.warning(f"Unexpected rule type: {type(rule)}, skipping")
                continue
        
        final_rules = validated_rules
        print(f"📊 Validated {len(final_rules)} rules for output")
        
        # Generate Enhanced CSV output with conditions per row
        print("📄 Generating enhanced CSV with conditions per row...")
        await self._generate_enhanced_conditions_csv(final_rules)
        
        # Generate Enhanced JSON output
        print("📄 Generating enhanced JSON output...")
        await self._generate_enhanced_json(final_rules)
        
        # Generate JSON Rules Engine format
        print("📄 Generating JSON Rules Engine format...")
        await self._generate_json_rules_engine_format(final_rules)
        
        # Generate comprehensive report
        print("📄 Generating comprehensive processing report...")
        await self._generate_enhanced_report(final_rules, state)
        
        # Generate React reasoning log
        reasoning_log_path = os.path.join(OUTPUT_PATH, "react_reasoning_log.json")
        reasoning_data = getattr(state, 'react_reasoning', [])
        with open(reasoning_log_path, 'w', encoding='utf-8') as logfile:
            json.dump(reasoning_data, logfile, indent=2, ensure_ascii=False)
        
        print(f"✅ React reasoning log saved to: {reasoning_log_path}")
        
        logger.info(f"Generated output files with enhanced machine-readable format")
        state.next_agent = "end"
        
        print("✅ OutputGenerationAgent completed successfully")
        return state
    
    async def _generate_enhanced_conditions_csv(self, rules: List[LegislationRule]):
        """Generate CSV with each condition per row and duplicated rule IDs"""
        csv_path = os.path.join(OUTPUT_PATH, "enhanced_rules_conditions.csv")
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'rule_id', 'rule_text', 'rule_definition', 'applies_to_countries', 'roles', 'data_categories',
                # Original condition fields
                'condition_text', 'condition_logical_operator', 'condition_roles', 'condition_is_negation',
                # Machine-readable condition fields
                'mr_condition_id', 'mr_fact', 'mr_operator', 'mr_value', 'mr_path', 'mr_logical_operator',
                'mr_roles', 'mr_is_negation', 'mr_variables', 'mr_original_condition_text',
                # Machine-readable action fields
                'mr_action_id', 'mr_action_type', 'mr_action_params', 'mr_action_target_roles', 
                'mr_action_message', 'mr_action_variables', 'mr_action_if_else_logic',
                # Rule metadata
                'adequacy_countries', 'references', 'confidence_score', 'priority', 
                'json_rules_engine_format', 'variables', 'if_else_logic'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for rule in rules:
                # Basic rule information
                rule_info = {
                    'rule_id': rule.rule_id,
                    'rule_text': rule.rule_text,
                    'rule_definition': rule.rule_definition,
                    'applies_to_countries': ", ".join(rule.applies_to_countries),
                    'roles': ", ".join([r.value for r in rule.roles]),
                    'data_categories': ", ".join([str(dc) for dc in rule.data_categories]),
                    'adequacy_countries': ", ".join(rule.adequacy_countries),
                    'references': ", ".join(rule.references),
                    'confidence_score': rule.confidence_score,
                    'priority': rule.priority,
                    'json_rules_engine_format': json.dumps(rule.json_rules_engine_format),
                    'variables': json.dumps(rule.variables),
                    'if_else_logic': json.dumps(rule.if_else_logic)
                }
                
                # Create rows for original conditions and machine-readable conditions
                max_conditions = max(len(rule.conditions), len(rule.machine_readable_conditions), 1)
                
                for i in range(max_conditions):
                    condition_row = rule_info.copy()
                    
                    # Add original condition if exists
                    if i < len(rule.conditions):
                        orig_condition = rule.conditions[i]
                        condition_row.update({
                            'condition_text': orig_condition.condition_text,
                            'condition_logical_operator': orig_condition.logical_operator or "",
                            'condition_roles': ", ".join([r.value for r in orig_condition.roles]),
                            'condition_is_negation': orig_condition.is_negation
                        })
                    
                    # Add machine-readable condition if exists
                    if i < len(rule.machine_readable_conditions):
                        mr_condition = rule.machine_readable_conditions[i]
                        condition_row.update({
                            'mr_condition_id': mr_condition.condition_id,
                            'mr_fact': mr_condition.fact,
                            'mr_operator': mr_condition.operator.value,
                            'mr_value': str(mr_condition.value),
                            'mr_path': mr_condition.path or "",
                            'mr_logical_operator': mr_condition.logical_operator.value if mr_condition.logical_operator else "",
                            'mr_roles': ", ".join([r.value for r in mr_condition.roles]),
                            'mr_is_negation': mr_condition.is_negation,
                            'mr_variables': json.dumps(mr_condition.variables),
                            'mr_original_condition_text': mr_condition.original_condition_text or ""
                        })
                        
                        # Add corresponding machine-readable action if exists
                        corresponding_actions = [a for a in rule.machine_readable_actions 
                                               if mr_condition.condition_id in a.conditions]
                        if corresponding_actions:
                            mr_action = corresponding_actions[0]
                            condition_row.update({
                                'mr_action_id': mr_action.action_id,
                                'mr_action_type': mr_action.type.value,
                                'mr_action_params': json.dumps(mr_action.params),
                                'mr_action_target_roles': ", ".join([r.value for r in mr_action.target_roles]),
                                'mr_action_message': mr_action.message or "",
                                'mr_action_variables': json.dumps(mr_action.variables),
                                'mr_action_if_else_logic': json.dumps(mr_action.if_else_logic)
                            })
                    
                    writer.writerow(condition_row)
        
        print(f"✅ Enhanced conditions CSV saved to: {csv_path}")
    
    async def _generate_enhanced_json(self, rules: List[LegislationRule]):
        """Generate enhanced JSON with complete machine-readable format"""
        json_path = os.path.join(OUTPUT_PATH, "enhanced_rules.json")
        
        rules_dict = []
        for rule in rules:
            rule_dict = {
                "rule_id": rule.rule_id,
                "rule_text": rule.rule_text,
                "rule_definition": rule.rule_definition,
                "applies_to_countries": rule.applies_to_countries,
                "roles": [r.value for r in rule.roles],
                "data_categories": [str(dc) for dc in rule.data_categories],
                # Original conditions preserved
                "conditions": [
                    {
                        "condition_text": cond.condition_text,
                        "logical_operator": cond.logical_operator,
                        "roles": [r.value for r in cond.roles],
                        "is_negation": cond.is_negation
                    }
                    for cond in rule.conditions
                ],
                # Machine-readable conditions
                "machine_readable_conditions": [
                    {
                        "condition_id": cond.condition_id,
                        "fact": cond.fact,
                        "operator": cond.operator.value,
                        "value": cond.value,
                        "path": cond.path,
                        "logical_operator": cond.logical_operator.value if cond.logical_operator else None,
                        "roles": [r.value for r in cond.roles],
                        "is_negation": cond.is_negation,
                        "variables": cond.variables,
                        "nested_conditions": [],  # Simplified for this version
                        "original_condition_text": cond.original_condition_text
                    }
                    for cond in rule.machine_readable_conditions
                ],
                # Machine-readable actions
                "machine_readable_actions": [
                    {
                        "action_id": action.action_id,
                        "type": action.type.value,
                        "params": action.params,
                        "target_roles": [r.value for r in action.target_roles],
                        "conditions": action.conditions,
                        "message": action.message,
                        "metadata": action.metadata,
                        "variables": action.variables,
                        "if_else_logic": action.if_else_logic
                    }
                    for action in rule.machine_readable_actions
                ],
                "json_rules_engine_format": rule.json_rules_engine_format,
                "condition_count": rule.condition_count,
                "references": rule.references,
                "adequacy_countries": rule.adequacy_countries,
                "extraction_metadata": rule.extraction_metadata,
                "confidence_score": rule.confidence_score,
                "duplicate_of": rule.duplicate_of,
                "priority": rule.priority,
                "nested_rules": [],  # Will be populated if nested rules are implemented
                "parent_rule_id": rule.parent_rule_id,
                "variables": rule.variables,
                "if_else_logic": rule.if_else_logic
            }
            rules_dict.append(rule_dict)
        
        with open(json_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(rules_dict, jsonfile, indent=2, ensure_ascii=False)
        
        print(f"✅ Enhanced JSON output saved to: {json_path}")
    
    async def _generate_json_rules_engine_format(self, rules: List[LegislationRule]):
        """Generate pure JSON Rules Engine format"""
        json_rules_path = os.path.join(OUTPUT_PATH, "json_rules_engine_format.json")
        
        json_rules = []
        for rule in rules:
            if rule.json_rules_engine_format:
                # Add rule metadata to the JSON rules engine format
                enhanced_format = rule.json_rules_engine_format.copy()
                enhanced_format["rule_id"] = rule.rule_id
                enhanced_format["rule_text"] = rule.rule_text
                enhanced_format["applies_to_countries"] = rule.applies_to_countries
                enhanced_format["adequacy_countries"] = rule.adequacy_countries
                json_rules.append(enhanced_format)
        
        with open(json_rules_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(json_rules, jsonfile, indent=2, ensure_ascii=False)
        
        print(f"✅ JSON Rules Engine format saved to: {json_rules_path}")
    
    async def _generate_enhanced_report(self, rules: List[LegislationRule], state: AgentState):
        """Generate comprehensive enhanced processing report"""
        report_path = os.path.join(OUTPUT_PATH, "enhanced_processing_report.json")
        
        # Calculate enhanced statistics
        total_conditions = sum(rule.condition_count for rule in rules)
        total_mr_conditions = sum(len(rule.machine_readable_conditions) for rule in rules)
        total_mr_actions = sum(len(rule.machine_readable_actions) for rule in rules)
        
        roles_stats = {}
        data_categories_stats = {}
        operator_stats = {}
        action_type_stats = {}
        
        for rule in rules:
            # Role statistics
            for role in rule.roles:
                roles_stats[role.value] = roles_stats.get(role.value, 0) + 1
            
            # Data category statistics
            for category in rule.data_categories:
                cat_str = str(category)
                data_categories_stats[cat_str] = data_categories_stats.get(cat_str, 0) + 1
            
            # Operator statistics from machine-readable conditions
            for condition in rule.machine_readable_conditions:
                op_str = condition.operator.value
                operator_stats[op_str] = operator_stats.get(op_str, 0) + 1
            
            # Action type statistics from machine-readable actions
            for action in rule.machine_readable_actions:
                action_str = action.type.value
                action_type_stats[action_str] = action_type_stats.get(action_str, 0) + 1
        
        adequacy_countries_all = set()
        for rule in rules:
            adequacy_countries_all.update(rule.adequacy_countries)
        
        # Calculate JSON rules engine compatibility
        rules_with_json_format = len([r for r in rules if r.json_rules_engine_format])
        rules_with_mr_conditions = len([r for r in rules if r.machine_readable_conditions])
        rules_with_mr_actions = len([r for r in rules if r.machine_readable_actions])
        
        report = {
            "enhanced_processing_summary": {
                "original_rule_count": len(getattr(state, 'rules', [])),
                "deduplicated_rule_count": len(rules),
                "rules_removed": len(getattr(state, 'rules', [])) - len(rules),
                "total_original_conditions_extracted": total_conditions,
                "total_machine_readable_conditions": total_mr_conditions,
                "total_machine_readable_actions": total_mr_actions,
                "average_conditions_per_rule": total_conditions / len(rules) if rules else 0,
                "average_mr_conditions_per_rule": total_mr_conditions / len(rules) if rules else 0,
                "average_mr_actions_per_rule": total_mr_actions / len(rules) if rules else 0
            },
            "machine_readable_analysis": {
                "operator_distribution": operator_stats,
                "action_type_distribution": action_type_stats,
                "total_unique_operators": len(operator_stats),
                "total_unique_action_types": len(action_type_stats),
                "rules_with_machine_readable_conditions": rules_with_mr_conditions,
                "rules_with_machine_readable_actions": rules_with_mr_actions,
                "machine_readable_coverage_percentage": (rules_with_mr_conditions / len(rules) * 100) if rules else 0
            },
            "json_rules_engine_compatibility": {
                "rules_with_json_format": rules_with_json_format,
                "compatibility_percentage": (rules_with_json_format / len(rules) * 100) if rules else 0,
                "fully_compatible_rules": len([r for r in rules if r.json_rules_engine_format and r.machine_readable_conditions and r.machine_readable_actions])
            },
            "adequacy_analysis": {
                "adequacy_countries_identified": getattr(state, 'adequacy_countries', []),
                "adequacy_countries_in_rules": list(adequacy_countries_all),
                "total_unique_adequacy_countries": len(adequacy_countries_all)
            },
            "geographic_scope": {
                "jurisdiction_processed": getattr(state, 'current_jurisdiction', 'Unknown'),
                "geography_regions_available": list(getattr(state, 'geography_data', {}).keys()),
                "countries_covered": list(set([country for rule in rules for country in getattr(rule, 'applies_to_countries', [])]))
            },
            "role_analysis": {
                "roles_distribution": roles_stats,
                "total_role_assignments": sum(roles_stats.values())
            },
            "data_category_analysis": {
                "categories_distribution": data_categories_stats,
                "total_category_assignments": sum(data_categories_stats.values()),
                "unique_categories_identified": len(data_categories_stats)
            },
            "processing_quality": {
                "average_confidence_score": sum(getattr(rule, 'confidence_score', 0) for rule in rules) / len(rules) if rules else 0,
                "high_confidence_rules": len([rule for rule in rules if getattr(rule, 'confidence_score', 0) >= 0.8]),
                "low_confidence_rules": len([rule for rule in rules if getattr(rule, 'confidence_score', 0) < 0.6]),
                "average_priority": sum(getattr(rule, 'priority', 50) for rule in rules) / len(rules) if rules else 0
            },
            "react_reasoning_steps": len(getattr(state, 'react_reasoning', [])),
            "processing_errors": getattr(state, 'error_messages', []),
            "enhanced_files_generated": {
                "enhanced_conditions_csv": os.path.join(OUTPUT_PATH, "enhanced_rules_conditions.csv"),
                "enhanced_json": os.path.join(OUTPUT_PATH, "enhanced_rules.json"),
                "json_rules_engine_format": os.path.join(OUTPUT_PATH, "json_rules_engine_format.json"),
                "enhanced_report": report_path,
                "react_reasoning_log": os.path.join(OUTPUT_PATH, "react_reasoning_log.json")
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as reportfile:
            json.dump(report, reportfile, indent=2, ensure_ascii=False)
        
        print(f"✅ Enhanced comprehensive report saved to: {report_path}")

class SupervisorAgent:
    """Supervisor agent that orchestrates the multi-agent workflow - preserving all original functionality"""
    
    def __init__(self):
        self.agents = {
            "document_processor": ReactDocumentProcessorAgent(),
            "segmentation": ReactIntelligentSegmentationAgent(),
            "entity_extraction": ReactComprehensiveEntityExtractionAgent(),
            "rule_extraction": ReactIntelligentRuleComponentExtractionAgent(),
            "rule_deduplication": RuleDeduplicationAgent(),
            "output_generation": OutputGenerationAgent()
        }
        
        # Setup LangGraph workflow with proper typing
        self.workflow = StateGraph(AgentState)
        
        # Add nodes
        self.workflow.add_node("document_processor", self._document_processor_node)
        self.workflow.add_node("segmentation", self._segmentation_node)
        self.workflow.add_node("entity_extraction", self._entity_extraction_node)
        self.workflow.add_node("rule_extraction", self._rule_extraction_node)
        self.workflow.add_node("rule_deduplication", self._rule_deduplication_node)
        self.workflow.add_node("sanity_check", self._sanity_check_node)
        self.workflow.add_node("output_generation", self._output_generation_node)
        self.workflow.add_node("supervisor", self._supervisor_node)
        
        # Define edges
        self.workflow.add_edge(START, "supervisor")
        self.workflow.add_edge("document_processor", "supervisor")
        self.workflow.add_edge("segmentation", "supervisor")
        self.workflow.add_edge("entity_extraction", "supervisor")
        self.workflow.add_edge("rule_extraction", "supervisor")
        self.workflow.add_edge("rule_deduplication", "supervisor")
        self.workflow.add_edge("sanity_check", "supervisor")
        self.workflow.add_edge("output_generation", END)
        
        # Add conditional edges from supervisor
        self.workflow.add_conditional_edges(
            "supervisor",
            self._route_next,
            {
                "document_processor": "document_processor",
                "segmentation": "segmentation",
                "entity_extraction": "entity_extraction",
                "rule_extraction": "rule_extraction",
                "rule_deduplication": "rule_deduplication",
                "sanity_check": "sanity_check",
                "output_generation": "output_generation",
                "end": END
            }
        )
        
        # Setup memory
        self.memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.memory)
    
    async def _document_processor_node(self, state: AgentState) -> AgentState:
        """Document processor node wrapper"""
        return await self.agents["document_processor"].process(state)
    
    async def _segmentation_node(self, state: AgentState) -> AgentState:
        """Segmentation node wrapper"""
        return await self.agents["segmentation"].process(state)
    
    async def _entity_extraction_node(self, state: AgentState) -> AgentState:
        """Entity extraction node wrapper"""
        return await self.agents["entity_extraction"].process(state)
    
    async def _rule_extraction_node(self, state: AgentState) -> AgentState:
        """Rule extraction node wrapper"""
        return await self.agents["rule_extraction"].process(state)
    
    async def _rule_deduplication_node(self, state: AgentState) -> AgentState:
        """Rule deduplication node wrapper"""
        return await self.agents["rule_deduplication"].process(state)
    
    async def _sanity_check_node(self, state: AgentState) -> AgentState:
        """Sanity check node wrapper"""
        return await self._perform_sanity_check_async(state)
    
    async def _output_generation_node(self, state: AgentState) -> AgentState:
        """Output generation node wrapper"""
        return await self.agents["output_generation"].process(state)
    
    def _supervisor_node(self, state: AgentState) -> AgentState:
        """Supervisor node for workflow coordination"""
        logger.info(f"Supervisor: Current agent = {state.next_agent}")
        logger.info(f"Documents processed: {len(state.documents)}")
        logger.info(f"Segments created: {len(state.segmented_content)}")
        logger.info(f"Entities extracted: {len(state.extracted_entities)}")
        logger.info(f"Rules generated: {len(state.rules)}")
        logger.info(f"Rules deduplicated: {len(state.deduplicated_rules)}")
        logger.info(f"Adequacy countries: {state.adequacy_countries}")
        logger.info(f"React reasoning steps: {len(state.react_reasoning)}")
        
        if state.error_messages:
            logger.error(f"Errors encountered: {state.error_messages}")
        
        return state
    
    async def _perform_sanity_check_async(self, state: AgentState) -> AgentState:
        """Perform final sanity check on extracted rules using React reasoning"""
        logger.info("SupervisorAgent: Performing final sanity check")
        print("\n🔍 SupervisorAgent: Performing final sanity check...")
        
        final_rules = state.deduplicated_rules if state.deduplicated_rules else state.rules
        
        # THOUGHT: Plan sanity check approach
        thought = """
        THOUGHT: I need to perform a comprehensive final validation of all extracted rules.
        This will ensure legal accuracy, geographic precision, role consistency, logical coherence, and machine-readable format compatibility.
        """
        state.react_reasoning.append({"step": "sanity_check_thought", "content": thought})
        print("🤔 THOUGHT:", thought.strip())
        
        # ACTION: Perform comprehensive validation
        print("\n🎬 ACTION: Performing comprehensive rule validation...")
        validated_rules = await self._perform_react_sanity_check(final_rules, state)
        
        # Update state with validated rules
        if state.deduplicated_rules:
            state.deduplicated_rules = validated_rules
        else:
            state.rules = validated_rules
        
        # OBSERVATION: Sanity check results
        observation = f"""
        OBSERVATION: Sanity check completed successfully:
        - Validated {len(validated_rules)} rules for legal accuracy
        - Checked geographic precision and adequacy country alignment
        - Verified role assignments and condition logic
        - Ensured reference accuracy to actual articles
        - Validated machine-readable format compatibility
        - Rules are ready for final output generation
        """
        state.react_reasoning.append({"step": "sanity_check_observation", "content": observation})
        print("👁️ OBSERVATION:", observation.strip())
        
        state.next_agent = "output_generation"
        logger.info(f"SupervisorAgent: Sanity check completed. Validated {len(validated_rules)} rules")
        print("✅ SupervisorAgent sanity check completed successfully")
        
        return state
    
    async def _perform_react_sanity_check(self, rules: List[LegislationRule], state: AgentState) -> List[LegislationRule]:
        """Perform comprehensive sanity check using React reasoning"""
        
        # Create comprehensive validation prompt
        sanity_check_prompt = self._create_react_sanity_check_prompt(rules, state)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst performing final quality assurance using React reasoning."),
            HumanMessage(content=sanity_check_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        validation_result = response.choices[0].message.content
        
        # Parse validation recommendations and apply corrections
        corrected_rules = await self._apply_react_validation_corrections(rules, validation_result, state)
        
        return corrected_rules
    
    def _create_react_sanity_check_prompt(self, rules: List[LegislationRule], state: AgentState) -> str:
        """Create comprehensive React-style sanity check prompt"""
        
        rules_summary = []
        for i, rule in enumerate(rules[:5]):  # Limit to first 5 for prompt size
            rules_summary.append(f"""
            Rule {i+1}:
            ID: {rule.rule_id}
            Text: {rule.rule_text}
            Definition: {rule.rule_definition}
            Countries: {rule.applies_to_countries}
            Roles: {[r.value for r in rule.roles]}
            Conditions: {len(rule.conditions)} conditions
            Machine-readable Conditions: {len(rule.machine_readable_conditions)} MR conditions
            Machine-readable Actions: {len(rule.machine_readable_actions)} MR actions
            Adequacy Countries: {rule.adequacy_countries}
            References: {rule.references}
            Confidence: {rule.confidence_score}
            Priority: {rule.priority}
            """)
        
        return f"""
        Use React reasoning to perform comprehensive final validation of extracted legal rules with machine-readable format.
        
        THOUGHT: I need to systematically validate these rules for legal accuracy, precision, consistency, and machine-readable format compatibility.
        
        VALIDATION FRAMEWORK:
        
        ACTION: Apply systematic validation checks:
        
        1. LEGAL ACCURACY VALIDATION:
        THOUGHT: Do these rules accurately reflect data protection law principles?
        ACTION: Check if:
        - Rule texts clearly state obligations, rights, or prohibitions
        - Conditions are logically connected and non-contradictory
        - Logical operators (AND, OR, NOT) make sense in context
        - Negations are properly handled with is_negation flags
        OBSERVATION: Document any legal inconsistencies or ambiguities.
        
        2. MACHINE-READABLE FORMAT VALIDATION:
        THOUGHT: Are the machine-readable components properly structured for JSON rules engines?
        ACTION: Verify that:
        - Machine-readable conditions have valid fact-operator-value structure
        - Operators are from the supported set (equal, in, contains, etc.)
        - Facts use standard naming conventions (user.role, data.category, etc.)
        - Actions have proper type and parameter structure
        - JSON rules engine format is complete and valid
        OBSERVATION: Note any machine-readable format issues.
        
        3. GEOGRAPHIC PRECISION VALIDATION:
        THOUGHT: Are country codes and adequacy decisions accurate?
        ACTION: Verify that:
        - Country codes are valid ISO2 codes from available geography
        - Adequacy countries align with actual adequacy decisions mentioned
        - Regional applications (EU, EEA) are correctly specified
        - Cross-border transfer implications are accurate
        OBSERVATION: Note any geographic inaccuracies or missing adequacy context.
        
        4. ROLE ASSIGNMENT VERIFICATION:
        THOUGHT: Are role assignments consistent and complete?
        ACTION: Check that:
        - Controller/Processor/Joint Controller/Data Subject roles are correctly assigned
        - Role assignments match the obligations described in rules
        - Multiple role scenarios are properly handled
        - Condition-level role assignments are consistent with rule-level roles
        - Machine-readable conditions include appropriate role assignments
        OBSERVATION: Identify any role assignment errors or inconsistencies.
        
        5. REFERENCE ACCURACY ASSESSMENT:
        THOUGHT: Do legal references match actual content?
        ACTION: Verify that:
        - Article references correspond to articles actually present in source
        - No phantom article references (e.g., referencing articles 44-50 when only 44-46 present)
        - Legal citations are appropriate and correctly formatted
        - References support the extracted rules
        OBSERVATION: Flag any incorrect or phantom references.
        
        6. CONDITION LOGIC VALIDATION:
        THOUGHT: Are conditions properly structured and implementable?
        ACTION: Check that:
        - Original conditions are atomic and testable
        - Machine-readable conditions use valid operators and values
        - Logical operators create valid logical expressions
        - Negations are clearly marked and logically sound
        - Role assignments for conditions make legal sense
        - Variables are properly defined and used
        OBSERVATION: Note any logical inconsistencies or unclear conditions.
        
        7. ADEQUACY INTEGRATION VALIDATION:
        THOUGHT: Are adequacy decisions properly integrated?
        ACTION: Verify that:
        - Adequacy countries are mentioned in appropriate rule contexts
        - Cross-border transfer rules reference relevant adequacy decisions
        - Geographic scope aligns with adequacy status
        - Transfer mechanism rules consider adequacy implications
        - Machine-readable conditions properly handle adequacy country lists
        OBSERVATION: Document adequacy integration completeness.
        
        8. JSON RULES ENGINE COMPATIBILITY VALIDATION:
        THOUGHT: Are rules compatible with JSON rules engine specifications?
        ACTION: Check that:
        - JSON rules engine format follows proper structure (conditions, event, priority)
        - All required fields are present and properly formatted
        - Conditions use "all", "any", or "not" operators correctly
        - Events have proper type and parameters
        - Variables and nested logic are properly structured
        OBSERVATION: Document compatibility issues or missing elements.
        
        RULES TO VALIDATE ({len(rules)} total):
        {chr(10).join(rules_summary)}
        
        CONTEXT FOR VALIDATION:
        - Original jurisdiction: {state.current_jurisdiction}
        - Adequacy countries identified: {state.adequacy_countries}
        - Available geography: {list(state.geography_data.keys()) if state.geography_data else []}
        - Legislation content: {len(state.legislation_content)} characters
        - Supporting content: {len(state.supporting_content)} characters
        
        THOUGHT: Now I need to provide structured validation feedback.
        
        ACTION: For the rule set overall, provide validation assessment:
        
        VALIDATION STATUS: VALID / NEEDS_MINOR_CORRECTIONS / NEEDS_MAJOR_CORRECTIONS
        
        SPECIFIC ISSUES IDENTIFIED:
        - Legal accuracy issues: [list any problems]
        - Machine-readable format issues: [list any problems]
        - Geographic precision issues: [list any problems]
        - Role assignment issues: [list any problems]
        - Reference accuracy issues: [list any problems]
        - Condition logic issues: [list any problems]
        - Adequacy integration issues: [list any problems]
        - JSON rules engine compatibility issues: [list any problems]
        
        RECOMMENDED CORRECTIONS:
        - High priority corrections: [critical fixes needed]
        - Medium priority corrections: [improvements suggested]
        - Low priority corrections: [minor enhancements]
        
        CONFIDENCE ASSESSMENT:
        - Overall rule set quality: [score 0-1]
        - Machine-readable format completeness: [score 0-1]
        - JSON rules engine compatibility: [score 0-1]
        - Recommended confidence adjustments: [specific rules needing adjustment]
        
        COMPLETENESS EVALUATION:
        - Coverage of data transfer concepts: [assessment]
        - Coverage of access rights concepts: [assessment]
        - Coverage of entitlement concepts: [assessment]
        - Coverage of role obligations: [assessment]
        - Machine-readable format coverage: [assessment]
        
        OBSERVATION: Provide final assessment of rule set coherence, legal soundness, and machine-readable format completeness.
        
        Focus on ensuring rules are:
        - Legally accurate and implementable
        - Geographically precise with correct adequacy integration
        - Logically consistent with proper role assignments
        - Properly referenced to actual source articles
        - Complete coverage of key data protection concepts
        - Properly formatted for JSON rules engines
        - Compatible with machine-readable execution environments
        """
    
    async def _apply_react_validation_corrections(self, rules: List[LegislationRule], validation_result: str, state: AgentState) -> List[LegislationRule]:
        """Apply validation corrections using React reasoning"""
        
        # Check if validation suggests keeping rules as-is
        if "VALID" in validation_result.upper() and ("NO MAJOR CORRECTIONS" in validation_result.upper() or "NEEDS_MINOR_CORRECTIONS" not in validation_result.upper()):
            logger.info("Validation completed - rules are valid as-is")
            return rules
        
        # For comprehensive system, we'll return rules as-is with validation logged
        # In a production system, you could implement specific correction logic here
        logger.info("Validation completed with recommendations logged")
        return rules
    
    def _route_next(self, state: AgentState) -> str:
        """Route to the next agent based on current state"""
        return state.next_agent
    
    async def run(self) -> AgentState:
        """Run the complete multi-agent workflow"""
        logger.info("Starting enhanced React-based multi-agent legislation processing workflow")
        
        initial_state = AgentState()
        thread_config = {"configurable": {"thread_id": "legislation_processing"}}
        
        final_state = await self.app.ainvoke(initial_state, config=thread_config)
        
        # Ensure final_state has proper rule objects
        if hasattr(final_state, 'rules') and isinstance(final_state.rules, list):
            final_state.rules = self._ensure_rule_objects(final_state.rules)
        if hasattr(final_state, 'deduplicated_rules') and isinstance(final_state.deduplicated_rules, list):
            final_state.deduplicated_rules = self._ensure_rule_objects(final_state.deduplicated_rules)
        
        logger.info("Workflow completed successfully")
        logger.info(f"Original rules: {len(final_state.rules) if hasattr(final_state, 'rules') else 0}")
        logger.info(f"Final deduplicated rules: {len(final_state.deduplicated_rules) if hasattr(final_state, 'deduplicated_rules') else 0}")
        logger.info(f"Adequacy countries identified: {getattr(final_state, 'adequacy_countries', [])}")
        logger.info(f"React reasoning steps: {len(getattr(final_state, 'react_reasoning', []))}")
        
        if hasattr(final_state, 'error_messages') and final_state.error_messages:
            logger.warning(f"Workflow completed with errors: {final_state.error_messages}")
        
        return final_state
    
    def _ensure_rule_objects(self, rules_list: List) -> List[LegislationRule]:
        """Ensure all items in the list are proper LegislationRule objects"""
        validated_rules = []
        
        for i, rule in enumerate(rules_list):
            if isinstance(rule, LegislationRule):
                validated_rules.append(rule)
            elif isinstance(rule, dict):
                try:
                    # Convert dict to LegislationRule
                    conditions = []
                    for cond_data in rule.get("conditions", []):
                        if isinstance(cond_data, dict):
                            cond_roles = []
                            for role_str in cond_data.get("roles", []):
                                if role_str in [e.value for e in RoleType]:
                                    cond_roles.append(RoleType(role_str))
                            conditions.append(RuleCondition(
                                condition_text=cond_data.get("condition_text", ""),
                                logical_operator=cond_data.get("logical_operator"),
                                roles=cond_roles,
                                is_negation=cond_data.get("is_negation", False)
                            ))
                    
                    rule_roles = []
                    for role_str in rule.get("roles", []):
                        if role_str in [e.value for e in RoleType]:
                            rule_roles.append(RoleType(role_str))
                    
                    validated_rule = LegislationRule(
                        rule_id=rule.get("rule_id", f"rule_converted_{i}"),
                        rule_text=rule.get("rule_text", ""),
                        rule_definition=rule.get("rule_definition", ""),
                        applies_to_countries=rule.get("applies_to_countries", []),
                        roles=rule_roles,
                        data_categories=rule.get("data_categories", []),
                        conditions=conditions,
                        condition_count=len(conditions),
                        references=rule.get("references", []),
                        adequacy_countries=rule.get("adequacy_countries", []),
                        extraction_metadata=rule.get("extraction_metadata", {}),
                        confidence_score=float(rule.get("confidence_score", 0.8)),
                        duplicate_of=rule.get("duplicate_of")
                    )
                    validated_rules.append(validated_rule)
                except Exception as e:
                    logger.error(f"Failed to convert rule dict to LegislationRule: {e}")
                    continue
            else:
                logger.warning(f"Unexpected rule type: {type(rule)}, skipping")
                continue
        
        return validated_rules

async def main():
    """Main entry point for the legislation rule extraction system"""
    
    # Validate OpenAI client
    if not openai_client:
        logger.error("OpenAI client not initialized. Please check OPENAI_API_KEY and network connectivity.")
        print("❌ Error: OpenAI client not initialized. Please check:")
        print("  - OPENAI_API_KEY environment variable is set correctly")
        print("  - Network connectivity to OpenAI API")
        print("  - API key has sufficient permissions and credits")
        sys.exit(1)
    
    # Validate required files exist
    required_files = [LEGISLATION_METADATA_PATH, GEOGRAPHY_PATH]
    for file_path in required_files:
        if not os.path.exists(file_path):
            logger.error(f"Required file not found: {file_path}")
            print(f"❌ Error: Required file not found: {file_path}")
            
            # Create example files if they don't exist
            if file_path == LEGISLATION_METADATA_PATH:
                print(f"Creating example {file_path}...")
                example_metadata = [
                    {
                        "path": "./input_pdfs/example_regulation.pdf",
                        "jurisdiction": "EU"
                    }
                ]
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(example_metadata, f, indent=2)
                    print(f"✅ Created example {file_path}")
                except Exception as e:
                    print(f"❌ Failed to create {file_path}: {e}")
                    sys.exit(1)
            
            elif file_path == GEOGRAPHY_PATH:
                print(f"❌ Please ensure {file_path} exists with proper geography data")
                sys.exit(1)
    
    # Validate that input PDF directory exists
    if not os.path.exists(INPUT_PDF_PATH):
        logger.warning(f"Input PDF directory not found: {INPUT_PDF_PATH}")
        print(f"⚠️  Warning: Input PDF directory not found: {INPUT_PDF_PATH}")
        print("Creating input directory...")
        os.makedirs(INPUT_PDF_PATH, exist_ok=True)
        print(f"✅ Created {INPUT_PDF_PATH}")
    
    logger.info("Starting Enhanced React-Based Legislation Rule Extraction System")
    logger.info(f"Model: {MODEL_NAME}")
    logger.info(f"Embedding Model: {EMBEDDING_MODEL}")
    logger.info(f"Output Path: {OUTPUT_PATH}")
    
    print("\n🚀 Starting Enhanced React-Based Legislation Rule Extraction System")
    print(f"📄 Model: {MODEL_NAME}")
    print(f"🔍 Embedding Model: {EMBEDDING_MODEL}")
    print(f"📁 Output Path: {OUTPUT_PATH}")
    
    # Initialize and run supervisor
    try:
        supervisor = SupervisorAgent()
        
        final_state = await supervisor.run()
        
        print("\n" + "="*80)
        print("ENHANCED REACT-BASED LEGISLATION RULE EXTRACTION COMPLETED")
        print("="*80)
        print(f"Original rules extracted: {len(final_state.rules)}")
        print(f"Deduplicated rules: {len(final_state.deduplicated_rules)}")
        print(f"Duplicates removed: {len(final_state.rules) - len(final_state.deduplicated_rules)}")
        print(f"Documents processed: {len(final_state.documents)}")
        print(f"Entities resolved: {len(final_state.extracted_entities)}")
        print(f"Adequacy countries identified: {final_state.adequacy_countries}")
        print(f"React reasoning steps: {len(final_state.react_reasoning)}")
        print(f"Machine-readable conditions created: ✓")
        print(f"JSON Rules Engine format generated: ✓")
        print(f"Sanity check completed: ✓")
        print(f"Enhanced output files generated in: {OUTPUT_PATH}")
        
        if final_state.error_messages:
            print(f"\n⚠️  Errors encountered: {len(final_state.error_messages)}")
            for error in final_state.error_messages:
                print(f"  - {error}")
        
        # Print sample validated rules with enhanced information
        final_rules = []
        if hasattr(final_state, 'deduplicated_rules') and final_state.deduplicated_rules:
            final_rules = final_state.deduplicated_rules
        elif hasattr(final_state, 'rules') and final_state.rules:
            final_rules = final_state.rules
        
        if final_rules:
            print("\n📋 Sample validated rules with enhanced details:")
            for i, rule in enumerate(final_rules[:3]):
                if hasattr(rule, 'rule_id'):  # Ensure it's a proper rule object
                    print(f"\nRule {i+1}:")
                    print(f"  ID: {rule.rule_id}")
                    print(f"  Text: {rule.rule_text[:80]}...")
                    print(f"  Definition: {rule.rule_definition[:100]}...")
                    print(f"  Countries: {getattr(rule, 'applies_to_countries', [])}")
                    print(f"  Roles: {[r.value for r in getattr(rule, 'roles', [])]}")
                    print(f"  Data Categories: {getattr(rule, 'data_categories', [])}")
                    print(f"  Original Conditions: {getattr(rule, 'condition_count', 0)}")
                    print(f"  Machine-readable Conditions: {len(getattr(rule, 'machine_readable_conditions', []))}")
                    print(f"  Machine-readable Actions: {len(getattr(rule, 'machine_readable_actions', []))}")
                    print(f"  Adequacy Countries: {getattr(rule, 'adequacy_countries', [])}")
                    print(f"  References: {getattr(rule, 'references', [])}")
                    print(f"  Priority: {getattr(rule, 'priority', 50)}")
                    print(f"  Confidence: {getattr(rule, 'confidence_score', 0):.2f}")
                    if hasattr(rule, 'duplicate_of') and rule.duplicate_of:
                        print(f"  Duplicate of: {rule.duplicate_of}")
                    
                    # Show sample original condition details
                    if hasattr(rule, 'conditions') and rule.conditions:
                        print(f"  Sample Original Condition:")
                        cond = rule.conditions[0]
                        if hasattr(cond, 'condition_text'):
                            print(f"    Text: {cond.condition_text}")
                            print(f"    Operator: {getattr(cond, 'logical_operator', 'N/A')}")
                            print(f"    Roles: {[r.value for r in getattr(cond, 'roles', [])]}")
                            print(f"    Negation: {getattr(cond, 'is_negation', False)}")
                    
                    # Show sample machine-readable condition details
                    if hasattr(rule, 'machine_readable_conditions') and rule.machine_readable_conditions:
                        print(f"  Sample Machine-readable Condition:")
                        mr_cond = rule.machine_readable_conditions[0]
                        if hasattr(mr_cond, 'fact'):
                            print(f"    ID: {getattr(mr_cond, 'condition_id', 'N/A')}")
                            print(f"    Fact: {mr_cond.fact}")
                            print(f"    Operator: {mr_cond.operator.value}")
                            print(f"    Value: {mr_cond.value}")
                            print(f"    Roles: {[r.value for r in getattr(mr_cond, 'roles', [])]}")
                            print(f"    Variables: {getattr(mr_cond, 'variables', {})}")
                    
                    # Show sample machine-readable action details
                    if hasattr(rule, 'machine_readable_actions') and rule.machine_readable_actions:
                        print(f"  Sample Machine-readable Action:")
                        mr_action = rule.machine_readable_actions[0]
                        if hasattr(mr_action, 'type'):
                            print(f"    ID: {getattr(mr_action, 'action_id', 'N/A')}")
                            print(f"    Type: {mr_action.type.value}")
                            print(f"    Params: {getattr(mr_action, 'params', {})}")
                            print(f"    Target Roles: {[r.value for r in getattr(mr_action, 'target_roles', [])]}")
                            print(f"    Message: {getattr(mr_action, 'message', 'N/A')}")
                    
                    # Show JSON Rules Engine format sample
                    if hasattr(rule, 'json_rules_engine_format') and rule.json_rules_engine_format:
                        print(f"  JSON Rules Engine Format Available: ✓")
                        conditions_count = 0
                        if 'conditions' in rule.json_rules_engine_format:
                            conditions_data = rule.json_rules_engine_format['conditions']
                            conditions_count = len(conditions_data.get('all', [])) + len(conditions_data.get('any', [])) + (1 if 'not' in conditions_data else 0)
                        print(f"    Conditions: {conditions_count}")
                        print(f"    Event Type: {rule.json_rules_engine_format.get('event', {}).get('type', 'N/A')}")
                        print(f"    Priority: {rule.json_rules_engine_format.get('priority', 'N/A')}")
                else:
                    print(f"\n⚠️  Rule {i+1}: Invalid rule object type: {type(rule)}")
        else:
            print("\n⚠️  No rules were extracted. Check the error messages above.")
        
        # Print machine-readable format summary
        if final_rules:
            total_original_conditions = sum(getattr(rule, 'condition_count', 0) for rule in final_rules)
            total_mr_conditions = sum(len(getattr(rule, 'machine_readable_conditions', [])) for rule in final_rules)
            total_mr_actions = sum(len(getattr(rule, 'machine_readable_actions', [])) for rule in final_rules)
            rules_with_json_format = len([r for r in final_rules if getattr(r, 'json_rules_engine_format', {})])
            
            print(f"\n🤖 Machine-readable Format Summary:")
            print(f"  Total original conditions: {total_original_conditions}")
            print(f"  Total machine-readable conditions: {total_mr_conditions}")
            print(f"  Total machine-readable actions: {total_mr_actions}")
            print(f"  Rules with JSON Rules Engine format: {rules_with_json_format}/{len(final_rules)}")
            print(f"  JSON Rules Engine compatibility: {(rules_with_json_format/len(final_rules)*100):.1f}%")
        
        # Print React reasoning summary
        if hasattr(final_state, 'react_reasoning') and final_state.react_reasoning:
            print(f"\n🧠 React Reasoning Process Summary:")
            print(f"  Total reasoning steps: {len(final_state.react_reasoning)}")
            reasoning_types = {}
            for step in final_state.react_reasoning:
                step_type = step.get("step", "unknown").replace("_", " ").title()
                reasoning_types[step_type] = reasoning_types.get(step_type, 0) + 1
            
            for reasoning_type, count in reasoning_types.items():
                print(f"  {reasoning_type}: {count} steps")
        
        # Print adequacy countries summary
        if hasattr(final_state, 'adequacy_countries') and final_state.adequacy_countries:
            print(f"\n🌍 Adequacy Countries Analysis:")
            print(f"  Total adequacy countries identified: {len(final_state.adequacy_countries)}")
            print(f"  Countries: {final_state.adequacy_countries}")
            
            # Show which rules include adequacy countries
            rules_with_adequacy = 0
            for rule in final_rules:
                if hasattr(rule, 'adequacy_countries') and rule.adequacy_countries:
                    rules_with_adequacy += 1
            print(f"  Rules including adequacy countries: {rules_with_adequacy}/{len(final_rules)}")
        
        # Print file output summary
        print(f"\n📁 Enhanced Output Files Generated:")
        print(f"  📄 Enhanced Conditions CSV: enhanced_rules_conditions.csv")
        print(f"  📄 Enhanced JSON: enhanced_rules.json") 
        print(f"  📄 JSON Rules Engine Format: json_rules_engine_format.json")
        print(f"  📄 Enhanced Report: enhanced_processing_report.json")
        print(f"  📄 React Reasoning Log: react_reasoning_log.json")
        print(f"  📁 All files saved in: {OUTPUT_PATH}")
        
        print(f"\n✅ Enhanced React-based legislation rule extraction completed successfully!")
        print(f"🎯 Ready for JSON rules engine integration with machine-readable format!")
        
    except Exception as e:
        logger.error(f"System failed: {str(e)}")
        print(f"\n❌ System failed: {str(e)}")
        print("Check the logs for more details.")
        import traceback
        traceback.print_exc()
        
        # Try to provide more specific error information
        if "attribute" in str(e).lower() and "rules" in str(e).lower():
            print("\n🔍 Debugging Information:")
            print("This appears to be an object attribute error.")
            print("The system has implemented additional safeguards to handle this issue.")
            print("Please ensure all input files are properly formatted.")
        
        sys.exit(1)

async def test_system_components():
    """Test system components before running main workflow"""
    print("🧪 Testing enhanced system components...")
    
    # Test OpenAI client
    try:
        if openai_client:
            test_response = openai_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": "Test connection"}]
            )
            print("✅ OpenAI client connection successful")
        else:
            print("❌ OpenAI client not available")
            return False
    except Exception as e:
        print(f"❌ OpenAI client test failed: {e}")
        return False
    
    # Test embeddings
    try:
        embeddings = CustomEmbeddings()
        test_embedding = embeddings.embed_query("test query")
        if len(test_embedding) > 0:
            print("✅ Embeddings working correctly")
        else:
            print("❌ Embeddings test failed")
            return False
    except Exception as e:
        print(f"❌ Embeddings test failed: {e}")
        return False
    
    # Test geography manager
    try:
        processor = LegislationProcessor()
        geography_data = processor.load_geography_data()
        if geography_data:
            geo_manager = GeographyManager(geography_data)
            test_country = geo_manager.get_country_info("US")
            if test_country:
                print("✅ Geography manager working correctly")
            else:
                print("⚠️  Geography manager loaded but no US country found")
        else:
            print("❌ Geography data could not be loaded")
            return False
    except Exception as e:
        print(f"❌ Geography manager test failed: {e}")
        return False
    
    # Test enhanced Pydantic models
    try:
        # Test MachineReadableCondition
        test_condition = MachineReadableCondition(
            condition_id="test_cond_001",
            fact="user.role",
            operator=OperatorType.EQUAL,
            value="Controller",
            roles=[RoleType.CONTROLLER]
        )
        
        # Test MachineReadableAction
        test_action = MachineReadableAction(
            action_id="test_action_001",
            type=ActionType.REQUIRE,
            params={"field": "consent"},
            target_roles=[RoleType.CONTROLLER]
        )
        
        # Test LegislationRule with enhanced fields
        test_rule = LegislationRule(
            rule_id="test_rule_001",
            rule_text="Test rule text",
            rule_definition="Test rule definition",
            applies_to_countries=["US"],
            roles=[RoleType.CONTROLLER],
            data_categories=["Personal Data"],
            conditions=[RuleCondition(condition_text="Test condition", roles=[RoleType.CONTROLLER])],
            condition_count=1,
            machine_readable_conditions=[test_condition],
            machine_readable_actions=[test_action],
            references=["Article 6"],
            adequacy_countries=["US"],
            confidence_score=0.9,
            priority=75
        )
        
        print("✅ Enhanced Pydantic models working correctly")
    except Exception as e:
        print(f"❌ Enhanced Pydantic models test failed: {e}")
        return False
    
    return True

if __name__ == "__main__":
    async def run_with_tests():
        # Run component tests first
        if await test_system_components():
            print("✅ All enhanced system components tested successfully\n")
            await main()
        else:
            print("❌ Enhanced system component tests failed. Please check configuration.")
            sys.exit(1)
    
    asyncio.run(run_with_tests())
