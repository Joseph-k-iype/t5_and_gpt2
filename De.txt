"""
Application Settings - Configuration settings and utilities for the API.

This module provides functions to retrieve and manage application settings, 
including language model configuration, vector database selection, and other
global settings.
"""

import logging
import os
from typing import Optional, Union, Dict, Any, TYPE_CHECKING
from langchain_openai import AzureChatOpenAI
from azure.identity import get_bearer_token_provider
from app.config.environment import get_os_env
from utils.auth_helper import get_azure_token_cached

# TYPE_CHECKING is used to avoid circular imports during type checking
if TYPE_CHECKING:
    from app.core.vector_store import VectorStore

logger = logging.getLogger(__name__)

def get_vector_db_type() -> str:
    """
    Get the vector database type from environment settings.
    
    Returns:
        str: Vector database type (always "chroma" to avoid pgvector issues)
    """
    # Always return "chroma" regardless of environment settings
    # This ensures we always use ChromaDB and avoid proxy issues with pgvector
    return "chroma"

def get_vector_store(vector_db_type: Optional[str] = None) -> 'VectorStore':
    """
    Get the vector store implementation based on settings.
    
    Args:
        vector_db_type: Override the vector database type from environment
    
    Returns:
        VectorStore: The vector store implementation (always ChromaDB)
    """
    from app.core.vector_store import VectorStore
    
    # Always use ChromaDB regardless of input parameter
    from app.core.vector_store_chroma import ChromaDBVectorStore
    persist_dir = os.environ.get("CHROMA_PERSIST_DIR", "./data/chroma_db")
    collection_name = os.environ.get("CHROMA_COLLECTION", "business_terms")
    logger.info(f"Using ChromaDB as vector store with collection '{collection_name}' in '{persist_dir}'")
    return ChromaDBVectorStore(collection_name=collection_name, persist_dir=persist_dir)

def get_llm(proxy_enabled: Optional[bool] = None) -> AzureChatOpenAI:
    """
    Get the language model for the application with cached token authentication.
    
    Args:
        proxy_enabled: Override the PROXY_ENABLED setting in the config file
    
    Returns:
        AzureChatOpenAI: The language model
    """
    # Get environment with proxy setting override if provided
    env = get_os_env(proxy_enabled=proxy_enabled)
    
    logger.info(f"Setting up Azure OpenAI client with proxy enabled: {env.get('PROXY_ENABLED', 'True')}")
    
    try:
        # Get tenant, client and secret info for token acquisition
        tenant_id = env.get("AZURE_TENANT_ID", "")
        client_id = env.get("AZURE_CLIENT_ID", "")
        client_secret = env.get("AZURE_CLIENT_SECRET", "")
        
        # Get cached token (or new token if not in cache)
        token = get_azure_token_cached(
            tenant_id=tenant_id,
            client_id=client_id, 
            client_secret=client_secret,
            scope="https://cognitiveservices.azure.com/.default"
        )
        
        if token:
            logger.info("Successfully obtained Azure token (using cache when available)")
            # Create a simple callable token provider
            token_provider = lambda: token
        else:
            logger.error("Failed to obtain Azure token")
            return MockLLM()
            
    except Exception as e:
        logger.error(f"Token acquisition failed: {e}")
        return MockLLM()
    
    # Get model configuration
    model_name = env.get("MODEL_NAME", "gpt-4o")
    temperature = float(env.get("TEMPERATURE", "0.3"))  # Lower temperature for more deterministic outputs
    max_tokens = int(env.get("MAX_TOKENS", "2000"))
    api_version = env.get("API_VERSION", "2023-05-15")
    azure_endpoint = env.get("AZURE_ENDPOINT", "")
    
    logger.info(f"Using model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")
    
    # Create and return the LLM
    return AzureChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        azure_ad_token_provider=token_provider
    )

# Mock LLM class for fallback when authentication fails
class MockLLM:
    """Mock LLM that logs errors instead of making API calls."""
    
    def __init__(self):
        self.error_message = "Azure authentication failed. Using mock LLM."
        logger.error(self.error_message)
    
    def invoke(self, *args, **kwargs):
        logger.error(f"Cannot invoke LLM: {self.error_message}")
        return "Azure authentication failed. Please check your proxy settings and credentials."
    
    async def ainvoke(self, *args, **kwargs):
        logger.error(f"Cannot invoke LLM asynchronously: {self.error_message}")
        return "Azure authentication failed. Please check your proxy settings and credentials."

def get_app_settings() -> Dict[str, Any]:
    """
    Get all application settings.
    
    Returns:
        Dict[str, Any]: Dictionary containing all application settings
    """
    env = get_os_env()
    
    # Vector database settings - always use ChromaDB
    vector_db_type = "chroma"  # Force ChromaDB
    vector_db_settings = {
        "type": vector_db_type,
        "persist_dir": env.get("CHROMA_PERSIST_DIR", "./data/chroma_db"),
        "collection": env.get("CHROMA_COLLECTION", "business_terms")
    }
    
    # PostgreSQL settings
    pg_settings = {
        "host": env.get("PG_HOST", "localhost"),
        "port": int(env.get("PG_PORT", "5432")),
        "database": env.get("PG_DB", "metadata_db"),
        "user": env.get("PG_USER", "postgres"),
        "db_schema": env.get("PG_SCHEMA", "ai_stitching_platform"),  # Changed from 'schema' to 'db_schema'
        "min_connections": int(env.get("PG_MIN_CONNECTIONS", "2")),
        "max_connections": int(env.get("PG_MAX_CONNECTIONS", "10"))
    }
    
    # OpenAI model settings
    model_settings = {
        "model": env.get("MODEL_NAME", "gpt-4o"),
        "temperature": float(env.get("TEMPERATURE", "0.3")),
        "max_tokens": int(env.get("MAX_TOKENS", "2000")),
        "api_version": env.get("API_VERSION", "2023-05-15"),
        "azure_endpoint": env.get("AZURE_ENDPOINT", "")
    }
    
    # Proxy settings - default to enabled
    proxy_settings = {
        "enabled": env.get("PROXY_ENABLED", "True").lower() in ("true", "t", "1", "yes", "y"),
        "domain": env.get("HTTPS_PROXY_DOMAIN", "")
    }
    
    # General settings
    general_settings = {
        "similarity_threshold": float(env.get("SIMILARITY_THRESHOLD", "0.5")),
        "monitoring_interval": int(env.get("MONITORING_INTERVAL", "300")),
        "secured_endpoints": env.get("SECURED_ENDPOINTS", "False").lower() in ("true", "t", "1", "yes", "y")
    }
    
    # Token caching settings
    token_settings = {
        "token_caching_enabled": True,
        "token_refresh_interval": int(env.get("TOKEN_REFRESH_INTERVAL", "300")),
        "token_validation_threshold": int(env.get("TOKEN_VALIDATION_THRESHOLD", "600"))
    }
    
    return {
        "version": "1.0.0",
        "vector_database": vector_db_settings,
        "postgresql": pg_settings,
        "model": model_settings,
        "proxy": proxy_settings,
        "general": general_settings,
        "token": token_settings
    }

def is_token_caching_enabled() -> bool:
    """
    Check if token caching is enabled.
    
    Returns:
        bool: True if token caching is enabled
    """
    env = get_os_env()
    return env.get("TOKEN_CACHING_ENABLED", "True").lower() in ("true", "t", "1", "yes", "y")

def get_token_refresh_interval() -> int:
    """
    Get the token refresh interval in seconds.
    
    Returns:
        int: Token refresh interval in seconds
    """
    env = get_os_env()
    return int(env.get("TOKEN_REFRESH_INTERVAL", "300"))

def get_token_validation_threshold() -> int:
    """
    Get the token validation threshold in seconds.
    
    This is the minimum remaining time a token must have before it's refreshed.
    
    Returns:
        int: Token validation threshold in seconds
    """
    env = get_os_env()
    return int(env.get("TOKEN_VALIDATION_THRESHOLD", "600"))
