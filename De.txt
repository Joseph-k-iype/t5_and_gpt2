"""
Enhanced Legal Text to Machine-Readable Rules Multi-Agent System
Converts legislation PDFs into formal ontologies and decision tables using LangGraph and LangChain
Uses latest PyMuPDF, advanced NLP features, and comprehensive multi-agent architecture
"""

import json
import os
import sqlite3
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Set
from datetime import datetime
import logging
import asyncio
import re
import enum
import sys

# Import Literal with better compatibility handling
if sys.version_info >= (3, 8):
    try:
        from typing import Literal
    except ImportError:
        from typing_extensions import Literal
else:
    from typing_extensions import Literal

# Core dependencies
from pydantic import BaseModel, Field, field_validator, model_validator
from openai import OpenAI
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# LangGraph and LangChain for advanced workflows
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Elasticsearch for vector storage
from elasticsearch import Elasticsearch
import ssl

# Latest PyMuPDF import
import pymupdf

# Ontology and RDF
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, XSD
import owlready2 as owl

# Graph processing for advanced analysis
from collections import defaultdict
import pickle
from concurrent.futures import ThreadPoolExecutor
import aiofiles

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
MODEL_NAME = "o3-mini-2025-01-31"
EMBEDDING_MODEL = "text-embedding-3-large"

# Elasticsearch Configuration
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost:9200")
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "your-elasticsearch-username")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "/path/to/elasticsearch.crt")
ELASTICSEARCH_INDEX = os.getenv("ELASTICSEARCH_INDEX", "legal_rules_index")

# Directory Configuration
PDF_DIRECTORY = Path("./legislation_pdfs")
METADATA_FILE = Path("./legislation_metadata.json")
OUTPUT_DIRECTORY = Path("./output")
ONTOLOGY_OUTPUT = Path("./output/ontologies")
DECISION_TABLES_OUTPUT = Path("./output/decision_tables")

# Setup directories
for directory in [OUTPUT_DIRECTORY, ONTOLOGY_OUTPUT, DECISION_TABLES_OUTPUT]:
    directory.mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client
openai_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# Initialize Elasticsearch client
def create_elasticsearch_client():
    """Create Elasticsearch client with authentication"""
    try:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        if os.path.exists(ELASTICSEARCH_CERT_PATH):
            context.load_verify_locations(ELASTICSEARCH_CERT_PATH)
            context.verify_mode = ssl.CERT_REQUIRED
        
        es_client = Elasticsearch(
            [f"https://{ELASTICSEARCH_HOST}"],
            basic_auth=(ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD),
            ssl_context=context,
            verify_certs=True if os.path.exists(ELASTICSEARCH_CERT_PATH) else False
        )
        
        # Test connection
        if es_client.ping():
            logger.info("Elasticsearch connection established successfully")
            return es_client
        else:
            logger.error("Failed to connect to Elasticsearch")
            return None
    except Exception as e:
        logger.error(f"Elasticsearch connection error: {e}")
        return None

# Global Elasticsearch client
ES_CLIENT = create_elasticsearch_client()

# ============================================================================
# ENHANCED PYDANTIC DATA MODELS
# ============================================================================

class LegalAuthorityLevel(str, enum.Enum):
    CONSTITUTIONAL = "constitutional"
    STATUTORY = "statutory"
    REGULATORY = "regulatory"
    ADMINISTRATIVE = "administrative"
    CASE_LAW = "case_law"

class JurisdictionScope(str, enum.Enum):
    NATIONAL = "national"
    REGIONAL = "regional"
    INTERNATIONAL = "international"
    SECTOR_SPECIFIC = "sector_specific"

class EntityType(str, enum.Enum):
    CONTROLLER = "Controller"
    PROCESSOR = "Processor"
    JOINT_CONTROLLER = "JointController"
    DATA_SUBJECT = "DataSubject"
    THIRD_COUNTRY = "ThirdCountry"
    SUPERVISING_AUTHORITY = "SupervisingAuthority"
    DATA_PROTECTION_OFFICER = "DataProtectionOfficer"
    REPRESENTATIVE = "Representative"

class ConceptType(str, enum.Enum):
    DATA_TRANSFER = "DataTransfer"
    DATA_ACCESS = "DataAccess"
    DATA_ENTITLEMENT = "DataEntitlement"
    PROCESSING = "Processing"
    COLLECTION = "Collection"
    STORAGE = "Storage"
    ERASURE = "Erasure"
    RECTIFICATION = "Rectification"
    PORTABILITY = "Portability"
    PROFILING = "Profiling"
    AUTOMATED_DECISION_MAKING = "AutomatedDecisionMaking"

class RuleComponentType(str, enum.Enum):
    RESTRICTION = "Restriction"
    CONDITION = "Condition"
    OBLIGATION = "Obligation"
    RIGHT = "Right"
    PROHIBITION = "Prohibition"
    PERMISSION = "Permission"
    EXCEPTION = "Exception"
    SAFEGUARD = "Safeguard"

class LogicalOperator(str, enum.Enum):
    AND = "AND"
    OR = "OR"
    NOT = "NOT"
    IF_THEN = "IF_THEN"
    IF_AND_ONLY_IF = "IF_AND_ONLY_IF"

class DeonticType(str, enum.Enum):
    OBLIGATORY = "obligatory"
    PERMISSIBLE = "permissible"
    FORBIDDEN = "forbidden"
    OPTIONAL = "optional"

class LegalEntity(BaseModel):
    """Enhanced legal entity with detailed classification"""
    name: str
    type: EntityType
    description: str
    context: str
    attributes: Dict[str, Any] = Field(default_factory=dict)
    relationships: List[str] = Field(default_factory=list)
    confidence: float = Field(ge=0.0, le=1.0)

class LegalConcept(BaseModel):
    """Enhanced legal concept with semantic relationships"""
    name: str
    type: ConceptType
    description: str
    context: str
    preconditions: List[str] = Field(default_factory=list)
    consequences: List[str] = Field(default_factory=list)
    semantic_relationships: Dict[str, List[str]] = Field(default_factory=dict)
    confidence: float = Field(ge=0.0, le=1.0)

class RuleComponent(BaseModel):
    """Enhanced rule component with logical relationships"""
    name: str
    type: RuleComponentType
    description: str
    applies_to: List[str]
    legal_basis: str
    enforcement_mechanism: str = ""
    penalty: str = ""
    exceptions: List[str] = Field(default_factory=list)
    logical_operator: LogicalOperator = LogicalOperator.AND
    confidence: float = Field(ge=0.0, le=1.0)

class LegalCitation(BaseModel):
    """Enhanced citation with hierarchical structure"""
    document_id: str
    article: Optional[str] = None
    section: Optional[str] = None
    subsection: Optional[str] = None
    paragraph: Optional[str] = None
    subparagraph: Optional[str] = None
    authority_level: LegalAuthorityLevel = LegalAuthorityLevel.STATUTORY
    jurisdiction: JurisdictionScope = JurisdictionScope.NATIONAL

class EnhancedAtomicRule(BaseModel):
    """Comprehensive atomic rule with full legal context"""
    id: str
    text: str
    entities: List[LegalEntity]
    concepts: List[LegalConcept]
    rule_components: List[RuleComponent]
    semantic_roles: Dict[str, str]
    source_document: str
    citation: LegalCitation
    confidence: float
    
    # Enhanced legal analysis
    legal_authority_level: LegalAuthorityLevel
    jurisdictional_scope: JurisdictionScope
    precedence_weight: float = 1.0
    conflicts_with: List[str] = Field(default_factory=list)
    supports: List[str] = Field(default_factory=list)
    exceptions: List[str] = Field(default_factory=list)
    
    # Logical structure
    deontic_type: DeonticType
    modal_operator: Optional[str] = None
    logical_structure: Dict[str, Any] = Field(default_factory=dict)
    
    # NLP analysis
    sentiment_score: float = 0.0
    complexity_score: float = 0.0
    entities_mentioned: List[str] = Field(default_factory=list)
    key_phrases: List[str] = Field(default_factory=list)
    
    # Reference metadata for traceability
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ProcessingState(BaseModel):
    """Enhanced state with comprehensive tracking"""
    document_path: str
    metadata: Dict[str, Any]
    raw_text: str = ""
    structured_text: Dict[str, Any] = Field(default_factory=dict)
    clauses: List[str] = Field(default_factory=list)
    enhanced_atomic_rules: List[EnhancedAtomicRule] = Field(default_factory=list)
    ontology_triples: List[Dict[str, str]] = Field(default_factory=list)
    decision_rules: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Processing tracking
    current_agent: str = "document_processor"
    processing_steps: List[str] = Field(default_factory=list)
    error_messages: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    quality_metrics: Dict[str, float] = Field(default_factory=dict)
    
    # NLP analysis results
    embeddings_cache: Dict[str, List[float]] = Field(default_factory=dict)
    similarity_matrix: List[List[float]] = Field(default_factory=list)
    semantic_clusters: List[List[str]] = Field(default_factory=list)
    entity_relationships: Dict[str, List[str]] = Field(default_factory=dict)
    entity_relationship_graph: Dict[str, Any] = Field(default_factory=dict)
    concept_hierarchy: Dict[str, Any] = Field(default_factory=dict)
    
    # Analysis results
    conflict_analysis: Dict[str, Any] = Field(default_factory=dict)
    compliance_validation: Dict[str, Any] = Field(default_factory=dict)
    
    # Final simplified output
    final_rules_output: List[Dict[str, Any]] = Field(default_factory=list)
    
    @model_validator(mode='before')
    @classmethod
    def validate_relationships(cls, values):
        """Ensure entity_relationships is properly formatted"""
        if isinstance(values, dict) and 'entity_relationships' in values:
            relationships = values['entity_relationships']
            if isinstance(relationships, dict):
                # Clean up the relationships to ensure proper format
                cleaned = {}
                for key, value in relationships.items():
                    if isinstance(key, str) and isinstance(value, list):
                        cleaned[key] = [str(v) for v in value if v is not None]
                    elif isinstance(key, str) and not isinstance(value, list):
                        cleaned[key] = [str(value)] if value is not None else []
                values['entity_relationships'] = cleaned
        return values

# ============================================================================
# UTILITY FUNCTIONS WITH OPENAI INTEGRATION
# ============================================================================

async def get_openai_completion(prompt: str, system_message: str = None) -> str:
    """Enhanced OpenAI completion with error handling"""
    try:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        return f"Error: {str(e)}"

async def get_embedding(text: str) -> List[float]:
    """Get embedding using text-embedding-3-large with caching"""
    try:
        response = openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        logger.error(f"Embedding error: {e}")
        return []

def extract_pdf_text_advanced(pdf_path: Path) -> Dict[str, Any]:
    """Advanced PDF text extraction using latest PyMuPDF with structure preservation"""
    try:
        doc = pymupdf.open(pdf_path)
        extracted_data = {
            "raw_text": "",
            "structured_content": [],
            "metadata": {},
            "page_count": len(doc)
        }
        
        for page_num, page in enumerate(doc):
            page_data = {
                "page_number": page_num + 1,
                "text": page.get_text(),
                "blocks": [],
                "tables": [],
                "images": []
            }
            
            # Extract structured content blocks
            blocks = page.get_text("dict")
            for block in blocks.get("blocks", []):
                if "lines" in block:
                    block_text = ""
                    for line in block["lines"]:
                        for span in line.get("spans", []):
                            block_text += span.get("text", "")
                    
                    if block_text.strip():
                        page_data["blocks"].append({
                            "text": block_text.strip(),
                            "bbox": block.get("bbox", []),
                            "font_info": line.get("spans", [{}])[0] if line.get("spans") else {}
                        })
            
            # Extract tables if available
            try:
                tables = page.find_tables()
                for table in tables:
                    page_data["tables"].append({
                        "data": table.extract(),
                        "bbox": table.bbox
                    })
            except:
                pass  # Tables might not be available
            
            extracted_data["raw_text"] += page_data["text"] + "\n"
            extracted_data["structured_content"].append(page_data)
        
        # Extract document metadata
        extracted_data["metadata"] = doc.metadata or {}
        doc.close()
        
        return extracted_data
        
    except Exception as e:
        logger.error(f"PDF extraction error for {pdf_path}: {e}")
        return {"raw_text": "", "structured_content": [], "metadata": {}, "page_count": 0}

def load_metadata() -> Dict[str, Any]:
    """Load legislation metadata from JSON file"""
    try:
        with open(METADATA_FILE, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Metadata loading error: {e}")
        return {}

# ============================================================================
# ENHANCED REACT AGENT BASE CLASS
# ============================================================================

class EnhancedReactAgent:
    """Enhanced ReAct agent with direct OpenAI integration and advanced NLP"""
    
    def __init__(self, name: str, role: str, tools: List[str] = None):
        self.name = name
        self.role = role
        self.tools = tools or []
        self.memory = []
        
        # Initialize text processing components
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
    async def think(self, observation: str, task: str, context: str = "") -> str:
        """Enhanced chain of thought reasoning"""
        thinking_prompt = f"""
        You are {self.role}, an expert agent in a multi-agent legal document processing system.
        
        TASK: {task}
        OBSERVATION: {observation}
        CONTEXT: {context}
        
        Think step by step about this task using chain of thought reasoning:
        1. What do I need to understand from this observation?
        2. What are the key legal elements relevant to my specialized role?
        3. What actions should I take to complete this task effectively?
        4. How does this relate to other components in the legal analysis pipeline?
        5. What potential issues or edge cases should I consider?
        
        Provide your detailed reasoning and analysis:
        """
        
        try:
            thought = await get_openai_completion(
                thinking_prompt,
                f"You are an expert legal analyst specializing in {self.role}."
            )
            
            # Store in memory for context
            self.memory.append({
                "type": "thinking",
                "content": thought,
                "timestamp": datetime.now().isoformat()
            })
            
            return thought
            
        except Exception as e:
            logger.error(f"Thinking error for {self.name}: {e}")
            return f"Unable to process reasoning: {str(e)}"
    
    async def act(self, thought: str, task: str, data: Any) -> Any:
        """Enhanced action execution - to be implemented by specific agents"""
        raise NotImplementedError("Each agent must implement its own act method")
    
    async def store_embedding_in_elasticsearch(self, text: str, doc_id: str, metadata: Dict[str, Any] = None):
        """Store text embedding in Elasticsearch"""
        if not ES_CLIENT:
            logger.warning("Elasticsearch client not available")
            return False
        
        try:
            # Get embedding
            embedding = await get_embedding(text)
            if not embedding:
                return False
            
            # Prepare document
            doc = {
                "text": text,
                "embedding": embedding,
                "metadata": metadata or {},
                "timestamp": datetime.now().isoformat()
            }
            
            # Index document
            response = ES_CLIENT.index(
                index=ELASTICSEARCH_INDEX,
                id=doc_id,
                body=doc
            )
            
            return response["result"] in ["created", "updated"]
            
        except Exception as e:
            logger.error(f"Elasticsearch storage error: {e}")
            return False
    
    async def search_similar_embeddings(self, text: str, size: int = 10) -> List[Dict[str, Any]]:
        """Search for similar embeddings in Elasticsearch"""
        if not ES_CLIENT:
            logger.warning("Elasticsearch client not available")
            return []
        
        try:
            # Get embedding for search query
            query_embedding = await get_embedding(text)
            if not query_embedding:
                return []
            
            # Prepare search query
            search_body = {
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                            "params": {"query_vector": query_embedding}
                        }
                    }
                },
                "size": size
            }
            
            # Execute search
            response = ES_CLIENT.search(
                index=ELASTICSEARCH_INDEX,
                body=search_body
            )
            
            results = []
            for hit in response["hits"]["hits"]:
                results.append({
                    "id": hit["_id"],
                    "score": hit["_score"],
                    "text": hit["_source"]["text"],
                    "metadata": hit["_source"].get("metadata", {})
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Elasticsearch search error: {e}")
            return []

# ============================================================================
# ENHANCED SPECIALIZED AGENTS
# ============================================================================

class AdvancedDocumentProcessorAgent(EnhancedReactAgent):
    """Enhanced document processor with advanced text analysis"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Document Processor",
            role="advanced document text extraction and preprocessing specialist",
            tools=["PyMuPDF", "LangChain TextSplitter", "structure detection", "metadata extraction"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced document processing with structure preservation"""
        logger.info(f"Advanced Document Processor: Processing {state.document_path}")
        
        try:
            # Extract text with structure preservation
            pdf_data = extract_pdf_text_advanced(Path(state.document_path))
            
            if not pdf_data["raw_text"]:
                state.error_messages.append("Failed to extract text from PDF")
                return state
            
            # Store raw and structured data
            state.raw_text = pdf_data["raw_text"]
            state.structured_text = pdf_data
            
            # Perform advanced text preprocessing
            preprocessing_result = await self._advanced_preprocessing(pdf_data["raw_text"])
            state.structured_text["preprocessed"] = preprocessing_result
            
            # Extract document structure using LLM
            structure_analysis = await self._analyze_document_structure(pdf_data["raw_text"])
            state.structured_text["structure_analysis"] = structure_analysis
            
            state.processing_steps.append("Advanced document processing completed")
            state.current_agent = "intelligent_segmentation"
            
            logger.info("Advanced Document Processor: Processing completed successfully")
            return state
            
        except Exception as e:
            error_msg = f"Document processing error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _advanced_preprocessing(self, text: str) -> Dict[str, Any]:
        """Advanced text preprocessing with LLM assistance"""
        
        preprocessing_prompt = f"""
        You are an expert in legal document preprocessing. Clean and enhance this legal text:
        
        TEXT TO PREPROCESS:
        {text[:3000]}...
        
        Perform these preprocessing tasks:
        1. Remove artifacts (page numbers, headers, footers, watermarks)
        2. Fix formatting issues (line breaks, spacing, hyphenation)
        3. Standardize legal references (expand abbreviations like "Art." to "Article")
        4. Identify and preserve document structure markers
        5. Correct obvious OCR errors
        6. Standardize terminology (no abbreviations in key legal terms)
        
        Return JSON with:
        {{
            "cleaned_text": "preprocessed text with improvements",
            "removed_artifacts": ["list of removed elements"],
            "expanded_abbreviations": {{"Art.": "Article", "Sec.": "Section"}},
            "structure_markers": ["Chapter 1", "Article 5", "Section 2.1"],
            "corrections_made": ["list of corrections"]
        }}
        """
        
        response = await get_openai_completion(
            preprocessing_prompt,
            "You are an expert legal document preprocessing specialist with knowledge of legal formatting and terminology standards."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error("Failed to parse preprocessing response")
            return {"cleaned_text": text, "error": "Preprocessing parsing failed"}
    
    async def _analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """Analyze document structure using LLM"""
        
        structure_prompt = f"""
        Analyze the structure of this legal document and identify its hierarchical organization:
        
        DOCUMENT TEXT:
        {text[:4000]}...
        
        Identify and map:
        1. Document title and type
        2. Major sections (Chapters, Parts, Titles)
        3. Articles and their numbering
        4. Sections and subsections
        5. Paragraphs and subparagraphs
        6. Cross-references between sections
        7. Definitions sections
        8. Appendices or annexes
        
        Return JSON with hierarchical structure:
        {{
            "document_type": "regulation|directive|law|statute",
            "title": "document title",
            "hierarchy": {{
                "chapters": [
                    {{
                        "number": "1",
                        "title": "General Provisions",
                        "articles": [
                            {{
                                "number": "1",
                                "title": "Article title",
                                "sections": ["section content"]
                            }}
                        ]
                    }}
                ]
            }},
            "cross_references": ["Article 5 references Article 3"],
            "definitions": {{"term": "definition"}}
        }}
        """
        
        response = await get_openai_completion(
            structure_prompt,
            "You are an expert in legal document structure analysis with deep knowledge of legislative drafting conventions."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error("Failed to parse structure analysis response")
            return {"error": "Structure analysis parsing failed"}

class IntelligentSegmentationAgent(EnhancedReactAgent):
    """Enhanced segmentation agent with advanced clause identification"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Segmentation Agent",
            role="advanced legal text segmentation and atomic rule extraction specialist",
            tools=["semantic segmentation", "logical decomposition", "legal clause analysis", "dependency parsing"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced segmentation with semantic analysis"""
        logger.info("Intelligent Segmentation: Creating atomic legal statements")
        
        try:
            # Get preprocessed text
            text_to_analyze = state.structured_text.get("preprocessed", {}).get("cleaned_text", state.raw_text)
            
            # Perform intelligent segmentation
            segmentation_result = await self._intelligent_segmentation(text_to_analyze, state.structured_text.get("structure_analysis", {}))
            
            # Extract atomic rules with enhanced analysis
            atomic_clauses = await self._extract_atomic_clauses(segmentation_result)
            
            # Perform semantic role labeling
            enhanced_clauses = await self._semantic_role_labeling(atomic_clauses)
            
            # Store results
            state.clauses = [clause["text"] for clause in enhanced_clauses]
            state.metadata["detailed_clauses"] = enhanced_clauses
            state.metadata["segmentation_analysis"] = segmentation_result
            
            state.processing_steps.append("Intelligent segmentation completed")
            state.current_agent = "comprehensive_entity_extraction"
            
            logger.info(f"Intelligent Segmentation: Identified {len(state.clauses)} atomic statements")
            return state
            
        except Exception as e:
            error_msg = f"Segmentation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _intelligent_segmentation(self, text: str, structure_info: Dict[str, Any]) -> Dict[str, Any]:
        """Intelligent text segmentation using document structure"""
        
        segmentation_prompt = f"""
        You are an expert in legal text segmentation. Break this legal text into the smallest possible logical statements while preserving legal meaning.
        
        TEXT TO SEGMENT:
        {text[:5000]}...
        
        DOCUMENT STRUCTURE CONTEXT:
        {json.dumps(structure_info, indent=2)}
        
        Segmentation guidelines:
        1. Each atomic statement should contain exactly one legal rule, obligation, right, or prohibition
        2. Preserve the logical completeness of each statement
        3. Maintain references to the source article/section
        4. Handle compound sentences by breaking them at logical conjunctions
        5. Preserve conditional statements (if-then relationships)
        6. Keep exception clauses with their main rules when they form a logical unit
        7. Extract page numbers and section titles where available
        
        Focus on statements related to:
        - Data protection and privacy rights
        - Data transfer, access, and entitlements
        - Obligations of controllers and processors
        - Rights of data subjects
        - Conditions and restrictions on data processing
        - Penalties and enforcement mechanisms
        
        Return JSON:
        {{
            "atomic_statements": [
                {{
                    "id": "stmt_001",
                    "text": "complete atomic legal statement",
                    "source_reference": "Article 5, Section 1",
                    "page_number": 15,
                    "section_title": "Principles relating to processing of personal data",
                    "document_title": "General Data Protection Regulation",
                    "statement_type": "obligation|right|prohibition|permission|condition",
                    "logical_structure": "simple|conditional|compound|exception",
                    "dependencies": ["other statement IDs this depends on"],
                    "legal_significance": "high|medium|low"
                }}
            ],
            "segmentation_summary": {{
                "total_statements": 0,
                "statement_types": {{"obligation": 0, "right": 0}},
                "average_statement_length": 0,
                "complex_statements": 0
            }}
        }}
        """
        
        response = await get_openai_completion(
            segmentation_prompt,
            "You are a legal text segmentation expert specializing in data protection law and atomic rule extraction."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error("Failed to parse segmentation response")
            return {"atomic_statements": [], "error": "Segmentation parsing failed"}
    
    async def _extract_atomic_clauses(self, segmentation_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract and enhance atomic clauses"""
        atomic_statements = segmentation_data.get("atomic_statements", [])
        
        enhanced_clauses = []
        for stmt in atomic_statements:
            # Perform additional analysis on each statement
            analysis = await self._analyze_clause_complexity(stmt["text"])
            
            enhanced_clause = {
                **stmt,
                "complexity_analysis": analysis,
                "word_count": len(stmt["text"].split()),
                "character_count": len(stmt["text"]),
                "sentence_count": len([s for s in stmt["text"].split('.') if s.strip()])
            }
            
            enhanced_clauses.append(enhanced_clause)
        
        return enhanced_clauses
    
    async def _analyze_clause_complexity(self, clause_text: str) -> Dict[str, Any]:
        """Analyze the complexity of a legal clause"""
        
        complexity_prompt = f"""
        Analyze the complexity and legal significance of this legal clause:
        
        CLAUSE: {clause_text}
        
        Assess:
        1. Syntactic complexity (sentence structure, nested clauses)
        2. Legal complexity (number of legal concepts, conditions, exceptions)
        3. Semantic ambiguity (potential for multiple interpretations)
        4. Implementation difficulty (how hard to operationalize)
        5. Enforcement clarity (how clear the enforcement mechanism is)
        
        Return JSON:
        {{
            "complexity_score": 0.8,
            "syntactic_complexity": "high|medium|low",
            "legal_complexity": "high|medium|low",
            "semantic_ambiguity": "high|medium|low",
            "implementation_difficulty": "high|medium|low",
            "enforcement_clarity": "clear|moderate|unclear",
            "key_challenges": ["list of implementation challenges"],
            "clarification_needed": ["areas needing clarification"]
        }}
        """
        
        response = await get_openai_completion(
            complexity_prompt,
            "You are a legal complexity analysis expert with experience in regulatory implementation."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"complexity_score": 0.5, "error": "Complexity analysis failed"}
    
    async def _semantic_role_labeling(self, clauses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Enhanced semantic role labeling"""
        
        enhanced_clauses = []
        for clause in clauses:
            role_analysis = await self._analyze_semantic_roles(clause["text"])
            clause["semantic_roles"] = role_analysis
            enhanced_clauses.append(clause)
        
        return enhanced_clauses
    
    async def _analyze_semantic_roles(self, text: str) -> Dict[str, str]:
        """Analyze semantic roles in legal text"""
        
        role_prompt = f"""
        Identify semantic roles in this legal statement:
        
        STATEMENT: {text}
        
        Identify these semantic roles:
        - AGENT: Who/what performs the action
        - ACTION: The main legal action or requirement
        - PATIENT: Who/what is affected by the action
        - CONDITION: Under what circumstances this applies
        - EXCEPTION: Any exceptions to the rule
        - MANNER: How the action should be performed
        - PURPOSE: Why the action is required
        - CONSEQUENCE: What happens if rule is followed/violated
        
        Return JSON with role assignments:
        {{
            "agent": "identified agent",
            "action": "main action",
            "patient": "affected party",
            "condition": "conditions",
            "exception": "exceptions",
            "manner": "how to perform",
            "purpose": "why required",
            "consequence": "outcomes"
        }}
        """
        
        response = await get_openai_completion(
            role_prompt,
            "You are an expert in semantic role labeling for legal texts with deep understanding of legal argument structure."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"error": "Semantic role analysis failed"}

class ComprehensiveEntityExtractionAgent(EnhancedReactAgent):
    """Enhanced entity extraction with advanced NLP"""
    
    def __init__(self):
        super().__init__(
            name="Comprehensive Entity Extraction Agent",
            role="advanced legal entity identification and relationship mapping specialist",
            tools=["NER", "entity linking", "relationship extraction", "coreference resolution"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Comprehensive entity extraction with relationship mapping"""
        logger.info("Comprehensive Entity Extraction: Identifying legal entities and relationships")
        
        try:
            # Extract entities from all clauses
            all_entities = []
            entity_relationships = {}
            
            clauses_to_process = state.metadata.get("detailed_clauses", [])
            
            for clause in clauses_to_process[:15]:  # Process in batches for efficiency
                entities = await self._extract_entities_from_clause(clause)
                relationships = await self._extract_entity_relationships(clause, entities)
                
                all_entities.extend(entities)
                entity_relationships.update(relationships)
            
            # Deduplicate and enhance entities
            unique_entities = await self._deduplicate_entities(all_entities)
            enhanced_entities = await self._enhance_entities(unique_entities)
            
            # Map entity relationships
            relationship_graph = await self._build_entity_relationship_graph(enhanced_entities, entity_relationships)
            
            # Clean entity_relationships to ensure it matches expected format
            cleaned_relationships = {}
            for key, value in entity_relationships.items():
                if isinstance(key, str) and isinstance(value, list):
                    # Ensure all values in the list are strings
                    cleaned_value = [str(v) for v in value if v is not None]
                    cleaned_relationships[key] = cleaned_value
            
            # Store results
            state.metadata["extracted_entities"] = enhanced_entities
            state.entity_relationships = cleaned_relationships  # Simple dict format
            state.entity_relationship_graph = relationship_graph  # Complex graph structure
            
            state.processing_steps.append("Comprehensive entity extraction completed")
            state.current_agent = "advanced_concept_extraction"
            
            logger.info(f"Comprehensive Entity Extraction: Found {len(enhanced_entities)} unique entities")
            return state
            
        except Exception as e:
            error_msg = f"Entity extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_entities_from_clause(self, clause: Dict[str, Any]) -> List[LegalEntity]:
        """Extract entities from a single clause with enhanced analysis"""
        
        entity_prompt = f"""
        You are an expert in legal entity extraction. Identify all legal entities in this clause with precise classification.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        
        Identify entities according to these enhanced categories:
        
        DATA PROTECTION ROLES:
        - Controller: Entity determining purposes and means of processing
        - Processor: Entity processing data on behalf of controller
        - JointController: Multiple controllers jointly determining purposes/means
        - DataSubject: Individual whose personal data is processed
        - DataProtectionOfficer: Designated data protection officer
        - Representative: Representative of controller/processor not established in Union
        
        JURISDICTIONAL ENTITIES:
        - ThirdCountry: Country outside the jurisdiction
        - SupervisingAuthority: Regulatory authority overseeing compliance
        
        For each entity found, provide:
        1. Exact textual mention
        2. Precise classification from above categories
        3. Detailed role description
        4. Context and surrounding text
        5. Confidence assessment
        6. Attributes (size, sector, location if mentioned)
        
        Return JSON:
        {{
            "entities": [
                {{
                    "name": "exact entity mention",
                    "type": "Controller",
                    "description": "detailed role and function description",
                    "context": "surrounding text context",
                    "attributes": {{"sector": "healthcare", "size": "large"}},
                    "relationships": ["related to other entity"],
                    "confidence": 0.95
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            entity_prompt,
            "You are a legal entity extraction expert with comprehensive knowledge of data protection law terminology and organizational structures."
        )
        
        try:
            entity_data = json.loads(response)
            entities = []
            for entity_dict in entity_data.get("entities", []):
                try:
                    # Ensure type is valid
                    entity_type = entity_dict.get("type", "Controller")
                    if hasattr(EntityType, entity_type.upper().replace(" ", "_")):
                        entity_dict["type"] = getattr(EntityType, entity_type.upper().replace(" ", "_"))
                    else:
                        entity_dict["type"] = EntityType.CONTROLLER  # Default fallback
                    
                    entity = LegalEntity(**entity_dict)
                    entities.append(entity)
                except Exception as e:
                    logger.warning(f"Failed to create entity: {e}")
                    continue
            return entities
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Entity extraction parsing error: {e}")
            return []
    
    async def _extract_entity_relationships(self, clause: Dict[str, Any], entities: List[LegalEntity]) -> Dict[str, List[str]]:
        """Extract relationships between entities"""
        
        if len(entities) < 2:
            return {}
        
        relationship_prompt = f"""
        Identify relationships between these entities in the given legal context:
        
        CLAUSE: {clause.get("text", "")}
        ENTITIES: {[f"{e.name} ({e.type})" for e in entities]}
        
        Identify these relationship types:
        - CONTROLS: One entity controls another
        - PROCESSES_FOR: One entity processes data for another
        - SUPERVISES: One entity supervises another
        - REPRESENTS: One entity represents another
        - CONTRACTS_WITH: Entities have contractual relationship
        - TRANSFERS_TO: Data is transferred from one to another
        - REPORTS_TO: One entity reports to another
        - COORDINATES_WITH: Entities coordinate activities
        
        Return JSON:
        {{
            "relationships": [
                {{
                    "subject": "entity name",
                    "relationship": "CONTROLS|PROCESSES_FOR|etc.",
                    "object": "related entity name",
                    "description": "description of relationship",
                    "strength": "strong|moderate|weak"
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            relationship_prompt,
            "You are an expert in legal entity relationship analysis with deep understanding of data protection organizational structures."
        )
        
        try:
            rel_data = json.loads(response)
            relationships = {}
            for rel in rel_data.get("relationships", []):
                try:
                    subject = str(rel.get("subject", "")).strip()
                    relationship = str(rel.get("relationship", "")).strip()
                    obj = str(rel.get("object", "")).strip()
                    
                    if subject and relationship and obj:
                        if subject not in relationships:
                            relationships[subject] = []
                        relationships[subject].append(f"{relationship} -> {obj}")
                except Exception as e:
                    logger.warning(f"Failed to process relationship: {e}")
                    continue
            return relationships
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Relationship extraction parsing error: {e}")
            return {}
    
    async def _deduplicate_entities(self, entities: List[LegalEntity]) -> List[LegalEntity]:
        """Deduplicate entities using semantic similarity"""
        
        if not entities:
            return []
        
        # Group entities by type first
        type_groups = defaultdict(list)
        for entity in entities:
            type_groups[entity.type].append(entity)
        
        unique_entities = []
        
        for entity_type, type_entities in type_groups.items():
            if len(type_entities) == 1:
                unique_entities.extend(type_entities)
                continue
            
            # Use embeddings to find duplicates
            entity_texts = [f"{e.name} {e.description}" for e in type_entities]
            embeddings = []
            
            for text in entity_texts:
                embedding = await get_embedding(text)
                if embedding:
                    embeddings.append(embedding)
                else:
                    embeddings.append([0.0] * 1536)  # Fallback
            
            if embeddings:
                similarity_matrix = cosine_similarity(embeddings)
                
                # Identify duplicates (similarity > 0.8)
                duplicates = set()
                for i in range(len(similarity_matrix)):
                    for j in range(i + 1, len(similarity_matrix)):
                        if similarity_matrix[i][j] > 0.8:
                            duplicates.add(j)
                
                # Keep only non-duplicate entities
                for i, entity in enumerate(type_entities):
                    if i not in duplicates:
                        unique_entities.append(entity)
            else:
                unique_entities.extend(type_entities)
        
        return unique_entities
    
    async def _enhance_entities(self, entities: List[LegalEntity]) -> List[LegalEntity]:
        """Enhance entities with additional analysis"""
        
        enhanced_entities = []
        
        for entity in entities:
            # Enhance with additional context analysis
            enhancement = await self._analyze_entity_context(entity)
            
            enhanced_entity = LegalEntity(
                name=entity.name,
                type=entity.type,
                description=entity.description,
                context=entity.context,
                attributes={**entity.attributes, **enhancement.get("attributes", {})},
                relationships=entity.relationships + enhancement.get("relationships", []),
                confidence=min(entity.confidence, enhancement.get("confidence", 1.0))
            )
            
            enhanced_entities.append(enhanced_entity)
        
        return enhanced_entities
    
    async def _analyze_entity_context(self, entity: LegalEntity) -> Dict[str, Any]:
        """Analyze entity context for enhancement"""
        
        context_prompt = f"""
        Analyze this legal entity for additional contextual information:
        
        ENTITY: {entity.name} ({entity.type})
        DESCRIPTION: {entity.description}
        CONTEXT: {entity.context}
        
        Identify:
        1. Sector or industry (if applicable)
        2. Jurisdiction or location references
        3. Size indicators (large, small, individual)
        4. Legal status (public, private, non-profit)
        5. Functional role beyond type classification
        6. Regulatory obligations specific to this entity
        7. Risk level (high, medium, low) for data protection
        
        Return JSON:
        {{
            "attributes": {{
                "sector": "healthcare|finance|tech|public|other",
                "jurisdiction": "EU|US|global|national",
                "size": "large|medium|small|individual",
                "legal_status": "public|private|non-profit",
                "functional_role": "specific function description",
                "risk_level": "high|medium|low"
            }},
            "relationships": ["additional relationship indicators"],
            "confidence": 0.9
        }}
        """
        
        response = await get_openai_completion(
            context_prompt,
            "You are an expert in legal entity context analysis with knowledge of organizational structures and regulatory frameworks."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"attributes": {}, "relationships": [], "confidence": 1.0}
    
    async def _build_entity_relationship_graph(self, entities: List[LegalEntity], relationships: Dict[str, List[str]]) -> Dict[str, Any]:
        """Build entity relationship graph"""
        
        graph = {
            "nodes": [],
            "edges": [],
            "clusters": {},
            "central_entities": []
        }
        
        # Add nodes
        for entity in entities:
            graph["nodes"].append({
                "id": entity.name,
                "type": entity.type,
                "attributes": entity.attributes,
                "description": entity.description
            })
        
        # Add edges from relationships
        for subject, relations in relationships.items():
            for relation in relations:
                if " -> " in relation:
                    rel_type, target = relation.split(" -> ", 1)
                    graph["edges"].append({
                        "source": subject,
                        "target": target,
                        "relationship": rel_type
                    })
        
        # Identify central entities (those with most connections)
        connection_counts = defaultdict(int)
        for edge in graph["edges"]:
            connection_counts[edge["source"]] += 1
            connection_counts[edge["target"]] += 1
        
        if connection_counts:
            sorted_entities = sorted(connection_counts.items(), key=lambda x: x[1], reverse=True)
            graph["central_entities"] = [entity for entity, count in sorted_entities[:5]]
        
        return graph

class AdvancedConceptExtractionAgent(EnhancedReactAgent):
    """Enhanced concept extraction with semantic relationships"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Concept Extraction Agent",
            role="comprehensive legal concept identification and semantic relationship mapping specialist",
            tools=["concept taxonomy", "semantic analysis", "legal ontology", "relationship mapping"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Advanced concept extraction with hierarchical analysis"""
        logger.info("Advanced Concept Extraction: Identifying legal concepts and building hierarchies")
        
        try:
            # Extract concepts from clauses
            all_concepts = []
            concept_relationships = {}
            
            clauses_to_process = state.metadata.get("detailed_clauses", [])
            
            for clause in clauses_to_process[:15]:
                concepts = await self._extract_concepts_from_clause(clause)
                relationships = await self._extract_concept_relationships(clause, concepts)
                
                all_concepts.extend(concepts)
                concept_relationships.update(relationships)
            
            # Build concept hierarchy
            concept_hierarchy = await self._build_concept_hierarchy(all_concepts, concept_relationships)
            
            # Perform semantic clustering
            concept_clusters = await self._cluster_concepts_semantically(all_concepts)
            
            # Store results
            state.metadata["extracted_concepts"] = all_concepts
            state.concept_hierarchy = concept_hierarchy
            state.semantic_clusters = concept_clusters
            
            state.processing_steps.append("Advanced concept extraction completed")
            state.current_agent = "intelligent_rule_component_extraction"
            
            logger.info(f"Advanced Concept Extraction: Found {len(all_concepts)} concepts in {len(concept_clusters)} clusters")
            return state
            
        except Exception as e:
            error_msg = f"Concept extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_concepts_from_clause(self, clause: Dict[str, Any]) -> List[LegalConcept]:
        """Extract concepts with enhanced classification"""
        
        concept_prompt = f"""
        You are an expert in legal concept extraction. Identify all data protection and privacy-related concepts in this clause.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        
        Identify concepts in these enhanced categories:
        
        DATA OPERATIONS:
        - DataTransfer: Moving data between entities/jurisdictions
        - DataAccess: Accessing or retrieving personal data
        - DataEntitlement: Rights to access or use data
        - Processing: Any operation on personal data (broad category)
        - Collection: Gathering personal data from data subjects
        - Storage: Retaining personal data
        - Erasure: Deleting personal data (right to be forgotten)
        - Rectification: Correcting inaccurate personal data
        - Portability: Moving data between controllers
        - Profiling: Automated analysis of personal aspects
        - AutomatedDecisionMaking: Automated decisions affecting individuals
        
        For each concept, provide:
        1. Precise classification from above categories
        2. Detailed description of how it applies
        3. Preconditions for the concept to apply
        4. Consequences or outcomes
        5. Semantic relationships to other concepts
        
        Return JSON:
        {{
            "concepts": [
                {{
                    "name": "specific concept name",
                    "type": "DataTransfer",
                    "description": "detailed description of application",
                    "context": "surrounding legal context",
                    "preconditions": ["conditions that must be met"],
                    "consequences": ["outcomes or results"],
                    "semantic_relationships": {{
                        "requires": ["concepts this requires"],
                        "enables": ["concepts this enables"],
                        "conflicts_with": ["conflicting concepts"]
                    }},
                    "confidence": 0.9
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            concept_prompt,
            "You are a legal concept extraction expert with comprehensive knowledge of data protection operations, legal bases, and safeguarding measures."
        )
        
        try:
            concept_data = json.loads(response)
            concepts = []
            for concept_dict in concept_data.get("concepts", []):
                try:
                    # Ensure type is valid
                    concept_type = concept_dict.get("type", "Processing")
                    if hasattr(ConceptType, concept_type.upper().replace(" ", "_")):
                        concept_dict["type"] = getattr(ConceptType, concept_type.upper().replace(" ", "_"))
                    else:
                        concept_dict["type"] = ConceptType.PROCESSING  # Default fallback
                    
                    concept = LegalConcept(**concept_dict)
                    concepts.append(concept)
                except Exception as e:
                    logger.warning(f"Failed to create concept: {e}")
                    continue
            return concepts
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Concept extraction parsing error: {e}")
            return []
    
    async def _extract_concept_relationships(self, clause: Dict[str, Any], concepts: List[LegalConcept]) -> Dict[str, List[str]]:
        """Extract relationships between concepts"""
        
        if len(concepts) < 2:
            return {}
        
        relationship_prompt = f"""
        Identify semantic and logical relationships between these legal concepts:
        
        CLAUSE: {clause.get("text", "")}
        CONCEPTS: {[f"{c.name} ({c.type})" for c in concepts]}
        
        Identify these relationship types:
        - PREREQUISITE: One concept is required before another
        - ENABLES: One concept makes another possible
        - CONFLICTS_WITH: Concepts are mutually exclusive
        - PART_OF: One concept is part of another
        - IMPLEMENTS: One concept implements another
        - OVERRIDES: One concept takes precedence over another
        - COMPLEMENTS: Concepts work together
        - SPECIALIZES: One concept is a specific case of another
        
        Return JSON:
        {{
            "relationships": [
                {{
                    "subject": "concept name",
                    "relationship": "PREREQUISITE|ENABLES|etc.",
                    "object": "related concept name",
                    "description": "explanation of relationship",
                    "strength": "strong|moderate|weak"
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            relationship_prompt,
            "You are an expert in legal concept relationship analysis with deep understanding of data protection frameworks and logical dependencies."
        )
        
        try:
            rel_data = json.loads(response)
            relationships = {}
            for rel in rel_data.get("relationships", []):
                try:
                    subject = str(rel.get("subject", "")).strip()
                    relationship = str(rel.get("relationship", "")).strip()
                    obj = str(rel.get("object", "")).strip()
                    
                    if subject and relationship and obj:
                        if subject not in relationships:
                            relationships[subject] = []
                        relationships[subject].append(f"{relationship} -> {obj}")
                except Exception as e:
                    logger.warning(f"Failed to process concept relationship: {e}")
                    continue
            return relationships
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"Concept relationship extraction parsing error: {e}")
            return {}
    
    async def _build_concept_hierarchy(self, concepts: List[LegalConcept], relationships: Dict[str, List[str]]) -> Dict[str, Any]:
        """Build hierarchical concept structure"""
        
        hierarchy = {
            "root_concepts": [],
            "hierarchy_levels": {},
            "concept_tree": {},
            "dependencies": relationships
        }
        
        # Group concepts by type to create initial hierarchy
        type_groups = defaultdict(list)
        for concept in concepts:
            type_groups[concept.type].append(concept.name)
        
        # Create hierarchy based on logical dependencies
        all_concept_names = {c.name for c in concepts}
        concepts_with_prerequisites = set()
        
        for concept_name, relations in relationships.items():
            for relation in relations:
                if "PREREQUISITE" in relation or "PART_OF" in relation:
                    concepts_with_prerequisites.add(concept_name)
        
        # Root concepts are those without prerequisites
        hierarchy["root_concepts"] = [c.name for c in concepts if c.name not in concepts_with_prerequisites]
        
        # Build concept tree structure
        for concept_type, concept_names in type_groups.items():
            hierarchy["concept_tree"][concept_type] = concept_names
        
        return hierarchy
    
    async def _cluster_concepts_semantically(self, concepts: List[LegalConcept]) -> List[List[str]]:
        """Cluster concepts based on semantic similarity"""
        
        if len(concepts) < 2:
            return [[c.name for c in concepts]]
        
        # Get embeddings for concepts
        concept_texts = [f"{c.name} {c.description}" for c in concepts]
        embeddings = []
        
        for text in concept_texts:
            embedding = await get_embedding(text)
            if embedding:
                embeddings.append(embedding)
            else:
                embeddings.append([0.0] * 1536)  # Fallback
        
        if not embeddings:
            return [[c.name for c in concepts]]
        
        # Perform clustering using cosine similarity
        similarity_matrix = cosine_similarity(embeddings)
        
        # Simple clustering: group concepts with similarity > 0.7
        clusters = []
        used_concepts = set()
        
        for i, concept in enumerate(concepts):
            if concept.name in used_concepts:
                continue
            
            cluster = [concept.name]
            used_concepts.add(concept.name)
            
            for j, other_concept in enumerate(concepts):
                if i != j and other_concept.name not in used_concepts:
                    if similarity_matrix[i][j] > 0.7:
                        cluster.append(other_concept.name)
                        used_concepts.add(other_concept.name)
            
            clusters.append(cluster)
        
        return clusters

class IntelligentRuleComponentExtractionAgent(EnhancedReactAgent):
    """Enhanced rule component extraction with logical analysis"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Rule Component Extraction Agent",
            role="comprehensive legal rule component identification and logical structure analysis specialist",
            tools=["deontic logic", "rule decomposition", "logical operators", "enforcement analysis"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Enhanced rule component extraction with logical structure analysis"""
        logger.info("Intelligent Rule Component Extraction: Analyzing logical rule structures")
        
        try:
            # Extract rule components from clauses
            all_rule_components = []
            logical_structures = {}
            
            clauses_to_process = state.metadata.get("detailed_clauses", [])
            entities = state.metadata.get("extracted_entities", [])
            concepts = state.metadata.get("extracted_concepts", [])
            
            for clause in clauses_to_process[:15]:
                components = await self._extract_rule_components_from_clause(clause, entities, concepts)
                logical_structure = await self._analyze_logical_structure(clause, components)
                
                all_rule_components.extend(components)
                logical_structures[clause.get("id", "")] = logical_structure
            
            # Analyze enforcement mechanisms
            enforcement_analysis = await self._analyze_enforcement_mechanisms(all_rule_components)
            
            # Create enhanced atomic rules
            enhanced_atomic_rules = await self._create_enhanced_atomic_rules(
                clauses_to_process, entities, concepts, all_rule_components, logical_structures
            )
            
            # Store results
            state.metadata["rule_components"] = all_rule_components
            state.metadata["logical_structures"] = logical_structures
            state.metadata["enforcement_analysis"] = enforcement_analysis
            state.enhanced_atomic_rules = enhanced_atomic_rules
            
            state.processing_steps.append("Intelligent rule component extraction completed")
            state.current_agent = "semantic_coherence_analysis"
            
            logger.info(f"Intelligent Rule Component Extraction: Found {len(all_rule_components)} rule components")
            return state
            
        except Exception as e:
            error_msg = f"Rule component extraction error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _extract_rule_components_from_clause(self, clause: Dict[str, Any], entities: List[LegalEntity], concepts: List[LegalConcept]) -> List[RuleComponent]:
        """Extract rule components with enhanced classification"""
        
        rule_component_prompt = f"""
        You are an expert in legal rule analysis and deontic logic. Extract all rule components from this legal clause.
        
        CLAUSE: {clause.get("text", "")}
        CONTEXT: {clause.get("source_reference", "")}
        AVAILABLE ENTITIES: {[f"{e.name} ({e.type})" for e in entities[:10]]}
        AVAILABLE CONCEPTS: {[f"{c.name} ({c.type})" for c in concepts[:10]]}
        
        Identify rule components in these enhanced categories:
        
        DEONTIC COMPONENTS:
        - Restriction: Limitations on actions or behaviors
        - Condition: Circumstances that must be met
        - Obligation: Required actions or duties (MUST/SHALL)
        - Right: Entitlements or permissions of data subjects
        - Prohibition: Forbidden actions (MUST NOT/SHALL NOT)
        - Permission: Explicitly allowed actions (MAY)
        - Exception: Specific exceptions to general rules
        - Safeguard: Protective measures that must be implemented
        
        For each component:
        1. Identify the precise component type
        2. Describe what it requires/restricts/permits
        3. Specify which entities it applies to
        4. Identify the legal basis/source
        5. Determine enforcement mechanisms
        6. Identify logical operators (AND, OR, NOT, IF-THEN)
        7. Assess penalties for non-compliance
        
        Return JSON:
        {{
            "rule_components": [
                {{
                    "name": "descriptive component name",
                    "type": "Restriction",
                    "description": "what it requires/restricts/permits",
                    "applies_to": ["entity names from available entities"],
                    "legal_basis": "specific legal source/article",
                    "enforcement_mechanism": "how it is enforced",
                    "penalty": "consequences for violation",
                    "exceptions": ["specific exceptions"],
                    "logical_operator": "AND",
                    "confidence": 0.85
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            rule_component_prompt,
            "You are a legal rule analysis expert specializing in deontic logic, regulatory compliance, and enforcement mechanisms."
        )
        
        try:
            component_data = json.loads(response)
            components = []
            for comp_dict in component_data.get("rule_components", []):
                try:
                    # Ensure type is valid
                    comp_type = comp_dict.get("type", "Condition")
                    if hasattr(RuleComponentType, comp_type.upper()):
                        comp_dict["type"] = getattr(RuleComponentType, comp_type.upper())
                    else:
                        comp_dict["type"] = RuleComponentType.CONDITION  # Default fallback
                    
                    # Ensure logical_operator is valid
                    logical_op = comp_dict.get("logical_operator", "AND")
                    if hasattr(LogicalOperator, logical_op.upper()):
                        comp_dict["logical_operator"] = getattr(LogicalOperator, logical_op.upper())
                    else:
                        comp_dict["logical_operator"] = LogicalOperator.AND  # Default fallback
                    
                    component = RuleComponent(**comp_dict)
                    components.append(component)
                except Exception as e:
                    logger.warning(f"Failed to create rule component: {e}")
                    continue
            return components
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"Rule component extraction parsing error: {e}")
            return []
    
    async def _analyze_logical_structure(self, clause: Dict[str, Any], components: List[RuleComponent]) -> Dict[str, Any]:
        """Analyze the logical structure of rule components"""
        
        logical_prompt = f"""
        Analyze the logical structure of this legal clause and its components:
        
        CLAUSE: {clause.get("text", "")}
        COMPONENTS: {[f"{c.name} ({c.type})" for c in components]}
        
        Analyze:
        1. Logical flow (sequential, conditional, parallel)
        2. Dependencies between components
        3. Boolean logic (AND, OR, NOT operations)
        4. Conditional statements (IF-THEN-ELSE)
        5. Exception handling logic
        6. Precedence and priority relationships
        
        Return JSON:
        {{
            "logical_structure": {{
                "type": "sequential|conditional|parallel|hybrid",
                "main_operator": "AND|OR|IF_THEN",
                "complexity_level": "simple|moderate|complex",
                "dependencies": [
                    {{
                        "component": "component name",
                        "depends_on": ["other components"],
                        "relationship": "prerequisite|conditional|parallel"
                    }}
                ],
                "conditional_logic": [
                    {{
                        "condition": "if condition",
                        "then_action": "required action",
                        "else_action": "alternative action"
                    }}
                ],
                "exception_handling": ["exception scenarios"],
                "precedence_order": ["ordered list of components by priority"]
            }}
        }}
        """
        
        response = await get_openai_completion(
            logical_prompt,
            "You are an expert in logical analysis of legal rules with deep understanding of Boolean logic and conditional reasoning."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"logical_structure": {"type": "simple", "complexity_level": "unknown"}}
    
    async def _analyze_enforcement_mechanisms(self, rule_components: List[RuleComponent]) -> Dict[str, Any]:
        """Analyze enforcement mechanisms across all rule components"""
        
        enforcement_types = defaultdict(int)
        penalty_types = defaultdict(int)
        
        for component in rule_components:
            if component.enforcement_mechanism:
                enforcement_types[component.enforcement_mechanism] += 1
            if component.penalty:
                penalty_types[component.penalty] += 1
        
        return {
            "enforcement_mechanisms": dict(enforcement_types),
            "penalty_types": dict(penalty_types),
            "total_enforceable_components": len([c for c in rule_components if c.enforcement_mechanism]),
            "components_with_penalties": len([c for c in rule_components if c.penalty])
        }
    
    async def _create_enhanced_atomic_rules(self, clauses: List[Dict[str, Any]], entities: List[LegalEntity], 
                                          concepts: List[LegalConcept], rule_components: List[RuleComponent],
                                          logical_structures: Dict[str, Any]) -> List[EnhancedAtomicRule]:
        """Create enhanced atomic rules with comprehensive analysis and references"""
        
        enhanced_rules = []
        
        for i, clause in enumerate(clauses):
            clause_id = clause.get("id", f"clause_{i:03d}")
            
            # Get semantic roles
            semantic_roles = clause.get("semantic_roles", {})
            
            # Determine deontic type
            deontic_type = self._determine_deontic_type(clause["text"], rule_components)
            
            # Calculate complexity scores
            complexity_analysis = clause.get("complexity_analysis", {})
            complexity_score = complexity_analysis.get("complexity_score", 0.5)
            
            # Extract key phrases and entities mentioned
            key_phrases = await self._extract_key_phrases(clause["text"])
            entities_mentioned = [e.name for e in entities if e.name.lower() in clause["text"].lower()]
            
            # Create citation with enhanced reference information
            citation = LegalCitation(
                document_id=clause.get("source_reference", "unknown"),
                article=self._extract_article_reference(clause.get("source_reference", "")),
                section=self._extract_section_reference(clause.get("source_reference", "")),
                authority_level=LegalAuthorityLevel.STATUTORY,
                jurisdiction=JurisdictionScope.NATIONAL
            )
            
            # Create enhanced atomic rule with reference tracking
            try:
                enhanced_rule = EnhancedAtomicRule(
                    id=f"rule_{i:03d}",
                    text=clause["text"],
                    entities=entities,
                    concepts=concepts,
                    rule_components=rule_components,
                    semantic_roles=semantic_roles,
                    source_document=clause.get("source_reference", "unknown"),
                    citation=citation,
                    confidence=clause.get("legal_significance", 0.8),
                    legal_authority_level=LegalAuthorityLevel.STATUTORY,
                    jurisdictional_scope=JurisdictionScope.NATIONAL,
                    deontic_type=deontic_type,
                    logical_structure=logical_structures.get(clause_id, {}),
                    complexity_score=complexity_score,
                    entities_mentioned=entities_mentioned,
                    key_phrases=key_phrases
                )
                
                # Store additional reference information for decision tables
                enhanced_rule.metadata = {
                    "page_number": clause.get("page_number"),
                    "section_title": clause.get("section_title", ""),
                    "document_title": clause.get("document_title", ""),
                    "text_excerpt": clause["text"][:200] + "..." if len(clause["text"]) > 200 else clause["text"]
                }
                
                enhanced_rules.append(enhanced_rule)
                
            except Exception as e:
                logger.warning(f"Failed to create enhanced atomic rule for clause {i}: {e}")
                continue
        
        return enhanced_rules
    
    def _determine_deontic_type(self, text: str, rule_components: List[RuleComponent]) -> DeonticType:
        """Determine the deontic type of a rule"""
        text_lower = text.lower()
        
        # Check for strong obligation indicators
        if any(word in text_lower for word in ["must", "shall", "required", "obligation"]):
            return DeonticType.OBLIGATORY
        
        # Check for prohibition indicators
        if any(word in text_lower for word in ["must not", "shall not", "prohibited", "forbidden"]):
            return DeonticType.FORBIDDEN
        
        # Check for permission indicators
        if any(word in text_lower for word in ["may", "can", "permitted", "allowed"]):
            return DeonticType.PERMISSIBLE
        
        # Check rule components for type hints
        obligation_components = [c for c in rule_components if c.type in [RuleComponentType.OBLIGATION, RuleComponentType.RESTRICTION]]
        if obligation_components:
            return DeonticType.OBLIGATORY
        
        permission_components = [c for c in rule_components if c.type in [RuleComponentType.PERMISSION, RuleComponentType.RIGHT]]
        if permission_components:
            return DeonticType.PERMISSIBLE
        
        return DeonticType.OPTIONAL
    
    async def _extract_key_phrases(self, text: str) -> List[str]:
        """Extract key phrases from text"""
        
        phrase_prompt = f"""
        Extract the most important legal phrases from this text:
        
        TEXT: {text}
        
        Identify key phrases that represent:
        1. Legal obligations and requirements
        2. Rights and entitlements
        3. Conditions and restrictions
        4. Enforcement mechanisms
        5. Penalties and consequences
        
        Return JSON:
        {{
            "key_phrases": ["phrase1", "phrase2", "phrase3"]
        }}
        """
        
        response = await get_openai_completion(
            phrase_prompt,
            "You are an expert in legal text analysis with expertise in identifying key legal phrases and terminology."
        )
        
        try:
            result = json.loads(response)
            return result.get("key_phrases", [])
        except json.JSONDecodeError:
            # Fallback: extract noun phrases using simple regex
            phrases = re.findall(r'\b[A-Z][a-z]+(?:\s+[a-z]+)*\b', text)
            return phrases[:10]  # Return top 10 phrases
    
    def _extract_article_reference(self, reference: str) -> Optional[str]:
        """Extract article reference from citation"""
        article_match = re.search(r'Article\s+(\d+)', reference, re.IGNORECASE)
        return article_match.group(1) if article_match else None
    
    def _extract_section_reference(self, reference: str) -> Optional[str]:
        """Extract section reference from citation"""
        section_match = re.search(r'Section\s+(\d+(?:\.\d+)*)', reference, re.IGNORECASE)
        return section_match.group(1) if section_match else None

# ============================================================================
# ADVANCED ANALYSIS AGENTS
# ============================================================================

class SemanticCoherenceAnalysisAgent(EnhancedReactAgent):
    """Advanced semantic coherence and conflict detection"""
    
    def __init__(self):
        super().__init__(
            name="Semantic Coherence Analysis Agent",
            role="comprehensive semantic consistency and conflict analysis specialist",
            tools=["semantic similarity", "conflict detection", "coherence analysis", "consistency checking"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Perform comprehensive semantic coherence analysis"""
        logger.info("Semantic Coherence Analysis: Analyzing rule consistency and conflicts")
        
        try:
            # Analyze semantic similarity between rules
            similarity_analysis = await self._analyze_semantic_similarity(state.enhanced_atomic_rules)
            
            # Detect potential conflicts
            conflict_analysis = await self._detect_rule_conflicts(state.enhanced_atomic_rules, similarity_analysis)
            
            # Analyze coherence across the document
            coherence_analysis = await self._analyze_document_coherence(state.enhanced_atomic_rules)
            
            # Store results
            state.similarity_matrix = similarity_analysis["similarity_matrix"]
            state.conflict_analysis = conflict_analysis
            state.quality_metrics["semantic_coherence"] = coherence_analysis["coherence_score"]
            
            state.processing_steps.append("Semantic coherence analysis completed")
            state.current_agent = "advanced_ontology_formalization"
            
            logger.info(f"Semantic Coherence Analysis: Found {len(conflict_analysis.get('conflicts', []))} potential conflicts")
            return state
            
        except Exception as e:
            error_msg = f"Semantic coherence analysis error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _analyze_semantic_similarity(self, rules: List[EnhancedAtomicRule]) -> Dict[str, Any]:
        """Analyze semantic similarity between rules"""
        
        if len(rules) < 2:
            return {"similarity_matrix": [], "average_similarity": 0.0}
        
        # Get embeddings for all rules
        rule_embeddings = []
        for rule in rules:
            embedding = await get_embedding(rule.text)
            rule_embeddings.append(embedding if embedding else [0.0] * 1536)
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(rule_embeddings)
        
        # Calculate statistics
        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
        average_similarity = float(np.mean(upper_triangle)) if len(upper_triangle) > 0 else 0.0
        
        return {
            "similarity_matrix": similarity_matrix.tolist(),
            "average_similarity": average_similarity,
            "max_similarity": float(np.max(upper_triangle)) if len(upper_triangle) > 0 else 0.0,
            "min_similarity": float(np.min(upper_triangle)) if len(upper_triangle) > 0 else 0.0
        }
    
    async def _detect_rule_conflicts(self, rules: List[EnhancedAtomicRule], similarity_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Detect conflicts between rules"""
        
        conflicts = []
        similarity_matrix = np.array(similarity_analysis["similarity_matrix"])
        
        for i, rule1 in enumerate(rules):
            for j, rule2 in enumerate(rules[i+1:], i+1):
                if similarity_matrix[i][j] > 0.8:  # High semantic similarity
                    conflict_assessment = await self._assess_rule_conflict(rule1, rule2, similarity_matrix[i][j])
                    if conflict_assessment["is_conflict"]:
                        conflicts.append({
                            "rule1_id": rule1.id,
                            "rule2_id": rule2.id,
                            "similarity": float(similarity_matrix[i][j]),
                            "conflict_type": conflict_assessment["conflict_type"],
                            "severity": conflict_assessment["severity"],
                            "explanation": conflict_assessment["explanation"],
                            "resolution_suggestion": conflict_assessment["resolution_suggestion"]
                        })
        
        return {
            "conflicts": conflicts,
            "conflict_count": len(conflicts),
            "high_severity_conflicts": len([c for c in conflicts if c["severity"] == "high"]),
            "conflict_types": list(set([c["conflict_type"] for c in conflicts]))
        }
    
    async def _assess_rule_conflict(self, rule1: EnhancedAtomicRule, rule2: EnhancedAtomicRule, similarity: float) -> Dict[str, Any]:
        """Assess if two semantically similar rules are in conflict"""
        
        conflict_prompt = f"""
        Analyze these two legal rules for potential conflicts:
        
        RULE 1: {rule1.text}
        - Deontic Type: {rule1.deontic_type}
        - Entities: {[e.name for e in rule1.entities]}
        - Authority Level: {rule1.legal_authority_level}
        
        RULE 2: {rule2.text}
        - Deontic Type: {rule2.deontic_type}
        - Entities: {[e.name for e in rule2.entities]}
        - Authority Level: {rule2.legal_authority_level}
        
        Semantic Similarity: {similarity:.3f}
        
        Determine if these rules conflict by checking:
        1. Do they address the same legal situation with contradictory requirements?
        2. Do they prescribe opposite deontic types (obligatory vs. forbidden)?
        3. Are there authority hierarchy conflicts?
        4. Do they create logical inconsistencies?
        5. Are there jurisdictional conflicts?
        
        Return JSON:
        {{
            "is_conflict": true/false,
            "conflict_type": "direct_contradiction|authority_hierarchy|logical_inconsistency|jurisdictional_conflict|scope_overlap",
            "severity": "high|medium|low",
            "explanation": "detailed explanation of the conflict",
            "resolution_suggestion": "suggested approach to resolve the conflict",
            "confidence": 0.9
        }}
        """
        
        response = await get_openai_completion(
            conflict_prompt,
            "You are a legal conflict analysis expert specializing in regulatory compliance and legal reasoning with expertise in conflict resolution."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"is_conflict": False, "explanation": "Analysis failed", "confidence": 0.0}
    
    async def _analyze_document_coherence(self, rules: List[EnhancedAtomicRule]) -> Dict[str, Any]:
        """Analyze overall document coherence"""
        
        # Calculate coherence metrics
        coherence_metrics = {
            "total_rules": len(rules),
            "deontic_distribution": {},
            "authority_consistency": 0.0,
            "coherence_score": 0.0
        }
        
        # Analyze deontic type distribution
        deontic_counts = defaultdict(int)
        for rule in rules:
            deontic_counts[rule.deontic_type] += 1
        coherence_metrics["deontic_distribution"] = dict(deontic_counts)
        
        # Calculate authority consistency
        authority_levels = [rule.legal_authority_level for rule in rules]
        most_common_authority = max(set(authority_levels), key=authority_levels.count) if authority_levels else None
        authority_consistency = authority_levels.count(most_common_authority) / len(authority_levels) if authority_levels else 0
        coherence_metrics["authority_consistency"] = authority_consistency
        
        # Calculate overall coherence score
        coherence_score = (authority_consistency + 0.5) / 1.5  # Simplified calculation
        coherence_metrics["coherence_score"] = coherence_score
        
        return coherence_metrics

class AdvancedOntologyFormalizationAgent(EnhancedReactAgent):
    """Enhanced ontology creation with reasoning capabilities"""
    
    def __init__(self):
        super().__init__(
            name="Advanced Ontology Formalization Agent",
            role="comprehensive formal ontology creation and reasoning specialist",
            tools=["OWL-DL", "RDFLib", "Owlready2", "ontology patterns", "semantic reasoning"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Create comprehensive formal ontology with reasoning"""
        logger.info("Advanced Ontology Formalization: Creating formal OWL-DL ontology with reasoning capabilities")
        
        try:
            # Create comprehensive ontology
            ontology_data = await self._create_comprehensive_ontology(state)
            
            # Generate RDF triples
            rdf_triples = await self._generate_rdf_triples(ontology_data, state)
            
            # Create Owlready2 ontology
            owlready_ontology = await self._create_owlready_ontology(state)
            
            # Save ontologies
            ontology_files = await self._save_ontologies(rdf_triples, owlready_ontology)
            
            # Store results
            state.ontology_triples = rdf_triples
            state.metadata["ontology_files"] = ontology_files
            state.metadata["ontology_data"] = ontology_data
            
            state.processing_steps.append("Advanced ontology formalization completed")
            state.current_agent = "intelligent_decision_table_generation"
            
            logger.info(f"Advanced Ontology Formalization: Created ontology with {len(rdf_triples)} triples")
            return state
            
        except Exception as e:
            error_msg = f"Ontology formalization error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _create_comprehensive_ontology(self, state: ProcessingState) -> Dict[str, Any]:
        """Create comprehensive ontology structure"""
        
        ontology_prompt = f"""
        Create a comprehensive OWL-DL ontology for this legal knowledge:
        
        ENHANCED ATOMIC RULES: {[rule.text for rule in state.enhanced_atomic_rules[:5]]}
        ENTITIES: {[f"{e.name} ({e.type})" for e in state.metadata.get('extracted_entities', [])[:10]]}
        CONCEPTS: {[f"{c.name} ({c.type})" for c in state.metadata.get('extracted_concepts', [])[:10]]}
        RULE COMPONENTS: {[f"{rc.name} ({rc.type})" for rc in state.metadata.get('rule_components', [])[:10]]}
        
        Design a comprehensive ontology with:
        
        1. CLASS HIERARCHY:
        - LegalEntity (Controller, Processor, DataSubject, etc.)
        - DataOperation (Transfer, Access, Processing, etc.)
        - RuleComponent (Obligation, Right, Restriction, etc.)
        - LegalDocument (Regulation, Directive, etc.)
        - ProtectionMeasure (Encryption, Pseudonymisation, etc.)
        - LegalBasis (Consent, LegitimateInterest, etc.)
        
        2. OBJECT PROPERTIES:
        - hasObligation, hasRight, hasRestriction
        - appliesTo, governedBy, requiresCondition
        - transfersDataTo, accessesDataOf, processesDataFor
        - implementsMeasure, providesProtection
        - hasLegalBasis, requiresConsent
        
        3. DATA PROPERTIES:
        - hasDescription, hasLegalBasis, hasConfidence
        - hasComplexityScore, hasSeverity
        - hasJurisdiction, hasAuthority
        
        4. INDIVIDUALS:
        - Specific instances from extracted entities
        - Concrete examples of concepts
        
        5. AXIOMS AND CONSTRAINTS:
        - Domain and range restrictions
        - Cardinality constraints
        - Disjointness axioms
        - Equivalence relationships
        
        Return JSON with complete ontology specification:
        {{
            "classes": [
                {{
                    "name": "Controller",
                    "parent": "LegalEntity",
                    "description": "Entity determining purposes and means",
                    "properties": ["hasObligation", "transfersDataTo"]
                }}
            ],
            "object_properties": [
                {{
                    "name": "hasObligation",
                    "domain": "LegalEntity",
                    "range": "Obligation",
                    "description": "Entity has legal obligation"
                }}
            ],
            "data_properties": [
                {{
                    "name": "hasDescription",
                    "domain": "Thing",
                    "range": "string",
                    "description": "Textual description"
                }}
            ],
            "individuals": [
                {{
                    "name": "EuropeanController",
                    "type": "Controller",
                    "properties": {{"hasJurisdiction": "EU"}}
                }}
            ],
            "axioms": [
                {{
                    "type": "disjoint_classes",
                    "classes": ["Controller", "Processor"]
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            ontology_prompt,
            "You are an expert in formal ontology design with comprehensive knowledge of OWL-DL and legal domain modeling."
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error("Failed to parse ontology structure")
            return {"classes": [], "object_properties": [], "data_properties": [], "individuals": [], "axioms": []}
    
    async def _generate_rdf_triples(self, ontology_data: Dict[str, Any], state: ProcessingState) -> List[Dict[str, str]]:
        """Generate RDF triples from ontology data"""
        
        triples = []
        
        # Create namespace
        LEGAL_NS = "http://legal-rules.org/ontology#"
        
        # Add ontology header triples
        triples.extend([
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdf:type", "object": "owl:Ontology"},
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdfs:label", "object": "Legal Rules Ontology"},
            {"subject": f"{LEGAL_NS}LegalRulesOntology", "predicate": "rdfs:comment", "object": "Comprehensive ontology for legal rules and data protection"}
        ])
        
        # Add class triples
        for cls in ontology_data.get("classes", []):
            class_uri = f"{LEGAL_NS}{cls['name']}"
            triples.extend([
                {"subject": class_uri, "predicate": "rdf:type", "object": "owl:Class"},
                {"subject": class_uri, "predicate": "rdfs:label", "object": cls['name']},
                {"subject": class_uri, "predicate": "rdfs:comment", "object": cls.get('description', '')}
            ])
            
            if cls.get('parent'):
                parent_uri = f"{LEGAL_NS}{cls['parent']}"
                triples.append({"subject": class_uri, "predicate": "rdfs:subClassOf", "object": parent_uri})
        
        # Add object property triples
        for prop in ontology_data.get("object_properties", []):
            prop_uri = f"{LEGAL_NS}{prop['name']}"
            triples.extend([
                {"subject": prop_uri, "predicate": "rdf:type", "object": "owl:ObjectProperty"},
                {"subject": prop_uri, "predicate": "rdfs:label", "object": prop['name']},
                {"subject": prop_uri, "predicate": "rdfs:comment", "object": prop.get('description', '')},
                {"subject": prop_uri, "predicate": "rdfs:domain", "object": f"{LEGAL_NS}{prop['domain']}"},
                {"subject": prop_uri, "predicate": "rdfs:range", "object": f"{LEGAL_NS}{prop['range']}"}
            ])
        
        # Add data property triples
        for prop in ontology_data.get("data_properties", []):
            prop_uri = f"{LEGAL_NS}{prop['name']}"
            triples.extend([
                {"subject": prop_uri, "predicate": "rdf:type", "object": "owl:DatatypeProperty"},
                {"subject": prop_uri, "predicate": "rdfs:label", "object": prop['name']},
                {"subject": prop_uri, "predicate": "rdfs:comment", "object": prop.get('description', '')},
                {"subject": prop_uri, "predicate": "rdfs:domain", "object": f"{LEGAL_NS}{prop['domain']}"},
                {"subject": prop_uri, "predicate": "rdfs:range", "object": f"xsd:{prop['range']}"}
            ])
        
        # Add individual triples
        for individual in ontology_data.get("individuals", []):
            ind_uri = f"{LEGAL_NS}{individual['name']}"
            triples.extend([
                {"subject": ind_uri, "predicate": "rdf:type", "object": f"{LEGAL_NS}{individual['type']}"},
                {"subject": ind_uri, "predicate": "rdfs:label", "object": individual['name']}
            ])
            
            for prop_name, prop_value in individual.get('properties', {}).items():
                triples.append({"subject": ind_uri, "predicate": f"{LEGAL_NS}{prop_name}", "object": prop_value})
        
        return triples
    
    async def _create_owlready_ontology(self, state: ProcessingState) -> str:
        """Create ontology using Owlready2"""
        
        try:
            # Create ontology
            onto = owl.get_ontology("http://legal-rules.org/ontology.owl")
            
            with onto:
                # Define top-level classes
                class LegalEntity(owl.Thing): pass
                class DataOperation(owl.Thing): pass
                class RuleComponent(owl.Thing): pass
                class LegalDocument(owl.Thing): pass
                class ProtectionMeasure(owl.Thing): pass
                class LegalBasis(owl.Thing): pass
                
                # Define specific entity classes
                class Controller(LegalEntity): pass
                class Processor(LegalEntity): pass
                class JointController(LegalEntity): pass
                class DataSubject(LegalEntity): pass
                class SupervisoryAuthority(LegalEntity): pass
                class ThirdCountry(LegalEntity): pass
                class DataProtectionOfficer(LegalEntity): pass
                
                # Define data operation classes
                class DataTransfer(DataOperation): pass
                class DataAccess(DataOperation): pass
                class DataProcessing(DataOperation): pass
                class DataCollection(DataOperation): pass
                class DataStorage(DataOperation): pass
                class DataErasure(DataOperation): pass
                class DataRectification(DataOperation): pass
                
                # Define rule component classes
                class Obligation(RuleComponent): pass
                class Right(RuleComponent): pass
                class Restriction(RuleComponent): pass
                class Condition(RuleComponent): pass
                class Permission(RuleComponent): pass
                class Prohibition(RuleComponent): pass
                class Exception(RuleComponent): pass
                class Safeguard(RuleComponent): pass
                
                # Define object properties
                class hasObligation(owl.ObjectProperty):
                    domain = [LegalEntity]
                    range = [Obligation]
                
                class hasRight(owl.ObjectProperty):
                    domain = [DataSubject]
                    range = [Right]
                
                class requiresCondition(owl.ObjectProperty):
                    domain = [DataOperation]
                    range = [Condition]
                
                class appliesTo(owl.ObjectProperty):
                    domain = [RuleComponent]
                    range = [LegalEntity]
                
                # Define data properties
                class hasDescription(owl.DataProperty):
                    domain = [owl.Thing]
                    range = [str]
                
                class hasConfidenceScore(owl.DataProperty):
                    domain = [owl.Thing]
                    range = [float]
                
                # Create instances from extracted data
                entities = state.metadata.get("extracted_entities", [])
                for entity in entities[:20]:  # Limit to prevent memory issues
                    try:
                        # Clean entity name for use as identifier
                        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', entity.name)
                        if hasattr(onto, entity.type):
                            entity_class = getattr(onto, entity.type)
                            instance = entity_class(clean_name)
                            instance.hasDescription = [entity.description]
                            instance.hasConfidenceScore = [entity.confidence]
                    except Exception as e:
                        logger.warning(f"Failed to create instance for {entity.name}: {e}")
                
                # Add disjointness constraints
                owl.AllDisjoint([Controller, Processor, DataSubject, SupervisoryAuthority, ThirdCountry])
                owl.AllDisjoint([DataTransfer, DataAccess, DataCollection, DataStorage, DataErasure])
                owl.AllDisjoint([Obligation, Right, Restriction, Permission, Prohibition])
            
            # Save ontology
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            ontology_file = ONTOLOGY_OUTPUT / f"comprehensive_ontology_{timestamp}.owl"
            onto.save(file=str(ontology_file), format="rdfxml")
            
            logger.info("Ontology created successfully")
            
            return str(ontology_file)
            
        except Exception as e:
            logger.error(f"Owlready2 ontology creation error: {e}")
            return ""
    
    async def _save_ontologies(self, rdf_triples: List[Dict[str, str]], owlready_file: str) -> Dict[str, str]:
        """Save ontologies in multiple formats"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        files = {}
        
        try:
            # Save RDF/XML format
            g = Graph()
            
            # Add namespaces
            LEGAL_NS = Namespace("http://legal-rules.org/ontology#")
            g.bind("legal", LEGAL_NS)
            g.bind("owl", OWL)
            g.bind("rdfs", RDFS)
            g.bind("xsd", XSD)
            
            # Add triples
            for triple in rdf_triples:
                subject = URIRef(triple["subject"]) if triple["subject"].startswith("http") else URIRef(LEGAL_NS + triple["subject"])
                
                # Handle predicate
                if triple["predicate"].startswith("http"):
                    predicate = URIRef(triple["predicate"])
                elif ":" in triple["predicate"]:
                    if triple["predicate"].startswith("rdf:"):
                        predicate = URIRef(f"http://www.w3.org/1999/02/22-rdf-syntax-ns#{triple['predicate'][4:]}")
                    elif triple["predicate"].startswith("rdfs:"):
                        predicate = URIRef(f"http://www.w3.org/2000/01/rdf-schema#{triple['predicate'][5:]}")
                    elif triple["predicate"].startswith("owl:"):
                        predicate = URIRef(f"http://www.w3.org/2002/07/owl#{triple['predicate'][4:]}")
                    else:
                        predicate = URIRef(LEGAL_NS + triple["predicate"])
                else:
                    predicate = URIRef(LEGAL_NS + triple["predicate"])
                
                # Handle object
                if triple["object"].startswith("http"):
                    obj = URIRef(triple["object"])
                elif triple["object"].startswith("xsd:"):
                    obj = URIRef(f"http://www.w3.org/2001/XMLSchema#{triple['object'][4:]}")
                elif triple["object"].startswith("owl:") or triple["object"].startswith("rdfs:") or triple["object"].startswith("rdf:"):
                    if triple["object"].startswith("owl:"):
                        obj = URIRef(f"http://www.w3.org/2002/07/owl#{triple['object'][4:]}")
                    elif triple["object"].startswith("rdfs:"):
                        obj = URIRef(f"http://www.w3.org/2000/01/rdf-schema#{triple['object'][5:]}")
                    elif triple["object"].startswith("rdf:"):
                        obj = URIRef(f"http://www.w3.org/1999/02/22-rdf-syntax-ns#{triple['object'][4:]}")
                    else:
                        obj = Literal(triple["object"])
                else:
                    obj = Literal(triple["object"])
                
                g.add((subject, predicate, obj))
            
            # Save in different formats
            rdfxml_file = ONTOLOGY_OUTPUT / f"legal_ontology_{timestamp}.rdf"
            turtle_file = ONTOLOGY_OUTPUT / f"legal_ontology_{timestamp}.ttl"
            ntriples_file = ONTOLOGY_OUTPUT / f"legal_ontology_{timestamp}.nt"
            
            g.serialize(destination=str(rdfxml_file), format="xml")
            g.serialize(destination=str(turtle_file), format="turtle")
            g.serialize(destination=str(ntriples_file), format="nt")
            
            files.update({
                "rdf_xml": str(rdfxml_file),
                "turtle": str(turtle_file),
                "ntriples": str(ntriples_file),
                "owlready": owlready_file
            })
            
            logger.info(f"Ontologies saved in {len(files)} formats")
            
        except Exception as e:
            logger.error(f"Ontology saving error: {e}")
        
        return files

class IntelligentDecisionTableGenerationAgent(EnhancedReactAgent):
    """Enhanced decision table generation with optimization"""
    
    def __init__(self):
        super().__init__(
            name="Intelligent Decision Table Generation Agent",
            role="comprehensive decision table creation and optimization specialist",
            tools=["decision tables", "rule optimization", "conflict resolution", "completeness analysis"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Generate optimized decision tables with comprehensive analysis"""
        logger.info("Intelligent Decision Table Generation: Creating optimized decision tables")
        
        try:
            # Generate decision tables with references
            decision_tables = await self._generate_comprehensive_decision_tables(state)
            
            # Add enhanced atomic rules to decision tables for reference lookup
            for table in decision_tables:
                table["enhanced_atomic_rules"] = state.enhanced_atomic_rules
            
            # Optimize tables for performance
            optimized_tables = await self._optimize_decision_tables(decision_tables)
            
            # Validate completeness
            completeness_analysis = await self._analyze_completeness(optimized_tables, state)
            
            # Store results
            state.decision_rules = optimized_tables
            state.metadata["completeness_analysis"] = completeness_analysis
            
            # Save decision tables
            saved_files = await self._save_decision_tables(optimized_tables, completeness_analysis)
            state.metadata["decision_table_files"] = saved_files
            
            state.processing_steps.append("Intelligent decision table generation completed")
            state.current_agent = "final_output_generation"
            
            logger.info(f"Intelligent Decision Table Generation: Created {len(optimized_tables)} optimized decision tables")
            return state
            
        except Exception as e:
            error_msg = f"Decision table generation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _generate_comprehensive_decision_tables(self, state: ProcessingState) -> List[Dict[str, Any]]:
        """Generate comprehensive decision tables with references"""
        
        decision_table_prompt = f"""
        Create comprehensive decision tables from this legal analysis:
        
        ENHANCED ATOMIC RULES: {[f"ID: {rule.id}, Text: {rule.text[:200]}..." for rule in state.enhanced_atomic_rules[:10]]}
        ENTITIES: {[f"{e.name} ({e.type})" for e in state.metadata.get('extracted_entities', [])[:15]]}
        CONCEPTS: {[f"{c.name} ({c.type})" for c in state.metadata.get('extracted_concepts', [])[:15]]}
        CONFLICTS: {state.conflict_analysis.get('conflicts', [])}
        
        Create decision tables with:
        
        1. COMPREHENSIVE CONDITIONS: All relevant input conditions
        2. CLEAR ACTIONS: Specific outputs and required actions
        3. PRIORITY HANDLING: Rule precedence for conflict resolution
        4. EXCEPTION MANAGEMENT: Handling of exceptions and edge cases
        5. COMPLIANCE VALIDATION: Built-in compliance checking
        6. REFERENCE TRACKING: Link each rule to source documents
        
        Focus on these decision areas:
        - Data transfer authorization decisions
        - Data access permission decisions
        - Data processing legitimacy decisions
        - Data subject rights fulfillment decisions
        - Compliance violation response decisions
        - Cross-border transfer adequacy decisions
        
        Return JSON with decision tables:
        {{
            "decision_tables": [
                {{
                    "table_id": "dt_001",
                    "name": "Data Transfer Authorization",
                    "description": "Determines authorization for data transfers",
                    "conditions": {{
                        "data_subject_location": ["EU", "Non-EU"],
                        "transfer_destination": ["Adequate Country", "Third Country", "International Organization"],
                        "transfer_purpose": ["Commercial", "Legal Obligation", "Public Interest"],
                        "adequacy_decision": [true, false],
                        "safeguards_in_place": [true, false],
                        "data_subject_consent": [true, false, "not_required"]
                    }},
                    "actions": [
                        "authorize_transfer",
                        "require_additional_safeguards",
                        "request_supervisory_authority_approval",
                        "deny_transfer",
                        "document_transfer_basis",
                        "notify_data_subject"
                    ],
                    "rules": [
                        {{
                            "rule_id": "dt_001_r01",
                            "conditions": {{
                                "transfer_destination": "Adequate Country",
                                "adequacy_decision": true
                            }},
                            "actions": ["authorize_transfer", "document_transfer_basis"],
                            "priority": 1,
                            "source_rule": "rule_001",
                            "references": [
                                {{
                                    "document": "GDPR",
                                    "article": "Article 45",
                                    "section": "Section 1",
                                    "text_excerpt": "relevant text from the source document",
                                    "page_number": 23,
                                    "document_title": "General Data Protection Regulation",
                                    "section_title": "Transfers on the basis of an adequacy decision",
                                    "confidence": 0.95,
                                    "legal_authority": "statutory",
                                    "jurisdiction": "national"
                                }}
                            ]
                        }}
                    ],
                    "exceptions": ["emergency_situations", "vital_interests"],
                    "compliance_requirements": ["documentation", "notification", "monitoring"],
                    "implementation_complexity": "medium"
                }}
            ]
        }}
        """
        
        response = await get_openai_completion(
            decision_table_prompt,
            "You are a business rules expert specializing in legal decision table design with expertise in data protection compliance automation."
        )
        
        try:
            result = json.loads(response)
            return result.get("decision_tables", [])
        except json.JSONDecodeError:
            logger.error("Failed to parse decision tables response")
            return []
    
    async def _optimize_decision_tables(self, decision_tables: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimize decision tables for performance and completeness"""
        
        optimized_tables = []
        
        for table in decision_tables:
            try:
                # Optimize rule ordering
                optimized_rules = self._optimize_rule_order(table.get("rules", []))
                
                # Eliminate redundant conditions
                optimized_conditions = await self._eliminate_redundant_conditions(table.get("conditions", {}))
                
                # Consolidate similar actions
                optimized_actions = self._consolidate_actions(table.get("actions", []))
                
                # Add performance optimizations
                performance_hints = self._generate_performance_hints(table)
                
                optimized_table = {
                    **table,
                    "rules": optimized_rules,
                    "conditions": optimized_conditions,
                    "actions": optimized_actions,
                    "performance_hints": performance_hints,
                    "optimization_applied": True
                }
                
                optimized_tables.append(optimized_table)
                
            except Exception as e:
                logger.warning(f"Failed to optimize table {table.get('table_id', 'unknown')}: {e}")
                optimized_tables.append(table)
        
        return optimized_tables
    
    def _optimize_rule_order(self, rules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Optimize rule execution order"""
        
        if not rules:
            return rules
        
        # Sort rules based on priority
        def rule_sort_key(rule):
            priority = rule.get("priority", 999)
            return priority
        
        return sorted(rules, key=rule_sort_key)
    
    async def _eliminate_redundant_conditions(self, conditions: Dict[str, Any]) -> Dict[str, Any]:
        """Eliminate redundant conditions"""
        
        # Simple redundancy elimination
        optimized_conditions = {}
        
        for condition_name, condition_values in conditions.items():
            if isinstance(condition_values, list) and len(set(condition_values)) == len(condition_values):
                # No duplicates, keep as is
                optimized_conditions[condition_name] = condition_values
            elif isinstance(condition_values, list):
                # Remove duplicates
                optimized_conditions[condition_name] = list(set(condition_values))
            else:
                optimized_conditions[condition_name] = condition_values
        
        return optimized_conditions
    
    def _consolidate_actions(self, actions: List[str]) -> List[str]:
        """Consolidate similar actions"""
        
        # Remove duplicates while preserving order
        seen = set()
        consolidated = []
        
        for action in actions:
            if action not in seen:
                seen.add(action)
                consolidated.append(action)
        
        return consolidated
    
    def _generate_performance_hints(self, table: Dict[str, Any]) -> Dict[str, Any]:
        """Generate performance optimization hints"""
        
        hints = {
            "condition_evaluation_order": [],
            "early_termination_possible": False,
            "indexing_recommendations": [],
            "caching_opportunities": []
        }
        
        conditions = table.get("conditions", {})
        
        # Suggest evaluation order based on selectivity
        selectivity_scores = {}
        for condition_name, condition_values in conditions.items():
            if isinstance(condition_values, list):
                # Higher selectivity for fewer possible values
                selectivity_scores[condition_name] = 1.0 / len(condition_values) if condition_values else 1.0
            else:
                selectivity_scores[condition_name] = 0.5
        
        # Order by selectivity (most selective first)
        hints["condition_evaluation_order"] = sorted(selectivity_scores.keys(), 
                                                   key=lambda x: selectivity_scores[x], 
                                                   reverse=True)
        
        # Check for early termination opportunities
        rules = table.get("rules", [])
        if len(rules) > 5:
            hints["early_termination_possible"] = True
        
        # Suggest indexing for frequently used conditions
        frequent_conditions = [name for name, values in conditions.items() 
                             if isinstance(values, list) and len(values) > 3]
        hints["indexing_recommendations"] = frequent_conditions
        
        # Suggest caching for complex rules
        if len(rules) > 10:
            hints["caching_opportunities"] = ["rule_evaluation_results", "condition_combinations"]
        
        return hints
    
    async def _analyze_completeness(self, decision_tables: List[Dict[str, Any]], state: ProcessingState) -> Dict[str, Any]:
        """Analyze completeness of decision tables"""
        
        completeness_analysis = {
            "total_tables": len(decision_tables),
            "coverage_analysis": {},
            "gap_analysis": {},
            "completeness_score": 0.0
        }
        
        # Analyze coverage for each table
        for table in decision_tables:
            table_id = table.get("table_id", "unknown")
            conditions = table.get("conditions", {})
            rules = table.get("rules", [])
            
            # Calculate theoretical maximum combinations
            max_combinations = 1
            for condition_values in conditions.values():
                if isinstance(condition_values, list):
                    max_combinations *= len(condition_values)
            
            # Count covered combinations
            covered_combinations = len(rules)
            
            coverage_score = covered_combinations / max_combinations if max_combinations > 0 else 0.0
            
            completeness_analysis["coverage_analysis"][table_id] = {
                "max_combinations": max_combinations,
                "covered_combinations": covered_combinations,
                "coverage_score": coverage_score,
                "uncovered_combinations": max_combinations - covered_combinations
            }
        
        # Calculate overall completeness score
        if decision_tables:
            coverage_scores = [analysis["coverage_score"] for analysis in completeness_analysis["coverage_analysis"].values()]
            completeness_analysis["completeness_score"] = sum(coverage_scores) / len(coverage_scores)
        
        return completeness_analysis
    
    async def _save_decision_tables(self, decision_tables: List[Dict[str, Any]], completeness_analysis: Dict[str, Any]) -> Dict[str, str]:
        """Save decision tables in multiple formats"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        try:
            # Save as JSON
            json_file = DECISION_TABLES_OUTPUT / f"decision_tables_{timestamp}.json"
            with open(json_file, 'w') as f:
                json.dump({
                    "decision_tables": decision_tables,
                    "completeness_analysis": completeness_analysis,
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "total_tables": len(decision_tables),
                        "format_version": "1.0"
                    }
                }, f, indent=2)
            saved_files["json"] = str(json_file)
            
            # Save as CSV (flattened format)
            csv_file = DECISION_TABLES_OUTPUT / f"decision_tables_{timestamp}.csv"
            csv_data = []
            
            for table in decision_tables:
                for rule in table.get("rules", []):
                    row = {
                        "table_id": table.get("table_id"),
                        "table_name": table.get("name"),
                        "rule_id": rule.get("rule_id"),
                        "priority": rule.get("priority"),
                        "source_rule": rule.get("source_rule"),
                        "conditions": json.dumps(rule.get("conditions", {})),
                        "actions": "; ".join(rule.get("actions", [])),
                        "exceptions": "; ".join(table.get("exceptions", [])),
                        "compliance_requirements": "; ".join(table.get("compliance_requirements", []))
                    }
                    csv_data.append(row)
            
            if csv_data:
                df = pd.DataFrame(csv_data)
                df.to_csv(csv_file, index=False)
                saved_files["csv"] = str(csv_file)
            
            logger.info(f"Decision tables saved in {len(saved_files)} formats")
            
        except Exception as e:
            logger.error(f"Decision table saving error: {e}")
        
        return saved_files

class FinalOutputGenerationAgent(EnhancedReactAgent):
    """Generate final simplified output with rules, conditions, domains and roles"""
    
    def __init__(self):
        super().__init__(
            name="Final Output Generation Agent",
            role="final output formatting and JSON generation specialist",
            tools=["output formatting", "JSON generation", "rule categorization"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Generate final simplified output in requested format"""
        logger.info("Final Output Generation: Creating simplified rules output")
        
        try:
            # Generate simplified rules output
            simplified_rules = await self._generate_simplified_rules_output(state)
            
            # Generate final decision tables in JSON with references
            final_decision_tables = await self._format_decision_tables_json(state.decision_rules)
            
            # Store final outputs
            state.final_rules_output = simplified_rules
            state.metadata["final_decision_tables_json"] = final_decision_tables
            
            # Save final outputs
            saved_files = await self._save_final_simplified_outputs(simplified_rules, final_decision_tables)
            state.metadata["final_output_files"] = saved_files
            
            state.processing_steps.append("Final output generation completed")
            state.current_agent = "quality_assurance"
            
            logger.info(f"Final Output Generation: Generated {len(simplified_rules)} simplified rules")
            return state
            
        except Exception as e:
            error_msg = f"Final output generation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _generate_simplified_rules_output(self, state: ProcessingState) -> List[Dict[str, Any]]:
        """Generate simplified rules output with required fields"""
        
        simplified_rules = []
        
        for rule in state.enhanced_atomic_rules:
            # Determine role based on entities
            role = self._determine_primary_role(rule.entities)
            
            # Extract domain from concepts
            domain = self._extract_domain(rule.concepts)
            
            # Extract conditions from rule components
            conditions = self._extract_conditions(rule.rule_components)
            
            simplified_rule = {
                "rule_id": rule.id,
                "rule_text": rule.text,
                "conditions": conditions,
                "domain": domain,
                "role": role,
                "confidence": rule.confidence,
                "deontic_type": rule.deontic_type,
                "source_document": rule.source_document,
                "legal_authority": rule.legal_authority_level.value,
                "jurisdiction": rule.jurisdictional_scope.value
            }
            
            simplified_rules.append(simplified_rule)
        
        return simplified_rules
    
    def _determine_primary_role(self, entities: List[LegalEntity]) -> str:
        """Determine primary role from entities"""
        
        # Priority order for roles
        role_priority = {
            EntityType.CONTROLLER: 1,
            EntityType.JOINT_CONTROLLER: 2,
            EntityType.PROCESSOR: 3,
            EntityType.DATA_SUBJECT: 4,
            EntityType.SUPERVISING_AUTHORITY: 5,
            EntityType.THIRD_COUNTRY: 6,
            EntityType.DATA_PROTECTION_OFFICER: 7
        }
        
        relevant_entities = [e for e in entities if e.type in role_priority]
        
        if not relevant_entities:
            return "general"
        
        # Return the highest priority role
        primary_entity = min(relevant_entities, key=lambda e: role_priority.get(e.type, 999))
        
        # Map to simplified role names
        role_mapping = {
            EntityType.CONTROLLER: "controller",
            EntityType.JOINT_CONTROLLER: "joint_controller", 
            EntityType.PROCESSOR: "processor",
            EntityType.DATA_SUBJECT: "data_subject",
            EntityType.SUPERVISING_AUTHORITY: "authority",
            EntityType.THIRD_COUNTRY: "third_country",
            EntityType.DATA_PROTECTION_OFFICER: "dpo"
        }
        
        return role_mapping.get(primary_entity.type, "general")
    
    def _extract_domain(self, concepts: List[LegalConcept]) -> str:
        """Extract domain from concepts"""
        
        # Domain mapping based on concept types
        domain_mapping = {
            ConceptType.DATA_TRANSFER: "data_transfer",
            ConceptType.DATA_ACCESS: "data_access", 
            ConceptType.DATA_ENTITLEMENT: "data_entitlement",
            ConceptType.PROCESSING: "data_processing",
            ConceptType.COLLECTION: "data_collection",
            ConceptType.STORAGE: "data_storage",
            ConceptType.ERASURE: "data_erasure",
            ConceptType.RECTIFICATION: "data_rectification",
            ConceptType.PORTABILITY: "data_portability",
            ConceptType.PROFILING: "profiling",
            ConceptType.AUTOMATED_DECISION_MAKING: "automated_decision_making"
        }
        
        # Find the primary domain
        for concept in concepts:
            if concept.type in domain_mapping:
                return domain_mapping[concept.type]
        
        return "general_compliance"
    
    def _extract_conditions(self, rule_components: List[RuleComponent]) -> List[str]:
        """Extract conditions from rule components"""
        
        conditions = []
        
        for component in rule_components:
            if component.type in [RuleComponentType.CONDITION, RuleComponentType.RESTRICTION]:
                conditions.append(component.description)
            elif component.type in [RuleComponentType.OBLIGATION, RuleComponentType.RIGHT] and "if" in component.description.lower():
                # Extract conditional parts from obligations and rights
                conditions.append(component.description)
        
        return conditions
    
    async def _format_decision_tables_json(self, decision_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Format decision tables as clean JSON with enhanced references"""
        
        formatted_tables = {
            "decision_tables": [],
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_tables": len(decision_rules),
                "format_version": "1.0"
            }
        }
        
        for table in decision_rules:
            formatted_table = {
                "table_id": table.get("table_id", "unknown"),
                "name": table.get("name", ""),
                "description": table.get("description", ""),
                "conditions": table.get("conditions", {}),
                "rules": []
            }
            
            for rule in table.get("rules", []):
                # Get references from the source rule
                references = self._get_rule_references(rule.get("source_rule", ""), table.get("enhanced_atomic_rules", []))
                
                formatted_rule = {
                    "rule_id": rule.get("rule_id", ""),
                    "conditions": rule.get("conditions", {}),
                    "actions": rule.get("actions", []),
                    "priority": rule.get("priority", 999),
                    "source_rule": rule.get("source_rule", ""),
                    "references": references
                }
                formatted_table["rules"].append(formatted_rule)
            
            formatted_tables["decision_tables"].append(formatted_table)
        
        return formatted_tables
    
    def _get_rule_references(self, source_rule_id: str, enhanced_rules: List[EnhancedAtomicRule]) -> List[Dict[str, Any]]:
        """Get references for a specific rule from enhanced atomic rules"""
        
        references = []
        
        # Find the matching enhanced atomic rule
        matching_rule = None
        for rule in enhanced_rules:
            if rule.id == source_rule_id:
                matching_rule = rule
                break
        
        if matching_rule:
            reference = {
                "document": matching_rule.source_document,
                "article": matching_rule.citation.article if matching_rule.citation.article else "Not specified",
                "section": matching_rule.citation.section if matching_rule.citation.section else "Not specified",
                "text_excerpt": matching_rule.metadata.get("text_excerpt", matching_rule.text[:200] + "..."),
                "page_number": matching_rule.metadata.get("page_number", "Not specified"),
                "document_title": matching_rule.metadata.get("document_title", ""),
                "section_title": matching_rule.metadata.get("section_title", ""),
                "confidence": matching_rule.confidence,
                "legal_authority": matching_rule.legal_authority_level.value,
                "jurisdiction": matching_rule.jurisdictional_scope.value
            }
            references.append(reference)
        
        return references
    
    async def _save_final_simplified_outputs(self, simplified_rules: List[Dict[str, Any]], 
                                           decision_tables: Dict[str, Any]) -> Dict[str, str]:
        """Save final outputs in requested formats"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        try:
            # Save simplified rules as JSON
            rules_json_file = OUTPUT_DIRECTORY / f"final_rules_output_{timestamp}.json"
            with open(rules_json_file, 'w') as f:
                json.dump({
                    "rules": simplified_rules,
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "total_rules": len(simplified_rules),
                        "format": "simplified_rules_output"
                    }
                }, f, indent=2)
            saved_files["rules_json"] = str(rules_json_file)
            
            # Save decision tables as JSON
            decision_tables_json_file = OUTPUT_DIRECTORY / f"decision_tables_{timestamp}.json"
            with open(decision_tables_json_file, 'w') as f:
                json.dump(decision_tables, f, indent=2)
            saved_files["decision_tables_json"] = str(decision_tables_json_file)
            
            # Save combined output
            combined_output_file = OUTPUT_DIRECTORY / f"complete_output_{timestamp}.json"
            with open(combined_output_file, 'w') as f:
                json.dump({
                    "simplified_rules": simplified_rules,
                    "decision_tables": decision_tables,
                    "summary": {
                        "total_rules": len(simplified_rules),
                        "total_decision_tables": len(decision_tables.get("decision_tables", [])),
                        "generated_at": datetime.now().isoformat()
                    }
                }, f, indent=2)
            saved_files["combined_output"] = str(combined_output_file)
            
            logger.info(f"Final simplified outputs saved in {len(saved_files)} files")
            
        except Exception as e:
            logger.error(f"Final output saving error: {e}")
        
        return saved_files

class QualityAssuranceValidationAgent(EnhancedReactAgent):
    """Final quality assurance and validation"""
    
    def __init__(self):
        super().__init__(
            name="Quality Assurance Validation Agent",
            role="comprehensive quality validation and system verification specialist",
            tools=["quality metrics", "validation frameworks", "consistency checking", "compliance verification"]
        )
    
    async def act(self, thought: str, task: str, state: ProcessingState) -> ProcessingState:
        """Perform comprehensive quality assurance validation"""
        logger.info("Quality Assurance Validation: Performing final validation and quality checks")
        
        try:
            # Validate all components
            validation_results = await self._comprehensive_validation(state)
            
            # Calculate quality metrics
            quality_metrics = await self._calculate_quality_metrics(state, validation_results)
            
            # Generate final report
            final_report = await self._generate_final_report(state, validation_results, quality_metrics)
            
            # Store results
            state.metadata["validation_results"] = validation_results
            state.quality_metrics.update(quality_metrics)
            state.metadata["final_report"] = final_report
            
            # Save final outputs
            saved_files = await self._save_final_outputs(state, final_report)
            state.metadata["final_output_files"].update(saved_files)
            
            state.processing_steps.append("Quality assurance validation completed")
            state.current_agent = "completed"
            
            logger.info("Quality Assurance Validation: All validations completed successfully")
            return state
            
        except Exception as e:
            error_msg = f"Quality assurance validation error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            return state
    
    async def _comprehensive_validation(self, state: ProcessingState) -> Dict[str, Any]:
        """Perform comprehensive validation of all components"""
        
        validation_results = {
            "data_extraction_validation": {},
            "entity_validation": {},
            "concept_validation": {},
            "rule_validation": {},
            "ontology_validation": {},
            "decision_table_validation": {},
            "overall_consistency": {}
        }
        
        # Validate data extraction
        validation_results["data_extraction_validation"] = {
            "text_extraction_quality": len(state.raw_text) > 0,
            "structure_preservation": bool(state.structured_text),
            "metadata_completeness": bool(state.metadata)
        }
        
        # Validate entities
        entities = state.metadata.get("extracted_entities", [])
        validation_results["entity_validation"] = {
            "entity_count": len(entities),
            "average_confidence": np.mean([e.confidence for e in entities]) if entities else 0.0,
            "type_distribution": len(set([e.type for e in entities])),
            "completeness": len(entities) > 0
        }
        
        # Validate concepts
        concepts = state.metadata.get("extracted_concepts", [])
        validation_results["concept_validation"] = {
            "concept_count": len(concepts),
            "average_confidence": np.mean([c.confidence for c in concepts]) if concepts else 0.0,
            "type_distribution": len(set([c.type for c in concepts])),
            "completeness": len(concepts) > 0
        }
        
        # Validate rules
        rules = state.enhanced_atomic_rules
        validation_results["rule_validation"] = {
            "rule_count": len(rules),
            "average_confidence": np.mean([r.confidence for r in rules]) if rules else 0.0,
            "average_complexity": np.mean([r.complexity_score for r in rules]) if rules else 0.0,
            "deontic_type_distribution": len(set([r.deontic_type for r in rules])),
            "completeness": len(rules) > 0
        }
        
        # Validate ontology
        ontology_triples = state.ontology_triples
        validation_results["ontology_validation"] = {
            "triple_count": len(ontology_triples),
            "ontology_files_created": bool(state.metadata.get("ontology_files")),
            "completeness": len(ontology_triples) > 0
        }
        
        # Validate decision tables
        decision_rules = state.decision_rules
        validation_results["decision_table_validation"] = {
            "table_count": len(decision_rules),
            "completeness_score": state.metadata.get("completeness_analysis", {}).get("completeness_score", 0.0),
            "optimization_applied": any(table.get("optimization_applied", False) for table in decision_rules),
            "completeness": len(decision_rules) > 0
        }
        
        # Validate final output
        final_output = state.final_rules_output
        validation_results["final_output_validation"] = {
            "simplified_rules_count": len(final_output),
            "rules_with_roles": len([r for r in final_output if r.get("role") != "general"]),
            "rules_with_conditions": len([r for r in final_output if r.get("conditions")]),
            "completeness": len(final_output) > 0
        }
        
        return validation_results
    
    async def _calculate_quality_metrics(self, state: ProcessingState, validation_results: Dict[str, Any]) -> Dict[str, float]:
        """Calculate comprehensive quality metrics"""
        
        quality_metrics = {}
        
        # Overall completeness score
        completeness_scores = []
        for component, validation in validation_results.items():
            if isinstance(validation, dict) and "completeness" in validation:
                completeness_scores.append(1.0 if validation["completeness"] else 0.0)
        
        quality_metrics["overall_completeness"] = np.mean(completeness_scores) if completeness_scores else 0.0
        
        # Confidence score
        entity_confidence = validation_results.get("entity_validation", {}).get("average_confidence", 0.0)
        concept_confidence = validation_results.get("concept_validation", {}).get("average_confidence", 0.0)
        rule_confidence = validation_results.get("rule_validation", {}).get("average_confidence", 0.0)
        
        quality_metrics["average_confidence"] = np.mean([entity_confidence, concept_confidence, rule_confidence])
        
        # Coverage score (based on decision table completeness)
        quality_metrics["coverage_score"] = validation_results.get("decision_table_validation", {}).get("completeness_score", 0.0)
        
        # Complexity handling score
        complexity_score = validation_results.get("rule_validation", {}).get("average_complexity", 0.0)
        quality_metrics["complexity_handling"] = min(complexity_score * 2, 1.0)  # Normalize to 0-1
        
        # Overall quality score (weighted average)
        weights = {
            "overall_completeness": 0.30,
            "average_confidence": 0.25,
            "coverage_score": 0.25,
            "complexity_handling": 0.20
        }
        
        quality_metrics["overall_quality_score"] = sum(
            quality_metrics.get(metric, 0.0) * weight 
            for metric, weight in weights.items()
        )
        
        return quality_metrics
    
    async def _generate_final_report(self, state: ProcessingState, validation_results: Dict[str, Any], 
                                   quality_metrics: Dict[str, float]) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        
        report = {
            "executive_summary": {},
            "processing_summary": {},
            "validation_summary": validation_results,
            "quality_metrics": quality_metrics,
            "outputs_generated": {},
            "recommendations": [],
            "next_steps": []
        }
        
        # Executive summary
        report["executive_summary"] = {
            "document_processed": state.document_path,
            "processing_completed": state.current_agent == "completed",
            "total_processing_steps": len(state.processing_steps),
            "errors_encountered": len(state.error_messages),
            "warnings_issued": len(state.warnings),
            "overall_quality_score": quality_metrics.get("overall_quality_score", 0.0),
            "success_rate": 1.0 - (len(state.error_messages) / max(len(state.processing_steps), 1))
        }
        
        # Processing summary
        entities = state.metadata.get("extracted_entities", [])
        concepts = state.metadata.get("extracted_concepts", [])
        rules = state.enhanced_atomic_rules
        
        report["processing_summary"] = {
            "text_extraction": {
                "characters_extracted": len(state.raw_text),
                "structure_preserved": bool(state.structured_text),
                "clauses_identified": len(state.clauses)
            },
            "entity_extraction": {
                "total_entities": len(entities),
                "entity_types": len(set([e.type for e in entities])),
                "average_confidence": np.mean([e.confidence for e in entities]) if entities else 0.0
            },
            "concept_extraction": {
                "total_concepts": len(concepts),
                "concept_types": len(set([c.type for c in concepts])),
                "average_confidence": np.mean([c.confidence for c in concepts]) if concepts else 0.0
            },
            "rule_extraction": {
                "total_rules": len(rules),
                "average_confidence": np.mean([r.confidence for r in rules]) if rules else 0.0,
                "average_complexity": np.mean([r.complexity_score for r in rules]) if rules else 0.0
            },
            "ontology_creation": {
                "triples_generated": len(state.ontology_triples),
                "ontology_files": list(state.metadata.get("ontology_files", {}).keys())
            },
            "decision_tables": {
                "tables_generated": len(state.decision_rules),
                "completeness_score": state.metadata.get("completeness_analysis", {}).get("completeness_score", 0.0)
            }
        }
        
        # Outputs generated
        report["outputs_generated"] = {
            "ontology_files": state.metadata.get("ontology_files", {}),
            "decision_table_files": state.metadata.get("decision_table_files", {}),
            "final_output_files": state.metadata.get("final_output_files", {}),
            "total_files_generated": (
                len(state.metadata.get("ontology_files", {})) +
                len(state.metadata.get("decision_table_files", {})) +
                len(state.metadata.get("final_output_files", {}))
            )
        }
        
        # Generate recommendations
        report["recommendations"] = await self._generate_recommendations(state, validation_results, quality_metrics)
        
        # Generate next steps
        report["next_steps"] = [
            "Review and validate extracted entities and concepts",
            "Test decision tables with real-world scenarios",
            "Integrate ontology with existing systems",
            "Implement monitoring for rule conflicts",
            "Establish regular update procedures for evolving legislation"
        ]
        
        if quality_metrics.get("overall_quality_score", 0.0) < 0.7:
            report["next_steps"].insert(0, "Review and improve extraction quality before deployment")
        
        return report
    
    async def _generate_recommendations(self, state: ProcessingState, validation_results: Dict[str, Any], 
                                      quality_metrics: Dict[str, float]) -> List[str]:
        """Generate specific recommendations based on analysis"""
        
        recommendations = []
        
        # Quality-based recommendations
        if quality_metrics.get("overall_quality_score", 0.0) < 0.5:
            recommendations.append("Consider manual review and validation of extracted data due to low quality scores")
        
        if quality_metrics.get("coverage_score", 0.0) < 0.8:
            recommendations.append("Improve decision table coverage to handle more legal scenarios")
        
        # Component-specific recommendations
        entities = state.metadata.get("extracted_entities", [])
        if len(entities) < 5:
            recommendations.append("Consider reviewing document for additional legal entities that may have been missed")
        
        concepts = state.metadata.get("extracted_concepts", [])
        if len(concepts) < 5:
            recommendations.append("Expand concept extraction to capture more legal concepts and operations")
        
        rules = state.enhanced_atomic_rules
        if len(rules) < 10:
            recommendations.append("Document may need more detailed rule extraction or may be too brief for comprehensive analysis")
        
        # Conflict-based recommendations
        conflicts = state.conflict_analysis.get("conflicts", [])
        if len(conflicts) > 0:
            recommendations.append(f"Resolve {len(conflicts)} detected rule conflicts before implementation")
        
        # Error-based recommendations
        if state.error_messages:
            recommendations.append("Address processing errors to ensure complete analysis")
        
        return recommendations
    
    async def _save_final_outputs(self, state: ProcessingState, final_report: Dict[str, Any]) -> Dict[str, str]:
        """Save final comprehensive outputs"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        try:
            # Save comprehensive report as JSON
            report_json_file = OUTPUT_DIRECTORY / f"comprehensive_report_{timestamp}.json"
            with open(report_json_file, 'w') as f:
                json.dump({
                    "final_report": final_report,
                    "processing_state": {
                        "document_path": state.document_path,
                        "processing_steps": state.processing_steps,
                        "error_messages": state.error_messages,
                        "warnings": state.warnings,
                        "quality_metrics": state.quality_metrics
                    },
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "system_version": "1.0",
                        "processing_complete": state.current_agent == "completed"
                    }
                }, f, indent=2)
            saved_files["comprehensive_report_json"] = str(report_json_file)
            
            logger.info(f"Final outputs saved in {len(saved_files)} formats")
            
        except Exception as e:
            logger.error(f"Final output saving error: {e}")
        
        return saved_files

# ============================================================================
# ENHANCED WORKFLOW ORCHESTRATION
# ============================================================================

class ComprehensiveLegalRulesWorkflow:
    """Comprehensive workflow orchestrator using LangGraph with all enhanced agents"""
    
    def __init__(self):
        self.agents = {
            "document_processor": AdvancedDocumentProcessorAgent(),
            "intelligent_segmentation": IntelligentSegmentationAgent(),
            "entity_extraction": ComprehensiveEntityExtractionAgent(),
            "concept_extraction": AdvancedConceptExtractionAgent(),
            "rule_component_extraction": IntelligentRuleComponentExtractionAgent(),
            "semantic_coherence": SemanticCoherenceAnalysisAgent(),
            "ontology_formalization": AdvancedOntologyFormalizationAgent(),
            "decision_table_generation": IntelligentDecisionTableGenerationAgent(),
            "final_output_generation": FinalOutputGenerationAgent(),
            "quality_assurance": QualityAssuranceValidationAgent()
        }
        
        self.checkpointer = MemorySaver()
        self.workflow = self._build_comprehensive_workflow()
    
    def _build_comprehensive_workflow(self) -> StateGraph:
        """Build the comprehensive LangGraph workflow"""
        
        workflow = StateGraph(ProcessingState)
        
        # Add nodes for each agent
        workflow.add_node("document_processor", self._document_processor_node)
        workflow.add_node("intelligent_segmentation", self._intelligent_segmentation_node)
        workflow.add_node("entity_extraction", self._entity_extraction_node)
        workflow.add_node("concept_extraction", self._concept_extraction_node)
        workflow.add_node("rule_component_extraction", self._rule_component_node)
        workflow.add_node("semantic_coherence", self._semantic_coherence_node)
        workflow.add_node("ontology_formalization", self._ontology_formalization_node)
        workflow.add_node("decision_table_generation", self._decision_table_generation_node)
        workflow.add_node("final_output_generation", self._final_output_generation_node)
        workflow.add_node("quality_assurance", self._quality_assurance_node)
        
        # Define the enhanced flow
        workflow.add_edge(START, "document_processor")
        workflow.add_edge("document_processor", "intelligent_segmentation")
        workflow.add_edge("intelligent_segmentation", "entity_extraction")
        workflow.add_edge("entity_extraction", "concept_extraction")
        workflow.add_edge("concept_extraction", "rule_component_extraction")
        workflow.add_edge("rule_component_extraction", "semantic_coherence")
        workflow.add_edge("semantic_coherence", "ontology_formalization")
        workflow.add_edge("ontology_formalization", "decision_table_generation")
        workflow.add_edge("decision_table_generation", "final_output_generation")
        workflow.add_edge("final_output_generation", "quality_assurance")
        workflow.add_edge("quality_assurance", END)
        
        return workflow
    
    # Node methods for each agent
    async def _document_processor_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["document_processor"]
        thought = await agent.think(f"Document: {state.document_path}", "Extract and preprocess document", "Starting comprehensive document analysis")
        return await agent.act(thought, "document_processing", state)
    
    async def _intelligent_segmentation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["intelligent_segmentation"]
        thought = await agent.think(f"Text length: {len(state.raw_text)}", "Segment into atomic clauses", "Analyzing preprocessed text structure")
        return await agent.act(thought, "intelligent_segmentation", state)
    
    async def _entity_extraction_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["entity_extraction"]
        thought = await agent.think(f"{len(state.clauses)} clauses identified", "Extract legal entities", "Processing identified clauses for entities")
        return await agent.act(thought, "entity_extraction", state)
    
    async def _concept_extraction_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["concept_extraction"]
        thought = await agent.think(f"Entities extracted, processing concepts", "Extract legal concepts", "Building on entity analysis for concept identification")
        return await agent.act(thought, "concept_extraction", state)
    
    async def _rule_component_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["rule_component_extraction"]
        thought = await agent.think("Entities and concepts identified", "Extract rule components", "Analyzing logical structure of rules")
        return await agent.act(thought, "rule_component_extraction", state)
    
    async def _semantic_coherence_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["semantic_coherence"]
        thought = await agent.think("Rule components extracted", "Analyze semantic coherence", "Checking consistency and conflicts")
        return await agent.act(thought, "semantic_analysis", state)
    
    async def _ontology_formalization_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["ontology_formalization"]
        thought = await agent.think("Semantic analysis complete", "Create formal ontology", "Formalizing knowledge into OWL-DL ontology")
        return await agent.act(thought, "ontology_creation", state)
    
    async def _decision_table_generation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["decision_table_generation"]
        thought = await agent.think("Ontology created", "Generate decision tables", "Creating operational decision tables")
        return await agent.act(thought, "decision_table_generation", state)
    
    async def _final_output_generation_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["final_output_generation"]
        thought = await agent.think("Decision tables generated", "Generate final simplified output", "Creating final JSON output")
        return await agent.act(thought, "final_output_generation", state)
    
    async def _quality_assurance_node(self, state: ProcessingState) -> ProcessingState:
        agent = self.agents["quality_assurance"]
        thought = await agent.think("All components created", "Perform quality validation", "Final validation and quality assurance")
        return await agent.act(thought, "quality_validation", state)
    
    async def process_document(self, document_path: str, metadata: Dict[str, Any]) -> ProcessingState:
        """Process a single document through the entire enhanced workflow"""
        
        logger.info(f"Starting comprehensive processing of: {document_path}")
        
        # Initialize state
        initial_state = ProcessingState(
            document_path=document_path,
            metadata=metadata
        )
        
        # Compile and run workflow
        app = self.workflow.compile(checkpointer=self.checkpointer)
        
        config = {"configurable": {"thread_id": f"doc_{hash(document_path)}"}}
        
        try:
            final_state = await app.ainvoke(initial_state, config)
            logger.info(f"Processing completed successfully for: {document_path}")
            return final_state
            
        except Exception as e:
            logger.error(f"Workflow error for {document_path}: {e}")
            initial_state.error_messages.append(f"Workflow error: {str(e)}")
            return initial_state

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Enhanced main execution function with comprehensive processing"""
    logger.info("Starting Enhanced Legal Rules Multi-Agent System")
    
    # Load metadata
    metadata = load_metadata()
    
    # Initialize comprehensive workflow
    workflow = ComprehensiveLegalRulesWorkflow()
    
    # Process all PDF documents
    pdf_files = list(PDF_DIRECTORY.glob("*.pdf"))
    
    if not pdf_files:
        logger.warning(f"No PDF files found in {PDF_DIRECTORY}")
        print(f"No PDF files found in {PDF_DIRECTORY}")
        print("Please ensure PDF files are placed in the legislation_pdfs directory")
        return
    
    results = []
    
    for pdf_file in pdf_files:
        logger.info(f"Processing {pdf_file.name}")
        print(f"\nProcessing: {pdf_file.name}")
        
        # Get metadata for this document
        doc_metadata = metadata.get(str(pdf_file), {})
        
        # Process document
        result = await workflow.process_document(str(pdf_file), doc_metadata)
        results.append(result)
        
        # Log results
        if result.error_messages:
            logger.error(f"Errors in {pdf_file.name}: {result.error_messages}")
            print(f"   Errors encountered: {len(result.error_messages)}")
            for error in result.error_messages:
                print(f"     - {error}")
        else:
            logger.info(f"Successfully processed {pdf_file.name}")
            print(f"   Processing completed successfully")
            
        # Print summary statistics
        print(f"   Results Summary:")
        print(f"     - Clauses extracted: {len(result.clauses)}")
        print(f"     - Enhanced atomic rules: {len(result.enhanced_atomic_rules)}")
        print(f"     - Ontology triples: {len(result.ontology_triples)}")
        print(f"     - Decision rules: {len(result.decision_rules)}")
        print(f"     - Final simplified rules: {len(result.final_rules_output)}")
        print(f"     - Quality score: {result.quality_metrics.get('overall_quality_score', 0.0):.2f}")
        
        if result.metadata.get("final_output_files"):
            print(f"     - Output files generated: {len(result.metadata['final_output_files'])}")
    
    # Generate overall summary report
    summary_file = OUTPUT_DIRECTORY / "comprehensive_processing_summary.json"
    summary = {
        "processing_session": {
            "timestamp": datetime.now().isoformat(),
            "system_version": "Enhanced Multi-Agent System v1.0",
            "total_documents": len(pdf_files),
            "successful_processing": len([r for r in results if not r.error_messages]),
            "failed_processing": len([r for r in results if r.error_messages])
        },
        "aggregate_statistics": {
            "total_clauses": sum(len(r.clauses) for r in results),
            "total_enhanced_rules": sum(len(r.enhanced_atomic_rules) for r in results),
            "total_ontology_triples": sum(len(r.ontology_triples) for r in results),
            "total_decision_rules": sum(len(r.decision_rules) for r in results),
            "total_final_simplified_rules": sum(len(r.final_rules_output) for r in results),
            "average_quality_score": np.mean([r.quality_metrics.get('overall_quality_score', 0.0) for r in results]) if results else 0.0
        },
        "processing_details": [
            {
                "document": r.document_path,
                "success": len(r.error_messages) == 0,
                "clauses": len(r.clauses),
                "rules": len(r.enhanced_atomic_rules),
                "final_rules": len(r.final_rules_output),
                "quality_score": r.quality_metrics.get('overall_quality_score', 0.0),
                "errors": r.error_messages
            }
            for r in results
        ],
        "output_files": {
            "ontologies": sum(len(r.metadata.get('ontology_files', {})) for r in results),
            "decision_tables": sum(len(r.metadata.get('decision_table_files', {})) for r in results),
            "final_outputs": sum(len(r.metadata.get('final_output_files', {})) for r in results)
        }
    }
    
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    logger.info(f"Enhanced processing complete. Comprehensive summary saved to {summary_file}")
    
    # Print final summary
    print(f"\n" + "="*80)
    print("ENHANCED LEGAL RULES MULTI-AGENT SYSTEM - PROCESSING COMPLETE")
    print(f"="*80)
    print(f" Documents processed: {len(pdf_files)}")
    print(f" Successful: {summary['processing_session']['successful_processing']}")
    print(f" Failed: {summary['processing_session']['failed_processing']}")
    print(f" Total rules extracted: {summary['aggregate_statistics']['total_enhanced_rules']}")
    print(f" Average quality score: {summary['aggregate_statistics']['average_quality_score']:.2f}")
    print(f" Total output files: {sum(summary['output_files'].values())}")
    print(f" Summary saved to: {summary_file}")
    print(f"="*80)
    
    return results

if __name__ == "__main__":
    asyncio.run(main())
