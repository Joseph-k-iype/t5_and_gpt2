"""
Document Chunking Utilities
Intelligently chunks large documents while preserving context and semantic boundaries
"""

from typing import List, Dict, Any, Optional
import re


class DocumentChunker:
    """
    Intelligently chunks documents for processing without losing information
    """
    
    def __init__(
        self,
        chunk_size: int = 4000,  # REDUCED from 8000 to 4000 for token optimization
        chunk_overlap: int = 300,  # Slightly increased overlap for better context
        respect_boundaries: bool = True
    ):
        """
        Initialize document chunker
        
        Args:
            chunk_size: Target size for each chunk (in characters)
            chunk_overlap: Number of characters to overlap between chunks
            respect_boundaries: Whether to respect paragraph/sentence boundaries
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_boundaries = respect_boundaries
    
    def chunk_document(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Chunk a document into processable segments
        
        Args:
            text: The full document text
            metadata: Optional metadata to attach to each chunk
            
        Returns:
            List of chunk dictionaries with text and metadata
        """
        if len(text) <= self.chunk_size:
            # Document fits in one chunk
            return [{
                "chunk_id": 0,
                "text": text,
                "start_pos": 0,
                "end_pos": len(text),
                "total_chunks": 1,
                "metadata": metadata or {}
            }]
        
        if self.respect_boundaries:
            chunks = self._chunk_with_boundaries(text)
        else:
            chunks = self._chunk_simple(text)
        
        # Add metadata to all chunks
        total_chunks = len(chunks)
        for i, chunk in enumerate(chunks):
            chunk["chunk_id"] = i
            chunk["total_chunks"] = total_chunks
            chunk["metadata"] = metadata or {}
        
        return chunks
    
    def _chunk_simple(self, text: str) -> List[Dict[str, Any]]:
        """
        Simple chunking with fixed size and overlap
        """
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            chunk_text = text[start:end]
            
            chunks.append({
                "text": chunk_text,
                "start_pos": start,
                "end_pos": end
            })
            
            start = end - self.chunk_overlap
            if start >= len(text):
                break
        
        return chunks
    
    def _chunk_with_boundaries(self, text: str) -> List[Dict[str, Any]]:
        """
        Chunk respecting paragraph and sentence boundaries
        """
        # Split into paragraphs
        paragraphs = self._split_paragraphs(text)
        
        chunks = []
        current_chunk = []
        current_size = 0
        current_start = 0
        
        for para_start, para_end, para_text in paragraphs:
            para_size = len(para_text)
            
            # If single paragraph exceeds chunk size, split it by sentences
            if para_size > self.chunk_size:
                # Save current chunk if any
                if current_chunk:
                    chunks.append({
                        "text": "".join(current_chunk),
                        "start_pos": current_start,
                        "end_pos": current_start + current_size
                    })
                    current_chunk = []
                    current_size = 0
                
                # Split large paragraph by sentences
                para_chunks = self._chunk_paragraph_by_sentences(para_text, para_start)
                chunks.extend(para_chunks)
                current_start = para_end
                
            # If adding this paragraph exceeds chunk size, save current chunk
            elif current_size + para_size > self.chunk_size and current_chunk:
                chunks.append({
                    "text": "".join(current_chunk),
                    "start_pos": current_start,
                    "end_pos": current_start + current_size
                })
                
                # Start new chunk with overlap
                overlap_start = max(0, len(chunks) - 1)
                if chunks and self.chunk_overlap > 0:
                    overlap_text = chunks[-1]["text"][-self.chunk_overlap:]
                    current_chunk = [overlap_text, para_text]
                    current_size = len(overlap_text) + para_size
                else:
                    current_chunk = [para_text]
                    current_size = para_size
                
                current_start = para_start
            
            else:
                # Add paragraph to current chunk
                current_chunk.append(para_text)
                current_size += para_size
        
        # Add final chunk
        if current_chunk:
            chunks.append({
                "text": "".join(current_chunk),
                "start_pos": current_start,
                "end_pos": current_start + current_size
            })
        
        return chunks
    
    def _split_paragraphs(self, text: str) -> List[tuple]:
        """
        Split text into paragraphs
        
        Returns:
            List of (start_pos, end_pos, paragraph_text) tuples
        """
        paragraphs = []
        
        # Split by double newlines or multiple newlines
        pattern = r'\n\s*\n'
        
        last_end = 0
        for match in re.finditer(pattern, text):
            start = last_end
            end = match.start()
            
            para_text = text[start:end].strip()
            if para_text:
                paragraphs.append((start, end, para_text + '\n\n'))
            
            last_end = match.end()
        
        # Add final paragraph
        if last_end < len(text):
            para_text = text[last_end:].strip()
            if para_text:
                paragraphs.append((last_end, len(text), para_text))
        
        return paragraphs
    
    def _chunk_paragraph_by_sentences(
        self,
        paragraph: str,
        para_start: int
    ) -> List[Dict[str, Any]]:
        """
        Chunk a large paragraph by sentences
        """
        # Split into sentences
        sentences = self._split_sentences(paragraph)
        
        chunks = []
        current_chunk = []
        current_size = 0
        current_start = para_start
        
        for sent_start, sent_end, sent_text in sentences:
            sent_size = len(sent_text)
            
            # If single sentence exceeds chunk size, split it forcefully
            if sent_size > self.chunk_size:
                # Save current chunk
                if current_chunk:
                    chunks.append({
                        "text": " ".join(current_chunk),
                        "start_pos": current_start,
                        "end_pos": current_start + current_size
                    })
                    current_chunk = []
                    current_size = 0
                
                # Force split the sentence
                force_chunks = self._force_split(sent_text, para_start + sent_start)
                chunks.extend(force_chunks)
                current_start = para_start + sent_end
                
            elif current_size + sent_size > self.chunk_size and current_chunk:
                # Save current chunk
                chunks.append({
                    "text": " ".join(current_chunk),
                    "start_pos": current_start,
                    "end_pos": current_start + current_size
                })
                
                # Start new chunk
                current_chunk = [sent_text]
                current_size = sent_size
                current_start = para_start + sent_start
            
            else:
                current_chunk.append(sent_text)
                current_size += sent_size
        
        # Add final chunk
        if current_chunk:
            chunks.append({
                "text": " ".join(current_chunk),
                "start_pos": current_start,
                "end_pos": current_start + current_size
            })
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[tuple]:
        """
        Split text into sentences
        
        Returns:
            List of (start_pos, end_pos, sentence_text) tuples
        """
        sentences = []
        
        # Simple sentence splitting by common terminators
        pattern = r'[.!?]+[\s\n]+'
        
        last_end = 0
        for match in re.finditer(pattern, text):
            start = last_end
            end = match.end()
            
            sent_text = text[start:end].strip()
            if sent_text:
                sentences.append((start, end, sent_text))
            
            last_end = end
        
        # Add final sentence if any
        if last_end < len(text):
            sent_text = text[last_end:].strip()
            if sent_text:
                sentences.append((last_end, len(text), sent_text))
        
        return sentences
    
    def _force_split(self, text: str, start_pos: int) -> List[Dict[str, Any]]:
        """
        Force split text that exceeds chunk size
        """
        chunks = []
        pos = 0
        
        while pos < len(text):
            chunk_text = text[pos:pos + self.chunk_size]
            chunks.append({
                "text": chunk_text,
                "start_pos": start_pos + pos,
                "end_pos": start_pos + pos + len(chunk_text)
            })
            pos += self.chunk_size - self.chunk_overlap
        
        return chunks
    
    def merge_chunk_analyses(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Merge analyses from multiple chunks into a single comprehensive analysis
        
        Args:
            chunk_analyses: List of analysis dictionaries from individual chunks
            
        Returns:
            Merged analysis dictionary
        """
        if not chunk_analyses:
            return {}
        
        if len(chunk_analyses) == 1:
            return chunk_analyses[0]
        
        # Initialize merged structure
        merged = {
            "description": "",
            "user_actions": [],
            "system_actions": [],
            "user_duties": [],
            "system_duties": [],
            "constraints": [],
            "rule_type": "",
            "confidence": "medium"
        }
        
        # Merge descriptions
        descriptions = [a.get("description", "") for a in chunk_analyses if a.get("description")]
        merged["description"] = " ".join(descriptions)
        
        # Merge actions and duties (deduplicate)
        for field in ["user_actions", "system_actions", "user_duties", "system_duties"]:
            items = []
            seen = set()
            for analysis in chunk_analyses:
                for item in analysis.get(field, []):
                    item_key = str(item).lower().strip()
                    if item_key and item_key not in seen:
                        items.append(item)
                        seen.add(item_key)
            merged[field] = items
        
        # Merge constraints (deduplicate by type and description)
        constraints = []
        seen_constraints = set()
        
        for analysis in chunk_analyses:
            for constraint in analysis.get("constraints", []):
                if isinstance(constraint, dict):
                    constraint_key = (
                        constraint.get("type", "").lower(),
                        str(constraint.get("description", "")).lower().strip()
                    )
                else:
                    constraint_key = ("general", str(constraint).lower().strip())
                
                if constraint_key[1] and constraint_key not in seen_constraints:
                    constraints.append(constraint)
                    seen_constraints.add(constraint_key)
        
        merged["constraints"] = constraints
        
        # Determine rule type (most common or most specific)
        rule_types = [a.get("rule_type", "") for a in chunk_analyses if a.get("rule_type")]
        if rule_types:
            # If any say "combination", use that
            if "combination" in rule_types:
                merged["rule_type"] = "combination"
            else:
                # Use most common
                from collections import Counter
                merged["rule_type"] = Counter(rule_types).most_common(1)[0][0]
        
        # Determine confidence (use minimum)
        confidence_levels = {"high": 3, "medium": 2, "low": 1}
        confidences = [a.get("confidence", "medium") for a in chunk_analyses]
        min_confidence_value = min(confidence_levels.get(c, 2) for c in confidences)
        
        for conf_name, conf_value in confidence_levels.items():
            if conf_value == min_confidence_value:
                merged["confidence"] = conf_name
                break
        
        # Add notes about chunking
        merged["notes"] = f"Analysis synthesized from {len(chunk_analyses)} document chunks to ensure complete coverage without truncation."
        
        return merged
    
    def get_chunk_context(self, chunk: Dict[str, Any]) -> str:
        """
        Generate context string for a chunk to help with analysis
        
        Args:
            chunk: Chunk dictionary with metadata
            
        Returns:
            Context string describing the chunk's position
        """
        chunk_id = chunk.get("chunk_id", 0)
        total_chunks = chunk.get("total_chunks", 1)
        
        if total_chunks == 1:
            return "This is the complete document."
        
        position = "beginning" if chunk_id == 0 else "end" if chunk_id == total_chunks - 1 else "middle"
        
        context = f"This is chunk {chunk_id + 1} of {total_chunks} (from the {position} of the document)."
        
        if chunk_id > 0:
            context += " Previous chunks have been analyzed."
        
        if chunk_id < total_chunks - 1:
            context += " More content follows in subsequent chunks."
        
        return context
    
    def summarize_analysis(self, analysis: Dict[str, Any], max_length: int = 500) -> str:
        """
        Create a concise summary of an analysis for context passing
        
        Args:
            analysis: Full analysis dictionary
            max_length: Maximum character length for summary
            
        Returns:
            Concise summary string
        """
        summary_parts = []
        
        # Rule type and confidence
        if analysis.get("rule_type"):
            summary_parts.append(f"Type: {analysis['rule_type']}")
        
        if analysis.get("confidence"):
            summary_parts.append(f"Confidence: {analysis['confidence']}")
        
        # Key findings
        if analysis.get("description"):
            desc = analysis["description"][:200] + "..." if len(analysis["description"]) > 200 else analysis["description"]
            summary_parts.append(f"Description: {desc}")
        
        # Action counts
        user_actions_count = len(analysis.get("user_actions", []))
        system_actions_count = len(analysis.get("system_actions", []))
        constraints_count = len(analysis.get("constraints", []))
        
        if user_actions_count > 0:
            summary_parts.append(f"{user_actions_count} user actions")
        if system_actions_count > 0:
            summary_parts.append(f"{system_actions_count} system actions")
        if constraints_count > 0:
            summary_parts.append(f"{constraints_count} constraints")
        
        summary = "; ".join(summary_parts)
        
        # Truncate if needed
        if len(summary) > max_length:
            summary = summary[:max_length-3] + "..."
        
        return summary
