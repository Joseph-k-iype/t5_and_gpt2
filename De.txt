#!/usr/bin/env python3
"""
Enhanced Multi-Agent Legal Document Rule Extraction System using LangGraph
NO TRUNCATION - Processes complete PDF content with knowledge graph construction
Dedicated agents with Chain of Thought and ReAct prompting + Knowledge Graphs
Uses geography.json for all country data - no hardcoding
ERROR-FREE VERSION with proper data type handling
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Literal, TypedDict
from datetime import datetime
import re
import uuid
import math
from dataclasses import dataclass

# Core libraries
import pandas as pd
import PyPDF2
from pydantic import BaseModel, Field, ValidationError, model_validator
from pydantic_core import from_json

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

# OpenAI for embeddings (if needed)
import openai

# Global Configuration
API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
BASE_URL = "https://api.openai.com/v1"
MODEL_NAME = "gpt-4o-mini"  # Using the model from existing code
EMBEDDING_MODEL = "text-embedding-3-large"
CHUNK_SIZE = 8000  # Increased chunk size for better processing
OVERLAP_SIZE = 1000  # Increased overlap to prevent information loss
MAX_CHUNKS = 50  # Process up to 50 chunks to handle large documents

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Pydantic v2 Models with exact required structure
class RuleCondition(BaseModel):
    """Rule condition with required structure and role assignment"""
    condition_id: str = Field(..., description="Unique condition identifier")
    condition_definition: str = Field(..., description="Clear condition definition in simple English")
    fact: str = Field(..., description="The fact to evaluate")
    operator: str = Field(..., description="Comparison operator")
    value: Union[str, int, float, bool, List[Any]] = Field(..., description="Value to compare against")
    role: str = Field(..., description="Role: controller, processor, joint_controller, or data_subject")

class ExtractedRule(BaseModel):
    """Extracted rule with exact required structure"""
    rule_id: str = Field(..., description="Unique rule identifier")
    rule_definition: str = Field(..., description="Complete rule definition in simple English")
    applicable_countries: List[str] = Field(..., description="Country ISO codes from metadata config")
    adequacy_countries: List[str] = Field(default_factory=list, description="Actual countries with adequacy decisions from PDF")
    conditions: List[RuleCondition] = Field(..., description="List of rule conditions with roles")
    aggregated_roles: List[str] = Field(default_factory=list, description="All roles involved in this rule")
    data_category: str = Field(..., description="Primary data category")
    domain: str = Field(default="access_and_entitlements", description="Domain focus")
    action: str = Field(..., description="Required action in simple English")
    reference: str = Field(..., description="Level and article/text reference")

class MetadataConfig(BaseModel):
    """Configuration model"""
    pdf_path: str = Field(..., description="Path to PDF file")
    applicable_countries: List[str] = Field(..., description="ISO country codes where rules apply")
    document_type: str = Field(default="regulation", description="Type of document")

# Document Chunk Management
@dataclass
class DocumentChunk:
    """Document chunk with metadata"""
    content: str
    chunk_id: str
    start_page: int
    end_page: int
    section_type: str
    overlap_content: str = ""

# Multi-Agent State Management (Enhanced)
class MultiAgentState(TypedDict):
    """Enhanced shared state between all agents"""
    # Input data
    document_text: str
    document_chunks: List[Dict[str, Any]]
    metadata_config: dict
    geography_data: dict
    
    # Processing stages
    parsed_sections: dict
    knowledge_graph: dict  # New: Knowledge graph representation
    identified_countries: dict
    extracted_rules: list
    validated_rules: list
    
    # Agent reasoning traces with knowledge graphs
    agent1_reasoning: list
    agent1_knowledge_graph: dict
    agent2_reasoning: list  
    agent2_knowledge_graph: dict
    agent3_reasoning: list
    agent3_knowledge_graph: dict
    agent4_reasoning: list
    agent4_knowledge_graph: dict
    
    # Processing control
    current_agent: str
    processing_complete: bool
    chunks_processed: int
    total_chunks: int

# Safe utility functions
def safe_get(obj: Any, key: str, default: Any = None) -> Any:
    """Safely get attribute from object"""
    try:
        if isinstance(obj, dict):
            return obj.get(key, default)
        elif hasattr(obj, key):
            return getattr(obj, key, default)
        else:
            return default
    except:
        return default

def safe_json_parse(json_str: str) -> Any:
    """Safely parse JSON string"""
    try:
        return json.loads(json_str)
    except Exception as e:
        logger.debug(f"JSON parsing failed: {e}")
        return None

def safe_extend_list(target_list: List, source: Any) -> None:
    """Safely extend list with source"""
    try:
        if isinstance(source, list):
            target_list.extend(source)
        elif isinstance(source, str) and source.strip():
            target_list.append(source)
        elif source is not None:
            target_list.append(str(source))
    except Exception as e:
        logger.debug(f"List extend failed: {e}")

def ensure_dict(obj: Any) -> Dict[str, Any]:
    """Ensure object is a dictionary"""
    if isinstance(obj, dict):
        return obj
    elif obj is None:
        return {}
    else:
        try:
            return {"value": str(obj)}
        except:
            return {}

def ensure_list(obj: Any) -> List[Any]:
    """Ensure object is a list"""
    if isinstance(obj, list):
        return obj
    elif obj is None:
        return []
    else:
        return [obj]

# Enhanced Geography Handler - NO HARDCODING
class GeographyHandler:
    """Enhanced geography handler with better country verification"""
    
    def __init__(self, geography_file: str):
        self.geography_data = self._load_geography_data(geography_file)
        self.all_countries = self._extract_all_countries()
        self.country_variations = self._build_country_variations()
        logger.info(f"ðŸŒ Geography data loaded: {len(self.all_countries)} countries")
    
    def _load_geography_data(self, file_path: str) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load geography data: {e}")
            return {}
    
    def _extract_all_countries(self) -> Dict[str, str]:
        """Extract all countries from geography.json - NO HARDCODING"""
        countries = {}
        
        try:
            # Process all regions in geography data
            for region_key, region_data in self.geography_data.items():
                if isinstance(region_data, dict):
                    # Direct countries list
                    if 'countries' in region_data:
                        for country in ensure_list(region_data['countries']):
                            if isinstance(country, dict):
                                iso2 = safe_get(country, 'iso2', '')
                                name = safe_get(country, 'name', '')
                                if iso2 and name:
                                    countries[iso2] = name
                    
                    # Nested continent structure
                    if region_key == 'By_Continent':
                        for continent, continent_data in region_data.items():
                            continent_dict = ensure_dict(continent_data)
                            if 'countries' in continent_dict:
                                for country in ensure_list(continent_dict['countries']):
                                    if isinstance(country, dict):
                                        iso2 = safe_get(country, 'iso2', '')
                                        name = safe_get(country, 'name', '')
                                        if iso2 and name:
                                            countries[iso2] = name
        except Exception as e:
            logger.error(f"Error extracting countries: {e}")
        
        return countries
    
    def _build_country_variations(self) -> Dict[str, str]:
        """Build country name variations for better matching"""
        variations = {}
        try:
            for iso, name in self.all_countries.items():
                if isinstance(name, str) and isinstance(iso, str):
                    variations[name.lower()] = iso.upper()
                    variations[iso.lower()] = iso.upper()
                    # Add common variations
                    if "," in name:
                        short_name = name.split(",")[0].strip()
                        variations[short_name.lower()] = iso.upper()
        except Exception as e:
            logger.error(f"Error building country variations: {e}")
        return variations
    
    def get_country_name(self, iso_code: str) -> Optional[str]:
        """Get country name from ISO code"""
        try:
            return self.all_countries.get(str(iso_code).upper()) if iso_code else None
        except:
            return None
    
    def get_country_iso(self, country_name: str) -> Optional[str]:
        """Get ISO code from country name with variations"""
        try:
            return self.country_variations.get(str(country_name).lower()) if country_name else None
        except:
            return None
    
    def is_valid_country(self, iso_code: str) -> bool:
        """Check if ISO code is valid country"""
        try:
            return str(iso_code).upper() in self.all_countries if iso_code else False
        except:
            return False
    
    def find_countries_in_text(self, text: str) -> List[str]:
        """Enhanced country finding with better matching"""
        found_countries = set()
        try:
            if not text:
                return []
            
            text_lower = str(text).lower()
            
            # Search for country names and ISO codes
            for variation, iso in self.country_variations.items():
                if len(variation) > 2:  # Avoid matching very short strings
                    if re.search(r'\b' + re.escape(variation) + r'\b', text_lower):
                        found_countries.add(iso)
        except Exception as e:
            logger.error(f"Error finding countries in text: {e}")
        
        return list(found_countries)

# Enhanced PDF Processing with NO TRUNCATION
class PDFProcessor:
    """Enhanced PDF processor with complete content extraction"""
    
    @staticmethod
    def extract_complete_text_from_pdf(pdf_path: str) -> Dict[str, Any]:
        """Extract complete text from PDF with page metadata"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                complete_text = ""
                page_contents = []
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            page_contents.append({
                                'page_number': page_num + 1,
                                'content': str(page_text),
                                'length': len(str(page_text))
                            })
                            complete_text += f"\n[PAGE {page_num + 1}]\n{page_text}\n"
                    except Exception as e:
                        logger.warning(f"Failed to extract page {page_num + 1}: {e}")
                        continue
                
                return {
                    'complete_text': complete_text,
                    'page_contents': page_contents,
                    'total_pages': len(pdf_reader.pages),
                    'total_length': len(complete_text)
                }
        except Exception as e:
            logger.error(f"Failed to extract text from PDF {pdf_path}: {e}")
            return {'complete_text': "", 'page_contents': [], 'total_pages': 0, 'total_length': 0}
    
    @staticmethod
    def create_overlapping_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP_SIZE) -> List[DocumentChunk]:
        """Create overlapping chunks from text for complete coverage"""
        chunks = []
        
        try:
            if not text:
                return chunks
            
            text = str(text)
            text_length = len(text)
            
            if text_length <= chunk_size:
                # Single chunk if text is small
                chunks.append(DocumentChunk(
                    content=text,
                    chunk_id="chunk_1",
                    start_page=1,
                    end_page=1,
                    section_type="complete_document"
                ))
                return chunks
            
            start = 0
            chunk_num = 1
            
            while start < text_length and chunk_num <= MAX_CHUNKS:
                end = min(start + chunk_size, text_length)
                
                # Extract chunk content
                chunk_content = text[start:end]
                
                # Add overlap from next chunk if available
                overlap_content = ""
                if end < text_length:
                    overlap_end = min(end + overlap, text_length)
                    overlap_content = text[end:overlap_end]
                
                # Determine page numbers (approximate based on [PAGE X] markers)
                try:
                    page_matches = re.findall(r'\[PAGE (\d+)\]', text[:start])
                    start_page = int(page_matches[-1]) if page_matches else 1
                    
                    page_matches_end = re.findall(r'\[PAGE (\d+)\]', text[:end])
                    end_page = int(page_matches_end[-1]) if page_matches_end else start_page
                except:
                    start_page, end_page = 1, 1
                
                chunk = DocumentChunk(
                    content=chunk_content,
                    chunk_id=f"chunk_{chunk_num}",
                    start_page=start_page,
                    end_page=end_page,
                    section_type="document_section",
                    overlap_content=overlap_content
                )
                
                chunks.append(chunk)
                start += chunk_size - overlap  # Move start position with overlap
                chunk_num += 1
            
            logger.info(f"ðŸ“„ Created {len(chunks)} overlapping chunks from {text_length} characters")
        except Exception as e:
            logger.error(f"Error creating chunks: {e}")
        
        return chunks

# AGENT 1: Enhanced Document Parser Agent with Knowledge Graphs
class DocumentParserAgent:
    """Enhanced Agent 1: Document parsing with knowledge graphs and NO truncation"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "DocumentParserAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process document with enhanced ReAct prompting and knowledge graphs"""
        
        logger.info(f"ðŸ¤– {self.name}: Starting complete document parsing (NO TRUNCATION)")
        
        try:
            # Create document chunks for complete processing
            pdf_processor = PDFProcessor()
            chunks = pdf_processor.create_overlapping_chunks(state['document_text'])
            
            system_prompt = """You are a specialized Document Parser Agent using ReAct prompting with Knowledge Graph construction.

Your task: Parse legal documents completely and create structured knowledge representations.

Knowledge Graph Construction Process:
1. Build internal knowledge graph of document structure
2. Identify entities: Sections, Articles, Regulations, References
3. Map relationships: Contains, References, Applies-to, Requires
4. Create hierarchical structure: Level-1, Level-2, Level-3

ReAct Framework Enhanced:
1. REASONING: Analyze document structure and build knowledge map
2. ACTION: Extract sections and populate knowledge graph
3. OBSERVATION: Validate completeness and structure integrity
4. KNOWLEDGE GRAPH: Update internal graph representation
5. CONCLUSION: Provide complete parsed sections

CRITICAL: Process ENTIRE document content - NO truncation allowed.
Build comprehensive knowledge graph for document structure understanding."""

            # Process all chunks to ensure complete coverage
            all_sections = {}
            reasoning_trace = []
            knowledge_graph = {"entities": [], "relationships": [], "sections": {}}
            
            for i, chunk in enumerate(chunks):
                if chunk and hasattr(chunk, 'content'):
                    logger.info(f"ðŸ“„ Processing chunk {i+1}/{len(chunks)} (pages {chunk.start_page}-{chunk.end_page})")
                    
                    user_prompt = f"""Parse this document chunk using ReAct methodology with Knowledge Graph construction:

CHUNK {i+1}/{len(chunks)} - Pages {chunk.start_page} to {chunk.end_page}:
{chunk.content[:4000]}

Use ReAct with Knowledge Graphs:
1. REASONING: Analyze chunk for legal structure and entities
2. ACTION: Extract sections and build knowledge graph representation
3. OBSERVATION: Evaluate extraction completeness 
4. KNOWLEDGE GRAPH: Create/update internal knowledge representation
5. CONCLUSION: Provide structured sections

Extract ALL Level-1-Regulation-*, Level-2-Regulator-Guidance, Level-3-Supporting-Information sections.
Return sections in JSON format."""

                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    
                    try:
                        response = self.llm.invoke(messages)
                        response_text = str(response.content) if response and response.content else ""
                        
                        # Extract reasoning and knowledge graph
                        chunk_reasoning = self._extract_reasoning_trace(response_text)
                        safe_extend_list(reasoning_trace, chunk_reasoning)
                        
                        # Extract sections from this chunk
                        chunk_sections = self._extract_sections_from_response(response_text, chunk.content)
                        
                        # Merge sections safely
                        for section_name, content in chunk_sections.items():
                            if isinstance(section_name, str) and content:
                                content_str = str(content)
                                if section_name in all_sections:
                                    # Safely combine content from multiple chunks
                                    all_sections[section_name] = str(all_sections[section_name]) + f"\n\n[CONTINUED FROM CHUNK {i+1}]\n{content_str}"
                                else:
                                    all_sections[section_name] = content_str
                        
                        # Update knowledge graph
                        chunk_kg = self._extract_knowledge_graph(response_text)
                        self._merge_knowledge_graphs(knowledge_graph, chunk_kg)
                        
                    except Exception as e:
                        logger.error(f"âŒ Chunk {i+1} processing failed: {e}")
                        reasoning_trace.append(f"ERROR Chunk {i+1}: {str(e)}")
            
            # Update state safely
            state['parsed_sections'] = all_sections
            state['agent1_reasoning'] = reasoning_trace
            state['agent1_knowledge_graph'] = knowledge_graph
            state['document_chunks'] = [self._chunk_to_dict(chunk) for chunk in chunks]
            state['chunks_processed'] = len(chunks)
            state['total_chunks'] = len(chunks)
            state['current_agent'] = 'GeographyAgent'
            
            logger.info(f"âœ… {self.name}: Successfully parsed {len(all_sections)} sections from {len(chunks)} chunks")
            logger.info(f"ðŸ“Š Knowledge Graph: {len(knowledge_graph.get('entities', []))} entities, {len(knowledge_graph.get('relationships', []))} relationships")
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: Processing failed: {e}")
            state['agent1_reasoning'] = [f"ERROR: {str(e)}"]
            state['agent1_knowledge_graph'] = {"entities": [], "relationships": []}
            state['parsed_sections'] = {"general": state.get('document_text', '')[:2000]}
        
        return state
    
    def _chunk_to_dict(self, chunk: DocumentChunk) -> Dict[str, Any]:
        """Convert chunk to dictionary safely"""
        try:
            return {
                'content': str(chunk.content) if chunk.content else "",
                'chunk_id': str(chunk.chunk_id) if chunk.chunk_id else "",
                'start_page': int(chunk.start_page) if chunk.start_page else 1,
                'end_page': int(chunk.end_page) if chunk.end_page else 1,
                'section_type': str(chunk.section_type) if chunk.section_type else "",
                'overlap_content': str(chunk.overlap_content) if chunk.overlap_content else ""
            }
        except Exception as e:
            logger.error(f"Error converting chunk to dict: {e}")
            return {'content': "", 'chunk_id': "", 'start_page': 1, 'end_page': 1, 'section_type': "", 'overlap_content': ""}
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced ReAct reasoning trace"""
        trace = []
        
        try:
            if not response_text:
                return trace
            
            response_text = str(response_text)
            
            patterns = [
                (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
                (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
                (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
                (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
                (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
            ]
            
            for pattern, label in patterns:
                matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if match and match.strip():
                        trace.append(f"{label}: {str(match).strip()}")
        except Exception as e:
            logger.debug(f"Error extracting reasoning trace: {e}")
        
        return trace
    
    def _extract_sections_from_response(self, response_text: str, original_content: str) -> Dict[str, str]:
        """Extract sections with enhanced pattern matching"""
        sections = {}
        
        try:
            # Try JSON extraction first
            json_data = self._safe_json_extract(response_text)
            if json_data and isinstance(json_data, dict):
                return {str(k): str(v) for k, v in json_data.items() if v}
            
            # Enhanced pattern matching for legal document structure
            level_patterns = {
                "Level-1": r"Level-1-Regulation-([^:]+):(.*?)(?=Level-[123]|$)",
                "Level-2": r"Level-2-Regulator-Guidance:(.*?)(?=Level-[123]|$)",
                "Level-3": r"Level-3-Supporting-Information:(.*?)(?=Level-[123]|$)"
            }
            
            # Search in both response and original content
            search_texts = [str(response_text), str(original_content)]
            
            for text in search_texts:
                if not text:
                    continue
                    
                for level, pattern in level_patterns.items():
                    matches = re.finditer(pattern, text, re.DOTALL | re.IGNORECASE)
                    for i, match in enumerate(matches):
                        try:
                            if level == "Level-1":
                                reg_name = str(match.group(1)).strip()
                                content = str(match.group(2)).strip()
                                if reg_name and content:
                                    section_key = f"{level}-Regulation-{reg_name}"
                                    sections[section_key] = content
                            else:
                                content = str(match.group(1)).strip()
                                if content and len(content) > 50:  # Only add substantial content
                                    sections[level] = content
                        except Exception as e:
                            logger.debug(f"Error extracting match: {e}")
                            continue
            
            # If no sections found, create general sections from content blocks
            if not sections and original_content:
                content_str = str(original_content)
                if len(content_str) > 2000:
                    sections["General-Content"] = content_str[:2000] + "..."
                else:
                    sections["General-Content"] = content_str
        
        except Exception as e:
            logger.error(f"Error extracting sections: {e}")
        
        return sections
    
    def _extract_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract knowledge graph from response"""
        kg = {"entities": [], "relationships": []}
        
        try:
            if not response_text:
                return kg
            
            response_text = str(response_text)
            
            # Look for knowledge graph patterns in response
            kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
            kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
            
            if kg_match:
                kg_text = str(kg_match.group(1))
                
                # Extract entities
                entity_pattern = r'Entity:\s*([^,\n]+)'
                entities = re.findall(entity_pattern, kg_text, re.IGNORECASE)
                for entity in entities:
                    if entity and entity.strip():
                        kg["entities"].append(str(entity.strip()))
                
                # Extract relationships
                rel_pattern = r'Relationship:\s*([^,\n]+)\s*â†’\s*([^,\n]+)'
                relationships = re.findall(rel_pattern, kg_text, re.IGNORECASE)
                for rel in relationships:
                    if len(rel) >= 2 and rel[0] and rel[1]:
                        kg["relationships"].append({
                            "from": str(rel[0]).strip(), 
                            "to": str(rel[1]).strip()
                        })
        
        except Exception as e:
            logger.debug(f"Error extracting knowledge graph: {e}")
        
        return kg
    
    def _merge_knowledge_graphs(self, main_kg: Dict, chunk_kg: Dict):
        """Merge knowledge graphs from different chunks"""
        try:
            # Ensure main_kg has proper structure
            if not isinstance(main_kg, dict):
                main_kg = {"entities": [], "relationships": []}
            if "entities" not in main_kg:
                main_kg["entities"] = []
            if "relationships" not in main_kg:
                main_kg["relationships"] = []
            
            # Ensure chunk_kg is valid
            chunk_kg = ensure_dict(chunk_kg)
            
            # Merge entities
            chunk_entities = ensure_list(chunk_kg.get("entities", []))
            for entity in chunk_entities:
                entity_str = str(entity) if entity else ""
                if entity_str and entity_str not in main_kg["entities"]:
                    main_kg["entities"].append(entity_str)
            
            # Merge relationships
            existing_rels = set()
            for rel in main_kg["relationships"]:
                if isinstance(rel, dict):
                    rel_from = safe_get(rel, "from", "")
                    rel_to = safe_get(rel, "to", "")
                    if rel_from and rel_to:
                        existing_rels.add((str(rel_from), str(rel_to)))
            
            chunk_rels = ensure_list(chunk_kg.get("relationships", []))
            for rel in chunk_rels:
                if isinstance(rel, dict):
                    rel_from = safe_get(rel, "from", "")
                    rel_to = safe_get(rel, "to", "")
                    if rel_from and rel_to:
                        rel_tuple = (str(rel_from), str(rel_to))
                        if rel_tuple not in existing_rels:
                            main_kg["relationships"].append({
                                "from": str(rel_from),
                                "to": str(rel_to)
                            })
                            existing_rels.add(rel_tuple)
        
        except Exception as e:
            logger.debug(f"Error merging knowledge graphs: {e}")
    
    def _safe_json_extract(self, text: str) -> Optional[Dict]:
        """Safely extract JSON from text"""
        try:
            if not text:
                return None
            
            text = str(text)
            json_patterns = [
                r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
                r'\{.*?\}',
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text, re.DOTALL)
                for match in matches:
                    try:
                        result = json.loads(match)
                        if isinstance(result, dict):
                            return result
                    except:
                        continue
        except Exception as e:
            logger.debug(f"JSON extraction error: {e}")
        
        return None

# AGENT 2: Enhanced Geography Agent with Knowledge Graphs
class GeographyAgent:
    """Enhanced Agent 2: Country identification with knowledge graphs"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "GeographyAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Process geography with enhanced knowledge graphs"""
        
        logger.info(f"ðŸŒ {self.name}: Starting country identification with knowledge graphs")
        
        try:
            # Prepare comprehensive country data for LLM
            country_data = self._prepare_country_knowledge_graph()
            
            system_prompt = f"""You are a specialized Geography Agent using ReAct prompting with Knowledge Graph construction.

Your task: Identify countries in legal documents using geography data and build location knowledge graphs.

Available Geography Knowledge Graph:
Countries: {len(self.geography_handler.all_countries)} total
Sample countries: {json.dumps(dict(list(self.geography_handler.all_countries.items())[:20]), indent=2)}

ReAct Framework Enhanced:
1. REASONING: Analyze document for location references and adequacy context
2. ACTION: Identify countries and build location knowledge graph
3. OBSERVATION: Validate against geography data and adequacy patterns
4. KNOWLEDGE GRAPH: Create location-legal framework mapping
5. CONCLUSION: Provide verified country identification

CRITICAL: Verify ALL countries against geography data. Build comprehensive location knowledge graph."""

            # Combine all document content for comprehensive analysis
            all_content = ""
            parsed_sections = ensure_dict(state.get('parsed_sections', {}))
            for section_name, content in parsed_sections.items():
                if content:
                    all_content += f"\n[SECTION: {section_name}]\n{str(content)}\n"
            
            if not all_content.strip():
                all_content = str(state.get('document_text', ''))[:5000]
            
            user_prompt = f"""Identify countries using ReAct methodology with Knowledge Graph construction:

COMPLETE DOCUMENT CONTENT:
{all_content[:8000]}

Applicable Countries (from config): {state['metadata_config'].get('applicable_countries', [])}

Use ReAct with Location Knowledge Graphs:
1. REASONING: Analyze all sections for country references and adequacy contexts
2. ACTION: Build location knowledge graph with countries and legal statuses
3. OBSERVATION: Verify against geography data and adequacy patterns
4. KNOWLEDGE GRAPH: Map Countries â†’ Regions â†’ Adequacy_Status â†’ Legal_Framework
5. CONCLUSION: Provide complete verified country identification

Return country analysis in JSON format:
{{
  "mentioned_countries": ["ISO codes found"],
  "adequacy_countries": ["ISO codes with adequacy context"],
  "verification_status": "verified against geography data"
}}"""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            response_text = str(response.content) if response and response.content else ""
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract knowledge graph
            knowledge_graph = self._extract_location_knowledge_graph(response_text)
            
            # Extract and verify country results
            country_results = self._extract_and_verify_countries(response_text, all_content)
            
            # Update state
            state['identified_countries'] = country_results
            state['agent2_reasoning'] = reasoning_trace
            state['agent2_knowledge_graph'] = knowledge_graph
            state['current_agent'] = 'RuleExtractionAgent'
            
            mentioned_count = len(country_results.get('mentioned_countries', []))
            adequacy_count = len(country_results.get('adequacy_countries', []))
            kg_entities = len(knowledge_graph.get('entities', []))
            
            logger.info(f"âœ… {self.name}: Identified {mentioned_count} countries")
            logger.info(f"   ðŸ“ Adequacy countries: {adequacy_count}")
            logger.info(f"ðŸ“Š Location KG: {kg_entities} entities")
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: Processing failed: {e}")
            state['agent2_reasoning'] = [f"ERROR: {str(e)}"]
            state['agent2_knowledge_graph'] = {"entities": [], "relationships": []}
            state['identified_countries'] = {
                "mentioned_countries": [],
                "adequacy_countries": [],
                "verification_status": "failed"
            }
        
        return state
    
    def _prepare_country_knowledge_graph(self) -> Dict[str, Any]:
        """Prepare country knowledge graph for LLM context"""
        kg = {
            "regions": {},
            "countries_by_region": {},
            "total_countries": len(self.geography_handler.all_countries)
        }
        
        try:
            # Process geography data into knowledge graph format
            for region_name, region_data in self.geography_handler.geography_data.items():
                region_dict = ensure_dict(region_data)
                if 'countries' in region_dict:
                    countries_list = ensure_list(region_dict['countries'])
                    sample_countries = []
                    for country in countries_list[:10]:  # Sample first 10
                        if isinstance(country, dict):
                            iso2 = safe_get(country, 'iso2', '')
                            if iso2:
                                sample_countries.append(iso2)
                    kg["regions"][str(region_name)] = sample_countries
        except Exception as e:
            logger.debug(f"Error preparing country knowledge graph: {e}")
        
        return kg
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        try:
            if not response_text:
                return trace
            
            response_text = str(response_text)
            
            patterns = [
                (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
                (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
                (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
                (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
                (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
            ]
            
            for pattern, label in patterns:
                matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if match and match.strip():
                        trace.append(f"{label}: {str(match).strip()}")
        except Exception as e:
            logger.debug(f"Error extracting reasoning trace: {e}")
        
        return trace
    
    def _extract_location_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract location knowledge graph"""
        kg = {"entities": [], "relationships": [], "adequacy_mapping": {}}
        
        try:
            if not response_text:
                return kg
            
            response_text = str(response_text)
            
            # Extract from knowledge graph section
            kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
            kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
            
            if kg_match:
                kg_text = str(kg_match.group(1))
                
                # Extract location entities
                entity_patterns = [
                    r'Country:\s*([A-Z]{2})\s*-\s*([^,\n]+)',
                    r'Region:\s*([^,\n]+)',
                    r'Adequacy:\s*([^,\n]+)'
                ]
                
                for pattern in entity_patterns:
                    matches = re.findall(pattern, kg_text, re.IGNORECASE)
                    for match in matches:
                        if match:
                            if isinstance(match, tuple):
                                kg["entities"].extend([str(m) for m in match if m])
                            else:
                                kg["entities"].append(str(match))
        except Exception as e:
            logger.debug(f"Error extracting location knowledge graph: {e}")
        
        return kg
    
    def _extract_and_verify_countries(self, response_text: str, document_text: str) -> Dict[str, Any]:
        """Extract and verify countries against geography data"""
        try:
            # Try JSON extraction first
            json_data = self._safe_json_extract(response_text)
            if json_data and isinstance(json_data, dict):
                # Verify countries
                mentioned = ensure_list(json_data.get('mentioned_countries', []))
                adequacy = ensure_list(json_data.get('adequacy_countries', []))
                
                verified_mentioned = []
                for c in mentioned:
                    if c and self.geography_handler.is_valid_country(str(c)):
                        verified_mentioned.append(str(c).upper())
                
                verified_adequacy = []
                for c in adequacy:
                    if c and self.geography_handler.is_valid_country(str(c)):
                        verified_adequacy.append(str(c).upper())
                
                return {
                    "mentioned_countries": verified_mentioned,
                    "adequacy_countries": verified_adequacy,
                    "verification_status": "verified against geography data"
                }
            
            # Fallback: comprehensive text analysis
            mentioned_countries = self.geography_handler.find_countries_in_text(document_text)
            
            # Enhanced adequacy country detection
            adequacy_countries = []
            if document_text:
                text_lower = str(document_text).lower()
                
                adequacy_patterns = [
                    r'adequacy decision[^.]*?([A-Z]{2})',
                    r'adequate protection[^.]*?([A-Z]{2})',
                    r'([A-Z]{2})[^.]*?adequacy decision',
                    r'([A-Z]{2})[^.]*?adequate protection'
                ]
                
                for pattern in adequacy_patterns:
                    try:
                        matches = re.findall(pattern, document_text, re.IGNORECASE)
                        for match in matches:
                            if match and self.geography_handler.is_valid_country(str(match)):
                                adequacy_countries.append(str(match).upper())
                    except Exception as e:
                        logger.debug(f"Error in adequacy pattern matching: {e}")
                        continue
                
                # Also check for country names in adequacy contexts
                for iso in mentioned_countries:
                    try:
                        country_name = self.geography_handler.get_country_name(iso)
                        if country_name:
                            name_lower = str(country_name).lower()
                            adequacy_context_patterns = [
                                rf'{re.escape(name_lower)}[^.]*?adequacy',
                                rf'adequacy[^.]*?{re.escape(name_lower)}',
                            ]
                            
                            for pattern in adequacy_context_patterns:
                                if re.search(pattern, text_lower):
                                    adequacy_countries.append(iso)
                                    break
                    except Exception as e:
                        logger.debug(f"Error checking adequacy context: {e}")
                        continue
            
            return {
                "mentioned_countries": list(set(mentioned_countries)),
                "adequacy_countries": list(set(adequacy_countries)),
                "verification_status": "verified against geography data"
            }
        
        except Exception as e:
            logger.error(f"Error extracting and verifying countries: {e}")
            return {
                "mentioned_countries": [],
                "adequacy_countries": [],
                "verification_status": "error"
            }
    
    def _safe_json_extract(self, text: str) -> Optional[Dict]:
        """Safely extract JSON from text"""
        try:
            if not text:
                return None
            
            text = str(text)
            json_patterns = [
                r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
                r'\{.*?\}'
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text, re.DOTALL)
                for match in matches:
                    try:
                        result = json.loads(match)
                        if isinstance(result, dict):
                            return result
                    except:
                        continue
        except Exception as e:
            logger.debug(f"JSON extraction error: {e}")
        
        return None

# AGENT 3: Enhanced Rule Extraction Agent with Knowledge Graphs
class RuleExtractionAgent:
    """Enhanced Agent 3: Rule extraction with knowledge graphs and role assignment"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.name = "RuleExtractionAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Extract rules with enhanced knowledge graphs and role assignment"""
        
        logger.info(f"âš–ï¸ {self.name}: Starting rule extraction with knowledge graphs")
        
        try:
            system_prompt = """You are a specialized Rule Extraction Agent using ReAct prompting with Legal Knowledge Graph construction.

Your task: Extract structured rules for access and entitlements with role assignments and comprehensive legal relationships.

Required Rule Structure with Roles:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule in simple English",
  "conditions": [
    {
      "condition_id": "unique_condition_id",
      "condition_definition": "condition in simple English", 
      "fact": "data.category or user.role etc",
      "operator": "equal/in/greaterThan etc",
      "value": "comparison_value",
      "role": "controller/processor/joint_controller/data_subject"
    }
  ],
  "aggregated_roles": ["all_roles_in_this_rule"],
  "data_category": "Personal Data/Special Category Data etc",
  "domain": "access_and_entitlements",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework Enhanced:
1. REASONING: Analyze sections for legal obligations and build legal knowledge graph
2. ACTION: Extract rules with conditions and assign appropriate roles
3. OBSERVATION: Validate rule completeness and role assignments
4. KNOWLEDGE GRAPH: Update legal relationship mapping
5. CONCLUSION: Provide structured rules with roles

Focus Areas:
- Access rights and data subject requests
- Controller and processor obligations
- Consent and data processing requirements  
- Compliance timeframes and actions
- Role-specific requirements

CRITICAL: Assign appropriate roles to each condition. Aggregate all roles at rule level."""

            # Process all sections comprehensively
            parsed_sections = ensure_dict(state.get('parsed_sections', {}))
            all_sections_text = ""
            for section_name, content in parsed_sections.items():
                if content:
                    all_sections_text += f"\n\n[SECTION: {section_name}]\n{str(content)}\n"
            
            if not all_sections_text.strip():
                all_sections_text = str(state.get('document_text', ''))[:10000]
            
            user_prompt = f"""Extract structured rules using ReAct methodology with Legal Knowledge Graphs:

COMPLETE DOCUMENT CONTENT:
{all_sections_text[:10000]}

Available Context:
- Applicable Countries: {state['metadata_config'].get('applicable_countries', [])}
- Adequacy Countries: {state.get('identified_countries', {}).get('adequacy_countries', [])}

Use ReAct with Legal Knowledge Graphs to extract rules for access and entitlements.
Return JSON array of rules with complete structure and role assignments."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            response_text = str(response.content) if response and response.content else ""
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract knowledge graph
            knowledge_graph = self._extract_legal_knowledge_graph(response_text)
            
            # Extract rules with role assignments
            extracted_rules = self._extract_rules_with_roles(response_text, state)
            
            # Update state
            state['extracted_rules'] = extracted_rules
            state['agent3_reasoning'] = reasoning_trace
            state['agent3_knowledge_graph'] = knowledge_graph
            state['current_agent'] = 'ValidationAgent'
            
            kg_entities = len(knowledge_graph.get('entities', []))
            logger.info(f"âœ… {self.name}: Extracted {len(extracted_rules)} rules")
            logger.info(f"ðŸ“Š Legal KG: {kg_entities} entities")
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: Processing failed: {e}")
            state['agent3_reasoning'] = [f"ERROR: {str(e)}"]
            state['agent3_knowledge_graph'] = {"entities": [], "relationships": []}
            state['extracted_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        try:
            if not response_text:
                return trace
            
            response_text = str(response_text)
            
            patterns = [
                (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
                (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
                (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
                (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
                (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
            ]
            
            for pattern, label in patterns:
                matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if match and match.strip():
                        trace.append(f"{label}: {str(match).strip()}")
        except Exception as e:
            logger.debug(f"Error extracting reasoning trace: {e}")
        
        return trace
    
    def _extract_legal_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract legal knowledge graph"""
        kg = {"entities": [], "relationships": [], "rule_mappings": {}}
        
        try:
            if not response_text:
                return kg
            
            response_text = str(response_text)
            
            # Extract legal entities and relationships
            kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
            kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
            
            if kg_match:
                kg_text = str(kg_match.group(1))
                
                # Extract legal entities
                entity_patterns = [
                    r'Rule:\s*([^,\n]+)',
                    r'Condition:\s*([^,\n]+)',
                    r'Role:\s*(controller|processor|joint_controller|data_subject)',
                    r'Data_Category:\s*([^,\n]+)',
                    r'Action:\s*([^,\n]+)'
                ]
                
                for pattern in entity_patterns:
                    matches = re.findall(pattern, kg_text, re.IGNORECASE)
                    for match in matches:
                        if match and match.strip():
                            kg["entities"].append(str(match.strip()))
                
                # Extract relationships
                rel_pattern = r'(\w+)\s*â†’\s*(\w+):\s*([^,\n]+)'
                relationships = re.findall(rel_pattern, kg_text, re.IGNORECASE)
                for rel in relationships:
                    if len(rel) >= 3 and rel[0] and rel[1]:
                        kg["relationships"].append({
                            "from": str(rel[0]), 
                            "to": str(rel[1]), 
                            "type": str(rel[2]) if len(rel) > 2 else ""
                        })
        
        except Exception as e:
            logger.debug(f"Error extracting legal knowledge graph: {e}")
        
        return kg
    
    def _extract_rules_with_roles(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Extract rules with proper role assignments"""
        rules = []
        
        try:
            # Try JSON extraction first
            json_data = self._safe_json_extract(response_text)
            if json_data:
                if isinstance(json_data, list):
                    rules = ensure_list(json_data)
                elif isinstance(json_data, dict):
                    rules = [json_data]
            
            # If no JSON rules found, create from text analysis
            if not rules:
                rules = self._create_rules_from_comprehensive_analysis(state)
            
            # Enhance rules with role assignments
            enhanced_rules = []
            for rule in rules:
                if rule:  # Check if rule is not None or empty
                    enhanced_rule = self._enhance_rule_with_roles(rule, state)
                    if enhanced_rule:
                        enhanced_rules.append(enhanced_rule)
            
            return enhanced_rules
        
        except Exception as e:
            logger.error(f"Error extracting rules with roles: {e}")
            return []
    
    def _enhance_rule_with_roles(self, rule: Any, state: MultiAgentState) -> Dict:
        """Enhance rule with proper role assignments"""
        try:
            rule_dict = ensure_dict(rule)
            
            # Ensure conditions have roles
            conditions = ensure_list(rule_dict.get('conditions', []))
            enhanced_conditions = []
            
            for condition in conditions:
                condition_dict = ensure_dict(condition)
                
                # Assign role if not present
                if 'role' not in condition_dict or not condition_dict['role']:
                    condition_dict['role'] = self._assign_role_from_context(
                        condition_dict.get('condition_definition', '')
                    )
                
                # Ensure all required fields
                enhanced_condition = {
                    "condition_id": safe_get(condition_dict, "condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    "condition_definition": safe_get(condition_dict, "condition_definition", "Data processing condition"),
                    "fact": safe_get(condition_dict, "fact", "data.category"),
                    "operator": safe_get(condition_dict, "operator", "equal"),
                    "value": safe_get(condition_dict, "value", "Personal Data"),
                    "role": safe_get(condition_dict, "role", "controller")
                }
                enhanced_conditions.append(enhanced_condition)
            
            # Ensure at least one condition
            if not enhanced_conditions:
                enhanced_conditions.append({
                    "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                    "condition_definition": "When processing personal data",
                    "fact": "data.category",
                    "operator": "equal",
                    "value": "Personal Data",
                    "role": "controller"
                })
            
            # Aggregate roles from conditions
            aggregated_roles = list(set(
                cond.get('role', 'controller') for cond in enhanced_conditions
            ))
            
            # Enhanced rule structure
            enhanced_rule = {
                "rule_id": safe_get(rule_dict, "rule_id", f"rule_{uuid.uuid4().hex[:8]}"),
                "rule_definition": safe_get(rule_dict, "rule_definition", "Legal compliance requirement"),
                "conditions": enhanced_conditions,
                "aggregated_roles": aggregated_roles,
                "data_category": safe_get(rule_dict, "data_category", "Personal Data"),
                "domain": "access_and_entitlements",
                "action": safe_get(rule_dict, "action", "Must ensure compliance"),
                "reference": safe_get(rule_dict, "reference", "Document reference")
            }
            
            return enhanced_rule
        
        except Exception as e:
            logger.error(f"Error enhancing rule with roles: {e}")
            return {}
    
    def _assign_role_from_context(self, condition_text: str) -> str:
        """Assign role based on condition context"""
        try:
            if not condition_text:
                return 'controller'
            
            text_lower = str(condition_text).lower()
            
            # Role assignment patterns
            if any(pattern in text_lower for pattern in ['controller', 'determine purpose', 'processing decisions']):
                return 'controller'
            elif any(pattern in text_lower for pattern in ['processor', 'on behalf', 'process data for']):
                return 'processor'
            elif any(pattern in text_lower for pattern in ['joint controller', 'shared decision', 'jointly determine']):
                return 'joint_controller'
            elif any(pattern in text_lower for pattern in ['data subject', 'individual', 'person', 'user right']):
                return 'data_subject'
            else:
                return 'controller'  # Default
        except Exception as e:
            logger.debug(f"Error assigning role from context: {e}")
            return 'controller'
    
    def _create_rules_from_comprehensive_analysis(self, state: MultiAgentState) -> List[Dict]:
        """Create rules from comprehensive text analysis"""
        rules = []
        
        try:
            parsed_sections = ensure_dict(state.get('parsed_sections', {}))
            
            # Analyze all sections for legal obligations
            for section_name, content in parsed_sections.items():
                if not content:
                    continue
                
                content_str = str(content)
                sentences = re.split(r'[.!?]+', content_str)
                
                for i, sentence in enumerate(sentences):
                    if not sentence:
                        continue
                    
                    sentence = str(sentence).strip()
                    if len(sentence) < 30:  # Skip very short sentences
                        continue
                    
                    # Look for obligation patterns
                    obligation_patterns = [
                        r'must\s+([^.]+)',
                        r'shall\s+([^.]+)',
                        r'required\s+to\s+([^.]+)',
                        r'obligation\s+to\s+([^.]+)',
                        r'right\s+to\s+([^.]+)'
                    ]
                    
                    for pattern in obligation_patterns:
                        try:
                            match = re.search(pattern, sentence.lower())
                            if match:
                                action = str(match.group(1)).strip()
                                
                                # Create rule with role assignment
                                rule = {
                                    "rule_id": f"rule_{str(section_name).lower().replace('-', '_').replace(' ', '_')}_{i}_{uuid.uuid4().hex[:8]}",
                                    "rule_definition": sentence,
                                    "conditions": [{
                                        "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                                        "condition_definition": f"When processing data in {section_name}",
                                        "fact": "data.category",
                                        "operator": "equal",
                                        "value": "Personal Data",
                                        "role": self._assign_role_from_context(sentence)
                                    }],
                                    "aggregated_roles": [self._assign_role_from_context(sentence)],
                                    "data_category": "Personal Data",
                                    "domain": "access_and_entitlements",
                                    "action": action,
                                    "reference": f"{section_name} - Article/Text reference"
                                }
                                
                                rules.append(rule)
                                
                                # Limit to avoid too many rules
                                if len(rules) >= 20:
                                    break
                        except Exception as e:
                            logger.debug(f"Error processing obligation pattern: {e}")
                            continue
                    
                    if len(rules) >= 20:
                        break
                
                if len(rules) >= 20:
                    break
        
        except Exception as e:
            logger.error(f"Error creating rules from analysis: {e}")
        
        return rules
    
    def _safe_json_extract(self, text: str) -> Optional[Union[Dict, List]]:
        """Safely extract JSON from text"""
        try:
            if not text:
                return None
            
            text = str(text)
            # Look for JSON arrays and objects
            json_patterns = [
                r'\[.*?\]',  # Array pattern
                r'\{.*?\}'   # Object pattern
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text, re.DOTALL)
                for match in matches:
                    try:
                        result = json.loads(match)
                        if isinstance(result, (dict, list)):
                            return result
                    except:
                        continue
        except Exception as e:
            logger.debug(f"JSON extraction error: {e}")
        
        return None

# AGENT 4: Enhanced Validation Agent with Knowledge Graphs
class ValidationAgent:
    """Enhanced Agent 4: Rule validation with knowledge graphs"""
    
    def __init__(self, llm: ChatOpenAI, geography_handler: GeographyHandler):
        self.llm = llm
        self.geography_handler = geography_handler
        self.name = "ValidationAgent"
    
    def process(self, state: MultiAgentState) -> MultiAgentState:
        """Validate and structure rules with enhanced knowledge graphs"""
        
        logger.info(f"âœ… {self.name}: Starting rule validation with knowledge graphs")
        
        try:
            system_prompt = """You are a specialized Validation Agent using ReAct prompting with Validation Knowledge Graph construction.

Your task: Validate and structure extracted rules with comprehensive verification and knowledge relationships.

EXACT Required Format with Roles:
{
  "rule_id": "unique_identifier",
  "rule_definition": "complete rule definition in simple English",
  "applicable_countries": ["ISO codes from metadata config"],
  "adequacy_countries": ["verified ISO codes from document"],
  "conditions": [
    {
      "condition_id": "unique_condition_id",
      "condition_definition": "clear condition in simple English",
      "fact": "property to evaluate", 
      "operator": "comparison operator",
      "value": "value to compare",
      "role": "controller/processor/joint_controller/data_subject"
    }
  ],
  "aggregated_roles": ["all_roles_from_conditions"],
  "data_category": "primary data category",
  "domain": "access_and_entitlements",
  "action": "required action in simple English",
  "reference": "Level and article/text reference"
}

ReAct Framework Enhanced:
1. REASONING: Analyze rule structure and build validation knowledge graph
2. ACTION: Validate rules against format requirements and verify data
3. OBSERVATION: Check validation results and identify compliance issues
4. KNOWLEDGE GRAPH: Update validation status and issue tracking
5. CONCLUSION: Provide validated rules with compliance verification

CRITICAL: All countries must be verified against geography data. Roles must be properly assigned and aggregated."""

            extracted_rules = ensure_list(state.get('extracted_rules', []))
            metadata_config = ensure_dict(state.get('metadata_config', {}))
            identified_countries = ensure_dict(state.get('identified_countries', {}))
            
            user_prompt = f"""Validate and structure rules using ReAct methodology with Validation Knowledge Graphs:

EXTRACTED RULES FOR VALIDATION:
{json.dumps(extracted_rules[:3], indent=2) if extracted_rules else '[]'}
... (total: {len(extracted_rules)} rules)

Validation Context:
- Applicable Countries (config): {metadata_config.get('applicable_countries', [])}
- Adequacy Countries (verified): {identified_countries.get('adequacy_countries', [])}
- Geography Data Available: {len(self.geography_handler.all_countries)} countries

Validation Requirements:
- applicable_countries: MUST use metadata config countries
- adequacy_countries: MUST be verified actual countries (not regions like "EU")
- conditions: Each MUST have proper role assignment
- aggregated_roles: MUST include all roles from conditions

Return complete validated rules array in exact required format."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            response_text = str(response.content) if response and response.content else ""
            
            # Parse reasoning trace
            reasoning_trace = self._extract_reasoning_trace(response_text)
            
            # Extract validation knowledge graph
            knowledge_graph = self._extract_validation_knowledge_graph(response_text)
            
            # Validate and structure rules
            validated_rules = self._comprehensive_rule_validation(response_text, state)
            
            # Update state
            state['validated_rules'] = validated_rules
            state['agent4_reasoning'] = reasoning_trace
            state['agent4_knowledge_graph'] = knowledge_graph
            state['processing_complete'] = True
            
            kg_entities = len(knowledge_graph.get('entities', []))
            logger.info(f"âœ… {self.name}: Validated {len(validated_rules)} rules")
            logger.info(f"ðŸ“Š Validation KG: {kg_entities} entities")
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: Processing failed: {e}")
            state['agent4_reasoning'] = [f"ERROR: {str(e)}"]
            state['agent4_knowledge_graph'] = {"entities": [], "relationships": []}
            state['validated_rules'] = []
        
        return state
    
    def _extract_reasoning_trace(self, response_text: str) -> List[str]:
        """Extract enhanced reasoning trace"""
        trace = []
        
        try:
            if not response_text:
                return trace
            
            response_text = str(response_text)
            
            patterns = [
                (r'REASONING:\s*(.*?)(?=ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'REASONING'),
                (r'ACTION:\s*(.*?)(?=REASONING:|OBSERVATION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'ACTION'),
                (r'OBSERVATION:\s*(.*?)(?=REASONING:|ACTION:|KNOWLEDGE GRAPH:|CONCLUSION:|$)', 'OBSERVATION'),
                (r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)', 'KNOWLEDGE GRAPH'),
                (r'CONCLUSION:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|KNOWLEDGE GRAPH:|$)', 'CONCLUSION')
            ]
            
            for pattern, label in patterns:
                matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if match and match.strip():
                        trace.append(f"{label}: {str(match).strip()}")
        except Exception as e:
            logger.debug(f"Error extracting reasoning trace: {e}")
        
        return trace
    
    def _extract_validation_knowledge_graph(self, response_text: str) -> Dict[str, List]:
        """Extract validation knowledge graph"""
        kg = {"entities": [], "relationships": [], "validation_results": {}}
        
        try:
            if not response_text:
                return kg
            
            response_text = str(response_text)
            
            kg_pattern = r'KNOWLEDGE GRAPH:\s*(.*?)(?=REASONING:|ACTION:|OBSERVATION:|CONCLUSION:|$)'
            kg_match = re.search(kg_pattern, response_text, re.DOTALL | re.IGNORECASE)
            
            if kg_match:
                kg_text = str(kg_match.group(1))
                
                # Extract validation entities
                entity_patterns = [
                    r'Validation_Check:\s*([^,\n]+)',
                    r'Compliance_Status:\s*([^,\n]+)',
                    r'Issue:\s*([^,\n]+)',
                    r'Rule:\s*([^,\n]+)'
                ]
                
                for pattern in entity_patterns:
                    matches = re.findall(pattern, kg_text, re.IGNORECASE)
                    for match in matches:
                        if match and match.strip():
                            kg["entities"].append(str(match.strip()))
        
        except Exception as e:
            logger.debug(f"Error extracting validation knowledge graph: {e}")
        
        return kg
    
    def _comprehensive_rule_validation(self, response_text: str, state: MultiAgentState) -> List[Dict]:
        """Comprehensive rule validation with Pydantic v2"""
        validated_rules = []
        
        try:
            # Try to extract validated rules from response
            json_data = self._safe_json_extract(response_text)
            if json_data:
                if isinstance(json_data, list):
                    rules_data = ensure_list(json_data)
                elif isinstance(json_data, dict):
                    rules_data = [json_data]
                else:
                    rules_data = ensure_list(state.get('extracted_rules', []))
            else:
                rules_data = ensure_list(state.get('extracted_rules', []))
            
            metadata_config = ensure_dict(state.get('metadata_config', {}))
            identified_countries = ensure_dict(state.get('identified_countries', {}))
            
            # Validate each rule comprehensively
            for rule_data in rules_data:
                if not rule_data:
                    continue
                
                try:
                    rule_dict = ensure_dict(rule_data)
                    
                    # Ensure complete rule structure
                    validated_rule_data = {
                        "rule_id": safe_get(rule_dict, "rule_id", f"rule_{uuid.uuid4().hex[:8]}"),
                        "rule_definition": safe_get(rule_dict, "rule_definition", "Legal compliance requirement"),
                        "applicable_countries": ensure_list(metadata_config.get('applicable_countries', [])),
                        "adequacy_countries": self._verify_adequacy_countries(
                            ensure_list(identified_countries.get('adequacy_countries', []))
                        ),
                        "conditions": self._validate_conditions_with_roles(
                            ensure_list(rule_dict.get("conditions", []))
                        ),
                        "aggregated_roles": [],  # Will be populated from conditions
                        "data_category": safe_get(rule_dict, "data_category", "Personal Data"),
                        "domain": "access_and_entitlements",
                        "action": safe_get(rule_dict, "action", "Must ensure compliance with requirements"),
                        "reference": safe_get(rule_dict, "reference", "Document - Level reference")
                    }
                    
                    # Aggregate roles from conditions
                    roles = set()
                    for cond in validated_rule_data['conditions']:
                        if isinstance(cond, dict) and cond.get('role'):
                            roles.add(str(cond['role']))
                    validated_rule_data["aggregated_roles"] = list(roles)
                    
                    # Validate with Pydantic v2
                    extracted_rule = ExtractedRule.model_validate(validated_rule_data)
                    validated_rules.append(extracted_rule.model_dump())
                    
                except ValidationError as e:
                    logger.warning(f"Rule validation failed: {e}")
                    # Create fallback rule
                    fallback_rule = self._create_fallback_rule(rule_dict, state)
                    if fallback_rule:
                        validated_rules.append(fallback_rule)
                except Exception as e:
                    logger.error(f"Unexpected error in rule validation: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"Error in comprehensive rule validation: {e}")
        
        return validated_rules
    
    def _verify_adequacy_countries(self, adequacy_countries: List[str]) -> List[str]:
        """Verify adequacy countries are actual countries (not regions)"""
        verified = []
        
        try:
            for country in ensure_list(adequacy_countries):
                if not country:
                    continue
                
                country_str = str(country)
                
                # Skip regions like "EU", "EEA"
                if len(country_str) == 2 and self.geography_handler.is_valid_country(country_str):
                    verified.append(country_str.upper())
                elif len(country_str) > 2:
                    # Try to convert country name to ISO
                    iso = self.geography_handler.get_country_iso(country_str)
                    if iso:
                        verified.append(iso.upper())
        except Exception as e:
            logger.debug(f"Error verifying adequacy countries: {e}")
        
        return list(set(verified))
    
    def _validate_conditions_with_roles(self, conditions: List) -> List[Dict]:
        """Validate conditions with proper role assignments"""
        validated_conditions = []
        
        try:
            valid_roles = ['controller', 'processor', 'joint_controller', 'data_subject']
            
            conditions_list = ensure_list(conditions)
            
            for condition in conditions_list:
                condition_dict = ensure_dict(condition)
                
                # Ensure role is valid
                role = safe_get(condition_dict, 'role', 'controller')
                if str(role) not in valid_roles:
                    role = 'controller'  # Default
                
                validated_condition = {
                    "condition_id": safe_get(condition_dict, "condition_id", f"cond_{uuid.uuid4().hex[:8]}"),
                    "condition_definition": safe_get(condition_dict, "condition_definition", "Data processing condition"),
                    "fact": safe_get(condition_dict, "fact", "data.category"),
                    "operator": safe_get(condition_dict, "operator", "equal"),
                    "value": safe_get(condition_dict, "value", "Personal Data"),
                    "role": str(role)
                }
                validated_conditions.append(validated_condition)
            
            # Ensure at least one condition
            if not validated_conditions:
                validated_conditions.append({
                    "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                    "condition_definition": "When processing personal data",
                    "fact": "data.category",
                    "operator": "equal",
                    "value": "Personal Data",
                    "role": "controller"
                })
        
        except Exception as e:
            logger.error(f"Error validating conditions with roles: {e}")
            # Return default condition
            validated_conditions = [{
                "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                "condition_definition": "When processing personal data",
                "fact": "data.category",
                "operator": "equal",
                "value": "Personal Data",
                "role": "controller"
            }]
        
        return validated_conditions
    
    def _create_fallback_rule(self, rule_data: Dict, state: MultiAgentState) -> Dict:
        """Create fallback rule when validation fails"""
        try:
            rule_dict = ensure_dict(rule_data)
            metadata_config = ensure_dict(state.get('metadata_config', {}))
            
            return {
                "rule_id": f"rule_fallback_{uuid.uuid4().hex[:8]}",
                "rule_definition": str(safe_get(rule_dict, "rule_definition", "Legal compliance requirement")),
                "applicable_countries": ensure_list(metadata_config.get('applicable_countries', [])),
                "adequacy_countries": [],
                "conditions": [{
                    "condition_id": f"cond_{uuid.uuid4().hex[:8]}",
                    "condition_definition": "When processing personal data",
                    "fact": "data.category",
                    "operator": "equal",
                    "value": "Personal Data",
                    "role": "controller"
                }],
                "aggregated_roles": ["controller"],
                "data_category": "Personal Data",
                "domain": "access_and_entitlements",
                "action": "Must ensure compliance with data protection requirements",
                "reference": "Document - General requirement"
            }
        except Exception as e:
            logger.error(f"Error creating fallback rule: {e}")
            return {}
    
    def _safe_json_extract(self, text: str) -> Optional[Union[Dict, List]]:
        """Safely extract JSON from text"""
        try:
            if not text:
                return None
            
            text = str(text)
            json_patterns = [
                r'\[.*?\]',  # Array pattern
                r'\{.*?\}'   # Object pattern
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text, re.DOTALL)
                for match in matches:
                    try:
                        result = json.loads(match)
                        if isinstance(result, (dict, list)):
                            return result
                    except:
                        continue
        except Exception as e:
            logger.debug(f"JSON extraction error: {e}")
        
        return None

# Enhanced Multi-Agent Orchestrator
class MultiAgentLegalProcessor:
    """Enhanced multi-agent orchestrator with complete processing"""
    
    def __init__(self, geography_file: str):
        try:
            # Initialize LLM
            self.llm = ChatOpenAI(
                model=MODEL_NAME,
                openai_api_key=API_KEY,
                openai_api_base=BASE_URL
            )
            
            # Initialize geography handler
            self.geography_handler = GeographyHandler(geography_file)
            
            # Initialize enhanced agents
            self.doc_parser_agent = DocumentParserAgent(self.llm)
            self.geography_agent = GeographyAgent(self.llm, self.geography_handler)
            self.rule_extraction_agent = RuleExtractionAgent(self.llm)
            self.validation_agent = ValidationAgent(self.llm, self.geography_handler)
            
            # Create enhanced workflow
            self.workflow = self._create_enhanced_workflow()
            
            logger.info("ðŸ¤– Enhanced Multi-Agent Legal Processor initialized")
            logger.info(f"ðŸ“ Geography data: {len(self.geography_handler.all_countries)} countries")
            logger.info(f"âš™ï¸ Configuration: {MODEL_NAME}, chunks={CHUNK_SIZE}, overlap={OVERLAP_SIZE}")
        
        except Exception as e:
            logger.error(f"Error initializing MultiAgentLegalProcessor: {e}")
            raise
    
    def _create_enhanced_workflow(self) -> StateGraph:
        """Create enhanced workflow with LangGraph"""
        
        try:
            workflow = StateGraph(MultiAgentState)
            
            # Add enhanced agent nodes
            workflow.add_node("document_parser", self.doc_parser_agent.process)
            workflow.add_node("geography_agent", self.geography_agent.process)
            workflow.add_node("rule_extraction", self.rule_extraction_agent.process)
            workflow.add_node("validation_agent", self.validation_agent.process)
            
            # Set entry point
            workflow.set_entry_point("document_parser")
            
            # Sequential processing
            workflow.add_edge("document_parser", "geography_agent")
            workflow.add_edge("geography_agent", "rule_extraction")
            workflow.add_edge("rule_extraction", "validation_agent")
            workflow.add_edge("validation_agent", END)
            
            return workflow.compile(checkpointer=MemorySaver())
        
        except Exception as e:
            logger.error(f"Error creating enhanced workflow: {e}")
            raise
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process document with enhanced complete processing"""
        
        try:
            logger.info(f"ðŸ“„ Starting enhanced processing: {metadata_config.pdf_path}")
            
            # Extract complete PDF content
            pdf_processor = PDFProcessor()
            pdf_data = pdf_processor.extract_complete_text_from_pdf(metadata_config.pdf_path)
            
            if not pdf_data['complete_text']:
                raise ValueError("No text extracted from PDF")
            
            logger.info(f"ðŸ“Š PDF extracted: {pdf_data['total_pages']} pages, {pdf_data['total_length']} characters")
            
            # Initialize enhanced state
            initial_state = MultiAgentState(
                document_text=pdf_data['complete_text'],
                document_chunks=[],
                metadata_config=metadata_config.model_dump(),
                geography_data=self.geography_handler.geography_data,
                parsed_sections={},
                knowledge_graph={"entities": [], "relationships": []},
                identified_countries={},
                extracted_rules=[],
                validated_rules=[],
                agent1_reasoning=[],
                agent1_knowledge_graph={},
                agent2_reasoning=[],
                agent2_knowledge_graph={},
                agent3_reasoning=[],
                agent3_knowledge_graph={},
                agent4_reasoning=[],
                agent4_knowledge_graph={},
                current_agent="DocumentParserAgent",
                processing_complete=False,
                chunks_processed=0,
                total_chunks=0
            )
            
            # Run enhanced workflow
            config = {"configurable": {"thread_id": f"enhanced_{uuid.uuid4().hex[:8]}"}}
            
            final_state = self.workflow.invoke(initial_state, config)
            
            # Convert to Pydantic models
            validated_rules = []
            validated_rules_data = ensure_list(final_state.get('validated_rules', []))
            
            for rule_data in validated_rules_data:
                if not rule_data:
                    continue
                
                try:
                    rule_dict = ensure_dict(rule_data)
                    rule = ExtractedRule.model_validate(rule_dict)
                    validated_rules.append(rule)
                except ValidationError as e:
                    logger.warning(f"Final rule validation failed: {e}")
                    continue
                except Exception as e:
                    logger.error(f"Error processing rule: {e}")
                    continue
            
            # Log comprehensive summary
            self._log_enhanced_summary(final_state, validated_rules, pdf_data)
            
            return validated_rules
            
        except Exception as e:
            logger.error(f"Enhanced processing failed: {e}")
            raise
    
    def _log_enhanced_summary(self, final_state: MultiAgentState, rules: List[ExtractedRule], pdf_data: Dict):
        """Log comprehensive processing summary"""
        
        try:
            logger.info("ðŸŽ¯ Enhanced Multi-Agent Processing Complete!")
            logger.info(f"ðŸ“„ PDF: {pdf_data.get('total_pages', 0)} pages, {pdf_data.get('total_length', 0)} characters")
            logger.info(f"ðŸ§© Chunks processed: {final_state.get('chunks_processed', 0)}")
            logger.info(f"ðŸ“‚ Sections parsed: {len(final_state.get('parsed_sections', {}))}")
            logger.info(f"ðŸŒ Countries identified: {len(final_state.get('identified_countries', {}).get('mentioned_countries', []))}")
            logger.info(f"âš–ï¸ Rules extracted: {len(final_state.get('extracted_rules', []))}")
            logger.info(f"âœ… Rules validated: {len(rules)}")
            
            # Log knowledge graphs
            for i, agent_name in enumerate(['DocumentParserAgent', 'GeographyAgent', 'RuleExtractionAgent', 'ValidationAgent'], 1):
                kg = final_state.get(f'agent{i}_knowledge_graph', {})
                entities = len(ensure_list(kg.get('entities', [])))
                relationships = len(ensure_list(kg.get('relationships', [])))
                logger.info(f"ðŸ§  {agent_name} KG: {entities} entities, {relationships} relationships")
            
            # Log role distribution
            if rules:
                all_roles = set()
                for rule in rules:
                    if hasattr(rule, 'aggregated_roles'):
                        all_roles.update(ensure_list(rule.aggregated_roles))
                logger.info(f"ðŸ‘¥ Roles identified: {', '.join(all_roles)}")
                
                # Sample rule
                sample_rule = rules[0]
                logger.info("ðŸ“‹ Sample validated rule:")
                logger.info(f"   ðŸ†” ID: {sample_rule.rule_id}")
                logger.info(f"   ðŸ“ Definition: {sample_rule.rule_definition[:100]}...")
                logger.info(f"   ðŸŒ Applicable: {sample_rule.applicable_countries}")
                logger.info(f"   âœ… Adequacy: {sample_rule.adequacy_countries}")
                logger.info(f"   ðŸ‘¥ Roles: {sample_rule.aggregated_roles}")
                logger.info(f"   ðŸ·ï¸ Domain: {sample_rule.domain}")
        
        except Exception as e:
            logger.debug(f"Error logging enhanced summary: {e}")

# Enhanced Pipeline
class LegalRuleExtractionPipeline:
    """Enhanced pipeline with complete processing"""
    
    def __init__(self, geography_file: str):
        try:
            self.processor = MultiAgentLegalProcessor(geography_file)
            logger.info("ðŸš€ Enhanced Legal Rule Extraction Pipeline initialized")
        except Exception as e:
            logger.error(f"Error initializing pipeline: {e}")
            raise
    
    async def process_document(self, metadata_config: MetadataConfig) -> List[ExtractedRule]:
        """Process single document with complete coverage"""
        return await self.processor.process_document(metadata_config)
    
    async def process_multiple_documents(self, config_file: str) -> List[ExtractedRule]:
        """Process multiple documents"""
        logger.info(f"ðŸ“ Processing multiple documents from: {config_file}")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                configs_data = json.load(f)
        except Exception as e:
            logger.error(f"Config loading failed: {e}")
            raise
        
        all_rules = []
        
        for config_data in ensure_list(configs_data):
            if not config_data:
                continue
            
            try:
                config_dict = ensure_dict(config_data)
                metadata_config = MetadataConfig.model_validate(config_dict)
                rules = await self.process_document(metadata_config)
                all_rules.extend(rules)
                
            except ValidationError as e:
                logger.error(f"Invalid config: {e}")
                continue
            except Exception as e:
                logger.error(f"Processing failed for {config_dict.get('pdf_path', 'unknown')}: {e}")
                continue
        
        return all_rules
    
    def save_results(self, rules: List[ExtractedRule], output_dir: str):
        """Save enhanced results with knowledge graphs"""
        try:
            logger.info(f"ðŸ’¾ Saving enhanced results to {output_dir}")
            
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # Convert to dictionaries
            rules_dicts = []
            for rule in rules:
                if rule:
                    try:
                        rules_dicts.append(rule.model_dump())
                    except Exception as e:
                        logger.debug(f"Error converting rule to dict: {e}")
                        continue
            
            # Save JSON
            json_file = Path(output_dir) / "extracted_rules.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(rules_dicts, f, indent=2, ensure_ascii=False)
            
            # Save enhanced CSV with role analysis
            csv_file = Path(output_dir) / "extracted_rules.csv"
            if rules_dicts:
                flattened_rules = []
                for rule in rules_dicts:
                    try:
                        rule_dict = ensure_dict(rule)
                        flat_rule = {
                            "rule_id": safe_get(rule_dict, "rule_id", ""),
                            "rule_definition": safe_get(rule_dict, "rule_definition", ""),
                            "applicable_countries": json.dumps(ensure_list(rule_dict.get("applicable_countries", [])), ensure_ascii=False),
                            "adequacy_countries": json.dumps(ensure_list(rule_dict.get("adequacy_countries", [])), ensure_ascii=False),
                            "aggregated_roles": json.dumps(ensure_list(rule_dict.get("aggregated_roles", [])), ensure_ascii=False),
                            "data_category": safe_get(rule_dict, "data_category", ""),
                            "domain": safe_get(rule_dict, "domain", ""),
                            "action": safe_get(rule_dict, "action", ""),
                            "reference": safe_get(rule_dict, "reference", ""),
                            "conditions_count": len(ensure_list(rule_dict.get("conditions", []))),
                            "conditions_details": json.dumps(ensure_list(rule_dict.get("conditions", [])), ensure_ascii=False)
                        }
                        flattened_rules.append(flat_rule)
                    except Exception as e:
                        logger.debug(f"Error flattening rule: {e}")
                        continue
                
                if flattened_rules:
                    df = pd.DataFrame(flattened_rules)
                    df.to_csv(csv_file, index=False, encoding='utf-8')
            
            # Save analysis summary
            summary_file = Path(output_dir) / "processing_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "processing_summary": f"Enhanced processing completed: {len(rules)} rules",
                    "agents_used": [
                        "DocumentParserAgent (with knowledge graphs)",
                        "GeographyAgent (with location knowledge)",
                        "RuleExtractionAgent (with legal knowledge)",
                        "ValidationAgent (with compliance knowledge)"
                    ],
                    "methodology": "Enhanced Chain of Thought with ReAct prompting and Knowledge Graph construction",
                    "features": [
                        "NO truncation - complete document processing",
                        "Overlapping chunks for full coverage",
                        "Knowledge graph construction in all agents",
                        "Role assignment for all conditions",
                        "Geography data verification",
                        "Enhanced ReAct prompting",
                        "Error-free data type handling"
                    ],
                    "configuration": {
                        "model": MODEL_NAME,
                        "chunk_size": CHUNK_SIZE,
                        "overlap_size": OVERLAP_SIZE,
                        "max_chunks": MAX_CHUNKS
                    }
                }, f, indent=2)
            
            logger.info(f"âœ… Enhanced results saved: {json_file}, {csv_file}, {summary_file}")
        
        except Exception as e:
            logger.error(f"Error saving results: {e}")
            raise

# CLI Interface
async def main():
    """Enhanced CLI interface"""
    import argparse
    
    try:
        parser = argparse.ArgumentParser(description="Enhanced Multi-Agent Legal Rule Extraction")
        parser.add_argument("--config", required=True, help="Metadata configuration JSON file")
        parser.add_argument("--geography", required=True, help="Geography JSON file")
        parser.add_argument("--output", default="./output", help="Output directory")
        
        args = parser.parse_args()
        
        # Initialize enhanced pipeline
        pipeline = LegalRuleExtractionPipeline(args.geography)
        
        # Process documents
        rules = await pipeline.process_multiple_documents(args.config)
        
        # Save results
        pipeline.save_results(rules, args.output)
        
        logger.info(f"ðŸŽ‰ Enhanced processing complete! Generated {len(rules)} rules")
        
        # Display enhanced sample
        if rules:
            sample_rule = rules[0]
            logger.info("ðŸ“‹ Sample enhanced rule:")
            logger.info(f"   ðŸ†” Rule ID: {sample_rule.rule_id}")
            logger.info(f"   ðŸ“ Definition: {sample_rule.rule_definition[:100]}...")
            logger.info(f"   ðŸŒ Applicable Countries: {sample_rule.applicable_countries}")
            logger.info(f"   âœ… Adequacy Countries: {sample_rule.adequacy_countries}")
            logger.info(f"   ðŸ‘¥ Aggregated Roles: {sample_rule.aggregated_roles}")
            logger.info(f"   ðŸ·ï¸ Data Category: {sample_rule.data_category}")
            logger.info(f"   ðŸŽ¯ Domain: {sample_rule.domain}")
            logger.info(f"   âš¡ Action: {sample_rule.action[:100]}...")
            logger.info(f"   ðŸ“ Reference: {sample_rule.reference}")
            logger.info(f"   ðŸ§© Conditions: {len(sample_rule.conditions)} with roles")
        
        return 0
        
    except Exception as e:
        logger.error(f"Enhanced pipeline failed: {e}")
        return 1

if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
