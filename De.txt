#!/usr/bin/env python3
"""
Enterprise-Scale App Categorizer for Large Datasets (18k+ rows)
==============================================================

This system uses intelligent sampling and progressive analysis to handle massive datasets:

1. INTELLIGENT SAMPLING: Representative sample selection for deep analysis
2. TAXONOMY CREATION: Build comprehensive taxonomy from sample  
3. BATCH CLASSIFICATION: Efficient classification of remaining apps
4. QUALITY VALIDATION: Continuous quality checks and refinement
5. SCALABLE PROCESSING: Optimized for speed and cost efficiency

Designed to handle 18k+ applications efficiently while maintaining analytical depth.
"""

import pandas as pd
import openai
from openai import OpenAI
import json
import time
import numpy as np
from collections import Counter, defaultdict
import logging
from typing import List, Dict, Tuple, Set
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import concurrent.futures
from threading import Lock

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnterpriseScalePhi4Categorizer:
    """
    Enterprise-scale categorizer designed for 18k+ applications
    Uses intelligent sampling + batch processing + quality validation
    """
    
    def __init__(self, api_key: str, base_url: str, model_name: str = "microsoft/phi-4"):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name
        self.sample_size = 800  # Representative sample for deep analysis
        self.batch_size = 50    # Batch size for remaining classifications
        self.taxonomy = None
        self.topic_library = None
        self.processing_stats = {}
    def _safe_json_parse(self, content: str, fallback_data: Dict = None) -> Dict:
        """
        Safely parse JSON content with proper error handling
        """
        try:
            # Clean the content
            content = content.strip()
            
            # Remove markdown formatting
            if content.startswith('```json'):
                content = content[7:]
            if content.startswith('```'):
                content = content[3:]
            if content.endswith('```'):
                content = content[:-3]
            
            content = content.strip()
            
            # Parse JSON
            return json.loads(content)
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON decode error: {e}")
            logger.warning(f"Problematic content: {content[:200]}...")
            
            # Try to extract JSON from content
            try:
                # Look for JSON-like structure
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != 0:
                    json_content = content[start:end]
                    return json.loads(json_content)
            except:
                pass
            
            if fallback_data:
                logger.warning("Using fallback data due to JSON parse failure")
                return fallback_data
            else:
                raise e
        except Exception as e:
            logger.error(f"Unexpected error parsing JSON: {e}")
            if fallback_data:
                return fallback_data
            else:
                raise e
        
    def stage1_intelligent_sampling(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Stage 1: Create representative sample using clustering and diversity metrics
        """
        logger.info(f"Stage 1: Creating intelligent sample from {len(df)} applications...")
        
        # Basic text cleaning
        descriptions = df['description'].fillna("").astype(str)
        
        # Use TF-IDF to find diverse descriptions
        logger.info("Computing TF-IDF vectors for diversity analysis...")
        vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )
        
        tfidf_matrix = vectorizer.fit_transform(descriptions)
        
        # Cluster into groups to ensure diverse sampling
        n_clusters = min(50, len(df) // 100)  # Adaptive cluster count
        logger.info(f"Clustering into {n_clusters} groups for diverse sampling...")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(tfidf_matrix)
        
        # Sample from each cluster proportionally
        sample_indices = []
        cluster_counts = Counter(clusters)
        
        for cluster_id, count in cluster_counts.items():
            cluster_indices = np.where(clusters == cluster_id)[0]
            
            # Sample proportionally, minimum 2 per cluster
            sample_count = max(2, int((count / len(df)) * self.sample_size))
            sample_count = min(sample_count, len(cluster_indices))
            
            sampled = np.random.choice(cluster_indices, sample_count, replace=False)
            sample_indices.extend(sampled)
        
        # Ensure we have exactly sample_size items
        if len(sample_indices) > self.sample_size:
            sample_indices = np.random.choice(sample_indices, self.sample_size, replace=False)
        elif len(sample_indices) < self.sample_size:
            remaining_indices = list(set(range(len(df))) - set(sample_indices))
            additional_needed = self.sample_size - len(sample_indices)
            additional_indices = np.random.choice(remaining_indices, min(additional_needed, len(remaining_indices)), replace=False)
            sample_indices.extend(additional_indices)
        
        sample_df = df.iloc[sample_indices].copy()
        remaining_df = df.drop(sample_indices).copy()
        
        logger.info(f"Created representative sample: {len(sample_df)} apps for deep analysis")
        logger.info(f"Remaining for batch processing: {len(remaining_df)} apps")
        
        return sample_df, remaining_df
    
    def stage2_deep_topic_discovery(self, sample_df: pd.DataFrame) -> Dict:
        """
        Stage 2: Perform deep topic discovery on representative sample
        """
        logger.info("Stage 2: Deep topic discovery from representative sample...")
        
        # Process sample in chunks for topic discovery
        descriptions = sample_df['description'].tolist()
        chunk_size = 100
        all_discovered_topics = []
        
        for i in range(0, len(descriptions), chunk_size):
            chunk = descriptions[i:i+chunk_size]
            chunk_text = "\n\n".join([f"App {j+1}: {desc}" for j, desc in enumerate(chunk)])
            
            prompt = f"""
Analyze this diverse sample of {len(descriptions)} app/EUC descriptions and identify the core semantic topics that emerge.

Sample chunk ({len(chunk)} descriptions):
{chunk_text}

Extract 8-15 distinct semantic topics that represent the functional landscape. Each topic should:
- Represent a clear functional domain or capability area
- Be specific enough to be meaningful but broad enough to apply to multiple apps
- Capture the essence of what applications in this topic actually do

For each topic provide:
- Name: Concise, descriptive topic name
- Description: What this topic represents functionally
- Functional_scope: The scope of capabilities this topic covers
- Business_context: What business needs this topic addresses

Respond with JSON:
{{
  "discovered_topics": [
    {{
      "name": "discovered_topic_name",
      "description": "detailed description of what this topic represents",
      "functional_scope": "the range of capabilities covered by this topic",
      "business_context": "business needs and contexts this topic serves"
    }}
  ]
}}
"""

            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are an expert at analyzing large application portfolios and discovering semantic topics. Focus on functional domains that emerge from the data."},
                        {"role": "user", "content": prompt}
                    ]
                )
                
                content = response.choices[0].message.content.strip()
                fallback = {"discovered_topics": [{"name": f"Topic_{i}", "description": "Failed to discover", "functional_scope": "Unknown", "business_context": "Unknown"}]}
                chunk_topics = self._safe_json_parse(content, fallback)
                
                if 'discovered_topics' in chunk_topics:
                    all_discovered_topics.extend(chunk_topics['discovered_topics'])
                    logger.info(f"Discovered {len(chunk_topics['discovered_topics'])} topics from chunk {i//chunk_size + 1}")
                else:
                    logger.warning(f"No discovered_topics key in response for chunk {i//chunk_size + 1}")
                
                time.sleep(1)
                
            except Exception as e:
                logger.warning(f"Topic discovery failed for chunk {i//chunk_size + 1}: {e}")
                # Add fallback topics for this chunk
                fallback_topics = [{"name": f"Fallback_Topic_Chunk_{i//chunk_size + 1}", "description": "Topic discovery failed", "functional_scope": "Unknown", "business_context": "Unknown"}]
                all_discovered_topics.extend(fallback_topics)
        
        # Consolidate and refine discovered topics
        consolidated_topics = self._consolidate_topics(all_discovered_topics)
        self.topic_library = consolidated_topics
        
        logger.info(f"Consolidated to {len(consolidated_topics['topics'])} final topics")
        return consolidated_topics
    
    def _consolidate_topics(self, all_topics: List[Dict]) -> Dict:
        """
        Consolidate discovered topics to remove duplicates and create coherent set
        """
        logger.info("Consolidating discovered topics...")
        
        topics_text = "\n".join([
            f"- {topic['name']}: {topic['description']}"
            for topic in all_topics
        ])
        
        prompt = f"""
Consolidate these discovered topics into a coherent, non-overlapping set of semantic topics:

All Discovered Topics:
{topics_text}

Create a final consolidated set of 12-20 topics that:
- Eliminates duplicates and overlaps
- Covers the full functional spectrum
- Maintains semantic clarity and distinctiveness
- Represents meaningful functional domains

For each final topic, provide:
- Consolidated name
- Comprehensive description
- Functional scope
- Key indicators (terms/concepts that signal this topic)

Respond with JSON:
{{
  "topics": [
    {{
      "name": "consolidated_topic_name",
      "description": "comprehensive description of this functional domain",
      "functional_scope": "detailed scope of capabilities",
      "indicators": ["key", "terms", "concepts", "that", "indicate", "this", "topic"]
    }}
  ]
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at consolidating semantic topics into coherent taxonomies. Create distinct, non-overlapping functional domains."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content.strip()
            fallback = {"topics": all_topics[:15]}  # Limit to 15 topics as fallback
            return self._safe_json_parse(content, fallback)
            
        except Exception as e:
            logger.error(f"Topic consolidation failed: {e}")
            # Return limited original topics as fallback
            return {"topics": all_topics[:15]}
    
    def stage3_hierarchy_creation(self, topics: Dict, sample_df: pd.DataFrame) -> Dict:
        """
        Stage 3: Create hierarchical taxonomy from consolidated topics
        """
        logger.info("Stage 3: Creating hierarchical taxonomy from topics...")
        
        topics_text = "\n".join([
            f"- {topic['name']}: {topic['description']} (Scope: {topic['functional_scope']})"
            for topic in topics['topics']
        ])
        
        prompt = f"""
Create a comprehensive 3-level hierarchical taxonomy from these discovered topics:

Discovered Topics:
{topics_text}

Create exactly 3 levels:
- LEVEL 1 (Domains): 4-7 broad organizational domains
- LEVEL 2 (Areas): 3-6 functional areas within each domain
- LEVEL 3 (Capabilities): 2-5 specific capabilities within each area

Map ALL discovered topics into this hierarchy based on their semantic relationships and functional scope.

Design principles:
- Level 1 domains should represent the highest-level organizational functions
- Level 2 areas should group related functional specializations
- Level 3 capabilities should represent specific operational abilities
- Ensure every topic is mapped to exactly one capability
- Create logical hierarchical relationships

Respond with JSON:
{{
  "taxonomy": {{
    "level_1_domains": [
      {{
        "name": "domain_name_from_analysis",
        "description": "what this domain encompasses",
        "level_2_areas": [
          {{
            "name": "area_name_from_analysis",
            "description": "functional area description",
            "level_3_capabilities": [
              {{
                "name": "capability_name_from_analysis",
                "description": "specific capability description",
                "mapped_topics": ["list_of_topics_mapped_here"]
              }}
            ]
          }}
        ]
      }}
    ]
  }}
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at creating logical hierarchical taxonomies. Build coherent 3-level hierarchies that fully encompass all discovered topics."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content.strip()
            
            # Create fallback taxonomy structure
            fallback_taxonomy = {
                "taxonomy": {
                    "level_1_domains": [
                        {
                            "name": "Business Applications",
                            "description": "General business applications",
                            "level_2_areas": [
                                {
                                    "name": "Core Operations",
                                    "description": "Core business operations",
                                    "level_3_capabilities": [
                                        {
                                            "name": "General Functionality",
                                            "description": "General application functionality",
                                            "mapped_topics": [topic['name'] for topic in topics['topics']]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            }
            
            hierarchy = self._safe_json_parse(content, fallback_taxonomy)
            
            if 'taxonomy' in hierarchy:
                self.taxonomy = hierarchy['taxonomy']
                logger.info(f"Created hierarchy with {len(self.taxonomy['level_1_domains'])} domains")
                return hierarchy['taxonomy']
            else:
                logger.warning("No taxonomy key in response, using fallback")
                self.taxonomy = fallback_taxonomy['taxonomy']
                return fallback_taxonomy['taxonomy']
            
        except Exception as e:
            logger.error(f"Hierarchy creation failed: {e}")
            # Create minimal fallback taxonomy
            fallback_taxonomy = {
                "level_1_domains": [
                    {
                        "name": "Applications",
                        "description": "All applications",
                        "level_2_areas": [
                            {
                                "name": "General",
                                "description": "General applications",
                                "level_3_capabilities": [
                                    {
                                        "name": "Standard",
                                        "description": "Standard functionality",
                                        "mapped_topics": [topic['name'] for topic in topics['topics']]
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
            self.taxonomy = fallback_taxonomy
            return fallback_taxonomy
    
    def stage4_sample_classification(self, sample_df: pd.DataFrame, taxonomy: Dict) -> pd.DataFrame:
        """
        Stage 4: Classify sample using detailed analysis for quality validation
        """
        logger.info("Stage 4: Detailed classification of sample for quality validation...")
        
        # Create a copy to avoid modifying original
        result_df = sample_df.copy()
        
        sample_results = []
        batch_size = 25
        
        for i in range(0, len(result_df), batch_size):
            batch = result_df.iloc[i:i+batch_size]
            batch_results = self._classify_batch_detailed(batch, taxonomy)
            sample_results.extend(batch_results)
            
            logger.info(f"Sample classification: {min(i+batch_size, len(result_df))}/{len(result_df)}")
            time.sleep(1)
        
        # Add result columns if they don't exist
        result_columns = ['primary_topics', 'level_1_domain', 'level_2_area', 'level_3_capability', 
                         'confidence', 'classification_reasoning', 'hierarchy_path']
        
        for col in result_columns:
            if col not in result_df.columns:
                result_df[col] = None
        
        # Add results to dataframe
        for idx, result in enumerate(sample_results):
            if idx < len(result_df):
                for key, value in result.items():
                    if key in result_columns:
                        result_df.iloc[idx, result_df.columns.get_loc(key)] = value
        
        return result_df
    
    def _classify_batch_detailed(self, batch_df: pd.DataFrame, taxonomy: Dict) -> List[Dict]:
        """
        Detailed classification for sample batch
        """
        batch_text = ""
        for idx, row in batch_df.iterrows():
            batch_text += f"\nApp {len(batch_text.split('App '))}: {row['name']} - {row['description']}"
        
        taxonomy_text = self._format_taxonomy_for_prompt(taxonomy)
        
        prompt = f"""
Classify each application into the hierarchical taxonomy using detailed analysis:

Taxonomy:
{taxonomy_text}

Applications:
{batch_text}

For each app, determine:
1. Which topics from our topic library best match the application
2. Map to the most appropriate Level 1 Domain, Level 2 Area, Level 3 Capability
3. Confidence level (High/Medium/Low) based on clarity of mapping
4. Detailed reasoning for the classification

Respond with JSON:
{{
  "1": {{
    "primary_topics": ["most_relevant_topics"],
    "level_1_domain": "mapped_domain",
    "level_2_area": "mapped_area",
    "level_3_capability": "mapped_capability",
    "confidence": "High/Medium/Low",
    "classification_reasoning": "detailed explanation of classification logic",
    "hierarchy_path": "Domain > Area > Capability"
  }}
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at precise application classification. Use detailed analysis to map applications to taxonomies accurately."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content.strip()
            
            # Create fallback result
            fallback_result = {}
            for i in range(1, len(batch_df) + 1):
                fallback_result[str(i)] = {
                    "primary_topics": ["Unclassified"],
                    "level_1_domain": "Unknown",
                    "level_2_area": "Unknown", 
                    "level_3_capability": "Unknown",
                    "confidence": "Low",
                    "classification_reasoning": "Classification failed - using fallback",
                    "hierarchy_path": "Unknown > Unknown > Unknown"
                }
            
            results = self._safe_json_parse(content, fallback_result)
            
            # Convert to list format
            batch_results = []
            for i in range(1, len(batch_df) + 1):
                if str(i) in results:
                    batch_results.append(results[str(i)])
                else:
                    batch_results.append(fallback_result[str(i)])
            
            return batch_results
            
        except Exception as e:
            logger.warning(f"Detailed classification failed: {e}")
            return [{
                "primary_topics": ["Error"],
                "level_1_domain": "Unknown",
                "level_2_area": "Unknown", 
                "level_3_capability": "Unknown",
                "confidence": "Low",
                "classification_reasoning": f"Error: {str(e)}",
                "hierarchy_path": "Unknown > Unknown > Unknown"
            } for _ in range(len(batch_df))]
    
    def stage5_quality_validation(self, classified_sample: pd.DataFrame) -> Dict:
        """
        Stage 5: Validate classification quality and identify improvements
        """
        logger.info("Stage 5: Quality validation of sample classifications...")
        
        # Ensure required columns exist
        if 'confidence' not in classified_sample.columns:
            logger.warning("Confidence column missing, assuming Low confidence for all")
            classified_sample['confidence'] = 'Low'
        
        if 'level_1_domain' not in classified_sample.columns:
            logger.warning("Domain column missing, assuming Unknown for all")
            classified_sample['level_1_domain'] = 'Unknown'
        
        # Analyze classification quality
        confidence_dist = Counter(classified_sample['confidence'].fillna('Low'))
        domain_dist = Counter(classified_sample['level_1_domain'].fillna('Unknown'))
        
        # Find quality issues
        low_confidence = classified_sample[classified_sample['confidence'] == 'Low']
        validation_stats = {
            'total_classified': len(classified_sample),
            'high_confidence': confidence_dist.get('High', 0),
            'medium_confidence': confidence_dist.get('Medium', 0),
            'low_confidence': confidence_dist.get('Low', 0),
            'confidence_rate': confidence_dist.get('High', 0) / len(classified_sample) if len(classified_sample) > 0 else 0,
            'domain_distribution': dict(domain_dist),
            'quality_issues': len(low_confidence)
        }
        
        logger.info(f"Quality validation - High confidence rate: {validation_stats['confidence_rate']:.2%}")
        return validation_stats
    
    def stage6_batch_classification(self, remaining_df: pd.DataFrame, taxonomy: Dict) -> pd.DataFrame:
        """
        Stage 6: Efficient batch classification of remaining applications
        """
        logger.info(f"Stage 6: Batch classification of {len(remaining_df)} remaining applications...")
        
        # Create a copy to avoid modifying original
        result_df = remaining_df.copy().reset_index(drop=True)
        
        all_results = []
        
        # Process in parallel batches for efficiency
        batch_size = self.batch_size
        total_batches = (len(result_df) + batch_size - 1) // batch_size
        
        for i in range(0, len(result_df), batch_size):
            batch = result_df.iloc[i:i+batch_size]
            batch_results = self._classify_batch_efficient(batch, taxonomy)
            all_results.extend(batch_results)
            
            batch_num = i // batch_size + 1
            logger.info(f"Batch classification: {batch_num}/{total_batches} ({min(i+batch_size, len(result_df))}/{len(result_df)})")
            time.sleep(0.5)  # Rate limiting
        
        # Add result columns if they don't exist
        result_columns = ['primary_topics', 'level_1_domain', 'level_2_area', 'level_3_capability', 
                         'confidence', 'hierarchy_path', 'classification_reasoning']
        
        for col in result_columns:
            if col not in result_df.columns:
                result_df[col] = None
        
        # Add results to remaining dataframe
        for idx, result in enumerate(all_results):
            if idx < len(result_df):
                for key, value in result.items():
                    if key in result_columns:
                        result_df.iloc[idx, result_df.columns.get_loc(key)] = value
        
        return result_df
    
    def _classify_batch_efficient(self, batch_df: pd.DataFrame, taxonomy: Dict) -> List[Dict]:
        """
        Efficient classification for large batches
        """
        batch_text = ""
        for idx, row in batch_df.iterrows():
            desc_short = row['description'][:200] + "..." if len(row['description']) > 200 else row['description']
            batch_text += f"\n{len(batch_text.split(chr(10)))}: {row['name']} - {desc_short}"
        
        # Simplified taxonomy for efficiency
        domains_text = "\n".join([
            f"- {domain['name']}: {domain['description']}"
            for domain in taxonomy['level_1_domains']
        ])
        
        prompt = f"""
Efficiently classify these applications into the taxonomy domains and areas:

Available Domains:
{domains_text}

Applications to classify:
{batch_text}

For each app, provide quick but accurate classification:
1. Primary domain match
2. Most appropriate area within that domain  
3. Best capability match
4. Confidence level

Focus on speed while maintaining accuracy. Use domain descriptions to guide classification.

Respond with JSON where keys are app numbers:
{{
  "1": {{
    "level_1_domain": "best_domain_match",
    "level_2_area": "best_area_match",
    "level_3_capability": "best_capability_match", 
    "confidence": "High/Medium/Low",
    "hierarchy_path": "Domain > Area > Capability"
  }}
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert at efficient application classification. Prioritize speed while maintaining classification accuracy."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = response.choices[0].message.content.strip()
            
            # Create fallback result
            fallback_result = {}
            for i in range(1, len(batch_df) + 1):
                fallback_result[str(i)] = {
                    "level_1_domain": "Unknown",
                    "level_2_area": "Unknown",
                    "level_3_capability": "Unknown", 
                    "confidence": "Low",
                    "hierarchy_path": "Unknown > Unknown > Unknown"
                }
            
            results = self._safe_json_parse(content, fallback_result)
            
            # Convert to list format and add missing fields
            batch_results = []
            for i in range(1, len(batch_df) + 1):
                if str(i) in results:
                    result = results[str(i)]
                    # Ensure all required fields are present
                    result['primary_topics'] = result.get('primary_topics', ["Batch_Classified"])
                    result['classification_reasoning'] = result.get('classification_reasoning', "Efficient batch classification")
                    if 'hierarchy_path' not in result:
                        result['hierarchy_path'] = f"{result.get('level_1_domain', 'Unknown')} > {result.get('level_2_area', 'Unknown')} > {result.get('level_3_capability', 'Unknown')}"
                    batch_results.append(result)
                else:
                    fallback = fallback_result[str(i)]
                    fallback['primary_topics'] = ["Unclassified"]
                    fallback['classification_reasoning'] = "Batch classification failed"
                    batch_results.append(fallback)
            
            return batch_results
            
        except Exception as e:
            logger.warning(f"Batch classification failed: {e}")
            return [{
                "primary_topics": ["Error"],
                "level_1_domain": "Unknown",
                "level_2_area": "Unknown",
                "level_3_capability": "Unknown", 
                "confidence": "Low",
                "hierarchy_path": "Unknown > Unknown > Unknown",
                "classification_reasoning": f"Batch error: {str(e)}"
            } for _ in range(len(batch_df))]
    
    def _format_taxonomy_for_prompt(self, taxonomy: Dict) -> str:
        """
        Format taxonomy for use in prompts
        """
        result = []
        for domain in taxonomy['level_1_domains']:
            result.append(f"\nDOMAIN: {domain['name']} - {domain['description']}")
            for area in domain['level_2_areas']:
                result.append(f"  AREA: {area['name']} - {area['description']}")
                for capability in area['level_3_capabilities']:
                    result.append(f"    CAPABILITY: {capability['name']} - {capability['description']}")
        return '\n'.join(result)
    
    def process_large_dataset(self, csv_file_path: str) -> Tuple[pd.DataFrame, Dict, Dict]:
        """
        Execute complete enterprise-scale processing pipeline
        """
        logger.info("Starting enterprise-scale processing pipeline...")
        start_time = time.time()
        
        # Read and validate dataset
        logger.info(f"Reading dataset from {csv_file_path}...")
        df = pd.read_csv(csv_file_path)
        
        if 'name' not in df.columns or 'description' not in df.columns:
            raise ValueError("CSV must contain 'name' and 'description' columns")
        
        logger.info(f"Dataset loaded: {len(df)} applications")
        
        # Execute pipeline stages
        sample_df, remaining_df = self.stage1_intelligent_sampling(df)
        topics = self.stage2_deep_topic_discovery(sample_df)
        taxonomy = self.stage3_hierarchy_creation(topics, sample_df)
        classified_sample = self.stage4_sample_classification(sample_df, taxonomy)
        quality_stats = self.stage5_quality_validation(classified_sample)
        classified_remaining = self.stage6_batch_classification(remaining_df, taxonomy)
        
        # Combine results
        # Reset indices to ensure proper concatenation
        classified_sample = classified_sample.reset_index(drop=True)
        classified_remaining = classified_remaining.reset_index(drop=True)
        
        # Ensure both DataFrames have the same columns
        all_columns = set(classified_sample.columns) | set(classified_remaining.columns)
        for col in all_columns:
            if col not in classified_sample.columns:
                classified_sample[col] = None
            if col not in classified_remaining.columns:
                classified_remaining[col] = None
        
        # Reorder columns to match
        column_order = list(classified_sample.columns)
        classified_remaining = classified_remaining[column_order]
        
        final_df = pd.concat([classified_sample, classified_remaining], ignore_index=True)
        
        # Calculate final statistics
        processing_time = time.time() - start_time
        
        # Safe confidence distribution calculation
        confidence_values = final_df['confidence'].fillna('Low')
        final_confidence_dist = dict(Counter(confidence_values))
        
        final_stats = {
            'processing_time_minutes': processing_time / 60,
            'total_applications': len(final_df),
            'sample_size': len(sample_df),
            'batch_processed': len(remaining_df),
            'final_confidence_distribution': final_confidence_dist,
            'domains_created': len(taxonomy['level_1_domains']),
            'topics_discovered': len(topics['topics']),
            'quality_validation': quality_stats
        }
        
        analysis_results = {
            'topics_library': topics,
            'hierarchical_taxonomy': taxonomy,
            'processing_statistics': final_stats,
            'sample_quality': quality_stats
        }
        
        logger.info(f"Enterprise processing complete in {processing_time/60:.1f} minutes")
        return final_df, taxonomy, analysis_results
    
    def generate_enterprise_report(self, df: pd.DataFrame, taxonomy: Dict, analysis: Dict) -> str:
        """
        Generate comprehensive enterprise analysis report
        """
        stats = analysis['processing_statistics']
        
        report = []
        report.append("=" * 90)
        report.append("ENTERPRISE-SCALE APPLICATION CATEGORIZATION REPORT")
        report.append(f"Dataset Size: {stats['total_applications']:,} Applications")
        report.append("=" * 90)
        report.append("")
        
        # Executive Summary
        report.append("EXECUTIVE SUMMARY")
        report.append("-" * 30)
        report.append(f"Total Applications Processed: {stats['total_applications']:,}")
        report.append(f"Processing Time: {stats['processing_time_minutes']:.1f} minutes")
        report.append(f"Sample Size for Deep Analysis: {stats['sample_size']:,}")
        report.append(f"Batch Processed: {stats['batch_processed']:,}")
        report.append(f"Domains Created: {stats['domains_created']}")
        report.append(f"Semantic Topics Discovered: {stats['topics_discovered']}")
        report.append("")
        
        # Quality Metrics
        report.append("QUALITY METRICS")
        report.append("-" * 20)
        total_apps = stats['total_applications']
        confidence_dist = stats['final_confidence_distribution']
        high_conf_rate = confidence_dist.get('High', 0) / total_apps * 100
        med_conf_rate = confidence_dist.get('Medium', 0) / total_apps * 100
        low_conf_rate = confidence_dist.get('Low', 0) / total_apps * 100
        
        report.append(f"High Confidence Classifications: {confidence_dist.get('High', 0):,} ({high_conf_rate:.1f}%)")
        report.append(f"Medium Confidence Classifications: {confidence_dist.get('Medium', 0):,} ({med_conf_rate:.1f}%)")
        report.append(f"Low Confidence Classifications: {confidence_dist.get('Low', 0):,} ({low_conf_rate:.1f}%)")
        report.append("")
        
        # Discovered Topics
        report.append("DISCOVERED SEMANTIC TOPICS")
        report.append("-" * 35)
        for topic in analysis['topics_library']['topics']:
            report.append(f"‚Ä¢ {topic['name']}")
            report.append(f"  Scope: {topic['functional_scope']}")
            report.append("")
        
        # Taxonomy Structure
        report.append("HIERARCHICAL TAXONOMY")
        report.append("-" * 30)
        domain_values = df['level_1_domain'].fillna('Unknown')
        domain_counts = Counter(domain_values)
        
        for domain in taxonomy['level_1_domains']:
            domain_count = domain_counts.get(domain['name'], 0)
            report.append(f"\nüìÅ {domain['name']} ({domain_count:,} applications)")
            report.append(f"   {domain['description']}")
            
            for area in domain.get('level_2_areas', []):
                area_count = len(df[df['level_2_area'] == area['name']]) if 'level_2_area' in df.columns else 0
                report.append(f"   ‚îî‚îÄ‚îÄ üìÇ {area['name']} ({area_count:,} apps)")
                
                for capability in area.get('level_3_capabilities', []):
                    cap_count = len(df[df['level_3_capability'] == capability['name']]) if 'level_3_capability' in df.columns else 0
                    report.append(f"       ‚îî‚îÄ‚îÄ üìÑ {capability['name']} ({cap_count:,} apps)")
        
        # Distribution Analysis
        report.append("\n\nDISTRIBUTION ANALYSIS")
        report.append("-" * 30)
        
        report.append("Top Domains by Application Count:")
        for domain, count in domain_counts.most_common(10):
            percentage = (count / len(df)) * 100
            report.append(f"  {domain:40} {count:6,} apps ({percentage:5.1f}%)")
        
        return "\n".join(report)

def validate_dependencies():
    """
    Validate that all required dependencies are installed
    """
    missing_packages = []
    
    try:
        import pandas
    except ImportError:
        missing_packages.append("pandas")
    
    try:
        import openai
    except ImportError:
        missing_packages.append("openai")
    
    try:
        import sklearn
    except ImportError:
        missing_packages.append("scikit-learn")
    
    try:
        import numpy
    except ImportError:
        missing_packages.append("numpy")
    
    if missing_packages:
        print("ERROR: Missing required packages:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nPlease install them using:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def main():
    """
    Execute enterprise-scale categorization
    """
    
    # Validate dependencies first
    if not validate_dependencies():
        return
    
    # ================================
    # CONFIGURATION
    # ================================
    
    API_KEY = "your_phi4_api_key_here"
    BASE_URL = "https://api.deepinfra.com/v1/openai"
    MODEL_NAME = "microsoft/phi-4"
    
    INPUT_CSV = "apps_data.csv"  # Your 18k row dataset
    OUTPUT_CSV = "enterprise_categorized_apps.csv"
    TAXONOMY_FILE = "enterprise_taxonomy.json"
    ANALYSIS_FILE = "enterprise_analysis.json" 
    REPORT_FILE = "enterprise_categorization_report.txt"
    
    # ================================
    # PROCESSING
    # ================================
    
    try:
        categorizer = EnterpriseScalePhi4Categorizer(
            api_key=API_KEY,
            base_url=BASE_URL,
            model_name=MODEL_NAME
        )
        
        # Execute enterprise pipeline
        df_result, taxonomy, analysis = categorizer.process_large_dataset(INPUT_CSV)
        
        # Generate report
        report = categorizer.generate_enterprise_report(df_result, taxonomy, analysis)
        
        # Save results
        logger.info("Saving enterprise results...")
        df_result.to_csv(OUTPUT_CSV, index=False)
        
        with open(TAXONOMY_FILE, 'w', encoding='utf-8') as f:
            json.dump(taxonomy, f, indent=2, ensure_ascii=False)
        
        with open(ANALYSIS_FILE, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)
        
        with open(REPORT_FILE, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Display summary
        stats = analysis['processing_statistics']
        print("\n" + "="*90)
        print("ENTERPRISE-SCALE CATEGORIZATION COMPLETE!")
        print("="*90)
        print(f"‚úì Processed {stats['total_applications']:,} applications in {stats['processing_time_minutes']:.1f} minutes")
        print(f"‚úì Created {stats['domains_created']}-domain taxonomy from {stats['topics_discovered']} discovered topics")
        print(f"‚úì Used intelligent sampling of {stats['sample_size']:,} apps for deep analysis")
        print(f"‚úì Batch processed {stats['batch_processed']:,} remaining applications")
        high_conf = stats['final_confidence_distribution'].get('High', 0)
        print(f"‚úì Achieved {high_conf:,} high-confidence classifications ({high_conf/stats['total_applications']*100:.1f}%)")
        print(f"\nüìÅ Enterprise results saved:")
        print(f"   ‚Ä¢ Full dataset: {OUTPUT_CSV}")
        print(f"   ‚Ä¢ Taxonomy: {TAXONOMY_FILE}")
        print(f"   ‚Ä¢ Analysis: {ANALYSIS_FILE}")
        print(f"   ‚Ä¢ Report: {REPORT_FILE}")
        
        print(f"\n{report}")
        
    except Exception as e:
        logger.error(f"Enterprise processing failed: {e}")
        raise

if __name__ == "__main__":
    main()
