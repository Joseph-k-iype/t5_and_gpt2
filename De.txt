"""
TermMatchingAgent - Agent responsible for finding and ranking business terms.

Enhanced with LangGraph for agentic RAG capabilities to improve matching accuracy.
"""
import logging
import json
import re
from typing import List, Dict, Any, Optional, Tuple
import time

from app.core.embedding import MyDocument
from app.config.settings import get_llm
from app.agents.agentic_rag.graph import AgenticRagGraph

logger = logging.getLogger(__name__)

# Constants for default configuration
CANDIDATE_FETCH_MULTIPLIER = 5  # Fetch 5x the number of final results needed
MIN_CANDIDATES_TO_FETCH = 20  # Fetch at least 20 candidates
DEFAULT_INITIAL_THRESHOLD = 0.2  # Lower threshold for broader retrieval

class TermMatchingAgent:
    """
    Agent for matching terms using an agentic RAG approach with LangGraph.
    
    This agent enhances term matching by combining vector search, keyword matching,
    synonym expansion, and LLM-based relevance evaluation in a graph-based workflow.
    """
    
    def __init__(self, business_term_manager):
        """
        Initialize the TermMatchingAgent.

        Args:
            business_term_manager: An instance of BusinessTermManager
        """
        self.bt_manager = business_term_manager
        self.embedding_client = business_term_manager.embedding_client
        
        try:
            # Initialize LLM
            self.llm = get_llm()
            logger.info("TermMatchingAgent initialized with LLM.")
            
            # Initialize Agentic RAG graph
            self.rag_graph = AgenticRagGraph(business_term_manager)
            logger.info("Agentic RAG graph initialized.")
            
        except Exception as e:
            logger.error(f"Failed to initialize TermMatchingAgent: {e}", exc_info=True)
            self.llm = None
            self.rag_graph = None
            logger.warning("TermMatchingAgent will fall back to basic vector search due to initialization failure.")
    
    async def _enhance_results_with_llm(self, element_name: str, element_description: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Use LLM to enhance vector results with semantic understanding.
        
        Args:
            element_name: Name of the element to match
            element_description: Description of the element
            candidates: List of candidate terms from vector search
            
        Returns:
            Enhanced list of candidates with adjusted similarity scores
        """
        if not self.llm or not candidates:
            return candidates
        
        try:
            # Format candidates for LLM analysis
            candidates_text = "\n".join([
                f"Candidate {i+1}:\nName: {c.get('name', '')}\n"
                f"Description: {c.get('description', '')}\n"
                f"Similarity: {c.get('similarity', 0.0):.3f}"
                for i, c in enumerate(candidates[:10])  # Limit to top 10
            ])
            
            prompt = f"""
You are a data governance expert. Evaluate how well these business terms match:

Data Element:
Name: {element_name}
Description: {element_description}

Candidates:
{candidates_text}

For each candidate, evaluate its semantic relevance considering:
1. Conceptual alignment beyond word matching
2. Business terminology equivalence (e.g., "account number" = "account identifier")
3. Domain context and purpose alignment

Return JSON array: [
  {{
    "index": 0,
    "adjusted_score": 0.85,
    "reasoning": "Brief explanation"
  }}
]
"""
            
            # Get LLM evaluation
            response = await self.llm.ainvoke(prompt)
            
            # Process response (parse JSON, handle errors)
            import json, re
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                evaluations = json.loads(json_match.group(0))
                enhanced_candidates = candidates.copy()
                
                # Update scores based on LLM evaluation
                for eval_item in evaluations:
                    idx = eval_item.get("index")
                    if isinstance(idx, int) and 0 <= idx < len(enhanced_candidates):
                        enhanced_candidates[idx]["original_similarity"] = enhanced_candidates[idx].get("similarity", 0.0)
                        enhanced_candidates[idx]["similarity"] = eval_item.get("adjusted_score", 0.0)
                        enhanced_candidates[idx]["llm_reasoning"] = eval_item.get("reasoning", "")
                
                # Sort by updated similarity
                enhanced_candidates.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
                return enhanced_candidates
                
            return candidates
        except Exception as e:
            logger.error(f"Error in LLM enhancement: {e}")
            return candidates
    
    async def find_matching_terms(
        self,
        element_id: str,
        element_name: str,
        element_description: str,
        top_k: int,
        cdm_context: Optional[str] = None,
        example_context: Optional[str] = None,
        process_name_context: Optional[str] = None,
        process_description_context: Optional[str] = None,
        initial_threshold: float = DEFAULT_INITIAL_THRESHOLD
    ) -> Tuple[List[Dict[str, Any]], List[float]]:
        """
        Find matching business terms using the agentic RAG approach.
        
        Falls back to simpler methods if agentic RAG isn't available.

        Args:
            element_id: Unique identifier for the element
            element_name: Name of the element to match
            element_description: Description of the element
            top_k: Number of top matches to return
            cdm_context: Optional CDM context
            example_context: Optional example context
            process_name_context: Optional process name context
            process_description_context: Optional process description context
            initial_threshold: Minimum similarity threshold
            
        Returns:
            Tuple containing:
            - List of matched term dictionaries
            - List of confidence scores
        """
        logger.info(f"Finding terms for '{element_name}' (ID: {element_id})")
        
        # Record start time for performance tracking
        start_time = time.time()
        
        # Use agentic RAG if available
        if self.rag_graph:
            try:
                matching_terms, confidence_scores, modeling_required, message = await self.rag_graph.run(
                    element_id=element_id,
                    element_name=element_name,
                    element_description=element_description,
                    top_k=top_k,
                    threshold=initial_threshold,
                    cdm_context=cdm_context,
                    example_context=example_context,
                    process_name_context=process_name_context,
                    process_description_context=process_description_context
                )
                
                execution_time = time.time() - start_time
                logger.info(f"Agentic RAG found {len(matching_terms)} terms in {execution_time:.2f}s, modeling_required={modeling_required}")
                
                # If no results or low confidence, try enhancing with LLM directly
                if len(matching_terms) == 0 or max(confidence_scores, default=0) < 0.5:
                    logger.info("Low confidence in agentic RAG results, trying fallback")
                    fallback_results, fallback_scores = await self._try_fallback_search(
                        element_id=element_id,
                        element_name=element_name,
                        element_description=element_description,
                        top_k=top_k,
                        cdm_context=cdm_context,
                        example_context=example_context,
                        process_name_context=process_name_context,
                        process_description_context=process_description_context,
                        initial_threshold=max(0.05, initial_threshold - 0.2)
                    )
                    
                    # If fallback found better results, use those
                    if len(fallback_results) > len(matching_terms) or (
                        len(fallback_results) > 0 and max(fallback_scores) > max(confidence_scores, default=0)
                    ):
                        logger.info(f"Using fallback results: {len(fallback_results)} terms")
                        matching_terms = fallback_results
                        confidence_scores = fallback_scores
                
                return matching_terms, confidence_scores
                
            except Exception as e:
                logger.error(f"Agentic RAG failed, falling back to simple search: {e}", exc_info=True)
                # Continue with fallback methods
        
        # Fallback to direct search methods
        return await self._try_fallback_search(
            element_id=element_id,
            element_name=element_name,
            element_description=element_description,
            top_k=top_k,
            cdm_context=cdm_context,
            example_context=example_context,
            process_name_context=process_name_context,
            process_description_context=process_description_context,
            initial_threshold=initial_threshold
        )
    
    async def _try_fallback_search(
        self,
        element_id: str,
        element_name: str,
        element_description: str,
        top_k: int,
        cdm_context: Optional[str] = None,
        example_context: Optional[str] = None,
        process_name_context: Optional[str] = None,
        process_description_context: Optional[str] = None,
        initial_threshold: float = DEFAULT_INITIAL_THRESHOLD
    ) -> Tuple[List[Dict[str, Any]], List[float]]:
        """
        Perform fallback search with vector similarity and LLM enhancement.
        
        Args:
            element_id: Unique identifier for the element
            element_name: Name of the element to match
            element_description: Description of the element
            top_k: Number of top matches to return
            cdm_context: Optional CDM context
            example_context: Optional example context
            process_name_context: Optional process name context
            process_description_context: Optional process description context
            initial_threshold: Minimum similarity threshold
            
        Returns:
            Tuple containing:
            - List of matched term dictionaries
            - List of confidence scores
        """
        # Fallback to basic vector search
        logger.info("Using basic vector search fallback")
        fallback_start = time.time()
        
        # Build query
        query_parts = [
            f"Item Name: {element_name}",
            f"Description: {element_description}"
        ]
        
        # Add context if available
        if example_context: 
            query_parts.append(f"Examples: {example_context}")
        if process_name_context: 
            query_parts.append(f"Related Process: {process_name_context}")
        if process_description_context: 
            query_parts.append(f"Process Description: {process_description_context}")
        if cdm_context: 
            query_parts.append(f"CDM: {cdm_context}")
        
        query_text = ". ".join(query_parts)
        
        # Create document and get embedding
        query_doc = MyDocument(id=f"query_{element_id}", text=query_text)
        try:
            embedded_query_doc = self.embedding_client.generate_embeddings(query_doc)
            if not embedded_query_doc.embedding:
                logger.error("Failed to generate embedding for the query.")
                return [], []
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return [], []
            
        # Perform vector search with lower threshold
        try:
            num_candidates = max(top_k * CANDIDATE_FETCH_MULTIPLIER, MIN_CANDIDATES_TO_FETCH)
            
            # Use much lower threshold
            search_threshold = max(0.05, initial_threshold - 0.2)
            logger.info(f"Using search threshold: {search_threshold} (original was {initial_threshold})")
            
            results = self.bt_manager.vector_store.find_similar_vectors(
                query_vector=embedded_query_doc.embedding,
                top_k=num_candidates,
                threshold=search_threshold
            )
            
            # Add LLM enhancement if we have results and an LLM
            if results and len(results) > 0 and self.llm:
                try:
                    logger.info(f"Enhancing {len(results)} vector results with LLM")
                    enhanced_results = await self._enhance_results_with_llm(
                        element_name=element_name,
                        element_description=element_description,
                        candidates=results[:min(20, len(results))]
                    )
                    if enhanced_results:
                        results = enhanced_results
                        logger.info(f"Successfully enhanced results with LLM")
                except Exception as e:
                    logger.error(f"Error enhancing with LLM: {e}")
                    # Continue with original results
            
            # Sort by similarity
            results.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
            
            # Take top_k
            top_results = results[:top_k]
            
            confidence_scores = [result.get("similarity", 0.0) for result in top_results]
            
            fallback_time = time.time() - fallback_start
            logger.info(f"Basic vector search fallback found {len(top_results)} results in {fallback_time:.2f}s")
            
            return top_results, confidence_scores
            
        except Exception as e:
            logger.error(f"Error performing vector search: {e}")
            return [], []

    async def evaluate_term_match(self, 
                                element_name: str, 
                                element_description: str, 
                                term_name: str, 
                                term_description: str) -> Tuple[float, str]:
        """
        Evaluate how well a specific term matches the given element.
        
        Args:
            element_name: Element name
            element_description: Element description
            term_name: Business term name
            term_description: Business term description
            
        Returns:
            Tuple containing:
            - Match score (0-1)
            - Reasoning for the score
        """
        if not self.llm:
            # Calculate basic similarity if LLM is not available
            from app.utils.text_processing import semantic_similarity
            
            combined_element = f"{element_name} {element_description}"
            combined_term = f"{term_name} {term_description}"
            score = semantic_similarity(combined_element, combined_term)
            
            return score, "Score based on text similarity (LLM not available)"
        
        try:
            # Use LLM to evaluate the match
            prompt = f"""
            You are a data governance expert evaluating how well a business term matches a data element.
            
            Data Element:
              Name: {element_name}
              Description: {element_description}
            
            Business Term:
              Name: {term_name}
              Description: {term_description}
            
            Evaluate how well this business term matches the data element semantically.
            Consider:
            1. How well the term's name matches the element's meaning
            2. How well the term's description aligns with the element's purpose
            3. Whether the concept is the same, even if expressed differently
            
            Rate the match on a scale of 0-10 and provide brief reasoning.
            
            Return your evaluation as a JSON object:
            {{
              "score": 8.5,
              "reasoning": "Brief explanation of why this is a good/bad match"
            }}
            
            Ensure your response is ONLY the JSON object.
            """
            
            response = await self.llm.ainvoke(prompt)
            
            # Parse response
            try:
                # Try direct JSON parsing
                eval_data = json.loads(response)
                score = eval_data.get("score", 0) / 10.0  # Convert to 0-1 scale
                reasoning = eval_data.get("reasoning", "")
                
                return score, reasoning
                
            except json.JSONDecodeError:
                # Try to extract JSON from text
                import re
                json_match = re.search(r'{.*}', response, re.DOTALL)
                if json_match:
                    try:
                        eval_data = json.loads(json_match.group(0))
                        score = eval_data.get("score", 0) / 10.0
                        reasoning = eval_data.get("reasoning", "")
                        
                        return score, reasoning
                    except:
                        pass
                
                # Fallback to basic similarity
                from app.utils.text_processing import semantic_similarity
                
                combined_element = f"{element_name} {element_description}"
                combined_term = f"{term_name} {term_description}"
                score = semantic_similarity(combined_element, combined_term)
                
                return score, "Score based on text similarity (LLM response parsing failed)"
                
        except Exception as e:
            logger.error(f"Error evaluating term match: {e}")
            
            # Fallback to basic similarity
            from app.utils.text_processing import semantic_similarity
            
            combined_element = f"{element_name} {element_description}"
            combined_term = f"{term_name} {term_description}"
            score = semantic_similarity(combined_element, combined_term)
            
            return score, f"Score based on text similarity (LLM evaluation failed: {str(e)})"
