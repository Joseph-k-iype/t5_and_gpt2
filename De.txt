"""
Text processing utilities for the Agentic RAG system.

This module provides utility functions for text processing, tokenization,
and semantic expansion of business terms.
"""

import re
import logging
from typing import List, Set, Dict, Any, Optional, Tuple

# Setup logger
logger = logging.getLogger(__name__)

# Stopwords for filtering
STOPWORDS = {
    "a", "an", "the", "and", "or", "but", "if", "because", "as", "what", 
    "when", "where", "how", "which", "who", "whom", "this", "that", "these", 
    "those", "in", "on", "at", "by", "for", "with", "about", "against", 
    "between", "into", "through", "during", "before", "after", "above", "below",
    "to", "from", "up", "down", "of", "off", "over", "under", "again", "further",
    "then", "once", "here", "there", "all", "any", "both", "each", "few", "more",
    "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same",
    "so", "than", "too", "very", "can", "will", "just", "should", "now"
}

# Common business terminology mappings
BUSINESS_TERM_MAPPINGS = {
    "number": ["identifier", "id", "code", "reference"],
    "id": ["identifier", "number", "code", "key"],
    "identifier": ["id", "number", "code", "key"],
    "customer": ["client", "consumer", "user", "patron"],
    "client": ["customer", "consumer", "user", "patron"],
    "amount": ["value", "sum", "total", "figure"],
    "date": ["timestamp", "time", "day", "period"],
    "name": ["label", "title", "designation"],
    "address": ["location", "place", "residence", "domicile"],
    "account": ["profile", "record", "membership"],
    "balance": ["amount", "sum", "total", "remainder"],
    "transaction": ["exchange", "transfer", "deal", "operation"],
    "payment": ["transfer", "remittance", "disbursement", "expenditure"],
    "email": ["e-mail", "electronic mail", "mail"],
    "phone": ["telephone", "mobile", "contact number", "cell"],
}

def clean_text(text: str) -> str:
    """
    Clean text by converting to lowercase and removing punctuation.
    
    Args:
        text: Input text to clean
        
    Returns:
        Cleaned text string
    """
    if not text:
        return ""
    
    # Convert to lowercase
    text = text.lower()
    
    # Replace punctuation with spaces
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def extract_tokens(text: str, min_length: int = 3) -> List[str]:
    """
    Extract tokens from text, filtering stopwords and short words.
    
    Args:
        text: Input text to tokenize
        min_length: Minimum token length to keep
        
    Returns:
        List of filtered tokens
    """
    if not text:
        return []
    
    # Clean text
    text = clean_text(text)
    
    # Split into tokens
    tokens = text.split()
    
    # Filter stopwords and short words
    filtered_tokens = [token for token in tokens if token not in STOPWORDS and len(token) >= min_length]
    
    return filtered_tokens

def calculate_token_overlap(text1: str, text2: str) -> float:
    """
    Calculate the token overlap (Jaccard similarity) between two texts.
    
    Args:
        text1: First text
        text2: Second text
        
    Returns:
        Jaccard similarity score (0-1)
    """
    tokens1 = set(extract_tokens(text1))
    tokens2 = set(extract_tokens(text2))
    
    if not tokens1 or not tokens2:
        return 0.0
    
    # Jaccard similarity: intersection size / union size
    intersection = tokens1.intersection(tokens2)
    union = tokens1.union(tokens2)
    
    return len(intersection) / len(union)

def calculate_token_overlap_with_weights(tokens1: Set[str], tokens2: Set[str]) -> float:
    """
    Calculate a weighted token overlap between two token sets.
    
    This gives higher weight to rarer tokens that are more likely to be significant.
    
    Args:
        tokens1: First set of tokens
        tokens2: Second set of tokens
        
    Returns:
        Weighted Jaccard similarity (0-1)
    """
    if not tokens1 or not tokens2:
        return 0.0
    
    # Base Jaccard
    intersection = tokens1.intersection(tokens2)
    union = tokens1.union(tokens2)
    
    base_score = len(intersection) / len(union)
    
    # No matches
    if len(intersection) == 0:
        return 0.0
    
    # Add a slight boost for each term in the intersection 
    # This helps when comparing short and long texts
    boost = min(0.2, 0.05 * len(intersection))
    
    return min(1.0, base_score + boost)

def expand_business_terms(term: str) -> List[str]:
    """
    Expand a business term with common synonyms.
    
    Args:
        term: Business term to expand
        
    Returns:
        List of expanded terms including synonyms
    """
    expanded_terms = [term]
    term_lower = term.lower()
    
    # Check if any key terms are in the mappings
    for key, synonyms in BUSINESS_TERM_MAPPINGS.items():
        if key in term_lower:
            for synonym in synonyms:
                # Create a new term by replacing the key with the synonym
                expanded = term_lower.replace(key, synonym)
                if expanded != term_lower:
                    expanded_terms.append(expanded)
    
    return list(set(expanded_terms))

def calculate_fuzzy_score(str1: str, str2: str) -> float:
    """
    Calculate fuzzy matching score between two strings using Levenshtein distance.
    
    Args:
        str1: First string
        str2: Second string
        
    Returns:
        Similarity score (0-1)
    """
    try:
        # Try to use python-Levenshtein for better performance if available
        from Levenshtein import ratio
        return ratio(str1.lower(), str2.lower())
    except ImportError:
        # Fallback to pure Python implementation
        logger.debug("Levenshtein library not available, using pure Python implementation")
        
        # Convert to lowercase for case-insensitive matching
        s1 = str1.lower()
        s2 = str2.lower()
        
        # If either string is empty, return 0
        if not s1 or not s2:
            return 0.0
        
        # If the strings are identical, return 1
        if s1 == s2:
            return 1.0
        
        # Calculate Levenshtein distance
        len_s1 = len(s1)
        len_s2 = len(s2)
        
        # Initialize matrix of size (len_s1+1) x (len_s2+1)
        matrix = [[0 for _ in range(len_s2 + 1)] for _ in range(len_s1 + 1)]
        
        # Initialize first row and column
        for i in range(len_s1 + 1):
            matrix[i][0] = i
        
        for j in range(len_s2 + 1):
            matrix[0][j] = j
        
        # Fill the matrix
        for i in range(1, len_s1 + 1):
            for j in range(1, len_s2 + 1):
                cost = 0 if s1[i-1] == s2[j-1] else 1
                matrix[i][j] = min(
                    matrix[i-1][j] + 1,      # Deletion
                    matrix[i][j-1] + 1,      # Insertion
                    matrix[i-1][j-1] + cost  # Substitution
                )
        
        # Get Levenshtein distance
        distance = matrix[len_s1][len_s2]
        
        # Convert to similarity score (0-1)
        max_length = max(len_s1, len_s2)
        if max_length == 0:
            return 0.0
        
        return 1.0 - (distance / max_length)

def semantic_similarity(term1: str, term2: str) -> float:
    """
    Calculate semantic similarity between two terms using multiple methods.
    
    This combines token overlap, fuzzy matching, and business term expansion
    to provide a more comprehensive similarity score.
    
    Args:
        term1: First term
        term2: Second term
        
    Returns:
        Combined similarity score (0-1)
    """
    # Basic cleaning
    term1_clean = clean_text(term1)
    term2_clean = clean_text(term2)
    
    # Direct token overlap (Jaccard similarity)
    tokens1 = set(extract_tokens(term1_clean))
    tokens2 = set(extract_tokens(term2_clean))
    overlap_score = calculate_token_overlap_with_weights(tokens1, tokens2)
    
    # Fuzzy string matching
    fuzzy_score = calculate_fuzzy_score(term1_clean, term2_clean)
    
    # Check business term expansions
    expansion_score = 0.0
    expanded_terms1 = expand_business_terms(term1_clean)
    expanded_terms2 = expand_business_terms(term2_clean)
    
    for exp1 in expanded_terms1:
        for exp2 in expanded_terms2:
            if exp1 == exp2 and exp1 != term1_clean and exp2 != term2_clean:
                # Found matching expansions that aren't the original terms
                expansion_score = 0.7  # High but not perfect score
                break
    
    # Combine scores with weights
    combined_score = (
        overlap_score * 0.4 +    # Token overlap is important
        fuzzy_score * 0.3 +      # Fuzzy matching helps with minor variations
        expansion_score * 0.3    # Business term equivalence is highly valuable
    )
    
    return min(1.0, combined_score)
