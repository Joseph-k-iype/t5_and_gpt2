"""
Legal Document Analyzer using ReAct Agents
Processes legal documents with advanced reasoning and action patterns
CRITICAL: No document truncation - uses intelligent chunking for complete coverage
OPTIMIZED: Token-efficient processing for 200K token limit

Location: src/analyzers/legal_document_analyzer.py
"""

from typing import Dict, List, Optional, Any
from enum import Enum
import json
from dataclasses import dataclass, field

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

from src.prompting.advanced_strategies import (
    AdvancedPromptingStrategies,
    ExpertRole,
    ReasoningMode
)
from src.services.openai_service import OpenAIService
from src.utils.document_chunker import DocumentChunker  # CORRECTED IMPORT PATH
from src.config import Config


@dataclass
class AnalysisState:
    """State for the legal document analysis workflow"""
    rule_name: str
    jurisdiction: str
    document_text: str
    level: int  # 1, 2, or 3
    
    # Chunking
    chunks: List[Dict[str, Any]] = field(default_factory=list)
    current_chunk_index: int = 0
    chunk_analyses: List[Dict[str, Any]] = field(default_factory=list)
    
    # Context (OPTIMIZED: Use summaries instead of full JSON)
    enterprise_context: Optional[Dict[str, Any]] = None
    previous_level_analysis: Optional[str] = None
    
    # Analysis components
    thoughts: List[str] = field(default_factory=list)
    actions_taken: List[str] = field(default_factory=list)
    observations: List[str] = field(default_factory=list)
    
    # Extracted information (for current chunk)
    rule_description: str = ""
    user_actions: List[str] = field(default_factory=list)
    system_actions: List[str] = field(default_factory=list)
    user_duties: List[str] = field(default_factory=list)
    system_duties: List[str] = field(default_factory=list)
    constraints: List[Dict[str, Any]] = field(default_factory=list)
    rule_type: str = ""
    
    # Processing flags
    completed: bool = False
    iteration: int = 0
    max_iterations: int = 5
    refinement_count: int = 0  # Track refinement attempts
    max_refinements: int = 1   # Limit refinements
    
    # Final output
    final_analysis: Dict[str, Any] = field(default_factory=dict)
    
    # Messages for LangGraph (OPTIMIZED: Limited history)
    messages: List[BaseMessage] = field(default_factory=list)


class LegalDocumentAnalyzer:
    """
    Sophisticated legal document analyzer using ReAct pattern with LangGraph
    CRITICAL: Uses document chunking to ensure complete coverage without truncation
    OPTIMIZED: Token-efficient for 200K context limit
    """
    
    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        # Validate API key
        if not self.config.API_KEY:
            raise ValueError(
                "OPENAI_API_KEY environment variable is required. "
                "Please set it using: export OPENAI_API_KEY='your-api-key'"
            )
        
        self.openai_service = OpenAIService()
        self.llm = ChatOpenAI(
            model=self.config.CHAT_MODEL,
            openai_api_key=self.config.API_KEY,
            openai_api_base=self.config.BASE_URL
        )
        self.strategies = None  # Will be initialized per document
        
        # OPTIMIZED: Use smaller chunk size from Config
        self.chunker = DocumentChunker(
            chunk_size=self.config.CHUNK_SIZE,  # Now 4000 instead of 8000
            chunk_overlap=self.config.OVERLAP_SIZE,  # Now 300 instead of 500
            respect_boundaries=True
        )
        
    def create_workflow(self) -> StateGraph:
        """Create the ReAct workflow graph with proper termination conditions"""
        workflow = StateGraph(AnalysisState)
        
        # Add nodes
        workflow.add_node("initialize", self.initialize_analysis)
        workflow.add_node("prepare_chunk", self.prepare_chunk)
        workflow.add_node("reason", self.reason_step)
        workflow.add_node("act", self.act_step)
        workflow.add_node("observe", self.observe_step)
        workflow.add_node("synthesize_chunk", self.synthesize_chunk_step)
        workflow.add_node("merge_chunks", self.merge_chunks_step)
        workflow.add_node("reflect", self.reflect_step)
        workflow.add_node("finalize", self.finalize_step)
        
        # Add edges
        workflow.set_entry_point("initialize")
        workflow.add_edge("initialize", "prepare_chunk")
        workflow.add_edge("prepare_chunk", "reason")
        workflow.add_conditional_edges(
            "reason",
            self.should_continue_reasoning,
            {
                "act": "act",
                "synthesize_chunk": "synthesize_chunk"
            }
        )
        workflow.add_edge("act", "observe")
        workflow.add_edge("observe", "reason")
        workflow.add_conditional_edges(
            "synthesize_chunk",
            self.should_process_next_chunk,
            {
                "prepare_chunk": "prepare_chunk",
                "merge_chunks": "merge_chunks"
            }
        )
        workflow.add_edge("merge_chunks", "reflect")
        workflow.add_conditional_edges(
            "reflect",
            self.should_refine,
            {
                "finalize": "finalize"
            }
        )
        workflow.add_edge("finalize", END)
        
        # Compile
        return workflow.compile()
    
    def initialize_analysis(self, state: AnalysisState) -> AnalysisState:
        """Initialize the analysis with system prompt and chunk the document"""
        self.strategies = AdvancedPromptingStrategies(
            rule_name=state.rule_name,
            jurisdiction=state.jurisdiction
        )
        
        print(f"  Initializing analysis for: {state.rule_name}")
        print(f"  Jurisdiction: {state.jurisdiction}")
        print(f"  Level: {state.level}")
        print(f"  Document length: {len(state.document_text)} characters")
        
        # CRITICAL: Chunk the document for complete processing
        state.chunks = self.chunker.chunk_document(
            text=state.document_text,
            metadata={
                "rule_name": state.rule_name,
                "jurisdiction": state.jurisdiction,
                "level": state.level
            }
        )
        
        print(f"  Created {len(state.chunks)} chunks (complete coverage, no truncation)")
        
        # Initialize messages with system prompt (OPTIMIZED: Concise)
        system_prompt = self.strategies.get_react_agent_system_prompt()
        state.messages = [SystemMessage(content=system_prompt)]
        
        # Add context message (OPTIMIZED: Brief)
        context_parts = [
            f"Analyzing Level {state.level} document",
            f"Chunks: {len(state.chunks)}",
            f"Length: {len(state.document_text)} chars"
        ]
        
        if state.enterprise_context:
            org = state.enterprise_context.get('organization', 'N/A')
            context_parts.append(f"Enterprise: {org}")
        
        if state.previous_level_analysis:
            context_parts.append("Previous level context available")
        
        context_msg = "\n".join(context_parts)
        state.messages.append(HumanMessage(content=context_msg))
        
        # Reset chunk index
        state.current_chunk_index = 0
        
        return state
    
    def prepare_chunk(self, state: AnalysisState) -> AnalysisState:
        """Prepare next chunk for analysis"""
        # Reset per-chunk state
        state.rule_description = ""
        state.user_actions = []
        state.system_actions = []
        state.user_duties = []
        state.system_duties = []
        state.constraints = []
        state.rule_type = ""
        state.iteration = 0
        state.completed = False
        state.thoughts = []
        state.actions_taken = []
        state.observations = []
        
        chunk = state.chunks[state.current_chunk_index]
        chunk_context = self.chunker.get_chunk_context(chunk)
        
        print(f"\n  Processing chunk {state.current_chunk_index + 1}/{len(state.chunks)}")
        print(f"    {chunk_context}")
        print(f"    Chunk size: {len(chunk['text'])} chars")
        
        # Add chunk to messages (OPTIMIZED: Concise format)
        chunk_msg = f"""{chunk_context}

TEXT:
{chunk['text']}

Analyze this chunk thoroughly for rules, actions, duties, and constraints."""
        
        state.messages.append(HumanMessage(content=chunk_msg))
        
        # OPTIMIZATION: Trim message history to prevent context overflow
        if len(state.messages) > Config.MAX_MESSAGE_HISTORY:
            system_msg = state.messages[0]
            recent_msgs = state.messages[-Config.MAX_MESSAGE_HISTORY+1:]
            state.messages = [system_msg] + recent_msgs
        
        return state
    
    def reason_step(self, state: AnalysisState) -> AnalysisState:
        """Reasoning step - decide what to analyze next"""
        state.iteration += 1
        
        # CRITICAL: Enforce hard limit
        if state.iteration > state.max_iterations:
            print(f"    Max iterations ({state.max_iterations}) reached")
            state.completed = True
            return state
        
        # Check bounds
        if state.current_chunk_index >= len(state.chunks):
            print(f"    ERROR: Chunk index out of bounds")
            state.completed = True
            return state
        
        chunk = state.chunks[state.current_chunk_index]
        
        # Determine what's missing
        missing = []
        if not state.rule_description:
            missing.append("rule description")
        if not state.user_actions:
            missing.append("user actions")
        if not state.system_actions:
            missing.append("system actions")
        if not state.constraints:
            missing.append("constraints")
        if not state.rule_type:
            missing.append("rule type")
        
        if not missing:
            state.completed = True
            return state
        
        # Build context from previous chunks (OPTIMIZED)
        prev_info = ""
        if state.chunk_analyses:
            last = state.chunk_analyses[-1]
            prev_info = f"\nPrevious: {len(last.get('user_actions', []))} user actions, {len(last.get('constraints', []))} constraints found."
        
        # Use strategy based on iteration (OPTIMIZED: Shorter prompts)
        chunk_text = chunk['text']
        chunk_ctx = self.chunker.get_chunk_context(chunk)
        
        if state.iteration == 1:
            # First iteration: Chain of Thought
            prompt = self.strategies.get_chain_of_thought_prompt(
                document_text=chunk_text,
                analysis_type=f"Extract {', '.join(missing)}",
                context=state.enterprise_context,
                chunk_info=chunk_ctx
            )
        elif state.iteration == 2:
            # Second iteration: Mixture of Experts
            prompt = self.strategies.get_mixture_of_experts_prompt(
                document_text=chunk_text,
                expert_roles=[ExpertRole.LEGAL_EXPERT, ExpertRole.COMPLIANCE_OFFICER],
                chunk_info=chunk_ctx
            )
        else:
            # Later iterations: Dynamic contextualized
            prompt = self.strategies.get_dynamic_contextualized_prompt(
                document_text=chunk_text,
                previous_analyses=[state.previous_level_analysis] if state.previous_level_analysis else None,
                enterprise_context=state.enterprise_context,
                chunk_info=chunk_ctx
            )
        
        prompt = f"{prompt}{prev_info}"
        
        state.messages.append(HumanMessage(content=prompt))
        response = self.llm.invoke(state.messages)
        state.messages.append(response)
        
        state.thoughts.append(response.content)
        
        return state
    
    def act_step(self, state: AnalysisState) -> AnalysisState:
        """Action step - extract specific information"""
        action = "extract_comprehensive_analysis"
        state.actions_taken.append(action)
        return state
    
    def observe_step(self, state: AnalysisState) -> AnalysisState:
        """Observation step - process and store extracted information"""
        last_response = state.messages[-1].content if state.messages else ""
        
        # Extract structured information
        extracted = self._extract_structured_info(last_response)
        
        if extracted.get("description"):
            state.rule_description = extracted["description"]
        
        if extracted.get("user_actions"):
            state.user_actions.extend(extracted["user_actions"])
        
        if extracted.get("system_actions"):
            state.system_actions.extend(extracted["system_actions"])
        
        if extracted.get("user_duties"):
            state.user_duties.extend(extracted["user_duties"])
        
        if extracted.get("system_duties"):
            state.system_duties.extend(extracted["system_duties"])
        
        if extracted.get("constraints"):
            state.constraints.extend(extracted["constraints"])
        
        if extracted.get("rule_type"):
            state.rule_type = extracted["rule_type"]
        
        state.observations.append("Extracted information successfully")
        
        return state
    
    def synthesize_chunk_step(self, state: AnalysisState) -> AnalysisState:
        """Synthesize current chunk analysis"""
        # OPTIMIZED: Concise synthesis prompt
        synthesis_prompt = f"""Summarize findings from chunk {state.current_chunk_index + 1}/{len(state.chunks)}:

Provide JSON with: description, user_actions, system_actions, user_duties, system_duties, constraints, rule_type, confidence."""
        
        state.messages.append(HumanMessage(content=synthesis_prompt))
        response = self.llm.invoke(state.messages)
        state.messages.append(response)
        
        # Parse synthesis
        try:
            chunk_analysis = self._parse_json_response(response.content)
        except:
            chunk_analysis = {
                "description": state.rule_description,
                "user_actions": list(set(state.user_actions)),
                "system_actions": list(set(state.system_actions)),
                "user_duties": list(set(state.user_duties)),
                "system_duties": list(set(state.system_duties)),
                "constraints": state.constraints,
                "rule_type": state.rule_type,
                "confidence": "medium"
            }
        
        chunk_analysis["chunk_id"] = state.current_chunk_index
        chunk_analysis["chunk_position"] = f"{state.current_chunk_index + 1}/{len(state.chunks)}"
        
        state.chunk_analyses.append(chunk_analysis)
        
        print(f"    ✓ Chunk {state.current_chunk_index + 1} complete")
        if chunk_analysis.get("user_actions"):
            print(f"      Found {len(chunk_analysis['user_actions'])} user actions")
        if chunk_analysis.get("constraints"):
            print(f"      Found {len(chunk_analysis['constraints'])} constraints")
        
        return state
    
    def merge_chunks_step(self, state: AnalysisState) -> AnalysisState:
        """Merge analyses from all chunks"""
        print(f"\n  Merging {len(state.chunk_analyses)} chunk analyses...")
        
        # Use chunker's merge method
        merged_analysis = self.chunker.merge_chunk_analyses(state.chunk_analyses)
        
        # OPTIMIZED: Concise LLM synthesis with summaries
        synthesis_prompt = f"""Analyzed {len(state.chunks)} chunks. Synthesize into complete analysis:

Chunks processed: {len(state.chunk_analyses)}
Total findings: {len(merged_analysis.get('user_actions', []))} user actions, {len(merged_analysis.get('system_actions', []))} system actions, {len(merged_analysis.get('constraints', []))} constraints

Provide comprehensive JSON: description, user_actions, system_actions, user_duties, system_duties, constraints, rule_type, confidence."""
        
        synthesis_messages = [
            SystemMessage(content=self.strategies.get_react_agent_system_prompt()),
            HumanMessage(content=synthesis_prompt)
        ]
        
        response = self.llm.invoke(synthesis_messages)
        
        try:
            llm_synthesis = self._parse_json_response(response.content)
            merged_analysis["description"] = llm_synthesis.get("description", merged_analysis.get("description"))
            merged_analysis["synthesis_notes"] = llm_synthesis.get("synthesis_notes", "")
            
            # Merge lists
            for key in ["user_actions", "system_actions", "user_duties", "system_duties"]:
                llm_items = set(llm_synthesis.get(key, []))
                chunker_items = set(merged_analysis.get(key, []))
                merged_analysis[key] = sorted(list(llm_items.union(chunker_items)))
            
        except:
            print(f"    Note: Using automatic merge")
        
        state.final_analysis = merged_analysis
        
        print(f"  ✓ Merge complete:")
        print(f"    Total user actions: {len(merged_analysis.get('user_actions', []))}")
        print(f"    Total system actions: {len(merged_analysis.get('system_actions', []))}")
        print(f"    Total constraints: {len(merged_analysis.get('constraints', []))}")
        
        return state
    
    def reflect_step(self, state: AnalysisState) -> AnalysisState:
        """Reflect on the complete analysis"""
        # CRITICAL: Enforce maximum refinements
        if state.refinement_count >= state.max_refinements:
            print(f"  ℹ Maximum refinements ({state.max_refinements}) reached, finalizing...")
            state.completed = True
            return state
        
        # OPTIMIZED: Simple reflection
        reflection_prompt = f"""Analysis complete: {len(state.chunks)} chunks processed.

Findings:
- Description: {len(state.final_analysis.get('description', ''))} chars
- User actions: {len(state.final_analysis.get('user_actions', []))}
- System actions: {len(state.final_analysis.get('system_actions', []))}
- Constraints: {len(state.final_analysis.get('constraints', []))}

Is this complete and comprehensive? Respond with JSON:
{{"complete": true/false, "issues": ["..."], "improvements": {{...}}}}"""
        
        reflection_messages = [
            SystemMessage(content=self.strategies.get_react_agent_system_prompt()),
            HumanMessage(content=reflection_prompt)
        ]
        
        response = self.llm.invoke(reflection_messages)
        
        try:
            reflection = self._parse_json_response(response.content)
            
            if reflection.get("complete") is False and reflection.get("improvements"):
                print(f"  Reflection identified improvements (attempt {state.refinement_count + 1}/{state.max_refinements})...")
                state.completed = False
                state.refinement_count += 1
                
                # Apply improvements
                improvements = reflection.get("improvements", {})
                for key, value in improvements.items():
                    if key in state.final_analysis and value:
                        if isinstance(value, list):
                            state.final_analysis[key].extend(value)
                            state.final_analysis[key] = sorted(list(set(state.final_analysis[key])))
                        elif key == "description" and value:
                            state.final_analysis["description"] = value
            else:
                print(f"  ✓ Reflection confirms analysis is complete")
                state.completed = True
                
        except Exception as e:
            print(f"    Note: Reflection issue: {str(e)}, proceeding")
            state.completed = True
        
        return state
    
    def finalize_step(self, state: AnalysisState) -> AnalysisState:
        """Finalize the analysis"""
        # Ensure uniqueness
        for key in ["user_actions", "system_actions", "user_duties", "system_duties"]:
            if state.final_analysis.get(key):
                state.final_analysis[key] = sorted(list(set(state.final_analysis[key])))
        
        # Add metadata
        state.final_analysis["metadata"] = {
            "rule_name": state.rule_name,
            "jurisdiction": state.jurisdiction,
            "level": state.level,
            "document_length": len(state.document_text),
            "chunks_processed": len(state.chunks),
            "enterprise_context": state.enterprise_context,
            "processing_note": f"Complete document processed in {len(state.chunks)} chunks with no truncation, optimized for token efficiency"
        }
        
        print(f"  ✓ Finalized analysis")
        
        return state
    
    def should_continue_reasoning(self, state: AnalysisState) -> str:
        """Decide whether to continue reasoning or synthesize chunk"""
        if state.iteration >= state.max_iterations:
            return "synthesize_chunk"
        
        if state.completed:
            return "synthesize_chunk"
        
        return "act"
    
    def should_process_next_chunk(self, state: AnalysisState) -> str:
        """Decide whether to process next chunk or merge all chunks"""
        state.current_chunk_index += 1
        
        # CRITICAL: Bounds checking
        if state.current_chunk_index >= len(state.chunks):
            return "merge_chunks"
        
        if state.current_chunk_index > 50:  # Safety limit
            print(f"  WARNING: Exceeded maximum chunks (50), stopping")
            return "merge_chunks"
        
        return "prepare_chunk"
    
    def should_refine(self, state: AnalysisState) -> str:
        """Decide whether to refine or finalize"""
        if state.refinement_count >= state.max_refinements:
            return "finalize"
        
        if state.completed:
            return "finalize"
        
        return "finalize"
    
    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """Parse JSON from LLM response with multiple fallback strategies"""
        try:
            # Strategy 1: Direct parsing
            return json.loads(text)
        except:
            pass
        
        try:
            # Strategy 2: Extract from markdown code blocks
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except:
            pass
        
        try:
            # Strategy 3: Extract from plain code blocks
            import re
            json_match = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except:
            pass
        
        try:
            # Strategy 4: Find first JSON object
            import re
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except:
            pass
        
        raise ValueError("No valid JSON found in response")
    
    def _extract_structured_info(self, text: str) -> Dict[str, Any]:
        """Extract structured information from text with pattern matching fallback"""
        try:
            # Try JSON parsing first
            return self._parse_json_response(text)
        except:
            pass
        
        # Fallback: Pattern matching extraction
        result = {
            "description": "",
            "user_actions": [],
            "system_actions": [],
            "user_duties": [],
            "system_duties": [],
            "constraints": [],
            "rule_type": "",
            "confidence": "medium"
        }
        
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detect sections
            if "description" in line.lower() and ":" in line:
                current_section = "description"
                result["description"] = line.split(":", 1)[1].strip()
            elif "user action" in line.lower():
                current_section = "user_actions"
                result["user_actions"] = []
            elif "system action" in line.lower():
                current_section = "system_actions"
                result["system_actions"] = []
            elif "user dut" in line.lower():
                current_section = "user_duties"
                result["user_duties"] = []
            elif "system dut" in line.lower():
                current_section = "system_duties"
                result["system_duties"] = []
            elif "constraint" in line.lower() or "condition" in line.lower():
                current_section = "constraints"
                result["constraints"] = []
            elif "rule type" in line.lower() or "classification" in line.lower():
                if "permission" in line.lower():
                    result["rule_type"] = "permission"
                elif "prohibition" in line.lower():
                    result["rule_type"] = "prohibition"
                elif "obligation" in line.lower():
                    result["rule_type"] = "obligation"
            elif current_section and (line.startswith("-") or line.startswith("•") or line.startswith("*")):
                # List item
                item = line.lstrip("-•* ").strip()
                if item and current_section in result:
                    if isinstance(result[current_section], list):
                        if current_section == "constraints":
                            result[current_section].append({"type": "general", "description": item})
                        else:
                            result[current_section].append(item)
        
        return result
    
    def analyze_document(
        self,
        rule_name: str,
        jurisdiction: str,
        document_text: str,
        level: int,
        enterprise_context: Optional[Dict[str, Any]] = None,
        previous_level_analysis: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Main method to analyze a legal document
        CRITICAL: Complete document processed via chunking, no truncation
        OPTIMIZED: Token-efficient processing
        """
        # OPTIMIZATION: Summarize previous level if provided
        prev_summary = None
        if previous_level_analysis:
            try:
                prev_dict = json.loads(previous_level_analysis)
                prev_summary = self.chunker.summarize_analysis(prev_dict, Config.MAX_SYNTHESIS_CONTEXT)
            except:
                prev_summary = previous_level_analysis[:Config.MAX_SYNTHESIS_CONTEXT]
        
        # Create initial state
        state = AnalysisState(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=document_text,
            level=level,
            enterprise_context=enterprise_context,
            previous_level_analysis=prev_summary
        )
        
        # Create and run workflow
        workflow = self.create_workflow()
        
        try:
            final_state = workflow.invoke(state, {"recursion_limit": 1000})
        except TypeError:
            print("  Note: Running without recursion_limit (not supported)")
            final_state = workflow.invoke(state)
        
        return final_state["final_analysis"]
    
    def analyze_multi_level(
        self,
        rule_name: str,
        jurisdiction: str,
        level_1_text: str,
        level_2_text: str,
        level_3_text: str,
        enterprise_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze all three levels and synthesize
        CRITICAL: Each level fully processed via chunking, no truncation
        OPTIMIZED: Token-efficient context passing
        """
        # Analyze each level
        print(f"\n{'='*80}")
        print(f"Analyzing Level 1 document (length: {len(level_1_text)} chars)...")
        print(f"{'='*80}")
        level_1_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_1_text,
            level=1,
            enterprise_context=enterprise_context
        )
        
        # OPTIMIZATION: Pass only summary to level 2
        level_1_summary = self.chunker.summarize_analysis(level_1_analysis)
        
        print(f"\n{'='*80}")
        print(f"Analyzing Level 2 document (length: {len(level_2_text)} chars)...")
        print(f"{'='*80}")
        level_2_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_2_text,
            level=2,
            enterprise_context=enterprise_context,
            previous_level_analysis=json.dumps({"level_1_summary": level_1_summary})
        )
        
        # OPTIMIZATION: Pass only summary to level 3
        level_2_summary = self.chunker.summarize_analysis(level_2_analysis)
        
        print(f"\n{'='*80}")
        print(f"Analyzing Level 3 document (length: {len(level_3_text)} chars)...")
        print(f"{'='*80}")
        level_3_analysis = self.analyze_document(
            rule_name=rule_name,
            jurisdiction=jurisdiction,
            document_text=level_3_text,
            level=3,
            enterprise_context=enterprise_context,
            previous_level_analysis=json.dumps({
                "level_1_summary": level_1_summary,
                "level_2_summary": level_2_summary
            })
        )
        
        # Synthesize all levels with OPTIMIZED prompt
        print(f"\n{'='*80}")
        print(f"Synthesizing all levels...")
        print(f"{'='*80}")
        
        synthesis_prompt = self.strategies.get_multi_level_synthesis_prompt(
            level_1_analysis=level_1_summary,
            level_2_analysis=level_2_summary,
            level_3_analysis=self.chunker.summarize_analysis(level_3_analysis)
        )
        
        messages = [
            SystemMessage(content=self.strategies.get_react_agent_system_prompt()),
            HumanMessage(content=synthesis_prompt)
        ]
        
        response = self.llm.invoke(messages)
        
        try:
            final_synthesis = self._parse_json_response(response.content)
        except:
            print("  Using automatic merge for level synthesis...")
            combined_analyses = [level_1_analysis, level_2_analysis, level_3_analysis]
            final_synthesis = self.chunker.merge_chunk_analyses(combined_analyses)
            final_synthesis["synthesis_method"] = "automatic_merge"
        
        # Add compact metadata
        final_synthesis["metadata"] = {
            "rule_name": rule_name,
            "jurisdiction": jurisdiction,
            "total_document_length": len(level_1_text) + len(level_2_text) + len(level_3_text),
            "level_1_chunks": level_1_analysis.get("metadata", {}).get("chunks_processed", 0),
            "level_2_chunks": level_2_analysis.get("metadata", {}).get("chunks_processed", 0),
            "level_3_chunks": level_3_analysis.get("metadata", {}).get("chunks_processed", 0),
            "enterprise_context": enterprise_context,
            "processing_note": "Complete analysis with optimized token usage"
        }
        
        total_chunks = (
            level_1_analysis.get("metadata", {}).get("chunks_processed", 0) +
            level_2_analysis.get("metadata", {}).get("chunks_processed", 0) +
            level_3_analysis.get("metadata", {}).get("chunks_processed", 0)
        )
        
        print(f"\n✓ Multi-level synthesis complete")
        print(f"  Total chunks processed: {total_chunks}")
        
        return final_synthesis
