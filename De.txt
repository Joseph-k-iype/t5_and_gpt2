"""
ISO 11179 Data Enrichment with Intelligent Context Usage
========================================================
UPDATED:
- Removed ALL modelling features
- FIXED: Semantic-focused rationale instead of method-focused
- Strict validation to prevent bad separate search results
- Search-first: intersection ‚Üí supervisor ‚Üí if rejected: separate ‚Üí supervisor with comparison
- Intersection fallback: reverts to intersection if separate is worse
- Confidence penalties: separate searches get lower scores (40-70 max vs 85-95 for intersection)

WORKFLOW:
1. Search intersection (object.property combos) 
2. Supervisor validates intersection with semantic reasoning
3. If rejected ‚Üí Search objects and properties separately
4. Supervisor compares separate result with intersection semantically
5. Decision: Use intersection if separate is worse/risky, otherwise use separate with penalties

OUTPUT INCLUDES:
- Mapping Rationale: WHY the mapping makes semantic sense (not just which method)
- Object Reasoning: WHY the object represents the correct entity/concept
- Property Reasoning: WHY the property represents the correct characteristic
- Comparison Note: How separate vs intersection compared (if applicable)
- Supporting/Contradicting Reasons: SEMANTIC evidence for the mapping

REQUIREMENTS:
pip install chromadb langchain-community

FIRST RUN: Creates embeddings and stores in ChromaDB
SUBSEQUENT RUNS: Only embeds new items, reuses existing embeddings
"""

import json
import os
import pandas as pd
from typing import TypedDict, Annotated, List, Dict, Any, Literal, Sequence, Tuple
import numpy as np
from openai import OpenAI
import time
from datetime import datetime
from collections import defaultdict
import networkx as nx
from difflib import SequenceMatcher
import pickle
import chromadb
from chromadb.config import Settings

# LangChain and LangGraph imports
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_REASONING_MODEL = "o3-mini"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"

openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

INPUT_JSON_PATH = "cib_long_json.json"
EXCEL_PATH = "pbt.xlsx"
ACRONYM_CSV_PATH = "acronyms.csv"
OUTPUT_CSV_PATH = "enriched_mapped_output.csv"
CACHE_JSON_PATH = "mapping_cache.json"
PROCEDURAL_MEMORY_PATH = "procedural_memory.json"
EPISODIC_MEMORY_PATH = "episodic_memory.json"
KNOWLEDGE_GRAPH_PATH = "semantic_knowledge_graph.gpickle"
CHROMADB_PATH = "./chroma_embeddings"

# Global stores
MAPPING_CACHE = {}
PROCEDURAL_MEMORY = {}
EPISODIC_MEMORY = []
SEMANTIC_KNOWLEDGE_GRAPH = nx.MultiDiGraph()

# ChromaDB client (will be initialized)
CHROMA_CLIENT = None


# ============================================================================
# CHROMADB PERSISTENT EMBEDDING STORAGE
# ============================================================================

def initialize_chromadb():
    """Initialize ChromaDB client for persistent embedding storage."""
    global CHROMA_CLIENT
    
    print(f"   ‚Üí Initializing ChromaDB at: {CHROMADB_PATH}")
    
    CHROMA_CLIENT = chromadb.PersistentClient(path=CHROMADB_PATH)
    
    # Get or create collections
    collections = [c.name for c in CHROMA_CLIENT.list_collections()]
    print(f"   ‚Üí Existing collections: {collections if collections else 'None'}")
    
    return CHROMA_CLIENT


def create_or_load_vector_stores(objects_dict, all_objects, all_properties, embeddings):
    """
    Create or load vector stores from ChromaDB.
    Only embeds new items that aren't already stored.
    """
    client = CHROMA_CLIENT
    
    # ========================================================================
    # 1. INTERSECTION STORE (object.property combinations)
    # ========================================================================
    print(f"\n   ‚Üí Processing INTERSECTION store (object.property)...")
    
    intersection_collection = client.get_or_create_collection(
        name="intersection_embeddings",
        metadata={"description": "Object.Property combination embeddings"}
    )
    
    # Get existing IDs
    existing_intersection = set()
    try:
        existing_data = intersection_collection.get()
        if existing_data and existing_data['ids']:
            existing_intersection = set(existing_data['ids'])
            print(f"      ‚Ä¢ Found {len(existing_intersection)} existing intersection embeddings")
    except:
        pass
    
    # Prepare all intersection documents
    intersection_docs = []
    intersection_ids = []
    new_intersection_count = 0
    
    for obj in all_objects:
        for prop in objects_dict[obj]:
            doc_id = f"{obj}|{prop}"
            if doc_id not in existing_intersection:
                doc_text = f"{obj} {prop}"
                doc = Document(
                    page_content=doc_text,
                    metadata={"object": obj, "property": prop, "id": doc_id}
                )
                intersection_docs.append(doc)
                intersection_ids.append(doc_id)
                new_intersection_count += 1
    
    print(f"      ‚Ä¢ Need to embed {new_intersection_count} new intersection combinations")
    
    # Create vector store from ChromaDB collection
    if new_intersection_count > 0:
        print(f"      ‚Ä¢ Embedding new items...")
        # Add new documents to collection
        for i, doc in enumerate(intersection_docs):
            print(f"        [{i+1}/{len(intersection_docs)}] Embedding: {doc.metadata['object']}.{doc.metadata['property']}")
            embedding = embeddings.embed_query(doc.page_content)
            intersection_collection.add(
                embeddings=[embedding],
                documents=[doc.page_content],
                metadatas=[{"object": doc.metadata['object'], "property": doc.metadata['property']}],
                ids=[intersection_ids[i]]
            )
            time.sleep(0.1)
    
    print(f"      ‚úì Intersection store ready: {intersection_collection.count()} total embeddings")
    
    # Wrap in Chroma vector store
    intersection_store = Chroma(
        client=client,
        collection_name="intersection_embeddings",
        embedding_function=embeddings
    )
    
    # ========================================================================
    # 2. OBJECT STORE
    # ========================================================================
    print(f"\n   ‚Üí Processing OBJECT store...")
    
    object_collection = client.get_or_create_collection(
        name="object_embeddings",
        metadata={"description": "Object embeddings"}
    )
    
    # Get existing IDs
    existing_objects = set()
    try:
        existing_data = object_collection.get()
        if existing_data and existing_data['ids']:
            existing_objects = set(existing_data['ids'])
            print(f"      ‚Ä¢ Found {len(existing_objects)} existing object embeddings")
    except:
        pass
    
    # Prepare new objects
    new_objects = []
    new_object_ids = []
    
    for obj in all_objects:
        obj_id = f"obj:{obj}"
        if obj_id not in existing_objects:
            new_objects.append(obj)
            new_object_ids.append(obj_id)
    
    print(f"      ‚Ä¢ Need to embed {len(new_objects)} new objects")
    
    if len(new_objects) > 0:
        print(f"      ‚Ä¢ Embedding new items...")
        for i, obj in enumerate(new_objects):
            print(f"        [{i+1}/{len(new_objects)}] Embedding: {obj}")
            embedding = embeddings.embed_query(obj)
            object_collection.add(
                embeddings=[embedding],
                documents=[obj],
                metadatas=[{"object": obj}],
                ids=[new_object_ids[i]]
            )
            time.sleep(0.1)
    
    print(f"      ‚úì Object store ready: {object_collection.count()} total embeddings")
    
    object_store = Chroma(
        client=client,
        collection_name="object_embeddings",
        embedding_function=embeddings
    )
    
    # ========================================================================
    # 3. PROPERTY STORE
    # ========================================================================
    print(f"\n   ‚Üí Processing PROPERTY store...")
    
    property_collection = client.get_or_create_collection(
        name="property_embeddings",
        metadata={"description": "Property embeddings"}
    )
    
    # Get existing IDs
    existing_properties = set()
    try:
        existing_data = property_collection.get()
        if existing_data and existing_data['ids']:
            existing_properties = set(existing_data['ids'])
            print(f"      ‚Ä¢ Found {len(existing_properties)} existing property embeddings")
    except:
        pass
    
    # Prepare new properties
    new_properties = []
    new_property_ids = []
    
    for prop in all_properties:
        prop_id = f"prop:{prop}"
        if prop_id not in existing_properties:
            new_properties.append(prop)
            new_property_ids.append(prop_id)
    
    print(f"      ‚Ä¢ Need to embed {len(new_properties)} new properties")
    
    if len(new_properties) > 0:
        print(f"      ‚Ä¢ Embedding new items...")
        for i, prop in enumerate(new_properties):
            print(f"        [{i+1}/{len(new_properties)}] Embedding: {prop}")
            embedding = embeddings.embed_query(prop)
            property_collection.add(
                embeddings=[embedding],
                documents=[prop],
                metadatas=[{"property": prop}],
                ids=[new_property_ids[i]]
            )
            time.sleep(0.1)
    
    print(f"      ‚úì Property store ready: {property_collection.count()} total embeddings")
    
    property_store = Chroma(
        client=client,
        collection_name="property_embeddings",
        embedding_function=embeddings
    )
    
    return intersection_store, object_store, property_store


# ============================================================================
# KNOWLEDGE GRAPH (Simplified)
# ============================================================================

def initialize_knowledge_graph():
    global SEMANTIC_KNOWLEDGE_GRAPH
    if os.path.exists(KNOWLEDGE_GRAPH_PATH):
        try:
            with open(KNOWLEDGE_GRAPH_PATH, 'rb') as f:
                SEMANTIC_KNOWLEDGE_GRAPH = pickle.load(f)
            print(f"   ‚Üí KG: {SEMANTIC_KNOWLEDGE_GRAPH.number_of_nodes()} nodes, {SEMANTIC_KNOWLEDGE_GRAPH.number_of_edges()} edges")
        except:
            SEMANTIC_KNOWLEDGE_GRAPH = nx.MultiDiGraph()
    else:
        SEMANTIC_KNOWLEDGE_GRAPH = nx.MultiDiGraph()

def save_knowledge_graph():
    try:
        with open(KNOWLEDGE_GRAPH_PATH, 'wb') as f:
            pickle.dump(SEMANTIC_KNOWLEDGE_GRAPH, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print(f"   ‚ö† KG save failed: {e}")

def add_field_to_graph(field_name, enriched_name, object_name, property_name, confidence, pii_category, application, eim_id):
    field_node = f"field:{field_name.lower()}"
    SEMANTIC_KNOWLEDGE_GRAPH.add_node(field_node, type='field', original_name=field_name, enriched_name=enriched_name)
    enriched_node = f"concept:{enriched_name.lower()}"
    SEMANTIC_KNOWLEDGE_GRAPH.add_node(enriched_node, type='concept', name=enriched_name)
    object_node = f"object:{object_name.lower()}"
    SEMANTIC_KNOWLEDGE_GRAPH.add_node(object_node, type='object', name=object_name)
    property_node = f"property:{property_name.lower()}"
    SEMANTIC_KNOWLEDGE_GRAPH.add_node(property_node, type='property', name=property_name)
    
    SEMANTIC_KNOWLEDGE_GRAPH.add_edge(field_node, enriched_node, relationship='enriched_to')
    SEMANTIC_KNOWLEDGE_GRAPH.add_edge(enriched_node, object_node, relationship='mapped_to_object')
    SEMANTIC_KNOWLEDGE_GRAPH.add_edge(enriched_node, property_node, relationship='mapped_to_property')


# ============================================================================
# LEXICAL + SEMANTIC MATCHING
# ============================================================================

def calculate_lexical_similarity(str1: str, str2: str) -> float:
    str1 = str1.lower()
    str2 = str2.lower()
    seq_similarity = SequenceMatcher(None, str1, str2).ratio()
    words1 = set(str1.split('_') + str1.split())
    words2 = set(str2.split('_') + str2.split())
    jaccard = len(words1 & words2) / len(words1 | words2) if words1 or words2 else 0
    return (0.5 * seq_similarity + 0.5 * jaccard)


def hybrid_similarity_search(query: str, vector_store: Any, all_items: List[str], top_k: int = 5) -> List[Tuple[str, float, Dict[str, float]]]:
    """Perform hybrid lexical + semantic similarity search with ChromaDB."""
    
    # Semantic search using ChromaDB
    semantic_results = vector_store.similarity_search_with_score(query, k=top_k * 2)
    semantic_dict = {}
    
    for doc, score in semantic_results:
        # Extract item from metadata
        item = doc.metadata.get('object') or doc.metadata.get('property')
        if item:
            semantic_dict[item] = float(score)
    
    # Lexical search
    lexical_scores = {item: calculate_lexical_similarity(query, item) for item in all_items}
    
    # Combine scores
    all_items_set = set(semantic_dict.keys()) | set(lexical_scores.keys())
    combined_results = []
    
    for item in all_items_set:
        semantic_score = semantic_dict.get(item, 0)
        lexical_score = lexical_scores.get(item, 0)
        
        # For ChromaDB, lower score = more similar (L2 distance)
        # Normalize: convert distance to similarity
        semantic_score_norm = 1 / (1 + semantic_score) if semantic_score > 0 else 1.0
        
        combined_score = (0.3 * lexical_score) + (0.7 * semantic_score_norm)
        combined_results.append((item, combined_score, {'lexical': lexical_score, 'semantic': semantic_score_norm, 'combined': combined_score}))
    
    combined_results.sort(key=lambda x: x[1], reverse=True)
    return combined_results[:top_k]


# ============================================================================
# MEMORY FUNCTIONS
# ============================================================================

def load_all_memory():
    global MAPPING_CACHE, PROCEDURAL_MEMORY, EPISODIC_MEMORY
    if os.path.exists(CACHE_JSON_PATH):
        with open(CACHE_JSON_PATH, 'r') as f:
            MAPPING_CACHE = json.load(f)
        print(f"   ‚Üí Cache: {len(MAPPING_CACHE)} entries")
    if os.path.exists(PROCEDURAL_MEMORY_PATH):
        with open(PROCEDURAL_MEMORY_PATH, 'r') as f:
            PROCEDURAL_MEMORY = json.load(f)
    else:
        PROCEDURAL_MEMORY = {"prefix_patterns": {}, "suffix_patterns": {}}
    if os.path.exists(EPISODIC_MEMORY_PATH):
        with open(EPISODIC_MEMORY_PATH, 'r') as f:
            EPISODIC_MEMORY = json.load(f)
    initialize_knowledge_graph()

def save_all_memory():
    with open(CACHE_JSON_PATH, 'w') as f:
        json.dump(MAPPING_CACHE, f, indent=2)
    with open(PROCEDURAL_MEMORY_PATH, 'w') as f:
        json.dump(PROCEDURAL_MEMORY, f, indent=2)
    with open(EPISODIC_MEMORY_PATH, 'w') as f:
        json.dump(EPISODIC_MEMORY, f, indent=2)
    save_knowledge_graph()

def update_all_memory(field_name, enriched_name, obj, prop, strategy, confidence, pii, application, eim_id):
    cache_key = f"{enriched_name.lower()}|{field_name.lower()[:100]}"
    MAPPING_CACHE[cache_key] = {'object': obj, 'property': prop, 'confidence': confidence}
    EPISODIC_MEMORY.append({"timestamp": datetime.now().isoformat(), "field": field_name, "enriched": enriched_name, "mapping": {"object": obj, "property": prop}, "confidence": confidence, "pii": pii})
    if len(EPISODIC_MEMORY) > 1000:
        EPISODIC_MEMORY[:] = EPISODIC_MEMORY[-1000:]
    add_field_to_graph(field_name, enriched_name, obj, prop, confidence, pii, application, eim_id)


# ============================================================================
# EMBEDDINGS
# ============================================================================

class OpenAIEmbeddings(Embeddings):
    def __init__(self, model: str = OPENAI_EMBEDDING_MODEL):
        self.model = model
        self.client = openai_client
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        for idx, text in enumerate(texts, 1):
            print(f"      üìä Embedding {idx}/{len(texts)}")
            embeddings.append(self.embed_query(text))
            time.sleep(0.1)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        response = self.client.embeddings.create(model=self.model, input=[text[:8000]])
        return response.data[0].embedding


def call_openai_with_retry(messages: List[Dict], max_retries: int = 10) -> str:
    for attempt in range(max_retries):
        try:
            response = openai_client.chat.completions.create(model=OPENAI_REASONING_MODEL, messages=messages)
            return response.choices[0].message.content
        except Exception as e:
            print(f"      ‚ö† Attempt {attempt + 1} failed: {e}")
            time.sleep(2 ** attempt)
    raise Exception(f"Failed after {max_retries} attempts")


# ============================================================================
# AGENT STATE
# ============================================================================

class EnrichmentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    current_field: Dict[str, Any]
    enriched_name: str
    enriched_description: str
    object_name: str
    property_name: str
    pii_classification: Dict[str, Any]
    enrichment_rationale: str
    mapping_rationale: str
    object_reasoning: str
    property_reasoning: str
    comparison_note: str
    enrichment_confidence: Dict[str, Any]
    mapping_confidence: Dict[str, Any]
    pii_confidence: Dict[str, Any]
    intersection_vector_store: Any
    object_vector_store: Any
    property_vector_store: Any
    all_objects: List[str]
    all_properties: List[str]
    objects_dict: Dict[str, List[str]]
    acronym_dict: Dict[str, str]
    mapping_strategy: str


# ============================================================================
# ENRICHMENT COORDINATOR WITH INTELLIGENT CONTEXT USAGE
# ============================================================================

def enrichment_coordinator_node(state: EnrichmentState) -> EnrichmentState:
    """Coordinator with intelligent context usage and multi-stage validation (NO MODELLING)."""
    
    field_name = state['current_field'].get('Field Name', '')
    app_name = state['current_field'].get('Application Name', '')
    app_desc = state['current_field'].get('Application Description', '')
    
    print(f"\n{'='*80}")
    print(f"PROCESSING: {field_name}")
    print(f"APPLICATION: {app_name}")
    print(f"{'='*80}")
    
    # ========================================================================
    # STEP 1: INTELLIGENT ENRICHMENT WITH SMART CONTEXT USAGE
    # ========================================================================
    print("\n[STEP 1] ISO 11179 Name Enrichment (Intelligent Context)...")
    
    # Format acronym list for prompt
    acronym_dict = state.get('acronym_dict', {})
    if acronym_dict:
        acronym_list = "ACRONYM REFERENCE (expand these when found):\n"
        for acr, exp in sorted(acronym_dict.items()):
            acronym_list += f"  - {acr} = {exp}\n"
    else:
        acronym_list = "ACRONYM REFERENCE: (None provided)\n"
    
    enrichment_prompt = f"""You are an ISO 11179 naming expert. Create a clear, logical, human-readable name.

FIELD NAME: {field_name}
APPLICATION CONTEXT: {app_name}
(Use context intelligently - only when it adds clarity without redundancy)

{acronym_list}

INTELLIGENT CONTEXT USAGE RULES:

1. CHECK IF FIELD HAS ITS OWN ENTITY REFERENCE:
   Look for entity words in the field name itself:
   - party, customer, account, transaction, product, employee, user, invoice, payment, order
   
   If field contains these, DON'T add context entity:
   ‚úì "partyModelID" ‚Üí "Party Model Identifier" (NOT "Customer Party Model Identifier")
   ‚úì "accountBalance" ‚Üí "Account Balance" (NOT "Customer Account Balance")
   ‚úì "transactionAmount" ‚Üí "Transaction Amount" (NOT "Customer Transaction Amount")
   ‚úì "partyReference" ‚Üí "Party Reference" (NOT "Customer Party Reference")
   ‚úì "productCode" ‚Üí "Product Code" (NOT "Customer Product Code")

2. USE CONTEXT ONLY FOR GENERIC FIELDS:
   If field is generic (no entity reference), use application context:
   ‚úì "alternateNameType" in Customer app ‚Üí "Customer Alternate Name Type"
   ‚úì "statusCode" in Customer app ‚Üí "Customer Status Code"
   ‚úì "identifier" in Customer app ‚Üí "Customer Identifier"
   ‚úì "referenceNumber" in Customer app ‚Üí "Customer Reference Number"

3. AVOID REDUNDANCY:
   Never create redundant names like:
   ‚ùå "Customer Party Identifier" (redundant - party IS the entity)
   ‚ùå "Customer Account Balance" (account is already the entity)
   ‚ùå "Customer Transaction Amount" (transaction is already the entity)

4. EXPAND ABBREVIATIONS LOGICALLY:
   - ID, Id ‚Üí Identifier
   - Num, No ‚Üí Number
   - Addr ‚Üí Address
   - Ref ‚Üí Reference
   - Amt ‚Üí Amount
   - Cd ‚Üí Code
   - Dt ‚Üí Date
   - Type ‚Üí Type (keep as is)
   - Model ‚Üí Model (keep as is)

5. FORMAT:
   - Title Case with proper spacing
   - Clear and concise (max 5 words)
   - Readable by non-technical people

EXAMPLES:

Field: "partyModelID" in Customer app
‚úì CORRECT: "Party Model Identifier" (expanded ID ‚Üí Identifier)
‚ùå WRONG: "Customer Party Model Identifier" (redundant)

Field: "alternateNameType" in Customer app
‚úì CORRECT: "Customer Alternate Name Type"
‚ùå WRONG: "Alternate Name Type" (loses context)

Field: "accountBalance" in Customer app
‚úì CORRECT: "Account Balance"
‚ùå WRONG: "Customer Account Balance" (redundant)

Field: "custStatusCd" in Customer app (assume CUST=Customer in acronym list)
‚úì CORRECT: "Customer Status Code" (expanded CUST ‚Üí Customer, Cd ‚Üí Code)
‚ùå WRONG: "Cust Status Code" (didn't expand acronym)

Field: "txnAmtRef" in Customer app (assume TXN=Transaction in acronym list)
‚úì CORRECT: "Transaction Amount Reference" (expanded TXN ‚Üí Transaction, Amt ‚Üí Amount, Ref ‚Üí Reference)
‚ùå WRONG: "Customer Transaction Amount Reference" (redundant, txn already specifies entity)

Field: "acctNum" in Customer app (assume ACCT=Account in acronym list)
‚úì CORRECT: "Account Number" (expanded ACCT ‚Üí Account, Num ‚Üí Number)
‚ùå WRONG: "Customer Account Number" (redundant)

Field: "statusCode" in Customer app
‚úì CORRECT: "Customer Status Code" (generic field, needs context)
‚ùå WRONG: "Status Code" (loses context)

Field: "transactionRef" in Customer app
‚úì CORRECT: "Transaction Reference"
‚ùå WRONG: "Customer Transaction Reference" (redundant)

Field: "partyIdentifier" in Customer app
‚úì CORRECT: "Party Identifier"
‚ùå WRONG: "Customer Party Identifier" (redundant)

Field: "referenceNumber" in Customer app
‚úì CORRECT: "Customer Reference Number" (generic, needs context)
‚ùå WRONG: "Reference Number" (loses context)

Field: "empID" in Customer app (assume EMP=Employee in acronym list)
‚úì CORRECT: "Employee Identifier" (expanded EMP ‚Üí Employee, ID ‚Üí Identifier)
‚ùå WRONG: "Customer Employee Identifier" (redundant)

THINK STEP BY STEP:
1. Does the field name contain an entity word (party, account, transaction, product, etc.)?
2. If YES ‚Üí Use that entity, don't add application context
3. If NO ‚Üí Field is generic, add application context for clarity
4. Check acronym reference list for any matching acronyms (exact or substring match)
5. Expand all acronyms and abbreviations properly
6. Ensure result is logical, human-readable, and non-redundant

Now enrich this field name following these rules.
OUTPUT: ONLY the enriched name, nothing else."""

    try:
        enriched_name = call_openai_with_retry([{"role": "user", "content": enrichment_prompt}]).strip()
        enriched_name = enriched_name.split('\n')[0].strip().strip('"').strip("'")
        state['enriched_name'] = enriched_name
        print(f"   ‚úì Enriched: {enriched_name}")
    except:
        state['enriched_name'] = field_name
        print(f"   ‚ùå Using original: {field_name}")
    
    # ========================================================================
    # STEP 2: DEFINITION
    # ========================================================================
    print("\n[STEP 2] Definition...")
    try:
        definition = call_openai_with_retry([{"role": "user", "content": f"Create a precise, 2-sentence ISO 11179 definition for: {state['enriched_name']}. Be clear and concise."}]).strip()
        state['enriched_description'] = definition
        print(f"   ‚úì Definition: {definition[:80]}...")
    except:
        state['enriched_description'] = f"A data element representing {state['enriched_name'].lower()}."
    
    # ========================================================================
    # STEP 3: INTERSECTION MATCHING
    # ========================================================================
    print("\n[STEP 3] Intersection Matching (Object.Property combos)...")
    
    query = f"{state['enriched_name']} {state['enriched_description']}"
    
    # ChromaDB similarity search
    intersection_results = state['intersection_vector_store'].similarity_search_with_score(query, k=5)
    
    top_matches = []
    for doc, score in intersection_results:
        obj = doc.metadata.get('object', '')
        prop = doc.metadata.get('property', '')
        if obj and prop:
            top_matches.append({'object': obj, 'property': prop, 'score': float(score)})
    
    if not top_matches:
        print("   ‚ö† No intersection matches found, will use separate search")
        top_matches = [{'object': 'Unknown', 'property': 'Unknown', 'score': 999.0}]
    
    print(f"   ‚Üí Top 3 intersection matches:")
    for i, match in enumerate(top_matches[:3], 1):
        print(f"      {i}. {match['object']}.{match['property']} (score: {match['score']:.4f})")
    
    # ========================================================================
    # STEP 4: SUPERVISOR VALIDATION - INTERSECTION
    # ========================================================================
    print("\n[STEP 4] Supervisor Validates Intersection Match...")
    
    supervisor_intersection_prompt = f"""You are a SUPERVISOR validating semantic mapping.

ENRICHED FIELD: {state['enriched_name']}
DEFINITION: {state['enriched_description']}

TOP INTERSECTION MATCH: {top_matches[0]['object']}.{top_matches[0]['property']}
MATCH SCORE: {top_matches[0]['score']:.4f}

CONTEXT: This is a PRE-EXISTING object.property combination from the PBT model (not separately constructed).

VALIDATION CRITERIA:
‚úì APPROVE if:
  - Object accurately represents the entity
  - Property accurately represents the characteristic  
  - Combination is semantically sound for this field
  - Any reasonable domain expert would agree

‚ö†Ô∏è BE STRICT but remember:
  - This is safer than separately finding object + property
  - Perfect semantic match is rare - "good enough" is acceptable
  - Reject ONLY if clearly wrong or nonsensical

OUTPUT (JSON only):
{{
    "approved": <true/false>,
    "semantic_rationale": "<WHY this object.property semantically matches the field - explain the conceptual fit, not the method>",
    "object_reasoning": "<Why this OBJECT represents the correct entity/concept>",
    "property_reasoning": "<Why this PROPERTY represents the correct characteristic>",
    "semantic_correctness_score": <0-100>,
    "confidence_level": "<high|medium|low>"
}}"""

    try:
        supervisor_response = call_openai_with_retry([{"role": "user", "content": supervisor_intersection_prompt}])
        if '```json' in supervisor_response:
            supervisor_response = supervisor_response.split('```json')[1].split('```')[0]
        supervisor_data = json.loads(supervisor_response.strip())
        
        intersection_approved = supervisor_data.get('approved', False)
        intersection_semantic_rationale = supervisor_data.get('semantic_rationale', '')
        intersection_object_reasoning = supervisor_data.get('object_reasoning', '')
        intersection_property_reasoning = supervisor_data.get('property_reasoning', '')
        intersection_score = supervisor_data.get('semantic_correctness_score', 0)
        intersection_confidence = supervisor_data.get('confidence_level', 'medium')
        
        print(f"   ‚Üí Supervisor: {'‚úì APPROVED' if intersection_approved else '‚úó REJECTED'}")
        print(f"   ‚Üí Score: {intersection_score}/100")
        print(f"   ‚Üí Confidence: {intersection_confidence}")
        print(f"   ‚Üí Semantic rationale: {intersection_semantic_rationale[:150]}...")
        
        if intersection_approved:
            # USE INTERSECTION MATCH
            state['object_name'] = top_matches[0]['object']
            state['property_name'] = top_matches[0]['property']
            state['mapping_strategy'] = 'intersection_approved'
            state['mapping_rationale'] = intersection_semantic_rationale
            state['object_reasoning'] = intersection_object_reasoning
            state['property_reasoning'] = intersection_property_reasoning
            state['comparison_note'] = 'Intersection match approved without need for separate search.'
            print(f"   ‚úì USING: {state['object_name']}.{state['property_name']}")
            
        else:
            print(f"   ‚Üí Intersection rejected (score: {intersection_score}). Trying separate search...")
            
            # Store intersection details for comparison
            intersection_details = {
                'object': top_matches[0]['object'],
                'property': top_matches[0]['property'],
                'score': intersection_score,
                'semantic_rationale': intersection_semantic_rationale,
                'object_reasoning': intersection_object_reasoning,
                'property_reasoning': intersection_property_reasoning
            }
            
            # ================================================================
            # STEP 5-6: SEPARATE OBJECT & PROPERTY SEARCH
            # ================================================================
            print("\n[STEP 5] Separate Object Search...")
            object_results = hybrid_similarity_search(query, state['object_vector_store'], state['all_objects'], top_k=5)
            print(f"   ‚Üí Top 3 objects:")
            for i, (obj, score, breakdown) in enumerate(object_results[:3], 1):
                print(f"      {i}. {obj} (score: {score:.3f})")
            
            print("\n[STEP 6] Separate Property Search...")
            property_results = hybrid_similarity_search(query, state['property_vector_store'], state['all_properties'], top_k=5)
            print(f"   ‚Üí Top 3 properties:")
            for i, (prop, score, breakdown) in enumerate(property_results[:3], 1):
                print(f"      {i}. {prop} (score: {score:.3f})")
            
            # ================================================================
            # STEP 7: SUPERVISOR VALIDATES SEPARATE MATCH
            # ================================================================
            print("\n[STEP 7] Supervisor Validates Separate Combo...")
            
            proposed_object = object_results[0][0]
            proposed_property = property_results[0][0]
            
            supervisor_separate_prompt = f"""You are a SUPERVISOR validating semantic mapping. BE EXTREMELY CRITICAL.

ENRICHED FIELD: {state['enriched_name']}
DEFINITION: {state['enriched_description']}

INTERSECTION MATCH (REJECTED with score {intersection_details['score']}/100):
- {intersection_details['object']}.{intersection_details['property']}
- Rejection reason: {intersection_details['semantic_rationale'][:200]}

SEPARATE SEARCH RESULTS (found independently):
- Object: {proposed_object}
- Property: {proposed_property}

‚ö†Ô∏è CRITICAL WARNING: These were found SEPARATELY (HIGH RISK of nonsensical combination)

YOUR TASK: Provide TWO types of analysis:

1. SEMANTIC ANALYSIS (focus on meaning, not method):
   - Does {proposed_object} semantically represent the correct entity/concept for this field?
   - Does {proposed_property} semantically represent the correct characteristic?
   - Does the combination make conceptual sense?

2. COMPARISON ANALYSIS:
   - Is this semantically better/worse than intersection?
   - Which mapping is more accurate conceptually?

OUTPUT (JSON only):
{{
    "approved": <true/false>,
    "semantic_rationale": "<WHY this object.property semantically matches - explain conceptual fit, not method>",
    "object_reasoning": "<Why this OBJECT represents the correct entity/concept>",
    "property_reasoning": "<Why this PROPERTY represents the correct characteristic>",
    "is_worse_than_intersection": <true/false>,
    "is_significantly_better": <true/false>,
    "comparison_rationale": "<Compare semantic accuracy of both mappings>",
    "semantic_correctness_score": <0-100>,
    "recommendation": "<use_separate | use_intersection | flag_for_review>"
}}"""

            try:
                supervisor_response2 = call_openai_with_retry([{"role": "user", "content": supervisor_separate_prompt}])
                if '```json' in supervisor_response2:
                    supervisor_response2 = supervisor_response2.split('```json')[1].split('```')[0]
                supervisor_data2 = json.loads(supervisor_response2.strip())
                
                separate_approved = supervisor_data2.get('approved', False)
                separate_semantic_rationale = supervisor_data2.get('semantic_rationale', '')
                separate_object_reasoning = supervisor_data2.get('object_reasoning', '')
                separate_property_reasoning = supervisor_data2.get('property_reasoning', '')
                is_worse_than_intersection = supervisor_data2.get('is_worse_than_intersection', False)
                is_significantly_better = supervisor_data2.get('is_significantly_better', False)
                comparison_rationale = supervisor_data2.get('comparison_rationale', '')
                separate_score = supervisor_data2.get('semantic_correctness_score', 0)
                recommendation = supervisor_data2.get('recommendation', 'use_separate')
                
                print(f"   ‚Üí Supervisor: {'‚úì APPROVED' if separate_approved else '‚úó REJECTED'}")
                print(f"   ‚Üí Score: {separate_score}/100 vs Intersection: {intersection_details['score']}/100")
                print(f"   ‚Üí Worse than intersection: {is_worse_than_intersection}")
                print(f"   ‚Üí Significantly better: {is_significantly_better}")
                print(f"   ‚Üí Recommendation: {recommendation}")
                
                # DECISION LOGIC with comparison to intersection
                if recommendation == 'use_intersection' or is_worse_than_intersection:
                    print(f"   ‚Üí REVERTING to intersection match (separate was worse)")
                    state['object_name'] = intersection_details['object']
                    state['property_name'] = intersection_details['property']
                    state['mapping_strategy'] = 'intersection_fallback'
                    state['mapping_rationale'] = intersection_details['semantic_rationale']
                    state['object_reasoning'] = intersection_details['object_reasoning']
                    state['property_reasoning'] = intersection_details['property_reasoning']
                    state['comparison_note'] = f"Separate search explored but intersection was semantically more accurate. {comparison_rationale}"
                    print(f"   ‚úì USING INTERSECTION: {state['object_name']}.{state['property_name']}")
                    
                elif separate_approved and is_significantly_better:
                    state['object_name'] = proposed_object
                    state['property_name'] = proposed_property
                    state['mapping_strategy'] = 'separate_search_approved'
                    state['mapping_rationale'] = separate_semantic_rationale
                    state['object_reasoning'] = separate_object_reasoning
                    state['property_reasoning'] = separate_property_reasoning
                    state['comparison_note'] = f"Separate search produced significantly better semantic match than intersection. {comparison_rationale}"
                    print(f"   ‚úì USING SEPARATE (SIGNIFICANTLY BETTER): {state['object_name']}.{state['property_name']}")
                    
                elif separate_approved and not is_significantly_better:
                    state['object_name'] = proposed_object
                    state['property_name'] = proposed_property  
                    state['mapping_strategy'] = 'separate_search_cautious'
                    state['mapping_rationale'] = separate_semantic_rationale
                    state['object_reasoning'] = separate_object_reasoning
                    state['property_reasoning'] = separate_property_reasoning
                    state['comparison_note'] = f"Separate search marginally better but with caution due to separate construction risk. {comparison_rationale}"
                    print(f"   ‚ö† USING SEPARATE (CAUTIOUSLY): {state['object_name']}.{state['property_name']}")
                    
                else:
                    # Not approved - use intersection as safer fallback
                    print(f"   ‚Üí Separate not approved, reverting to intersection")
                    state['object_name'] = intersection_details['object']
                    state['property_name'] = intersection_details['property']
                    state['mapping_strategy'] = 'intersection_safer_fallback'
                    state['mapping_rationale'] = intersection_details['semantic_rationale']
                    state['object_reasoning'] = intersection_details['object_reasoning']
                    state['property_reasoning'] = intersection_details['property_reasoning']
                    state['comparison_note'] = f"Separate search not validated. Using intersection as safer semantic choice. {comparison_rationale}"
                    print(f"   ‚úì USING INTERSECTION (SAFER): {state['object_name']}.{state['property_name']}")
                
            except Exception as e:
                print(f"   ‚ùå Error: {e}")
                # On error, fall back to intersection (safer than using unvalidated separate result)
                state['object_name'] = intersection_details['object']
                state['property_name'] = intersection_details['property']
                state['mapping_strategy'] = 'error_fallback_to_intersection'
                state['mapping_rationale'] = f"Error during validation, using intersection as safer fallback: {str(e)}"
                print(f"   ‚úì USING INTERSECTION (ERROR FALLBACK): {state['object_name']}.{state['property_name']}")
    
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
        state['object_name'] = top_matches[0]['object'] if top_matches else 'Unknown'
        state['property_name'] = top_matches[0]['property'] if top_matches else 'Unknown'
        state['mapping_strategy'] = 'error'
        state['mapping_rationale'] = f"Error: {str(e)}"
    
    # ========================================================================
    # STEP 8: PII CLASSIFICATION
    # ========================================================================
    print("\n[STEP 8] PII Classification...")
    try:
        pii_prompt = f"""Classify PII: {state['enriched_name']}
CATEGORIES: NON PERSONAL DATA, PERSONAL DATA, SENSITIVE PERSONAL DATA
OUTPUT (JSON): {{"pii_category": "<category>", "is_pii": <bool>, "regulatory_considerations": ["<regs>"], "detailed_rationale": "<why>"}}"""
        
        pii_response = call_openai_with_retry([{"role": "user", "content": pii_prompt}])
        if '```json' in pii_response:
            pii_response = pii_response.split('```json')[1].split('```')[0]
        pii_data = json.loads(pii_response.strip())
        state['pii_classification'] = pii_data
        print(f"   ‚úì PII: {pii_data.get('pii_category', 'UNKNOWN')}")
    except:
        state['pii_classification'] = {"pii_category": "NON PERSONAL DATA", "is_pii": False, "regulatory_considerations": [], "detailed_rationale": "Failed"}
    
    # ========================================================================
    # STEP 9: FINAL CONFIDENCE SCORING
    # ========================================================================
    print("\n[STEP 9] Final Confidence Scoring...")
    
    # Apply confidence penalties based on mapping strategy
    strategy_context = ""
    if state.get('mapping_strategy') == 'intersection_approved':
        strategy_context = "STRATEGY: Intersection match was approved (highest confidence scenario). Target score: 85-95."
    elif state.get('mapping_strategy') == 'intersection_fallback':
        strategy_context = "STRATEGY: Reverted to intersection because separate search was worse (medium-high confidence). Target score: 75-85."
    elif state.get('mapping_strategy') == 'intersection_safer_fallback':
        strategy_context = "STRATEGY: Used intersection as safer fallback when separate wasn't validated (medium confidence). Target score: 65-80."
    elif state.get('mapping_strategy') == 'separate_search_approved':
        strategy_context = "STRATEGY: Separate search significantly better than intersection (medium confidence due to separate risk). Target score: 65-75. APPLY PENALTY: -10 points."
    elif state.get('mapping_strategy') == 'separate_search_cautious':
        strategy_context = "STRATEGY: Separate search only slightly better (low-medium confidence). Target score: 50-65. APPLY PENALTY: -20 points."
    elif state.get('mapping_strategy') == 'low_confidence_match':
        strategy_context = "STRATEGY: Low confidence - neither validated properly (low confidence). Target score: 30-50. APPLY PENALTY: -30 points."
    else:
        strategy_context = "STRATEGY: Unknown or error scenario (very low confidence). Target score: 20-40. APPLY PENALTY: -40 points."
    
    final_prompt = f"""Final SUPERVISOR confidence scoring.

MAPPING RESULT:
- Field: {state['enriched_name']}
- Description: {state['enriched_description']}
- Mapped to: {state['object_name']}.{state['property_name']}

SEMANTIC REASONING (from validation):
- Mapping Rationale: {state.get('mapping_rationale', '')}
- Object Reasoning: {state.get('object_reasoning', '')}
- Property Reasoning: {state.get('property_reasoning', '')}

{strategy_context}

IMPORTANT: Focus on SEMANTIC CORRECTNESS, not methodology.

Your supporting and contradicting reasons should explain:
‚úì WHY the object semantically represents the correct entity/concept
‚úì WHY the property semantically captures the correct characteristic  
‚úì WHY the combination makes conceptual sense (or doesn't)
‚úì Specific evidence from the field name/description that supports/contradicts the mapping

‚ùå DON'T just say "intersection was used" or "separate search found this"
‚úì DO explain the semantic/conceptual fit

EXAMPLE GOOD REASONS:
- Supporting: "Field name contains 'party' which directly indicates this is an entity characteristic, matching the Party object"
- Supporting: "The term 'identifier' in the field clearly represents a unique reference, aligning with Identifier property"
- Contradicting: "Field might represent a code rather than pure identifier, introducing some ambiguity"
- Contradicting: "The Account object might be more specific than Party for this banking context"

CONFIDENCE SCORING (follow strictly):
- Intersection-based: 85-95 (pre-validated combo) or 75-85 (fallback) or 65-80 (safer default)
- Separate search: 60-70 (significantly better) or 40-55 (cautious) with penalties applied

OUTPUT (JSON):
{{
    "enrichment_rationale": "<why the enriched name is accurate>",
    "enrichment_confidence": {{
        "supporting_reasons": [
            {{"reason": "<specific semantic evidence>", "weight": <0.0-1.0>}},
            {{"reason": "<specific semantic evidence>", "weight": <0.0-1.0>}},
            {{"reason": "<specific semantic evidence>", "weight": <0.0-1.0>}}
        ],
        "contradictory_reasons": [
            {{"reason": "<specific semantic concern>", "weight": <0.0-1.0>}},
            {{"reason": "<specific semantic concern>", "weight": <0.0-1.0>}}
        ],
        "confidence_score": <0-100>,
        "confidence_rationale": "<overall assessment of enrichment quality>"
    }},
    "mapping_confidence": {{
        "supporting_reasons": [
            {{"reason": "<WHY object semantically fits - specific evidence>", "weight": <0.0-1.0>}},
            {{"reason": "<WHY property semantically fits - specific evidence>", "weight": <0.0-1.0>}},
            {{"reason": "<WHY combination makes conceptual sense - specific evidence>", "weight": <0.0-1.0>}}
        ],
        "contradictory_reasons": [
            {{"reason": "<semantic concern or alternative interpretation>", "weight": <0.0-1.0>}},
            {{"reason": "<semantic ambiguity or limitation>", "weight": <0.0-1.0>}}
        ],
        "base_confidence_score": <0-100>,
        "strategy_penalty": <0 to -40>,
        "final_confidence_score": <0-100>,
        "confidence_rationale": "<focus on semantic correctness, mention strategy impact briefly>"
    }},
    "pii_confidence": {{
        "supporting_reasons": [
            {{"reason": "<specific PII indicator>", "weight": <0.0-1.0>}},
            {{"reason": "<specific PII indicator>", "weight": <0.0-1.0>}},
            {{"reason": "<specific PII indicator>", "weight": <0.0-1.0>}}
        ],
        "contradictory_reasons": [
            {{"reason": "<reason it might not be PII>", "weight": <0.0-1.0>}},
            {{"reason": "<reason it might not be PII>", "weight": <0.0-1.0>}}
        ],
        "confidence_score": <0-100>,
        "confidence_rationale": "<PII classification reasoning>"
    }}
}}"""

    try:
        final_response = call_openai_with_retry([{"role": "user", "content": final_prompt}])
        if '```json' in final_response:
            final_response = final_response.split('```json')[1].split('```')[0]
        final_data = json.loads(final_response.strip())
        
        state['enrichment_rationale'] = final_data.get('enrichment_rationale', '')
        state['enrichment_confidence'] = final_data.get('enrichment_confidence', {})
        
        # Handle mapping confidence with penalty
        mapping_conf = final_data.get('mapping_confidence', {})
        base_score = mapping_conf.get('base_confidence_score', mapping_conf.get('final_confidence_score', 0))
        penalty = mapping_conf.get('strategy_penalty', 0)
        final_score = mapping_conf.get('final_confidence_score', base_score + penalty)
        
        # Ensure final score is within bounds
        final_score = max(0, min(100, final_score))
        
        mapping_conf['confidence_score'] = final_score
        state['mapping_confidence'] = mapping_conf
        
        state['pii_confidence'] = final_data.get('pii_confidence', {})
        
        print(f"   ‚úì Confidence: E={state['enrichment_confidence'].get('confidence_score', 0)}, M={final_score} (base: {base_score}, penalty: {penalty}), P={state['pii_confidence'].get('confidence_score', 0)}")
    except Exception as e:
        print(f"   ‚ö† Scoring failed: {e}")
        state['enrichment_rationale'] = 'Failed'
        state['enrichment_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
        state['mapping_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
        state['pii_confidence'] = {'confidence_score': 0, 'supporting_reasons': [], 'contradictory_reasons': [], 'confidence_rationale': 'Failed'}
    
    # UPDATE MEMORY
    print("\n[STEP 10] Updating Memory...")
    try:
        update_all_memory(field_name, state['enriched_name'], state['object_name'], state['property_name'], state['mapping_strategy'], state['mapping_confidence'].get('confidence_score', 0), state['pii_classification'].get('pii_category', 'NON PERSONAL DATA'), app_name, state['current_field'].get('EIM ID', ''))
    except Exception as e:
        print(f"   ‚ö† Memory update failed: {e}")
    
    print(f"{'='*80}\n")
    return state


def should_continue(state: EnrichmentState) -> Literal["end"]:
    return "end"


def create_enrichment_graph():
    workflow = StateGraph(EnrichmentState)
    workflow.add_node("enrichment_coordinator", enrichment_coordinator_node)
    workflow.set_entry_point("enrichment_coordinator")
    workflow.add_conditional_edges("enrichment_coordinator", should_continue, {"end": END})
    return workflow.compile()


# ============================================================================
# MAIN PROCESSING
# ============================================================================

def process_data_enrichment_and_mapping():
    print("="*80)
    print("ISO 11179 WITH INTELLIGENT CONTEXT USAGE (NO MODELLING)")
    print("Strict Validation + Intersection Fallback")
    print("Persistent ChromaDB Embeddings")
    print("="*80)
    
    print("\n[1] Loading Memory...")
    load_all_memory()
    
    print("\n[2] Loading Input...")
    with open(INPUT_JSON_PATH, 'r') as f:
        input_data = json.load(f)
    print(f"   ‚Üí {len(input_data)} records")
    
    # Load acronyms
    print("\n[2.5] Loading Acronym Reference...")
    acronym_dict = {}
    if os.path.exists(ACRONYM_CSV_PATH):
        try:
            acronym_df = pd.read_csv(ACRONYM_CSV_PATH)
            for _, row in acronym_df.iterrows():
                acronym = str(row['acronym']).strip().upper()
                expansion = str(row['expansion']).strip()
                acronym_dict[acronym] = expansion
            print(f"   ‚Üí Loaded {len(acronym_dict)} acronyms")
            if acronym_dict:
                sample = list(acronym_dict.items())[:5]
                for acr, exp in sample:
                    print(f"      ‚Ä¢ {acr} = {exp}")
                if len(acronym_dict) > 5:
                    print(f"      ‚Ä¢ ... and {len(acronym_dict) - 5} more")
        except Exception as e:
            print(f"   ‚ö† Error loading acronyms: {e}")
            print(f"   ‚Üí Expected CSV format: columns 'acronym' and 'expansion'")
            acronym_dict = {}
    else:
        print(f"   ‚Üí Acronym file not found: {ACRONYM_CSV_PATH}")
        print(f"   ‚Üí Continuing without acronym reference")
    
    print("\n[3] Initializing ChromaDB...")
    initialize_chromadb()
    
    print("\n[4] Loading PBT Excel...")
    excel_df = pd.read_excel(EXCEL_PATH)
    
    objects_dict = {}
    all_objects = []
    all_properties = []
    
    for _, row in excel_df.iterrows():
        obj = str(row['Object name']).strip()
        prop = str(row['Property name']).strip()
        
        if obj not in objects_dict:
            objects_dict[obj] = []
            all_objects.append(obj)
        if prop not in objects_dict[obj]:
            objects_dict[obj].append(prop)
        if prop not in all_properties:
            all_properties.append(prop)
    
    print(f"   ‚Üí {len(all_objects)} objects, {len(all_properties)} properties")
    
    print("\n[5] Creating/Loading Vector Stores with ChromaDB...")
    embeddings = OpenAIEmbeddings()
    
    intersection_store, object_store, property_store = create_or_load_vector_stores(
        objects_dict, all_objects, all_properties, embeddings
    )
    
    print(f"\n   ‚úì All vector stores ready!")
    
    print("\n[6] Initializing Workflow...")
    app = create_enrichment_graph()
    
    print(f"\n[7] Processing Records...")
    
    successful = 0
    failed = 0
    strategy_stats = defaultdict(int)
    
    for idx, record in enumerate(input_data):
        eim_id = record.get("EIM ID", "")
        
        try:
            initial_state = {
                "messages": [], "current_field": record, "enriched_name": "", "enriched_description": "",
                "object_name": "", "property_name": "",
                "pii_classification": {}, "enrichment_rationale": "", "mapping_rationale": "",
                "object_reasoning": "", "property_reasoning": "", "comparison_note": "",
                "enrichment_confidence": {}, "mapping_confidence": {}, "pii_confidence": {},
                "intersection_vector_store": intersection_store, "object_vector_store": object_store,
                "property_vector_store": property_store, "all_objects": all_objects, "all_properties": all_properties,
                "objects_dict": objects_dict, "acronym_dict": acronym_dict, "mapping_strategy": "unknown"
            }
            
            final_state = app.invoke(initial_state)
            
            def fmt_reasons(reasons):
                if not reasons:
                    return ""
                return " | ".join([f"{i+1}. {r.get('reason', 'N/A')} (w:{r.get('weight', 0.0):.2f})" for i, r in enumerate(reasons)])
            
            pii = final_state['pii_classification']
            ec = final_state.get('enrichment_confidence', {})
            mc = final_state.get('mapping_confidence', {})
            pc = final_state.get('pii_confidence', {})
            
            result = {
                "EIM ID": eim_id,
                "Application Name": record.get("Application Name", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": final_state['enriched_name'],
                "Enriched Description": final_state['enriched_description'],
                "Mapped Object": final_state['object_name'],
                "Mapped Property": final_state['property_name'],
                "Mapping Strategy": final_state.get('mapping_strategy', 'unknown'),
                "Mapping Rationale": final_state.get('mapping_rationale', ''),
                "Object Reasoning": final_state.get('object_reasoning', ''),
                "Property Reasoning": final_state.get('property_reasoning', ''),
                "Comparison Note": final_state.get('comparison_note', ''),
                "PII Category": pii.get('pii_category', ''),
                "Is PII": pii.get('is_pii', False),
                "Regulatory Considerations": ", ".join(pii.get('regulatory_considerations', [])),
                "PII Rationale": pii.get('detailed_rationale', ''),
                "Enrichment Rationale": final_state.get('enrichment_rationale', ''),
                "Enrichment Confidence Score": ec.get('confidence_score', 0),
                "Enrichment Supporting Reasons": fmt_reasons(ec.get('supporting_reasons', [])),
                "Enrichment Contradictory Reasons": fmt_reasons(ec.get('contradictory_reasons', [])),
                "Enrichment Confidence Rationale": ec.get('confidence_rationale', ''),
                "Mapping Confidence Score": mc.get('confidence_score', 0),
                "Mapping Supporting Reasons": fmt_reasons(mc.get('supporting_reasons', [])),
                "Mapping Contradictory Reasons": fmt_reasons(mc.get('contradictory_reasons', [])),
                "Mapping Confidence Rationale": mc.get('confidence_rationale', ''),
                "PII Confidence Score": pc.get('confidence_score', 0),
                "PII Supporting Reasons": fmt_reasons(pc.get('supporting_reasons', [])),
                "PII Contradictory Reasons": fmt_reasons(pc.get('contradictory_reasons', [])),
                "PII Confidence Rationale": pc.get('confidence_rationale', ''),
                "Status": "SUCCESS"
            }
            
            successful += 1
            strategy_stats[result["Mapping Strategy"]] += 1
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            
            result = {
                "EIM ID": eim_id, "Application Name": record.get("Application Name", ""),
                "Original Field Name": record.get("Field Name", ""),
                "Enriched Field Name": record.get("Field Name", ""),
                "Enriched Description": f"Error: {str(e)[:200]}",
                "Mapped Object": "Unknown", "Mapped Property": "Unknown",
                "Mapping Strategy": "error",
                "Mapping Rationale": "Error in processing",
                "Object Reasoning": "Error in processing",
                "Property Reasoning": "Error in processing",
                "Comparison Note": "Error prevented normal processing",
                "PII Category": "NON PERSONAL DATA", "Is PII": False,
                "Regulatory Considerations": "", "PII Rationale": "",
                "Enrichment Rationale": "Error", 
                "Enrichment Confidence Score": 0, "Enrichment Supporting Reasons": "",
                "Enrichment Contradictory Reasons": "", "Enrichment Confidence Rationale": "",
                "Mapping Confidence Score": 0, "Mapping Supporting Reasons": "",
                "Mapping Contradictory Reasons": "", "Mapping Confidence Rationale": "",
                "PII Confidence Score": 0, "PII Supporting Reasons": "",
                "PII Contradictory Reasons": "", "PII Confidence Rationale": "",
                "Status": "FAILED"
            }
            failed += 1
        
        # Write incrementally
        pd.DataFrame([result]).to_csv(OUTPUT_CSV_PATH, mode='a', header=(successful+failed==1), index=False)
    
    print("\n[8] Saving Memory...")
    save_all_memory()
    
    print(f"\n{'='*80}")
    print("COMPLETE!")
    print(f"{'='*80}")
    print(f"Success: {successful}, Failed: {failed}")
    print(f"\nüìä Mapping Strategy Breakdown:")
    for strategy, count in sorted(strategy_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / successful * 100) if successful > 0 else 0
        print(f"   ‚Ä¢ {strategy}: {count} ({percentage:.1f}%)")
    print(f"\nüíæ ChromaDB Collections:")
    try:
        for collection in CHROMA_CLIENT.list_collections():
            print(f"   ‚Ä¢ {collection.name}: {collection.count()} embeddings")
    except:
        pass
    print(f"\nOutput: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    process_data_enrichment_and_mapping()
