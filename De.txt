# GCP Architecture Team Platform: Streamlined Design Document

## Executive Summary

This document outlines a streamlined GCP project architecture designed for the architecture team, supporting semantic utility, data management, data provisioning, and observability use cases. The platform leverages FalkorDB for graph operations, user-selectable compute resources, open-source observability tools, and Apache Jena Fuseki for SPARQL queries.

---

## 1. Core Architecture Principles

### Design Philosophy
- **User Choice**: Self-service compute resource selection (CPU, GPU, memory configurations)
- **Open Source First**: Leverage proven open-source tools integrated with GCP native services
- **Simplified Stack**: Focus on essential components, avoid redundant services
- **Policy-Driven**: Centralized policy management with Open Policy Agent (OPA)
- **Enterprise Integration**: Secure API connectivity to enterprise AI platforms

---

## 2. Core Infrastructure Components

### 2.1 Network Architecture

**Component**: Virtual Private Cloud (VPC)

**Configuration**:
- Custom VPC with subnet segmentation
- Private Google Access enabled
- Cloud NAT for outbound connectivity
- VPC Service Controls for data perimeter security

**Subnet Strategy**:
```
Dev Environment:     10.0.0.0/16
Staging Environment: 10.1.0.0/16
Prod Environment:    10.2.0.0/16
GKE Pods:            10.100.0.0/14
GKE Services:        10.104.0.0/20
```

**Firewall Rules**:
- Default deny all traffic
- Allow SSH via Identity-Aware Proxy (IAP)
- Allow internal communication between subnets
- Allow HTTPS from Cloud Load Balancer
- Egress rules for external API calls

---

## 3. Compute Layer - User-Selectable Resources

### 3.1 Google Kubernetes Engine (GKE)

**Rationale**: 
- Container orchestration for all microservices
- Dynamic resource allocation based on user selection
- Auto-scaling for cost optimization
- Native integration with GCP services

**Cluster Configuration**:
- **Mode**: Autopilot (recommended) or Standard with custom node pools
- **Region**: us-central1 (multi-zone for HA)
- **Release Channel**: Regular (balanced stability and features)

**Node Pool Strategy - User Selectable**:

```yaml
# CPU-Optimized Pool (General Workloads)
cpu_pool:
  machine_types: 
    - e2-standard-2  # 2 vCPU, 8 GB RAM - Light
    - n2-standard-4  # 4 vCPU, 16 GB RAM - Medium
    - n2-standard-8  # 8 vCPU, 32 GB RAM - Heavy
  autoscaling:
    min_nodes: 1
    max_nodes: 20
  
# GPU Pool (ML/AI Workloads)
gpu_pool:
  machine_types:
    - g2-standard-4   # 4 vCPU, 16 GB + NVIDIA L4
    - g2-standard-8   # 8 vCPU, 32 GB + NVIDIA L4
    - g2-standard-16  # 16 vCPU, 64 GB + NVIDIA L4
    - a2-highgpu-1g   # 12 vCPU, 85 GB + A100 40GB
  gpu_types:
    - nvidia-l4      # Inference, general ML
    - nvidia-tesla-t4 # Cost-effective
    - nvidia-tesla-a100 # Training, large models
  autoscaling:
    min_nodes: 0
    max_nodes: 10

# Memory-Optimized Pool (Data Processing)
memory_pool:
  machine_types:
    - n2-highmem-4   # 4 vCPU, 32 GB RAM
    - n2-highmem-8   # 8 vCPU, 64 GB RAM
    - n2-highmem-16  # 16 vCPU, 128 GB RAM
  autoscaling:
    min_nodes: 0
    max_nodes: 8

# Spot/Preemptible Pool (Dev/Test, Fault-Tolerant)
spot_pool:
  machine_types:
    - e2-standard-4  # 80% cost savings
    - n2-standard-8
  preemptible: true
  autoscaling:
    min_nodes: 0
    max_nodes: 10
```

**Resource Selection Interface**:
Users can request compute via:
1. **Web UI**: Self-service portal (React/Vue.js)
2. **API**: RESTful endpoints for programmatic access
3. **CLI**: Custom CLI tool wrapping gcloud
4. **Terraform**: Infrastructure as Code templates

**Example API Request**:
```json
POST /api/v1/compute/request
{
  "workload_type": "graph_processing",
  "compute_profile": {
    "cpu_cores": 8,
    "memory_gb": 32,
    "gpu": {
      "type": "nvidia-l4",
      "count": 1
    },
    "storage": {
      "type": "ssd",
      "size_gb": 500
    }
  },
  "duration": "4h",
  "spot_instance": false,
  "labels": {
    "project": "knowledge-graph",
    "team": "data-science"
  }
}
```

### 3.2 Compute Engine - Dynamic VM Provisioning

**Rationale**:
- Workloads requiring persistent local storage
- Custom OS configurations
- Legacy application support
- Temporary high-performance compute

**Machine Family Options**:

| Family | Use Case | Machine Types | Features |
|--------|----------|---------------|----------|
| **E2** | Cost-optimized | e2-standard-2 to e2-standard-32 | General purpose, budget-friendly |
| **N2** | Balanced | n2-standard-2 to n2-standard-128 | Intel Cascade Lake, predictable performance |
| **N2D** | AMD-powered | n2d-standard-2 to n2d-standard-224 | AMD EPYC, cost-effective |
| **C2** | Compute-optimized | c2-standard-4 to c2-standard-60 | Intel Cascade Lake, high CPU |
| **M2** | Memory-optimized | m2-ultramem-208 to m2-ultramem-416 | SAP HANA, large in-memory databases |
| **G2** | GPU-accelerated | g2-standard-4 to g2-standard-96 | NVIDIA L4, AI/ML inference |
| **A2** | GPU training | a2-highgpu-1g to a2-ultragpu-8g | NVIDIA A100, model training |

**Auto-Provisioning Service**:
- **Request Queue**: Redis-based job queue
- **Provisioner**: Cloud Run service that provisions VMs
- **Lifecycle Management**: Automatic cleanup after job completion
- **Cost Tracking**: Per-user cost allocation with labels

---

## 4. Database Architecture

### 4.1 FalkorDB (Graph Database)

**Rationale**:
- Open-source graph database (Redis-compatible)
- Cypher query language support
- High-performance graph operations
- Native integration with Redis ecosystem
- Self-hosted on GKE for full control

**Deployment Architecture**:
```yaml
# StatefulSet on GKE
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: falkordb
  namespace: databases
spec:
  serviceName: falkordb
  replicas: 3  # High availability
  template:
    spec:
      containers:
      - name: falkordb
        image: falkordb/falkordb:latest
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
          limits:
            memory: "32Gi"
            cpu: "8"
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "ssd-retain"
      resources:
        requests:
          storage: 500Gi
```

**Features**:
- **Persistence**: RDB snapshots + AOF (Append-Only File)
- **Replication**: Master-replica setup for read scaling
- **Backup**: Automated snapshots to Cloud Storage
- **Monitoring**: Redis exporter for Prometheus

**Graph Schema Design**:
```cypher
// Data Governance Graph
CREATE (:Dataset {name, owner, created_at, classification})
CREATE (:Field {name, data_type, is_pii, sensitivity})
CREATE (:Pipeline {name, schedule, owner, last_run})
CREATE (:User {email, role, team})
CREATE (:Policy {name, rule, enforcement_level})

// Relationships
CREATE (:Dataset)-[:HAS_FIELD]->(:Field)
CREATE (:Pipeline)-[:READS]->(:Dataset)
CREATE (:Pipeline)-[:WRITES]->(:Dataset)
CREATE (:Dataset)-[:DERIVED_FROM]->(:Dataset)
CREATE (:User)-[:OWNS]->(:Dataset)
CREATE (:Policy)-[:APPLIES_TO]->(:Dataset)
```

**Connection String**:
```
redis://falkordb-0.falkordb.databases.svc.cluster.local:6379
```

### 4.2 Cloud SQL (PostgreSQL)

**Rationale**:
- Relational data for transactional workloads
- Metadata storage for data catalog
- User authentication and authorization
- ACID compliance

**Configuration**:
- **Instance Type**: db-n1-standard-4 (dev), db-n1-standard-8 (prod)
- **Version**: PostgreSQL 15
- **HA Setup**: Regional instance with automatic failover
- **Backups**: Automated daily + PITR (7 days)
- **Storage**: 500GB SSD with automatic expansion

**Schema Design**:
```sql
-- Users and Teams
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    role VARCHAR(50),
    team_id UUID REFERENCES teams(team_id),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE teams (
    team_id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    budget_limit DECIMAL(10,2)
);

-- Compute Resources
CREATE TABLE compute_requests (
    request_id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(user_id),
    workload_type VARCHAR(50),
    machine_type VARCHAR(50),
    gpu_type VARCHAR(50),
    status VARCHAR(20), -- pending, running, completed, failed
    cost DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP
);

-- Data Catalog
CREATE TABLE datasets (
    dataset_id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    owner_id UUID REFERENCES users(user_id),
    classification VARCHAR(50), -- public, internal, confidential
    storage_location TEXT,
    row_count BIGINT,
    size_bytes BIGINT,
    last_updated TIMESTAMP
);

-- Policy Compliance
CREATE TABLE policies (
    policy_id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    rule JSONB, -- OPA Rego rules
    enforcement_level VARCHAR(20), -- advisory, enforce
    applies_to VARCHAR(50), -- dataset, pipeline, user
    created_by UUID REFERENCES users(user_id),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Audit Logs
CREATE TABLE audit_logs (
    log_id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(user_id),
    action VARCHAR(100),
    resource_type VARCHAR(50),
    resource_id UUID,
    status VARCHAR(20),
    details JSONB,
    timestamp TIMESTAMP DEFAULT NOW()
);
```

**Extensions**:
```sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";      -- UUID generation
CREATE EXTENSION IF NOT EXISTS "pg_trgm";        -- Full-text search
CREATE EXTENSION IF NOT EXISTS "hstore";         -- Key-value pairs
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements"; -- Query performance
```

### 4.3 Memorystore for Redis

**Rationale**:
- In-memory caching for API responses
- Session storage for user authentication
- Message queue for lightweight tasks
- Rate limiting and throttling

**Configuration**:
- **Tier**: Standard (HA with automatic failover)
- **Capacity**: 10 GB (dev), 50 GB (prod)
- **Version**: Redis 7.x
- **Network**: Private VPC connection
- **Persistence**: RDB snapshots enabled

**Use Cases**:
- API response caching (TTL: 5 minutes)
- User session data (TTL: 24 hours)
- Job queue for compute provisioning
- Real-time leaderboards/counters
- Distributed locks for resource allocation

---

## 5. Semantic Web Layer

### 5.1 Apache Jena Fuseki (SPARQL Endpoint)

**Rationale**:
- Industry-standard RDF triple store
- SPARQL 1.1 query and update support
- RESTful HTTP API
- Inference and reasoning capabilities
- Integration with FalkorDB via data pipeline

**Deployment on GKE**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fuseki
  namespace: semantic-web
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: fuseki
        image: stain/jena-fuseki:latest
        env:
        - name: ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: fuseki-secrets
              key: admin-password
        ports:
        - containerPort: 3030
        volumeMounts:
        - name: fuseki-data
          mountPath: /fuseki
        resources:
          requests:
            memory: "8Gi"
            cpu: "2"
          limits:
            memory: "16Gi"
            cpu: "4"
      volumes:
      - name: fuseki-data
        persistentVolumeClaim:
          claimName: fuseki-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: fuseki
  namespace: semantic-web
spec:
  type: ClusterIP
  ports:
  - port: 3030
    targetPort: 3030
  selector:
    app: fuseki
```

**Data Pipeline: FalkorDB → Fuseki**:
```python
# ETL Service (Python on Cloud Run)
from falkordb import FalkorDB
from rdflib import Graph, Namespace, URIRef, Literal
from SPARQLWrapper import SPARQLWrapper, POST

# Connect to FalkorDB
graph_db = FalkorDB(host='falkordb', port=6379)

# Query graph data
query = """
MATCH (d:Dataset)-[:HAS_FIELD]->(f:Field)
RETURN d.name, d.owner, f.name, f.data_type
"""
result = graph_db.query(query)

# Convert to RDF triples
rdf_graph = Graph()
SCHEMA = Namespace("http://arch-team.internal/schema#")
DATA = Namespace("http://arch-team.internal/data#")

for record in result:
    dataset_uri = URIRef(DATA[f"dataset/{record['d.name']}"])
    field_uri = URIRef(DATA[f"field/{record['f.name']}"])
    
    rdf_graph.add((dataset_uri, SCHEMA.hasField, field_uri))
    rdf_graph.add((dataset_uri, SCHEMA.owner, Literal(record['d.owner'])))
    rdf_graph.add((field_uri, SCHEMA.dataType, Literal(record['f.data_type'])))

# Upload to Fuseki
sparql = SPARQLWrapper("http://fuseki:3030/arch-team/data")
sparql.setMethod(POST)
sparql.setQuery(f"""
    INSERT DATA {{
        {rdf_graph.serialize(format='turtle')}
    }}
""")
sparql.query()
```

**SPARQL Query Examples**:
```sparql
# Find all datasets owned by a user
SELECT ?dataset ?field
WHERE {
  ?dataset schema:owner "john.doe@company.com" .
  ?dataset schema:hasField ?field .
  ?field schema:dataType ?type .
  FILTER (?type = "PII")
}

# Data lineage query
SELECT ?source ?transformation ?target
WHERE {
  ?source schema:transformedBy ?transformation .
  ?transformation schema:produces ?target .
}
```

**API Endpoints**:
- **Query**: `POST /arch-team/sparql` (SPARQL SELECT)
- **Update**: `POST /arch-team/update` (SPARQL INSERT/DELETE)
- **Graph Store**: `GET/PUT/POST/DELETE /arch-team/data?graph=<uri>`
- **Admin UI**: `http://fuseki:3030/#/manage`

---

## 6. Data Processing & Orchestration

### 6.1 Cloud Composer (Managed Apache Airflow)

**Rationale**:
- Workflow orchestration for data pipelines
- Python-based DAG definitions
- Integration with all GCP services
- Monitoring and alerting built-in
- No need to manage Airflow infrastructure

**Configuration**:
- **Environment Size**: Medium (3 workers)
- **Airflow Version**: 2.7+
- **Python Version**: 3.11
- **Location**: us-central1

**DAG Examples**:

```python
# dag_falkordb_to_fuseki.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.cloud_run import CloudRunExecuteJobOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'falkordb_to_fuseki_sync',
    default_args=default_args,
    schedule_interval='0 */4 * * *',  # Every 4 hours
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['semantic-web', 'etl']
) as dag:
    
    extract_from_falkordb = PythonOperator(
        task_id='extract_graph_data',
        python_callable=extract_graph_data
    )
    
    transform_to_rdf = PythonOperator(
        task_id='transform_to_rdf',
        python_callable=transform_to_rdf_triples
    )
    
    load_to_fuseki = CloudRunExecuteJobOperator(
        task_id='load_to_fuseki',
        job_name='fuseki-loader',
        region='us-central1'
    )
    
    validate_data = PythonOperator(
        task_id='validate_sparql_queries',
        python_callable=validate_data_quality
    )
    
    extract_from_falkordb >> transform_to_rdf >> load_to_fuseki >> validate_data


# dag_compute_provisioning.py
with DAG(
    'compute_cleanup',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    catchup=False
) as dag:
    
    cleanup_idle_vms = PythonOperator(
        task_id='cleanup_idle_vms',
        python_callable=cleanup_idle_compute_resources
    )
    
    generate_cost_report = PythonOperator(
        task_id='generate_cost_report',
        python_callable=generate_daily_cost_report
    )
    
    cleanup_idle_vms >> generate_cost_report
```

**Custom Operators**:
- `FalkorDBOperator`: Execute Cypher queries
- `FusekiOperator`: Execute SPARQL queries
- `ComputeProvisionOperator`: Provision GCE/GKE resources

---

## 7. Observability Stack (Open Source + GCP Native)

### 7.1 Metrics - Prometheus + Grafana

**Prometheus Deployment**:
```yaml
# Using kube-prometheus-stack Helm chart
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=500Gi \
  --set grafana.adminPassword=<secure-password>
```

**Architecture**:
- **Prometheus**: Metrics collection and storage
- **Grafana**: Visualization and dashboards
- **Alertmanager**: Alert routing and deduplication
- **Node Exporter**: Host-level metrics
- **kube-state-metrics**: Kubernetes cluster metrics

**Custom Exporters**:
- **FalkorDB Exporter**: Redis exporter adapted for FalkorDB
- **Fuseki Exporter**: JMX exporter for Jena Fuseki
- **Cloud SQL Exporter**: Cloud SQL Proxy with Prometheus endpoint

**Key Metrics**:
```yaml
# Infrastructure
- node_cpu_seconds_total
- node_memory_MemAvailable_bytes
- node_disk_io_time_seconds_total
- node_network_transmit_bytes_total

# Kubernetes
- kube_pod_status_phase
- kube_deployment_status_replicas_available
- container_cpu_usage_seconds_total
- container_memory_working_set_bytes

# Databases
- falkordb_commands_processed_total
- falkordb_connected_clients
- pg_up (PostgreSQL)
- pg_stat_database_tup_returned

# Application
- http_requests_total
- http_request_duration_seconds
- http_requests_in_flight
```

**Grafana Dashboards**:
1. **Cluster Overview**: Node health, pod status, resource utilization
2. **Database Dashboard**: FalkorDB, PostgreSQL, Redis metrics
3. **Application Dashboard**: API latency, error rates, throughput
4. **Cost Dashboard**: Per-team resource usage and costs
5. **Semantic Web**: Fuseki query performance, triple count

**Alerting Rules**:
```yaml
groups:
- name: infrastructure
  rules:
  - alert: HighCPUUsage
    expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 10m
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85
    for: 10m
    
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m

- name: databases
  rules:
  - alert: FalkorDBDown
    expr: up{job="falkordb"} == 0
    for: 2m
    
  - alert: HighQueryLatency
    expr: histogram_quantile(0.95, rate(falkordb_command_duration_seconds_bucket[5m])) > 1
    for: 10m

- name: application
  rules:
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    
  - alert: APILatencyHigh
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
    for: 10m
```

### 7.2 Logging - Loki + Grafana

**Loki Deployment**:
```yaml
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack \
  --namespace monitoring \
  --set loki.persistence.enabled=true \
  --set loki.persistence.size=500Gi \
  --set promtail.enabled=true \
  --set grafana.enabled=false  # Use existing Grafana
```

**Architecture**:
- **Loki**: Log aggregation and storage
- **Promtail**: Log shipper (DaemonSet on every node)
- **Grafana**: Log exploration and visualization

**Log Collection**:
```yaml
# Promtail configuration
clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Kubernetes pod logs
  - job_name: kubernetes-pods
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
    pipeline_stages:
      - json:
          expressions:
            level: level
            timestamp: timestamp
      - labels:
          level:
          
  # FalkorDB logs
  - job_name: falkordb
    static_configs:
      - targets:
          - falkordb-0:6379
        labels:
          job: falkordb
          
  # Application logs
  - job_name: applications
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - default
            - applications
```

**LogQL Queries**:
```logql
# Error logs in last hour
{namespace="production"} |= "error" | json | level="ERROR"

# FalkorDB slow queries
{job="falkordb"} |~ "slow query" | json | duration > 1000

# API errors by endpoint
sum by (endpoint) (rate({namespace="production",app="api"} 
  |~ "status=5" [5m]))

# Failed compute provisioning
{job="compute-provisioner"} |= "failed" | json 
  | user_id!="" | line_format "{{.user_id}}: {{.error}}"
```

### 7.3 Tracing - Tempo (Grafana Tempo)

**Tempo Deployment**:
```yaml
helm install tempo grafana/tempo \
  --namespace monitoring \
  --set tempo.storage.trace.backend=gcs \
  --set tempo.storage.trace.gcs.bucket_name=arch-team-traces
```

**Instrumentation with OpenTelemetry**:
```python
# Python FastAPI application
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Initialize tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Configure OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint="http://tempo:4317",
    insecure=True
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# Instrument FastAPI
app = FastAPI()
FastAPIInstrumentor.instrument_app(app)

# Custom spans
@app.get("/query")
async def query_graph():
    with tracer.start_as_current_span("falkordb_query"):
        result = falkordb.query("MATCH (n) RETURN n LIMIT 10")
    
    with tracer.start_as_current_span("fuseki_query"):
        sparql_result = fuseki.query("SELECT * WHERE {?s ?p ?o} LIMIT 10")
    
    return {"results": result}
```

**Trace Visualization in Grafana**:
- End-to-end request traces
- Service dependency graphs
- Latency analysis by span
- Error tracking across services

### 7.4 GCP Native Observability (Complementary)

**Cloud Monitoring Integration**:
- **GKE Metrics**: Automatic collection of cluster metrics
- **Cloud SQL Insights**: Query performance and database health
- **Ops Agent**: System-level metrics from Compute Engine
- **Uptime Checks**: External endpoint monitoring

**Cloud Logging Integration**:
- **Audit Logs**: Admin activity, data access, system events
- **Security Logs**: VPC Flow Logs, Firewall Logs
- **Log Router**: Send logs to Loki for unified view

**Unified Dashboard in Grafana**:
```json
{
  "dashboard": {
    "title": "Unified Observability",
    "panels": [
      {
        "title": "GKE Cluster Health",
        "datasource": "Google Cloud Monitoring",
        "targets": [{
          "metricType": "container.googleapis.com/container/cpu/usage_time"
        }]
      },
      {
        "title": "Application Metrics",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(http_requests_total[5m])"
        }]
      },
      {
        "title": "Application Logs",
        "datasource": "Loki",
        "targets": [{
          "expr": "{namespace=\"production\"}"
        }]
      },
      {
        "title": "Distributed Traces",
        "datasource": "Tempo",
        "targets": [{
          "query": "service.name=\"api\""
        }]
      }
    ]
  }
}
```

---

## 8. Policy Management with Open Policy Agent (OPA)

### 8.1 OPA Deployment

**Rationale**:
- Centralized policy enforcement
- Policy-as-code with Rego language
- Integration with Kubernetes admission control
- Runtime policy evaluation for APIs

**Deployment on GKE**:
```yaml
helm repo add opa https://open-policy-agent.github.io/helm-charts
helm install opa opa/opa \
  --namespace policy \
  --set mgmt.enabled=true \
  --set admissionControllerKind=MutatingWebhookConfiguration
```

### 8.2 Policy Examples

**Compute Resource Policies**:
```rego
# policies/compute_limits.rego
package compute

import future.keywords.if

# Deny GPU requests exceeding team quota
deny_gpu_request[msg] if {
    input.request.gpu.count > 0
    team_quota := data.teams[input.user.team_id].gpu_quota
    current_usage := data.metrics.gpu_usage[input.user.team_id]
    current_usage + input.request.gpu.count > team_quota
    msg := sprintf("Team %s exceeds GPU quota: %d/%d", 
                   [input.user.team_id, current_usage + input.request.gpu.count, team_quota])
}

# Enforce spot instances for non-production
require_spot_instance if {
    input.workload.environment != "production"
    input.workload.priority == "low"
    not input.request.spot_instance
}

# Cost control: Max VM size per user role
max_machine_type[machine_type] if {
    input.user.role == "developer"
    machine_type := "n2-standard-8"  # Max 8 vCPU
}

max_machine_type[machine_type] if {
    input.user.role == "data_scientist"
    machine_type := "n2-standard-16"  # Max 16 vCPU
}
```

**Data Access Policies**:
```rego
# policies/data_access.rego
package data_access

# PII data requires specific role
deny_pii_access[msg] if {
    dataset := data.datasets[input.dataset_id]
    dataset.classification == "PII"
    not input.user.role in ["data_steward", "privacy_officer"]
    msg := "PII data access requires data_steward or privacy_officer role"
}

# Confidential data requires approval
deny_confidential_access[msg] if {
    dataset := data.datasets[input.dataset_id]
    dataset.classification == "confidential"
    not has_approval(input.user.id, input.dataset_id)
    msg := "Access to confidential data requires approval"
}

has_approval(user_id, dataset_id) if {
    approval := data.approvals[dataset_id][user_id]
    approval.status == "approved"
    time.now_ns() < approval.expires_at
}
```

**Kubernetes Admission Control**:
```rego
# policies/k8s_admission.rego
package kubernetes.admission

# Deny pods without resource limits
deny[msg] if {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not container.resources.limits
    msg := sprintf("Container %s must have resource limits", [container.name])
}

# Require security context
deny[msg] if {
    input.request.kind.kind == "Pod"
    container := input.request.object.spec.containers[_]
    not container.securityContext.runAsNonRoot
    msg := sprintf("Container %s must run as non-root", [container.name])
}

# Enforce namespace quotas
deny[msg] if {
    input.request.kind.kind == "Pod"
    namespace := input.request.namespace
    quota := data.quotas[namespace]
    current_pods := count([p | p := data.pods[namespace][_]])
    current_pods >= quota.max_pods
    msg := sprintf("Namespace %s has reached pod limit", [namespace])
}
```

### 8.3 Policy Evaluation API

```python
# policy_evaluator.py (Cloud Run service)
from fastapi import FastAPI, HTTPException
from opa import OPA

app = FastAPI()
opa_client = OPA(url="http://opa.policy:8181")

@app.post("/evaluate/compute")
async def evaluate_compute_request(request: ComputeRequest):
    """Evaluate compute resource request against policies"""
    decision = opa_client.check_policy(
        policy_path="compute/deny_gpu_request",
        input_data={
            "user": request.user,
            "request": request.compute_profile,
            "workload": request.workload_info
        },
        data={
            "teams": get_team_quotas(),
            "metrics": get_current_usage()
        }
    )
    
    if decision.get("deny"):
        raise HTTPException(status_code=403, detail=decision["deny"])
    
    return {"allowed": True, "decision": decision}

@app.post("/evaluate/data-access")
async def evaluate_data_access(request: DataAccessRequest):
    """Evaluate data access request against policies"""
    decision = opa_client.check_policy(
        policy_path="data_access",
        input_data={
            "user": request.user,
            "dataset_id": request.dataset_id,
            "operation": request.operation
        },
        data={
            "datasets": get_datasets(),
            "approvals": get_approvals()
        }
    )
    
    if decision.get("deny"):
        raise HTTPException(status_code=403, detail=decision["deny"])
    
    return {"allowed": True, "decision": decision}
```

---

## 9. Enterprise AI Integration

### 9.1 Service Account Authentication

**Architecture**:
```
Enterprise AI Platform → GCP Service Account → API Gateway → Internal Services
```

**Service Account Setup**:
```bash
# Create service account for enterprise AI platform
gcloud iam service-accounts create enterprise-ai-sa \
  --display-name="Enterprise AI Platform Service Account"

# Grant necessary roles
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:enterprise-ai-sa@$PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/aiplatform.user"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:enterprise-ai-sa@$PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/run.invoker"

# Create JSON key (store securely in enterprise platform)
gcloud iam service-accounts keys create enterprise-ai-key.json \
  --iam-account=enterprise-ai-sa@$PROJECT_ID.iam.gserviceaccount.com
```

### 9.2 API Gateway Configuration

```yaml
# api-gateway-config.yaml
swagger: '2.0'
info:
  title: Architecture Team API Gateway
  version: 1.0.0
host: api.arch-team.internal
schemes:
  - https
security:
  - google_service_account: []
securityDefinitions:
  google_service_account:
    authorizationUrl: ""
    flow: "implicit"
    type: "oauth2"
    x-google-issuer: "enterprise-ai-sa@$PROJECT_ID.iam.gserviceaccount.com"
    x-google-jwks_uri: "https://www.googleapis.com/robot/v1/metadata/x509/enterprise-ai-sa@$PROJECT_ID.iam.gserviceaccount.com"
    x-google-audiences: "https://api.arch-team.internal"

paths:
  /v1/graph/query:
    post:
      summary: Query knowledge graph
      operationId: queryGraph
      x-google-backend:
        address: https://graph-api-service.default.svc.cluster.local
        jwt_audience: https://api.arch-team.internal
      parameters:
        - name: body
          in: body
          schema:
            type: object
            properties:
              query:
                type: string
                description: Cypher query
      responses:
        200:
          description: Query results
          
  /v1/sparql/query:
    post:
      summary: Execute SPARQL query
      operationId: sparqlQuery
      x-google-backend:
        address: https://fuseki.semantic-web.svc.cluster.local:3030
      parameters:
        - name: body
          in: body
          schema:
            type: object
            properties:
              query:
                type: string
                description: SPARQL query
      responses:
        200:
          description: SPARQL results
          
  /v1/compute/provision:
    post:
      summary: Provision compute resources
      operationId: provisionCompute
      x-google-backend:
        address: https://compute-provisioner.default.svc.cluster.local
      parameters:
        - name: body
          in: body
          schema:
            $ref: '#/definitions/ComputeRequest'
      responses:
        202:
          description: Provisioning request accepted
```

### 9.3 Client Integration Example

```python
# Enterprise AI Platform Integration
from google.oauth2 import service_account
from google.auth.transport.requests import AuthorizedSession

# Load service account credentials
credentials = service_account.Credentials.from_service_account_file(
    'enterprise-ai-key.json',
    scopes=['https://www.googleapis.com/auth/cloud-platform']
)

# Create authenticated session
authed_session = AuthorizedSession(credentials)

# Query knowledge graph
response = authed_session.post(
    'https://api.arch-team.internal/v1/graph/query',
    json={
        'query': '''
            MATCH (d:Dataset)-[:HAS_FIELD]->(f:Field)
            WHERE f.is_pii = true
            RETURN d.name, f.name, d.owner
        '''
    }
)

graph_data = response.json()

# Execute SPARQL query for semantic reasoning
response = authed_session.post(
    'https://api.arch-team.internal/v1/sparql/query',
    json={
        'query': '''
            PREFIX schema: <http://arch-team.internal/schema#>
            SELECT ?dataset ?classification ?policy
            WHERE {
                ?dataset schema:classification ?classification .
                ?policy schema:appliesTo ?dataset .
                FILTER (?classification = "confidential")
            }
        '''
    }
)

semantic_data = response.json()

# Provision GPU for model training
response = authed_session.post(
    'https://api.arch-team.internal/v1/compute/provision',
    json={
        'workload_type': 'model_training',
        'compute_profile': {
            'cpu_cores': 16,
            'memory_gb': 64,
            'gpu': {
                'type': 'nvidia-tesla-a100',
                'count': 2
            },
            'storage': {
                'type': 'ssd',
                'size_gb': 1000
            }
        },
        'duration': '8h',
        'labels': {
            'project': 'enterprise-ai',
            'purpose': 'llm-fine-tuning'
        }
    }
)

provisioning_status = response.json()
```

### 9.4 Rate Limiting & Throttling

```python
# Implemented in API Gateway with Redis
from fastapi import FastAPI, Request, HTTPException
from redis import Redis
import time

app = FastAPI()
redis_client = Redis(host='redis', port=6379)

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    # Extract service account from JWT
    sa_email = request.state.service_account
    
    # Rate limit key
    key = f"rate_limit:{sa_email}:{int(time.time() / 60)}"
    
    # Increment counter
    current = redis_client.incr(key)
    redis_client.expire(key, 60)
    
    # Check limit (100 requests per minute per service account)
    if current > 100:
        raise HTTPException(
            status_code=429,
            detail="Rate limit exceeded. Max 100 requests/minute."
        )
    
    response = await call_next(request)
    response.headers["X-Rate-Limit-Limit"] = "100"
    response.headers["X-Rate-Limit-Remaining"] = str(100 - current)
    return response
```

---

## 10. Infrastructure as Code (Terraform)

### 10.1 Repository Structure

```
terraform/
├── modules/
│   ├── networking/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── gke/
│   ├── cloud-sql/
│   ├── compute/
│   ├── redis/
│   └── monitoring/
├── environments/
│   ├── dev/
│   │   ├── main.tf
│   │   ├── terraform.tfvars
│   │   └── backend.tf
│   ├── staging/
│   └── prod/
├── global/
│   ├── iam.tf
│   ├── service-accounts.tf
│   └── secrets.tf
└── README.md
```

### 10.2 User-Selectable Compute Module

```hcl
# modules/compute-selector/main.tf
resource "google_compute_instance" "dynamic_vm" {
  count        = var.instance_count
  name         = "${var.name_prefix}-${count.index}"
  machine_type = var.machine_type
  zone         = var.zone

  boot_disk {
    initialize_params {
      image = var.boot_image
      size  = var.disk_size_gb
      type  = var.disk_type
    }
  }

  network_interface {
    network    = var.network
    subnetwork = var.subnetwork
  }

  # GPU configuration (conditional)
  dynamic "guest_accelerator" {
    for_each = var.gpu_config != null ? [var.gpu_config] : []
    content {
      type  = guest_accelerator.value.type
      count = guest_accelerator.value.count
    }
  }

  scheduling {
    preemptible       = var.preemptible
    on_host_maintenance = var.gpu_config != null ? "TERMINATE" : "MIGRATE"
    automatic_restart = !var.preemptible
  }

  metadata = {
    user-data = templatefile("${path.module}/cloud-init.yaml", {
      workload_type = var.workload_type
      user_id       = var.user_id
    })
  }

  labels = merge(
    {
      managed_by    = "terraform"
      workload_type = var.workload_type
      user_id       = var.user_id
    },
    var.custom_labels
  )
}

# modules/compute-selector/variables.tf
variable "machine_types" {
  description = "Available machine types for selection"
  type = map(object({
    cpu_cores  = number
    memory_gb  = number
    cost_per_hour = number
  }))
  default = {
    "e2-standard-2"  = { cpu_cores = 2,  memory_gb = 8,   cost_per_hour = 0.067 }
    "n2-standard-4"  = { cpu_cores = 4,  memory_gb = 16,  cost_per_hour = 0.194 }
    "n2-standard-8"  = { cpu_cores = 8,  memory_gb = 32,  cost_per_hour = 0.388 }
    "n2-highmem-8"   = { cpu_cores = 8,  memory_gb = 64,  cost_per_hour = 0.475 }
    "g2-standard-4"  = { cpu_cores = 4,  memory_gb = 16,  cost_per_hour = 0.35 }
    "a2-highgpu-1g"  = { cpu_cores = 12, memory_gb = 85,  cost_per_hour = 3.67 }
  }
}

variable "gpu_types" {
  description = "Available GPU types"
  type = map(object({
    memory_gb = number
    cost_per_hour = number
  }))
  default = {
    "nvidia-l4"         = { memory_gb = 24, cost_per_hour = 0.73 }
    "nvidia-tesla-t4"   = { memory_gb = 16, cost_per_hour = 0.35 }
    "nvidia-tesla-a100" = { memory_gb = 40, cost_per_hour = 2.93 }
  }
}
```

### 10.3 FalkorDB Module

```hcl
# modules/falkordb/main.tf
resource "kubernetes_namespace" "databases" {
  metadata {
    name = "databases"
  }
}

resource "kubernetes_stateful_set" "falkordb" {
  metadata {
    name      = "falkordb"
    namespace = kubernetes_namespace.databases.metadata[0].name
  }

  spec {
    service_name = "falkordb"
    replicas     = var.replicas

    selector {
      match_labels = {
        app = "falkordb"
      }
    }

    template {
      metadata {
        labels = {
          app = "falkordb"
        }
      }

      spec {
        container {
          name  = "falkordb"
          image = "falkordb/falkordb:${var.falkordb_version}"

          port {
            container_port = 6379
            name          = "redis"
          }

          resources {
            requests = {
              cpu    = var.cpu_request
              memory = var.memory_request
            }
            limits = {
              cpu    = var.cpu_limit
              memory = var.memory_limit
            }
          }

          volume_mount {
            name       = "data"
            mount_path = "/data"
          }

          env {
            name  = "FALKORDB_ARGS"
            value = "--loadmodule /usr/lib/redis/modules/graphblas.so"
          }
        }
      }
    }

    volume_claim_template {
      metadata {
        name = "data"
      }

      spec {
        access_modes       = ["ReadWriteOnce"]
        storage_class_name = var.storage_class
        resources {
          requests = {
            storage = var.storage_size
          }
        }
      }
    }
  }
}

resource "kubernetes_service" "falkordb" {
  metadata {
    name      = "falkordb"
    namespace = kubernetes_namespace.databases.metadata[0].name
  }

  spec {
    selector = {
      app = "falkordb"
    }

    port {
      port        = 6379
      target_port = 6379
      protocol    = "TCP"
    }

    type             = "ClusterIP"
    cluster_ip       = "None"  # Headless service for StatefulSet
  }
}
```

---

## 11. Cost Estimation (Streamlined Architecture)

### Monthly Cost Summary

| Component | Configuration | Monthly Cost (USD) | Notes |
|-----------|---------------|-------------------|-------|
| **GKE Autopilot** | Control plane | $73 | Managed K8s |
| **GKE Nodes - CPU** | 3x n2-standard-4 | $420 | Auto-scales 1-10 |
| **GKE Nodes - GPU** | 1x g2-standard-4 + L4 | $600 | Auto-scales 0-5 |
| **Compute Engine** | 2x e2-standard-4 | $240 | For specific workloads |
| **Cloud SQL (PostgreSQL)** | db-n1-standard-4, HA | $530 | Metadata, catalog |
| **Memorystore (Redis)** | 10 GB Standard | $80 | Caching |
| **Persistent Disks** | 2 TB SSD | $340 | For databases, PVCs |
| **Cloud Storage** | 2 TB Standard | $46 | Backups, logs |
| **Cloud Composer** | Medium (3 workers) | $310 | Airflow |
| **Cloud NAT** | 3 gateways | $135 | Egress |
| **Monitoring (Prometheus/Grafana)** | 500 GB storage | $150 | Open-source on GKE |
| **Cloud Monitoring** | Basic metrics | $50 | GCP native |
| **Cloud Logging** | 100 GB | $50 | GCP native |
| **Committed Use Discount** | 3-year | -$300 | 57% savings |
| **Auto-scaling Savings** | Off-hours scale down | -$200 | Nights/weekends |
| **Total** | | **$2,524/month** | **$30,288/year** |

### Cost by Use Case
- **Semantic Utility** (FalkorDB, Fuseki): $400/month (16%)
- **Data Management** (SQL, Storage): $916/month (36%)
- **Data Provisioning** (Composer, Compute): $970/month (38%)
- **Observability** (Prometheus, Grafana, Loki, Tempo): $200/month (8%)

---

## 12. Implementation Checklist (20 Weeks)

### Phase 1: Foundation (Weeks 1-4)
- [ ] Create GCP project structure
- [ ] Set up VPC networking with subnets
- [ ] Configure IAM and service accounts
- [ ] Initialize Terraform backend (GCS)
- [ ] Set up GitHub repository with Actions
- [ ] Configure Workload Identity Federation

### Phase 2: Compute & Orchestration (Weeks 5-8)
- [ ] Deploy GKE Autopilot cluster
- [ ] Create CPU, GPU, memory node pools
- [ ] Deploy Cloud Composer (Airflow)
- [ ] Set up dynamic VM provisioning API
- [ ] Implement compute resource selector UI
- [ ] Deploy ArgoCD for GitOps

### Phase 3: Database Layer (Weeks 9-12)
- [ ] Deploy FalkorDB on GKE
- [ ] Set up Cloud SQL (PostgreSQL)
- [ ] Configure Memorystore (Redis)
- [ ] Deploy Apache Jena Fuseki
- [ ] Build FalkorDB → Fuseki sync pipeline
- [ ] Create graph schema and indexes

### Phase 4: Observability (Weeks 13-16)
- [ ] Deploy Prometheus stack (kube-prometheus-stack)
- [ ] Configure Grafana dashboards
- [ ] Deploy Loki for log aggregation
- [ ] Set up Tempo for distributed tracing
- [ ] Configure Alertmanager with routing
- [ ] Integrate GCP native monitoring
- [ ] Create custom exporters (FalkorDB, Fuseki)

### Phase 5: Policy & Security (Weeks 17-18)
- [ ] Deploy Open Policy Agent (OPA)
- [ ] Write compute resource policies
- [ ] Implement data access policies
- [ ] Configure Kubernetes admission control
- [ ] Build policy evaluation API
- [ ] Set up audit logging

### Phase 6: Enterprise Integration & Testing (Weeks 19-20)
- [ ] Create service account for enterprise AI
- [ ] Deploy API Gateway with authentication
- [ ] Build client SDK for enterprise platform
- [ ] Implement rate limiting
- [ ] Conduct load testing
- [ ] Performance optimization
- [ ] Documentation and training

---

## 13. Key Advantages of This Architecture

### 1. **Cost-Effective**
- ~70% cheaper than original architecture ($2,524 vs $8,028/month)
- No expensive managed services (BigQuery, Bigtable, Neo4j Aura)
- Open-source tools eliminate licensing costs

### 2. **User Empowerment**
- Self-service compute selection
- Real-time resource provisioning
- Transparent cost visibility per team/user

### 3. **Open Source & Portable**
- FalkorDB: Redis-compatible, easy to migrate
- Prometheus/Grafana: Industry-standard observability
- Apache Fuseki: Standard SPARQL endpoint
- Minimal vendor lock-in

### 4. **Enterprise-Ready**
- Policy-driven governance with OPA
- Secure service account authentication
- Comprehensive audit logging
- Multi-layer security (network, identity, data)

### 5. **Operational Excellence**
- GitOps with ArgoCD for declarative deployments
- Infrastructure as Code with Terraform
- Automated scaling and cost optimization
- Unified observability (metrics, logs, traces)

---

## 14. Success Metrics

- **Uptime**: 99.9% (< 43 min downtime/month)
- **Compute Provisioning**: < 5 minutes from request to ready
- **Query Performance**: Graph queries < 1 second (p95)
- **SPARQL Queries**: < 2 seconds (p95)
- **Cost Efficiency**: < $3,000/month
- **User Satisfaction**: > 90% positive feedback
