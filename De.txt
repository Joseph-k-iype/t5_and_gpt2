import os
import logging
import requests
import httpx
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential, ClientSecretCredential
from openai import OpenAI

# --- Load configuration files from the 'config' folder ---
load_dotenv("config/dev")        # loads variables from config/dev
load_dotenv("config/dev.creds")  # loads variables from config/dev.creds

# --- Set up Proxy URL and CA bundle ---
ad_username = os.getenv("AD_USERNAME")
ad_user_id = os.getenv("AD_USER_ID")
http_proxy_config = os.getenv("HTTP_PROXY")  # e.g. "@abc.uk.systems:80"
proxy_url = f"http://{ad_username}:{ad_user_id}{http_proxy_config}"

os.environ['HTTP_PROXY'] = proxy_url
os.environ['HTTPS_PROXY'] = proxy_url
os.environ["REQUESTS_CA_BUNDLE"] = os.getenv("CONF_PEM_PATH", "cacert.pem")

custom_client = httpx.Client(verify=os.getenv("CONF_PEM_PATH", "cacert.pem"), proxy=proxy_url)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Configure NO_PROXY if not already set
if not os.getenv("NO_PROXY"):
    NO_PROXY_DOMAINS = [
        '.cognitiveservices.azure.com',
        '.search.windows.net',
        '.openai.azure.com',
        '.core.windows.net',
        '.azurewebsites.net'
    ]
    os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

session = requests.Session()
session.verify = os.getenv("CONF_PEM_PATH", "cacert.pem")
session.proxies = {'http': None, 'https': None}

# --- Initialize Azure Credential and token for Embeddings ---
try:
    credential = DefaultAzureCredential()
    embeddings_token = credential.get_token('https://cognitiveservices.azure.com/.default')

    def get_embeddings(texts, endpoint, deployment_name="text-embedding-3-large", batch_size=100):
        headers = {
            'Authorization': f'Bearer {embeddings_token.token}',
            'Content-Type': 'application/json'
        }
        openai_api_version = os.getenv("OPENAI_API_VERSION", "2023-05-15")
        api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version={openai_api_version}"
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            try:
                payload = {"input": batch}
                response = session.post(api_url, headers=headers, json=payload)
                if response.status_code == 200:
                    response_data = response.json()
                    batch_embeddings = [item['embedding'] for item in response_data['data']]
                    embeddings.extend(batch_embeddings)
                    logger.info(f"Received embeddings for batch {i+1}-{min(i+batch_size, len(texts))}")
                else:
                    logger.error(f"Failed to receive embeddings for batch {i+1}-{min(i+batch_size, len(texts))}, status code: {response.status_code}")
                    embeddings.extend([None] * len(batch))
            except Exception as e:
                logger.error(f"Error processing batch {i+1}-{min(i+batch_size, len(texts))}: {str(e)}")
                embeddings.extend([None] * len(batch))
        return embeddings

    def test_connection(endpoint):
        try:
            test_texts = ['Hello World']
            embedings = get_embeddings(test_texts, endpoint)
            if embedings and embedings[0]:
                print(f"Embedding Dimension: {len(embedings[0])}")
                return True
            else:
                print("Failed to retrieve embeddings")
                return False
        except Exception as e:
            print(f"Failed to connect to {endpoint}: {str(e)}")
            return False

except Exception as e:
    print(f"Failed to initialize Azure SDK for embeddings: {str(e)}")
    raise

# --- Configure Azure OpenAI for LLM using Azure AD Authentication ---
try:
    # Generate an access token using your client credentials from dev.creds
    llm_credential = ClientSecretCredential(
        tenant_id=os.environ["AZURE_TENANT_ID"],
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"]
    )
    llm_token = llm_credential.get_token('https://cognitiveservices.azure.com/.default')
    
    # Instantiate the OpenAI client with Azure OpenAI settings.
    # Here the token from Azure AD acts as the API key.
    llm_client = OpenAI(
        api_key=llm_token.token,
        api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),  # e.g. "https://your-resource-name.openai.azure.com"
        api_version=os.getenv("OPENAI_API_VERSION", "2023-05-15")
    )
    
    def run_llm_agent(prompt: str) -> str:
        """
        Uses Azure OpenAI's Chat Completion endpoint (via the new client interface)
        with the deployment 'gpt-4o-mini' to generate a response for the given prompt.
        """
        try:
            response = llm_client.chat.completions.create(
                model="gpt-4o-mini",  # Deployment name for the GPT-4o-mini model
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
            )
            # Use the new attribute access to extract the content
            answer = response.choices[0].message.content
            logger.info(f"LLM agent response: {answer}")
            return answer
        except Exception as e:
            logger.error(f"Error in LLM agent: {str(e)}")
            return "Error generating response from the LLM agent."
            
except Exception as e:
    print(f"Failed to configure Azure OpenAI for LLM: {str(e)}")
    raise

# --- Main Execution ---
if __name__ == "__main__":
    # Validate the connection using the embeddings endpoint
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-azure-endpoint.cognitiveservices.azure.com")
    if azure_endpoint.startswith("https:") and not azure_endpoint.startswith("https://"):
        azure_endpoint = azure_endpoint.replace("https:", "https://", 1)
    
    if test_connection(azure_endpoint):
        print("Connection successful")
    else:
        print("Connection failed")
    
    # Test the LLM agent function
    user_prompt = "What is the capital of France?"
    agent_response = run_llm_agent(user_prompt)
    print("LLM Agent Response:")
    print(agent_response)
