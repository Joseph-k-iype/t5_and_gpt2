"""
Legal Summary Pipeline - COMPLETE VERSION
Uses FalkorDB, embeddings, and enhanced analyzer
"""

import json
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
import argparse

from src.processors.pdf_processor import PDFProcessor
from src.analyzers.legal_document_analyzer_enhanced import EnhancedLegalDocumentAnalyzer
from src.generators.legal_summary_generator_enhanced import LegalSummaryGenerator
from src.config import Config


class LegalSummaryPipeline:
    """Complete pipeline with FalkorDB and embeddings"""
    
    def __init__(self, metadata_file: str = "config/legislation_metadata.json",
                 output_dir: str = "output/legal_summaries",
                 config: Optional[Config] = None):
        self.metadata_file = metadata_file
        self.output_dir = output_dir
        self.config = config or Config()
        
        self.pdf_processor = PDFProcessor()
        self.analyzer = EnhancedLegalDocumentAnalyzer(config=self.config)
        self.generator = LegalSummaryGenerator(output_dir=output_dir)
        
        self.metadata = self._load_metadata()
        
        print(f"âœ“ Initialized Legal Summary Pipeline")
        print(f"âœ“ FalkorDB: {self.config.FALKORDB_HOST}:{self.config.FALKORDB_PORT}")
        print(f"âœ“ Embeddings: {self.config.EMBEDDING_MODEL}")
        print(f"âœ“ LangGraph workflows enabled")
        print(f"âœ“ Loaded {len(self.metadata)} rules")
    
    def _load_metadata(self) -> Dict[str, Any]:
        """Load legislation metadata"""
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"Warning: Metadata file not found: {self.metadata_file}")
            return {}
        except json.JSONDecodeError as e:
            print(f"Error parsing metadata: {e}")
            return {}
    
    def _detect_enterprise_context(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Detect enterprise context with specific tool recognition
        DataVisa is an internal data governance tool
        """
        context = {}
        
        # Detect organization
        organizations = {
            "HSBC": ["HSBC", "hsbc"],
            "Acme Corp": ["Acme", "acme"],
            "Global Bank": ["Global Bank", "GlobalBank"]
        }
        
        for org_name, patterns in organizations.items():
            if any(pattern in text for pattern in patterns):
                context["organization"] = org_name
                break
        
        # Detect internal tools - specifically recognize DataVisa
        tools = []
        for tool in self.config.ENTERPRISE_TOOLS:
            if tool in text or tool.lower() in text.lower():
                tools.append(tool)
                
                # Add specific metadata for DataVisa
                if tool == "DataVisa":
                    context["datavisa_detected"] = True
                    context["tool_types"] = context.get("tool_types", [])
                    context["tool_types"].append("data_governance")
        
        if tools:
            context["internal_tools"] = tools
        
        # Check if this is enterprise policy
        if context.get("organization"):
            context["is_enterprise_policy"] = True
        
        # Detect business units
        business_units = []
        unit_patterns = [
            r"(?:department|division|unit|team)[\s:]+([\w\s]+)",
            r"(Risk Management|Compliance|Legal|IT|Data Protection) (?:Department|Division|Unit|Team)"
        ]
        
        import re
        for pattern in unit_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            business_units.extend(matches)
        
        if business_units:
            # Clean and deduplicate
            context["business_units"] = list(set([u.strip() for u in business_units if u.strip()]))[:5]
        
        return context if context else None
    
    def _extract_pdf_text(self, pdf_path: str) -> str:
        """Extract text from PDF"""
        try:
            if not os.path.exists(pdf_path):
                print(f"Warning: PDF not found: {pdf_path}")
                return ""
            
            text = self.pdf_processor.extract_text_from_pdf(pdf_path)
            return text
        except Exception as e:
            print(f"Error extracting from {pdf_path}: {e}")
            return ""
    
    def process_rule(self, rule_name: str, rule_config: Dict[str, Any],
                    generate_doc: bool = True) -> Optional[Dict[str, Any]]:
        """Process single rule with complete analysis"""
        print(f"\n{'='*80}")
        print(f"Processing Rule: {rule_name}")
        print(f"{'='*80}")
        
        countries = rule_config.get("country", [])
        adequacy_countries = rule_config.get("adequacy_country", [])
        jurisdiction = ", ".join(countries) if countries else "General"
        
        print(f"Jurisdiction: {jurisdiction}")
        if adequacy_countries:
            print(f"Adequacy Countries: {', '.join(adequacy_countries)}")
        
        level_1_file = rule_config.get("file_level_1")
        level_2_file = rule_config.get("file_level_2")
        level_3_file = rule_config.get("file_level_3")
        
        if not all([level_1_file, level_2_file, level_3_file]):
            print(f"Error: Missing file paths for '{rule_name}'")
            return None
        
        print(f"\nExtracting text from PDFs...")
        print(f"  Level 1: {level_1_file}")
        level_1_text = self._extract_pdf_text(level_1_file)
        
        print(f"  Level 2: {level_2_file}")
        level_2_text = self._extract_pdf_text(level_2_file)
        
        print(f"  Level 3: {level_3_file}")
        level_3_text = self._extract_pdf_text(level_3_file)
        
        if not (level_1_text or level_2_text or level_3_text):
            print(f"Error: No text extracted")
            return None
        
        all_text = f"{level_1_text} {level_2_text} {level_3_text}"
        enterprise_context = self._detect_enterprise_context(all_text)
        
        if enterprise_context:
            print(f"\nâœ“ Enterprise context detected:")
            if enterprise_context.get("organization"):
                print(f"  Organization: {enterprise_context['organization']}")
            if enterprise_context.get("internal_tools"):
                tools_str = ", ".join(enterprise_context['internal_tools'])
                print(f"  Internal Tools: {tools_str}")
                if enterprise_context.get("datavisa_detected"):
                    print(f"  â†’ DataVisa recognized as data governance tool")
            if enterprise_context.get("business_units"):
                print(f"  Business Units: {', '.join(enterprise_context['business_units'][:3])}")
        
        print(f"\nðŸ”¬ Analyzing with:")
        print(f"  â€¢ FalkorDB Graph RAG")
        print(f"  â€¢ {self.config.EMBEDDING_MODEL} embeddings")
        print(f"  â€¢ LangGraph workflows")
        print(f"  â€¢ Chain of Thought + Mixture of Experts + Tree of Thought")
        
        try:
            analysis = self.analyzer.analyze_multi_level(
                rule_name=rule_name,
                jurisdiction=jurisdiction,
                level_1_text=level_1_text,
                level_2_text=level_2_text,
                level_3_text=level_3_text,
                enterprise_context=enterprise_context
            )
            
            if not analysis.get("metadata"):
                analysis["metadata"] = {}
            
            analysis["metadata"]["adequacy_countries"] = adequacy_countries
            analysis["metadata"]["countries"] = countries
            
            print(f"\nâœ“ Analysis complete:")
            print(f"  Classification: {analysis.get('classification', 'N/A').upper()}")
            print(f"  Description: {len(analysis.get('description', ''))} chars")
            print(f"  Citations: {len(analysis.get('citations', []))}")
            print(f"  Data Actions: {len(analysis.get('data_actions', []))}")
            print(f"  User Evidence: {len(analysis.get('user_evidence', []))}")
            print(f"  System Evidence: {len(analysis.get('system_evidence', []))}")
            print(f"  Constraints: {len(analysis.get('constraints', []))}")
            
            kg_stats = analysis.get('metadata', {}).get('kg_stats', {})
            if kg_stats:
                print(f"  FalkorDB:")
                for key, value in kg_stats.items():
                    if isinstance(value, int):
                        print(f"    - {key}: {value}")
            
        except Exception as e:
            print(f"âœ— Error during analysis: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        if generate_doc:
            print(f"\nðŸ“„ Generating Word document...")
            try:
                doc_path = self.generator.generate_legal_summary(analysis)
                print(f"âœ“ Document saved: {doc_path}")
                
                json_path = doc_path.replace('.docx', '.json')
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, indent=2, ensure_ascii=False)
                print(f"âœ“ JSON saved: {json_path}")
                
            except Exception as e:
                print(f"âœ— Error generating document: {e}")
                import traceback
                traceback.print_exc()
        
        return analysis
    
    def process_all_rules(self, generate_docs: bool = True,
                         generate_index: bool = True) -> Dict[str, Dict[str, Any]]:
        """Process all rules"""
        if not self.metadata:
            print("No metadata loaded")
            return {}
        
        all_analyses = {}
        
        print(f"\n{'#'*80}")
        print(f"# Processing {len(self.metadata)} rules")
        print(f"{'#'*80}")
        
        for i, (rule_name, rule_config) in enumerate(self.metadata.items(), 1):
            print(f"\n[{i}/{len(self.metadata)}] {rule_name}")
            
            analysis = self.process_rule(
                rule_name=rule_name,
                rule_config=rule_config,
                generate_doc=generate_docs
            )
            
            if analysis:
                all_analyses[rule_name] = analysis
            else:
                print(f"âš  Skipped {rule_name}")
        
        if generate_index and all_analyses:
            print(f"\n{'='*80}")
            print(f"Generating Index")
            print(f"{'='*80}")
            
            try:
                index_path = self.generator.create_index_document(all_analyses)
                print(f"âœ“ Index: {index_path}")
            except Exception as e:
                print(f"âœ— Index error: {e}")
        
        print(f"\n{'#'*80}")
        print(f"# Complete")
        print(f"{'#'*80}")
        print(f"Processed: {len(all_analyses)}/{len(self.metadata)} rules")
        print(f"Output: {self.output_dir}")
        
        return all_analyses
    
    def process_single_rule(self, rule_name: str,
                           generate_doc: bool = True) -> Optional[Dict[str, Any]]:
        """Process single rule"""
        if rule_name not in self.metadata:
            print(f"Error: Rule '{rule_name}' not found")
            print(f"Available: {', '.join(self.metadata.keys())}")
            return None
        
        return self.process_rule(
            rule_name=rule_name,
            rule_config=self.metadata[rule_name],
            generate_doc=generate_doc
        )


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Legal Summary Pipeline with FalkorDB"
    )
    parser.add_argument(
        "--metadata",
        type=str,
        default="config/legislation_metadata.json",
        help="Metadata JSON file"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/legal_summaries",
        help="Output directory"
    )
    parser.add_argument(
        "--rule",
        type=str,
        help="Process specific rule"
    )
    parser.add_argument(
        "--no-docs",
        action="store_true",
        help="Skip document generation"
    )
    parser.add_argument(
        "--no-index",
        action="store_true",
        help="Skip index generation"
    )
    parser.add_argument(
        "--falkordb-host",
        type=str,
        default="localhost",
        help="FalkorDB host"
    )
    parser.add_argument(
        "--falkordb-port",
        type=int,
        default=6379,
        help="FalkorDB port"
    )
    
    args = parser.parse_args()
    
    # Set FalkorDB config if provided
    if args.falkordb_host:
        os.environ['FALKORDB_HOST'] = args.falkordb_host
    if args.falkordb_port:
        os.environ['FALKORDB_PORT'] = str(args.falkordb_port)
    
    pipeline = LegalSummaryPipeline(
        metadata_file=args.metadata,
        output_dir=args.output
    )
    
    if args.rule:
        pipeline.process_single_rule(
            rule_name=args.rule,
            generate_doc=not args.no_docs
        )
    else:
        pipeline.process_all_rules(
            generate_docs=not args.no_docs,
            generate_index=not args.no_index
        )


if __name__ == "__main__":
    main()
