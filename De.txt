"""
Enhanced Azure OpenAI service with detailed logging and robust error handling.
"""

import os
import logging
import json
import time
import socket
import traceback
import ssl
import requests
from typing import List, Dict, Any, Optional, Union
from dotenv import dotenv_values
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, ClientSecretCredential, get_bearer_token_provider
from azure.core.exceptions import ClientAuthenticationError, ServiceRequestError

# Configure detailed logging
logger = logging.getLogger(__name__)

# Load credentials directly from file
credentials_path = os.path.join("env", "credentials.env")
config_path = os.path.join("env", "config.env")
credentials_values = {}
config_values = {}

try:
    if os.path.isfile(credentials_path):
        logger.info(f"AzureOpenAIService loading credentials from {credentials_path}")
        credentials_values = dotenv_values(credentials_path)
        logger.info(f"Loaded {len(credentials_values)} values from {credentials_path}")
    else:
        logger.warning(f"Credentials file not found: {credentials_path}")
        
    if os.path.isfile(config_path):
        logger.info(f"AzureOpenAIService loading config from {config_path}")
        config_values = dotenv_values(config_path)
        logger.info(f"Loaded {len(config_values)} values from {config_path}")
    else:
        logger.warning(f"Config file not found: {config_path}")
except Exception as e:
    logger.error(f"Error loading env files: {e}")

# Combine values, with credentials taking precedence
all_values = {**config_values, **credentials_values}

# Extract values directly
AZURE_TENANT_ID = all_values.get("AZURE_TENANT_ID", "")
AZURE_CLIENT_ID = all_values.get("AZURE_CLIENT_ID", "")
AZURE_CLIENT_SECRET = all_values.get("AZURE_CLIENT_SECRET", "")
AZURE_OPENAI_ENDPOINT = all_values.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_EMBEDDING_MODEL = all_values.get("AZURE_EMBEDDING_MODEL", "text-embedding-3-large")
AZURE_EMBEDDING_DEPLOYMENT = all_values.get("AZURE_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
AZURE_LLM_MODEL = all_values.get("AZURE_LLM_MODEL", "gpt-4o-mini")
AZURE_LLM_DEPLOYMENT = all_values.get("AZURE_LLM_DEPLOYMENT", "gpt-4o-mini")

# Helper function to validate endpoint connectivity
def check_endpoint_connectivity(endpoint: str, timeout: int = 5) -> Dict[str, Any]:
    """
    Check connectivity to the endpoint.
    
    Args:
        endpoint: Endpoint URL
        timeout: Timeout in seconds
        
    Returns:
        Dictionary with connectivity status and details
    """
    try:
        # Parse the URL to get hostname
        from urllib.parse import urlparse
        parsed_url = urlparse(endpoint)
        hostname = parsed_url.netloc
        
        # Check basic socket connectivity
        logger.info(f"Testing socket connectivity to {hostname}...")
        socket_start = time.time()
        
        # Create a socket connection
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        
        # Get port (default 443 for https)
        port = parsed_url.port if parsed_url.port else (443 if parsed_url.scheme == 'https' else 80)
        
        # Try to connect
        result = sock.connect_ex((hostname, port))
        socket_time = time.time() - socket_start
        
        if result == 0:
            logger.info(f"Socket connection to {hostname}:{port} successful ({socket_time:.2f}s)")
            sock.close()
            
            # Try HTTPS request
            logger.info(f"Testing HTTPS connectivity to {endpoint}...")
            https_start = time.time()
            try:
                response = requests.get(
                    endpoint, 
                    timeout=timeout, 
                    verify=True  # Use system CA certificates
                )
                https_time = time.time() - https_start
                status_code = response.status_code
                logger.info(f"HTTPS connection to {endpoint} returned status {status_code} ({https_time:.2f}s)")
                
                return {
                    "socket_connectivity": True,
                    "socket_time": socket_time,
                    "https_connectivity": True,
                    "https_time": https_time,
                    "status_code": status_code,
                    "error": None
                }
            except requests.exceptions.SSLError as e:
                logger.error(f"SSL error connecting to {endpoint}: {e}")
                return {
                    "socket_connectivity": True,
                    "socket_time": socket_time,
                    "https_connectivity": False,
                    "error": f"SSL error: {str(e)}"
                }
            except Exception as e:
                logger.error(f"HTTPS error connecting to {endpoint}: {e}")
                return {
                    "socket_connectivity": True,
                    "socket_time": socket_time,
                    "https_connectivity": False,
                    "error": str(e)
                }
        else:
            logger.error(f"Socket connection to {hostname}:{port} failed with error code {result}")
            sock.close()
            return {
                "socket_connectivity": False,
                "socket_time": socket_time,
                "https_connectivity": False,
                "error": f"Socket error code {result}"
            }
    except Exception as e:
        logger.error(f"Error checking endpoint connectivity: {e}")
        return {
            "socket_connectivity": False,
            "https_connectivity": False,
            "error": str(e)
        }

class AzureOpenAIService:
    """Service for interacting with Azure OpenAI models with enhanced logging and diagnostics."""
    
    def __init__(self, diagnose_connectivity: bool = True):
        """
        Initialize the Azure OpenAI service.
        
        Args:
            diagnose_connectivity: Whether to run connectivity diagnostics at startup
        """
        self.tenant_id = AZURE_TENANT_ID
        self.client_id = AZURE_CLIENT_ID
        self.client_secret = AZURE_CLIENT_SECRET
        self.endpoint = AZURE_OPENAI_ENDPOINT
        self.embedding_model = AZURE_EMBEDDING_MODEL
        self.embedding_deployment = AZURE_EMBEDDING_DEPLOYMENT
        self.llm_model = AZURE_LLM_MODEL
        self.llm_deployment = AZURE_LLM_DEPLOYMENT
        self.api_version = "2023-05-15"
        
        # Show the actual values being used (partially masked for security)
        masked_tenant = f"{self.tenant_id[:4]}...{self.tenant_id[-4:]}" if len(self.tenant_id) > 8 else "***"
        masked_client = f"{self.client_id[:4]}...{self.client_id[-4:]}" if len(self.client_id) > 8 else "***"
        logger.info(f"AzureOpenAIService initializing with:")
        logger.info(f"  - Tenant ID: {masked_tenant}")
        logger.info(f"  - Client ID: {masked_client}") 
        logger.info(f"  - Client secret length: {len(self.client_secret)} characters")
        logger.info(f"  - OpenAI endpoint: {self.endpoint}")
        logger.info(f"  - Embedding model: {self.embedding_model}")
        logger.info(f"  - Embedding deployment: {self.embedding_deployment}")
        logger.info(f"  - LLM model: {self.llm_model}")
        logger.info(f"  - LLM deployment: {self.llm_deployment}")
        
        # Check endpoint connectivity if requested
        if diagnose_connectivity and self.endpoint:
            logger.info("Running connectivity diagnostics...")
            connectivity = check_endpoint_connectivity(self.endpoint)
            if connectivity["socket_connectivity"] and connectivity.get("https_connectivity", False):
                logger.info("Endpoint connectivity check passed")
            else:
                logger.warning(f"Endpoint connectivity issues detected: {connectivity.get('error', 'Unknown error')}")
                logger.warning("Attempting to initialize client anyway...")
        
        # Initialize the Azure OpenAI client
        self._initialize_client()
        logger.info(f"AzureOpenAIService initialized successfully")
    
    def _initialize_client(self):
        """Initialize the Azure OpenAI client using Azure AD credentials."""
        try:
            logger.info("Creating ClientSecretCredential...")
            # Create credentials
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            
            logger.info("Getting bearer token provider...")
            # Get token provider
            token_provider = get_bearer_token_provider(
                credential,
                "https://cognitiveservices.azure.com/.default"
            )
            
            logger.info(f"Initializing AzureOpenAI client with endpoint: {self.endpoint}")
            # Initialize the client with token provider
            self.client = AzureOpenAI(
                azure_endpoint=self.endpoint,
                api_version=self.api_version,
                azure_ad_token_provider=token_provider
            )
            
            logger.info("Azure OpenAI client initialized successfully")
            
            # Test the client by making a simple API call
            logger.info("Testing client with a simple API call...")
            try:
                # Try to list models as a simple API test
                models = self.client.models.list()
                model_list = [model.id for model in models.data]
                logger.info(f"API test successful. Available models: {model_list}")
            except Exception as e:
                logger.warning(f"API test failed: {e}")
                logger.warning("Client initialization completed but API access might be problematic")
                
        except ClientAuthenticationError as e:
            logger.error(f"Azure AD authentication error: {e}")
            logger.error("Check your tenant_id, client_id, and client_secret")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize Azure OpenAI client: {e}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            raise
    
    def refresh_tokens(self):
        """Refresh the Azure tokens by reinitializing the client."""
        try:
            logger.info("Refreshing Azure AD token by reinitializing the client...")
            self._initialize_client()
            logger.info("Azure OpenAI client reinitialized with fresh token")
            return True
        except Exception as e:
            logger.error(f"Failed to refresh Azure OpenAI token: {e}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            return False
    
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts using Azure OpenAI SDK.
        
        Args:
            texts: List of texts to embed
            
        Returns:
            List of embedding vectors
        """
        try:
            # Return empty list if no texts provided
            if not texts:
                return []
                
            logger.info(f"Generating embeddings for {len(texts)} texts...")
            
            # Batch the requests to avoid overloading the API
            # Process in chunks of 20 texts at a time
            batch_size = 10
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # Debug the first few characters of each text
                for j, text in enumerate(batch_texts):
                    preview = text[:50] + "..." if len(text) > 50 else text
                    logger.debug(f"Batch {i//batch_size + 1}, Text {j+1}: {preview}")
                
                try:
                    logger.info(f"Sending batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} to API...")
                    start_time = time.time()
                    
                    # Generate embeddings for the batch
                    response = self.client.embeddings.create(
                        input=batch_texts,
                        model=self.embedding_deployment,
                        dimensions=3072  # Maximum dimensions for text-embedding-3-large
                    )
                    
                    # Extract embeddings from response
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"Generated embeddings for batch of {len(batch_texts)} texts in {elapsed_time:.2f}s")
                    
                except Exception as e:
                    logger.error(f"Error generating embeddings for batch {i//batch_size + 1}: {e}")
                    logger.error(f"Stack trace: {traceback.format_exc()}")
                    
                    # Check for specific error types and provide helpful messages
                    error_str = str(e)
                    if "authentication" in error_str.lower() or "unauthorized" in error_str.lower():
                        logger.info("Authentication error detected, attempting to refresh token...")
                        if self.refresh_tokens():
                            logger.info("Token refreshed, retrying the request...")
                            try:
                                # Retry the request
                                response = self.client.embeddings.create(
                                    input=batch_texts,
                                    model=self.embedding_deployment,
                                    dimensions=3072
                                )
                                batch_embeddings = [item.embedding for item in response.data]
                                all_embeddings.extend(batch_embeddings)
                                logger.info(f"Successfully generated embeddings after token refresh")
                            except Exception as retry_e:
                                logger.error(f"Retry failed after token refresh: {retry_e}")
                                raise Exception(f"Failed to generate embeddings even after token refresh: {retry_e}")
                        else:
                            logger.error("Failed to refresh token, cannot continue")
                            raise Exception("Failed to refresh token after authentication error")
                    elif "not found" in error_str.lower():
                        logger.error(f"Deployment not found. Check if '{self.embedding_deployment}' exists in your Azure OpenAI resource.")
                        raise Exception(f"Deployment '{self.embedding_deployment}' not found. Please check your Azure OpenAI setup.")
                    elif "model" in error_str.lower() and "not" in error_str.lower():
                        logger.error(f"Model issue detected. Check if model '{self.embedding_model}' is correctly set up.")
                        raise Exception(f"Model issue with '{self.embedding_model}'. Please verify your Azure OpenAI configuration.")
                    elif "timeout" in error_str.lower() or "connection" in error_str.lower():
                        logger.error("Connection or timeout issue detected.")
                        # Try to check connectivity again
                        connectivity = check_endpoint_connectivity(self.endpoint)
                        logger.error(f"Connectivity check results: {connectivity}")
                        raise Exception(f"Connection issues with endpoint {self.endpoint}. Check network and proxy settings.")
                    else:
                        raise
                
                # Add a small delay between batches to avoid rate limiting
                if i + batch_size < len(texts):
                    logger.debug("Adding delay between batches...")
                    time.sleep(1.0)
            
            logger.info(f"Successfully generated {len(all_embeddings)} embeddings")
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            raise
    
    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate embeddings for a single text using Azure OpenAI SDK.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        try:
            logger.info("Generating embedding for single text...")
            preview = text[:50] + "..." if len(text) > 50 else text
            logger.debug(f"Text: {preview}")
            
            start_time = time.time()
            
            # Generate embedding
            response = self.client.embeddings.create(
                input=text,
                model=self.embedding_deployment,
                dimensions=3072  # Maximum dimensions for text-embedding-3-large
            )
            
            # Extract embedding from response
            embedding = response.data[0].embedding
            
            elapsed_time = time.time() - start_time
            logger.info(f"Generated single embedding in {elapsed_time:.2f}s")
            
            return embedding
            
        except Exception as e:
            logger.error(f"Error generating single embedding: {e}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            
            # Try to refresh token and retry once
            if "authentication" in str(e).lower() or "unauthorized" in str(e).lower():
                logger.info("Authentication error, attempting to refresh token...")
                if self.refresh_tokens():
                    # Retry the request
                    try:
                        response = self.client.embeddings.create(
                            input=text,
                            model=self.embedding_deployment,
                            dimensions=3072
                        )
                        embedding = response.data[0].embedding
                        logger.info("Successfully generated embedding after token refresh")
                        return embedding
                    except Exception as retry_e:
                        logger.error(f"Retry failed after token refresh: {retry_e}")
                        raise Exception(f"Failed to generate embedding even after token refresh: {retry_e}")
                else:
                    raise Exception("Failed to refresh token after authentication error")
            else:
                raise
    
    async def generate_completion(self, 
                                messages: List[Dict[str, str]], 
                                temperature: float = 0.0,
                                max_tokens: int = 2000) -> str:
        """
        Generate a completion using Azure OpenAI SDK.
        
        Args:
            messages: List of messages (system, user, assistant)
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        try:
            logger.info(f"Generating completion with temperature={temperature}, max_tokens={max_tokens}")
            logger.debug(f"Using {len(messages)} messages")
            
            start_time = time.time()
            
            # Generate completion
            response = self.client.chat.completions.create(
                model=self.llm_deployment,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Extract content from response
            content = response.choices[0].message.content
            
            elapsed_time = time.time() - start_time
            logger.info(f"Generated completion in {elapsed_time:.2f}s")
            
            return content
            
        except Exception as e:
            logger.error(f"Error generating completion: {e}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            
            # Try to refresh token and retry once
            if "authentication" in str(e).lower() or "unauthorized" in str(e).lower():
                logger.info("Authentication error, attempting to refresh token...")
                if self.refresh_tokens():
                    try:
                        # Retry the request
                        response = self.client.chat.completions.create(
                            model=self.llm_deployment,
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        content = response.choices[0].message.content
                        logger.info("Successfully generated completion after token refresh")
                        return content
                    except Exception as retry_e:
                        logger.error(f"Retry failed after token refresh: {retry_e}")
                        raise Exception(f"Failed to generate completion even after token refresh: {retry_e}")
                else:
                    raise Exception("Failed to refresh token after authentication error")
            else:
                raise
