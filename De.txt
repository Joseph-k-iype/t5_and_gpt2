"""
Intelligent Document Chunker with Context Management
Optimized for token efficiency while maintaining quality
"""

from typing import List, Dict, Any, Optional
import re


class DocumentChunker:
    """
    Smart document chunker that respects semantic boundaries
    and provides efficient context management
    """
    
    def __init__(
        self,
        chunk_size: int = 8000,
        chunk_overlap: int = 500,
        respect_boundaries: bool = True
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.respect_boundaries = respect_boundaries
        
    def chunk_document(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Chunk document into manageable pieces with smart boundaries
        
        Args:
            text: Full document text
            metadata: Optional metadata about the document
            
        Returns:
            List of chunk dictionaries with text, metadata, and context
        """
        if len(text) <= self.chunk_size:
            return [{
                "text": text,
                "chunk_id": 0,
                "total_chunks": 1,
                "start_pos": 0,
                "end_pos": len(text),
                "metadata": metadata or {}
            }]
        
        chunks = []
        start = 0
        chunk_id = 0
        
        # Calculate approximate total chunks
        estimated_chunks = max(1, len(text) // (self.chunk_size - self.chunk_overlap))
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # Try to break at semantic boundaries if enabled
            if self.respect_boundaries and end < len(text):
                end = self._find_semantic_boundary(text, start, end)
            
            chunk_text = text[start:end].strip()
            
            if chunk_text:
                chunks.append({
                    "text": chunk_text,
                    "chunk_id": chunk_id,
                    "total_chunks": estimated_chunks,  # Will update after
                    "start_pos": start,
                    "end_pos": end,
                    "metadata": metadata or {}
                })
                chunk_id += 1
            
            # Move to next position with overlap
            if end >= len(text):
                break
            start = max(end - self.chunk_overlap, start + 1)
        
        # Update total_chunks for all chunks
        total = len(chunks)
        for chunk in chunks:
            chunk["total_chunks"] = total
        
        return chunks
    
    def _find_semantic_boundary(
        self,
        text: str,
        start: int,
        end: int
    ) -> int:
        """
        Find the best semantic boundary near the end position
        Prioritizes: paragraph > sentence > word
        """
        search_window = 300  # Look back up to 300 chars
        search_start = max(end - search_window, start)
        
        # Try to break at paragraph boundary (double newline)
        para_match = text.rfind('\n\n', search_start, end)
        if para_match > search_start:
            return para_match + 2
        
        # Try to break at sentence boundary
        sentence_endings = ['. ', '! ', '? ', '.\n', '!\n', '?\n']
        best_boundary = -1
        
        for ending in sentence_endings:
            pos = text.rfind(ending, search_start, end)
            if pos > best_boundary:
                best_boundary = pos + len(ending)
        
        if best_boundary > search_start:
            return best_boundary
        
        # Fall back to word boundary
        space_pos = text.rfind(' ', search_start, end)
        if space_pos > search_start:
            return space_pos + 1
        
        # Last resort: use original end
        return end
    
    def get_chunk_context(self, chunk: Dict[str, Any]) -> str:
        """
        Generate a compact context description for a chunk
        
        Args:
            chunk: Chunk dictionary
            
        Returns:
            Compact context string
        """
        chunk_id = chunk["chunk_id"]
        total = chunk["total_chunks"]
        
        return f"Chunk {chunk_id + 1}/{total} (chars {chunk['start_pos']}-{chunk['end_pos']})"
    
    def merge_chunk_analyses(
        self,
        chunk_analyses: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Merge multiple chunk analyses into a single comprehensive analysis
        Uses set operations to avoid duplicates while preserving all unique items
        
        Args:
            chunk_analyses: List of analysis dictionaries from individual chunks
            
        Returns:
            Merged analysis dictionary
        """
        if not chunk_analyses:
            return {}
        
        if len(chunk_analyses) == 1:
            return chunk_analyses[0]
        
        # Initialize merged result
        merged = {
            "description": "",
            "user_actions": [],
            "system_actions": [],
            "user_duties": [],
            "system_duties": [],
            "constraints": [],
            "rule_type": "",
            "confidence": "medium",
            "chunks_processed": len(chunk_analyses)
        }
        
        # Collect all unique items using sets
        all_user_actions = set()
        all_system_actions = set()
        all_user_duties = set()
        all_system_duties = set()
        all_descriptions = []
        
        # Merge constraints carefully (they're dicts)
        constraint_set = {}  # Use dict to deduplicate by type+description
        
        for analysis in chunk_analyses:
            # Collect actions and duties
            all_user_actions.update(analysis.get("user_actions", []))
            all_system_actions.update(analysis.get("system_actions", []))
            all_user_duties.update(analysis.get("user_duties", []))
            all_system_duties.update(analysis.get("system_duties", []))
            
            # Collect descriptions
            desc = analysis.get("description", "").strip()
            if desc:
                all_descriptions.append(desc)
            
            # Merge constraints
            for constraint in analysis.get("constraints", []):
                key = f"{constraint.get('type', '')}:{constraint.get('description', '')}"
                if key not in constraint_set:
                    constraint_set[key] = constraint
        
        # Build merged result
        merged["user_actions"] = sorted(list(all_user_actions))
        merged["system_actions"] = sorted(list(all_system_actions))
        merged["user_duties"] = sorted(list(all_user_duties))
        merged["system_duties"] = sorted(list(all_system_duties))
        merged["constraints"] = list(constraint_set.values())
        
        # Merge descriptions intelligently
        if all_descriptions:
            # Take the longest/most detailed description
            merged["description"] = max(all_descriptions, key=len)
        
        # Use most confident rule_type if specified
        rule_types = [a.get("rule_type", "") for a in chunk_analyses if a.get("rule_type")]
        if rule_types:
            merged["rule_type"] = rule_types[0]  # Take first non-empty
        
        # Confidence is lowest among chunks (most conservative)
        confidences = [a.get("confidence", "medium") for a in chunk_analyses]
        confidence_order = {"high": 3, "medium": 2, "low": 1}
        min_confidence = min(confidences, key=lambda x: confidence_order.get(x, 2))
        merged["confidence"] = min_confidence
        
        return merged
    
    def compress_analysis_for_context(
        self,
        analysis: Dict[str, Any],
        max_length: int = 1000
    ) -> str:
        """
        Compress an analysis into a compact summary for use in context
        
        Args:
            analysis: Full analysis dictionary
            max_length: Maximum length of compressed summary
            
        Returns:
            Compressed summary string
        """
        summary_parts = []
        
        # Description (truncated if needed)
        desc = analysis.get("description", "")
        if desc:
            desc_limit = max_length // 3
            if len(desc) > desc_limit:
                desc = desc[:desc_limit] + "..."
            summary_parts.append(f"Description: {desc}")
        
        # Actions (count only for brevity)
        user_actions = analysis.get("user_actions", [])
        system_actions = analysis.get("system_actions", [])
        
        if user_actions:
            summary_parts.append(f"User Actions ({len(user_actions)}): {', '.join(user_actions[:5])}")
        if system_actions:
            summary_parts.append(f"System Actions ({len(system_actions)}): {', '.join(system_actions[:5])}")
        
        # Constraints (count only)
        constraints = analysis.get("constraints", [])
        if constraints:
            summary_parts.append(f"Constraints: {len(constraints)} items")
        
        # Confidence and type
        if analysis.get("rule_type"):
            summary_parts.append(f"Type: {analysis['rule_type']}")
        if analysis.get("confidence"):
            summary_parts.append(f"Confidence: {analysis['confidence']}")
        
        return " | ".join(summary_parts)


class MessageWindowManager:
    """
    Manages message history with sliding window to prevent context overflow
    """
    
    def __init__(self, max_messages: int = 15):
        """
        Args:
            max_messages: Maximum number of messages to keep in history
        """
        self.max_messages = max_messages
    
    def trim_messages(
        self,
        messages: List[Any],
        keep_system: bool = True
    ) -> List[Any]:
        """
        Trim messages to stay within max_messages limit
        
        Args:
            messages: List of messages
            keep_system: If True, always keep the first system message
            
        Returns:
            Trimmed list of messages
        """
        if len(messages) <= self.max_messages:
            return messages
        
        if keep_system and len(messages) > 0:
            # Keep first system message + most recent messages
            system_msg = messages[0]
            recent_messages = messages[-(self.max_messages - 1):]
            return [system_msg] + recent_messages
        else:
            # Just keep most recent messages
            return messages[-self.max_messages:]
    
    def create_summary_message(
        self,
        old_messages: List[Any],
        summary_template: str = "Previous context summary: {summary}"
    ) -> str:
        """
        Create a summary message from old messages being removed
        
        Args:
            old_messages: Messages to summarize
            summary_template: Template for summary message
            
        Returns:
            Summary string
        """
        # Extract key information from old messages
        key_points = []
        
        for msg in old_messages:
            content = getattr(msg, 'content', '')
            if isinstance(content, str) and len(content) > 50:
                # Extract first sentence or first 100 chars
                first_part = content[:100].split('.')[0]
                key_points.append(first_part)
        
        if key_points:
            summary = " | ".join(key_points[:3])  # Keep top 3 points
            return summary_template.format(summary=summary)
        
        return ""
