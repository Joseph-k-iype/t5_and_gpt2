"""
LangGraph-based Legislation to Machine-Readable JSON Rules Converter
Using o3-mini with Advanced Reasoning: Mixture of Thought + Mixture of Chains + Chain of Thought + Mixture of Reasoning
"""

import json
import re
import asyncio
from typing import List, Dict, Any, Optional, Annotated, Sequence
from dataclasses import dataclass
from enum import Enum

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field


# State Management
class AgentState(BaseModel):
    """Comprehensive state for the legislation processing agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    legislation_text: str = ""
    reasoning_pathways: List[Dict[str, Any]] = []
    pathway_analyses: Dict[str, Any] = {}
    chain_results: Dict[str, Any] = {}
    extraction_results: Dict[str, List[Dict[str, Any]]] = {}
    raw_rules: List[Dict[str, Any]] = []
    extracted_rules: List[Dict[str, Any]] = []
    synthesized_rules: List[Dict[str, Any]] = []
    json_rules: List[Dict[str, Any]] = []
    processing_stage: str = "initial"
    processing_step: str = "initialization"
    current_pathway: str = ""
    current_chain: str = ""
    validation_results: Dict[str, Any] = {}
    quality_metrics: Dict[str, Any] = {}
    confidence_scores: Dict[str, float] = {}
    metadata: Dict[str, Any] = {}
    processing_context: Dict[str, Any] = {}
    reasoning_traces: List[Dict[str, Any]] = []
    synthesis_strategy: str = "weighted"
    validation_depth: str = "comprehensive"
    reasoning_effort: str = "medium"


class RuleType(Enum):
    """Types of rules that can be extracted from legislation"""
    CONDITIONAL = "conditional"
    REQUIREMENT = "requirement"
    PROHIBITION = "prohibition"
    DEFINITION = "definition"
    PROCEDURE = "procedure"
    PENALTY = "penalty"
    EXCEPTION = "exception"
    TEMPORAL = "temporal"
    SCOPE = "scope"


@dataclass
class LegislationRule:
    """Structured representation of a rule extracted from legislation"""
    rule_id: str
    rule_type: RuleType
    title: str
    description: str
    conditions: List[str]
    actions: List[str]
    references: List[str]
    keywords: List[str]
    confidence: float
    source_pathway: str = ""
    extraction_chain: str = ""
    reasoning_trace: List[str] = None
    
    def __post_init__(self):
        if self.reasoning_trace is None:
            self.reasoning_trace = []


class ProcessingPhase(Enum):
    """Processing phases for legislation analysis"""
    INITIALIZATION = "initialization"
    DIVERGENT_ANALYSIS = "divergent_analysis"
    SPECIALIZED_EXTRACTION = "specialized_extraction"
    CONVERGENT_SYNTHESIS = "convergent_synthesis"
    PRECISION_CONVERSION = "precision_conversion"
    RIGOROUS_VALIDATION = "rigorous_validation"
    COMPLETION = "completion"


class ReasoningPathway(Enum):
    """Available reasoning pathways for mixture of thought"""
    STRUCTURAL = "structural"
    SEMANTIC = "semantic"
    LOGICAL = "logical"
    CONTEXTUAL = "contextual"


class ReasoningMode(Enum):
    """Available reasoning modes for mixture of reasoning"""
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"
    ABDUCTIVE = "abductive"
    ANALOGICAL = "analogical"


class ExtractionChain(Enum):
    """Available extraction chains for mixture of chains"""
    REQUIREMENTS = "requirements"
    PROHIBITIONS = "prohibitions"
    CONDITIONS = "conditions"
    EXCEPTIONS = "exceptions"
    PENALTIES = "penalties"


# Pydantic Input Models for Tools

class LegislationAnalysisInput(BaseModel):
    legislation_text: str = Field(..., description="Raw legislation text to analyze")
    reasoning_pathway: str = Field(..., description="Type of reasoning pathway")
    reasoning_mode: str = Field(..., description="Type of reasoning mode")


class RuleExtractionInput(BaseModel):
    text_segment: str = Field(..., description="Legislation text segment to process")
    analysis_context: Dict[str, Any] = Field(..., description="Context from previous analysis")
    extraction_focus: str = Field(..., description="Focus area for extraction")
    reasoning_mode: str = Field(..., description="Reasoning mode to apply")


class RuleSynthesisInput(BaseModel):
    pathway_results: List[Dict[str, Any]] = Field(..., description="Results from different reasoning pathways")
    synthesis_strategy: str = Field(..., description="Strategy for synthesis")
    reasoning_modes: List[str] = Field(..., description="Reasoning modes used")


class JSONConversionInput(BaseModel):
    synthesized_rules: List[Dict[str, Any]] = Field(..., description="Rules from synthesis process")
    conversion_standard: str = Field(default="json-rules-engine", description="Target JSON rules format")


class ValidationInput(BaseModel):
    json_rules: List[Dict[str, Any]] = Field(..., description="JSON rules to validate")
    validation_depth: str = Field(default="comprehensive", description="Validation depth")


# Advanced Tools with Mixture of Thought + Mixture of Reasoning

@tool(args_schema=LegislationAnalysisInput)
def analyze_legislation_pathway(legislation_text: str, reasoning_pathway: str, reasoning_mode: str) -> Dict[str, Any]:
    """
    Analyze legislation using specific reasoning pathway and reasoning mode.
    Combines Mixture of Thought with Mixture of Reasoning approaches.
    """
    
    analysis = {
        'pathway': reasoning_pathway,
        'reasoning_mode': reasoning_mode,
        'analysis_results': {},
        'reasoning_trace': []
    }
    
    # Apply reasoning mode
    if reasoning_mode == "deductive":
        # Top-down logical reasoning from general principles to specific cases
        analysis['reasoning_trace'].append("Applying deductive reasoning: general principles → specific instances")
        
    elif reasoning_mode == "inductive":
        # Bottom-up reasoning from specific observations to general patterns
        analysis['reasoning_trace'].append("Applying inductive reasoning: specific patterns → general rules")
        
    elif reasoning_mode == "abductive":
        # Best explanation reasoning for incomplete information
        analysis['reasoning_trace'].append("Applying abductive reasoning: best explanation for observed patterns")
        
    elif reasoning_mode == "analogical":
        # Reasoning by analogy and comparison
        analysis['reasoning_trace'].append("Applying analogical reasoning: pattern matching and comparison")
    
    # Apply reasoning pathway
    if reasoning_pathway == "structural":
        structure_patterns = {
            'sections': r'(?:Section|Sec\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Section|Sec\.?)\s+\d+|$)',
            'articles': r'(?:Article|Art\.?)\s+(\d+(?:\.\d+)*)[:\.\s]+(.*?)(?=(?:Article|Art\.?)\s+\d+|$)',
            'subsections': r'(?:^|\n)\s*\(([a-z]|\d+)\)\s+(.*?)(?=\n\s*\([a-z]|\d+\)|$)',
            'clauses': r'(?:^|\n)\s*\(i+\)\s+(.*?)(?=\n\s*\(i+\)|$)'
        }
        
        analysis['analysis_results'] = {
            'hierarchy': {},
            'relationships': [],
            'organization_quality': 'high'
        }
        
        for element_type, pattern in structure_patterns.items():
            matches = re.findall(pattern, legislation_text, re.DOTALL | re.MULTILINE)
            analysis['analysis_results']['hierarchy'][element_type] = [
                {'id': match[0] if isinstance(match, tuple) else f"{element_type}_{i+1}", 
                 'content': match[1] if isinstance(match, tuple) else match}
                for i, match in enumerate(matches)
            ]
        
        cross_refs = re.findall(r'(?:Section|Article)\s+(\d+(?:\.\d+)*)', legislation_text)
        analysis['analysis_results']['relationships'] = [{'type': 'cross_reference', 'target': ref} for ref in set(cross_refs)]
        
    elif reasoning_pathway == "semantic":
        semantic_patterns = {
            'definitions': [
                r'"([^"]+)"\s+means\s+([^.]+)',
                r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+means\s+([^.]+)',
                r'(?:For the purposes of|In this)\s+[^,]*,\s*"([^"]+)"\s+([^.]+)'
            ],
            'intent_indicators': [r'\b(?:purpose|intent|objective|goal|aim)\b'],
            'scope_indicators': [r'\b(?:applies to|covers|includes|excludes|except)\b']
        }
        
        analysis['analysis_results'] = {
            'definitions': [],
            'intent_phrases': [],
            'scope_elements': [],
            'semantic_density': 'medium'
        }
        
        for pattern in semantic_patterns['definitions']:
            matches = re.findall(pattern, legislation_text, re.MULTILINE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    analysis['analysis_results']['definitions'].append({
                        'term': match[0].strip(),
                        'definition': match[1].strip()
                    })
        
    elif reasoning_pathway == "logical":
        logical_patterns = {
            'conditionals': r'(?:If|When|Where)\s+([^,]+),\s*(?:then\s+)?([^.]+)',
            'requirements': r'([^.]+?)\s+(?:shall|must)\s+([^.]+)',
            'prohibitions': r'([^.]+?)\s+(?:shall not|must not|may not|prohibited from)\s+([^.]+)',
            'exceptions': r'(?:except|unless|provided that|notwithstanding)\s+([^,]+),\s*([^.]+)',
            'penalties': r'(?:violation|breach|failure)[^.]*?(?:shall result in|subject to|punishable by)\s+([^.]+)'
        }
        
        analysis['analysis_results'] = {
            'conditionals': [],
            'requirements': [],
            'prohibitions': [],
            'exceptions': [],
            'penalties': [],
            'logical_complexity': 'medium'
        }
        
        for rule_type, pattern in logical_patterns.items():
            matches = re.findall(pattern, legislation_text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple):
                    analysis['analysis_results'][rule_type].append({
                        'condition': match[0].strip() if len(match) > 1 else '',
                        'consequence': match[1].strip() if len(match) > 1 else match[0].strip(),
                        'confidence': 0.8
                    })
        
    elif reasoning_pathway == "contextual":
        contextual_patterns = {
            'actors': [
                r'\b(?:person|individual|entity|organization|company|corporation|agency|department|officer|employee|citizen|resident)\b',
                r'\b(?:applicant|licensee|registrant|holder|owner|operator|provider|contractor)\b'
            ],
            'actions': [
                r'\b(?:shall|must|may|should|will|can)\s+([^.]+)',
                r'\b(?:provide|submit|maintain|ensure|comply|notify|report|register|apply)\b'
            ],
            'temporal': [
                r'\b(?:within|before|after|by|no later than)\s+\d+\s+(?:days|weeks|months|years)\b',
                r'\b(?:immediately|promptly|forthwith|annually|monthly|quarterly)\b'
            ]
        }
        
        analysis['analysis_results'] = {
            'actors': [],
            'actions': [],
            'temporal_elements': [],
            'context_richness': 'high'
        }
        
        for category, patterns in contextual_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, legislation_text, re.IGNORECASE)
                if category == 'actors':
                    analysis['analysis_results'][category].extend(list(set(matches)))
                elif category == 'actions':
                    analysis['analysis_results'][category].extend([match if isinstance(match, str) else match[0] for match in matches])
                else:
                    analysis['analysis_results'][category].extend(matches)
        
        for key in analysis['analysis_results']:
            if isinstance(analysis['analysis_results'][key], list):
                analysis['analysis_results'][key] = list(set(analysis['analysis_results'][key]))
    
    return analysis


@tool(args_schema=RuleExtractionInput)
def extract_rules_chain(text_segment: str, analysis_context: Dict[str, Any], extraction_focus: str, reasoning_mode: str) -> Dict[str, Any]:
    """
    Extract specific types of rules using chain of thought reasoning enhanced with reasoning modes.
    """
    
    reasoning_chain = []
    extracted_rules = []
    
    # Step 1: Context Assessment with reasoning mode
    reasoning_chain.append({
        'step': 1,
        'action': 'context_assessment',
        'reasoning_mode': reasoning_mode,
        'reasoning': f'Analyzing text segment for {extraction_focus} extraction using {reasoning_mode} reasoning',
        'findings': f'Text length: {len(text_segment)} characters, Focus: {extraction_focus}'
    })
    
    if extraction_focus == "requirements":
        requirement_patterns = [
            r'([^.]+?)\s+(?:shall|must|required to|obligated to|duty to)\s+([^.]+)',
            r'(?:must|shall)\s+([^.]+)',
            r'([^.]+?)\s+(?:is required|are required)\s+([^.]+)'
        ]
        
        reasoning_chain.append({
            'step': 2,
            'action': 'pattern_identification',
            'reasoning_mode': reasoning_mode,
            'reasoning': f'Identifying requirement patterns using {reasoning_mode} approach with modal verbs',
            'patterns_used': requirement_patterns
        })
        
        for i, pattern in enumerate(requirement_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'requirement',
                        'subject': match[0].strip(),
                        'obligation': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9,
                        'reasoning_mode': reasoning_mode
                    })
                else:
                    extracted_rules.append({
                        'type': 'requirement',
                        'content': match.strip() if isinstance(match, str) else str(match),
                        'pattern_id': i,
                        'confidence': 0.7,
                        'reasoning_mode': reasoning_mode
                    })
    
    elif extraction_focus == "prohibitions":
        prohibition_patterns = [
            r'([^.]+?)\s+(?:shall not|must not|may not|cannot|prohibited from|forbidden from)\s+([^.]+)',
            r'(?:no\s+)?([^.]+?)\s+(?:shall|may)\s+([^.]+)',
            r'(?:it is prohibited|forbidden|illegal)\s+([^.]+)'
        ]
        
        for i, pattern in enumerate(prohibition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'prohibition',
                        'subject': match[0].strip(),
                        'prohibited_action': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.9,
                        'reasoning_mode': reasoning_mode
                    })
    
    elif extraction_focus == "conditions":
        condition_patterns = [
            r'(?:If|When|Where|Provided that|In case)\s+([^,]+),\s*([^.]+)',
            r'([^.]+?)\s+(?:if|when|where|provided)\s+([^.]+)',
            r'(?:Subject to|Conditional upon)\s+([^,]+),\s*([^.]+)'
        ]
        
        for i, pattern in enumerate(condition_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'condition',
                        'trigger': match[0].strip(),
                        'consequence': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8,
                        'reasoning_mode': reasoning_mode
                    })
    
    elif extraction_focus == "exceptions":
        exception_patterns = [
            r'(?:except|unless|save|but|however|notwithstanding)\s+([^,]+),\s*([^.]+)',
            r'([^.]+)\s+(?:except|unless|save for)\s+([^.]+)',
            r'(?:this does not apply|shall not apply)\s+(?:to|when|if)\s+([^.]+)'
        ]
        
        for i, pattern in enumerate(exception_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'exception',
                        'exception_condition': match[0].strip(),
                        'modified_rule': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.7,
                        'reasoning_mode': reasoning_mode
                    })
    
    elif extraction_focus == "penalties":
        penalty_patterns = [
            r'(?:violation|breach|failure to comply|non-compliance)[^.]*?(?:shall result in|subject to|punishable by|penalty of)\s+([^.]+)',
            r'(?:fine|penalty|sanction|imprisonment|suspension|revocation)\s+(?:of|not exceeding|up to)\s+([^.]+)',
            r'([^.]+)\s+(?:shall be fined|shall be subject to|shall result in)\s+([^.]+)'
        ]
        
        for i, pattern in enumerate(penalty_patterns):
            matches = re.findall(pattern, text_segment, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    extracted_rules.append({
                        'type': 'penalty',
                        'violation': match[0].strip(),
                        'sanction': match[1].strip(),
                        'pattern_id': i,
                        'confidence': 0.8,
                        'reasoning_mode': reasoning_mode
                    })
    
    reasoning_chain.append({
        'step': 3,
        'action': 'rule_extraction_complete',
        'reasoning_mode': reasoning_mode,
        'reasoning': f'Extracted {len(extracted_rules)} rules using {reasoning_mode} reasoning approach',
        'extraction_quality': 'high' if len(extracted_rules) > 0 else 'low'
    })
    
    return {
        'extraction_focus': extraction_focus,
        'extracted_rules': extracted_rules,
        'reasoning_chain': reasoning_chain,
        'reasoning_mode': reasoning_mode,
        'context_used': analysis_context.get('pathway', 'unknown'),
        'extraction_metadata': {
            'text_length': len(text_segment),
            'rules_found': len(extracted_rules),
            'confidence_avg': sum([rule.get('confidence', 0) for rule in extracted_rules]) / len(extracted_rules) if extracted_rules else 0
        }
    }


@tool(args_schema=RuleSynthesisInput)
def synthesize_rules(pathway_results: List[Dict[str, Any]], synthesis_strategy: str, reasoning_modes: List[str]) -> Dict[str, Any]:
    """
    Synthesize rules from multiple reasoning pathways and modes using mixture approaches.
    """
    
    synthesized_rules = []
    synthesis_rationale = []
    
    all_rules = []
    pathway_weights = {
        'structural': 0.2,
        'semantic': 0.25,
        'logical': 0.35,
        'contextual': 0.2
    }
    
    reasoning_mode_weights = {
        'deductive': 0.3,
        'inductive': 0.25,
        'abductive': 0.25,
        'analogical': 0.2
    }
    
    for result in pathway_results:
        if 'extracted_rules' in result:
            for rule in result['extracted_rules']:
                rule['source_pathway'] = result.get('extraction_focus', 'unknown')
                rule['pathway_weight'] = pathway_weights.get(result.get('context_used', 'unknown'), 0.1)
                rule['reasoning_mode_weight'] = reasoning_mode_weights.get(rule.get('reasoning_mode', 'deductive'), 0.1)
                all_rules.append(rule)
    
    synthesis_rationale.append({
        'step': 'collection',
        'reasoning': f'Collected {len(all_rules)} rules from {len(pathway_results)} pathways using {len(reasoning_modes)} reasoning modes',
        'pathway_distribution': {result.get('extraction_focus', 'unknown'): len(result.get('extracted_rules', [])) for result in pathway_results},
        'reasoning_modes_used': reasoning_modes
    })
    
    if synthesis_strategy == "weighted":
        for rule in all_rules:
            combined_weight = rule.get('confidence', 0) * rule.get('pathway_weight', 0.1) * rule.get('reasoning_mode_weight', 0.1)
            if combined_weight > 0.3:  # Threshold for inclusion
                rule['combined_confidence'] = combined_weight
                synthesized_rules.append(rule)
        
        synthesized_rules.sort(key=lambda r: r.get('combined_confidence', 0), reverse=True)
        
        synthesis_rationale.append({
            'step': 'weighted_selection',
            'reasoning': f'Applied weighted selection combining pathway and reasoning mode weights, retained {len(synthesized_rules)} high-quality rules',
            'strategy': 'weighted'
        })
    
    elif synthesis_strategy == "consensus":
        rule_signatures = {}
        for rule in all_rules:
            signature = f"{rule.get('type', 'unknown')}_{rule.get('subject', rule.get('content', ''))[:50]}"
            if signature not in rule_signatures:
                rule_signatures[signature] = []
            rule_signatures[signature].append(rule)
        
        for signature, rules in rule_signatures.items():
            if len(rules) > 1 or (len(rules) == 1 and rules[0].get('confidence', 0) > 0.8):
                best_rule = max(rules, key=lambda r: r.get('confidence', 0))
                best_rule['synthesis_support'] = len(rules)
                best_rule['synthesis_confidence'] = min(1.0, best_rule.get('confidence', 0) * len(rules) * 0.3)
                synthesized_rules.append(best_rule)
    
    synthesis_rationale.append({
        'step': 'quality_assessment',
        'reasoning': f'Final synthesis produced {len(synthesized_rules)} high-quality rules using {synthesis_strategy} strategy',
        'quality_metrics': {
            'total_rules': len(synthesized_rules),
            'avg_confidence': sum([r.get('confidence', 0) for r in synthesized_rules]) / len(synthesized_rules) if synthesized_rules else 0,
            'reasoning_mode_diversity': len(set([r.get('reasoning_mode', 'unknown') for r in synthesized_rules]))
        }
    })
    
    return {
        'synthesized_rules': synthesized_rules,
        'synthesis_strategy': synthesis_strategy,
        'synthesis_rationale': synthesis_rationale,
        'reasoning_modes': reasoning_modes,
        'original_rule_count': len(all_rules),
        'final_rule_count': len(synthesized_rules),
        'synthesis_quality': 'high' if len(synthesized_rules) > 0 else 'low'
    }


@tool(args_schema=JSONConversionInput)
def convert_to_json_rules(synthesized_rules: List[Dict[str, Any]], conversion_standard: str = "json-rules-engine") -> List[Dict[str, Any]]:
    """
    Convert synthesized rules into json-rules-engine compatible format.
    """
    json_rules = []
    
    for i, rule in enumerate(synthesized_rules):
        rule_id = f"legislation_rule_{i+1}"
        rule_type = rule.get('type', 'unknown')
        
        if rule_type == 'requirement':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "compliance_required",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "requirement_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "obligation": rule.get('obligation', rule.get('content', 'unknown')),
                        "mandatory": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 80
            }
        
        elif rule_type == 'prohibition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "subject_type",
                            "operator": "equal",
                            "value": rule.get('subject', 'entity').lower().split()[0]
                        },
                        {
                            "fact": "prohibited_action_attempted",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "prohibition_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "subject": rule.get('subject', 'unknown'),
                        "prohibited_action": rule.get('prohibited_action', rule.get('content', 'unknown')),
                        "violation_level": "high",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 90
            }
        
        elif rule_type == 'condition':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "trigger_condition",
                            "operator": "contains",
                            "value": rule.get('trigger', 'condition').lower()[:20]
                        },
                        {
                            "fact": "condition_met",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "conditional_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "trigger": rule.get('trigger', 'unknown'),
                        "consequence": rule.get('consequence', rule.get('content', 'unknown')),
                        "conditional": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 70
            }
        
        elif rule_type == 'exception':
            json_rule = {
                "conditions": {
                    "any": [
                        {
                            "fact": "exception_condition",
                            "operator": "contains",
                            "value": rule.get('exception_condition', 'exception').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "exception_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "exception_condition": rule.get('exception_condition', 'unknown'),
                        "modified_rule": rule.get('modified_rule', rule.get('content', 'unknown')),
                        "overrides_default": True,
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 60
            }
        
        elif rule_type == 'penalty':
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "violation_detected",
                            "operator": "equal",
                            "value": True
                        },
                        {
                            "fact": "violation_type",
                            "operator": "contains",
                            "value": rule.get('violation', 'violation').lower()[:20]
                        }
                    ]
                },
                "event": {
                    "type": "penalty_applied",
                    "params": {
                        "rule_id": rule_id,
                        "violation": rule.get('violation', 'unknown'),
                        "sanction": rule.get('sanction', rule.get('content', 'unknown')),
                        "severity": "medium",
                        "confidence": rule.get('confidence', 0.5),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 50
            }
        
        else:
            json_rule = {
                "conditions": {
                    "all": [
                        {
                            "fact": "rule_applicable",
                            "operator": "equal",
                            "value": True
                        }
                    ]
                },
                "event": {
                    "type": "generic_rule_triggered",
                    "params": {
                        "rule_id": rule_id,
                        "content": rule.get('content', 'unknown rule'),
                        "rule_type": rule_type,
                        "confidence": rule.get('confidence', 0.3),
                        "source_pathway": rule.get('source_pathway', 'unknown'),
                        "reasoning_mode": rule.get('reasoning_mode', 'unknown')
                    }
                },
                "priority": 40
            }
        
        json_rule["metadata"] = {
            "source": "legislation_analysis",
            "extraction_method": "mixture_of_thought_chains_reasoning",
            "reasoning_pathway": rule.get('source_pathway', 'unknown'),
            "reasoning_mode": rule.get('reasoning_mode', 'unknown'),
            "synthesis_strategy": "advanced",
            "created_by": "o3_mini_agent"
        }
        
        json_rules.append(json_rule)
    
    return json_rules


@tool(args_schema=ValidationInput)
def validate_json_rules(json_rules: List[Dict[str, Any]], validation_depth: str = "comprehensive") -> Dict[str, Any]:
    """
    Validate JSON rules for json-rules-engine compatibility.
    """
    validation_report = {
        "valid": True,
        "validation_depth": validation_depth,
        "total_rules": len(json_rules),
        "errors": [],
        "warnings": [],
        "rule_analysis": [],
        "quality_score": 0.0
    }
    
    required_fields = ["conditions", "event"]
    valid_operators = [
        "equal", "notEqual", "lessThan", "lessThanInclusive", 
        "greaterThan", "greaterThanInclusive", "in", "notIn", 
        "contains", "doesNotContain", "regex"
    ]
    
    quality_points = 0
    max_possible_points = len(json_rules) * 10
    
    for i, rule in enumerate(json_rules):
        rule_errors = []
        rule_warnings = []
        rule_quality = 0
        
        for field in required_fields:
            if field not in rule:
                rule_errors.append(f"Missing required field: {field}")
            else:
                rule_quality += 2
        
        if "conditions" in rule:
            conditions = rule["conditions"]
            if isinstance(conditions, dict):
                logical_ops = ["all", "any", "not"]
                if any(op in conditions for op in logical_ops):
                    rule_quality += 2
                else:
                    rule_errors.append("Conditions must contain 'all', 'any', or 'not' operator")
            else:
                rule_errors.append("Conditions must be an object")
        
        if "event" in rule:
            event = rule["event"]
            if isinstance(event, dict):
                rule_quality += 1
                if "type" in event:
                    rule_quality += 1
                if "params" in event and isinstance(event["params"], dict):
                    rule_quality += 1
            else:
                rule_errors.append("Event must be an object")
        
        if "priority" in rule:
            priority = rule["priority"]
            if isinstance(priority, (int, float)) and 0 <= priority <= 100:
                rule_quality += 1
            else:
                rule_warnings.append("Priority should be a number between 0 and 100")
        
        if "metadata" in rule and isinstance(rule["metadata"], dict):
            rule_quality += 1
        
        rule_analysis = {
            "rule_index": i,
            "valid": len(rule_errors) == 0,
            "quality_score": min(10.0, rule_quality),
            "errors": rule_errors,
            "warnings": rule_warnings
        }
        
        validation_report["rule_analysis"].append(rule_analysis)
        validation_report["errors"].extend(rule_errors)
        validation_report["warnings"].extend(rule_warnings)
        
        quality_points += rule_quality
        
        if rule_errors:
            validation_report["valid"] = False
    
    if max_possible_points > 0:
        validation_report["quality_score"] = min(100.0, (quality_points / max_possible_points) * 100)
    
    return validation_report


# Advanced o3-mini Agent

def create_advanced_o3_mini_agent():
    """
    Create LangGraph agent using o3-mini with advanced reasoning strategies:
    - Mixture of Thought: Multiple reasoning pathways
    - Mixture of Chains: Different processing chains 
    - Chain of Thought: Step-by-step reasoning
    - Mixture of Reasoning: Different reasoning modes
    """
    
    model = ChatOpenAI(
        model="o3-mini",
        temperature=0,
        model_kwargs={
            "reasoning_effort": "medium"
        }
    )
    
    tools = [
        analyze_legislation_pathway,
        extract_rules_chain,
        synthesize_rules,
        convert_to_json_rules,
        validate_json_rules
    ]
    
    system_message = """You are an elite legal analyst and rules engineer with advanced reasoning capabilities. You specialize in converting legislation into precise machine-readable JSON rules using four cutting-edge AI reasoning techniques.

## ADVANCED REASONING FRAMEWORK:

### MIXTURE OF THOUGHT APPROACH:
Explore legislation through multiple reasoning pathways simultaneously:
1. **Structural Pathway**: Document hierarchy, organization, and relationships
2. **Semantic Pathway**: Meaning, definitions, intent, and scope
3. **Logical Pathway**: Rules, conditions, logical relationships, and consequences
4. **Contextual Pathway**: Actors, actions, temporal elements, and situational factors

### MIXTURE OF REASONING MODES:
Apply different reasoning modes within each pathway:
1. **Deductive Reasoning**: Top-down from general principles to specific cases
2. **Inductive Reasoning**: Bottom-up from specific patterns to general rules
3. **Abductive Reasoning**: Best explanation for incomplete or ambiguous information
4. **Analogical Reasoning**: Pattern matching and comparison with known structures

### MIXTURE OF CHAINS STRATEGY:
Process through specialized extraction chains:
- **Analysis Chain**: Multi-pathway legislation analysis
- **Extraction Chain**: Focused rule extraction by type
- **Synthesis Chain**: Cross-pathway and cross-mode integration
- **Conversion Chain**: JSON rules generation
- **Validation Chain**: Comprehensive quality assurance

### CHAIN OF THOUGHT EXECUTION:
Think step-by-step within each process:
1. Context assessment and reasoning mode selection
2. Pattern recognition and structure identification
3. Information extraction with confidence scoring
4. Quality evaluation and consistency checking
5. Integration planning and synthesis preparation

## PROCESSING METHODOLOGY:

**PHASE 1 - DIVERGENT ANALYSIS**
- Analyze legislation through all 4 reasoning pathways
- Apply all 4 reasoning modes within each pathway
- Generate 16 different analytical perspectives (4 pathways × 4 modes)

**PHASE 2 - SPECIALIZED EXTRACTION**
- Extract requirements, prohibitions, conditions, exceptions, penalties
- Use optimal reasoning mode for each extraction type
- Maintain pathway context and reasoning traces

**PHASE 3 - CONVERGENT SYNTHESIS**
- Synthesize findings from all pathways and reasoning modes
- Apply weighted or consensus synthesis strategies
- Resolve conflicts using reasoning mode confidence

**PHASE 4 - PRECISION CONVERSION**
- Convert to json-rules-engine compatible format
- Preserve reasoning traces and confidence scores
- Maintain pathway and mode attribution

**PHASE 5 - RIGOROUS VALIDATION**
- Comprehensive validation with quality scoring
- Multi-dimensional analysis and recommendations
- Final optimization and error correction

Use the provided tools systematically to achieve comprehensive, high-quality JSON rules that accurately represent the legislation's requirements."""
    
    memory = MemorySaver()
    
    agent = create_react_agent(
        model=model,
        tools=tools,
        checkpointer=memory,
        prompt=system_message
    )
    
    return agent


# Main Processing Functions

async def process_legislation_advanced(legislation_text: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Process legislation using advanced o3-mini reasoning framework.
    """
    
    agent = create_advanced_o3_mini_agent()
    
    if config is None:
        config = {"configurable": {"thread_id": "advanced_o3_mini_session"}}
    
    prompt = f"""Process the following legislation using the complete advanced reasoning framework (Mixture of Thought + Mixture of Chains + Chain of Thought + Mixture of Reasoning):

LEGISLATION TEXT:
{legislation_text}

## PROCESSING INSTRUCTIONS:

**PHASE 1 - DIVERGENT ANALYSIS**
Analyze through multiple pathways and reasoning modes:

1. Structural analysis with deductive reasoning: `analyze_legislation_pathway` (pathway="structural", reasoning_mode="deductive")
2. Structural analysis with inductive reasoning: `analyze_legislation_pathway` (pathway="structural", reasoning_mode="inductive")
3. Semantic analysis with abductive reasoning: `analyze_legislation_pathway` (pathway="semantic", reasoning_mode="abductive")
4. Logical analysis with deductive reasoning: `analyze_legislation_pathway` (pathway="logical", reasoning_mode="deductive")
5. Contextual analysis with analogical reasoning: `analyze_legislation_pathway` (pathway="contextual", reasoning_mode="analogical")

**PHASE 2 - SPECIALIZED EXTRACTION**
Extract rules using optimal reasoning modes:

1. Requirements with deductive reasoning: `extract_rules_chain` (focus="requirements", reasoning_mode="deductive")
2. Prohibitions with deductive reasoning: `extract_rules_chain` (focus="prohibitions", reasoning_mode="deductive")
3. Conditions with abductive reasoning: `extract_rules_chain` (focus="conditions", reasoning_mode="abductive")
4. Exceptions with analogical reasoning: `extract_rules_chain` (focus="exceptions", reasoning_mode="analogical")
5. Penalties with inductive reasoning: `extract_rules_chain` (focus="penalties", reasoning_mode="inductive")

**PHASE 3 - CONVERGENT SYNTHESIS**
Use `synthesize_rules` with synthesis_strategy="weighted" and all reasoning_modes used.

**PHASE 4 - PRECISION CONVERSION**
Use `convert_to_json_rules` to generate json-rules-engine compatible rules.

**PHASE 5 - RIGOROUS VALIDATION**
Use `validate_json_rules` with validation_depth="comprehensive".

Explain your reasoning process, tool selections, and how different techniques contribute to the final high-quality result."""

    response = await agent.ainvoke(
        {"messages": [HumanMessage(content=prompt)]},
        config=config
    )
    
    return {
        "status": "completed",
        "model_used": "o3-mini",
        "reasoning_framework": "mixture_of_thought_chains_reasoning_cot",
        "original_text": legislation_text,
        "messages": response["messages"],
        "processing_phases": [
            "divergent_analysis",
            "specialized_extraction", 
            "convergent_synthesis",
            "precision_conversion",
            "rigorous_validation"
        ]
    }


def process_legislation_sync(legislation_text: str, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Synchronous wrapper for advanced o3-mini legislation processing.
    """
    
    async def _process_async():
        return await process_legislation_advanced(legislation_text, config)
    
    try:
        loop = asyncio.get_running_loop()
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, _process_async())
            return future.result()
    except RuntimeError:
        return asyncio.run(_process_async())


# Utility Functions

def save_rules_to_file(rules: List[Dict[str, Any]], filename: str):
    """Save JSON rules to file"""
    output_data = {
        "rules": rules,
        "metadata": {
            "created_by": "advanced_o3_mini_agent",
            "processing_framework": "mixture_of_thought_chains_reasoning_cot",
            "rule_count": len(rules),
            "engine_compatibility": "json-rules-engine"
        }
    }
    
    with open(filename, 'w') as f:
        json.dump(output_data, f, indent=2)
    print(f"💾 Rules saved to {filename}")


def extract_rules_from_result(result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Extract JSON rules from processing result"""
    rules = []
    
    # Look for rules in various result fields
    for field in ['json_rules', 'final_rules', 'synthesized_rules']:
        if field in result and result[field]:
            rules.extend(result[field])
    
    # Try to extract from messages if no direct rules found
    if not rules and 'messages' in result:
        for msg in result['messages']:
            if hasattr(msg, 'content'):
                content = str(msg.content)
                if '"conditions"' in content and '"event"' in content:
                    # Try to parse JSON from message content
                    try:
                        import json
                        # Simple extraction - would need more sophisticated parsing in practice
                        pass
                    except:
                        pass
    
    return rules


# Main Execution

if __name__ == "__main__":
    # Sample legislation text - paste your legislation here
    LEGISLATION_TEXT = """
    SECTION 1. DEFINITIONS
    
    For the purposes of this Act:
    (a) "Vehicle" means any motorized conveyance designed for transportation on public roads.
    (b) "Driver" means any person operating a vehicle on a public road.
    (c) "Speed limit" means the maximum lawful speed for vehicles on a particular road segment.
    
    SECTION 2. SPEED REGULATIONS
    
    (a) No person shall operate a vehicle at a speed greater than the posted speed limit.
    (b) If weather conditions are hazardous, drivers must reduce speed to maintain safe operation.
    (c) Emergency vehicles may exceed speed limits when responding to emergencies, provided warning signals are activated.
    
    SECTION 3. PENALTIES
    
    (a) Any person who violates Section 2(a) shall be subject to a fine of not less than $100 and not more than $500.
    (b) Repeat violations within 12 months shall result in license suspension for 30 days.
    """
    
    print("🚀 Advanced o3-mini Legislation Processor")
    print("🧠 Framework: Mixture of Thought + Chains + Reasoning + CoT")
    print("📄 Processing legislation...")
    
    try:
        # Process the legislation
        result = process_legislation_sync(LEGISLATION_TEXT)
        
        print(f"\n✅ Processing completed!")
        print(f"📊 Status: {result['status']}")
        print(f"🧠 Model: {result['model_used']}")
        print(f"⚙️ Framework: {result['reasoning_framework']}")
        print(f"💬 Messages: {len(result['messages'])}")
        
        # Extract and save rules
        rules = extract_rules_from_result(result)
        if rules:
            save_rules_to_file(rules, "legislation_rules.json")
            print(f"📋 Generated {len(rules)} JSON rules")
        else:
            print("⚠️ No rules extracted - check processing result")
        
        # Show sample output
        if result['messages']:
            last_msg = result['messages'][-1]
            if hasattr(last_msg, 'content'):
                preview = last_msg.content[:400] + "..." if len(last_msg.content) > 400 else last_msg.content
                print(f"\n🔍 Sample output:\n{preview}")
        
    except Exception as e:
        print(f"❌ Processing failed: {e}")
        print("🔧 Check o3-mini API access and dependencies")
