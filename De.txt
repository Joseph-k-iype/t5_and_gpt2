"""
PBT Data Manager for loading, managing, and searching PBT data.
"""

import os
import logging
import uuid
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Set
from sklearn.metrics.pairwise import cosine_similarity
from langchain_core.documents import Document
from app.config.settings import get_settings
from app.core.services.embeddings import get_embedding_service
from app.core.vector_store.chroma_store import get_chroma_store
from app.core.models.pbt import PBT, MatchedPBT, MatchType, PBTStatistics
from app.core.auth.auth_helper import refresh_token_if_needed, get_azure_token_cached

logger = logging.getLogger(__name__)

class PBTManager:
    """Manager for Preferred Business Terms (PBT) data."""
    
    _instance = None
    
    def __new__(cls):
        """Implement singleton pattern."""
        if cls._instance is None:
            cls._instance = super(PBTManager, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """Initialize the PBT manager."""
        if self._initialized:
            return
            
        self._initialized = True
        self.settings = get_settings()
        self.embedding_service = get_embedding_service()
        self.vector_store = get_chroma_store()
        
        # PBT data
        self.pbt_data = []
        self.concept_hierarchy = {}
        
        logger.info("PBT Manager initialized")
    
    async def load_csv(self, csv_path: Optional[str] = None, reload: bool = False) -> Dict[str, Any]:
        """
        Load PBT data from CSV file and store in vector store with synonyms.
        
        Args:
            csv_path: Path to the CSV file
            reload: Whether to reload the data if already loaded
            
        Returns:
            Dict with load status and count
        """
        try:
            # Use provided path or default from settings
            file_path = csv_path or self.settings.pbt_csv_path
            
            # Check if data is already loaded and reload flag is not set
            if not reload and len(self.pbt_data) > 0:
                logger.info("PBT data already loaded. Use reload=True to force reload.")
                return {
                    "status": "success", 
                    "message": "PBT data already loaded", 
                    "total_loaded": len(self.pbt_data)
                }
            
            # Ensure the file exists
            if not os.path.exists(file_path):
                logger.error(f"CSV file not found: {file_path}")
                return {"status": "error", "message": f"CSV file not found: {file_path}", "total_loaded": 0}
            
            # Load the CSV file
            try:
                df = pd.read_csv(file_path)
                logger.info(f"Successfully loaded CSV with {len(df)} rows and columns: {list(df.columns)}")
            except Exception as e:
                logger.error(f"Failed to load CSV: {e}")
                return {"status": "error", "message": f"Failed to load CSV: {str(e)}", "total_loaded": 0}
            
            # Check for required columns
            required_columns = {'id', 'PBT_NAME', 'PBT_DEFINITION'}
            if not required_columns.issubset(df.columns):
                missing = required_columns - set(df.columns)
                logger.error(f"CSV file missing required columns: {missing}")
                return {
                    "status": "error", 
                    "message": f"CSV file missing required columns: {missing}", 
                    "total_loaded": 0
                }
            
            # Add CDM column if not already present
            if 'CDM' not in df.columns:
                logger.warning("CSV file does not contain CDM column. Adding empty CDM values.")
                df['CDM'] = None
            
            # Convert DataFrame to list of dictionaries
            self.pbt_data = df.to_dict('records')
            logger.info(f"Converted {len(self.pbt_data)} rows to dictionary records")
            
            # Delete all existing data in the vector store
            try:
                self.vector_store.delete()
                logger.info("Successfully deleted existing vector store data")
            except Exception as e:
                logger.error(f"Error deleting existing vector store data: {e}")
                # Continue with the process despite deletion errors
            
            # Create documents for embedding with synonyms
            all_docs = []
            all_ids = []
            
            # Process in smaller batches for synonym generation
            synonym_batch_size = 5  # Reduced batch size to avoid timeouts
            for i in range(0, len(self.pbt_data), synonym_batch_size):
                batch = self.pbt_data[i:i + synonym_batch_size]
                batch_docs = []
                batch_ids = []
                
                logger.info(f"Generating synonyms for batch {i//synonym_batch_size + 1} of {(len(self.pbt_data) - 1)//synonym_batch_size + 1}")
                
                # Refresh token before processing each batch
                settings = self.settings
                
                # Explicitly refresh token
                token_refreshed = refresh_token_if_needed(
                    tenant_id=settings.azure.tenant_id,
                    client_id=settings.azure.client_id,
                    client_secret=settings.azure.client_secret,
                    scope="https://cognitiveservices.azure.com/.default",
                    min_validity_seconds=1800  # 30 minutes
                )
                
                if not token_refreshed:
                    # Try to get a fresh token
                    token = get_azure_token_cached(
                        tenant_id=settings.azure.tenant_id, 
                        client_id=settings.azure.client_id,
                        client_secret=settings.azure.client_secret
                    )
                    if not token:
                        logger.error("Failed to refresh Azure token during PBT data loading")
                        raise ValueError("Failed to get Azure token")
                
                # Reinitialize embedding service with fresh token
                self.embedding_service = get_embedding_service(force_refresh=True)
                
                for item in batch:
                    try:
                        # Ensure id is a string
                        item_id = str(item['id'])
                        
                        # Generate synonyms for this term
                        synonyms = []
                        try:
                            synonyms = self.embedding_service.generate_synonyms(
                                term_name=item['PBT_NAME'],
                                term_definition=item['PBT_DEFINITION']
                            )
                            logger.debug(f"Generated {len(synonyms)} synonyms for term {item['PBT_NAME']}")
                        except Exception as syn_error:
                            logger.warning(f"Error generating synonyms for {item['PBT_NAME']}: {syn_error}")
                            # Continue without synonyms rather than failing the entire process
                        
                        # Convert synonyms to a comma-separated string
                        synonyms_str = ", ".join(synonyms) if synonyms else ""
                        
                        # Combined text including the term, definition, and CDM (if available)
                        combined_text = f"{item['PBT_NAME']} - {item['PBT_DEFINITION']}"
                        if 'CDM' in item and item['CDM']:
                            combined_text += f" - {item['CDM']}"
                        
                        # Add synonyms to the combined text to improve matching
                        if synonyms_str:
                            combined_text += f" - Synonyms: {synonyms_str}"
                        
                        # Create metadata
                        metadata = {
                            'id': item_id,
                            'name': item['PBT_NAME'],
                            'definition': item['PBT_DEFINITION'],
                            'cdm': item.get('CDM', ''),
                            'synonyms_str': synonyms_str,
                            'synonym_count': len(synonyms)
                        }
                        
                        # Create a document
                        doc = Document(
                            page_content=combined_text,
                            metadata=metadata
                        )
                        
                        batch_docs.append(doc)
                        batch_ids.append(item_id)
                    except Exception as item_error:
                        logger.error(f"Error processing item {item.get('id', 'unknown')}: {item_error}")
                        continue  # Skip this item but continue with others
                
                all_docs.extend(batch_docs)
                all_ids.extend(batch_ids)
            
            logger.info(f"Created {len(all_docs)} documents for vector storage")
            
            # Process documents in batches for embedding and storage
            chunk_size = 20  # Smaller chunk size for better handling
            successful_chunks = 0
            
            for i in range(0, len(all_docs), chunk_size):
                try:
                    # Refresh token before processing each batch
                    refresh_token_if_needed(
                        tenant_id=settings.azure.tenant_id,
                        client_id=settings.azure.client_id,
                        client_secret=settings.azure.client_secret,
                        min_validity_seconds=1800  # 30 minutes
                    )
                    
                    # Get chunk of documents and IDs
                    chunk_docs = all_docs[i:i + chunk_size]
                    chunk_ids = all_ids[i:i + chunk_size]
                    
                    if not chunk_docs:
                        logger.warning(f"Empty document chunk at index {i}")
                        continue
                    
                    # Add to vector store
                    self.vector_store.add_documents(chunk_docs, ids=chunk_ids)
                    successful_chunks += 1
                    logger.info(f"Added document chunk {i//chunk_size + 1} of {(len(all_docs) - 1)//chunk_size + 1} to ChromaDB")
                except Exception as chunk_error:
                    logger.error(f"Error adding chunk {i//chunk_size + 1} to ChromaDB: {chunk_error}")
                    # Continue with next chunk instead of failing entire process
            
            if successful_chunks == 0:
                logger.error("No chunks were successfully added to the vector store")
                return {"status": "error", "message": "Failed to add any documents to vector store", "total_loaded": 0}
            
            # Build concept hierarchy
            try:
                await self._build_concept_hierarchy()
                logger.info("Successfully built concept hierarchy")
            except Exception as hierarchy_error:
                logger.error(f"Error building concept hierarchy: {hierarchy_error}")
                # Continue without concept hierarchy rather than failing entire process
            
            successful_docs = successful_chunks * chunk_size
            logger.info(f"Loaded {len(self.pbt_data)} PBT records with synonyms from CSV into vector store")
            
            return {
                "status": "success", 
                "message": f"Loaded {len(self.pbt_data)} PBT records, indexed approximately {successful_docs}", 
                "total_loaded": len(self.pbt_data)
            }
        
        except Exception as e:
            logger.error(f"Error loading CSV: {e}")
            return {"status": "error", "message": f"Error loading CSV: {str(e)}", "total_loaded": 0}
    
    async def _build_concept_hierarchy(self):
        """
        Build a concept hierarchy based on the similarity between terms.
        This identifies broader and more specific terms.
        """
        try:
            # Get all documents with embeddings
            all_docs = []
            try:
                all_docs = self.vector_store.similarity_search("", k=min(len(self.pbt_data) + 1, 1000))
                logger.info(f"Retrieved {len(all_docs)} documents for concept hierarchy building")
            except Exception as search_error:
                logger.error(f"Error retrieving documents for concept hierarchy: {search_error}")
                return  # Exit if we can't get documents
            
            if not all_docs:
                logger.warning("No documents retrieved for concept hierarchy building")
                return
            
            # Prepare for similarity calculation
            embeddings = []
            doc_ids = []
            doc_texts = []
            
            for doc in all_docs:
                doc_id = doc.metadata.get('id')
                if doc_id:
                    doc_ids.append(doc_id)
                    doc_texts.append(doc.page_content)
                    
                    # Create a dummy document to get the embedding
                    query_text = doc.page_content
                    try:
                        doc_embedding = self.embedding_service.generate_embedding(
                            self.embedding_service.Document(id=doc_id, text=query_text)
                        ).embedding
                        
                        if not doc_embedding:
                            logger.warning(f"Empty embedding for document {doc_id}")
                            continue
                            
                        embeddings.append(doc_embedding)
                    except Exception as embed_error:
                        logger.error(f"Error generating embedding for document {doc_id}: {embed_error}")
                        continue
            
            if not embeddings:
                logger.warning("No embeddings generated for concept hierarchy")
                return
                
            # Calculate similarity matrix
            similarity_matrix = np.zeros((len(embeddings), len(embeddings)))
            
            for i in range(len(embeddings)):
                for j in range(len(embeddings)):
                    if i != j:  # Skip self-comparison
                        try:
                            similarity_matrix[i, j] = cosine_similarity(
                                [embeddings[i]], 
                                [embeddings[j]]
                            )[0][0]
                        except Exception as sim_error:
                            logger.error(f"Error calculating similarity for docs {i} and {j}: {sim_error}")
                            similarity_matrix[i, j] = 0.0
            
            # Calculate average similarities (higher means more general)
            avg_similarities = np.mean(similarity_matrix, axis=1)
            
            # Calculate term specificity factors
            term_generality = {}
            term_lengths = []
            
            for i, doc_id in enumerate(doc_ids):
                # Find the corresponding PBT data item
                item = next((item for item in self.pbt_data if str(item['id']) == doc_id), None)
                if item:
                    # Calculate term specificity based on term length and average similarity
                    name_length = len(item['PBT_NAME'].split())
                    term_lengths.append(name_length)
                    
                    term_generality[doc_id] = {
                        'avg_similarity': float(avg_similarities[i]),
                        'name_length': name_length,
                        'id': item['id'],
                        'name': item['PBT_NAME'],
                        'cdm': item.get('CDM', '')
                    }
            
            # Normalize term lengths
            max_length = max(term_lengths) if term_lengths else 1
            for term_id, data in term_generality.items():
                # Higher score means more general
                data['generality_score'] = data['avg_similarity'] * (1 - (data['name_length'] / max_length))
            
            # Sort terms by generality score
            sorted_terms = sorted(
                term_generality.items(), 
                key=lambda x: x[1]['generality_score'], 
                reverse=True
            )
            
            # Take the top 20% as broader terms
            broader_terms_count = max(1, int(len(sorted_terms) * 0.2))
            broader_terms = [term[0] for term in sorted_terms[:broader_terms_count]]
            
            # Store in concept hierarchy
            self.concept_hierarchy = {
                'broader_terms': [item for item in self.pbt_data 
                                if str(item['id']) in broader_terms],
                'term_generality': term_generality
            }
            
            logger.info(f"Built concept hierarchy with {len(broader_terms)} broader terms")
        
        except Exception as e:
            logger.error(f"Error building concept hierarchy: {e}")
            self.concept_hierarchy = {'broader_terms': [], 'term_generality': {}}
    
    async def find_similar_items(self, query_text: str, top_n: int = 5, include_broader_terms: bool = True,
                                similarity_threshold: float = 0.3) -> List[MatchedPBT]:
        """
        Find the most similar PBT items to the query text.
        
        Args:
            query_text: Text to search for
            top_n: Number of results to return
            include_broader_terms: Whether to include broader terms
            similarity_threshold: Minimum similarity threshold (lower = more results)
            
        Returns:
            List of matched PBT items
        """
        try:
            logger.info(f"Finding similar items for: '{query_text}' (top_n={top_n}, include_broader={include_broader_terms})")
            
            # If vector store is empty, return empty list
            try:
                stats = self.vector_store.get_collection_stats()
                doc_count = stats.get("document_count", 0)
                if isinstance(doc_count, str):
                    doc_count = 0
                
                if doc_count == 0:
                    logger.warning("Vector store is empty, no documents to search")
                    return []
            except Exception as stats_error:
                logger.error(f"Error getting vector store stats: {stats_error}")
                # Continue with search despite error
            
            # Try different approaches for search with fallbacks
            doc_score_pairs = []
            try:
                # Get more results initially to ensure we have enough after filtering
                doc_score_pairs = self.vector_store.similarity_search_with_score(
                    query_text, 
                    k=min(top_n * 3, 30)  # Get more results, but not too many
                )
                
                logger.info(f"Initial similarity search returned {len(doc_score_pairs)} results")
                
                if not doc_score_pairs and not query_text.strip():
                    # If empty query and no results, try to get any documents
                    doc_score_pairs = self.vector_store.similarity_search_with_score(
                        "term", 
                        k=min(top_n * 3, 30)
                    )
                    logger.info(f"Fallback search with 'term' returned {len(doc_score_pairs)} results")
            except Exception as search_error:
                logger.error(f"Error in similarity search: {search_error}")
                # Try a more basic search approach as fallback
                try:
                    docs = self.vector_store.similarity_search(
                        query_text,
                        k=min(top_n * 3, 30)
                    )
                    # Create artificial scores for these docs
                    doc_score_pairs = [(doc, 0.5) for doc in docs]
                    logger.info(f"Fallback basic search returned {len(doc_score_pairs)} results")
                except Exception as fallback_error:
                    logger.error(f"Error in fallback search: {fallback_error}")
                    return []  # Return empty if all searches fail
            
            # Process results to check for synonym matches
            exact_matches = []
            
            for doc, score in doc_score_pairs:
                doc_id = doc.metadata.get('id')
                synonyms_str = doc.metadata.get('synonyms_str', "")
                
                # Skip very low similarity scores
                if score < similarity_threshold:
                    continue
                
                # Split synonyms string into a list
                synonyms = [s.strip() for s in synonyms_str.split(',')] if synonyms_str else []
                
                # Check for synonym matches
                query_terms = set(query_text.lower().split())
                
                synonym_match = False
                matched_synonym = None
                
                for synonym in synonyms:
                    if not synonym:
                        continue
                    synonym_terms = set(synonym.lower().split())
                    if query_terms.intersection(synonym_terms):
                        synonym_match = True
                        matched_synonym = synonym
                        break
                
                # Find the corresponding PBT data item
                item = next((dict(item) for item in self.pbt_data if str(item['id']) == doc_id), None)
                
                if not item:
                    logger.warning(f"Item with ID {doc_id} not found in PBT data")
                    # Create a synthetic item from metadata if not found in PBT data
                    item = {
                        'id': doc_id,
                        'PBT_NAME': doc.metadata.get('name', 'Unknown'),
                        'PBT_DEFINITION': doc.metadata.get('definition', 'No definition available'),
                        'CDM': doc.metadata.get('cdm', None)
                    }
                
                # Make sure required fields are present and are valid strings
                if 'PBT_NAME' not in item or not isinstance(item['PBT_NAME'], str):
                    logger.warning(f"Invalid PBT_NAME in item {doc_id}: {item.get('PBT_NAME')}")
                    continue
                    
                if 'PBT_DEFINITION' not in item or not isinstance(item['PBT_DEFINITION'], str):
                    logger.warning(f"Invalid PBT_DEFINITION in item {doc_id}: {item.get('PBT_DEFINITION')}")
                    continue
                
                # Directly use original field names without conversion
                try:
                    matched_pbt = MatchedPBT(
                        id=str(item['id']),
                        PBT_NAME=item['PBT_NAME'],
                        PBT_DEFINITION=item['PBT_DEFINITION'],
                        CDM=item.get('CDM', None),
                        match_type=MatchType.SPECIFIC,
                        similarity_score=float(score),
                        synonym_match=synonym_match,
                        matched_synonym=matched_synonym
                    )
                    
                    # Boost score for synonym matches
                    if synonym_match:
                        matched_pbt.similarity_score *= 1.2  # 20% boost
                    
                    exact_matches.append(matched_pbt)
                except Exception as e:
                    logger.error(f"Error creating MatchedPBT for item {doc_id}: {e}")
                    continue
            
            # Sort by score and take top_n
            exact_matches.sort(key=lambda x: x.similarity_score, reverse=True)
            specific_results = exact_matches[:top_n]
            
            final_results = specific_results
            
            # If no results so far, try a more aggressive approach
            if not final_results:
                logger.warning(f"No specific matches found for '{query_text}', trying more aggressive search")
                # Try with lower similarity threshold
                return await self.find_similar_items(query_text, top_n, include_broader_terms, similarity_threshold=0.1)
            
            # Include broader terms if requested
            if include_broader_terms and self.concept_hierarchy.get('broader_terms'):
                broader_matches = []
                
                for broader_term in self.concept_hierarchy.get('broader_terms', []):
                    # Skip terms already in specific results
                    if any(r.id == str(broader_term['id']) for r in specific_results):
                        continue
                    
                    # Make sure required fields are present and are valid strings
                    if 'PBT_NAME' not in broader_term or not isinstance(broader_term['PBT_NAME'], str):
                        logger.warning(f"Invalid PBT_NAME in broader term {broader_term.get('id')}: {broader_term.get('PBT_NAME')}")
                        continue
                        
                    if 'PBT_DEFINITION' not in broader_term or not isinstance(broader_term['PBT_DEFINITION'], str):
                        logger.warning(f"Invalid PBT_DEFINITION in broader term {broader_term.get('id')}: {broader_term.get('PBT_DEFINITION')}")
                        continue
                    
                    # Create MatchedPBT object with original field names
                    try:
                        matched_pbt = MatchedPBT(
                            id=str(broader_term['id']),
                            PBT_NAME=broader_term['PBT_NAME'],
                            PBT_DEFINITION=broader_term['PBT_DEFINITION'],
                            CDM=broader_term.get('CDM'),
                            match_type=MatchType.BROADER,
                            similarity_score=0.5,  # Default score for broader terms
                            synonym_match=False
                        )
                        
                        broader_matches.append(matched_pbt)
                    except Exception as e:
                        logger.error(f"Error creating MatchedPBT for broader term {broader_term.get('id')}: {e}")
                        continue
                
                # Sort broader matches by name length (shorter names are usually more general)
                broader_matches.sort(key=lambda x: len(x.PBT_NAME.split()), reverse=False)
                
                # Take up to 3 broader matches
                broader_matches = broader_matches[:3]
                
                # Add broader matches to results
                final_results = specific_results + broader_matches
            
            logger.info(f"Returning {len(final_results)} matched items ({len(specific_results)} specific, {len(final_results) - len(specific_results)} broader)")
            return final_results
        
        except Exception as e:
            logger.error(f"Error finding similar items: {e}")
            return []
    
    async def get_pbt_by_id(self, pbt_id: str) -> Optional[PBT]:
        """
        Get a PBT by its ID.
        
        Args:
            pbt_id: PBT ID
            
        Returns:
            PBT if found, None otherwise
        """
        try:
            item = next((item for item in self.pbt_data if str(item['id']) == pbt_id), None)
            if item:
                # Get synonyms from vector store
                filter_dict = {"id": pbt_id}
                docs = self.vector_store.similarity_search("", k=1, filter=filter_dict)
                
                synonyms = []
                if docs:
                    synonyms_str = docs[0].metadata.get('synonyms_str', "")
                    synonyms = [s.strip() for s in synonyms_str.split(',')] if synonyms_str else []
                
                return PBT(
                    id=str(item['id']),
                    PBT_NAME=item['PBT_NAME'],
                    PBT_DEFINITION=item['PBT_DEFINITION'],
                    CDM=item.get('CDM'),
                    synonyms=synonyms
                )
            return None
        
        except Exception as e:
            logger.error(f"Error getting PBT by ID: {e}")
            return None
    
    async def get_statistics(self) -> PBTStatistics:
        """
        Get statistics about the PBT data.
        
        Returns:
            Statistics about the PBT data
        """
        try:
            total_count = len(self.pbt_data)
            
            # Get vector store stats
            vector_store_stats = self.vector_store.get_collection_stats()
            indexed_count = vector_store_stats.get("document_count", 0)
            if isinstance(indexed_count, str):
                try:
                    indexed_count = int(indexed_count)
                except (ValueError, TypeError):
                    indexed_count = total_count  # Fallback if we couldn't get accurate count
            
            # Count PBTs by CDM category
            cdm_categories = {}
            for item in self.pbt_data:
                cdm = item.get('CDM', '')
                if not cdm:
                    cdm = "Uncategorized"
                if cdm in cdm_categories:
                    cdm_categories[cdm] += 1
                else:
                    cdm_categories[cdm] = 1
            
            # Count PBTs with synonyms
            has_synonyms_count = 0
            total_synonyms = 0
            
            # Query the vector store for synonym counts
            for item in self.pbt_data:
                filter_dict = {"id": str(item['id'])}
                docs = self.vector_store.similarity_search("", k=1, filter=filter_dict)
                
                if docs:
                    synonym_count = docs[0].metadata.get('synonym_count', 0)
                    if synonym_count > 0:
                        has_synonyms_count += 1
                        total_synonyms += synonym_count
            
            # Calculate average synonyms per PBT
            avg_synonyms = total_synonyms / max(1, total_count)
            
            # Get top CDM categories
            top_cdm = sorted(
                [{"category": k, "count": v} for k, v in cdm_categories.items()],
                key=lambda x: x["count"],
                reverse=True
            )[:5]
            
            return PBTStatistics(
                total_pbt_count=total_count,
                indexed_count=indexed_count,
                cdm_categories=cdm_categories,
                has_synonyms_count=has_synonyms_count,
                average_synonyms_per_pbt=avg_synonyms,
                top_cdm_categories=top_cdm
            )
        
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return PBTStatistics(
                total_pbt_count=0,
                indexed_count=0,
                cdm_categories={},
                has_synonyms_count=0,
                average_synonyms_per_pbt=0,
                top_cdm_categories=[]
            )


# Get the PBT manager instance
def get_pbt_manager() -> PBTManager:
    """
    Get the PBT manager instance.
    
    Returns:
        PBTManager: PBT manager instance
    """
    return PBTManager()
