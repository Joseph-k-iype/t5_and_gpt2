#!/usr/bin/env python3
"""
Multi-Agent Legislation Rule Extraction System
Converts legislation into machine-readable rules and conditions using LangGraph
Enhanced with Chain of Thought, Mixture of Experts, and Rule Deduplication
"""

import os
import sys
import json
import csv
import asyncio
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Annotated
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

# Core dependencies
import pymupdf
import openai
import numpy as np
from pydantic import BaseModel, Field

# LangChain and LangGraph dependencies  
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_core.tools import BaseTool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Global Configuration
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = "o3-mini-2025-01-31"
EMBEDDING_MODEL = "text-embedding-3-large"

# Paths
INPUT_PDF_PATH = os.getenv("INPUT_PDF_PATH", "./input_pdfs/")
LEGISLATION_METADATA_PATH = os.getenv("LEGISLATION_METADATA_PATH", "./legislation_metadata.json")
GEOGRAPHY_PATH = os.getenv("GEOGRAPHY_PATH", "./geography.json")
OUTPUT_PATH = os.getenv("OUTPUT_PATH", "./output/")

# Ensure output directory exists
Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('legislation_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Validate OpenAI client initialization
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set, OpenAI client initialization will fail at runtime")
    openai_client = None
else:
    try:
        openai_client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}")
        openai_client = None

def _convert_messages_for_openai(messages: List[BaseMessage]) -> List[Dict[str, str]]:
    """Convert LangChain messages to OpenAI format"""
    openai_messages = []
    for msg in messages:
        if msg.type == "human":
            role = "user"
        elif msg.type == "ai":
            role = "assistant"
        elif msg.type == "system":
            role = "system"
        else:
            logger.warning(f"Unknown message type: {msg.type}, defaulting to 'user'")
            role = "user"  # Default fallback
        
        openai_messages.append({
            "role": role,
            "content": msg.content
        })
    
    logger.debug(f"Converted {len(messages)} messages for OpenAI API")
    return openai_messages

class RoleType(Enum):
    CONTROLLER = "Controller"
    PROCESSOR = "Processor" 
    JOINT_CONTROLLER = "Joint Controller"
    DATA_SUBJECT = "Data Subject"

class RuleCondition(BaseModel):
    """Individual rule condition with logical operators"""
    condition_text: str = Field(description="Clear, atomic condition statement")
    logical_operator: Optional[str] = Field(description="AND, OR, NOT operator", default=None)
    roles: List[RoleType] = Field(description="Applicable roles for this condition", default_factory=list)
    is_negation: bool = Field(description="Whether this is a negation (must not)", default=False)

class LegislationRule(BaseModel):
    """Complete legislation rule with conditions and metadata"""
    rule_id: str = Field(description="Unique identifier for the rule")
    rule_text: str = Field(description="Main rule statement")
    rule_definition: str = Field(description="Detailed rule definition", default="")
    applies_to_countries: List[str] = Field(description="List of country/region codes")
    roles: List[RoleType] = Field(description="Primary roles this rule applies to", default_factory=list)
    conditions: List[RuleCondition] = Field(description="List of conditions for this rule")
    condition_count: int = Field(description="Number of conditions")
    references: List[str] = Field(description="Legal references and citations")
    adequacy_countries: List[str] = Field(description="Countries with adequacy decisions mentioned", default_factory=list)
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    confidence_score: float = Field(default=0.0, description="Confidence in rule extraction")
    duplicate_of: Optional[str] = Field(default=None, description="ID of original rule if this is a duplicate")

class AgentState(BaseModel):
    """State object for the multi-agent workflow"""
    messages: List[BaseMessage] = Field(default_factory=list)
    documents: List[Document] = Field(default_factory=list)
    processed_text: str = Field(default="")
    legislation_content: str = Field(default="")
    supporting_content: str = Field(default="")
    segmented_content: List[Dict[str, Any]] = Field(default_factory=list)
    extracted_entities: List[Dict[str, Any]] = Field(default_factory=list)
    rules: List[LegislationRule] = Field(default_factory=list)
    deduplicated_rules: List[LegislationRule] = Field(default_factory=list)
    current_jurisdiction: str = Field(default="")
    geography_data: Dict[str, Any] = Field(default_factory=dict)
    adequacy_countries: List[str] = Field(default_factory=list)
    # Remove vector_store from state to avoid serialization issues
    vector_documents: List[Document] = Field(default_factory=list)
    next_agent: str = Field(default="document_processor")
    error_messages: List[str] = Field(default_factory=list)
    react_reasoning: List[Dict[str, str]] = Field(default_factory=list)

    class Config:
        arbitrary_types_allowed = True

class CustomEmbeddings(Embeddings):
    """Custom embeddings using OpenAI API directly"""
    
    def __init__(self):
        if not openai_client:
            raise ValueError("OpenAI client not initialized. Please check OPENAI_API_KEY.")
        self.client = openai_client
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text[:8000]  # Limit input size
            )
            embeddings.append(response.data[0].embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        response = self.client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text[:8000]  # Limit input size
        )
        return response.data[0].embedding

class GeographyManager:
    """Manages geography data and country/region mappings"""
    
    def __init__(self, geography_data: Dict[str, Any]):
        self.geography_data = geography_data
        self.country_lookup = self._build_country_lookup()
        self.region_lookup = self._build_region_lookup()
    
    def _build_country_lookup(self) -> Dict[str, Dict[str, Any]]:
        """Build lookup table for countries"""
        lookup = {}
        
        # Add countries from regional groupings
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    for country in continent_data.get("countries", []):
                        lookup[country["iso2"]] = {
                            "name": country["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": country["iso2"]
                        }
                    for territory in continent_data.get("territories", []):
                        lookup[territory["iso2"]] = {
                            "name": territory["name"],
                            "region": f"By_Continent.{continent}",
                            "iso2": territory["iso2"],
                            "dependency_of": territory["dependency_of"]
                        }
            else:
                # Regional groupings like EU, EEA, MENAT
                for country in region_data.get("countries", []):
                    lookup[country["iso2"]] = {
                        "name": country["name"],
                        "region": region_key,
                        "iso2": country["iso2"]
                    }
                for territory in region_data.get("territories", []):
                    lookup[territory["iso2"]] = {
                        "name": territory["name"],
                        "region": region_key,
                        "iso2": territory["iso2"],
                        "dependency_of": territory["dependency_of"]
                    }
        
        return lookup
    
    def _build_region_lookup(self) -> Dict[str, List[str]]:
        """Build lookup table for regions to countries"""
        lookup = {}
        
        for region_key, region_data in self.geography_data.items():
            if region_key == "By_Continent":
                for continent, continent_data in region_data.items():
                    country_codes = [c["iso2"] for c in continent_data.get("countries", [])]
                    territory_codes = [t["iso2"] for t in continent_data.get("territories", [])]
                    lookup[f"By_Continent.{continent}"] = country_codes + territory_codes
            else:
                country_codes = [c["iso2"] for c in region_data.get("countries", [])]
                territory_codes = [t["iso2"] for t in region_data.get("territories", [])]
                lookup[region_key] = country_codes + territory_codes
        
        return lookup
    
    def get_country_info(self, iso_code: str) -> Optional[Dict[str, Any]]:
        """Get country information by ISO code"""
        return self.country_lookup.get(iso_code)
    
    def get_region_countries(self, region: str) -> List[str]:
        """Get all countries in a region"""
        return self.region_lookup.get(region, [])
    
    def find_countries_by_name(self, name_pattern: str) -> List[str]:
        """Find countries by name pattern"""
        matches = []
        name_lower = name_pattern.lower()
        for iso_code, info in self.country_lookup.items():
            if name_lower in info["name"].lower():
                matches.append(iso_code)
        return matches

class LegislationProcessor:
    """Main processor for legislation documents"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        self.embeddings = CustomEmbeddings()
        self.geography_manager = None
        
    def load_geography_data(self) -> Dict[str, Any]:
        """Load geography data from JSON file"""
        try:
            with open(GEOGRAPHY_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.geography_manager = GeographyManager(data)
                return data
        except FileNotFoundError:
            logger.error(f"Geography file not found: {GEOGRAPHY_PATH}")
            return {}
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing geography JSON: {e}")
            return {}
    
    def load_legislation_metadata(self) -> List[Dict[str, Any]]:
        """Load legislation metadata from JSON file"""
        try:
            with open(LEGISLATION_METADATA_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Ensure we return a list of dictionaries
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    # If it's a single dict, wrap it in a list
                    return [data]
                else:
                    logger.error(f"Unexpected data format in legislation metadata: {type(data)}")
                    return []
        except FileNotFoundError:
            logger.error(f"Legislation metadata file not found: {LEGISLATION_METADATA_PATH}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing legislation metadata JSON: {e}")
            return []
    
    def extract_pdf_content(self, pdf_path: str) -> Tuple[str, List[Document], str, str]:
        """Extract content from PDF using PyMuPDF and separate legislation from supporting info"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            documents = []
            
            supporting_info_start = False
            legislation_text = ""
            supporting_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += page_text + "\n"
                
                # Check for "Supporting Information" section
                if "supporting information" in page_text.lower():
                    supporting_info_start = True
                
                if not supporting_info_start:
                    legislation_text += page_text + "\n"
                else:
                    supporting_text += page_text + "\n"
                
                # Create document for each page
                documents.append(Document(
                    page_content=page_text,
                    metadata={
                        "page_number": page_num + 1,
                        "source": pdf_path,
                        "is_supporting": supporting_info_start,
                        "content_type": "supporting" if supporting_info_start else "legislation"
                    }
                ))
            
            doc.close()
            logger.info(f"Extracted {len(documents)} pages from {pdf_path}")
            logger.info(f"Legislation text: {len(legislation_text)} chars, Supporting text: {len(supporting_text)} chars")
            return full_text, documents, legislation_text, supporting_text
            
        except Exception as e:
            logger.error(f"Error extracting PDF content from {pdf_path}: {e}")
            return "", [], "", ""
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """Split documents into smaller chunks"""
        return self.text_splitter.split_documents(documents)
    
    def create_vector_store(self, documents: List[Document]) -> InMemoryVectorStore:
        """Create vector store for semantic search"""
        vector_store = InMemoryVectorStore(self.embeddings)
        vector_store.add_documents(documents)
        return vector_store

class ReactDocumentProcessorAgent:
    """Enhanced React Agent for document processing with reasoning"""
    
    def __init__(self):
        self.processor = LegislationProcessor()
    
    async def process(self, state: AgentState) -> AgentState:
        """Process documents using React reasoning pattern"""
        logger.info("ReactDocumentProcessorAgent: Starting document processing")
        print("\nðŸ“‚ ReactDocumentProcessorAgent: Starting document processing...")
        
        # THOUGHT: Plan the document processing approach
        thought = """
        THOUGHT: I need to systematically process PDF documents to extract legislation rules.
        My approach will be:
        1. Load geography and metadata configurations
        2. Process each PDF file and separate legislation from supporting information
        3. Create document chunks for better processing
        4. Analyze content using multiple expert perspectives
        5. Extract adequacy countries from both legislation and supporting sections
        """
        state.react_reasoning.append({"step": "document_processing_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Load configurations
        print("\nðŸŽ¬ ACTION: Loading configurations...")
        state.geography_data = self.processor.load_geography_data()
        legislation_metadata = self.processor.load_legislation_metadata()
        
        # OBSERVATION: Assess what was loaded
        observation = f"""
        OBSERVATION: Successfully loaded configurations:
        - Geography data: {len(state.geography_data)} regions
        - Legislation metadata: {len(legislation_metadata)} files
        Available regions: {list(state.geography_data.keys())}
        """
        state.react_reasoning.append({"step": "document_processing_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        all_documents = []
        all_legislation_text = ""
        all_supporting_text = ""
        
        # ACTION: Process each PDF file
        print("\nðŸŽ¬ ACTION: Processing PDF files...")
        for i, item in enumerate(legislation_metadata):
            pdf_path = item.get("path", "")
            jurisdiction = item.get("jurisdiction", "")
            
            print(f"\n--- Processing item {i+1}/{len(legislation_metadata)} ---")
            print(f"ðŸ“ PDF Path: {pdf_path}")
            print(f"ðŸ›ï¸  Jurisdiction: {jurisdiction}")
            
            if not pdf_path or not os.path.exists(pdf_path):
                print("âŒ File not found, skipping")
                continue
            
            # Extract content with separation
            full_text, documents, legislation_text, supporting_text = self.processor.extract_pdf_content(pdf_path)
            
            # Add jurisdiction metadata
            for doc in documents:
                doc.metadata["jurisdiction"] = jurisdiction
            
            all_documents.extend(documents)
            all_legislation_text += legislation_text + "\n"
            all_supporting_text += supporting_text + "\n"
            state.current_jurisdiction = jurisdiction
            
            print(f"ðŸ“„ Extracted {len(documents)} pages")
            print(f"ðŸ“ Legislation: {len(legislation_text)} chars, Supporting: {len(supporting_text)} chars")
        
        # OBSERVATION: Document extraction results
        observation = f"""
        OBSERVATION: Document extraction completed:
        - Total documents: {len(all_documents)}
        - Legislation content: {len(all_legislation_text)} characters
        - Supporting content: {len(all_supporting_text)} characters
        - Current jurisdiction: {state.current_jurisdiction}
        """
        state.react_reasoning.append({"step": "extraction_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        # Store separated content
        state.legislation_content = all_legislation_text
        state.supporting_content = all_supporting_text
        
        # ACTION: Create document chunks
        print("\nðŸŽ¬ ACTION: Creating document chunks...")
        chunked_docs = self.processor.chunk_documents(all_documents)
        state.documents = chunked_docs
        state.vector_documents = chunked_docs  # Store for later vector operations
        
        # THOUGHT: Plan expert analysis
        thought = """
        THOUGHT: Now I need to analyze the content using multiple expert perspectives.
        I'll focus on:
        1. Legal structure and obligations
        2. Geographic scope and adequacy decisions
        3. Data protection roles and responsibilities
        4. Technical requirements and safeguards
        """
        state.react_reasoning.append({"step": "analysis_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Perform expert analysis
        print("\nðŸŽ¬ ACTION: Performing multi-expert analysis...")
        analysis_prompt = self._create_react_analysis_prompt(state.legislation_content, state.supporting_content, state.current_jurisdiction)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst with expertise in data protection law using React reasoning."),
            HumanMessage(content=analysis_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        state.processed_text = response.choices[0].message.content
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=state.processed_text))
        
        # ACTION: Extract adequacy countries from both sections
        print("\nðŸŽ¬ ACTION: Extracting adequacy countries...")
        state.adequacy_countries = await self._extract_adequacy_countries_comprehensive(
            state.legislation_content, state.supporting_content, state.geography_data
        )
        
        # OBSERVATION: Final analysis results
        observation = f"""
        OBSERVATION: Analysis completed successfully:
        - Expert analysis generated: {len(state.processed_text)} characters
        - Adequacy countries identified: {state.adequacy_countries}
        - Document chunks created: {len(chunked_docs)}
        - Ready for next phase: semantic segmentation
        """
        state.react_reasoning.append({"step": "final_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        state.next_agent = "segmentation"
        print("âœ… ReactDocumentProcessorAgent completed successfully")
        
        return state
    
    def _create_react_analysis_prompt(self, legislation_text: str, supporting_text: str, jurisdiction: str) -> str:
        """Create React-style analysis prompt"""
        return f"""
        Use React reasoning pattern (Thought-Action-Observation) to analyze this legal content systematically.

        THOUGHT: I need to analyze this legal document to understand its structure, obligations, and geographic scope.
        
        CONTENT TO ANALYZE:
        
        LEGISLATION SECTION ({len(legislation_text)} characters):
        {legislation_text[:3000]}...
        
        SUPPORTING INFORMATION SECTION ({len(supporting_text)} characters):
        {supporting_text[:2000]}...
        
        JURISDICTION: {jurisdiction}
        
        ACTION: Apply multiple expert perspectives systematically:
        
        1. LEGAL STRUCTURE EXPERT ANALYSIS:
        - Identify specific articles, sections, and legal provisions
        - Determine what articles are actually present (be precise - don't reference articles not in the text)
        - Extract core legal obligations and prohibitions
        - Identify rights and entitlements
        
        2. GEOGRAPHIC SCOPE EXPERT ANALYSIS:
        - Identify specific countries and regions mentioned
        - Look for adequacy decisions in both legislation and supporting sections
        - Determine territorial application and cross-border implications
        
        3. DATA PROTECTION ROLES EXPERT ANALYSIS:
        - Identify Controller, Processor, Joint Controller definitions and obligations
        - Extract Data Subject rights and procedures
        - Determine role-specific requirements
        
        4. TECHNICAL SAFEGUARDS EXPERT ANALYSIS:
        - Identify required technical and organizational measures
        - Extract data transfer mechanisms and requirements
        - Determine security and protection standards
        
        OBSERVATION: Provide structured findings for each expert analysis.
        
        THOUGHT: Now I need to synthesize these findings into actionable insights.
        
        ACTION: Create comprehensive synthesis focusing on:
        - Precise article references (only those actually present)
        - Clear distinction between legislation vs supporting information
        - Adequacy countries mentioned in either section
        - Role definitions and responsibilities
        - Data transfer, access, and entitlement rules
        
        OBSERVATION: Provide final structured analysis with confidence assessment.
        
        Be extremely precise about article numbers - only reference articles that are explicitly present in the provided text.
        """
    
    async def _extract_adequacy_countries_comprehensive(self, legislation_text: str, supporting_text: str, geography_data: Dict[str, Any]) -> List[str]:
        """Extract adequacy countries from both legislation and supporting sections"""
        
        extraction_prompt = f"""
        Use React reasoning to extract adequacy countries from both legislation and supporting information.
        
        THOUGHT: I need to find all countries mentioned as having adequacy decisions or special data transfer status.
        
        ACTION: Analyze both sections systematically:
        
        LEGISLATION SECTION:
        {legislation_text[:2000]}...
        
        SUPPORTING INFORMATION SECTION:
        {supporting_text[:2000]}...
        
        OBSERVATION: Look for mentions of:
        - Adequacy decisions
        - Adequate level of protection
        - Countries with special transfer status
        - Approved transfer mechanisms
        
        THOUGHT: Match found countries with available geography data.
        
        Available geography regions: {list(geography_data.keys()) if geography_data else []}
        
        ACTION: Return ONLY a JSON array of ISO2 country codes for countries with adequacy status.
        
        OBSERVATION: Provide precise country codes based on the text analysis.
        
        Example format: ["US", "CA", "JP", "GB"]
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Extract country codes using React reasoning. Return only valid JSON array."},
                {"role": "user", "content": extraction_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        # Extract JSON array from the text
        import re
        json_match = re.search(r'\[.*?\]', result_text)
        if json_match:
            result_text = json_match.group()
        
        try:
            adequacy_countries = json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse adequacy countries JSON, using fallback extraction")
            # Fallback: simple text analysis
            adequacy_countries = []
            combined_text = (legislation_text + " " + supporting_text).lower()
            
            # Look for common adequacy country patterns
            if "united states" in combined_text or "usa" in combined_text:
                adequacy_countries.append("US")
            if "canada" in combined_text:
                adequacy_countries.append("CA")
            if "japan" in combined_text:
                adequacy_countries.append("JP")
            if "united kingdom" in combined_text or "uk" in combined_text:
                adequacy_countries.append("GB")
        
        # Validate country codes if geography manager is available
        if hasattr(self.processor, 'geography_manager') and self.processor.geography_manager:
            validated_countries = []
            for country in adequacy_countries:
                if isinstance(country, str) and self.processor.geography_manager.get_country_info(country):
                    validated_countries.append(country)
            return validated_countries
        
        return [c for c in adequacy_countries if isinstance(c, str)]

class ReactIntelligentSegmentationAgent:
    """Enhanced React Agent for semantic segmentation"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Perform segmentation using React reasoning"""
        logger.info("ReactIntelligentSegmentationAgent: Starting segmentation")
        print("\nðŸ”„ ReactIntelligentSegmentationAgent: Starting segmentation...")
        
        # THOUGHT: Plan segmentation approach
        thought = """
        THOUGHT: I need to segment this legislation using analytical questions to break down complex legal language.
        My approach:
        1. Apply Why/What/When/Where/Who/How framework systematically
        2. Focus on data transfer, access rights, and entitlements
        3. Create structured segments for better rule extraction
        4. Use vector search to enhance segmentation with semantic context
        """
        state.react_reasoning.append({"step": "segmentation_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Perform analytical segmentation
        print("\nðŸŽ¬ ACTION: Performing analytical segmentation...")
        segmentation_prompt = self._create_react_segmentation_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal segmentation specialist using React reasoning and analytical questions."),
            HumanMessage(content=segmentation_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        segmentation_result = response.choices[0].message.content
        
        # OBSERVATION: Segmentation results
        observation = f"""
        OBSERVATION: Segmentation analysis completed:
        - Generated comprehensive analytical breakdown
        - Applied Why/What/When/Where/Who/How framework
        - Focused on data protection key concepts
        - Ready for structured segment creation
        """
        state.react_reasoning.append({"step": "segmentation_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        # ACTION: Create structured segments with vector search
        print("\nðŸŽ¬ ACTION: Creating structured segments with semantic context...")
        segments = await self._create_react_structured_segments(segmentation_result, state)
        
        state.segmented_content = segments
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=segmentation_result))
        
        # OBSERVATION: Final segmentation results
        observation = f"""
        OBSERVATION: Structured segmentation completed:
        - Created {len(segments)} analytical segments
        - Each segment enhanced with semantic context
        - Segments cover key data protection concepts
        - Ready for entity extraction phase
        """
        state.react_reasoning.append({"step": "final_segmentation_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        state.next_agent = "entity_extraction"
        logger.info(f"ReactIntelligentSegmentationAgent: Completed with {len(segments)} segments")
        print("âœ… ReactIntelligentSegmentationAgent completed successfully")
        
        return state
    
    def _create_react_segmentation_prompt(self, state: AgentState) -> str:
        """Create React-style segmentation prompt"""
        return f"""
        Use React reasoning to systematically segment this legislation using analytical questions.
        
        THOUGHT: I need to break down complex legal language into understandable segments using structured analytical questions.
        
        ACTION: Apply analytical framework systematically:
        
        LEGISLATION CONTENT TO SEGMENT:
        {state.legislation_content[:3000]}...
        
        SUPPORTING INFORMATION CONTEXT:
        {state.supporting_content[:1500]}...
        
        ADEQUACY COUNTRIES IDENTIFIED: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        
        ANALYTICAL QUESTIONS FRAMEWORK:
        
        1. WHY ANALYSIS:
        - What is the legislative purpose behind each provision?
        - What risks or problems are being addressed?
        - What policy objectives are being achieved?
        
        2. WHAT ANALYSIS:
        - What specific obligations are created?
        - What actions are required, prohibited, or permitted?
        - What rights and entitlements are established?
        - What are the precise article numbers mentioned?
        
        3. WHEN ANALYSIS:
        - Under what circumstances do rules apply?
        - What are the triggering conditions?
        - Are there temporal requirements or deadlines?
        - What are the exception conditions?
        
        4. WHERE ANALYSIS:
        - What geographic scope applies?
        - Which countries and jurisdictions are covered?
        - What cross-border implications exist?
        - Which adequacy countries are referenced?
        
        5. WHO ANALYSIS:
        - Which roles are involved (Controller, Processor, Joint Controller, Data Subject)?
        - What entities have obligations or rights?
        - Who has enforcement authority?
        - What role combinations are possible?
        
        6. HOW ANALYSIS:
        - What procedures must be followed?
        - What technical/organizational measures are required?
        - How is compliance demonstrated?
        - What are the implementation mechanisms?
        
        OBSERVATION: Document findings for each analytical dimension.
        
        THOUGHT: Focus especially on data transfer, access rights, and entitlement concepts.
        
        ACTION: Provide structured analysis organized by analytical questions, with specific attention to:
        - Data transfer requirements and restrictions
        - Access rights and procedures
        - Entitlement conditions and criteria
        - Role-specific obligations
        - Edge cases and negations
        
        OBSERVATION: Ensure precise references to actual article numbers present in the text.
        """
    
    async def _create_react_structured_segments(self, segmentation_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Create structured segments using React reasoning and vector search"""
        segments = []
        
        # Create vector store temporarily for this operation
        if state.vector_documents:
            vector_store = self.processor.create_vector_store(state.vector_documents) if hasattr(self, 'processor') else None
            
            # If no processor, create one
            if not hasattr(self, 'processor'):
                from legislation_processor import LegislationProcessor
                self.processor = LegislationProcessor()
                vector_store = self.processor.create_vector_store(state.vector_documents)
            
            # THOUGHT: Plan vector search queries
            thought = """
            THOUGHT: I need to use semantic search to find relevant content for each analytical dimension.
            This will help create more precise and contextual segments.
            """
            state.react_reasoning.append({"step": "vector_search_thought", "content": thought})
            
            # ACTION: Perform vector searches for key concepts
            analytical_queries = [
                "data transfer requirements cross-border international adequacy",
                "access rights data subject procedures requests", 
                "controller processor joint controller responsibilities duties",
                "consent legal basis legitimate interest conditions",
                "adequacy decisions transfer mechanisms safeguards standards",
                "compliance enforcement penalties supervisory authority powers",
                "entitlements rights obligations negations exceptions"
            ]
            
            for query in analytical_queries:
                if vector_store:
                    relevant_docs = vector_store.similarity_search(query, k=3)
                else:
                    # Fallback to using all documents
                    relevant_docs = state.vector_documents[:3]
                
                segments.append({
                    "analytical_focus": query.replace(" ", "_"),
                    "query_used": query,
                    "relevant_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "segmentation_analysis": segmentation_result,
                    "adequacy_countries_context": state.adequacy_countries
                })
            
            # OBSERVATION: Vector search results
            observation = f"""
            OBSERVATION: Vector search enhanced segmentation:
            - Performed {len(analytical_queries)} semantic queries
            - Found relevant content for each analytical dimension
            - Enhanced segments with contextual information
            - Maintained adequacy countries context
            """
            state.react_reasoning.append({"step": "vector_search_observation", "content": observation})
        
        return segments

class ReactComprehensiveEntityExtractionAgent:
    """Enhanced React Agent for entity extraction with resolution"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract and resolve entities using React reasoning"""
        logger.info("ReactComprehensiveEntityExtractionAgent: Starting entity extraction")
        print("\nðŸ” ReactComprehensiveEntityExtractionAgent: Starting entity extraction...")
        
        # THOUGHT: Plan entity extraction and resolution approach
        thought = """
        THOUGHT: I need to extract and resolve entities comprehensively using multiple expert perspectives.
        My approach:
        1. Apply mixture of experts for different entity types
        2. Perform semantic and hierarchical entity resolution
        3. Integrate geographic context with adequacy decisions
        4. Handle role combinations and edge cases
        5. Create canonical entity mappings
        """
        state.react_reasoning.append({"step": "entity_extraction_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Perform expert-based entity extraction
        print("\nðŸŽ¬ ACTION: Performing multi-expert entity extraction...")
        entity_prompt = self._create_react_entity_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are an expert entity extraction and resolution specialist using React reasoning."),
            HumanMessage(content=entity_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        extraction_result = response.choices[0].message.content
        
        # OBSERVATION: Extraction analysis
        observation = f"""
        OBSERVATION: Entity extraction analysis completed:
        - Applied multiple expert perspectives
        - Generated comprehensive entity categorization
        - Identified resolution requirements
        - Ready for structured entity processing
        """
        state.react_reasoning.append({"step": "entity_extraction_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        # ACTION: Structure entities with geography integration and resolution
        print("\nðŸŽ¬ ACTION: Structuring entities with geographic integration...")
        entities = await self._structure_entities_with_react_resolution(extraction_result, state)
        
        state.extracted_entities = entities
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=extraction_result))
        
        # OBSERVATION: Final entity extraction results
        observation = f"""
        OBSERVATION: Entity extraction and resolution completed:
        - Created {len(entities)} entity categories
        - Applied semantic and hierarchical resolution
        - Integrated geographic context and adequacy decisions
        - Resolved role combinations and relationships
        - Ready for rule component extraction
        """
        state.react_reasoning.append({"step": "final_entity_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        state.next_agent = "rule_extraction"
        logger.info(f"ReactComprehensiveEntityExtractionAgent: Completed with {len(entities)} entity categories")
        print("âœ… ReactComprehensiveEntityExtractionAgent completed successfully")
        
        return state
    
    def _create_react_entity_extraction_prompt(self, state: AgentState) -> str:
        """Create React-style entity extraction prompt with resolution"""
        geography_summary = self._create_geography_summary(state.geography_data)
        
        return f"""
        Use React reasoning with multiple expert perspectives to extract and resolve entities systematically.
        
        THOUGHT: I need to identify all entities and resolve them into canonical forms using expert knowledge.
        
        ACTION: Apply expert consultation framework:
        
        CONTENT FOR ENTITY EXTRACTION:
        
        LEGISLATION CONTENT:
        {state.legislation_content[:2000]}...
        
        SUPPORTING INFORMATION:
        {state.supporting_content[:1500]}...
        
        ADEQUACY COUNTRIES: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        
        EXPERT PERSPECTIVES:
        
        1. LEGAL ENTITIES EXPERT:
        THOUGHT: Identify all legal persons, roles, and entities.
        ACTION: Extract and categorize:
        - Controllers, Processors, Joint Controllers (with role combinations)
        - Data Subjects and their categories  
        - Supervisory authorities and enforcement bodies
        - Legal representatives and designated contacts
        OBSERVATION: Document role definitions and relationships.
        
        2. DATA CLASSIFICATION EXPERT:
        THOUGHT: Categorize all data-related entities and concepts.
        ACTION: Extract and classify:
        - Personal data types and special categories
        - Processing operations and purposes
        - Technical measures (pseudonymization, encryption, etc.)
        - Data protection principles and legal bases
        OBSERVATION: Create data type hierarchies and relationships.
        
        3. GEOGRAPHIC/JURISDICTIONAL EXPERT:
        THOUGHT: Map all geographic entities and adequacy relationships.
        ACTION: Extract and standardize:
        - Countries and regions with specific legal status
        - Adequacy jurisdictions and their implications
        - Cross-border transfer mechanisms and frameworks
        - Territorial scope and applicability rules
        OBSERVATION: Standardize to ISO codes and create regional mappings.
        
        4. CONDITIONAL/PROCEDURAL EXPERT:
        THOUGHT: Identify all conditions, procedures, and exceptions.
        ACTION: Extract and organize:
        - Legal bases (consent, legitimate interest, etc.)
        - Triggering conditions and circumstances
        - Exceptions, derogations, and edge cases
        - Procedural requirements and timelines
        OBSERVATION: Group related conditions and map exception hierarchies.
        
        ENTITY RESOLUTION REQUIREMENTS:
        
        SEMANTIC RESOLUTION:
        - Identify synonyms: "data controller" = "controller" = "person responsible for processing"
        - Resolve abbreviations: "DPA" = "Data Protection Authority"
        - Handle variations: "personal data" = "personal information"
        
        HIERARCHICAL RESOLUTION:
        - Map specific to general: "biometric data" â†’ "special category personal data" â†’ "personal data"
        - Create parent-child relationships between concepts
        - Establish role hierarchies and combinations
        
        GEOGRAPHIC RESOLUTION:
        - Standardize country names to ISO2 codes
        - Resolve regional groupings (EU â†’ member states)
        - Map adequacy relationships and implications
        
        CONTEXTUAL RESOLUTION:
        - Distinguish context-specific meanings
        - Handle multiple role assignments (Controller AND Processor)
        - Resolve temporal and conditional variations
        
        AVAILABLE GEOGRAPHY DATA:
        {geography_summary}
        
        THOUGHT: Now I need to synthesize all expert findings into structured entity categories.
        
        ACTION: Create comprehensive entity extraction with:
        - Clear categorization by expert domain
        - Resolution mappings for all identified entities
        - Geographic integration with adequacy context
        - Role relationship mappings
        - Confidence assessments for each entity type
        
        OBSERVATION: Provide structured entity analysis with resolution details.
        """
    
    def _create_geography_summary(self, geography_data: Dict[str, Any]) -> str:
        """Create a summary of available geography data"""
        summary = []
        for region, data in geography_data.items():
            if region == "By_Continent":
                summary.append(f"Continental groupings: {list(data.keys())}")
            else:
                country_count = len(data.get("countries", []))
                territory_count = len(data.get("territories", []))
                summary.append(f"{region}: {country_count} countries, {territory_count} territories")
        return "\n".join(summary)
    
    async def _structure_entities_with_react_resolution(self, extraction_result: str, state: AgentState) -> List[Dict[str, Any]]:
        """Structure entities with React reasoning and comprehensive resolution"""
        entities = []
        
        # THOUGHT: Plan entity structuring approach
        thought = """
        THOUGHT: I need to structure the extracted entities using semantic search and perform comprehensive resolution.
        This will create canonical entity mappings that support accurate rule extraction.
        """
        state.react_reasoning.append({"step": "entity_structuring_thought", "content": thought})
        
        # ACTION: Use vector search to enhance entity extraction
        if state.vector_documents:
            # Create temporary vector store
            processor = LegislationProcessor()
            vector_store = processor.create_vector_store(state.vector_documents)
            
            entity_queries = [
                "controller responsibilities obligations data protection duties",
                "processor requirements instructions data processing activities", 
                "data subject rights access rectification erasure portability",
                "cross-border transfer adequacy decisions safeguards mechanisms",
                "supervisory authority enforcement powers penalties investigations",
                "personal data categories special sensitive biometric health",
                "joint controller shared responsibilities decision making"
            ]
            
            for query in entity_queries:
                relevant_docs = vector_store.similarity_search(query, k=2)
                
                # Enhance with geography and adequacy context
                geographic_context = []
                if state.geography_data:
                    geo_manager = GeographyManager(state.geography_data)
                    # Look for country mentions in relevant docs
                    for doc in relevant_docs:
                        for iso_code in geo_manager.country_lookup.keys():
                            country_info = geo_manager.get_country_info(iso_code)
                            if country_info and country_info["name"].lower() in doc.page_content.lower():
                                geographic_context.append(country_info)
                
                # ACTION: Perform entity resolution using LLM
                resolved_entities = await self._perform_react_entity_resolution(
                    [doc.page_content for doc in relevant_docs], 
                    query, 
                    state
                )
                
                entities.append({
                    "entity_category": query.replace(" ", "_"),
                    "query_used": query,
                    "extracted_content": [doc.page_content for doc in relevant_docs],
                    "source_metadata": [doc.metadata for doc in relevant_docs],
                    "geographic_context": geographic_context,
                    "adequacy_countries_context": state.adequacy_countries,
                    "resolved_entities": resolved_entities,
                    "extraction_analysis": extraction_result
                })
        
        # OBSERVATION: Entity structuring results
        observation = f"""
        OBSERVATION: Entity structuring completed:
        - Created {len(entities)} entity categories with semantic enhancement
        - Applied comprehensive entity resolution for each category
        - Integrated geographic context and adequacy decisions
        - Mapped role relationships and hierarchies
        """
        state.react_reasoning.append({"step": "entity_structuring_observation", "content": observation})
        
        return entities
    
    async def _perform_react_entity_resolution(self, content_chunks: List[str], category: str, state: AgentState) -> Dict[str, Any]:
        """Perform entity resolution using React reasoning"""
        content_text = "\n".join(content_chunks[:2])  # Limit content for API
        
        resolution_prompt = f"""
        Use React reasoning to perform comprehensive entity resolution for category: {category}
        
        THOUGHT: I need to identify all entities in this content and resolve them into canonical forms.
        
        ACTION: Perform systematic entity resolution:
        
        CONTENT TO ANALYZE:
        {content_text}
        
        ADEQUACY COUNTRIES CONTEXT: {state.adequacy_countries}
        AVAILABLE GEOGRAPHY: {list(state.geography_data.keys()) if state.geography_data else []}
        
        RESOLUTION TASKS:
        1. Identify all entities mentioned in the content
        2. Resolve synonyms, abbreviations, and alternative names
        3. Create canonical entity names with clear definitions
        4. Map hierarchical relationships (parent/child/peer)
        5. Standardize geographic references to ISO codes
        6. Handle role combinations and multiple assignments
        7. Identify negations and edge cases
        
        OBSERVATION: Document all resolution mappings and relationships.
        
        THOUGHT: Create structured output that supports accurate rule extraction.
        
        ACTION: Return comprehensive resolution data in JSON format:
        {{
            "canonical_entities": [
                {{
                    "canonical_name": "standardized entity name",
                    "definition": "clear definition of the entity",
                    "aliases": ["alternative name 1", "abbreviation", "synonym"],
                    "entity_type": "controller/processor/data_type/country/condition/etc",
                    "hierarchy_level": "parent/child/peer",
                    "parent_entities": ["broader category entities"],
                    "child_entities": ["more specific entities"],
                    "related_entities": ["associated entities"],
                    "geographic_codes": ["ISO2 codes if applicable"],
                    "role_combinations": ["possible role combinations"],
                    "adequacy_status": "adequacy decision status if applicable",
                    "negation_forms": ["negative forms or exceptions"],
                    "confidence_score": 0.0
                }}
            ],
            "resolution_mappings": {{
                "original_term": "canonical_name",
                "abbreviation": "canonical_name"
            }},
            "role_relationships": {{
                "controller_processor": "relationship description",
                "joint_controller": "shared responsibility description"
            }},
            "geographic_mappings": {{
                "country_name": "ISO2_code",
                "region_name": ["list_of_country_codes"]
            }}
        }}
        
        OBSERVATION: Ensure all entities support multiple role assignments and handle edge cases.
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are an entity resolution specialist using React reasoning. Return only valid JSON."},
                {"role": "user", "content": resolution_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            result_text = result_text.split("```")[1]
        
        try:
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse entity resolution JSON, returning empty structure")
            return {
                "canonical_entities": [],
                "resolution_mappings": {},
                "role_relationships": {},
                "geographic_mappings": {}
            }

class ReactIntelligentRuleComponentExtractionAgent:
    """Enhanced React Agent for precise rule extraction with multiple roles and edge cases"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Extract rules using React reasoning with precision and role handling"""
        logger.info("ReactIntelligentRuleComponentExtractionAgent: Starting rule extraction")
        print("\nâš™ï¸ ReactIntelligentRuleComponentExtractionAgent: Starting rule extraction...")
        
        # THOUGHT: Plan comprehensive rule extraction approach
        thought = """
        THOUGHT: I need to extract rules with precision, handling multiple roles, edge cases, and negations.
        My approach:
        1. Convert complex legal language to atomic logical statements
        2. Handle multiple role assignments (Controller AND Processor)
        3. Extract precise article references from actual content
        4. Handle negations and edge cases (MUST NOT, exceptions)
        5. Integrate adequacy countries and geographic scope
        6. Create clear rule definitions and conditions
        """
        state.react_reasoning.append({"step": "rule_extraction_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Perform comprehensive rule extraction
        print("\nðŸŽ¬ ACTION: Performing comprehensive rule extraction...")
        rule_extraction_prompt = self._create_react_rule_extraction_prompt(state)
        
        messages = [
            SystemMessage(content="You are a legal rule extraction specialist using React reasoning with precision for roles and edge cases."),
            HumanMessage(content=rule_extraction_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        rule_text = response.choices[0].message.content
        
        # OBSERVATION: Rule extraction analysis
        observation = f"""
        OBSERVATION: Rule extraction analysis completed:
        - Generated comprehensive rule breakdown
        - Applied logical decomposition with role handling
        - Identified precise article references
        - Handled edge cases and negations
        - Ready for structured rule parsing
        """
        state.react_reasoning.append({"step": "rule_extraction_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        # ACTION: Parse and structure rules with enhanced handling
        print("\nðŸŽ¬ ACTION: Parsing and structuring rules with geographic integration...")
        rules = await self._parse_rules_with_react_precision(rule_text, state)
        
        state.rules = rules
        state.messages.extend(messages)
        state.messages.append(AIMessage(content=rule_text))
        
        # OBSERVATION: Final rule extraction results
        observation = f"""
        OBSERVATION: Rule extraction and structuring completed:
        - Extracted {len(rules)} precise rules
        - Handled multiple role assignments and combinations
        - Integrated adequacy countries and geographic scope
        - Processed edge cases and negations
        - Created atomic conditions with logical operators
        - Ready for deduplication phase
        """
        state.react_reasoning.append({"step": "final_rule_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        state.next_agent = "rule_deduplication"
        logger.info(f"ReactIntelligentRuleComponentExtractionAgent: Extracted {len(rules)} rules")
        print("âœ… ReactIntelligentRuleComponentExtractionAgent completed successfully")
        
        return state
    
    def _create_react_rule_extraction_prompt(self, state: AgentState) -> str:
        """Create comprehensive React-style rule extraction prompt"""
        available_regions = list(state.geography_data.keys()) if state.geography_data else []
        
        return f"""
        Use React reasoning with expert consultation to convert complex legal language into precise, atomic logical statements.
        
        THOUGHT: I need to extract rules with maximum precision, handling role combinations and edge cases systematically.
        
        CONTENT FOR RULE EXTRACTION:
        
        LEGISLATION CONTENT:
        {state.legislation_content[:3000]}...
        
        SUPPORTING INFORMATION (for context and adequacy countries):
        {state.supporting_content[:1500]}...
        
        EXTRACTED ENTITIES: {len(state.extracted_entities)} categories
        ADEQUACY COUNTRIES: {state.adequacy_countries}
        JURISDICTION: {state.current_jurisdiction}
        AVAILABLE REGIONS: {available_regions}
        
        ACTION: Apply expert consultation framework for rule types:
        
        1. RULE STRUCTURE EXPERT:
        THOUGHT: Identify different types of legal rules and their structure.
        ACTION: Categorize rules as:
        - Obligation rules (MUST do X)
        - Prohibition rules (MUST NOT do Y) 
        - Permission rules (MAY do Z under conditions)
        - Conditional rules (IF condition THEN consequence)
        - Exception rules (EXCEPT when, UNLESS)
        OBSERVATION: Document rule types and their logical structure.
        
        2. LOGICAL STRUCTURE EXPERT:
        THOUGHT: Break down complex statements into testable atomic components.
        ACTION: Create:
        - Atomic conditions (simple, testable statements)
        - Logical operators (AND, OR, NOT) with proper precedence
        - Clear subject-predicate-object structure
        - Unambiguous, implementable language
        - Negation handling (MUST NOT, SHALL NOT)
        OBSERVATION: Ensure each condition can be independently evaluated.
        
        3. ROLE ASSIGNMENT EXPERT:
        THOUGHT: Determine precise role assignments including combinations.
        ACTION: Assign roles with precision:
        - Controller: Makes decisions about processing purposes and means
        - Processor: Processes data on behalf of and under instructions from controller
        - Joint Controller: Shares decision-making responsibility with other controllers
        - Data Subject: Individual whose personal data is processed
        - Multiple role combinations: Controller AND Processor, Joint Controller combinations
        OBSERVATION: Handle cases where entities have multiple simultaneous roles.
        
        4. GEOGRAPHIC SCOPE EXPERT:
        THOUGHT: Determine precise territorial application using available data.
        ACTION: Map geographic scope using:
        - Available regions: {available_regions}
        - Adequacy countries: {state.adequacy_countries}
        - Current jurisdiction: {state.current_jurisdiction}
        - Cross-border transfer implications and restrictions
        - Regional grouping applications (EU, EEA, etc.)
        OBSERVATION: Ensure geographic scope matches actual legal applicability.
        
        SYSTEMATIC RULE DECOMPOSITION PROCESS:
        
        For each legal provision:
        
        Step 1: IDENTIFY CORE OBLIGATION/RIGHT/PROHIBITION
        THOUGHT: What is the fundamental requirement, entitlement, or restriction?
        ACTION: Extract the primary legal obligation or right.
        OBSERVATION: Document whether it's mandatory, prohibited, or permissive.
        
        Step 2: EXTRACT AND STRUCTURE CONDITIONS
        THOUGHT: What circumstances must exist for this rule to apply?
        ACTION: Create atomic conditions with logical operators:
        - AND: All conditions must be true simultaneously
        - OR: At least one condition must be true
        - NOT: Condition must be false (negation)
        - Nested conditions: (A AND B) OR (C AND NOT D)
        OBSERVATION: Ensure conditions are independently testable.
        
        Step 3: ASSIGN ROLES AND HANDLE COMBINATIONS
        THOUGHT: Which roles have obligations, rights, or restrictions?
        ACTION: Assign specific roles considering:
        - Primary role responsibility
        - Secondary role involvement
        - Joint role scenarios
        - Role combination requirements
        OBSERVATION: Document all applicable role assignments.
        
        Step 4: DETERMINE GEOGRAPHIC SCOPE WITH ADEQUACY
        THOUGHT: Where does this rule apply and what are the adequacy implications?
        ACTION: Map territorial application:
        - Base jurisdiction applicability
        - Cross-border transfer rules
        - Adequacy decision implications
        - Regional grouping effects
        OBSERVATION: Ensure geographic scope is legally accurate.
        
        Step 5: SIMPLIFY LANGUAGE AND HANDLE EDGE CASES
        THOUGHT: Make the rule implementable while preserving legal precision.
        ACTION: 
        - Remove legal jargon, replace with plain English
        - Make statements testable and implementable
        - Replace vague references ("this article", "such provision") with explicit references
        - Handle negations clearly (must not, shall not, except when)
        - Address edge cases and exceptions
        OBSERVATION: Ensure clarity without losing legal accuracy.
        
        PRIORITY EXTRACTION AREAS:
        
        1. DATA TRANSFER RULES:
        - Cross-border transfer requirements and restrictions
        - Adequacy decision applications and implications
        - Transfer mechanism requirements (SCCs, BCRs, derogations)
        - Third country transfer conditions
        
        2. ACCESS RIGHTS RULES:
        - Data subject access request procedures
        - Controller response obligations and timelines
        - Information provision requirements
        - Access limitation conditions and exceptions
        
        3. ENTITLEMENT RULES:
        - Legal basis requirements for processing
        - Consent mechanisms, conditions, and withdrawal
        - Legitimate interest balancing and assessments
        - Special category data processing conditions
        
        4. ROLE-SPECIFIC OBLIGATIONS:
        - Controller duties, responsibilities, and decision-making
        - Processor obligations, instructions, and limitations
        - Joint controller arrangements and shared responsibilities
        - Data subject rights and procedural entitlements
        
        RULE FORMULATION REQUIREMENTS:
        
        Each extracted rule must include:
        - Unique identifier (based on source article/section)
        - Clear, unambiguous rule text
        - Detailed rule definition explaining the obligation/right
        - Specific country/region application with adequacy context
        - Atomic conditions with logical operators and negation flags
        - Multiple role assignments where applicable
        - Precise legal references to actual articles present
        - Adequacy country implications
        - Confidence score based on text clarity
        
        EXAMPLE TRANSFORMATION:
        
        Complex Legal Text: "Controllers shall, where personal data are transferred to a third country or international organisation, ensure that the level of protection of natural persons afforded by this Regulation is not undermined."
        
        Extracted Rule:
        - Rule ID: "art_44_transfer_protection"
        - Rule Text: "Controllers must ensure adequate protection when transferring personal data internationally"
        - Rule Definition: "Data controllers have the obligation to maintain the level of data protection required by the regulation when transferring personal data to countries or organizations outside the regulatory jurisdiction"
        - Roles: [Controller]
        - Conditions:
          * "Data involves personal information" (AND, Controller, not negation)
          * "Transfer destination is outside regulatory jurisdiction" (AND, Controller, not negation)
          * "Adequate protection level exists OR appropriate safeguards are implemented" (OR, Controller, not negation)
        - Countries: [EU member states based on regulation scope]
        - Adequacy Countries: [Based on actual adequacy decisions mentioned]
        - References: ["Article 44"]
        
        THOUGHT: Now I need to systematically extract all rules using this precise methodology.
        
        ACTION: Perform comprehensive rule extraction focusing on precision, role combinations, and edge case handling.
        
        OBSERVATION: Ensure each rule is implementable, legally accurate, and handles all identified role and geographic combinations.
        
        Be extremely precise about article references - only mention articles that are explicitly present in the provided content.
        """
    
    async def _parse_rules_with_react_precision(self, rule_text: str, state: AgentState) -> List[LegislationRule]:
        """Parse LLM response into structured rules with React precision and enhanced handling"""
        
        # THOUGHT: Plan rule parsing approach
        thought = """
        THOUGHT: I need to convert the rule extraction into structured LegislationRule objects with enhanced precision.
        This requires careful handling of multiple roles, adequacy countries, and geographic scope.
        """
        state.react_reasoning.append({"step": "rule_parsing_thought", "content": thought})
        
        rules = []
        geo_manager = GeographyManager(state.geography_data) if state.geography_data else None
        
        # ACTION: Structure the extracted rules using LLM
        structure_prompt = f"""
        Use React reasoning to convert rule extraction into precise structured JSON format.
        
        THOUGHT: I need to create structured rule objects that preserve all extracted information with enhanced precision.
        
        ACTION: Convert the following rule extraction into structured JSON:
        
        {rule_text}
        
        CONTEXT FOR STRUCTURING:
        - Adequacy countries identified: {state.adequacy_countries}
        - Default jurisdiction: {state.current_jurisdiction}
        - Available regions: {list(state.geography_data.keys()) if state.geography_data else []}
        - Geography integration required for ISO2 codes
        
        OBSERVATION: Structure must include all role combinations and edge cases.
        
        THOUGHT: Create comprehensive rule objects with enhanced fields.
        
        ACTION: Return JSON array with this exact enhanced structure:
        [{{
            "rule_id": "unique_identifier_based_on_article",
            "rule_text": "clear, simple rule statement",
            "rule_definition": "detailed explanation of the rule obligation/right",
            "applies_to_countries": ["ISO2_codes_for_territorial_scope"],
            "roles": ["Controller", "Processor", "Joint Controller", "Data Subject"],
            "conditions": [
                {{
                    "condition_text": "atomic condition statement",
                    "logical_operator": "AND/OR/NOT",
                    "roles": ["Controller", "Processor"],
                    "is_negation": false
                }}
            ],
            "condition_count": 0,
            "references": ["Article 44", "Section 2.1"],
            "adequacy_countries": ["ISO2_codes_mentioned_in_rule_context"],
            "extraction_metadata": {{
                "confidence_score": 0.0,
                "complexity_level": "low/medium/high",
                "article_numbers_present": ["44", "45"],
                "role_combinations": ["Controller+Processor"],
                "edge_cases_handled": ["negations", "exceptions"]
            }}
        }}]
        
        REQUIREMENTS:
        - Use only ISO2 codes that exist in geography data
        - Include multiple roles where applicable (Controller AND Processor)
        - Handle negations with is_negation flag
        - Reference only articles actually present in source text
        - Include rule_definition field with detailed explanation
        - Add adequacy_countries field with context from rule
        - Ensure atomic conditions with clear logical operators
        - Assign multiple roles to conditions where applicable
        
        OBSERVATION: Ensure comprehensive rule coverage with precision.
        """
        
        structure_response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a JSON formatter using React reasoning. Return only valid JSON with enhanced rule structure."},
                {"role": "user", "content": structure_prompt}
            ]
        )
        
        json_text = structure_response.choices[0].message.content
        
        # Clean JSON response
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
        elif "```" in json_text:
            json_text = json_text.split("```")[1]
        
        try:
            rules_data = json.loads(json_text.strip())
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse rules JSON: {e}, returning empty list")
            state.react_reasoning.append({"step": "rule_parsing_error", "content": f"JSON parsing failed: {e}"})
            return []
        
        # ACTION: Convert parsed data to LegislationRule objects
        for rule_data in rules_data:
            conditions = []
            for cond in rule_data.get("conditions", []):
                # Handle multiple roles in conditions
                cond_roles = []
                for role_str in cond.get("roles", []):
                    if role_str in [e.value for e in RoleType]:
                        cond_roles.append(RoleType(role_str))
                
                conditions.append(RuleCondition(
                    condition_text=cond.get("condition_text", ""),
                    logical_operator=cond.get("logical_operator"),
                    roles=cond_roles,
                    is_negation=cond.get("is_negation", False)
                ))
            
            # Handle multiple roles for the rule
            rule_roles = []
            for role_str in rule_data.get("roles", []):
                if role_str in [e.value for e in RoleType]:
                    rule_roles.append(RoleType(role_str))
            
            # Validate and clean country codes
            applies_to = rule_data.get("applies_to_countries", [state.current_jurisdiction])
            adequacy_countries = rule_data.get("adequacy_countries", [])
            
            if geo_manager:
                # Validate applies_to countries
                validated_countries = []
                for country_code in applies_to:
                    if geo_manager.get_country_info(country_code):
                        validated_countries.append(country_code)
                    else:
                        # Try to find by name
                        matches = geo_manager.find_countries_by_name(country_code)
                        validated_countries.extend(matches)
                applies_to = validated_countries if validated_countries else [state.current_jurisdiction]
                
                # Validate adequacy countries
                validated_adequacy = []
                for country_code in adequacy_countries:
                    if geo_manager.get_country_info(country_code):
                        validated_adequacy.append(country_code)
                adequacy_countries = validated_adequacy
            
            extraction_metadata = rule_data.get("extraction_metadata", {})
            confidence_score = float(extraction_metadata.get("confidence_score", 0.8))
            
            rule = LegislationRule(
                rule_id=rule_data.get("rule_id", f"rule_{len(rules) + 1}"),
                rule_text=rule_data.get("rule_text", ""),
                rule_definition=rule_data.get("rule_definition", rule_data.get("rule_text", "")),
                applies_to_countries=applies_to,
                roles=rule_roles,
                conditions=conditions,
                condition_count=len(conditions),
                references=rule_data.get("references", []),
                adequacy_countries=adequacy_countries,
                extraction_metadata=extraction_metadata,
                confidence_score=confidence_score
            )
            rules.append(rule)
        
        # OBSERVATION: Rule parsing results
        observation = f"""
        OBSERVATION: Rule parsing completed successfully:
        - Converted {len(rules)} rule extractions to structured objects
        - Handled multiple role assignments and combinations
        - Integrated adequacy countries and geographic scope
        - Processed edge cases and negations
        - Validated country codes against geography data
        """
        state.react_reasoning.append({"step": "rule_parsing_observation", "content": observation})
        
        return rules

class RuleDeduplicationAgent:
    """Agent to identify and handle duplicate rules using LLM semantic analysis"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Identify and deduplicate rules using LLM semantic analysis"""
        logger.info("RuleDeduplicationAgent: Starting rule deduplication")
        
        try:
            if len(state.rules) <= 1:
                state.deduplicated_rules = state.rules
                state.next_agent = "output_generation"
                return state
            
            # Perform pairwise semantic comparison using LLM
            duplicate_pairs = await self._identify_duplicate_pairs(state.rules)
            
            # Create deduplication plan
            deduplication_plan = await self._create_deduplication_plan(state.rules, duplicate_pairs)
            
            # Execute deduplication
            deduplicated_rules = await self._execute_deduplication(state.rules, deduplication_plan)
            
            state.deduplicated_rules = deduplicated_rules
            state.next_agent = "sanity_check"
            
            logger.info(f"RuleDeduplicationAgent: Reduced {len(state.rules)} rules to {len(deduplicated_rules)} after deduplication")
            
        except Exception as e:
            error_msg = f"RuleDeduplicationAgent error: {str(e)}"
            logger.error(error_msg)
            state.error_messages.append(error_msg)
            # Continue with original rules if deduplication fails
            state.deduplicated_rules = state.rules
            state.next_agent = "sanity_check"
        
        return state
    
    async def _identify_duplicate_pairs(self, rules: List[LegislationRule]) -> List[Tuple[int, int, str]]:
        """Identify potentially duplicate rule pairs using LLM analysis"""
        duplicate_pairs = []
        
        # Compare rules pairwise
        for i in range(len(rules)):
            for j in range(i + 1, len(rules)):
                rule1 = rules[i]
                rule2 = rules[j]
                
                comparison_prompt = f"""
                Analyze if these two legal rules are duplicates or substantially similar.
                
                CHAIN OF THOUGHT ANALYSIS:
                
                Step 1: SEMANTIC SIMILARITY ASSESSMENT
                Compare the core meaning and intent of both rules.
                
                Step 2: LOGICAL EQUIVALENCE CHECK
                Do the rules create the same obligations/rights under the same conditions?
                
                Step 3: SCOPE AND APPLICABILITY
                Do they apply to the same roles, countries, and circumstances?
                
                Step 4: CONDITION ANALYSIS
                Are the conditions logically equivalent or overlapping?
                
                RULE 1:
                ID: {rule1.rule_id}
                Text: {rule1.rule_text}
                Definition: {rule1.rule_definition}
                Countries: {rule1.applies_to_countries}
                Roles: {[r.value for r in rule1.roles]}
                Conditions: {[c.condition_text for c in rule1.conditions]}
                
                RULE 2: 
                ID: {rule2.rule_id}
                Text: {rule2.rule_text}
                Definition: {rule2.rule_definition}
                Countries: {rule2.applies_to_countries}
                Roles: {[r.value for r in rule2.roles]}
                Conditions: {[c.condition_text for c in rule2.conditions]}
                
                DECISION CATEGORIES:
                - DUPLICATE: Essentially the same rule (merge recommended)
                - SIMILAR: Related but distinct (keep both, note relationship)
                - DIFFERENT: Clearly distinct rules (no action needed)
                
                Respond with only: DUPLICATE, SIMILAR, or DIFFERENT
                """
                
                response = openai_client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {"role": "system", "content": "You are a legal analysis expert. Analyze rule similarities systematically."},
                        {"role": "user", "content": comparison_prompt}
                    ]
                )
                
                result = response.choices[0].message.content.strip().upper()
                
                if result in ["DUPLICATE", "SIMILAR"]:
                    duplicate_pairs.append((i, j, result))
        
        return duplicate_pairs
    
    async def _create_deduplication_plan(self, rules: List[LegislationRule], duplicate_pairs: List[Tuple[int, int, str]]) -> Dict[str, Any]:
        """Create a plan for handling duplicates"""
        if not duplicate_pairs:
            return {"action": "no_duplicates", "rules_to_keep": list(range(len(rules)))}
        
        plan_prompt = f"""
        Create a deduplication plan for these legal rules based on identified similarities.
        
        RULES: {len(rules)} total rules
        DUPLICATE/SIMILAR PAIRS: {len(duplicate_pairs)} pairs identified
        
        PAIR DETAILS:
        {chr(10).join([f"Rules {pair[0]} and {pair[1]}: {pair[2]}" for pair in duplicate_pairs])}
        
        DEDUPLICATION STRATEGY:
        
        For DUPLICATE pairs:
        - Keep the rule with higher confidence score
        - If confidence is equal, keep the one with more comprehensive conditions
        - Mark the removed rule as duplicate_of the kept rule
        
        For SIMILAR pairs:
        - Keep both rules but note their relationship
        - Add cross-references in metadata
        
        CHAIN OF THOUGHT PLANNING:
        
        Step 1: Identify which rules to keep vs remove
        Step 2: Determine merge strategies for duplicates  
        Step 3: Plan metadata updates for relationships
        Step 4: Ensure no orphaned conditions or important details are lost
        
        Return a JSON plan with this structure:
        {{
            "action": "deduplicate",
            "rules_to_keep": [list of rule indices],
            "rules_to_remove": [list of rule indices], 
            "merge_instructions": [
                {{
                    "keep_rule": index,
                    "remove_rule": index,
                    "merge_conditions": true/false,
                    "merge_countries": true/false
                }}
            ],
            "relationship_notes": [
                {{
                    "rule1": index,
                    "rule2": index, 
                    "relationship": "similar/related"
                }}
            ]
        }}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Create deduplication plans in JSON format only."},
                {"role": "user", "content": plan_prompt}
            ]
        )
        
        plan_text = response.choices[0].message.content.strip()
        
        # Clean JSON
        if "```json" in plan_text:
            plan_text = plan_text.split("```json")[1].split("```")[0]
        elif "```" in plan_text:
            plan_text = plan_text.split("```")[1]
        
        try:
            return json.loads(plan_text)
        except json.JSONDecodeError:
            logger.warning("Failed to parse deduplication plan JSON, returning no action plan")
            return {"action": "no_duplicates", "rules_to_keep": list(range(len(rules)))}
    
    async def _execute_deduplication(self, rules: List[LegislationRule], plan: Dict[str, Any]) -> List[LegislationRule]:
        """Execute the deduplication plan"""
        if plan.get("action") == "no_duplicates":
            return rules
        
        deduplicated_rules = []
        rules_to_keep = plan.get("rules_to_keep", [])
        merge_instructions = plan.get("merge_instructions", [])
        
        # Create a mapping of removed rules to their kept counterparts
        removal_mapping = {}
        for instruction in merge_instructions:
            keep_idx = instruction.get("keep_rule")
            remove_idx = instruction.get("remove_rule")
            if keep_idx is not None and remove_idx is not None:
                removal_mapping[remove_idx] = keep_idx
        
        # Process rules
        for i, rule in enumerate(rules):
            if i in rules_to_keep:
                # Check if this rule should be merged with others
                merged_rule = rule.model_copy()
                
                # Find all rules that should be merged into this one
                for instruction in merge_instructions:
                    if instruction.get("keep_rule") == i:
                        remove_idx = instruction.get("remove_rule")
                        if remove_idx < len(rules):
                            removed_rule = rules[remove_idx]
                            
                            # Merge conditions if requested
                            if instruction.get("merge_conditions", False):
                                for condition in removed_rule.conditions:
                                    if condition not in merged_rule.conditions:
                                        merged_rule.conditions.append(condition)
                                merged_rule.condition_count = len(merged_rule.conditions)
                            
                            # Merge countries if requested
                            if instruction.get("merge_countries", False):
                                for country in removed_rule.applies_to_countries:
                                    if country not in merged_rule.applies_to_countries:
                                        merged_rule.applies_to_countries.append(country)
                            
                            # Merge references
                            for ref in removed_rule.references:
                                if ref not in merged_rule.references:
                                    merged_rule.references.append(ref)
                            
                            # Update metadata
                            merged_rule.extraction_metadata["merged_from"] = merged_rule.extraction_metadata.get("merged_from", [])
                            merged_rule.extraction_metadata["merged_from"].append(removed_rule.rule_id)
                
                deduplicated_rules.append(merged_rule)
            
            elif i in removal_mapping:
                # Mark as duplicate
                duplicate_rule = rule.model_copy()
                duplicate_rule.duplicate_of = rules[removal_mapping[i]].rule_id
                # Don't add to final list, but log the relationship
                logger.info(f"Rule {rule.rule_id} marked as duplicate of {duplicate_rule.duplicate_of}")
        
        return deduplicated_rules

class OutputGenerationAgent:
    """Agent to generate final CSV and JSON output with enhanced fields"""
    
    async def process(self, state: AgentState) -> AgentState:
        """Generate output files in CSV and JSON format with enhanced rule structure"""
        logger.info("OutputGenerationAgent: Generating output files")
        print("\nðŸ“„ OutputGenerationAgent: Starting output generation...")
        
        # Use deduplicated rules if available, otherwise use original rules
        final_rules = state.deduplicated_rules if state.deduplicated_rules else state.rules
        print(f"ðŸ“Š Processing {len(final_rules)} final rules for output")
        
        # Validate that we have LegislationRule objects
        validated_rules = []
        for i, rule in enumerate(final_rules):
            if isinstance(rule, LegislationRule):
                validated_rules.append(rule)
            elif isinstance(rule, dict):
                # Convert dict back to LegislationRule
                try:
                    conditions = []
                    for cond_data in rule.get("conditions", []):
                        if isinstance(cond_data, dict):
                            cond_roles = []
                            for role_str in cond_data.get("roles", []):
                                if role_str in [e.value for e in RoleType]:
                                    cond_roles.append(RoleType(role_str))
                            
                            conditions.append(RuleCondition(
                                condition_text=cond_data.get("condition_text", ""),
                                logical_operator=cond_data.get("logical_operator"),
                                roles=cond_roles,
                                is_negation=cond_data.get("is_negation", False)
                            ))
                    
                    rule_roles = []
                    for role_str in rule.get("roles", []):
                        if role_str in [e.value for e in RoleType]:
                            rule_roles.append(RoleType(role_str))
                    
                    validated_rule = LegislationRule(
                        rule_id=rule.get("rule_id", f"rule_{i+1}"),
                        rule_text=rule.get("rule_text", ""),
                        rule_definition=rule.get("rule_definition", rule.get("rule_text", "")),
                        applies_to_countries=rule.get("applies_to_countries", []),
                        roles=rule_roles,
                        conditions=conditions,
                        condition_count=len(conditions),
                        references=rule.get("references", []),
                        adequacy_countries=rule.get("adequacy_countries", []),
                        extraction_metadata=rule.get("extraction_metadata", {}),
                        confidence_score=float(rule.get("confidence_score", 0.8)),
                        duplicate_of=rule.get("duplicate_of")
                    )
                    validated_rules.append(validated_rule)
                except Exception as e:
                    logger.warning(f"Failed to convert rule dict to LegislationRule: {e}")
                    continue
            else:
                logger.warning(f"Unexpected rule type: {type(rule)}, skipping")
                continue
        
        final_rules = validated_rules
        print(f"ðŸ“Š Validated {len(final_rules)} rules for output")
        
        # Generate Enhanced CSV output
        print("ðŸ“„ Generating enhanced CSV output...")
        csv_path = os.path.join(OUTPUT_PATH, "extracted_rules.csv")
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'rule_id', 'rule_definition', 'applies_to_countries', 'roles',
                'conditions', 'condition_count', 'references', 'adequacy_countries'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for rule in final_rules:
                # Format conditions with roles and logical operators
                conditions_list = []
                for cond in rule.conditions:
                    logical_op = cond.logical_operator or 'N/A'
                    roles_str = ", ".join([r.value for r in cond.roles]) if cond.roles else "N/A"
                    negation_prefix = "NOT " if cond.is_negation else ""
                    conditions_list.append(f"{negation_prefix}{cond.condition_text} ({logical_op}) [Roles: {roles_str}]")
                conditions_text = "; ".join(conditions_list)
                
                # Format main roles
                roles_str = ", ".join([r.value for r in rule.roles]) if rule.roles else "N/A"
                
                # Format other fields
                applies_to_str = ", ".join(rule.applies_to_countries)
                references_str = ", ".join(rule.references)
                adequacy_str = ", ".join(rule.adequacy_countries)
                
                writer.writerow({
                    'rule_id': rule.rule_id,
                    'rule_definition': rule.rule_definition,
                    'applies_to_countries': applies_to_str,
                    'roles': roles_str,
                    'conditions': conditions_text,
                    'condition_count': rule.condition_count,
                    'references': references_str,
                    'adequacy_countries': adequacy_str
                })
        
        print(f"âœ… Enhanced CSV output saved to: {csv_path}")
        
        # Generate Enhanced JSON output
        print("ðŸ“„ Generating enhanced JSON output...")
        json_path = os.path.join(OUTPUT_PATH, "extracted_rules.json")
        rules_dict = []
        
        for rule in final_rules:
            # Format conditions with enhanced structure
            conditions_json = []
            for cond in rule.conditions:
                cond_dict = {
                    "condition_text": cond.condition_text,
                    "logical_operator": cond.logical_operator,
                    "roles": [r.value for r in cond.roles] if cond.roles else [],
                    "is_negation": cond.is_negation
                }
                conditions_json.append(cond_dict)
            
            rule_dict = {
                "rule_id": rule.rule_id,
                "rule_text": rule.rule_text,
                "rule_definition": rule.rule_definition,
                "applies_to_countries": rule.applies_to_countries,
                "roles": [r.value for r in rule.roles] if rule.roles else [],
                "conditions": conditions_json,
                "condition_count": rule.condition_count,
                "references": rule.references,
                "adequacy_countries": rule.adequacy_countries,
                "extraction_metadata": rule.extraction_metadata,
                "confidence_score": rule.confidence_score,
                "duplicate_of": rule.duplicate_of
            }
            rules_dict.append(rule_dict)
        
        with open(json_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(rules_dict, jsonfile, indent=2, ensure_ascii=False)
        
        print(f"âœ… Enhanced JSON output saved to: {json_path}")
        
        # Generate comprehensive report
        print("ðŸ“„ Generating comprehensive processing report...")
        report_path = os.path.join(OUTPUT_PATH, "processing_report.json")
        
        # Calculate statistics
        total_conditions = sum(rule.condition_count for rule in final_rules)
        roles_stats = {}
        for rule in final_rules:
            for role in rule.roles:
                roles_stats[role.value] = roles_stats.get(role.value, 0) + 1
        
        adequacy_countries_all = set()
        for rule in final_rules:
            adequacy_countries_all.update(rule.adequacy_countries)
        
        report = {
            "processing_summary": {
                "original_rule_count": len(state.rules),
                "deduplicated_rule_count": len(final_rules),
                "rules_removed": len(state.rules) - len(final_rules),
                "total_conditions_extracted": total_conditions,
                "average_conditions_per_rule": total_conditions / len(final_rules) if final_rules else 0
            },
            "adequacy_analysis": {
                "adequacy_countries_identified": state.adequacy_countries,
                "adequacy_countries_in_rules": list(adequacy_countries_all),
                "total_unique_adequacy_countries": len(adequacy_countries_all)
            },
            "geographic_scope": {
                "jurisdiction_processed": state.current_jurisdiction,
                "geography_regions_available": list(state.geography_data.keys()) if state.geography_data else [],
                "countries_covered": list(set([country for rule in final_rules for country in rule.applies_to_countries]))
            },
            "role_analysis": {
                "roles_distribution": roles_stats,
                "total_role_assignments": sum(roles_stats.values())
            },
            "processing_quality": {
                "average_confidence_score": sum(rule.confidence_score for rule in final_rules) / len(final_rules) if final_rules else 0,
                "high_confidence_rules": len([rule for rule in final_rules if rule.confidence_score >= 0.8]),
                "low_confidence_rules": len([rule for rule in final_rules if rule.confidence_score < 0.6])
            },
            "react_reasoning_steps": len(state.react_reasoning),
            "processing_errors": state.error_messages,
            "files_generated": {
                "csv_output": csv_path,
                "json_output": json_path,
                "report": report_path
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as reportfile:
            json.dump(report, reportfile, indent=2, ensure_ascii=False)
        
        print(f"âœ… Comprehensive report saved to: {report_path}")
        
        # Generate React reasoning log
        reasoning_log_path = os.path.join(OUTPUT_PATH, "react_reasoning_log.json")
        with open(reasoning_log_path, 'w', encoding='utf-8') as logfile:
            json.dump(state.react_reasoning, logfile, indent=2, ensure_ascii=False)
        
        print(f"âœ… React reasoning log saved to: {reasoning_log_path}")
        
        logger.info(f"Generated output files: {csv_path}, {json_path}, {report_path}, {reasoning_log_path}")
        state.next_agent = "end"
        
        print("âœ… OutputGenerationAgent completed successfully")
        return state

class SupervisorAgent:
    """Supervisor agent that orchestrates the multi-agent workflow"""
    
    def __init__(self):
        self.agents = {
            "document_processor": ReactDocumentProcessorAgent(),
            "segmentation": ReactIntelligentSegmentationAgent(),
            "entity_extraction": ReactComprehensiveEntityExtractionAgent(),
            "rule_extraction": ReactIntelligentRuleComponentExtractionAgent(),
            "rule_deduplication": RuleDeduplicationAgent(),
            "output_generation": OutputGenerationAgent()
        }
        
        # Setup LangGraph workflow with proper typing
        self.workflow = StateGraph(AgentState)
        
        # Add nodes
        self.workflow.add_node("document_processor", self._document_processor_node)
        self.workflow.add_node("segmentation", self._segmentation_node)
        self.workflow.add_node("entity_extraction", self._entity_extraction_node)
        self.workflow.add_node("rule_extraction", self._rule_extraction_node)
        self.workflow.add_node("rule_deduplication", self._rule_deduplication_node)
        self.workflow.add_node("sanity_check", self._sanity_check_node)
        self.workflow.add_node("output_generation", self._output_generation_node)
        self.workflow.add_node("supervisor", self._supervisor_node)
        
        # Define edges
        self.workflow.add_edge(START, "supervisor")
        self.workflow.add_edge("document_processor", "supervisor")
        self.workflow.add_edge("segmentation", "supervisor")
        self.workflow.add_edge("entity_extraction", "supervisor")
        self.workflow.add_edge("rule_extraction", "supervisor")
        self.workflow.add_edge("rule_deduplication", "supervisor")
        self.workflow.add_edge("sanity_check", "supervisor")
        self.workflow.add_edge("output_generation", END)
        
        # Add conditional edges from supervisor
        self.workflow.add_conditional_edges(
            "supervisor",
            self._route_next,
            {
                "document_processor": "document_processor",
                "segmentation": "segmentation",
                "entity_extraction": "entity_extraction",
                "rule_extraction": "rule_extraction",
                "rule_deduplication": "rule_deduplication",
                "sanity_check": "sanity_check",
                "output_generation": "output_generation",
                "end": END
            }
        )
        
        # Setup memory
        self.memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.memory)
    
    async def _document_processor_node(self, state: AgentState) -> AgentState:
        """Document processor node wrapper"""
        return await self.agents["document_processor"].process(state)
    
    async def _segmentation_node(self, state: AgentState) -> AgentState:
        """Segmentation node wrapper"""
        return await self.agents["segmentation"].process(state)
    
    async def _entity_extraction_node(self, state: AgentState) -> AgentState:
        """Entity extraction node wrapper"""
        return await self.agents["entity_extraction"].process(state)
    
    async def _rule_extraction_node(self, state: AgentState) -> AgentState:
        """Rule extraction node wrapper"""
        return await self.agents["rule_extraction"].process(state)
    
    async def _rule_deduplication_node(self, state: AgentState) -> AgentState:
        """Rule deduplication node wrapper"""
        return await self.agents["rule_deduplication"].process(state)
    
    async def _sanity_check_node(self, state: AgentState) -> AgentState:
        """Sanity check node wrapper"""
        return await self._perform_sanity_check_async(state)
    
    async def _output_generation_node(self, state: AgentState) -> AgentState:
        """Output generation node wrapper"""
        return await self.agents["output_generation"].process(state)
    
    def _supervisor_node(self, state: AgentState) -> AgentState:
        """Supervisor node for workflow coordination"""
        logger.info(f"Supervisor: Current agent = {state.next_agent}")
        logger.info(f"Documents processed: {len(state.documents)}")
        logger.info(f"Segments created: {len(state.segmented_content)}")
        logger.info(f"Entities extracted: {len(state.extracted_entities)}")
        logger.info(f"Rules generated: {len(state.rules)}")
        logger.info(f"Rules deduplicated: {len(state.deduplicated_rules)}")
        logger.info(f"Adequacy countries: {state.adequacy_countries}")
        logger.info(f"React reasoning steps: {len(state.react_reasoning)}")
        
        if state.error_messages:
            logger.error(f"Errors encountered: {state.error_messages}")
        
        return state
    
    async def _perform_sanity_check_async(self, state: AgentState) -> AgentState:
        """Perform final sanity check on extracted rules using React reasoning"""
        logger.info("SupervisorAgent: Performing final sanity check")
        print("\nðŸ” SupervisorAgent: Performing final sanity check...")
        
        final_rules = state.deduplicated_rules if state.deduplicated_rules else state.rules
        
        # THOUGHT: Plan sanity check approach
        thought = """
        THOUGHT: I need to perform a comprehensive final validation of all extracted rules.
        This will ensure legal accuracy, geographic precision, role consistency, and logical coherence.
        """
        state.react_reasoning.append({"step": "sanity_check_thought", "content": thought})
        print("ðŸ¤” THOUGHT:", thought.strip())
        
        # ACTION: Perform comprehensive validation
        print("\nðŸŽ¬ ACTION: Performing comprehensive rule validation...")
        validated_rules = await self._perform_react_sanity_check(final_rules, state)
        
        # Update state with validated rules
        if state.deduplicated_rules:
            state.deduplicated_rules = validated_rules
        else:
            state.rules = validated_rules
        
        # OBSERVATION: Sanity check results
        observation = f"""
        OBSERVATION: Sanity check completed successfully:
        - Validated {len(validated_rules)} rules for legal accuracy
        - Checked geographic precision and adequacy country alignment
        - Verified role assignments and condition logic
        - Ensured reference accuracy to actual articles
        - Rules are ready for final output generation
        """
        state.react_reasoning.append({"step": "sanity_check_observation", "content": observation})
        print("ðŸ‘ï¸ OBSERVATION:", observation.strip())
        
        state.next_agent = "output_generation"
        logger.info(f"SupervisorAgent: Sanity check completed. Validated {len(validated_rules)} rules")
        print("âœ… SupervisorAgent sanity check completed successfully")
        
        return state
    
    async def _perform_react_sanity_check(self, rules: List[LegislationRule], state: AgentState) -> List[LegislationRule]:
        """Perform comprehensive sanity check using React reasoning"""
        
        # Create comprehensive validation prompt
        sanity_check_prompt = self._create_react_sanity_check_prompt(rules, state)
        
        messages = [
            SystemMessage(content="You are a senior legal analyst performing final quality assurance using React reasoning."),
            HumanMessage(content=sanity_check_prompt)
        ]
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=_convert_messages_for_openai(messages)
        )
        
        validation_result = response.choices[0].message.content
        
        # Parse validation recommendations and apply corrections
        corrected_rules = await self._apply_react_validation_corrections(rules, validation_result, state)
        
        return corrected_rules
    
    def _create_react_sanity_check_prompt(self, rules: List[LegislationRule], state: AgentState) -> str:
        """Create comprehensive React-style sanity check prompt"""
        
        rules_summary = []
        for i, rule in enumerate(rules[:5]):  # Limit to first 5 for prompt size
            rules_summary.append(f"""
            Rule {i+1}:
            ID: {rule.rule_id}
            Text: {rule.rule_text}
            Definition: {rule.rule_definition}
            Countries: {rule.applies_to_countries}
            Roles: {[r.value for r in rule.roles]}
            Conditions: {len(rule.conditions)} conditions
            Adequacy Countries: {rule.adequacy_countries}
            References: {rule.references}
            Confidence: {rule.confidence_score}
            """)
        
        return f"""
        Use React reasoning to perform comprehensive final validation of extracted legal rules.
        
        THOUGHT: I need to systematically validate these rules for legal accuracy, precision, and consistency.
        
        VALIDATION FRAMEWORK:
        
        ACTION: Apply systematic validation checks:
        
        1. LEGAL ACCURACY VALIDATION:
        THOUGHT: Do these rules accurately reflect data protection law principles?
        ACTION: Check if:
        - Rule texts clearly state obligations, rights, or prohibitions
        - Conditions are logically connected and non-contradictory
        - Logical operators (AND, OR, NOT) make sense in context
        - Negations are properly handled with is_negation flags
        OBSERVATION: Document any legal inconsistencies or ambiguities.
        
        2. GEOGRAPHIC PRECISION VALIDATION:
        THOUGHT: Are country codes and adequacy decisions accurate?
        ACTION: Verify that:
        - Country codes are valid ISO2 codes from available geography
        - Adequacy countries align with actual adequacy decisions mentioned
        - Regional applications (EU, EEA) are correctly specified
        - Cross-border transfer implications are accurate
        OBSERVATION: Note any geographic inaccuracies or missing adequacy context.
        
        3. ROLE ASSIGNMENT VERIFICATION:
        THOUGHT: Are role assignments consistent and complete?
        ACTION: Check that:
        - Controller/Processor/Joint Controller/Data Subject roles are correctly assigned
        - Role assignments match the obligations described in rules
        - Multiple role scenarios are properly handled
        - Condition-level role assignments are consistent with rule-level roles
        OBSERVATION: Identify any role assignment errors or inconsistencies.
        
        4. REFERENCE ACCURACY ASSESSMENT:
        THOUGHT: Do legal references match actual content?
        ACTION: Verify that:
        - Article references correspond to articles actually present in source
        - No phantom article references (e.g., referencing articles 44-50 when only 44-46 present)
        - Legal citations are appropriate and correctly formatted
        - References support the extracted rules
        OBSERVATION: Flag any incorrect or phantom references.
        
        5. CONDITION LOGIC VALIDATION:
        THOUGHT: Are conditions properly structured and implementable?
        ACTION: Check that:
        - Conditions are atomic and testable
        - Logical operators create valid logical expressions
        - Negations are clearly marked and logically sound
        - Role assignments for conditions make legal sense
        OBSERVATION: Note any logical inconsistencies or unclear conditions.
        
        6. ADEQUACY INTEGRATION VALIDATION:
        THOUGHT: Are adequacy decisions properly integrated?
        ACTION: Verify that:
        - Adequacy countries are mentioned in appropriate rule contexts
        - Cross-border transfer rules reference relevant adequacy decisions
        - Geographic scope aligns with adequacy status
        - Transfer mechanism rules consider adequacy implications
        OBSERVATION: Document adequacy integration completeness.
        
        RULES TO VALIDATE ({len(rules)} total):
        {chr(10).join(rules_summary)}
        
        CONTEXT FOR VALIDATION:
        - Original jurisdiction: {state.current_jurisdiction}
        - Adequacy countries identified: {state.adequacy_countries}
        - Available geography: {list(state.geography_data.keys()) if state.geography_data else []}
        - Legislation content: {len(state.legislation_content)} characters
        - Supporting content: {len(state.supporting_content)} characters
        
        THOUGHT: Now I need to provide structured validation feedback.
        
        ACTION: For the rule set overall, provide validation assessment:
        
        VALIDATION STATUS: VALID / NEEDS_MINOR_CORRECTIONS / NEEDS_MAJOR_CORRECTIONS
        
        SPECIFIC ISSUES IDENTIFIED:
        - Legal accuracy issues: [list any problems]
        - Geographic precision issues: [list any problems]
        - Role assignment issues: [list any problems]
        - Reference accuracy issues: [list any problems]
        - Condition logic issues: [list any problems]
        - Adequacy integration issues: [list any problems]
        
        RECOMMENDED CORRECTIONS:
        - High priority corrections: [critical fixes needed]
        - Medium priority corrections: [improvements suggested]
        - Low priority corrections: [minor enhancements]
        
        CONFIDENCE ASSESSMENT:
        - Overall rule set quality: [score 0-1]
        - Recommended confidence adjustments: [specific rules needing adjustment]
        
        COMPLETENESS EVALUATION:
        - Coverage of data transfer concepts: [assessment]
        - Coverage of access rights concepts: [assessment]
        - Coverage of entitlement concepts: [assessment]
        - Coverage of role obligations: [assessment]
        
        OBSERVATION: Provide final assessment of rule set coherence and legal soundness.
        
        Focus on ensuring rules are:
        - Legally accurate and implementable
        - Geographically precise with correct adequacy integration
        - Logically consistent with proper role assignments
        - Properly referenced to actual source articles
        - Complete coverage of key data protection concepts
        """
    
    async def _apply_react_validation_corrections(self, rules: List[LegislationRule], validation_result: str, state: AgentState) -> List[LegislationRule]:
        """Apply validation corrections using React reasoning"""
        
        # Check if validation suggests keeping rules as-is
        if "VALID" in validation_result.upper() and ("NO MAJOR CORRECTIONS" in validation_result.upper() or "NEEDS_MINOR_CORRECTIONS" not in validation_result.upper()):
            logger.info("Validation completed - rules are valid as-is")
            return rules
        
        correction_prompt = f"""
        Use React reasoning to determine and apply necessary corrections based on validation analysis.
        
        THOUGHT: I need to analyze the validation feedback and determine if corrections are needed.
        
        VALIDATION ANALYSIS:
        {validation_result}
        
        ORIGINAL RULES: {len(rules)} rules
        
        ACTION: Analyze validation feedback to determine correction approach:
        
        If validation indicates rules are valid with only minor issues that don't affect structure:
        Respond with: "NO_STRUCTURAL_CORRECTIONS_NEEDED"
        
        If significant corrections are needed that affect rule structure, content, or metadata:
        Provide corrected rules in JSON array format with the enhanced structure:
        [{{
            "rule_id": "precise_identifier",
            "rule_text": "corrected_rule_statement",
            "rule_definition": "detailed_corrected_definition",
            "applies_to_countries": ["validated_ISO2_codes"],
            "roles": ["Controller", "Processor", "Joint Controller", "Data Subject"],
            "conditions": [
                {{
                    "condition_text": "corrected_atomic_condition",
                    "logical_operator": "AND/OR/NOT",
                    "roles": ["Controller"],
                    "is_negation": false
                }}
            ],
            "condition_count": 0,
            "references": ["corrected_article_references"],
            "adequacy_countries": ["relevant_adequacy_ISO2_codes"],
            "extraction_metadata": {{
                "confidence_score": 0.0,
                "corrections_applied": ["list_of_corrections"]
            }},
            "duplicate_of": null
        }}]
        
        CORRECTION REQUIREMENTS:
        - Fix any phantom article references (only reference articles actually present)
        - Correct geographic assignments using validated country codes
        - Adjust role assignments where inconsistent
        - Fix logical operator issues in conditions
        - Integrate adequacy countries where missing
        - Update confidence scores based on correction quality
        
        OBSERVATION: Apply only necessary corrections while preserving legal accuracy.
        
        Available geography codes: {list(state.geography_data.keys()) if state.geography_data else []}
        Adequacy countries context: {state.adequacy_countries}
        """
        
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "Apply rule corrections using React reasoning based on validation analysis."},
                {"role": "user", "content": correction_prompt}
            ]
        )
        
        correction_text = response.choices[0].message.content.strip()
        
        # Check if no corrections are needed
        if "NO_STRUCTURAL_CORRECTIONS_NEEDED" in correction_text:
            logger.info("Validation completed without structural corrections needed")
            return rules
        
        # Parse corrected rules if JSON format is returned
        if "```json" in correction_text or "[" in correction_text:
            try:
                # Clean JSON response
                if "```json" in correction_text:
                    correction_text = correction_text.split("```json")[1].split("```")[0]
                elif "```" in correction_text:
                    correction_text = correction_text.split("```")[1]
                
                corrected_data = json.loads(correction_text.strip())
                
                corrected_rules = []
                for rule_data in corrected_data:
                    # Rebuild conditions with corrections
                    conditions = []
                    for cond in rule_data.get("conditions", []):
                        cond_roles = []
                        for role_str in cond.get("roles", []):
                            if role_str in [e.value for e in RoleType]:
                                cond_roles.append(RoleType(role_str))
                        
                        conditions.append(RuleCondition(
                            condition_text=cond.get("condition_text", ""),
                            logical_operator=cond.get("logical_operator"),
                            roles=cond_roles,
                            is_negation=cond.get("is_negation", False)
                        ))
                    
                    # Rebuild rule roles
                    rule_roles = []
                    for role_str in rule_data.get("roles", []):
                        if role_str in [e.value for e in RoleType]:
                            rule_roles.append(RoleType(role_str))
                    
                    corrected_rule = LegislationRule(
                        rule_id=rule_data.get("rule_id", ""),
                        rule_text=rule_data.get("rule_text", ""),
                        rule_definition=rule_data.get("rule_definition", ""),
                        applies_to_countries=rule_data.get("applies_to_countries", []),
                        roles=rule_roles,
                        conditions=conditions,
                        condition_count=len(conditions),
                        references=rule_data.get("references", []),
                        adequacy_countries=rule_data.get("adequacy_countries", []),
                        extraction_metadata=rule_data.get("extraction_metadata", {}),
                        confidence_score=float(rule_data.get("confidence_score", 0.8)),
                        duplicate_of=rule_data.get("duplicate_of")
                    )
                    corrected_rules.append(corrected_rule)
                
                logger.info(f"Applied structural corrections to {len(corrected_rules)} rules")
                return corrected_rules
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse corrected rules JSON: {e}, returning original rules")
                return rules
        else:
            # No structural corrections needed, return original rules
            logger.info("Validation completed without needing structural corrections")
            return rules
    
    def _route_next(self, state: AgentState) -> str:
        """Route to the next agent based on current state"""
        return state.next_agent
    
    async def run(self) -> AgentState:
        """Run the complete multi-agent workflow"""
        logger.info("Starting enhanced React-based multi-agent legislation processing workflow")
        
        initial_state = AgentState()
        thread_config = {"configurable": {"thread_id": "legislation_processing"}}
        
        final_state = await self.app.ainvoke(initial_state, config=thread_config)
        
        logger.info("Workflow completed successfully")
        logger.info(f"Original rules: {len(final_state.rules)}")
        logger.info(f"Final deduplicated rules: {len(final_state.deduplicated_rules)}")
        logger.info(f"Adequacy countries identified: {final_state.adequacy_countries}")
        logger.info(f"React reasoning steps: {len(final_state.react_reasoning)}")
        
        if final_state.error_messages:
            logger.warning(f"Workflow completed with errors: {final_state.error_messages}")
        
        return final_state

async def main():
    """Main entry point for the legislation rule extraction system"""
    
    # Validate OpenAI client
    if not openai_client:
        logger.error("OpenAI client not initialized. Please check OPENAI_API_KEY and network connectivity.")
        print("âŒ Error: OpenAI client not initialized. Please check:")
        print("  - OPENAI_API_KEY environment variable is set correctly")
        print("  - Network connectivity to OpenAI API")
        print("  - API key has sufficient permissions and credits")
        sys.exit(1)
    
    # Validate required files exist
    required_files = [LEGISLATION_METADATA_PATH, GEOGRAPHY_PATH]
    for file_path in required_files:
        if not os.path.exists(file_path):
            logger.error(f"Required file not found: {file_path}")
            print(f"âŒ Error: Required file not found: {file_path}")
            
            # Create example files if they don't exist
            if file_path == LEGISLATION_METADATA_PATH:
                print(f"Creating example {file_path}...")
                example_metadata = [
                    {
                        "path": "./input_pdfs/example_regulation.pdf",
                        "jurisdiction": "EU"
                    }
                ]
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(example_metadata, f, indent=2)
                    print(f"âœ… Created example {file_path}")
                except Exception as e:
                    print(f"âŒ Failed to create {file_path}: {e}")
                    sys.exit(1)
            
            elif file_path == GEOGRAPHY_PATH:
                print(f"âŒ Please ensure {file_path} exists with proper geography data")
                sys.exit(1)
    
    # Validate that input PDF directory exists
    if not os.path.exists(INPUT_PDF_PATH):
        logger.warning(f"Input PDF directory not found: {INPUT_PDF_PATH}")
        print(f"âš ï¸  Warning: Input PDF directory not found: {INPUT_PDF_PATH}")
        print("Creating input directory...")
        os.makedirs(INPUT_PDF_PATH, exist_ok=True)
        print(f"âœ… Created {INPUT_PDF_PATH}")
    
    logger.info("Starting Enhanced React-Based Legislation Rule Extraction System")
    logger.info(f"Model: {MODEL_NAME}")
    logger.info(f"Embedding Model: {EMBEDDING_MODEL}")
    logger.info(f"Output Path: {OUTPUT_PATH}")
    
    print("\nðŸš€ Starting Enhanced React-Based Legislation Rule Extraction System")
    print(f"ðŸ“„ Model: {MODEL_NAME}")
    print(f"ðŸ” Embedding Model: {EMBEDDING_MODEL}")
    print(f"ðŸ“ Output Path: {OUTPUT_PATH}")
    
    # Initialize and run supervisor
    try:
        supervisor = SupervisorAgent()
        
        final_state = await supervisor.run()
        
        print("\n" + "="*70)
        print("ENHANCED REACT-BASED LEGISLATION RULE EXTRACTION COMPLETED")
        print("="*70)
        print(f"Original rules extracted: {len(final_state.rules)}")
        print(f"Deduplicated rules: {len(final_state.deduplicated_rules)}")
        print(f"Duplicates removed: {len(final_state.rules) - len(final_state.deduplicated_rules)}")
        print(f"Documents processed: {len(final_state.documents)}")
        print(f"Entities resolved: {len(final_state.extracted_entities)}")
        print(f"Adequacy countries identified: {final_state.adequacy_countries}")
        print(f"React reasoning steps: {len(final_state.react_reasoning)}")
        print(f"Sanity check completed: âœ“")
        print(f"Output files generated in: {OUTPUT_PATH}")
        
        if final_state.error_messages:
            print(f"\nâš ï¸  Errors encountered: {len(final_state.error_messages)}")
            for error in final_state.error_messages:
                print(f"  - {error}")
        
        # Print sample validated rules with enhanced information
        final_rules = final_state.deduplicated_rules if final_state.deduplicated_rules else final_state.rules
        if final_rules:
            print("\nðŸ“‹ Sample validated rules with enhanced details:")
            for i, rule in enumerate(final_rules[:3]):
                print(f"\nRule {i+1}:")
                print(f"  ID: {rule.rule_id}")
                print(f"  Text: {rule.rule_text[:80]}...")
                print(f"  Definition: {rule.rule_definition[:100]}...")
                print(f"  Countries: {rule.applies_to_countries}")
                print(f"  Roles: {[r.value for r in rule.roles]}")
                print(f"  Conditions: {rule.condition_count}")
                print(f"  Adequacy Countries: {rule.adequacy_countries}")
                print(f"  References: {rule.references}")
                print(f"  Confidence: {rule.confidence_score:.2f}")
                if rule.duplicate_of:
                    print(f"  Duplicate of: {rule.duplicate_of}")
                
                # Show sample condition details
                if rule.conditions:
                    print(f"  Sample Condition:")
                    cond = rule.conditions[0]
                    print(f"    Text: {cond.condition_text}")
                    print(f"    Operator: {cond.logical_operator}")
                    print(f"    Roles: {[r.value for r in cond.roles]}")
                    print(f"    Negation: {cond.is_negation}")
        else:
            print("\nâš ï¸  No rules were extracted. Check the error messages above.")
        
        # Print React reasoning summary
        if final_state.react_reasoning:
            print(f"\nðŸ§  React Reasoning Process Summary:")
            print(f"  Total reasoning steps: {len(final_state.react_reasoning)}")
            reasoning_types = {}
            for step in final_state.react_reasoning:
                step_type = step.get("step", "unknown").replace("_", " ").title()
                reasoning_types[step_type] = reasoning_types.get(step_type, 0) + 1
            
            for reasoning_type, count in reasoning_types.items():
                print(f"  {reasoning_type}: {count} steps")
        
    except Exception as e:
        logger.error(f"System failed: {str(e)}")
        print(f"\nâŒ System failed: {str(e)}")
        print("Check the logs for more details.")
        import traceback
        traceback.print_exc()
        sys.exit(1)

async def test_system_components():
    """Test system components before running main workflow"""
    print("ðŸ§ª Testing system components...")
    
    # Test OpenAI client
    try:
        if openai_client:
            test_response = openai_client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": "Test connection"}],
                max_tokens=10
            )
            print("âœ… OpenAI client connection successful")
        else:
            print("âŒ OpenAI client not available")
            return False
    except Exception as e:
        print(f"âŒ OpenAI client test failed: {e}")
        return False
    
    # Test embeddings
    try:
        embeddings = CustomEmbeddings()
        test_embedding = embeddings.embed_query("test query")
        if len(test_embedding) > 0:
            print("âœ… Embeddings working correctly")
        else:
            print("âŒ Embeddings test failed")
            return False
    except Exception as e:
        print(f"âŒ Embeddings test failed: {e}")
        return False
    
    # Test geography manager
    try:
        processor = LegislationProcessor()
        geography_data = processor.load_geography_data()
        if geography_data:
            geo_manager = GeographyManager(geography_data)
            test_country = geo_manager.get_country_info("US")
            if test_country:
                print("âœ… Geography manager working correctly")
            else:
                print("âš ï¸  Geography manager loaded but no US country found")
        else:
            print("âŒ Geography data could not be loaded")
            return False
    except Exception as e:
        print(f"âŒ Geography manager test failed: {e}")
        return False
    
    return True


if __name__ == "__main__":
    async def run_with_tests():
        # Run component tests first
        if await test_system_components():
            print("âœ… All system components tested successfully\n")
            await main()
        else:
            print("âŒ System component tests failed. Please check configuration.")
            sys.exit(1)
    
    asyncio.run(run_with_tests())
