# First, add these imports
import asyncio
from typing import Iterable
from langgraph.graph import END, StateGraph
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage
import re
from concurrent.futures import ThreadPoolExecutor

# 1. Text Cleaning Function
def clean_input_data(data: dict) -> dict:
    """Clean and normalize input data for LLM processing"""
    cleaned = {}
    for key, value in data.items():
        # Convert all values to string
        str_value = str(value).strip() if value is not None else ""
        
        # Remove special characters except basic punctuation
        str_value = re.sub(r'[^\w\s.,;:!?\-()]', '', str_value)
        
        # Normalize whitespace
        str_value = re.sub(r'\s+', ' ', str_value)
        
        # Truncate to 10k characters for LLM context
        str_value = str_value[:10000]
        
        cleaned[key] = str_value
    return cleaned

# 2. Optimized Classification Workflow
class ClassificationWorkflow:
    def __init__(self, classifier, validator):
        self.classifier = classifier
        self.validator = validator
        self.chain = self._build_chain()

    def _build_chain(self):
        workflow = StateGraph(ClassificationState)
        
        workflow.add_node("classify", self.classify_node)
        workflow.add_node("validate", self.validate_node)
        
        workflow.add_edge("classify", "validate")
        workflow.add_edge("validate", END)
        
        workflow.set_entry_point("classify")
        return workflow.compile()

    async def classify_node(self, state: ClassificationState):
        data = state["data"]
        result = await self.classifier.aclassify(data)
        return {"classification": result}

    async def validate_node(self, state: ClassificationState):
        data = state["data"]
        classification = state["classification"]
        validation = await self.validator.avalidation(data, classification)
        return {"validation": validation}

    async def ainvoke(self, input_data: dict):
        return await self.chain.ainvoke(input_data)

# 3. Async-Enabled Agents
class ClassificationAgent:
    def __init__(self, llm):
        self.llm = llm
        self.parser = JsonOutputParser(pydantic_object=ClassificationResult)
        self.prompt = ChatPromptTemplate.from_template(...)  # Keep previous template

    async def aclassify(self, data: DataInput) -> ClassificationResult:
        chain = self.prompt | self.llm | self.parser
        return await chain.ainvoke({
            "name": data.name,
            "definition": data.definition
        })

class ValidationAgent:
    def __init__(self, llm):
        self.llm = llm
        self.parser = JsonOutputParser(pydantic_object=ValidatedClassification)
        self.prompt = ChatPromptTemplate.from_template(...)  # Keep previous template

    async def avalidate(self, data: DataInput, result: ClassificationResult) -> ValidatedClassification:
        chain = self.prompt | self.llm | self.parser
        return await chain.ainvoke({
            "input_data": data.json(),
            "classification_result": result.json()
        })

# 4. Batch Processing in AzureChatbot
class AzureChatbot:
    def __init__(self, config_file=str, creds_file=str, cert_file=str):
        # Existing initialization
        self.classifier = ClassificationAgent(self.llm)
        self.validator = ValidationAgent(self.llm)
        self.workflow = ClassificationWorkflow(self.classifier, self.validator)
        self.executor = ThreadPoolExecutor(max_workers=10)  # Adjust based on rate limits

    async def process_single(self, input_data: dict) -> dict:
        try:
            cleaned = clean_input_data(input_data)
            data = DataInput(**cleaned)
            result = await self.workflow.ainvoke({"data": data})
            return result.get("validation", {}).dict()
        except Exception as e:
            logger.error(f"Error processing record: {e}")
            return {"error": str(e), "input": input_data}

    async def process_batch(self, inputs: Iterable[dict], batch_size=50) -> list:
        """Process records in parallel batches"""
        semaphore = asyncio.Semaphore(10)  # Concurrent requests
        
        async def process_with_semaphore(data):
            async with semaphore:
                return await self.process_single(data)
                
        results = []
        batch = []
        
        for idx, record in enumerate(inputs):
            batch.append(record)
            if len(batch) >= batch_size:
                tasks = [process_with_semaphore(data) for data in batch]
                results += await asyncio.gather(*tasks, return_exceptions=True)
                batch = []
                logger.info(f"Processed {idx+1} records")
        
        if batch:
            tasks = [process_with_semaphore(data) for data in batch]
            results += await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

    def process_from_file(self, input_path: str, output_path: str):
        """Process large JSON file with progress tracking"""
        with open(input_path, 'r') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            data = [data]
        
        loop = asyncio.get_event_loop()
        results = loop.run_until_complete(self.process_batch(data))
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        return results

bot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)

# Process large file
bot.process_from_file("input_data.json", "results.json")

# Process single record
async def test_single():
    input_data = {"name": "Patient Records", "definition": "Medical history..."}
    result = await bot.process_single(input_data)
    print(result)

asyncio.run(test_single())
