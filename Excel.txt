#########################
# main.py (Row-by-Row Ingestion, Chunking, No Large Batches)
#########################

import os
import re
import uuid
import json
import logging
import chardet
import pandas as pd
import requests
import chromadb
from chromadb.config import Settings
from pathlib import Path
from typing import Optional, List, Dict, Any

from dotenv import load_dotenv, dotenv_values
from azure.identity import ClientSecretCredential

# For LangChain 0.3.18
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# We'll assume your snippet in "ooai_client_sdk.azopenaiembrdding" defines:
#   class Document(BaseModel):
#       text: str
#       embedding: List[float] = []
#   class EmbeddingClient:
#       def generate_embeddings(self, doc: Document) -> Document
from ooai_client_sdk.azopenaiembrdding import Document, EmbeddingClient

#########################
# Logging Setup
#########################
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

#########################
# Helper Functions
#########################
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def remove_invisible_chars(s: str) -> str:
    """Remove invisible directional characters like \\u202a, \\u202b, etc."""
    import re
    return re.sub(r'[\u202a\u202b\u202c\u202d\u202e]', '', s).strip()

def guess_file_encoding(file_path: str, num_bytes: int = 4096) -> Optional[str]:
    """Use chardet to guess file encoding."""
    import chardet
    with open(file_path, 'rb') as f:
        raw_data = f.read(num_bytes)
    guess = chardet.detect(raw_data)
    encoding = guess.get('encoding', None)
    if encoding:
        logger.info(f"chardet guessed encoding='{encoding}' for {file_path}")
    else:
        logger.warning(f"chardet could not guess encoding for {file_path}")
    return encoding

def read_csv_flexible(csv_path: str) -> pd.DataFrame:
    """Try reading a CSV with chardet guess, then fallback encodings, skipping bad lines."""
    enc = guess_file_encoding(csv_path)
    if enc:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            return df
        except Exception as e:
            logger.warning(f"Reading with guessed encoding '{enc}' failed: {e}")

    fallback_encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']
    for enc in fallback_encodings:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            logger.info(f"Successfully read CSV with fallback encoding='{enc}'")
            return df
        except Exception as e:
            logger.warning(f"Failed reading with encoding={enc}: {e}")

    raise ValueError("Unable to read CSV in any known encoding or chardet guess.")

def stable_chunk_text(text: str, max_chars: int = 3000) -> List[str]:
    """
    Splits a string into chunks up to max_chars each, 
    attempting to split on sentence boundaries for clarity.
    Adjust max_chars as needed to avoid 'batch size exceeded.'
    """
    import re
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sent in sentences:
        if len(current_chunk) + len(sent) + 1 <= max_chars:
            current_chunk = (current_chunk + " " + sent).strip() if current_chunk else sent
        else:
            chunks.append(current_chunk)
            current_chunk = sent
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def average_embeddings(embeddings: List[List[float]]) -> List[float]:
    """Average multiple chunk embeddings into a single vector."""
    if not embeddings:
        return []
    dim = len(embeddings[0])
    sum_vec = [0.0] * dim
    for emb in embeddings:
        for i, val in enumerate(emb):
            sum_vec[i] += val
    return [v / len(embeddings) for v in sum_vec]

#########################
# OSEnv Class
#########################
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list: List[str] = []
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main config from {config_file}")
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded creds from {creds_file}")
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")

        if self.get("PROXY_ENABLED", "False") == "True":
            self.set_proxy()
            logger.info("Proxy set")

        if self.get("SECURED_ENDPOINTS", "False") == "True":
            logger.info("Fetching Azure AD token")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        import os
        if not os.path.isabs(dotenvfile):
            dotenvfile = os.path.abspath(dotenvfile)
        if is_file_readable(dotenvfile):
            from dotenv import dotenv_values
            temp_dict = dotenv_values(dotenvfile)
            for k, v in temp_dict.items():
                self.set(k, v, print_val)

    def set_certificate_path(self, certificate_path: str) -> None:
        import os
        if not os.path.isabs(certificate_path):
            certificate_path = os.path.abspath(certificate_path)
        if not is_file_readable(certificate_path):
            raise Exception("Cert file missing/not readable")
        self.set("REQUESTS_CA_BUNDLE", certificate_path)
        self.set("SSL_CERT_FILE", certificate_path)
        self.set("CURL_CA_BUNDLE", certificate_path)

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        import os
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        import os
        return os.environ.get(var_name, default)

    def set_proxy(self) -> None:
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            raise ValueError("Missing proxy credentials for AD_USERNAME, AD_USER_PW, HTTPS_PROXY_DOMAIN.")
        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)

    def get_azure_token(self) -> str:
        from azure.identity import ClientSecretCredential
        tenant_id = self.get("AZURE_TENANT_ID")
        client_id = self.get("AZURE_CLIENT_ID")
        client_secret = self.get("AZURE_CLIENT_SECRET")
        credential = ClientSecretCredential(tenant_id, client_id, client_secret)
        token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
        token_val = token_obj.token
        self.set("AZURE_TOKEN", token_val, print_val=False)
        return token_val

#########################
# AzureChatbot (LangChain 0.3.18)
#########################
class AzureChatbot:
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        logger.info("Initializing AzureChatbot...")
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()

    def _setup_chat_model(self) -> None:
        azure_openai_api_base = self.env.get("AZURE_OPENAI_API_BASE")
        if not azure_openai_api_base:
            raise ValueError("Missing AZURE_OPENAI_API_BASE in env. Rename old AZURE_OPENAI_ENDPOINT if needed.")

        azure_openai_api_version = self.env.get("AZURE_OPENAI_API_VERSION", "2023-03-15-preview")
        deployment_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))

        if not self.env.token:
            raise ValueError("Missing Azure AD token. SECURED_ENDPOINTS=True or credentials not set.")

        # For LangChain 0.3.18:
        self.llm = AzureChatOpenAI(
            azure_openai_api_base=azure_openai_api_base,
            azure_openai_api_version=azure_openai_api_version,
            azure_ad_token=self.env.token,  # AD token
            deployment_name=deployment_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )

        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        logger.info("AzureChatOpenAI (LangChain 0.3.18) initalized")

    def validate_matches(self, query: str, matches: List[Dict[str, Any]]) -> str:
        prompt = f"You are an expert validation agent.\nQuery: {query}\n\nCandidate matches:\n"
        for idx, match in enumerate(matches, start=1):
            prompt += f"{idx}. Name: {match.get('name')}\n   Definition: {match.get('definition')}\n"
        prompt += "\nIf these are good, reply 'OK'. Otherwise, suggest a better match."

        try:
            response = self.conversation.predict(input=prompt)
            return response.strip()
        except Exception as e:
            logger.error(f"Validation error: {str(e)}")
            return "Validation failed"

    def chat(self, message: str) -> str:
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            response = self.conversation.predict(input=message)
            return response
        except Exception as e:
            logger.error(f"Chat error: {str(e)}")
            return f"Error: {str(e)}"

#########################
# VectorStoreManager (Row-by-Row Ingestion)
#########################
class VectorStoreManager:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(name="knowledge_base")

    def add_documents(self, documents: List[Dict[str, Any]], embeddings: List[List[float]]):
        """
        Instead of big batches, ingest row by row in a loop to avoid 'batch size exceeded'.
        """
        if len(documents) != len(embeddings):
            raise ValueError("Docs/embeddings mismatch.")

        for i, doc in enumerate(documents):
            emb = embeddings[i]
            if not emb:
                # skip empty embedding
                continue

            # Single row add
            row_id = str(uuid.uuid4())
            meta = {"name": doc["name"], "definition": doc["definition"]}
            content = doc["name"] + " " + doc["definition"]

            self.collection.add(
                ids=[row_id],
                documents=[content],
                embeddings=[emb],
                metadatas=[meta]
            )

    def query(self, query_embedding: List[float], n_results: int = 4) -> List[Dict[str, Any]]:
        """
        Flatten in case "metadatas" is nested
        """
        results = self.collection.query(query_embedding, n_results=n_results)
        out = []
        if results and "metadatas" in results:
            for metas_for_one_query in results["metadatas"]:
                for meta in metas_for_one_query:
                    out.append(meta)
        return out

#########################
# Main with Row-by-Row Ingestion
#########################
def main():
    """
    1) Loads environment from config & creds
    2) Creates AzureChatbot (LangChain 0.3.18) with azure_ad_token
    3) For each row in knowledge CSV:
       - chunk -> embed -> average -> add doc row-by-row to Chroma
    4) For each row in query CSV:
       - chunk -> embed -> average -> query top-4 -> validate
    5) Save final CSV
    """
    from pathlib import Path
    if '__file__' in globals():
        base_path = Path(__file__).resolve().parent
    else:
        base_path = Path.cwd()
    base_dir = base_path / "env"

    config_path = str(base_dir / "config.env")
    creds_path = str(base_dir / "credentials.env")
    cert_path = str(base_dir / "cacert.pem")

    for f in [config_path, creds_path, cert_path]:
        if not os.path.exists(f):
            print(f"Missing required file:", f)
            return

    logger.info("Init environment (OSEnv)...")
    env = OSEnv(config_path, creds_path, cert_path)

    logger.info("Init AzureChatbot (LangChain 0.3.18, azure_ad_token).")
    chatbot = AzureChatbot(config_path, creds_path, cert_path)

    logger.info("Init EmbeddingClient (pydantic doc).")
    embedding_client = EmbeddingClient()

    logger.info("Init local ChromaDB store (row-by-row ingest).")
    vector_store = VectorStoreManager(persist_dir="./chroma_db")

    # Step 1: Knowledge CSV
    kb_csv = input("Enter path to knowledge base CSV: ").strip()
    kb_csv = remove_invisible_chars(kb_csv)
    kb_df = read_csv_flexible(kb_csv)
    if not {"name", "definition"}.issubset(set(kb_df.columns)):
        raise ValueError("Knowledge CSV must have 'name' and 'definition' columns.")

    # We'll build docs & embeddings row by row
    kb_docs = []
    kb_embeddings = []
    for _, row in kb_df.iterrows():
        name = str(row["name"])
        definition = str(row["definition"])
        combined_text = name + " " + definition

        # 1) chunk
        chunks = stable_chunk_text(combined_text, max_chars=3000)
        chunk_embs = []
        for ch in chunks:
            doc_obj = Document(text=ch)
            doc_with_emb = embedding_client.generate_embeddings(doc_obj)
            if doc_with_emb.embedding:
                chunk_embs.append(doc_with_emb.embedding)
        # 2) average
        final_emb = average_embeddings(chunk_embs)

        # 3) single doc + embedding
        kb_docs.append({"name": name, "definition": definition})
        kb_embeddings.append(final_emb)

    # row-by-row ingestion
    vector_store.add_documents(kb_docs, kb_embeddings)

    # Step 2: Query CSV
    query_csv = input("Enter path to query CSV: ").strip()
    query_csv = remove_invisible_chars(query_csv)
    query_df = read_csv_flexible(query_csv)
    if not {"name", "definition"}.issubset(set(query_df.columns)):
        raise ValueError("Query CSV must have 'name' and 'definition' columns.")

    results = []
    for index, row in query_df.iterrows():
        q_name = str(row["name"])
        q_def = str(row["definition"])
        combined_text = q_name + " " + q_def

        q_chunks = stable_chunk_text(combined_text, max_chars=3000)
        q_chunk_embs = []
        for ch in q_chunks:
            doc_q = Document(text=ch)
            doc_q_emb = embedding_client.generate_embeddings(doc_q)
            if doc_q_emb.embedding:
                q_chunk_embs.append(doc_q_emb.embedding)
        final_q_emb = average_embeddings(q_chunk_embs)

        if not final_q_emb:
            logger.error(f"Embedding error for query row name={q_name[:20]}...")
            matches = []
            validation = "Embedding error"
        else:
            matches = vector_store.query(final_q_emb, n_results=4)
            validation = chatbot.validate_matches(combined_text, matches)

        results.append({
            "query_name": q_name,
            "query_definition": q_def,
            "matches": json.dumps(matches),
            "validation": validation
        })

    # Step 3: Save final CSV
    output_csv = input("Enter path for output CSV (e.g. output.csv): ").strip()
    output_csv = remove_invisible_chars(output_csv)
    pd.DataFrame(results).to_csv(output_csv, index=False)
    logger.info(f"Results saved to {output_csv}")
    print("Workflow completed successfully.")

if __name__ == "__main__":
    main()
