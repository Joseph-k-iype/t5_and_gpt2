def rag_quality_check_item(self, name: str, definition: str, top_k: int = 4) -> Dict[str, Any]:
    """
    Complete implementation with:
    - Proper CSV column handling
    - Combined name+definition processing
    - Top 4 results analysis
    - Full error handling
    """
    result_template = {
        "input": {"name": name, "definition": definition},
        "matches": [],
        "best_match": None,
        "final_rating": "Red",
        "final_reason": "No valid matches found"
    }

    try:
        # 1. Validate input
        if not name.strip() or not definition.strip():
            raise ValueError("Empty name or definition")

        # 2. Create combined query
        query_text = f"NAME: {name}\nDEFINITION: {definition}"

        # 3. Vector similarity search
        try:
            vector_results = self.vs.similarity_search_with_score(query_text, k=top_k)
        except Exception as e:
            logger.error(f"Vector search failed: {str(e)}")
            vector_results = []

        # 4. Process matches
        valid_candidates = []
        for doc, score in vector_results:
            try:
                # Get match details from metadata
                match_name = doc.metadata.get('name', '')
                match_def = doc.metadata.get('definition', '')
                
                # Calculate confidence (1 - cosine distance)
                confidence = max(0.0, round(1.0 - float(score), 2))
                
                # Create candidate text
                candidate_text = f"NAME: {match_name}\nDEFINITION: {match_def}"

                # Get quality assessment
                llm_response = self.quality_chain.chain.run(
                    user_input=query_text,
                    candidate_doc=candidate_text
                ).strip()

                # Parse rating and reason
                rating, reason = self._parse_llm_response(llm_response)

                # Store match details
                match_data = {
                    "name": match_name,
                    "definition": match_def,
                    "confidence": confidence,
                    "rating": rating,
                    "reason": reason,
                    "metadata": doc.metadata
                }
                result_template["matches"].append(match_data)

                # Track valid candidates
                if rating != "Red":
                    valid_candidates.append((confidence, match_data))

            except Exception as match_error:
                logger.error(f"Match processing error: {str(match_error)}")
                result_template["matches"].append({
                    "error": str(match_error),
                    "metadata": doc.metadata
                })

        # 5. Determine best match
        if valid_candidates:
            # Sort by confidence descending
            valid_candidates.sort(key=lambda x: x[0], reverse=True)
            best_confidence, best_match = valid_candidates[0]
            
            result_template.update({
                "best_match": best_match,
                "final_rating": best_match["rating"],
                "final_reason": f"{best_match['reason']} (Confidence: {best_confidence})"
            })

        # Ensure exactly top_k matches
        result_template["matches"] = result_template["matches"][:top_k]

    except Exception as e:
        logger.error(f"RAG quality check failed: {str(e)}")
        result_template["error"] = str(e)

    return result_template

def _parse_llm_response(self, raw_response: str) -> tuple[str, str]:
    """Robust parsing of LLM response with fallbacks"""
    try:
        # Extract JSON substring
        start = raw_response.find('{')
        end = raw_response.rfind('}') + 1
        json_str = raw_response[start:end] if start != -1 and end != 0 else '{}'
        
        # Case-insensitive key matching
        response_data = json.loads(json_str)
        normalized = {k.strip().lower(): v for k, v in response_data.items()}
        
        rating = normalized.get('rating', 'Red').capitalize()
        reason = normalized.get('reason', 'No reason provided')
        
        # Validate rating
        if rating not in {"Green", "Amber", "Red"}:
            rating = "Red"
            reason = f"Invalid rating value: {rating}"

        return rating, reason
        
    except json.JSONDecodeError:
        return "Red", "Invalid JSON format"
    except Exception as e:
        return "Red", f"Parse error: {str(e)}"

class KnowledgeBase:
    def _read_csv_and_build(self):
        """Fixed CSV processing with proper column handling"""
        # Detect encoding
        with open(self.csv_path, "rb") as f:
            rawdata = f.read(100000)
        encoding = chardet.detect(rawdata)['encoding'] or 'utf-8'

        # Read CSV
        df = pd.read_csv(self.csv_path, encoding=encoding, keep_default_na=False)
        
        # Validate columns
        required_columns = {'name', 'definition'}
        if not required_columns.issubset(df.columns):
            missing = required_columns - set(df.columns)
            raise ValueError(f"CSV missing required columns: {missing}")

        # Process rows
        for _, row in df.iterrows():
            # Get values with fallbacks
            name_val = str(row['name']).strip()
            def_val = str(row['definition']).strip()
            doc_id = str(row.get('id', uuid.uuid4()))

            # Create document with combined content
            combined_text = f"NAME: {name_val}\nDEFINITION: {def_val}"
            doc = LC_Document(
                page_content=combined_text,
                metadata={
                    "name": name_val,
                    "definition": def_val,
                    "id": doc_id
                }
            )
            self.docs.append(doc)
            
            # Add to graph
            self.graph.add_node(name_val, type="concept", id=doc_id)
            self.graph.add_node(def_val, type="definition")
            self.graph.add_edge(name_val, def_val, label="defined_as")
