# main_rag.py

import os
import sys
import csv
import uuid
import logging
from pathlib import Path
from typing import Optional

# 1) Import your existing classes (without modifying them)
from genai_env_setup import OSEnv
from azoai_embedding_client import EmbeddingClient, Document as AzoaiDocument

# Additional LangChain + Community imports
from langchain.chat_models import AzureChatOpenAI
from langchain.docstore.document import Document as LC_Document
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.agents import Tool, ZeroShotAgent, AgentExecutor
from langchain_community.graphs.index_creator import GraphIndexCreator
from langchain_community.graphs.networkx_graph import NetworkxEntityGraph
from langchain_community.chains.graph_qa.base import GraphQAChain
from chromadb.config import Settings

# For user input loop, logging, etc.
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

###############################################################################
# KnowledgeBase: loads CSV with columns 'name' and 'definition'
###############################################################################
class KnowledgeBase:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.docs = self._load_csv_as_documents()

    def _load_csv_as_documents(self):
        docs = []
        with open(self.csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                text = f"{row['name']}: {row['definition']}"
                doc_id = str(uuid.uuid4())
                metadata = {"name": row["name"], "id": doc_id}
                docs.append(LC_Document(page_content=text, metadata=metadata))
        return docs

###############################################################################
# Adapter to wrap your EmbeddingClient for LangChain
###############################################################################
class EmbeddingClientLangChainAdapter(Embeddings):
    """
    Adapts your `EmbeddingClient` so it can be used with LangChain's Chroma, etc.
    """
    def __init__(self, embedding_client: EmbeddingClient, embeddings_model: str = "text-embedding-3-large"):
        self.embedding_client = embedding_client
        self.embeddings_model = embeddings_model

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        embeddings = []
        for txt in texts:
            doc = AzoaiDocument(text=txt, id="")
            updated_doc = self.embedding_client.generate_embeddings(doc, embeddings_model=self.embeddings_model)
            embeddings.append(updated_doc.embedding)
        return embeddings

    def embed_query(self, text: str) -> list[float]:
        doc = AzoaiDocument(text=text, id="")
        updated_doc = self.embedding_client.generate_embeddings(doc, embeddings_model=self.embeddings_model)
        return updated_doc.embedding

###############################################################################
# AzureChatRAG: similar pattern to your "AzureChatbot" class, but with Graph RAG
###############################################################################
class AzureChatRAG:
    """
    Combines:
      - OSEnv for environment setup
      - AzureChatOpenAI for chat model
      - Chroma vector store for semantic search
      - Graph Index for knowledge-graph QA
      - Multi-tool agent that can call either graph or vector search
    """

    def __init__(self, config_file: str, creds_file: str, cert_file: str, csv_path: str):
        """
        - config_file, creds_file, cert_file => pass to OSEnv
        - csv_path => path to CSV with columns 'name','definition'
        """
        logger.info("Initializing AzureChatRAG...")
        # 1) Set up environment
        self.env = OSEnv(config_file, creds_file, cert_file)
        # self.env.token => The azure_ad_token or similar, if SECURED_ENDPOINTS is True

        # 2) Create Chat model
        self._setup_chat_model()

        # 3) Load knowledge base from CSV
        self.kb = KnowledgeBase(csv_path)

        # 4) Create embeddings + vector store
        self._setup_vectorstore()

        # 5) Build knowledge graph from the docs
        self._setup_graph()

        # 6) Build multi-tool agent
        self._setup_agent()

        logger.info("AzureChatRAG initialized successfully!")

    def _setup_chat_model(self) -> None:
        """Use environment variables to configure AzureChatOpenAI."""
        try:
            # Fallback logic for environment variables
            model_name = self.env.get("MODEL_NAME") or "gpt-4o-mini"
            api_version = self.env.get("API_VERSION") or "2023-05-15"
            azure_endpoint = self.env.get("AZURE_OPENAI_ENDPOINT") or ""
            # azure_ad_token => self.env.token
            temperature_str = self.env.get("MODEL_TEMPERATURE") or "0.7"
            max_tokens_str = self.env.get("MAX_TOKENS") or "800"

            # Convert numeric env strings to floats/ints
            temperature = float(temperature_str)
            max_tokens = int(max_tokens_str)

            # Construct AzureChatOpenAI
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                openai_api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token=self.env.token
            )
            logger.info("Chat model initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize chat model: {str(e)}")
            raise

    def _setup_vectorstore(self) -> None:
        """Create a Chroma vector store from the CSV docs."""
        try:
            # EmbeddingClient from azoai_embedding_client
            embedding_client = EmbeddingClient()
            adapter = EmbeddingClientLangChainAdapter(embedding_client)

            # Chroma config
            chroma_settings = Settings(
                anonymized_telemetry=False,
                persist_directory="chromadb-data"
            )

            self.vs = Chroma.from_documents(
                documents=self.kb.docs,
                embedding=adapter,
                collection_name="kb_collection",
                client_settings=chroma_settings
            )
            logger.info("Chroma vector store created successfully")
        except Exception as e:
            logger.error(f"Failed to set up vector store: {str(e)}")
            raise

    def _setup_graph(self) -> None:
        """Build a single knowledge graph by merging partial graphs from each doc."""
        try:
            graph_creator = GraphIndexCreator(llm=self.llm, graph_type=NetworkxEntityGraph)
            main_graph = graph_creator.graph_type()  # empty NetworkxEntityGraph
            for doc in self.kb.docs:
                partial_graph = graph_creator.from_text(doc.page_content)
                for triple in partial_graph.get_triples():
                    main_graph.add_triple(triple)
            self.graph = main_graph
            self.graph_qa_chain = GraphQAChain.from_llm(self.llm, graph=self.graph)
            logger.info("Knowledge graph created successfully")
        except Exception as e:
            logger.error(f"Failed to set up knowledge graph: {str(e)}")
            raise

    def _setup_agent(self) -> None:
        """Define Tools for Graph QA vs. Vector store search, create an agent."""
        try:
            # Tools
            graph_tool = Tool(
                name="GraphQATool",
                func=self._graph_qa,
                description="Query the knowledge graph for relationships or definitions"
            )
            vs_tool = Tool(
                name="VectorStoreSearch",
                func=self._vectorstore_search,
                description="Semantic search with confidence, rating, reason"
            )
            self.tools = [graph_tool, vs_tool]

            prefix = "You are an AI assistant with access to the following tools:"
            suffix = "Begin!"
            prompt = ZeroShotAgent.create_prompt(
                self.tools,
                prefix=prefix,
                suffix=suffix,
                input_variables=["input"]
            )
            agent = ZeroShotAgent(
                llm=self.llm,
                tools=self.tools,
                prompt=prompt,
                verbose=True
            )
            self.agent_executor = AgentExecutor.from_agent_and_tools(
                agent=agent,
                tools=self.tools,
                verbose=True
            )
            logger.info("Multi-tool agent created successfully")
        except Exception as e:
            logger.error(f"Failed to set up agent: {str(e)}")
            raise

    def _graph_qa(self, query: str) -> str:
        """Run a query through the GraphQAChain."""
        return self.graph_qa_chain.run(query)

    def _vectorstore_search(self, query: str) -> str:
        """Search the Chroma vector store for top matches (R/A/G rating)."""
        results = self.vs.similarity_search_with_score(query, k=3)
        if not results:
            return "No relevant matches found."

        lines = []
        for idx, (doc, score) in enumerate(results, start=1):
            confidence = max(0.0, min(1.0, 1.0 - score))
            if confidence >= 0.8:
                rating = "Green"
            elif confidence >= 0.5:
                rating = "Amber"
            else:
                rating = "Red"
            reason = (
                f"Confidence is {confidence:.2f}, which is {rating}. "
                f"Definition matched: {doc.page_content}"
            )
            line = (
                f"Match #{idx}\n"
                f"Name: {doc.metadata.get('name','Unknown')}\n"
                f"ID: {doc.metadata.get('id','No ID')}\n"
                f"Confidence: {confidence:.2f}\n"
                f"Rating: {rating}\n"
                f"Reason: {reason}\n"
            )
            lines.append(line)
        return "\n".join(lines)

    def chat(self, user_input: str) -> str:
        """
        For demonstration: run a user query through the multi-tool agent.
        The agent decides whether to use GraphQATool or VectorStoreSearch.
        """
        if not user_input.strip():
            return "Please provide a non-empty message."
        try:
            response = self.agent_executor.run(user_input)
            return response
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

###############################################################################
# main() replicates the pattern of checking for config, creds, cert, then looping
###############################################################################
def main():
    # 1) Setup paths to config files, etc.
    script_dir = os.path.dirname(os.path.abspath(__file__))
    env_dir = os.path.join(script_dir, "..", "env")  # Example structure
    config_path = os.path.join(env_dir, "config.env")
    creds_path = os.path.join(env_dir, "credentials.env")
    cert_path = os.path.join(env_dir, "cacert.pem")

    # CSV path for knowledge base
    csv_path = os.path.join(script_dir, "knowledgebase.csv")

    # 2) Check for missing files
    required_files = {
        "config.env": config_path,
        "credentials.env": creds_path,
        "cacert.pem": cert_path,
        "knowledgebase.csv": csv_path
    }
    missing = []
    for name, path in required_files.items():
        if not os.path.exists(path):
            missing.append(name)
    if missing:
        print(f"Missing required files: {', '.join(missing)}")
        sys.exit(1)

    # 3) Initialize AzureChatRAG
    try:
        print("Initializing AzureChatRAG with environment + CSV knowledge base...")
        rag_bot = AzureChatRAG(
            config_file=config_path,
            creds_file=creds_path,
            cert_file=cert_path,
            csv_path=csv_path
        )
        print("AzureChatRAG initialized successfully!\n")
    except Exception as e:
        print(f"Error initializing: {str(e)}")
        sys.exit(1)

    # 4) Command loop
    print("Type 'quit' to exit, 'env' to list environment variables, or ask a question.\n")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Goodbye!")
            break
        if user_input.lower() == "env":
            rag_bot.env.list_env_vars()
            continue
        response = rag_bot.chat(user_input)
        print(f"\nBot: {response}\n")

if __name__ == "__main__":
    main()
