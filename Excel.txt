class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:
        try:
            # Log the exact input
            print(f"DEBUG - Parser received text: {text}")
            
            # More permissive pattern matching
            pattern = (
                r"(?:Regex|Pattern):\s*([^\n]+)\s*\n"
                r"(?:Reason|Explanation):\s*([^\n]+(?:\n(?!\s*(?:Confidence|Status):)[^\n]+)*)\s*\n"
                r"Confidence:\s*(\d+)[^\n]*\n"
                r"Status:\s*(RED|AMBER|GREEN)"
            )
            
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            
            if not match:
                print(f"DEBUG - No match found in text: {text}")
                return {
                    "name": "",
                    "regex": ".*",  # Default to match anything if parsing fails
                    "reason": f"Failed to parse output. Using default pattern. Original text: {text[:100]}...",
                    "confidence": 30,
                    "status": "AMBER"
                }
            
            # Extract values with detailed logging
            regex = match.group(1).strip()
            reason = match.group(2).strip()
            confidence = int(match.group(3))
            status = match.group(4).upper()
            
            print(f"DEBUG - Parsed values: regex={regex}, confidence={confidence}, status={status}")
            
            return {
                "name": "",
                "regex": regex,
                "reason": reason,
                "confidence": min(max(confidence, 0), 100),
                "status": status if status in ("RED", "AMBER", "GREEN") else "AMBER"
            }
            
        except Exception as e:
            print(f"DEBUG - Parser error: {str(e)}")
            # Return a safe default instead of raising an error
            return {
                "name": "",
                "regex": ".*",
                "reason": f"Error parsing output: {str(e)}. Using default pattern.",
                "confidence": 30,
                "status": "AMBER"
            }

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a flexible and generic regex pattern for identifying the term '{name}' in text.
            
            Term Details:
            - Name: {name}
            - Definition: {definition}
            - Related Context: {related_terms}

            Guidelines for the regex pattern:
            1. Must be a valid regex pattern that matches the term and its variations
            2. Include common variations:
               - Different word separators (spaces, hyphens, underscores)
               - Optional parts that might be omitted
               - Common abbreviations
               - Case insensitivity
            3. Use word boundaries where appropriate
            4. Make it as generic as possible while maintaining accuracy

            Your response MUST follow this EXACT format:
            Regex: [your pattern here]
            Reason: [explain why this pattern will work]
            Confidence: [number between 0-100]
            Status: [RED, AMBER, or GREEN]

            Example response:
            Regex: (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b[- _]*(?:id|identifier|code|ref)?\\b
            Reason: Matches 'counterparty' and its variations, with optional identifier terms
            Confidence: 85
            Status: GREEN
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_single_term(self, term: dict) -> dict:
        try:
            print(f"DEBUG - Analyzing term: {term.get('name', 'unknown')}")
            
            # Prepare related terms info
            related_terms = term.get('related_terms', [])
            related_terms_info = "\n".join(
                [f"- {rt.get('definition', '')} (Example: {rt.get('example', '')})" 
                 for rt in related_terms]
            ) if related_terms else "No related terms provided."
            
            # Get LLM response
            chain_input = {
                "name": term.get("name", ""),
                "definition": term.get("definition", ""),
                "related_terms": related_terms_info
            }
            print(f"DEBUG - Chain input: {chain_input}")
            
            result = self.chain.invoke(chain_input)
            print(f"DEBUG - Chain result: {result}")
            
            # Ensure result is a dictionary
            if not isinstance(result, dict):
                print(f"DEBUG - Unexpected result type: {type(result)}")
                raise ValueError(f"Unexpected result type: {type(result)}")
            
            # Add the term name to the result
            result["name"] = term.get("name", "")
            
            # Validate the regex pattern
            try:
                re.compile(result["regex"])
            except re.error as e:
                print(f"DEBUG - Invalid regex pattern: {str(e)}")
                result["confidence"] = max(0, result.get("confidence", 0) - 50)
                result["status"] = "RED"
                result["reason"] += f" (Invalid regex pattern: {str(e)})"
            
            return result
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_single_term: {str(e)}")
            return {
                "name": term.get("name", "unknown"),
                "regex": ".*",
                "reason": f"Processing error: {str(e)}",
                "confidence": 30,
                "status": "AMBER"
            }

    def analyze_terms(self, input_path: str) -> dict:
        try:
            print(f"DEBUG - Reading input file: {input_path}")
            with open(input_path) as f:
                terms = json.load(f)
            
            if not isinstance(terms, list):
                terms = [terms]  # Handle single term case
            
            results = []
            for term in terms:
                result = self.analyze_single_term(term)
                results.append(result)
            
            return self._create_final_result(results)
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_terms: {str(e)}")
            return {
                "results": [],
                "summary": {
                    "total_terms": 0,
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }

    def _create_final_result(self, results: List[dict]) -> dict:
        try:
            print(f"DEBUG - Creating final result for {len(results)} terms")
            
            if not results:
                return {
                    "results": [],
                    "summary": {
                        "total_terms": 0,
                        "red_status": 0,
                        "amber_status": 0,
                        "green_status": 0,
                        "average_confidence": 0
                    }
                }
            
            status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
            total_confidence = 0
            
            for result in results:
                status = result.get("status", "AMBER")
                status_counts[status] += 1
                total_confidence += result.get("confidence", 0)
            
            return {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": status_counts["RED"],
                    "amber_status": status_counts["AMBER"],
                    "green_status": status_counts["GREEN"],
                    "average_confidence": total_confidence // len(results) if results else 0
                }
            }
            
        except Exception as e:
            print(f"DEBUG - Error in _create_final_result: {str(e)}")
            return {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }





___________________________________________________________________________________

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a flexible and generic regex pattern for identifying '{name}' in text.
            
            Term: {name}
            Definition: {definition}
            Context: {related_terms}

            Guidelines for the regex pattern:
            1. Make it as generic as possible while maintaining accuracy
            2. Include common variations:
               - Different word separators (spaces, hyphens, underscores)
               - Optional parts that might be omitted
               - Common abbreviations and shortcuts
               - Case insensitivity
            3. Allow for flexible word ordering where appropriate
            4. Consider boundary conditions (word boundaries, spacing)
            5. Balance between coverage and precision

            Provide your response in EXACTLY this format:
            Regex: <your_regex_pattern>
            Reason: <explain why this pattern will work>
            Confidence: <0-100 score>
            Status: <RED|AMBER|GREEN>

            Example of a good generic pattern:
            Regex: (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b[- _]*(?:id|identifier|number|code|ref)?\\b
            Reason: Matches various forms including just 'counterparty' or 'cp', with optional identifier terms
            Confidence: 90
            Status: GREEN
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    # Rest of the class implementation remains the same...

    def analyze_terms(self, input_path: str) -> dict:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                # Add logging for debugging
                logger.debug(f"Processing term: {term['name']}")
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                if not isinstance(result, dict):
                    logger.error(f"Unexpected result type: {type(result)}")
                    raise ValueError(f"Unexpected result type: {type(result)}")
                
                result["name"] = term["name"]
                
                # Validate regex
                try:
                    re.compile(result["regex"])
                except re.error as e:
                    logger.error(f"Invalid regex pattern: {str(e)}")
                    result["confidence"] = max(0, result["confidence"] - 50)
                    result["status"] = "RED"
                    result["reason"] += f" (Invalid regex pattern: {str(e)})"
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing term {term['name']}: {str(e)}")
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)




















class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:  # Changed to return dict instead of RegexResult
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            confidence = int(match.group("confidence"))
            if not 0 <= confidence <= 100:
                confidence = max(0, min(100, confidence))
            
            status = match.group("status").strip()
            if status not in ('RED', 'AMBER', 'GREEN'):
                status = 'RED'
            
            return {
                "name": "",  # Will be filled later
                "regex": match.group("regex").strip(),
                "reason": match.group("reason").strip(),
                "confidence": confidence,
                "status": status
            }
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()  # No pydantic object needed
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> dict:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Result is already a dict, just update the name
                result["name"] = term["name"]
                
                # Validate regex
                try:
                    re.compile(result["regex"])
                except re.error:
                    result["confidence"] = max(0, result["confidence"] - 50)
                    result["status"] = "RED"
                    result["reason"] += " (Invalid regex pattern)"
                
                results.append(result)
                
            except Exception as e:
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[dict]) -> dict:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res["status"]] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r["confidence"] for r in results) // len(results)
            }
        }






# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        # Get result as dictionary
        result = app.run_analysis("input.json")
        
        # Result is already a dict, so we can dump it directly
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)


if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(State)
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            # Get the analysis result as a dictionary
            analysis_result = self.analyzer.analyze_terms(state["input_path"])
            state["result"] = analysis_result
            return state

        def finalize_output(self, state: dict) -> dict:
            # Access dictionary keys directly
            if isinstance(state["result"], dict) and "results" in state["result"]:
                logger.info(f"Processed {len(state['result']['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            # Return the result dictionary directly
            return final_state["result"]

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> dict:  # Changed return type to dict
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Convert Pydantic model to dict
                result_dict = {
                    "name": term["name"],
                    "regex": result.regex,
                    "reason": result.reason,
                    "confidence": result.confidence,
                    "status": result.status
                }
                
                # Validate regex
                try:
                    re.compile(result_dict["regex"])
                except re.error:
                    result_dict["confidence"] = max(0, result_dict["confidence"] - 50)
                    result_dict["status"] = "RED"
                    result_dict["reason"] += " (Invalid regex pattern)"
                
                results.append(result_dict)
                
            except Exception as e:
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[dict]) -> dict:  # Changed return type to dict
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res["status"]] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r["confidence"] for r in results) // len(results)
            }
        }

if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(State)
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:  # Changed return type to dict
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            return final_state["result"]



class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:  # Changed return type to dict
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        # Result is already a dict, so we can directly dump it
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)



_________________________________________________________________________________________________________________________________________________


[
  {
    "name": "Python",
    "definition": "A high-level programming language",
    "related_terms": [
      {
        "name": "scripting",
        "definition": "A style of programming that automates tasks",
        "example": "Example: Python scripts can automate file management."
      },
      {
        "name": "interpreted",
        "definition": "Executed line-by-line rather than compiled",
        "example": "Example: Python code is executed in an interpreter."
      }
    ]
  },
  {
    "name": "Java",
    "definition": "A widely-used, class-based programming language",
    "related_terms": [
      {
        "name": "OOP",
        "definition": "Object-Oriented Programming paradigm",
        "example": "Example: Java’s design encourages encapsulation and inheritance."
      }
    ]
  }
]


if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            # Initialize StateGraph with a defined state type in the constructor
            self.workflow = StateGraph(State)  # State should be your state type definition
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            config = {"input_path": input_path}
            result = self.workflow.compile().run(config)
            return result["result"]

# ... [Keep all existing imports and utility functions] ...

# Add these additional imports
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import BaseOutputParser
from typing import Tuple
import re
try:
    from langgraph.graph import END, StateGraph
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

# ================================
# Pydantic Models for Analysis Results
# ================================
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str

    @field_validator('confidence')
    def validate_confidence(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('Confidence must be between 0-100')
        return v

    @field_validator('status')
    def validate_status(cls, v):
        if v not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Status must be RED, AMBER, or GREEN')
        return v

class TermAnalysisResult(BaseModel):
    results: List[RegexResult]
    summary: Dict[str, int]

# ================================
# Regex Processing Components
# ================================
class RegexOutputParser(PydanticOutputParser):
    def parse(self, text: str) -> RegexResult:
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            return RegexResult(
                name="",  # Will be filled later
                regex=match.group("regex").strip(),
                reason=match.group("reason").strip(),
                confidence=int(match.group("confidence")),
                status=match.group("status").strip()
            )
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> TermAnalysisResult:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Validate and adjust confidence
                try:
                    re.compile(result.regex)
                except re.error:
                    result.confidence = max(0, result.confidence - 50)
                    result.status = "RED"
                    result.reason += " (Invalid regex pattern)"
                
                results.append(RegexResult(
                    name=term["name"],
                    regex=result.regex,
                    reason=result.reason,
                    confidence=result.confidence,
                    status=result.status
                ))
                
            except Exception as e:
                results.append(RegexResult(
                    name=term["name"],
                    regex="",
                    reason=f"Processing error: {str(e)}",
                    confidence=0,
                    status="RED"
                ))
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[RegexResult]) -> TermAnalysisResult:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res.status] += 1
        
        return TermAnalysisResult(
            results=results,
            summary={
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r.confidence for r in results) // len(results)
            }
        )

# ================================
# LangGraph Workflow (if available)
# ================================
if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(update_state_type=dict)
            
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            return self.workflow.invoke({"input_path": input_path})["result"]

# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> TermAnalysisResult:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        print(json.dumps(result.dict(), indent=2))
        with open("output.json", "w") as f:
            json.dump(result.dict(), f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)
