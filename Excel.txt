import os
import json
import uuid
import logging
import chardet
import pandas as pd
from typing import List, Dict, Any
from pathlib import Path

from dotenv import dotenv_values
from azure.identity import ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel

# LangChain
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

###############################################################################
# GLOBAL CONSTANTS
###############################################################################
CONFIG_FILE = "config.env"
CREDS_FILE = "credentials.env"
CERT_FILE  = "cacert.pem"
CSV_FILE   = "knowledgebase.csv"  # Must have columns: name, definition

###############################################################################
# 1) OSEnv
###############################################################################
class OSEnv:
    """Environment variable loader for Azure + config, no embeddings needed."""
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self._bulk_set(config_file, print_val=True)
        logger.info(f"Loaded main configuration from {config_file}")

        self._bulk_set(creds_file, print_val=False)
        logger.info(f"Loaded credentials from {creds_file}")

        self._set_certificate_path(certificate_path)
        logger.info("Certificate path configured")

        if self.get("PROXY_ENABLED","False").lower() == "true":
            self._set_proxy()
            logger.info("Proxy configured")

        if self.get("SECURED_ENDPOINTS","False").lower() == "true":
            logger.info("Securing endpoints")
            self.token = self._get_azure_token()
        else:
            self.token = None

    def _bulk_set(self, dotenvfile: str, print_val: bool):
        dotenvfile = os.path.abspath(dotenvfile)
        if not os.path.isfile(dotenvfile):
            logger.warning(f"No such env file: {dotenvfile}")
            return
        logger.info(f"Loading env from {dotenvfile}")
        temp_dict = dotenv_values(dotenvfile)
        for k, v in temp_dict.items():
            self.set(k, v, print_val=print_val)

    def set(self, var_name: str, val: str, print_val: bool=True):
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: str=None) -> str:
        return os.environ.get(var_name, default)

    def _set_certificate_path(self, certificate_path: str):
        certificate_path = os.path.abspath(certificate_path)
        if not os.path.isfile(certificate_path):
            logger.warning(f"Certificate file missing: {certificate_path}")
            return
        self.set("REQUESTS_CA_BUNDLE", certificate_path)
        self.set("SSL_CERT_FILE", certificate_path)
        self.set("CURL_CA_BUNDLE", certificate_path)

    def _set_proxy(self):
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            logger.warning("Missing proxy credentials, skipping proxy setup.")
            return
        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)

    def _get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID",""),
                client_id=self.get("AZURE_CLIENT_ID",""),
                client_secret=self.get("AZURE_CLIENT_SECRET","")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token acquired successfully")
            return token.token
        except Exception as e:
            logger.error(f"Failed to get Azure token: {str(e)}")
            return ""

    def list_env_vars(self):
        for var in self.var_list:
            if var in {"AZURE_TOKEN","AD_USER_PW","AZURE_CLIENT_SECRET"}:
                logger.info(f"{var}: [HIDDEN]")
            else:
                logger.info(f"{var}: {os.environ.get(var,'')}")

###############################################################################
# 2) ISRClassificationChain
###############################################################################
class ISRClassificationChain:
    """
    Classify data into an ISR classification level, returning JSON:
    {
      "classification": "...",
      "reason": "...",
      "confidence": 0.0,
      "rating": "Green"/"Amber"/"Red"
    }
    """
    def __init__(self, llm):
        self.llm = llm

        template = """
You are an ISR classification agent. You receive a data item with:
Name + Definition

You must classify it into an ISR classification level (like "Top Secret","Secret","Confidential","Unclassified", etc.)
Then produce valid JSON with EXACTLY these fields:
{
  "classification": "...",
  "reason": "...",
  "confidence": 0.0,
  "rating": "Green"/"Amber"/"Red"
}

No extra text outside the JSON. Example:
{
  "classification": "Secret",
  "reason": "Because it meets secret data criteria",
  "confidence": 0.85,
  "rating": "Green"
}

User input: {user_input}
"""
        self.prompt = PromptTemplate(
            input_variables=["user_input"],
            template=template
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def _normalize_keys(self, data: dict) -> dict:
        new_data = {}
        for k,v in data.items():
            new_k = k.replace('"','').replace("'","").replace('\n','').replace('\r','').strip()
            new_data[new_k] = v
        return new_data

    def classify(self, user_input: str) -> dict:
        """
        Returns { classification, reason, confidence, rating } or fallback.
        """
        try:
            resp = self.chain.run(user_input=user_input).strip()

            # 1) direct parse
            try:
                data = json.loads(resp)
                data = self._normalize_keys(data)
                classification = str(data.get("classification","Unknown"))
                reason = str(data.get("reason","No reason"))
                confidence = float(data.get("confidence",0.0))
                rating = str(data.get("rating","Red"))
                return {
                    "classification": classification,
                    "reason": reason,
                    "confidence": confidence,
                    "rating": rating
                }
            except:
                pass

            # 2) substring parse
            start_idx = resp.find("{")
            end_idx = resp.rfind("}")
            if start_idx == -1 or end_idx == -1:
                return {
                    "classification":"Unknown",
                    "reason":"Failed to parse JSON (no braces)",
                    "confidence":0.0,
                    "rating":"Red"
                }
            json_str = resp[start_idx:end_idx+1]
            try:
                data = json.loads(json_str)
                data = self._normalize_keys(data)
                classification = str(data.get("classification","Unknown"))
                reason = str(data.get("reason","No reason"))
                confidence = float(data.get("confidence",0.0))
                rating = str(data.get("rating","Red"))
                return {
                    "classification": classification,
                    "reason": reason,
                    "confidence": confidence,
                    "rating": rating
                }
            except:
                return {
                    "classification":"Unknown",
                    "reason":"Failed to parse JSON substring",
                    "confidence":0.0,
                    "rating":"Red"
                }

        except Exception as e:
            logger.error(f"Error in classify: {str(e)}")
            return {
                "classification":"Unknown",
                "reason":"Parsing or chain error",
                "confidence":0.0,
                "rating":"Red"
            }

###############################################################################
# 3) MinimalAzureChatbot: purely classification, no embeddings needed
###############################################################################
class MinimalAzureChatbot:
    """
    - Loads environment
    - Creates an LLM (AzureChatOpenAI)
    - Has an ISRClassificationChain for classifying data
    - Also can do normal conversation if you like
    """
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        logger.info("Initializing MinimalAzureChatbot for classification.")
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_llm()

        # Optional conversation memory if you want chat
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)

        # The chain for ISR classification
        self.isr_chain = ISRClassificationChain(llm=self.llm)
        logger.info("MinimalAzureChatbot init complete.")

    def _setup_llm(self):
        model_name = self.env.get("MODEL_NAME","gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE","0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS","800"))
        api_version = self.env.get("API_VERSION","2023-05-15")
        azure_endpoint = self.env.get("AZURE_OPENAI_ENDPOINT","")
        azure_ad_token = self.env.token

        try:
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                openai_api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token=azure_ad_token
            )
            logger.info("LLM initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to init LLM: {str(e)}")
            raise

    def list_env(self):
        self.env.list_env_vars()

    def chat(self, message: str) -> str:
        """Optional normal conversation."""
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            return self.conversation.predict(input=message)
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

    def classify_isr_item(self, name: str, definition: str) -> dict:
        """
        Call the classification chain with name+definition, returns
        { classification, reason, confidence, rating }.
        """
        user_input = f"Name: {name}\nDefinition: {definition}"
        return self.isr_chain.classify(user_input)

###############################################################################
# 4) Main logic => read CSV with name+definition => classify
###############################################################################

def main():
    logger.info("Starting classification script with global constants for config, creds, cert, CSV.")
    
    # 1) Create chatbot
    chatbot = MinimalAzureChatbot(CONFIG_FILE, CREDS_FILE, CERT_FILE)

    # 2) Read CSV with chardet => name, definition
    if not os.path.isfile(CSV_FILE):
        print(f"CSV not found: {CSV_FILE}")
        return

    # detect encoding
    with open(CSV_FILE,"rb") as f:
        rawdata = f.read(100000)
    detect_result = chardet.detect(rawdata)
    encoding = detect_result["encoding"] or "utf-8"
    logger.info(f"Detected CSV encoding: {encoding}")

    df = pd.read_csv(CSV_FILE, encoding=encoding, keep_default_na=False)
    if "name" not in df.columns or "definition" not in df.columns:
        print("CSV must have columns: name, definition")
        return

    if "id" not in df.columns:
        df["id"] = [str(uuid.uuid4()) for _ in range(len(df))]

    # 3) For each row => classify
    results = []
    for _, row in df.iterrows():
        name_val = str(row["name"])
        def_val = str(row["definition"])
        item_id = str(row["id"])

        classification_dict = chatbot.classify_isr_item(name_val, def_val)
        # Add original row info
        classification_dict["id"] = item_id
        classification_dict["name"] = name_val
        classification_dict["definition"] = def_val

        results.append(classification_dict)

    # 4) Print final results
    print("\n=== Classification Results ===")
    for r in results:
        print(json.dumps(r, indent=2))

if __name__ == "__main__":
    main()
