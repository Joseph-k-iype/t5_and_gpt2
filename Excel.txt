#!/usr/bin/env python3
"""
Similarity Search with LanceDB + OpenAI Embeddings:

1) We have a "source.csv" => columns: [name, definition]
2) We have a "target.csv" => columns: [pbt-name, pbt-definition]
3) We'll index the target CSV in LanceDB with Azure/OpenAI embeddings once.
4) For each row in source.csv, we embed it, do top-K similarity search in LanceDB,
   and save the matches (with similarity scores) in a final CSV + JSON.

We also incorporate a custom requests session to mitigate any chunked-encoding or SSL issues.
See the "configure_requests_session" function for timeouts and retries.
"""

import os
import time
import json
import logging
import requests
from pathlib import Path
from typing import List, Dict
import pandas as pd
import numpy as np
from tqdm import tqdm

# Azure Identity if we do SECURED_ENDPOINTS=True
from azure.identity import ClientSecretCredential

# LanceDB + LangChain
from lancedb import connect
from langchain.vectorstores import LanceDB
from langchain.embeddings import OpenAIEmbeddings
from langchain.docstore.document import Document

# For environment loading
from dotenv import dotenv_values

# Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

###########################
# OSEnv Class
###########################
def is_file_readable(filepath: str) -> bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"File '{filepath}' not found or not readable.")
    return True

def str_to_bool(s: str) -> bool:
    ls = s.strip().lower()
    if ls == "true":
        return True
    elif ls == "false":
        return False
    else:
        raise ValueError(f"Invalid boolean: {s}")

class OSEnv:
    """
    Loads environment from config.env & credentials.env, sets up:
     - SSL certificate
     - Proxy (NO_PROXY merges)
     - Optional Azure AD token if SECURED_ENDPOINTS=True
    """
    def __init__(self, config_file:str, creds_file:str, cert_file:str):
        self.var_list = []
        self._bulk_load(config_file, print_vals=True)
        logger.info(f"Loaded config from {config_file}")
        self._bulk_load(creds_file, print_vals=False)
        logger.info(f"Loaded creds from {creds_file}")

        self._set_certificate(cert_file)
        logger.info("Certificate configured.")

        if str_to_bool(self.get("PROXY_ENABLED","false")):
            self._set_proxy()
            logger.info("Proxy configured.")

        if str_to_bool(self.get("SECURED_ENDPOINTS","false")):
            logger.info("Acquiring Azure AD token.")
            self.token = self._get_aad_token()
        else:
            self.token = None

    def _bulk_load(self, file_path:str, print_vals:bool):
        abs_path = os.path.abspath(file_path)
        if is_file_readable(abs_path):
            with open(abs_path) as f:
                lines = f.readlines()
            for line in lines:
                line=line.strip()
                if line and not line.startswith("#"):
                    if "=" in line:
                        k,v = line.split("=",1)
                        k=k.strip()
                        v=v.strip().strip("'\"")
                        os.environ[k] = v
                        if print_vals:
                            logger.info(f"Set {k}={v}")

    def _set_certificate(self, cert_file:str):
        cf_abs = os.path.abspath(cert_file)
        if is_file_readable(cf_abs):
            os.environ["REQUESTS_CA_BUNDLE"] = cf_abs
            os.environ["SSL_CERT_FILE"]      = cf_abs
            os.environ["CURL_CA_BUNDLE"]     = cf_abs
            logger.info(f"Using CA cert: {cf_abs}")

    def _set_proxy(self):
        ad_user   = self.get("AD_USERNAME","")
        ad_pass   = self.get("AD_USER_PW","")
        dom       = self.get("HTTPS_PROXY_DOMAIN","")
        if not all([ad_user, ad_pass, dom]):
            raise ValueError("Missing proxy credentials for PROXY_ENABLED=TRUE")

        proxy_url = f"http://{ad_user}:{ad_pass}@{dom}"
        os.environ["HTTP_PROXY"]  = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url

        # Merge NO_PROXY if needed
        existing_no_proxy = self.get("NO_PROXY","")
        no_proxy_list = [d.strip() for d in existing_no_proxy.split(",") if d.strip()]
        # Add required domains if not present
        required_bypass = [
            "cognitiveservices.azure.com",
            "search.windows.net",
            "openai.azure.com",
            "core.windows.net",
            "azurewebsites.net",
            # 'login.microsoftonline.com' if needed
        ]
        for dom in required_bypass:
            if dom not in no_proxy_list:
                no_proxy_list.append(dom)
        merged = ",".join(no_proxy_list)
        os.environ["NO_PROXY"] = merged
        logger.info(f"NO_PROXY => {merged}")

    def _get_aad_token(self) -> str:
        tenant = self.get("AZURE_TENANT_ID","")
        cid    = self.get("AZURE_CLIENT_ID","")
        cs     = self.get("AZURE_CLIENT_SECRET","")
        credential = ClientSecretCredential(tenant, cid, cs)
        token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
        logger.info("Acquired Azure AD token for openai usage.")
        return token_obj.token

    def get(self, k:str, default:str="") -> str:
        return os.getenv(k, default)

############################
# Custom Requests Session to fix chunked encoding
############################
def configure_requests_session(cacert_path:str, connect_timeout:int=10, read_timeout:int=300):
    """
    Create a custom requests session with:
     - verify = cacert_path
     - retries on 429/5xx
     - read_timeout
    Then set openai.requestssession to this session, to mitigate chunked encoding errors.
    """
    import requests
    from requests.adapters import HTTPAdapter, Retry

    session = requests.Session()
    session.verify = cacert_path

    retries = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429,500,502,503,504]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    # Force openai to use this session
    import openai
    openai.requestssession = session
    # we can set openai timeout
    openai.timeout = read_timeout

    logger.info(f"Custom requests session: connect_timeout={connect_timeout}, read_timeout={read_timeout}")

############################
# Index + Search with LanceDB
############################
class LancedbMatcher:
    """
    1) Index target CSV in LanceDB with OpenAI embeddings
    2) For each row in source CSV => embed => top-K search => store matches
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self.db_dir = "./lance_vectors"
        self.table_name = "pbt_vectors"
        self._setup_embeddings()

    def _setup_embeddings(self):
        import openai
        from langchain.embeddings import OpenAIEmbeddings

        # Azure settings
        api_base   = self.env.get("AZURE_OPENAI_ENDPOINT","")
        api_version= self.env.get("API_VERSION","2023-03-15-preview")
        token      = self.env.token

        # We assume you have a deployment for text-embedding-ada-002 or similar
        deployment_name = self.env.get("AZURE_EMBEDDINGS_DEPLOYMENT","my-embedding-deployment")
        model_name      = self.env.get("AZURE_EMBEDDINGS_MODEL","text-embedding-ada-002")

        # In azure usage, we specify: openai_api_type="azure"
        self.embedding_fn = OpenAIEmbeddings(
            deployment=deployment_name,
            model=model_name,
            openai_api_key=token,
            openai_api_base=api_base,
            openai_api_type="azure",
            openai_api_version=api_version
        )
        logger.info(f"Embeddings ready: deployment={deployment_name}, model={model_name}")

    def index_target_data(self, target_df: pd.DataFrame):
        """
        Convert each row => Document and embed into LanceDB
        """
        from lancedb import connect
        from langchain.vectorstores import LanceDB
        from langchain.docstore.document import Document

        db = connect(self.db_dir)
        # If table exists, drop
        if self.table_name in db.table_names():
            logger.info(f"Dropping existing table '{self.table_name}'.")
            db.drop_table(self.table_name)

        docs = []
        for _, row in target_df.iterrows():
            text = f"{row['pbt-name']} : {row['pbt-definition']}"
            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "pbt-name": row["pbt-name"],
                        "pbt-definition": row["pbt-definition"]
                    }
                )
            )

        LanceDB.from_documents(
            documents=docs,
            embedding=self.embedding_fn,
            connection=db,
            table_name=self.table_name
        )
        logger.info(f"Indexed {len(docs)} PBTs into LanceDB at {self.db_dir}.")

    def search_similar_pbt(self, query_text:str, top_k:int=3) -> List[Dict]:
        """
        1) embed query
        2) top-K search in LanceDB
        3) return list of results w/ similarity_score
        """
        from lancedb import connect
        from langchain.vectorstores import LanceDB
        db = connect(self.db_dir)
        store = LanceDB(
            connection=db,
            table_name=self.table_name,
            embedding=self.embedding_fn
        )
        # search w/score
        results = store.similarity_search_with_score(query_text, k=top_k)
        # convert to list of dict
        out = []
        for doc, score in results:
            out.append({
                "pbt-name": doc.metadata["pbt-name"],
                "pbt-definition": doc.metadata["pbt-definition"],
                "similarity_score": float(score)
            })
        return out

############################
# main
############################
def main():
    """
    Steps:
    1) env => OSEnv
    2) custom requests session => chunked encoding fix
    3) index target => LanceDB
    4) for each row in source => embed => top-K => store in final CSV
    """
    try:
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"

        for d in [env_dir, data_dir, output_dir, log_dir]:
            d.mkdir(exist_ok=True)

        config_file = env_dir / "config.env"
        creds_file  = env_dir / "credentials.env"
        cert_file   = env_dir / "cacert.pem"
        source_csv  = data_dir / "source.csv"
        target_csv  = data_dir / "target.csv"

        # Check if missing
        missing = []
        for f in [config_file, creds_file, cert_file, source_csv, target_csv]:
            if not f.exists():
                missing.append(str(f))
        if missing:
            logger.error(f"Missing required files: {missing}")
            return

        # 1) Env
        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))

        # 2) custom requests session for openai => fix chunked encoding
        cacert_path = os.environ.get("REQUESTS_CA_BUNDLE","")
        if not cacert_path or not Path(cacert_path).exists():
            logger.warning("No valid cacert for custom session. We'll proceed with default.")
        else:
            configure_requests_session(cacert_path, connect_timeout=10, read_timeout=300)

        # 3) read target => index
        target_df = pd.read_csv(str(target_csv), dtype=str).fillna("")
        if not {"pbt-name","pbt-definition"}.issubset(target_df.columns):
            raise ValueError("target.csv must have columns: pbt-name, pbt-definition")

        matcher = LancedbMatcher(env_setup)
        logger.info("Indexing target data in LanceDB...")
        matcher.index_target_data(target_df)

        # 4) read source => do top-K
        source_df = pd.read_csv(str(source_csv), dtype=str).fillna("")
        if not {"name","definition"}.issubset(source_df.columns):
            raise ValueError("source.csv must have columns: name, definition")

        top_k = int(env_setup.get("TOP_K","3"))
        results = []

        for idx, row in tqdm(source_df.iterrows(), total=len(source_df), desc="Matching"):
            name_val = row["name"].strip()
            def_val  = row["definition"].strip()
            query_text = f"{name_val}\n{def_val}"

            # run similarity search
            top_matches = matcher.search_similar_pbt(query_text, top_k=top_k)
            # store them
            for match in top_matches:
                results.append({
                    "source_name": name_val,
                    "source_definition": def_val,
                    "matched_pbt_name": match["pbt-name"],
                    "matched_pbt_definition": match["pbt-definition"],
                    "similarity_score": match["similarity_score"]
                })

        # 5) save
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        out_file = output_dir / f"matches_{timestamp}.csv"
        pd.DataFrame(results).to_csv(str(out_file), index=False)
        out_json = out_file.with_suffix(".json")
        with open(out_json,'w',encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"\nDone! Results => {out_file} and {out_json}")

    except Exception as e:
        logger.exception(f"Error in main: {str(e)}")
        print(f"Error: {str(e)}")

if __name__=="__main__":
    main()
