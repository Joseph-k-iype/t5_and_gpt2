#!/usr/bin/env python3
"""
HYBRID APPROACH to Speed Up Matching:
-------------------------------------
We have:
 - source.csv => columns: [name, definition]
 - target.csv => columns: [pbt-name, pbt-definition]  (5,000+ rows)

STEPS:
1) Embed all PBTs (pbt-name+pbt-definition) with Azure/OpenAI embeddings
   and store in LanceDB (or any vector DB).
2) For each source row:
   a) Embed (name + definition).
   b) Vector search => get top-K PBTs (e.g. 30).
   c) Then do chunked GPT approach on just those top-K => final best PBT.

Result: Far fewer GPT calls dealing with large data, much faster.

NOTE: 
 - You must have 'langchain', 'lancedb', 'pandas', 'numpy', 'tqdm', 
   'azure-identity', 'openai' installed.
 - Adjust CHUNK_SIZE, K, or the chunking logic for your needs.
"""

import os
import time
import json
import logging
from pathlib import Path
from typing import Dict, List
import pandas as pd
import numpy as np
from tqdm import tqdm

# OSEnv + Azure Identity
from azure.identity import ClientSecretCredential
from dotenv import dotenv_values

# LangChain
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# Local Vector Store (LanceDB)
from lancedb import connect
from langchain.vectorstores import LanceDB
from langchain.docstore.document import Document

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

#############################
# OSEnv
#############################
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"{filepath} not found or not readable")
    return True

def str_to_bool(s: str) -> bool:
    ls = s.strip().lower()
    if ls == 'true':
        return True
    elif ls == 'false':
        return False
    else:
        raise ValueError(f"Invalid boolean: {s}")

class OSEnv:
    """
    Basic environment manager for config + proxy + Azure token if needed.
    """
    def __init__(self, config_file:str, creds_file:str, cert_file:str):
        self.var_list = []
        self._load_file(config_file, True)
        self._load_file(creds_file, False)
        self._set_cert(cert_file)

        if str_to_bool(self.get("PROXY_ENABLED","false")):
            self._set_proxy()

        if str_to_bool(self.get("SECURED_ENDPOINTS","false")):
            self.token = self._aad_token()
        else:
            self.token = None

    def _load_file(self, file_path:str, print_val:bool):
        if is_file_readable(file_path):
            with open(file_path) as f:
                lines = f.readlines()
            for line in lines:
                line=line.strip()
                if line and not line.startswith("#"):
                    if "=" in line:
                        k,v = line.split("=",1)
                        k=k.strip()
                        v=v.strip().strip("'\"")
                        os.environ[k] = v
                        if print_val:
                            logger.info(f"Set {k}={v}")

    def _set_cert(self, cert_file:str):
        cert_abs = os.path.abspath(cert_file)
        if is_file_readable(cert_abs):
            os.environ["REQUESTS_CA_BUNDLE"] = cert_abs
            os.environ["SSL_CERT_FILE"]      = cert_abs
            os.environ["CURL_CA_BUNDLE"]     = cert_abs
            logger.info(f"Using cert: {cert_abs}")

    def _set_proxy(self):
        ad_user   = self.get("AD_USERNAME","")
        ad_pass   = self.get("AD_USER_PW","")
        proxy_dom = self.get("HTTPS_PROXY_DOMAIN","")
        if not all([ad_user, ad_pass, proxy_dom]):
            raise ValueError("Missing proxy credentials for PROXY_ENABLED=TRUE")
        proxy_url = f"http://{ad_user}:{ad_pass}@{proxy_dom}"
        os.environ["HTTP_PROXY"]  = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url
        # Merge NO_PROXY if needed, omitted for brevity
        logger.info(f"Proxy set to {proxy_url}")

    def _aad_token(self) -> str:
        tenant = self.get("AZURE_TENANT_ID","")
        cid    = self.get("AZURE_CLIENT_ID","")
        cs     = self.get("AZURE_CLIENT_SECRET","")
        credential = ClientSecretCredential(tenant, cid, cs)
        token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
        logger.info("Acquired AAD token for Azure OpenAI.")
        return token_obj.token

    def get(self, k:str, default:str="") -> str:
        return os.getenv(k, default)

#############################
# Hybrid Approach Classes
#############################
class PBTVecStore:
    """
    Handles:
      - reading target.csv
      - embedding each pbt
      - storing in LanceDB
      - searching top-K
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self.db = connect("./lance_pbt")   # local folder for LanceDB
        self.table_name = "pbt_vectors"
        self._setup_embeddings()

    def _setup_embeddings(self):
        # Basic openai config from env
        api_base   = self.env.get("AZURE_OPENAI_ENDPOINT","")
        api_version= self.env.get("API_VERSION","2023-03-15-preview")
        token      = self.env.token

        # If you want "text-embedding-ada-002", ensure you have that model deployed
        model_deployment = self.env.get("AZURE_EMB_DEPLOY","my-embedding-deployment")
        self.embeddings = OpenAIEmbeddings(
            deployment=model_deployment,
            model=self.env.get("AZURE_EMB_MODEL","text-embedding-ada-002"),
            openai_api_base=api_base,
            openai_api_version=api_version,
            openai_api_key=token,
            openai_api_type="azure"
        )
        logger.info(f"Embeddings setup, model={self.env.get('AZURE_EMB_MODEL','text-embedding-ada-002')}")

    def index_pbt(self, pbt_df: pd.DataFrame):
        """
        Convert pbt df into Documents, store in LanceDB. If table exists, drop & rebuild.
        """
        if self.table_name in self.db.table_names():
            logger.info("Dropping old pbt_vectors table.")
            self.db.drop_table(self.table_name)

        docs = []
        for _, row in pbt_df.iterrows():
            # combine name & definition
            text = f"{row['pbt-name']} | {row['pbt-definition']}"
            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "pbt-name": row["pbt-name"],
                        "pbt-definition": row["pbt-definition"]
                    }
                )
            )

        from langchain.vectorstores import LanceDB
        LanceDB.from_documents(
            documents=docs,
            embedding=self.embeddings,
            connection=self.db,
            table_name=self.table_name
        )
        logger.info(f"Indexed {len(docs)} PBT docs in LanceDB.")

    def top_k_pbt(self, query_text:str, k:int=30) -> List[Dict[str,str]]:
        """
        1) embed query_text
        2) search top-K in LanceDB
        3) return list of {pbt-name, pbt-definition, score}
        """
        from langchain.vectorstores import LanceDB
        store = LanceDB(
            connection=self.db,
            table_name=self.table_name,
            embedding=self.embeddings
        )
        results = store.similarity_search_with_score(query_text, k=k)
        # results => List[(Document, float)]
        pbt_result = []
        for doc, score in results:
            pbt_result.append({
                "pbt-name": doc.metadata["pbt-name"],
                "pbt-definition": doc.metadata["pbt-definition"],
                "similarity_score": score
            })
        return pbt_result

class ChunkedGPT:
    """
    Chat-based chunk approach to pick 1 best PBT from smaller lists.
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self._setup_chat()
        self.chunk_size = int(self.env.get("CHUNK_SIZE","20"))

    def _setup_chat(self):
        model_name = self.env.get("MODEL_NAME","gpt-4")
        temp       = float(self.env.get("MODEL_TEMPERATURE","0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS","800"))
        api_ver    = self.env.get("API_VERSION","2023-03-15-preview")
        endpoint   = self.env.get("AZURE_OPENAI_ENDPOINT","")
        token      = self.env.token

        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temp,
            max_tokens=max_tokens,
            openai_api_version=api_ver,
            azure_endpoint=endpoint,
            azure_ad_token=token
        )
        logger.info(f"ChunkedGPT => model={model_name}, chunk_size={self.chunk_size}")

    def pick_best_in_chunk(self, name:str, definition:str, chunk:List[Dict]) -> str:
        """
        Ask GPT: "which single pbt is best in this chunk?"
        Return entire text from GPT as 1 answer. 
        (Could parse further if you only want the pbt name.)
        """
        system_text = "You must pick exactly one best match from this chunk."
        user_text   = (
            f"Name: {name}\nDefinition: {definition}\n\n"
            f"Among these PBTs, pick only ONE best match:\n"
        )
        for idx, pbt in enumerate(chunk, start=1):
            user_text += (f"{idx}) pbt-name={pbt['pbt-name']}\n"
                          f"   pbt-definition={pbt['pbt-definition']}\n\n")

        user_text += "Which pbt-name is best, and why?"

        messages = [
            SystemMessage(content=system_text),
            HumanMessage(content=user_text)
        ]
        try:
            response = self.llm(messages)
            return response.content.strip()
        except Exception as e:
            logger.error(f"Error in pick_best_in_chunk: {e}")
            return f"ERROR chunk: {str(e)}"

    def tournament_pick(self, name:str, definition:str, pbt_list:List[Dict]) -> str:
        """
        "Tournament" approach:
          - chunk pbt_list => pick winner from each chunk => new "virtual pbt list"
          - repeat until 1 final
          - Return GPT's final text
        """
        round_idx=1
        current_list = pbt_list
        answers_text = []
        while True:
            logger.info(f"Tournament Round {round_idx}, pbt size={len(current_list)}")
            if len(current_list) <= self.chunk_size:
                # final chunk
                final_ans = self.pick_best_in_chunk(name, definition, current_list)
                answers_text.append(f"Round{round_idx} final: {final_ans}")
                break
            else:
                # multiple chunks
                chunk_answers = []
                for i in range(0, len(current_list), self.chunk_size):
                    chunk = current_list[i : i+self.chunk_size]
                    ans = self.pick_best_in_chunk(name, definition, chunk)
                    chunk_answers.append(ans)
                # Now treat each chunk's textual answer as 1 "virtual pbt"
                new_list = []
                for idx, ans_text in enumerate(chunk_answers, start=1):
                    new_list.append({
                        "pbt-name": f"Round{round_idx}_Chunk{idx}",
                        "pbt-definition": ans_text
                    })
                answers_text.append(f"Round{round_idx} => {chunk_answers}")

                if len(new_list) == 1:
                    # done
                    answers_text.append(f"Single final => {new_list[0]['pbt-definition']}")
                    break
                else:
                    current_list = new_list
                    round_idx += 1
        # Combine all round logs
        return "\n".join(answers_text)

#############################
# Main
#############################
def main():
    """
    1) Load env
    2) Index target.csv in LanceDB
    3) For each row in source.csv, embed + top-K => chunked GPT => final best
    4) Save results
    """
    try:
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"

        config_file = env_dir / "config.env"
        creds_file  = env_dir / "credentials.env"
        cert_file   = env_dir / "cacert.pem"
        source_csv  = data_dir / "source.csv"
        target_csv  = data_dir / "target.csv"

        for f in [config_file, creds_file, cert_file, source_csv, target_csv]:
            if not f.exists():
                logger.error(f"Missing file: {f}")
                return
        
        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))

        # Load PBT
        pbt_df = pd.read_csv(str(target_csv)).fillna("")
        if not {"pbt-name","pbt-definition"}.issubset(pbt_df.columns):
            raise ValueError("target.csv must have columns: pbt-name, pbt-definition")

        # 1) Index target in local vector store
        pbt_indexer = PBTVecStore(env_setup)
        pbt_indexer.index_pbt(pbt_df)

        # 2) Prepare chunk-based GPT
        chunked_gpt = ChunkedGPT(env_setup)

        # 3) For each row in source => embed => topK => chunk => final
        src_df = pd.read_csv(str(source_csv)).fillna("")
        if not {"name","definition"}.issubset(src_df.columns):
            raise ValueError("source.csv must have columns: name, definition")

        top_k = int(env_setup.get("TOP_K","30"))

        results = []
        for idx, row in tqdm(src_df.iterrows(), total=len(src_df), desc="Matching"):
            name_val = row["name"].strip()
            def_val  = row["definition"].strip()

            # embed => local search
            query_text = f"{name_val} | {def_val}"
            top_pbt = pbt_indexer.top_k_pbt(query_text, k=top_k)  # returns list of pbt dict with similarity_score

            # chunk approach on top_pbt
            # remove 'similarity_score' field, not needed in chunk
            smaller_list = []
            for p in top_pbt:
                smaller_list.append({
                    "pbt-name": p["pbt-name"],
                    "pbt-definition": p["pbt-definition"]
                })
            final_ans = chunked_gpt.tournament_pick(name_val, def_val, smaller_list)

            results.append({
                "source_name": name_val,
                "source_definition": def_val,
                "model_answer": final_ans
            })

        # 4) Save
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        out_csv = output_dir / f"matches_{timestamp}.csv"
        pd.DataFrame(results).to_csv(str(out_csv), index=False)
        out_json = out_csv.with_suffix(".json")
        with open(out_json,"w",encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Done! Saved results => {out_csv}, {out_json}")

    except Exception as e:
        logger.exception(f"Error in main: {str(e)}")
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()
