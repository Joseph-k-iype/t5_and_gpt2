# --------------------------
# Embedding Client (Your Existing Implementation)
# --------------------------
class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}

class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", 
                 embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(
            azure_endpoint=os.getenv("AZURE_ENDPOINT"),
            api_version=self.azure_api_version,
            azure_ad_token_provider=token_provider
        )
    
    def generate_embeddings(self, doc: MyDocument) -> MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc

# --------------------------
# Modified RegexGenerator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embedding_client = EmbeddingClient()
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        # Create Chroma collection with your embedding client
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self._chroma_embedding_fn,
            persist_directory="./chroma_db",
            client_settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

    def _chroma_embedding_fn(self, texts: List[str]) -> List[List[float]]:
        """Adapter for Chroma to use your EmbeddingClient"""
        embeddings = []
        for text in texts:
            doc = MyDocument(text=text)
            embedded_doc = self.embedding_client.generate_embeddings(doc)
            embeddings.append(embedded_doc.embedding)
        return embeddings

    # Rest of the RegexGenerator class remains the same as previous implementation
    # ... [include all other methods unchanged] ...






import os
import re
import json
import numpy as np
from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
from langchain_community.retrievers import BM25Retriever
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import AzureOpenAIEmbeddings
from langchain_community.chat_models import AzureChatOpenAI

# --------------------------
# Environment Configuration
# --------------------------
class OSEnv:
    # (Include the full OSEnv implementation from previous code here)
    
# --------------------------
# Data Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1, description="Related term name")
    definition: str = Field(..., description="Description of related term")
    example: str = Field(..., description="Example usage of the term")

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1, description="Primary term name")
    definition: str = Field(..., description="Term definition")
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

# --------------------------
# Regex Generation System
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment="text-embedding-3-large"
        )
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self.embeddings,
            persist_directory="./chroma_db"
        )

    def _build_workflow(self):
        workflow = StateGraph(State)
        
        workflow.add_node("enrich_context", self.enrich_context)
        workflow.add_node("generate_candidates", self.generate_candidates)
        workflow.add_node("validate_patterns", self.validate_patterns)
        workflow.add_node("select_pattern", self.select_pattern)
        
        workflow.add_edge("enrich_context", "generate_candidates")
        workflow.add_edge("generate_candidates", "validate_patterns")
        workflow.add_edge("validate_patterns", "select_pattern")
        
        workflow.add_conditional_edges(
            "select_pattern",
            self.retry_decision,
            {"retry": "generate_candidates", "complete": END}
        )
        
        workflow.set_entry_point("enrich_context")
        return workflow.compile()

    # --------------------------
    # Workflow Components
    # --------------------------
    def enrich_context(self, state: State) -> State:
        """Hybrid context enrichment with semantic and related terms"""
        state["synonyms"] = self._get_linguistic_variations(state["name"])
        
        # Add related terms and examples
        for term in state["related_terms"]:
            state["synonyms"].extend([
                term.name,
                term.example,
                term.name.lower(),
                term.name.replace("-", ""),
                term.name.replace("_", "")
            ])
        
        # Semantic search expansion
        semantic_results = self.knowledge_base.similarity_search(
            query=state["definition"],
            k=3
        )
        state["semantic_terms"] = [doc.page_content for doc in semantic_results]
        
        # Keyword search expansion
        keyword_retriever = BM25Retriever.from_texts(
            [doc.page_content for doc in self.knowledge_base.get().documents]
        )
        keyword_results = keyword_retriever.invoke(state["name"])
        state["keyword_terms"] = [res.page_content for res in keyword_results]
        
        # Combine all terms
        state["all_terms"] = list(set(
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        ))
        return state

    def generate_candidates(self, state: State) -> State:
        """Generate multiple regex candidates with LLM"""
        prompt = ChatPromptTemplate.from_template("""
        Generate 3 regex patterns for '{name}' ({definition}).
        Must include: {all_terms}
        Requirements:
        - Strictly match '{name}' exactly
        - Include all variations and related terms
        - Prevent false positives
        - Use proper anchoring and boundaries
        
        Output JSON with 'patterns' list and 'reasoning' for each pattern.
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        response = chain.invoke({
            "name": state["name"],
            "definition": state["definition"],
            "all_terms": state["all_terms"]
        })
        
        state["candidates"] = response.get("patterns", [])
        state["generation_reasons"] = response.get("reasoning", [])
        return state

    def validate_patterns(self, state: State) -> State:
        """Multi-dimensional pattern validation"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            validations = {
                "syntax": self._validate_syntax(pattern),
                "main_term": self._validate_main_term(pattern, state["name"]),
                "coverage": self._calculate_coverage(pattern, state["all_terms"]),
                "false_positives": self._check_false_positives(pattern, state)
            }
            
            validation_results.append({
                "pattern": pattern,
                "score": self._calculate_score(validations),
                "reason": state["generation_reasons"][idx],
                "validations": validations
            })
        
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    def select_pattern(self, state: State) -> State:
        """Select best pattern or trigger retry"""
        if state["validation_results"]:
            best = state["validation_results"][0]
            if best["score"] >= 0.8:
                state["best_pattern"] = best["pattern"]
                state["confidence"] = best["score"]
                state["reason"] = best["reason"]
                state["status"] = "complete"
                return state
        
        state["status"] = "retry" if state.get("iterations", 0) < 3 else "complete"
        state["iterations"] = state.get("iterations", 0) + 1
        return state

    def retry_decision(self, state: State) -> str:
        return "retry" if state.get("status") == "retry" else "complete"

    # --------------------------
    # Validation Utilities
    # --------------------------
    def _validate_syntax(self, pattern: str) -> bool:
        try:
            re.compile(pattern)
            return True
        except re.error:
            return False

    def _validate_main_term(self, pattern: str, main_term: str) -> bool:
        try:
            return bool(re.fullmatch(pattern, main_term))
        except:
            return False

    def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
        try:
            matched = sum(1 for term in terms if re.fullmatch(pattern, term))
            return matched / len(terms)
        except:
            return 0.0

    def _check_false_positives(self, pattern: str, state: State) -> int:
        try:
            regex = re.compile(pattern)
            return sum(
                1 for rt in state["related_terms"]
                if not regex.fullmatch(rt.example)
            )
        except:
            return len(state["related_terms"])

    def _calculate_score(self, validations: Dict) -> float:
        weights = {
            "syntax": 0.3,
            "main_term": 0.25,
            "coverage": 0.35,
            "false_positives": -0.1
        }
        score = 0
        score += weights["syntax"] * validations["syntax"]
        score += weights["main_term"] * validations["main_term"]
        score += weights["coverage"] * validations["coverage"]
        score += weights["false_positives"] * validations["false_positives"]
        return max(0.0, min(1.0, score))

    # --------------------------
    # Helper Methods
    # --------------------------
    def _get_linguistic_variations(self, term: str) -> List[str]:
        return [
            term,
            term.lower(),
            term.upper(),
            term.title(),
            term.replace("-", ""),
            term.replace("_", ""),
            term.replace(" ", ""),
            term + "s",  # Plural
            term[:-1] if term.endswith("s") else ""  # Singular
        ]

    def process_request(self, input_data: Dict) -> RegexOutput:
        try:
            request = RegexRequest(**input_data)
            state = {
                "name": request.name,
                "definition": request.definition,
                "related_terms": request.related_terms,
                "iterations": 0
            }
            
            final_state = self.workflow.invoke(state)
            
            return RegexOutput(
                name=final_state["name"],
                definition=final_state["definition"],
                regex=final_state.get("best_pattern", ""),
                reason=final_state.get("reason", "Generation failed"),
                confidence_score=final_state.get("confidence", 0.0),
                validation_passes=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] >= 0.7
                ],
                validation_failures=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] < 0.7
                ],
                iterations=final_state.get("iterations", 0)
            )
        except ValidationError as e:
            return self._error_output(input_data, f"Validation error: {e}")

    def _error_output(self, input_data: Dict, error: str) -> RegexOutput:
        return RegexOutput(
            name=input_data.get("name", ""),
            definition=input_data.get("definition", ""),
            regex="",
            reason=error,
            confidence_score=0.0,
            validation_passes=[],
            validation_failures=[],
            iterations=0
        )

# --------------------------
# Usage Example
# --------------------------
if __name__ == "__main__":
    # Initialize generator
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )

    # Sample input with related terms
    input_data = {
        "name": "email",
        "definition": "Electronic mail address",
        "related_terms": [
            {
                "name": "e-mail",
                "definition": "Alternative email format",
                "example": "user@domain.com"
            },
            {
                "name": "email_address",
                "definition": "Formal email identifier",
                "example": "name@organization.org"
            }
        ]
    }

    # Process request
    result = generator.process_request(input_data)
    
    print(json.dumps(result.dict(), indent=2))
