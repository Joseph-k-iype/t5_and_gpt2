from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
import re
import logging

logger = logging.getLogger(__name__)

# --------------------------
# Pydantic Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    example: str

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

class State(TypedDict):
    name: str
    definition: str
    related_terms: List[RelatedTerm]
    synonyms: List[str]
    semantic_terms: List[str]
    keyword_terms: List[str]
    all_terms: List[str]
    candidates: List[str]
    validation_results: List[Dict]
    best_pattern: Optional[str]
    confidence: Optional[float]
    reason: Optional[str]
    iterations: int
    status: Optional[str]

# --------------------------
# Regex Generator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        # ... (previous initialization code) ...

    def enrich_context(self, state: State) -> State:
        """Type-safe context enrichment with exhaustive validation"""
        state["synonyms"] = self._get_sanitized_variations(state["name"])
        
        # Process related terms with strict type checking
        for term in state["related_terms"]:
            if not isinstance(term, RelatedTerm):
                logger.error("Invalid related term type: %s", type(term))
                continue
                
            state["synonyms"].extend([
                self._str_safe(term.name),
                self._str_safe(term.example),
                self._str_safe(term.name).lower(),
                self._str_safe(term.name).replace("-", ""),
                self._str_safe(term.name).replace("_", "")
            ])

        # Semantic search with document validation
        try:
            semantic_results = self.knowledge_base.similarity_search(
                query=self._str_safe(state["definition"]),
                k=3
            )
            state["semantic_terms"] = [
                self._str_safe(doc.page_content) 
                for doc in semantic_results
                if hasattr(doc, 'page_content')
            ]
        except Exception as e:
            logger.error("Semantic search failed: %s", e)
            state["semantic_terms"] = []

        # Keyword search with input sanitation
        try:
            documents = [
                self._str_safe(doc.page_content) 
                for doc in self.knowledge_base.get().documents
                if hasattr(doc, 'page_content')
            ]
            keyword_retriever = BM25Retriever.from_texts(documents)
            keyword_results = keyword_retriever.invoke(
                self._str_safe(state["name"])
            )
            state["keyword_terms"] = [
                self._str_safe(res.page_content)
                for res in keyword_results
                if hasattr(res, 'page_content')
            ]
        except Exception as e:
            logger.error("Keyword search failed: %s", e)
            state["keyword_terms"] = []

        # Combine and sanitize all terms
        all_terms = (
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        )
        
        state["all_terms"] = list(set(
            filter(None, [
                self._str_safe(term).strip() 
                for term in all_terms
                if self._is_valid_term(term)
            ])
        ))
        
        logger.debug("Sanitized all_terms: %s", state["all_terms"])
        return state

    def _get_sanitized_variations(self, term: str) -> List[str]:
        """Generate linguistic variations with type safety"""
        base_term = self._str_safe(term)
        return [
            var for var in [
                base_term,
                base_term.lower(),
                base_term.upper(),
                base_term.title(),
                base_term.replace("-", ""),
                base_term.replace("_", ""),
                base_term.replace(" ", ""),
                f"{base_term}s",
                base_term[:-1] if base_term.endswith("s") else ""
            ]
            if self._is_valid_term(var)
        ]

    def _str_safe(self, value: any) -> str:
        """Convert value to string with fallback handling"""
        try:
            return str(value)
        except Exception as e:
            logger.warning("String conversion failed: %s", e)
            return ""

    def _is_valid_term(self, term: any) -> bool:
        """Validate term type and content"""
        if not isinstance(term, str):
            logger.warning("Invalid term type discarded: %s", type(term))
            return False
        return bool(term.strip())

    def validate_patterns(self, state: State) -> State:
        """Type-safe pattern validation"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            try:
                valid_pattern = self._str_safe(pattern)
                validations = {
                    "syntax": self._validate_syntax(valid_pattern),
                    "main_term": self._validate_main_term(valid_pattern, state["name"]),
                    "coverage": self._calculate_coverage(valid_pattern, state["all_terms"]),
                    "false_positives": self._check_false_positives(valid_pattern, state)
                }
                
                validation_results.append({
                    "pattern": valid_pattern,
                    "score": self._calculate_score(validations),
                    "reason": self._str_safe(state["generation_reasons"][idx]),
                    "validations": validations
                })
            except Exception as e:
                logger.error("Pattern validation failed: %s", e)
                
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    # ... (rest of the class methods with similar type safety checks) ...






# --------------------------
# Modified enrich_context Method
# --------------------------
def enrich_context(self, state: State) -> State:
    """Hybrid context enrichment with type safety checks"""
    # Initialize synonyms with string conversions
    state["synonyms"] = [str(var) for var in self._get_linguistic_variations(state["name"])]
    
    # Process related terms with explicit string casting
    for term in state["related_terms"]:
        term_name = str(term.name)
        term_example = str(term.example)
        state["synonyms"].extend([
            term_name,
            term_example,
            term_name.lower(),
            term_name.replace("-", ""),
            term_name.replace("_", "")
        ])
    
    # Semantic search with type safety
    try:
        semantic_results = self.knowledge_base.similarity_search(
            query=str(state["definition"]),  # Ensure string query
            k=3
        )
        state["semantic_terms"] = [str(doc.page_content) for doc in semantic_results]
    except Exception as e:
        logger.error(f"Semantic search failed: {e}")
        state["semantic_terms"] = []

    # Keyword search with type safety
    try:
        documents = [str(doc.page_content) for doc in self.knowledge_base.get().documents]
        keyword_retriever = BM25Retriever.from_texts(documents)
        keyword_results = keyword_retriever.invoke(str(state["name"]))  # Ensure string input
        state["keyword_terms"] = [str(res.page_content) for res in keyword_results]
    except Exception as e:
        logger.error(f"Keyword search failed: {e}")
        state["keyword_terms"] = []

    # Create all_terms with guaranteed string types
    all_terms = [
        *state["synonyms"],
        *state["semantic_terms"],
        *state["keyword_terms"]
    ]
    
    # Filter out empty strings and convert to hashable types
    state["all_terms"] = list(set(
        filter(None, [term.strip() for term in all_terms if isinstance(term, str)])
    ))
    
    return state

# --------------------------
# Updated Validation Methods
# --------------------------
def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
    """Safe coverage calculation with type checks"""
    try:
        compiled = re.compile(str(pattern))  # Ensure pattern is string
        matched = 0
        for term in terms:
            # Skip non-string terms
            if not isinstance(term, str):
                continue
            try:
                if compiled.fullmatch(term):
                    matched += 1
            except re.error:
                continue
        return matched / len(terms) if terms else 0.0
    except:
        return 0.0

def _check_false_positives(self, pattern: str, state: State) -> int:
    """Safe false positive check with type validation"""
    try:
        compiled = re.compile(str(pattern))  # Ensure pattern is string
        fp_count = 0
        for rt in state["related_terms"]:
            example = str(rt.example)  # Ensure example is string
            if not compiled.fullmatch(example):
                fp_count += 1
        return fp_count
    except:
        return len(state["related_terms"])







# --------------------------
# Updated Chroma Integration
# --------------------------
class ChromaEmbeddingWrapper:
    def __init__(self, embedding_client: EmbeddingClient):
        self.embedding_client = embedding_client
        
    def embed_query(self, text: str) -> List[float]:
        """Wrapper for single query embedding"""
        doc = MyDocument(text=text)
        embedded_doc = self.embedding_client.generate_embeddings(doc)
        return embedded_doc.embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Wrapper for document batch embedding"""
        return [self.embed_query(text) for text in texts]

class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embedding_client = EmbeddingClient()
        self.embedding_wrapper = ChromaEmbeddingWrapper(self.embedding_client)
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self.embedding_wrapper,
            persist_directory="./chroma_db",
            client_settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

    def enrich_context(self, state: State) -> State:
        """Hybrid context enrichment with proper embedding handling"""
        state["synonyms"] = self._get_linguistic_variations(state["name"])
        
        # Add related terms and examples
        for term in state["related_terms"]:
            state["synonyms"].extend([
                term.name,
                term.example,
                term.name.lower(),
                term.name.replace("-", ""),
                term.name.replace("_", "")
            ])
        
        # Semantic search expansion
        try:
            semantic_results = self.knowledge_base.similarity_search(
                query=state["definition"],
                k=3
            )
            state["semantic_terms"] = [doc.page_content for doc in semantic_results]
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            state["semantic_terms"] = []

        # Keyword search expansion
        try:
            keyword_retriever = BM25Retriever.from_texts(
                [doc.page_content for doc in self.knowledge_base.get().documents]
            )
            keyword_results = keyword_retriever.invoke(state["name"])
            state["keyword_terms"] = [res.page_content for res in keyword_results]
        except Exception as e:
            logger.error(f"Keyword search failed: {e}")
            state["keyword_terms"] = []

        # Combine all terms
        state["all_terms"] = list(set(
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        ))
        return state

    # Rest of the class remains unchanged




# --------------------------
# Embedding Client (Your Existing Implementation)
# --------------------------
class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}

class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", 
                 embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(
            azure_endpoint=os.getenv("AZURE_ENDPOINT"),
            api_version=self.azure_api_version,
            azure_ad_token_provider=token_provider
        )
    
    def generate_embeddings(self, doc: MyDocument) -> MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc

# --------------------------
# Modified RegexGenerator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embedding_client = EmbeddingClient()
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        # Create Chroma collection with your embedding client
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self._chroma_embedding_fn,
            persist_directory="./chroma_db",
            client_settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

    def _chroma_embedding_fn(self, texts: List[str]) -> List[List[float]]:
        """Adapter for Chroma to use your EmbeddingClient"""
        embeddings = []
        for text in texts:
            doc = MyDocument(text=text)
            embedded_doc = self.embedding_client.generate_embeddings(doc)
            embeddings.append(embedded_doc.embedding)
        return embeddings

    # Rest of the RegexGenerator class remains the same as previous implementation
    # ... [include all other methods unchanged] ...






import os
import re
import json
import numpy as np
from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
from langchain_community.retrievers import BM25Retriever
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import AzureOpenAIEmbeddings
from langchain_community.chat_models import AzureChatOpenAI

# --------------------------
# Environment Configuration
# --------------------------
class OSEnv:
    # (Include the full OSEnv implementation from previous code here)
    
# --------------------------
# Data Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1, description="Related term name")
    definition: str = Field(..., description="Description of related term")
    example: str = Field(..., description="Example usage of the term")

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1, description="Primary term name")
    definition: str = Field(..., description="Term definition")
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

# --------------------------
# Regex Generation System
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment="text-embedding-3-large"
        )
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self.embeddings,
            persist_directory="./chroma_db"
        )

    def _build_workflow(self):
        workflow = StateGraph(State)
        
        workflow.add_node("enrich_context", self.enrich_context)
        workflow.add_node("generate_candidates", self.generate_candidates)
        workflow.add_node("validate_patterns", self.validate_patterns)
        workflow.add_node("select_pattern", self.select_pattern)
        
        workflow.add_edge("enrich_context", "generate_candidates")
        workflow.add_edge("generate_candidates", "validate_patterns")
        workflow.add_edge("validate_patterns", "select_pattern")
        
        workflow.add_conditional_edges(
            "select_pattern",
            self.retry_decision,
            {"retry": "generate_candidates", "complete": END}
        )
        
        workflow.set_entry_point("enrich_context")
        return workflow.compile()

    # --------------------------
    # Workflow Components
    # --------------------------
    def enrich_context(self, state: State) -> State:
        """Hybrid context enrichment with semantic and related terms"""
        state["synonyms"] = self._get_linguistic_variations(state["name"])
        
        # Add related terms and examples
        for term in state["related_terms"]:
            state["synonyms"].extend([
                term.name,
                term.example,
                term.name.lower(),
                term.name.replace("-", ""),
                term.name.replace("_", "")
            ])
        
        # Semantic search expansion
        semantic_results = self.knowledge_base.similarity_search(
            query=state["definition"],
            k=3
        )
        state["semantic_terms"] = [doc.page_content for doc in semantic_results]
        
        # Keyword search expansion
        keyword_retriever = BM25Retriever.from_texts(
            [doc.page_content for doc in self.knowledge_base.get().documents]
        )
        keyword_results = keyword_retriever.invoke(state["name"])
        state["keyword_terms"] = [res.page_content for res in keyword_results]
        
        # Combine all terms
        state["all_terms"] = list(set(
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        ))
        return state

    def generate_candidates(self, state: State) -> State:
        """Generate multiple regex candidates with LLM"""
        prompt = ChatPromptTemplate.from_template("""
        Generate 3 regex patterns for '{name}' ({definition}).
        Must include: {all_terms}
        Requirements:
        - Strictly match '{name}' exactly
        - Include all variations and related terms
        - Prevent false positives
        - Use proper anchoring and boundaries
        
        Output JSON with 'patterns' list and 'reasoning' for each pattern.
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        response = chain.invoke({
            "name": state["name"],
            "definition": state["definition"],
            "all_terms": state["all_terms"]
        })
        
        state["candidates"] = response.get("patterns", [])
        state["generation_reasons"] = response.get("reasoning", [])
        return state

    def validate_patterns(self, state: State) -> State:
        """Multi-dimensional pattern validation"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            validations = {
                "syntax": self._validate_syntax(pattern),
                "main_term": self._validate_main_term(pattern, state["name"]),
                "coverage": self._calculate_coverage(pattern, state["all_terms"]),
                "false_positives": self._check_false_positives(pattern, state)
            }
            
            validation_results.append({
                "pattern": pattern,
                "score": self._calculate_score(validations),
                "reason": state["generation_reasons"][idx],
                "validations": validations
            })
        
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    def select_pattern(self, state: State) -> State:
        """Select best pattern or trigger retry"""
        if state["validation_results"]:
            best = state["validation_results"][0]
            if best["score"] >= 0.8:
                state["best_pattern"] = best["pattern"]
                state["confidence"] = best["score"]
                state["reason"] = best["reason"]
                state["status"] = "complete"
                return state
        
        state["status"] = "retry" if state.get("iterations", 0) < 3 else "complete"
        state["iterations"] = state.get("iterations", 0) + 1
        return state

    def retry_decision(self, state: State) -> str:
        return "retry" if state.get("status") == "retry" else "complete"

    # --------------------------
    # Validation Utilities
    # --------------------------
    def _validate_syntax(self, pattern: str) -> bool:
        try:
            re.compile(pattern)
            return True
        except re.error:
            return False

    def _validate_main_term(self, pattern: str, main_term: str) -> bool:
        try:
            return bool(re.fullmatch(pattern, main_term))
        except:
            return False

    def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
        try:
            matched = sum(1 for term in terms if re.fullmatch(pattern, term))
            return matched / len(terms)
        except:
            return 0.0

    def _check_false_positives(self, pattern: str, state: State) -> int:
        try:
            regex = re.compile(pattern)
            return sum(
                1 for rt in state["related_terms"]
                if not regex.fullmatch(rt.example)
            )
        except:
            return len(state["related_terms"])

    def _calculate_score(self, validations: Dict) -> float:
        weights = {
            "syntax": 0.3,
            "main_term": 0.25,
            "coverage": 0.35,
            "false_positives": -0.1
        }
        score = 0
        score += weights["syntax"] * validations["syntax"]
        score += weights["main_term"] * validations["main_term"]
        score += weights["coverage"] * validations["coverage"]
        score += weights["false_positives"] * validations["false_positives"]
        return max(0.0, min(1.0, score))

    # --------------------------
    # Helper Methods
    # --------------------------
    def _get_linguistic_variations(self, term: str) -> List[str]:
        return [
            term,
            term.lower(),
            term.upper(),
            term.title(),
            term.replace("-", ""),
            term.replace("_", ""),
            term.replace(" ", ""),
            term + "s",  # Plural
            term[:-1] if term.endswith("s") else ""  # Singular
        ]

    def process_request(self, input_data: Dict) -> RegexOutput:
        try:
            request = RegexRequest(**input_data)
            state = {
                "name": request.name,
                "definition": request.definition,
                "related_terms": request.related_terms,
                "iterations": 0
            }
            
            final_state = self.workflow.invoke(state)
            
            return RegexOutput(
                name=final_state["name"],
                definition=final_state["definition"],
                regex=final_state.get("best_pattern", ""),
                reason=final_state.get("reason", "Generation failed"),
                confidence_score=final_state.get("confidence", 0.0),
                validation_passes=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] >= 0.7
                ],
                validation_failures=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] < 0.7
                ],
                iterations=final_state.get("iterations", 0)
            )
        except ValidationError as e:
            return self._error_output(input_data, f"Validation error: {e}")

    def _error_output(self, input_data: Dict, error: str) -> RegexOutput:
        return RegexOutput(
            name=input_data.get("name", ""),
            definition=input_data.get("definition", ""),
            regex="",
            reason=error,
            confidence_score=0.0,
            validation_passes=[],
            validation_failures=[],
            iterations=0
        )

# --------------------------
# Usage Example
# --------------------------
if __name__ == "__main__":
    # Initialize generator
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )

    # Sample input with related terms
    input_data = {
        "name": "email",
        "definition": "Electronic mail address",
        "related_terms": [
            {
                "name": "e-mail",
                "definition": "Alternative email format",
                "example": "user@domain.com"
            },
            {
                "name": "email_address",
                "definition": "Formal email identifier",
                "example": "name@organization.org"
            }
        ]
    }

    # Process request
    result = generator.process_request(input_data)
    
    print(json.dumps(result.dict(), indent=2))
