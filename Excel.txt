class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> dict:  # Changed return type to dict
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Convert Pydantic model to dict
                result_dict = {
                    "name": term["name"],
                    "regex": result.regex,
                    "reason": result.reason,
                    "confidence": result.confidence,
                    "status": result.status
                }
                
                # Validate regex
                try:
                    re.compile(result_dict["regex"])
                except re.error:
                    result_dict["confidence"] = max(0, result_dict["confidence"] - 50)
                    result_dict["status"] = "RED"
                    result_dict["reason"] += " (Invalid regex pattern)"
                
                results.append(result_dict)
                
            except Exception as e:
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[dict]) -> dict:  # Changed return type to dict
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res["status"]] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r["confidence"] for r in results) // len(results)
            }
        }

if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(State)
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:  # Changed return type to dict
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            return final_state["result"]



class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:  # Changed return type to dict
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        # Result is already a dict, so we can directly dump it
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)



_________________________________________________________________________________________________________________________________________________


[
  {
    "name": "Python",
    "definition": "A high-level programming language",
    "related_terms": [
      {
        "name": "scripting",
        "definition": "A style of programming that automates tasks",
        "example": "Example: Python scripts can automate file management."
      },
      {
        "name": "interpreted",
        "definition": "Executed line-by-line rather than compiled",
        "example": "Example: Python code is executed in an interpreter."
      }
    ]
  },
  {
    "name": "Java",
    "definition": "A widely-used, class-based programming language",
    "related_terms": [
      {
        "name": "OOP",
        "definition": "Object-Oriented Programming paradigm",
        "example": "Example: Javaâ€™s design encourages encapsulation and inheritance."
      }
    ]
  }
]


if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            # Initialize StateGraph with a defined state type in the constructor
            self.workflow = StateGraph(State)  # State should be your state type definition
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            config = {"input_path": input_path}
            result = self.workflow.compile().run(config)
            return result["result"]

# ... [Keep all existing imports and utility functions] ...

# Add these additional imports
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import BaseOutputParser
from typing import Tuple
import re
try:
    from langgraph.graph import END, StateGraph
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

# ================================
# Pydantic Models for Analysis Results
# ================================
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str

    @field_validator('confidence')
    def validate_confidence(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('Confidence must be between 0-100')
        return v

    @field_validator('status')
    def validate_status(cls, v):
        if v not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Status must be RED, AMBER, or GREEN')
        return v

class TermAnalysisResult(BaseModel):
    results: List[RegexResult]
    summary: Dict[str, int]

# ================================
# Regex Processing Components
# ================================
class RegexOutputParser(PydanticOutputParser):
    def parse(self, text: str) -> RegexResult:
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            return RegexResult(
                name="",  # Will be filled later
                regex=match.group("regex").strip(),
                reason=match.group("reason").strip(),
                confidence=int(match.group("confidence")),
                status=match.group("status").strip()
            )
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> TermAnalysisResult:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Validate and adjust confidence
                try:
                    re.compile(result.regex)
                except re.error:
                    result.confidence = max(0, result.confidence - 50)
                    result.status = "RED"
                    result.reason += " (Invalid regex pattern)"
                
                results.append(RegexResult(
                    name=term["name"],
                    regex=result.regex,
                    reason=result.reason,
                    confidence=result.confidence,
                    status=result.status
                ))
                
            except Exception as e:
                results.append(RegexResult(
                    name=term["name"],
                    regex="",
                    reason=f"Processing error: {str(e)}",
                    confidence=0,
                    status="RED"
                ))
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[RegexResult]) -> TermAnalysisResult:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res.status] += 1
        
        return TermAnalysisResult(
            results=results,
            summary={
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r.confidence for r in results) // len(results)
            }
        )

# ================================
# LangGraph Workflow (if available)
# ================================
if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(update_state_type=dict)
            
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            return self.workflow.invoke({"input_path": input_path})["result"]

# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> TermAnalysisResult:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        print(json.dumps(result.dict(), indent=2))
        with open("output.json", "w") as f:
            json.dump(result.dict(), f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)
