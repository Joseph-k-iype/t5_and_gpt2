#!/usr/bin/env python3
"""
PROBLEM STATEMENT
-----------------
We have two CSV files:

1) The first CSV has:
   - name: the original label or identifier for a concept
   - definition: a descriptive explanation or context for that concept

2) The second CSV has:
   - pbt-name: the business-approved or "preferred" label for that concept
   - pbt-definition: the descriptive explanation or context for that PBT

We want to 'semantically match' each 'name' in the first CSV with the most similar 'pbt-name'
from the second CSV, taking advantage of both definitions to provide context.

IMPLEMENTATION
--------------
- Use your original `OSEnv` class for environment, certificate, and Azure token handling.
- Use Azure OpenAI embeddings (LangChain) + LanceDB (instead of Faiss) for vector search.
- Integrate a structured prompt in `_prepare_text()` to leverage definitions for contextual understanding.
"""

import os
import time
import logging
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional

import pandas as pd
from tqdm import tqdm
import numpy as np

# LanceDB & LangChain
from lancedb import connect
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
from langchain.docstore.document import Document

# OpenAI for Azure usage
import openai

# Azure Identity only if used by OSEnv's get_azure_token
from azure.identity import ClientSecretCredential

# -------------- Original OSEnv Class --------------
# (Copied verbatim from your previous script, with minimal adaptation if needed)
def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    import os
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert string to boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

class OSEnv:
    """Environment and certificate management class (original)."""
    
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        """Initialize with configuration files and certificate path."""
        self.var_list = []
        
        # Load configurations
        self.bulk_set(config_file, True)
        logging.info(f"Loaded main configuration from {config_file}")
        
        self.bulk_set(creds_file, False)
        logging.info(f"Loaded credentials from {creds_file}")
        
        # Set up certificates
        self.set_certificate_path(certificate_path)
        logging.info("Certificate path configured")
        
        # Configure proxy if enabled
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logging.info("Proxy configured")
        
        # Set up Azure token if secure endpoints enabled
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logging.info("Securing endpoints via AAD token flow")
            self.token = self.get_azure_token()
        else:
            self.token = self.get("AZURE_API_KEY", None)  # Maybe you have direct API key
            if not self.token:
                logging.warning("No token or API key found. Make sure your environment is correct.")

    def set_certificate_path(self, certificate_path: str) -> None:
        """Set up the certificate path for SSL verification."""
        try:
            if is_file_readable(certificate_path):
                from pathlib import Path
                cert_path = str(Path(certificate_path))
                self.set("REQUESTS_CA_BUNDLE", cert_path)
                self.set("SSL_CERT_FILE", cert_path)
                self.set("CURL_CA_BUNDLE", cert_path)
                logging.info(f"Certificate path set to: {cert_path}")
        except Exception as e:
            logging.error(f"Certificate configuration failed: {str(e)}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        """Read and set environment variables from a dotenv file."""
        try:
            if is_file_readable(dotenvfile):
                logging.info(f"Loading environment variables from {dotenvfile}")
                with open(dotenvfile) as f:
                    lines = f.readlines()
                
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip("'").strip('"')
                            self.set(key, value, print_val)
                        except ValueError:
                            continue
                            
                logging.info(f"Successfully loaded variables from {dotenvfile}")
                
        except Exception as e:
            logging.error(f"Failed to load environment file {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        """Set an environment variable."""
        try:
            os.environ[var_name] = val
            if var_name not in self.var_list:
                self.var_list.append(var_name)
            if print_val and var_name not in ['AZURE_CLIENT_SECRET', 'AD_USER_PW']:
                logging.info(f"Set {var_name}={val}")
        except Exception as e:
            logging.error(f"Failed to set environment variable {var_name}: {str(e)}")
            raise

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        """Get an environment variable value."""
        return os.getenv(var_name, default)

    def set_proxy(self) -> None:
        """Set up proxy configuration with authentication."""
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Missing proxy credentials")
            
            proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains))
            
            logging.info("Proxy configuration completed")
        except Exception as e:
            logging.error(f"Proxy configuration failed: {str(e)}")
            raise

    def get_azure_token(self) -> str:
        """
        Get Azure authentication token using ClientSecretCredential
        (assuming environment has AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET).
        """
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logging.info("Azure token acquired successfully via OSEnv.get_azure_token()")
            return token.token
        except Exception as e:
            logging.error(f"Failed to get Azure token: {str(e)}")
            raise

# -------------- Logging Setup --------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# -------------- SemanticMatcher Class (using LanceDB) --------------
class SemanticMatcher:
    """
    Performs semantic matching using:
      - Your existing OSEnv for environment & token (self.env.token)
      - Azure OpenAI embeddings (LangChain)
      - LanceDB for vector similarity
      - A structured prompt for combining name/definition
    """
    def __init__(self, env_setup: OSEnv):
        self.env = env_setup
        self._setup_openai()
        self._setup_embeddings()
        self._setup_lancedb()

    def _setup_openai(self):
        """
        Configure the openai package with your OSEnv settings.
        openai.api_key is assigned from env_setup.token
        """
        openai.api_type = "azure"
        openai.api_base = self.env.get("AZURE_OPENAI_ENDPOINT", "")
        openai.api_version = self.env.get("API_VERSION", "2023-03-15-preview")

        if not self.env.token:
            msg = "No token found in OSEnv. Ensure SECURED_ENDPOINTS=True or AZURE_API_KEY is set."
            logger.error(msg)
            raise ValueError(msg)

        openai.api_key = self.env.token
        logger.info("OpenAI configured with OSEnv token.")

    def _setup_embeddings(self):
        """
        Create a LangChain Embeddings object referencing your Azure deployment.
        """
        deployment_name = self.env.get("AZURE_EMBEDDINGS_DEPLOYMENT", "my-embedding-deployment")
        model_name = self.env.get("AZURE_EMBEDDINGS_MODEL", "text-embedding-3-large")

        self.embeddings_model = OpenAIEmbeddings(
            deployment=deployment_name,
            model=model_name,
            openai_api_key=openai.api_key,
            openai_api_base=openai.api_base,
            openai_api_type=openai.api_type,
            openai_api_version=openai.api_version
        )
        logger.info(f"LangChain Embeddings set up for deployment '{deployment_name}' with model '{model_name}'.")

    def _setup_lancedb(self):
        """
        Connect or create local LanceDB storage for vector embeddings.
        """
        self.db = connect("./lance_vectors")  # stored in local folder
        self.table_name = "pbt_vectors"
        logger.info("Connected to LanceDB at ./lance_vectors")

    def _prepare_text(self, name: str, definition: str) -> str:
        """
        Integrate a structured prompt that references the problem statement:
        We combine 'name' and 'definition' to provide full context for embeddings.
        """
        prompt = (
            f"You are given the following information about a concept:\n\n"
            f"- Original Name: {name}\n"
            f"- Original Definition: {definition}\n\n"
            f"Your task is to find the best matching 'Preferred Business Term (PBT)' "
            f"from a set of PBT-Names and PBT-Definitions.\n"
            f"Focus on semantic similarity.\n"
        )
        return prompt

    # ---------------- CSV Loading ------------------
    def load_csv_data(self, source_csv: str, target_csv: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Load and validate CSV files:
          source.csv => columns: [name, definition]
          target.csv => columns: [pbt-name, pbt-definition]
        """
        def read_csv_safely(file_path: str) -> pd.DataFrame:
            encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
            for enc in encodings:
                try:
                    return pd.read_csv(file_path, encoding=enc)
                except Exception:
                    continue
            raise ValueError(f"Unable to read {file_path} with standard encodings.")

        src_df = read_csv_safely(source_csv).fillna("")
        tgt_df = read_csv_safely(target_csv).fillna("")

        if not {"name", "definition"}.issubset(src_df.columns):
            raise ValueError("Source CSV must have 'name' and 'definition' columns.")
        if not {"pbt-name", "pbt-definition"}.issubset(tgt_df.columns):
            raise ValueError("Target CSV must have 'pbt-name' and 'pbt-definition' columns.")

        # Clean whitespace
        for df in [src_df, tgt_df]:
            for col in df.columns:
                df[col] = df[col].astype(str).str.strip()

        logger.info(f"Loaded {len(src_df)} source records from {source_csv}")
        logger.info(f"Loaded {len(tgt_df)} target records from {target_csv}")
        return src_df, tgt_df

    # ---------------- Indexing Target Data ------------
    def index_target_data(self, target_df: pd.DataFrame):
        """
        Build a LanceDB index from pbt-name + pbt-definition.
        If the table already exists, we drop it and re-create.
        """
        if self.table_name in self.db.table_names():
            logger.info(f"Dropping existing LanceDB table '{self.table_name}' to rebuild.")
            self.db.drop_table(self.table_name)

        docs = []
        for _, row in target_df.iterrows():
            pbt_text = (
                f"PBT-Name: {row['pbt-name']}\n"
                f"PBT-Definition: {row['pbt-definition']}\n"
            )
            docs.append(
                Document(
                    page_content=pbt_text,
                    metadata={
                        "pbt-name": row["pbt-name"],
                        "pbt-definition": row["pbt-definition"]
                    }
                )
            )

        logger.info("Indexing target data into LanceDB...")
        LanceDB.from_documents(
            documents=docs,
            embedding=self.embeddings_model,
            connection=self.db,
            table_name=self.table_name
        )
        logger.info("Target data indexed successfully.")

    # ---------------- Matching Process -----------------
    def process_matches(self, source_df: pd.DataFrame, similarity_threshold=0.75, top_k=3) -> List[Dict]:
        """
        For each row in source_df, embed the _prepare_text() result,
        similarity search in LanceDB, and return matches above threshold.
        """
        vector_store = LanceDB(
            connection=self.db,
            table_name=self.table_name,
            embedding=self.embeddings_model
        )

        matches = []
        for _, src_row in tqdm(source_df.iterrows(), total=len(source_df), desc="Matching Rows"):
            query_prompt = self._prepare_text(src_row["name"], src_row["definition"])
            results = vector_store.similarity_search_with_score(query_prompt, k=top_k)

            for doc, score in results:
                # Typically 'score' is 0..1 if using cosine similarity
                if score >= similarity_threshold:
                    matches.append({
                        "source_name": src_row["name"],
                        "source_definition": src_row["definition"],
                        "matched_pbt_name": doc.metadata["pbt-name"],
                        "matched_pbt_definition": doc.metadata["pbt-definition"],
                        "similarity_score": float(score),
                    })

        logger.info(f"Found {len(matches)} total matches >= {similarity_threshold}")
        return matches

    # --------------- Save Results to CSV/JSON ----------------
    def save_results(self, matches: List[Dict], output_file: str):
        """
        Save matched results to a CSV and JSON file with the same base name.
        """
        if not matches:
            logger.warning("No matches to save.")
            return

        df = pd.DataFrame(matches)
        df.to_csv(output_file, index=False)

        json_file = output_file.replace(".csv", ".json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(matches, f, indent=2, ensure_ascii=False)

        logger.info(f"Results saved to: {output_file} and {json_file}")

# -------------- Main --------------
def main():
    """
    1) Build paths for env/, data/, output/, logs/ from one level above src/
    2) Initialize OSEnv with the original logic for token retrieval
    3) Load CSV data
    4) Index target data in LanceDB
    5) Match source data
    6) Save results
    """
    try:
        # We are in src/, so go up 1 level to find env, data, etc.
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"

        for directory in [data_dir, output_dir, log_dir]:
            directory.mkdir(exist_ok=True)

        # Required files
        config_file = env_dir / "config.env"
        creds_file = env_dir / "credentials.env"
        cert_file = env_dir / "cacert.pem"
        source_csv = data_dir / "source.csv"
        target_csv = data_dir / "target.csv"

        missing_files = []
        for f in [config_file, creds_file, source_csv, target_csv]:
            if not f.exists():
                missing_files.append(str(f))
        if missing_files:
            print("\nMissing required files:")
            for mf in missing_files:
                print(f" - {mf}")
            return

        # Initialize environment
        print("Initializing environment with original OSEnv...")
        env_setup = OSEnv(
            config_file=str(config_file),
            creds_file=str(creds_file),
            certificate_path=str(cert_file)
        )

        # Initialize matcher
        matcher = SemanticMatcher(env_setup)

        # Prepare output file
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"matches_{timestamp}.csv"

        # Load CSV data
        print("\nLoading CSV data...")
        source_df, target_df = matcher.load_csv_data(str(source_csv), str(target_csv))

        # Index target data
        print("Indexing target data in LanceDB...")
        matcher.index_target_data(target_df)

        # Process matches
        print("Processing semantic matches...")
        matches = matcher.process_matches(
            source_df=source_df,
            similarity_threshold=0.75,  # Adjust threshold as desired
            top_k=3                     # top 3 neighbors
        )

        # Save results
        print("Saving match results...")
        matcher.save_results(matches, str(output_file))

        print(f"\nProcess completed successfully. Results saved at {output_file}")

    except FileNotFoundError as e:
        print(f"File Error: {str(e)}")
        logger.exception("File path issue")
    except ValueError as e:
        print(f"Value Error: {str(e)}")
        logger.exception("Value issue encountered")
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        logger.exception("Unexpected error occurred")
        raise

if __name__ == "__main__":
    main()
