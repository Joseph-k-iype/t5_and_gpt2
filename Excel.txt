#!/usr/bin/env python3
"""
Local LanceDB Storage with OpenAI Embeddings (No remote OpenAI Vector Store):

1) We call the OpenAI Embeddings API only to get vectors (one-time per doc).
2) We store + query those vectors locally with LanceDB. 
   - target.csv => columns: [pbt-name, pbt-definition]
   - source.csv => columns: [name, definition]
3) We do top-K similarity search locally in LanceDB for each source row.

Everything beyond embedding the text is done entirely on your local machine.
"""

import os
import time
import json
import logging
from pathlib import Path
from typing import List, Dict
import pandas as pd
from tqdm import tqdm

# Azure Identity if needed for SECURED_ENDPOINTS
from azure.identity import ClientSecretCredential
from dotenv import dotenv_values

# LanceDB & LangChain Embeddings
from lancedb import connect
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
from langchain.docstore.document import Document

import openai  # For embedding calls

###########################
# Logging
###########################
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

###########################
# Helper Functions
###########################
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"File '{filepath}' not found or not readable.")
    return True

def str_to_bool(s: str) -> bool:
    ls = s.strip().lower()
    if ls == 'true':
        return True
    elif ls == 'false':
        return False
    else:
        raise ValueError(f"Invalid boolean: {s}")

###########################
# OSEnv for environment, proxy, cert, Azure AD token
###########################
class OSEnv:
    """
    Loads environment variables, sets cert & proxy,
    optionally fetches Azure AD token if SECURED_ENDPOINTS=True.
    No remote vector store usage; we only embed texts and store locally.
    """
    def __init__(self, config_file:str, creds_file:str, cert_file:str):
        self.var_list = []
        self._load_env_file(config_file, True)
        logger.info(f"Loaded config from {config_file}")
        
        self._load_env_file(creds_file, False)
        logger.info(f"Loaded creds from {creds_file}")
        
        self._set_certificate(cert_file)
        logger.info("Certificate path configured.")

        if str_to_bool(self.get("PROXY_ENABLED","false")):
            self._set_proxy()
            logger.info("Proxy configured.")

        if str_to_bool(self.get("SECURED_ENDPOINTS","false")):
            logger.info("Secured endpoints => Acquiring Azure AD token.")
            self.token = self._get_aad_token()
        else:
            self.token = None

    def _load_env_file(self, path: str, print_vals: bool):
        abs_path = os.path.abspath(path)
        if is_file_readable(abs_path):
            with open(abs_path, 'r') as f:
                lines = f.readlines()
            for line in lines:
                line=line.strip()
                if line and not line.startswith("#"):
                    if "=" in line:
                        k,v = line.split("=",1)
                        k=k.strip()
                        v=v.strip().strip("'\"")
                        os.environ[k]=v
                        if print_vals:
                            logger.info(f"Set {k}={v}")

    def _set_certificate(self, cert_file:str):
        cf_abs = os.path.abspath(cert_file)
        if is_file_readable(cf_abs):
            os.environ["REQUESTS_CA_BUNDLE"]=cf_abs
            os.environ["SSL_CERT_FILE"]     =cf_abs
            os.environ["CURL_CA_BUNDLE"]    =cf_abs
            logger.info(f"Using CA cert: {cf_abs}")

    def _set_proxy(self):
        user = self.get("AD_USERNAME","")
        pw   = self.get("AD_USER_PW","")
        dom  = self.get("HTTPS_PROXY_DOMAIN","")
        if not all([user,pw,dom]):
            raise ValueError("Missing proxy credentials for PROXY_ENABLED=TRUE")
        px_url = f"http://{user}:{pw}@{dom}"
        os.environ["HTTP_PROXY"]  = px_url
        os.environ["HTTPS_PROXY"] = px_url

        no_proxy_existing = self.get("NO_PROXY","")
        no_proxy_list = [d.strip() for d in no_proxy_existing.split(",") if d.strip()]
        must_bypass = [
            'cognitiveservices.azure.com',
            'search.windows.net',
            'openai.azure.com',
            'core.windows.net',
            'azurewebsites.net',
            # If needed: 'login.microsoftonline.com'
        ]
        for domain in must_bypass:
            if domain not in no_proxy_list:
                no_proxy_list.append(domain)
        merged = ",".join(no_proxy_list)
        os.environ["NO_PROXY"]=merged
        logger.info(f"NO_PROXY => {merged}")

    def _get_aad_token(self)->str:
        tenant = self.get("AZURE_TENANT_ID","")
        cid    = self.get("AZURE_CLIENT_ID","")
        cs     = self.get("AZURE_CLIENT_SECRET","")
        credential = ClientSecretCredential(tenant, cid, cs)
        token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
        logger.info("Acquired Azure AD token for embeddings usage.")
        return token_obj.token

    def get(self, k:str, default:str="") -> str:
        return os.getenv(k,default)

###########################
# Setup a custom requests session (optional) 
# to fix chunked encoding errors or partial reads
###########################
def configure_requests_session(cacert_path:str, connect_timeout:int=10, read_timeout:int=300):
    """
    Creates a custom requests.Session with 
    - verify=cacert_path 
    - retries on 429/5xx
    - extended timeouts
    Then assigns openai.requestssession to it, 
    so all embedding calls are more robust.
    """
    import requests
    from requests.adapters import HTTPAdapter, Retry
    import openai

    session = requests.Session()
    session.verify = cacert_path

    retries = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429,500,502,503,504]
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    openai.requestssession = session
    openai.timeout = read_timeout

    logger.info(f"Custom session => connect_timeout={connect_timeout}, read_timeout={read_timeout}")

###########################
# LanceDBIndex: local storing of vectors
###########################
class LanceDBIndex:
    """
    1) Use OpenAI embeddings to embed target CSV rows => store in local LanceDB
    2) For each source row => embed => do top-K local search => collect matches
    No connection to a remote OpenAI vector storeâ€”this is all local.
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self.db_path = "./lance_vectors"   # local folder
        self.table_name = "pbt_vectors"
        self._setup_embeddings()

    def _setup_embeddings(self):
        # read environment for azure openai embedding usage
        api_base = self.env.get("AZURE_OPENAI_ENDPOINT","")
        api_ver  = self.env.get("API_VERSION","2023-03-15-preview")
        api_key  = self.env.token  # if SECURED_ENDPOINTS => AAD token

        # e.g., "text-embedding-ada-002"
        model_name = self.env.get("AZURE_EMBEDDINGS_MODEL","text-embedding-ada-002")
        deployment_name = self.env.get("AZURE_EMBEDDINGS_DEPLOYMENT","my-embedding-deployment")

        self.embedding_fn = OpenAIEmbeddings(
            deployment=deployment_name,
            model=model_name,
            openai_api_type="azure",
            openai_api_key=api_key,
            openai_api_base=api_base,
            openai_api_version=api_ver
        )
        logger.info(f"Embedding setup => deployment={deployment_name}, model={model_name}")

    def index_target_data(self, target_df: pd.DataFrame):
        """
        Convert each target row => Document => embed => store in local LanceDB
        """
        from lancedb import connect
        from langchain.vectorstores import LanceDB
        from langchain.docstore.document import Document

        # connect local
        db = connect(self.db_path)
        # drop old table if exists
        if self.table_name in db.table_names():
            logger.info(f"Dropping old table '{self.table_name}'.")
            db.drop_table(self.table_name)

        docs = []
        for _, row in target_df.iterrows():
            text = f"{row['pbt-name']} : {row['pbt-definition']}"
            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "pbt-name": row["pbt-name"],
                        "pbt-definition": row["pbt-definition"]
                    }
                )
            )

        LanceDB.from_documents(
            documents=docs,
            embedding=self.embedding_fn,
            connection=db,
            table_name=self.table_name
        )
        logger.info(f"Indexed {len(docs)} PBT rows into local LanceDB => {self.db_path}")

    def search_similar(
        self, 
        text: str, 
        top_k: int=5
    ) -> List[Dict]:
        """
        embed 'text', do top-K local LanceDB search,
        return list of match dict {pbt-name, pbt-definition, similarity_score}
        """
        from lancedb import connect
        from langchain.vectorstores import LanceDB
        db = connect(self.db_path)
        store = LanceDB(
            connection=db,
            table_name=self.table_name,
            embedding=self.embedding_fn
        )

        results = store.similarity_search_with_score(text, k=top_k)
        out=[]
        for doc,score in results:
            out.append({
                "pbt-name": doc.metadata["pbt-name"],
                "pbt-definition": doc.metadata["pbt-definition"],
                "similarity_score": float(score)
            })
        return out

###########################
# main
###########################
def main():
    """
    1) Load environment with OSEnv => proxy, cert, optional AAD token
    2) Optionally configure custom requests session => fix chunked encoding
    3) Index target.csv in local LanceDB 
    4) For each row in source.csv => embed => top-K local search => collect results
    5) Save final matches in CSV & JSON
    """
    try:
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir= base_dir / "data"
        output_dir= base_dir / "output"
        log_dir= base_dir / "logs"

        for d in [env_dir, data_dir, output_dir, log_dir]:
            d.mkdir(exist_ok=True)

        config_file = env_dir / "config.env"
        creds_file  = env_dir / "credentials.env"
        cert_file   = env_dir / "cacert.pem"
        source_csv  = data_dir / "source.csv"
        target_csv  = data_dir / "target.csv"

        # Validate
        missing = []
        for f in [config_file, creds_file, cert_file, source_csv, target_csv]:
            if not f.exists():
                missing.append(str(f))
        if missing:
            logger.error(f"Missing files: {missing}")
            return

        # 1) OSEnv
        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))

        # 2) custom requests session for openai => optional
        cacert_path = os.environ.get("REQUESTS_CA_BUNDLE","")
        if cacert_path and Path(cacert_path).exists():
            configure_requests_session(cacert_path, connect_timeout=10, read_timeout=300)

        # read CSV
        target_df = pd.read_csv(str(target_csv)).fillna("")
        if not {"pbt-name","pbt-definition"}.issubset(target_df.columns):
            raise ValueError("target.csv must have columns: pbt-name, pbt-definition")

        # 3) Index target
        indexer = LanceDBIndex(env_setup)
        indexer.index_target_data(target_df)

        # read source
        source_df = pd.read_csv(str(source_csv)).fillna("")
        if not {"name","definition"}.issubset(source_df.columns):
            raise ValueError("source.csv must have columns: name, definition")

        top_k = int(env_setup.get("TOP_K","5"))
        similarity_threshold = float(env_setup.get("SIMILARITY_THRESHOLD","0.75"))
        # store matches
        matches = []

        for idx, row in tqdm(source_df.iterrows(), total=len(source_df), desc="Matching"):
            name_val = row["name"].strip()
            def_val  = row["definition"].strip()
            text = f"{name_val} : {def_val}"

            results = indexer.search_similar(text, top_k=top_k)
            # filter by threshold or keep all top_k
            for r in results:
                if r["similarity_score"] >= similarity_threshold:
                    matches.append({
                        "source_name": name_val,
                        "source_definition": def_val,
                        "pbt_name": r["pbt-name"],
                        "pbt_definition": r["pbt-definition"],
                        "similarity_score": r["similarity_score"]
                    })

        # 5) Save final
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        out_csv  = output_dir / f"matches_{timestamp}.csv"
        out_json = out_csv.with_suffix(".json")

        pd.DataFrame(matches).to_csv(str(out_csv), index=False)
        with open(out_json, 'w', encoding='utf-8') as f:
            json.dump(matches, f, indent=2, ensure_ascii=False)

        logger.info(f"Done. Found {len(matches)} matches >= threshold {similarity_threshold}.")
        logger.info(f"Results => {out_csv}, {out_json}")

    except Exception as e:
        logger.exception(f"Unexpected error: {str(e)}")
        print(f"Error: {str(e)}")

if __name__=="__main__":
    main()
