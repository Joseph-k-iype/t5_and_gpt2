import os
import sys
import uuid
import json
import logging
import re
from typing import Optional, Any, Dict, List, Union, Tuple
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel, ValidationError, field_validator, model_validator
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema import BaseOutputParser
from collections import namedtuple

# Optional: if using LangGraph for workflow orchestration
try:
    from langgraph.graph import END, StateGraph
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Global Constants
ENV_DIR = "env"
CONFIG_PATH = os.path.join(ENV_DIR, "config.env")
CREDS_PATH = os.path.join(ENV_DIR, "credentials.env")
CERT_PATH = os.path.join(ENV_DIR, "cacert.pem")

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## Utility functions
def is_file_readable(filepath: str) -> bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert a string to a boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class for environment configuration
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None

    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )

    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            # If file is not directly readable, use dotenv_values to load variables
            if not os.path.isfile(dotenvfile):
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
                del temp_dict
            else:
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise

    def set(self, key: str, value: str, print_val: bool = False) -> None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise

    def get(self, key: str, default: Optional[str] = None) -> str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise

    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise

    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None

    def list_env_vars(self) -> None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")

## Embedding related classes
class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}

class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()

    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(token_provider, self.azure_api_version)

    def generate_embeddings(self, doc: MyDocument) -> MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc

## AzureChatbot components
class AzureChatbot:
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)

    def _setup_chat_model(self):
        try:
            token_provider = get_bearer_token_provider(
                self.env._get_credential(),
                "https://cognitiveservices.azure.com/.default"
            )
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            azure_ad_token_provider = token_provider
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=azure_ad_token_provider
            )
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise

## Enhanced RegexResult with testing and optimization details
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str
    positive_tests: List[Tuple[bool, str]] = []
    negative_tests: List[Tuple[bool, str]] = []
    syntax_valid: bool = False
    optimization_suggestions: List[str] = []

    @field_validator('confidence')
    def validate_confidence(cls, value):
        if not 0 <= value <= 100:
            raise ValueError('Confidence must be between 0 and 100')
        return value

    @field_validator('status')
    def validate_status(cls, value):
        if value not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Invalid status')
        return value

    @field_validator('regex')
    def validate_regex(cls, value):
        try:
            re.compile(value)
        except re.error as e:
            raise ValueError(f'Invalid regex: {str(e)}')
        return value

    @model_validator(mode='after')
    def validate_tests(self):
        passed_positive = sum(result for result, _ in self.positive_tests)
        passed_negative = sum(not result for result, _ in self.negative_tests)
        total_tests = len(self.positive_tests) + len(self.negative_tests)
        success_rate = (passed_positive + passed_negative) / total_tests if total_tests else 0

        # Adjust confidence based on test performance (70% original confidence, 30% test success)
        if total_tests > 0:
            self.confidence = min(100, max(0, int(self.confidence * 0.7 + success_rate * 100 * 0.3)))
        return self

## Regex Optimizer: post-process regex patterns
class RegexOptimizer:
    """Post-processes regex patterns to improve efficiency and correctness"""
    
    @staticmethod
    def optimize(pattern: str) -> str:
        optimizations = [
            (r'\(\?P<\w+>', '(?:'),  # Convert named groups to non-capturing
            (r'\(\?P=\w+\)', ''),     # Remove backreferences
            (r'\s+', ''),             # Remove extra whitespace
            (r'(\\[a-z]){2,}', lambda m: f'(?:{m.group(0)}+)'),  # Consolidate repeated escapes
            (r'(\\b|\\B){2,}', ''),   # Remove redundant word boundaries
        ]
        try:
            original = pattern
            for regex, replacement in optimizations:
                pattern = re.sub(regex, replacement, pattern)
            # Validate that the optimized pattern is still valid
            re.compile(pattern)
            return pattern
        except re.error:
            return original

## Regex Tester: run test cases against the pattern
class RegexTester:
    """Validates regex patterns against test cases"""
    
    @staticmethod
    def test_pattern(pattern: str, positive_examples: List[str], 
                     negative_examples: List[str]) -> Tuple[List[Tuple[bool, str]], List[Tuple[bool, str]]]:
        try:
            compiled = re.compile(pattern)
        except re.error:
            return [], []
        positive_results = []
        for example in positive_examples:
            match = compiled.search(example)
            positive_results.append((bool(match), example))
        negative_results = []
        for example in negative_examples:
            match = compiled.search(example)
            negative_results.append((not bool(match), example))
        return positive_results, negative_results

## Enhanced RegexOutputParser with multiple parsing strategies
class RegexOutputParser(BaseOutputParser):
    """Robust parser with multiple fallback strategies"""
    
    def parse(self, text: str) -> dict:
        parsing_strategies = [
            self._parse_standard_format,
            self._parse_markdown_format,
            self._parse_line_by_line
        ]
        for strategy in parsing_strategies:
            result = strategy(text)
            if result.get('status', 'RED') != 'RED':
                return result
        return self._create_error_result("All parsing strategies failed")

    def _parse_standard_format(self, text: str) -> dict:
        patterns = [
            r"Regex:\s*(?P<regex>.+?)\s*Reason:\s*(?P<reason>.+?)\s*Confidence:\s*(?P<confidence>\d+).*?Status:\s*(?P<status>RED|AMBER|GREEN)",
            r"Pattern:\s*(?P<regex>.+?)\s*Explanation:\s*(?P<reason>.+?)\s*Confidence Level:\s*(?P<confidence>\d+).*?Status:\s*(?P<status>RED|AMBER|GREEN)",
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                return self._process_match(match)
        return self._create_error_result("Standard format not found")

    def _parse_markdown_format(self, text: str) -> dict:
        try:
            sections = {}
            current_key = None
            for line in text.split('\n'):
                if line.startswith('**'):
                    key = line.strip('** :').lower()
                    sections[key] = []
                    current_key = key
                elif current_key:
                    sections[current_key].append(line.strip())
            if all(k in sections for k in ['regex', 'reason', 'confidence', 'status']):
                return {
                    "regex": ' '.join(sections['regex']),
                    "reason": ' '.join(sections['reason']),
                    "confidence": int(''.join(filter(str.isdigit, ' '.join(sections['confidence'])))),
                    "status": sections['status'][0].upper()
                }
        except Exception:
            pass
        return self._create_error_result("Markdown parsing failed")

    def _parse_line_by_line(self, text: str) -> dict:
        result = {}
        for line in text.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip().lower()
                value = value.strip()
                if key in ['regex', 'pattern']:
                    result['regex'] = value
                elif key == 'reason':
                    result['reason'] = value
                elif key == 'confidence':
                    result['confidence'] = int(value) if value.isdigit() else 0
                elif key == 'status':
                    result['status'] = value.upper()
        if all(k in result for k in ['regex', 'reason', 'confidence', 'status']):
            return result
        return self._create_error_result("Line-by-line parsing failed")

    def _process_match(self, match: re.Match) -> dict:
        try:
            return {
                "regex": match.group('regex').strip(),
                "reason": match.group('reason').strip(),
                "confidence": min(100, max(0, int(match.group('confidence')))),
                "status": match.group('status').upper()
            }
        except Exception as e:
            return self._create_error_result(f"Match processing error: {str(e)}")

    def _create_error_result(self, message: str) -> dict:
        return {
            "regex": "",
            "reason": message,
            "confidence": 0,
            "status": "RED"
        }

## Enhanced TermAnalyzer using the updated components
class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        self.prompt_template = self._create_enhanced_prompt()
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def _create_enhanced_prompt(self) -> PromptTemplate:
        template = """**Task:** Create a comprehensive regex pattern for detecting '{name}' in unstructured text.

**Term Details:**
- Primary Name: {name}
- Definition: {definition}
- Related Terms: {related_terms}
- Positive Examples: {positive_examples}
- Negative Examples: {negative_examples}

**Regex Requirements:**
1. Must match all case variants (use (?i) flag)
2. Handle common separators: [-_/\\s]*
3. Account for optional plurals/suffixes
4. Include common abbreviations/acronyms
5. Prevent false positives using word boundaries
6. Allow for number variations (e.g., IDs, codes)
7. Consider possible OCR errors (e.g., 0/O, 1/l)

**Development Strategy:**
1. Start with base term variations
2. Add contextual modifiers
3. Include validation components
4. Optimize for performance

**Response Format:**
```markdown
**Regex:** <pattern>
**Reason:** <justification>
**Confidence:** <0-100>
**Status:** <RED|AMBER|GREEN>
**Regex:** (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b(?:[-_/\\s]*(?:id|code))?\\b
**Reason:** Matches the base term with variations and optional identifier suffixes.
**Confidence:** 90
**Status:** GREEN
```"""
        return PromptTemplate(
            input_variables=["name", "definition", "related_terms", "positive_examples", "negative_examples"],
            template=template
        )

    def analyze_single_term(self, term: dict) -> RegexResult:
        try:
            # Prepare test cases
            positive_examples = term.get("positive_examples", [])
            negative_examples = term.get("negative_examples", [])
            
            # Generate initial regex pattern via LLM
            response = self.chain.invoke({
                "name": term["name"],
                "definition": term.get("definition", ""),
                "related_terms": ", ".join(term.get("related_terms", [])),
                "positive_examples": "\n- ".join(positive_examples),
                "negative_examples": "\n- ".join(negative_examples)
            })
            
            # Parse response (if not already a dict)
            parsed = response if isinstance(response, dict) else self.parser.parse(str(response))
            parsed["name"] = term["name"]
            
            # Post-process: Optimize the regex pattern
            optimized_regex = RegexOptimizer.optimize(parsed["regex"])
            if optimized_regex != parsed["regex"]:
                parsed["optimization_suggestions"] = ["Applied basic optimizations"]
                parsed["regex"] = optimized_regex

            # Run test cases against the regex
            positive_results, negative_results = RegexTester.test_pattern(
                parsed["regex"],
                positive_examples,
                negative_examples
            )
            
            return RegexResult(
                **parsed,
                positive_tests=positive_results,
                negative_tests=negative_results,
                syntax_valid=True
            )
            
        except Exception as e:
            return RegexResult(
                name=term.get("name", ""),
                regex="",
                reason=f"Error: {str(e)}",
                confidence=0,
                status="RED",
                syntax_valid=False
            )
    
    def analyze_terms(self, input_path: str) -> dict:
        try:
            with open(input_path) as f:
                terms = json.load(f)
            if not isinstance(terms, list):
                terms = [terms]
            results = []
            for term in terms:
                result = self.analyze_single_term(term)
                results.append(result.dict())
            return self._create_final_result(results)
        except Exception as e:
            return {
                "results": [],
                "summary": {
                    "total_terms": 0,
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                }
            }
    
    def _create_final_result(self, results: List[dict]) -> dict:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        total_confidence = 0
        for result in results:
            status = result.get("status", "RED")
            status_counts[status] += 1
            total_confidence += result.get("confidence", 0)
        average_confidence = total_confidence // len(results) if results else 0
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": average_confidence
            }
        }

## AnalysisWorkflow using LangGraph (if available)
if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(dict)
            
            # Define workflow nodes
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow edges
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            analysis_result = self.analyzer.analyze_terms(state["input_path"])
            state["result"] = analysis_result
            return state

        def finalize_output(self, state: dict) -> dict:
            if isinstance(state.get("result"), dict) and "results" in state["result"]:
                logger.info(f"Processed {len(state['result']['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            return final_state["result"]

## Application class tying everything together
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

## Main function
if __name__ == "__main__":
    try:
        app = Application()
        # Provide the path to your input JSON file (adjust if necessary)
        result = app.run_analysis("input.json")
        
        # Print the results in JSON format and also write to output.json
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)








































import os
import sys
import uuid
import json
import logging
from typing import Optional, Any, Dict, List, Union, Tuple
from pathlib import Path
import re
from pydantic import BaseModel, ValidationError, field_validator, model_validator
from langchain.schema import BaseOutputParser

# Enhanced RegexResult with testing validation
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str
    positive_tests: List[Tuple[bool, str]] = []
    negative_tests: List[Tuple[bool, str]] = []
    syntax_valid: bool = False
    optimization_suggestions: List[str] = []

    @field_validator('confidence')
    def validate_confidence(cls, value):
        if not 0 <= value <= 100:
            raise ValueError('Confidence must be 0-100')
        return value

    @field_validator('status')
    def validate_status(cls, value):
        if value not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Invalid status')
        return value

    @field_validator('regex')
    def validate_regex(cls, value):
        try:
            re.compile(value)
        except re.error as e:
            raise ValueError(f'Invalid regex: {str(e)}')
        return value

    @model_validator(mode='after')
    def validate_tests(self):
        passed_positive = sum(result for result, _ in self.positive_tests)
        passed_negative = sum(not result for result, _ in self.negative_tests)
        
        total_tests = len(self.positive_tests) + len(self.negative_tests)
        success_rate = (passed_positive + passed_negative) / total_tests if total_tests else 0
        
        # Adjust confidence based on test performance
        if total_tests > 0:
            self.confidence = min(100, max(0, int(self.confidence * 0.7 + success_rate * 100 * 0.3)))
        
        return self

class RegexOptimizer:
    """Post-processes regex patterns to improve efficiency and correctness"""
    
    @staticmethod
    def optimize(pattern: str) -> str:
        optimizations = [
            (r'\(\?P<\w+>', '(?:'),  # Convert named groups to non-capturing
            (r'\(\?P=\w+\)', ''),     # Remove backreferences
            (r'\s+', ''),             # Remove whitespace outside character classes
            (r'(\\[a-z]){2,}', lambda m: f'(?:{m.group(0)}+)'),  # Quantifier consolidation
            (r'(\\b|\\B){2,}', ''),   # Remove redundant word boundaries
        ]
        
        try:
            compiled = re.compile(pattern)
            original = pattern
            for regex, replacement in optimizations:
                pattern = re.sub(regex, replacement, pattern)
            
            # Validate optimization didn't break the pattern
            re.compile(pattern)
            return pattern
        except re.error:
            return original

class RegexTester:
    """Validates regex patterns against test cases"""
    
    @staticmethod
    def test_pattern(pattern: str, positive_examples: List[str], 
                    negative_examples: List[str]) -> Tuple[List[Tuple[bool, str]], List[Tuple[bool, str]]]:
        try:
            compiled = re.compile(pattern)
        except re.error:
            return [], []

        positive_results = []
        for example in positive_examples:
            match = compiled.search(example)
            positive_results.append((bool(match), example))

        negative_results = []
        for example in negative_examples:
            match = compiled.search(example)
            negative_results.append((not bool(match), example))

        return positive_results, negative_results

class RegexOutputParser(BaseOutputParser):
    """Robust parser with multiple fallback strategies"""
    
    def parse(self, text: str) -> dict:
        parsing_strategies = [
            self._parse_standard_format,
            self._parse_markdown_format,
            self._parse_line_by_line
        ]
        
        for strategy in parsing_strategies:
            result = strategy(text)
            if result['status'] != 'RED':
                return result
        return self._create_error_result("All parsing strategies failed")

    def _parse_standard_format(self, text: str) -> dict:
        patterns = [
            r"Regex:\s*(?P<regex>.+?)\s*Reason:\s*(?P<reason>.+?)\s*Confidence:\s*(?P<confidence>\d+).*?Status:\s*(?P<status>RED|AMBER|GREEN)",
            r"Pattern:\s*(?P<regex>.+?)\s*Explanation:\s*(?P<reason>.+?)\s*Confidence Level:\s*(?P<confidence>\d+).*?Status:\s*(?P<status>RED|AMBER|GREEN)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                return self._process_match(match)
        return self._create_error_result("Standard format not found")

    def _parse_markdown_format(self, text: str) -> dict:
        try:
            sections = {}
            current_key = None
            for line in text.split('\n'):
                if line.startswith('**'):
                    key = line.strip('** :').lower()
                    sections[key] = []
                    current_key = key
                elif current_key:
                    sections[current_key].append(line.strip())
            
            if all(k in sections for k in ['regex', 'reason', 'confidence', 'status']):
                return {
                    "regex": ' '.join(sections['regex']),
                    "reason": ' '.join(sections['reason']),
                    "confidence": int(''.join(filter(str.isdigit, ' '.join(sections['confidence'])))),
                    "status": sections['status'][0].upper()
                }
        except Exception:
            pass
        return self._create_error_result("Markdown parsing failed")

    def _parse_line_by_line(self, text: str) -> dict:
        result = {}
        for line in text.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip().lower()
                value = value.strip()
                if key in ['regex', 'pattern']:
                    result['regex'] = value
                elif key == 'reason':
                    result['reason'] = value
                elif key == 'confidence':
                    result['confidence'] = int(value) if value.isdigit() else 0
                elif key == 'status':
                    result['status'] = value.upper()
        if all(k in result for k in ['regex', 'reason', 'confidence', 'status']):
            return result
        return self._create_error_result("Line-by-line parsing failed")

    def _process_match(self, match: re.Match) -> dict:
        try:
            return {
                "regex": match.group('regex').strip(),
                "reason": match.group('reason').strip(),
                "confidence": min(100, max(0, int(match.group('confidence')))),
                "status": match.group('status').upper()
            }
        except Exception as e:
            return self._create_error_result(f"Match processing error: {str(e)}")

    def _create_error_result(self, message: str) -> dict:
        return {
            "regex": "",
            "reason": message,
            "confidence": 0,
            "status": "RED"
        }

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        self.prompt_template = self._create_enhanced_prompt()
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def _create_enhanced_prompt(self) -> PromptTemplate:
        template = """**Task:** Create a comprehensive regex pattern for detecting '{name}' in unstructured text.

**Term Details:**
- Primary Name: {name}
- Definition: {definition}
- Related Terms: {related_terms}
- Positive Examples: {positive_examples}
- Negative Examples: {negative_examples}

**Regex Requirements:**
1. Must match all case variants (use (?i) flag)
2. Handle common separators: [-_/\\s]*
3. Account for optional plurals/suffixes
4. Include common abbreviations/acronyms
5. Prevent false positives using word boundaries
6. Allow for number variations (e.g., IDs, codes)
7. Consider possible OCR errors (e.g., 0/O, 1/l)

**Development Strategy:**
1. Start with base term variations
2. Add contextual modifiers
3. Include validation components
4. Optimize for performance

**Response Format:**
```markdown
**Regex:** <pattern>
**Reason:** <justification>
**Confidence:** <0-100>
**Status:** <RED|AMBER|GREEN>
Example:
Regex: (?i)\b(?:c (?:ounter )?[- ]?(?:party |pt)(?:ies |y)?\b(?:[-/\s](?:id |code|num|#)?\s\d+)?\b
Reason: Matches base term, separators, plurals, and common ID formats
Confidence: 90
Status: GREEN
"""

    return PromptTemplate(
        input_variables=["name", "definition", "related_terms", "positive_examples", "negative_examples"],
        template=template
    )

def analyze_single_term(self, term: dict) -> RegexResult:
    try:
        # Prepare test cases
        positive_examples = term.get("positive_examples", [])
        negative_examples = term.get("negative_examples", [])
        
        # Generate initial regex
        response = self.chain.invoke({
            "name": term["name"],
            "definition": term.get("definition", ""),
            "related_terms": ", ".join(term.get("related_terms", [])),
            "positive_examples": "\n- ".join(positive_examples),
            "negative_examples": "\n- ".join(negative_examples)
        })
        
        # Parse and validate
        parsed = response if isinstance(response, dict) else self.parser.parse(str(response))
        parsed["name"] = term["name"]
        
        # Post-process regex
        optimized_regex = RegexOptimizer.optimize(parsed["regex"])
        if optimized_regex != parsed["regex"]:
            parsed["optimization_suggestions"] = ["Applied basic optimizations"]
            parsed["regex"] = optimized_regex

        # Run test cases
        positive_results, negative_results = RegexTester.test_pattern(
            parsed["regex"],
            positive_examples,
            negative_examples
        )
        
        return RegexResult(
            **parsed,
            positive_tests=positive_results,
            negative_tests=negative_results,
            syntax_valid=True
        )
        
    except Exception as e:
        return RegexResult(
            name=term.get("name", ""),
            regex="",
            reason=f"Error: {str(e)}",
            confidence=0,
            status="RED",
            syntax_valid=False
        )

# Rest of the class remains similar with updated error handling
_________________________________________________







































class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:
        try:
            print(f"DEBUG - Parser received text: {text}")
            
            # Simple regex to extract the components
            pattern = (
                r"Regex:\s*([^\n]+?)[\n\r]+"
                r"Reason:\s*([^\n]+?)[\n\r]+"
                r"Confidence:\s*(\d+)[\n\r]+"
                r"Status:\s*(RED|AMBER|GREEN)"
            )
            
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            
            if match:
                result = {
                    "regex": match.group(1).strip(),
                    "reason": match.group(2).strip(),
                    "confidence": int(match.group(3)),
                    "status": match.group(4).upper()
                }
                print(f"DEBUG - Parsed result: {result}")
                return result
            
            return {
                "regex": "",
                "reason": "Failed to parse output",
                "confidence": 0,
                "status": "RED"
            }
                
        except Exception as e:
            print(f"DEBUG - Parser error: {str(e)}")
            return {
                "regex": "",
                "reason": f"Error: {str(e)}",
                "confidence": 0,
                "status": "RED"
            }

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the term '{name}'.
            Definition: {definition}
            Context: {related_terms}

            Your response MUST use this EXACT format:
            Regex: [pattern]
            Reason: [explanation]
            Confidence: [0-100]
            Status: [RED|AMBER|GREEN]
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_single_term(self, term: dict) -> dict:
        try:
            # Get LLM response
            chain_output = self.chain.invoke({
                "name": term.get("name", ""),
                "definition": term.get("definition", ""),
                "related_terms": str(term.get("related_terms", ""))
            })
            
            # If it's already a dict, use it; otherwise parse it
            if isinstance(chain_output, dict):
                result = chain_output
            else:
                result = self.parser.parse(str(chain_output))
            
            # Add the name
            result["name"] = term.get("name", "")
            return result
            
        except Exception as e:
            print(f"DEBUG - Error: {str(e)}")
            return {
                "name": term.get("name", ""),
                "regex": "",
                "reason": f"Error: {str(e)}",
                "confidence": 0,
                "status": "RED"
            }

    def analyze_terms(self, input_path: str) -> dict:
        try:
            with open(input_path) as f:
                terms = json.load(f)
            
            if not isinstance(terms, list):
                terms = [terms]
            
            results = []
            for term in terms:
                result = self.analyze_single_term(term)
                results.append(result)
            
            return self._create_final_result(results)
            
        except Exception as e:
            return {
                "results": [],
                "summary": {
                    "total_terms": 0,
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                }
            }

    def _create_final_result(self, results: List[dict]) -> dict:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for result in results:
            status = result.get("status", "RED")
            status_counts[status] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r.get("confidence", 0) for r in results) // len(results) if results else 0
            }
        }










class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:
        try:
            print(f"DEBUG - Parser received text: {text}")
            
            # Handle text that might already be formatted
            if isinstance(text, dict):
                print("DEBUG - Received dictionary input")
                return text
                
            # Strip any extra whitespace or quotes
            text = text.strip().strip('"\'')
            
            # More flexible pattern matching
            pattern = (
                r"Regex:\s*([^\n]+?)[\n\r]+"
                r"Reason:\s*([^\n]+?)[\n\r]+"
                r"Confidence:\s*(\d+)[\n\r]+"
                r"Status:\s*(RED|AMBER|GREEN)"
            )
            
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            
            if match:
                regex_pattern = match.group(1).strip().replace('\\\\', '\\')
                reason = match.group(2).strip()
                confidence = int(match.group(3))
                status = match.group(4).upper()
                
                print(f"DEBUG - Successfully parsed regex: {regex_pattern}")
                print(f"DEBUG - Parsed components: reason={reason}, confidence={confidence}, status={status}")
                
                result = {
                    "name": "",  # Will be filled later
                    "regex": regex_pattern,
                    "reason": reason,
                    "confidence": min(max(confidence, 0), 100),
                    "status": status if status in ("RED", "AMBER", "GREEN") else "AMBER"
                }
                print(f"DEBUG - Generated result dictionary: {result}")
                return result
            else:
                print(f"DEBUG - No match found in text: {text}")
                # Try alternative pattern matching if the first one fails
                alt_pattern = r'Regex:\s*([^"\n]+)'
                alt_match = re.search(alt_pattern, text)
                if alt_match:
                    regex_pattern = alt_match.group(1).strip().replace('\\\\', '\\')
                    return {
                        "name": "",
                        "regex": regex_pattern,
                        "reason": "Pattern extracted with fallback parser",
                        "confidence": 50,
                        "status": "AMBER"
                    }
                
                return {
                    "name": "",
                    "regex": "",
                    "reason": "Failed to parse output",
                    "confidence": 0,
                    "status": "RED"
                }
                
        except Exception as e:
            print(f"DEBUG - Parser error: {str(e)}")
            return {
                "name": "",
                "regex": "",
                "reason": f"Error parsing output: {str(e)}",
                "confidence": 0,
                "status": "RED"
            }




def analyze_single_term(self, term: dict) -> dict:
        try:
            print(f"DEBUG - Analyzing term: {term.get('name', 'unknown')}")
            
            # Prepare related terms info
            related_terms = term.get('related_terms', [])
            related_terms_info = "\n".join(
                [f"- {rt.get('definition', '')} (Example: {rt.get('example', '')})" 
                 for rt in related_terms]
            ) if related_terms else "No related terms provided."
            
            # Get LLM response
            chain_input = {
                "name": term.get("name", ""),
                "definition": term.get("definition", ""),
                "related_terms": related_terms_info
            }
            
            # Execute the chain
            chain_output = self.chain.invoke(chain_input)
            print(f"DEBUG - Chain output type: {type(chain_output)}")
            print(f"DEBUG - Chain output content: {chain_output}")
            
            # Parse the output
            if isinstance(chain_output, dict):
                parsed_result = chain_output
            else:
                parsed_result = self.parser.parse(str(chain_output))
            
            print(f"DEBUG - Parsed result: {parsed_result}")
            
            # Ensure we have all required fields with proper values
            result = {
                "name": term.get("name", ""),
                "regex": parsed_result.get("regex", "").strip(),
                "reason": parsed_result.get("reason", "").strip(),
                "confidence": int(parsed_result.get("confidence", 0)),
                "status": parsed_result.get("status", "RED")
            }
            
            # Extra validation
            if not result["regex"]:
                result.update({
                    "regex": "",
                    "reason": "No regex pattern generated",
                    "confidence": 0,
                    "status": "RED"
                })
            else:
                # Validate the regex pattern
                try:
                    re.compile(result["regex"])
                except re.error as e:
                    result.update({
                        "confidence": max(0, result["confidence"] - 50),
                        "status": "RED",
                        "reason": f"{result['reason']} (Invalid regex pattern: {str(e)})"
                    })
            
            print(f"DEBUG - Final result for term: {result}")
            return result
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_single_term: {str(e)}")
            return {
                "name": term.get("name", "unknown"),
                "regex": "",
                "reason": f"Processing error: {str(e)}",
                "confidence": 0,
                "status": "RED"
            }














class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a flexible and generic regex pattern for identifying the term '{name}' in text.
            
            Term Details:
            - Name: {name}
            - Definition: {definition}
            - Related Context: {related_terms}

            Guidelines for the regex pattern:
            1. Must be a valid regex pattern that matches the term and its variations
            2. Include common variations:
               - Different word separators (spaces, hyphens, underscores)
               - Optional parts that might be omitted
               - Common abbreviations
               - Case insensitivity
            3. Use word boundaries where appropriate
            4. Make it as generic as possible while maintaining accuracy

            Your response MUST follow this EXACT format:
            Regex: [your pattern here]
            Reason: [explain why this pattern will work]
            Confidence: [number between 0-100]
            Status: [RED, AMBER, or GREEN]

            Example response:
            Regex: (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b[- _]*(?:id|identifier|code|ref)?\\b
            Reason: Matches 'counterparty' and its variations, with optional identifier terms
            Confidence: 85
            Status: GREEN
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_single_term(self, term: dict) -> dict:
        try:
            print(f"DEBUG - Analyzing term: {term.get('name', 'unknown')}")
            
            # Prepare related terms info
            related_terms = term.get('related_terms', [])
            related_terms_info = "\n".join(
                [f"- {rt.get('definition', '')} (Example: {rt.get('example', '')})" 
                 for rt in related_terms]
            ) if related_terms else "No related terms provided."
            
            # Get LLM response
            chain_input = {
                "name": term.get("name", ""),
                "definition": term.get("definition", ""),
                "related_terms": related_terms_info
            }
            print(f"DEBUG - Chain input: {chain_input}")
            
            # Execute the chain and get the parsed result
            chain_output = self.chain.invoke(chain_input)
            print(f"DEBUG - Raw chain output: {chain_output}")
            
            # If chain_output is not a dict, parse it
            if not isinstance(chain_output, dict):
                parsed_result = self.parser.parse(str(chain_output))
            else:
                parsed_result = chain_output
                
            print(f"DEBUG - Parsed result: {parsed_result}")
            
            # Ensure we have a valid dictionary with all required keys
            result = {
                "name": term.get("name", ""),
                "regex": parsed_result.get("regex", ""),
                "reason": parsed_result.get("reason", ""),
                "confidence": parsed_result.get("confidence", 0),
                "status": parsed_result.get("status", "RED")
            }
            
            # Validate the regex pattern
            if result["regex"]:
                try:
                    re.compile(result["regex"])
                except re.error as e:
                    print(f"DEBUG - Invalid regex pattern: {str(e)}")
                    result["confidence"] = max(0, result["confidence"] - 50)
                    result["status"] = "RED"
                    result["reason"] += f" (Invalid regex pattern: {str(e)})"
            else:
                result["reason"] = "No regex pattern generated"
                result["status"] = "RED"
                result["confidence"] = 0
            
            print(f"DEBUG - Final result: {result}")
            return result
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_single_term: {str(e)}")
            error_result = {
                "name": term.get("name", "unknown"),
                "regex": "",
                "reason": f"Processing error: {str(e)}",
                "confidence": 0,
                "status": "RED"
            }
            print(f"DEBUG - Returning error result: {error_result}")
            return error_result

    def analyze_terms(self, input_path: str) -> dict:
        try:
            print(f"DEBUG - Reading input file: {input_path}")
            with open(input_path) as f:
                terms = json.load(f)
            
            if not isinstance(terms, list):
                terms = [terms]  # Handle single term case
            
            results = []
            for term in terms:
                result = self.analyze_single_term(term)
                if result:  # Only append if we got a valid result
                    results.append(result)
            
            return self._create_final_result(results)
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_terms: {str(e)}")
            return {
                "results": [],
                "summary": {
                    "total_terms": 0,
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }

    def _create_final_result(self, results: List[dict]) -> dict:
        try:
            print(f"DEBUG - Creating final result for {len(results)} terms")
            if not results:
                return {
                    "results": results,
                    "summary": {
                        "total_terms": 0,
                        "red_status": 0,
                        "amber_status": 0,
                        "green_status": 0,
                        "average_confidence": 0
                    }
                }
            
            status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
            total_confidence = 0
            
            for result in results:
                status = result.get("status", "AMBER")
                status_counts[status] += 1
                total_confidence += result.get("confidence", 0)
            
            final_result = {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": status_counts["RED"],
                    "amber_status": status_counts["AMBER"],
                    "green_status": status_counts["GREEN"],
                    "average_confidence": total_confidence // len(results) if results else 0
                }
            }
            print(f"DEBUG - Final result structure: {final_result}")
            return final_result
            
        except Exception as e:
            print(f"DEBUG - Error in _create_final_result: {str(e)}")
            return {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }






















class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:
        try:
            # Log the exact input
            print(f"DEBUG - Parser received text: {text}")
            
            # More permissive pattern matching
            pattern = (
                r"(?:Regex|Pattern):\s*([^\n]+)\s*\n"
                r"(?:Reason|Explanation):\s*([^\n]+(?:\n(?!\s*(?:Confidence|Status):)[^\n]+)*)\s*\n"
                r"Confidence:\s*(\d+)[^\n]*\n"
                r"Status:\s*(RED|AMBER|GREEN)"
            )
            
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            
            if not match:
                print(f"DEBUG - No match found in text: {text}")
                return {
                    "name": "",
                    "regex": ".*",  # Default to match anything if parsing fails
                    "reason": f"Failed to parse output. Using default pattern. Original text: {text[:100]}...",
                    "confidence": 30,
                    "status": "AMBER"
                }
            
            # Extract values with detailed logging
            regex = match.group(1).strip()
            reason = match.group(2).strip()
            confidence = int(match.group(3))
            status = match.group(4).upper()
            
            print(f"DEBUG - Parsed values: regex={regex}, confidence={confidence}, status={status}")
            
            return {
                "name": "",
                "regex": regex,
                "reason": reason,
                "confidence": min(max(confidence, 0), 100),
                "status": status if status in ("RED", "AMBER", "GREEN") else "AMBER"
            }
            
        except Exception as e:
            print(f"DEBUG - Parser error: {str(e)}")
            # Return a safe default instead of raising an error
            return {
                "name": "",
                "regex": ".*",
                "reason": f"Error parsing output: {str(e)}. Using default pattern.",
                "confidence": 30,
                "status": "AMBER"
            }

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a flexible and generic regex pattern for identifying the term '{name}' in text.
            
            Term Details:
            - Name: {name}
            - Definition: {definition}
            - Related Context: {related_terms}

            Guidelines for the regex pattern:
            1. Must be a valid regex pattern that matches the term and its variations
            2. Include common variations:
               - Different word separators (spaces, hyphens, underscores)
               - Optional parts that might be omitted
               - Common abbreviations
               - Case insensitivity
            3. Use word boundaries where appropriate
            4. Make it as generic as possible while maintaining accuracy

            Your response MUST follow this EXACT format:
            Regex: [your pattern here]
            Reason: [explain why this pattern will work]
            Confidence: [number between 0-100]
            Status: [RED, AMBER, or GREEN]

            Example response:
            Regex: (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b[- _]*(?:id|identifier|code|ref)?\\b
            Reason: Matches 'counterparty' and its variations, with optional identifier terms
            Confidence: 85
            Status: GREEN
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_single_term(self, term: dict) -> dict:
        try:
            print(f"DEBUG - Analyzing term: {term.get('name', 'unknown')}")
            
            # Prepare related terms info
            related_terms = term.get('related_terms', [])
            related_terms_info = "\n".join(
                [f"- {rt.get('definition', '')} (Example: {rt.get('example', '')})" 
                 for rt in related_terms]
            ) if related_terms else "No related terms provided."
            
            # Get LLM response
            chain_input = {
                "name": term.get("name", ""),
                "definition": term.get("definition", ""),
                "related_terms": related_terms_info
            }
            print(f"DEBUG - Chain input: {chain_input}")
            
            result = self.chain.invoke(chain_input)
            print(f"DEBUG - Chain result: {result}")
            
            # Ensure result is a dictionary
            if not isinstance(result, dict):
                print(f"DEBUG - Unexpected result type: {type(result)}")
                raise ValueError(f"Unexpected result type: {type(result)}")
            
            # Add the term name to the result
            result["name"] = term.get("name", "")
            
            # Validate the regex pattern
            try:
                re.compile(result["regex"])
            except re.error as e:
                print(f"DEBUG - Invalid regex pattern: {str(e)}")
                result["confidence"] = max(0, result.get("confidence", 0) - 50)
                result["status"] = "RED"
                result["reason"] += f" (Invalid regex pattern: {str(e)})"
            
            return result
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_single_term: {str(e)}")
            return {
                "name": term.get("name", "unknown"),
                "regex": ".*",
                "reason": f"Processing error: {str(e)}",
                "confidence": 30,
                "status": "AMBER"
            }

    def analyze_terms(self, input_path: str) -> dict:
        try:
            print(f"DEBUG - Reading input file: {input_path}")
            with open(input_path) as f:
                terms = json.load(f)
            
            if not isinstance(terms, list):
                terms = [terms]  # Handle single term case
            
            results = []
            for term in terms:
                result = self.analyze_single_term(term)
                results.append(result)
            
            return self._create_final_result(results)
            
        except Exception as e:
            print(f"DEBUG - Error in analyze_terms: {str(e)}")
            return {
                "results": [],
                "summary": {
                    "total_terms": 0,
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }

    def _create_final_result(self, results: List[dict]) -> dict:
        try:
            print(f"DEBUG - Creating final result for {len(results)} terms")
            
            if not results:
                return {
                    "results": [],
                    "summary": {
                        "total_terms": 0,
                        "red_status": 0,
                        "amber_status": 0,
                        "green_status": 0,
                        "average_confidence": 0
                    }
                }
            
            status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
            total_confidence = 0
            
            for result in results:
                status = result.get("status", "AMBER")
                status_counts[status] += 1
                total_confidence += result.get("confidence", 0)
            
            return {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": status_counts["RED"],
                    "amber_status": status_counts["AMBER"],
                    "green_status": status_counts["GREEN"],
                    "average_confidence": total_confidence // len(results) if results else 0
                }
            }
            
        except Exception as e:
            print(f"DEBUG - Error in _create_final_result: {str(e)}")
            return {
                "results": results,
                "summary": {
                    "total_terms": len(results),
                    "red_status": 0,
                    "amber_status": 0,
                    "green_status": 0,
                    "average_confidence": 0
                },
                "error": str(e)
            }





___________________________________________________________________________________

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a flexible and generic regex pattern for identifying '{name}' in text.
            
            Term: {name}
            Definition: {definition}
            Context: {related_terms}

            Guidelines for the regex pattern:
            1. Make it as generic as possible while maintaining accuracy
            2. Include common variations:
               - Different word separators (spaces, hyphens, underscores)
               - Optional parts that might be omitted
               - Common abbreviations and shortcuts
               - Case insensitivity
            3. Allow for flexible word ordering where appropriate
            4. Consider boundary conditions (word boundaries, spacing)
            5. Balance between coverage and precision

            Provide your response in EXACTLY this format:
            Regex: <your_regex_pattern>
            Reason: <explain why this pattern will work>
            Confidence: <0-100 score>
            Status: <RED|AMBER|GREEN>

            Example of a good generic pattern:
            Regex: (?i)\\b(?:counter[- _]?part(?:y|ies)|cp)\\b[- _]*(?:id|identifier|number|code|ref)?\\b
            Reason: Matches various forms including just 'counterparty' or 'cp', with optional identifier terms
            Confidence: 90
            Status: GREEN
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    # Rest of the class implementation remains the same...

    def analyze_terms(self, input_path: str) -> dict:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                # Add logging for debugging
                logger.debug(f"Processing term: {term['name']}")
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                if not isinstance(result, dict):
                    logger.error(f"Unexpected result type: {type(result)}")
                    raise ValueError(f"Unexpected result type: {type(result)}")
                
                result["name"] = term["name"]
                
                # Validate regex
                try:
                    re.compile(result["regex"])
                except re.error as e:
                    logger.error(f"Invalid regex pattern: {str(e)}")
                    result["confidence"] = max(0, result["confidence"] - 50)
                    result["status"] = "RED"
                    result["reason"] += f" (Invalid regex pattern: {str(e)})"
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing term {term['name']}: {str(e)}")
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)




















class RegexOutputParser(BaseOutputParser):
    def parse(self, text: str) -> dict:  # Changed to return dict instead of RegexResult
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            confidence = int(match.group("confidence"))
            if not 0 <= confidence <= 100:
                confidence = max(0, min(100, confidence))
            
            status = match.group("status").strip()
            if status not in ('RED', 'AMBER', 'GREEN'):
                status = 'RED'
            
            return {
                "name": "",  # Will be filled later
                "regex": match.group("regex").strip(),
                "reason": match.group("reason").strip(),
                "confidence": confidence,
                "status": status
            }
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser()  # No pydantic object needed
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> dict:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Result is already a dict, just update the name
                result["name"] = term["name"]
                
                # Validate regex
                try:
                    re.compile(result["regex"])
                except re.error:
                    result["confidence"] = max(0, result["confidence"] - 50)
                    result["status"] = "RED"
                    result["reason"] += " (Invalid regex pattern)"
                
                results.append(result)
                
            except Exception as e:
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[dict]) -> dict:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res["status"]] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r["confidence"] for r in results) // len(results)
            }
        }






# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        # Get result as dictionary
        result = app.run_analysis("input.json")
        
        # Result is already a dict, so we can dump it directly
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)


if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(State)
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            # Get the analysis result as a dictionary
            analysis_result = self.analyzer.analyze_terms(state["input_path"])
            state["result"] = analysis_result
            return state

        def finalize_output(self, state: dict) -> dict:
            # Access dictionary keys directly
            if isinstance(state["result"], dict) and "results" in state["result"]:
                logger.info(f"Processed {len(state['result']['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            # Return the result dictionary directly
            return final_state["result"]

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> dict:  # Changed return type to dict
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Convert Pydantic model to dict
                result_dict = {
                    "name": term["name"],
                    "regex": result.regex,
                    "reason": result.reason,
                    "confidence": result.confidence,
                    "status": result.status
                }
                
                # Validate regex
                try:
                    re.compile(result_dict["regex"])
                except re.error:
                    result_dict["confidence"] = max(0, result_dict["confidence"] - 50)
                    result_dict["status"] = "RED"
                    result_dict["reason"] += " (Invalid regex pattern)"
                
                results.append(result_dict)
                
            except Exception as e:
                results.append({
                    "name": term["name"],
                    "regex": "",
                    "reason": f"Processing error: {str(e)}",
                    "confidence": 0,
                    "status": "RED"
                })
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[dict]) -> dict:  # Changed return type to dict
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res["status"]] += 1
        
        return {
            "results": results,
            "summary": {
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r["confidence"] for r in results) // len(results)
            }
        }

if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(State)
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result['results'])} terms")
            return state

        def run(self, input_path: str) -> dict:  # Changed return type to dict
            config = {"input_path": input_path}
            compiled_graph = self.workflow.compile()
            final_state = compiled_graph.invoke(config)
            return final_state["result"]



class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> dict:  # Changed return type to dict
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        # Result is already a dict, so we can directly dump it
        print(json.dumps(result, indent=2))
        with open("output.json", "w") as f:
            json.dump(result, f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)



_________________________________________________________________________________________________________________________________________________


[
  {
    "name": "Python",
    "definition": "A high-level programming language",
    "related_terms": [
      {
        "name": "scripting",
        "definition": "A style of programming that automates tasks",
        "example": "Example: Python scripts can automate file management."
      },
      {
        "name": "interpreted",
        "definition": "Executed line-by-line rather than compiled",
        "example": "Example: Python code is executed in an interpreter."
      }
    ]
  },
  {
    "name": "Java",
    "definition": "A widely-used, class-based programming language",
    "related_terms": [
      {
        "name": "OOP",
        "definition": "Object-Oriented Programming paradigm",
        "example": "Example: Java’s design encourages encapsulation and inheritance."
      }
    ]
  }
]


if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            # Initialize StateGraph with a defined state type in the constructor
            self.workflow = StateGraph(State)  # State should be your state type definition
            
            # Add nodes with their respective functions
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            # Set up the workflow
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            config = {"input_path": input_path}
            result = self.workflow.compile().run(config)
            return result["result"]

# ... [Keep all existing imports and utility functions] ...

# Add these additional imports
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import BaseOutputParser
from typing import Tuple
import re
try:
    from langgraph.graph import END, StateGraph
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

# ================================
# Pydantic Models for Analysis Results
# ================================
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str

    @field_validator('confidence')
    def validate_confidence(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('Confidence must be between 0-100')
        return v

    @field_validator('status')
    def validate_status(cls, v):
        if v not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Status must be RED, AMBER, or GREEN')
        return v

class TermAnalysisResult(BaseModel):
    results: List[RegexResult]
    summary: Dict[str, int]

# ================================
# Regex Processing Components
# ================================
class RegexOutputParser(PydanticOutputParser):
    def parse(self, text: str) -> RegexResult:
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            return RegexResult(
                name="",  # Will be filled later
                regex=match.group("regex").strip(),
                reason=match.group("reason").strip(),
                confidence=int(match.group("confidence")),
                status=match.group("status").strip()
            )
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> TermAnalysisResult:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Validate and adjust confidence
                try:
                    re.compile(result.regex)
                except re.error:
                    result.confidence = max(0, result.confidence - 50)
                    result.status = "RED"
                    result.reason += " (Invalid regex pattern)"
                
                results.append(RegexResult(
                    name=term["name"],
                    regex=result.regex,
                    reason=result.reason,
                    confidence=result.confidence,
                    status=result.status
                ))
                
            except Exception as e:
                results.append(RegexResult(
                    name=term["name"],
                    regex="",
                    reason=f"Processing error: {str(e)}",
                    confidence=0,
                    status="RED"
                ))
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[RegexResult]) -> TermAnalysisResult:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res.status] += 1
        
        return TermAnalysisResult(
            results=results,
            summary={
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r.confidence for r in results) // len(results)
            }
        )

# ================================
# LangGraph Workflow (if available)
# ================================
if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(update_state_type=dict)
            
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            return self.workflow.invoke({"input_path": input_path})["result"]

# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> TermAnalysisResult:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        print(json.dumps(result.dict(), indent=2))
        with open("output.json", "w") as f:
            json.dump(result.dict(), f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)
