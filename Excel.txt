def find_top_k_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame, k: int = 4, threshold: float = 0.60) -> pd.DataFrame:
    """
    Find top k semantic matches using enhanced matching logic with threshold.
    
    Args:
        source_df: Source dataframe
        target_df: Target dataframe
        k: Number of matches to find
        threshold: Minimum similarity score threshold (default: 0.60)
    """
    logger.info(f"Processing semantic matches with {threshold*100}% threshold...")
    
    # Preprocess dataframes
    source_df = source_df.copy()
    target_df = target_df.copy()
    
    # Convert columns to string and clean
    source_df['name'] = source_df['name'].astype(str).apply(lambda x: x.strip())
    source_df['description'] = source_df['description'].astype(str).apply(lambda x: x.strip())
    target_df['pbt-name'] = target_df['pbt-name'].astype(str).apply(lambda x: x.strip())
    target_df['pbt-definition'] = target_df['pbt-definition'].astype(str).apply(lambda x: x.strip())
    
    # Process target entries with enhanced context
    logger.info("Processing target entries...")
    target_data = self.process_embeddings(
        target_df, 
        is_source=False, 
        desc="Processing target entries"
    )
    
    # Create target table in LanceDB with semantic configuration
    target_table = self.db.create_table(
        "target_vectors",
        data=target_data,
        mode="overwrite"
    )
    
    # Process source entries and find matches
    results = []
    batch_size = min(self.batch_size, len(source_df))
    
    for i in tqdm(range(0, len(source_df), batch_size), desc="Finding matches"):
        batch_df = source_df.iloc[i:i + batch_size]
        source_data = self.process_embeddings(
            batch_df,
            is_source=True,
            desc=f"Processing batch {i//batch_size + 1}"
        )
        
        for source_item in source_data:
            try:
                # Get initial matches
                initial_matches = target_table.search(source_item['vector'])\
                    .limit(k * 3)\  # Get more matches initially for filtering
                    .to_list()
                
                # Enhanced filtering and scoring
                scored_matches = []
                for match in initial_matches:
                    # Calculate base similarity score
                    base_score = 1 / (1 + float(match['_distance']))  # Convert distance to similarity
                    
                    # Calculate additional similarity scores
                    name_similarity = self._calculate_name_similarity(
                        source_item['name'],
                        match['pbt_name']
                    )
                    
                    context_similarity = self._calculate_context_similarity(
                        source_item['description'],
                        match['pbt_definition']
                    )
                    
                    # Combined score with weights
                    final_score = (
                        base_score * 0.4 +  # Base embedding similarity
                        name_similarity * 0.3 +  # Name similarity importance
                        context_similarity * 0.3  # Context similarity importance
                    )
                    
                    # Only keep matches above threshold
                    if final_score >= threshold:
                        scored_matches.append({
                            'pbt_name': match['pbt_name'],
                            'pbt_definition': match['pbt_definition'],
                            'score': final_score
                        })
                
                # Sort matches by score
                scored_matches.sort(key=lambda x: x['score'], reverse=True)
                
                # Create result entry
                result = {
                    'name': source_item['name'],
                    'description': source_item['description']
                }
                
                # Add matches if we found any above threshold
                if scored_matches:
                    for rank in range(1, k + 1):
                        if rank <= len(scored_matches):
                            match = scored_matches[rank - 1]
                            result.update({
                                f'match_{rank}_pbt_name': match['pbt_name'],
                                f'match_{rank}_score': round(match['score'], 4),
                                f'match_{rank}_definition': match['pbt_definition']
                            })
                        else:
                            result.update({
                                f'match_{rank}_pbt_name': 'PBT not available',
                                f'match_{rank}_score': 0.0,
                                f'match_{rank}_definition': 'No match found above threshold'
                            })
                else:
                    # No matches above threshold
                    for rank in range(1, k + 1):
                        result.update({
                            f'match_{rank}_pbt_name': 'PBT not available',
                            f'match_{rank}_score': 0.0,
                            f'match_{rank}_definition': 'No match found above threshold'
                        })
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing match: {str(e)}")
                self._add_error_result(results, source_item, k, str(e))
    
    # Convert to DataFrame
    result_df = pd.DataFrame(results)
    
    # Add summary statistics
    score_cols = [f'match_{i}_score' for i in range(1, k+1)]
    result_df['avg_score'] = result_df[score_cols].mean(axis=1)
    result_df['max_score'] = result_df[score_cols].max(axis=1)
    result_df['min_score'] = result_df[score_cols].min(axis=1)
    
    # Add match quality indicators
    result_df['has_match'] = result_df['match_1_pbt_name'] != 'PBT not available'
    result_df['match_count'] = result_df.apply(
        lambda row: sum(1 for i in range(1, k+1) 
                       if row[f'match_{i}_pbt_name'] != 'PBT not available'),
        axis=1
    )
    
    # Sort by best match score
    result_df.sort_values(by=['has_match', 'match_1_score'], 
                         ascending=[False, False], 
                         inplace=True)
    
    # Log matching statistics
    total_items = len(result_df)
    items_with_matches = result_df['has_match'].sum()
    logger.info(f"Matching complete: {items_with_matches}/{total_items} items matched above {threshold*100}% threshold")
    
    return result_df

def _calculate_name_similarity(self, source_name: str, target_name: str) -> float:
    """Calculate normalized similarity between source and target names."""
    try:
        # Convert to lowercase and split into words
        source_words = set(source_name.lower().split())
        target_words = set(target_name.lower().split())
        
        # Calculate word overlap
        common_words = source_words.intersection(target_words)
        total_words = source_words.union(target_words)
        
        if not total_words:
            return 0.0
        
        # Calculate Jaccard similarity
        return len(common_words) / len(total_words)
    except Exception:
        return 0.0

def _calculate_context_similarity(self, source_desc: str, target_desc: str) -> float:
    """Calculate normalized context similarity between descriptions."""
    try:
        # Convert to lowercase
        source_desc = source_desc.lower()
        target_desc = target_desc.lower()
        
        # Split into words and remove common stop words
        source_words = set(source_desc.split())
        target_words = set(target_desc.split())
        
        # Calculate word overlap
        common_words = source_words.intersection(target_words)
        total_words = source_words.union(target_words)
        
        if not total_words:
            return 0.0
            
        # Calculate weighted similarity
        word_similarity = len(common_words) / len(total_words)
        
        # Add phrase matching
        source_phrases = self._extract_key_phrases(source_desc)
        target_phrases = self._extract_key_phrases(target_desc)
        
        phrase_similarity = len(set(source_phrases) & set(target_phrases)) / \
                          max(1, len(set(source_phrases) | set(target_phrases)))
        
        # Combine word and phrase similarity
        return (word_similarity * 0.6) + (phrase_similarity * 0.4)
    except Exception:
        return 0.0

def _add_error_result(self, results: List[Dict], source_item: Dict, k: int, error_msg: str):
    """Add error result entry."""
    result = {
        'name': source_item.get('name', 'Error'),
        'description': source_item.get('description', 'Error')
    }
    for rank in range(1, k + 1):
        result.update({
            f'match_{rank}_pbt_name': 'PBT not available',
            f'match_{rank}_score': 0.0,
            f'match_{rank}_definition': f'Error: {error_msg}'
        })
    results.append(result)
