import os
import re
import uuid
import json
import logging
import chardet
import pandas as pd
import requests
import chromadb
from chromadb.config import Settings
from pathlib import Path
from typing import Optional, List, Dict, Any

from dotenv import load_dotenv, dotenv_values
from azure.identity import ClientSecretCredential
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# Import your EmbeddingClient from the snippet in 'ooai_client_sdk.azopenaiembrdding'
# which has a method: generate_embeddings(text: str)
from ooai_client_sdk.azopenaiembrdding import EmbeddingClient

# ------------------------------
# Logging Configuration
# ------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ------------------------------
# Helper Functions
# ------------------------------
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

def stable_chunk_text(text: str, max_chars: int = 1000) -> List[str]:
    """Chunk text into smaller pieces at sentence boundaries."""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chars:
            current_chunk = (current_chunk + " " + sentence).strip() if current_chunk else sentence
        else:
            chunks.append(current_chunk)
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def remove_invisible_chars(s: str) -> str:
    """Remove invisible directional characters like \u202a, \u202b, etc."""
    return re.sub(r'[\u202a\u202b\u202c\u202d\u202e]', '', s).strip()

def guess_file_encoding(file_path: str, num_bytes: int = 4096) -> Optional[str]:
    """Use chardet to guess file encoding."""
    with open(file_path, 'rb') as f:
        raw_data = f.read(num_bytes)
    guess = chardet.detect(raw_data)
    encoding = guess.get('encoding', None)
    if encoding:
        logger.info(f"chardet guessed encoding='{encoding}' for {file_path}")
    else:
        logger.warning(f"chardet could not guess encoding for {file_path}")
    return encoding

def read_csv_flexible(csv_path: str) -> pd.DataFrame:
    """
    Attempt reading a CSV with chardet guess first, then fallback to known encodings,
    skipping bad lines and replacing weird characters.
    """
    enc = guess_file_encoding(csv_path)
    if enc:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            return df
        except Exception as e:
            logger.warning(f"Reading with guessed encoding '{enc}' failed: {e}")

    fallback_encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']
    for enc in fallback_encodings:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            logger.info(f"Successfully read CSV with fallback encoding='{enc}'")
            return df
        except Exception as e:
            logger.warning(f"Failed reading with encoding={enc}: {e}")

    raise ValueError("Unable to read CSV in any known encoding or chardet guess.")

# ------------------------------
# OSEnv Class: Environment, Proxy, Certificate, and Token
# ------------------------------
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")

        # If PROXY_ENABLED is True, set the proxy environment
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")

        # If SECURED_ENDPOINTS is True, get an Azure AD token
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints using Azure AD")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        if not os.path.isabs(certificate_path):
            certificate_path = os.path.abspath(certificate_path)
        if not is_file_readable(certificate_path):
            raise Exception("Certificate file missing or not readable")
        self.set("REQUESTS_CA_BUNDLE", certificate_path)
        self.set("SSL_CERT_FILE", certificate_path)
        self.set("CURL_CA_BUNDLE", certificate_path)
        logger.info(f"Certificate path set to: {certificate_path}")

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        if not os.path.isabs(dotenvfile):
            dotenvfile = os.path.abspath(dotenvfile)
        if is_file_readable(dotenvfile):
            logger.info(f"Loading environment variables from {dotenvfile}")
            temp_dict = dotenv_values(dotenvfile)
            for k, v in temp_dict.items():
                self.set(k, v, print_val)
            del temp_dict

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        return os.environ.get(var_name, default)

    def set_proxy(self) -> None:
        """
        We do not set NO_PROXY so that Azure calls also go through the proxy.
        """
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            raise ValueError("Missing proxy credentials for AD_USERNAME, AD_USER_PW, HTTPS_PROXY_DOMAIN.")
        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)
        logger.info("Proxy environment variables set (NO_PROXY not used).")

    def get_azure_token(self) -> str:
        tenant_id = self.get("AZURE_TENANT_ID")
        client_id = self.get("AZURE_CLIENT_ID")
        client_secret = self.get("AZURE_CLIENT_SECRET")
        credential = ClientSecretCredential(tenant_id, client_id, client_secret)
        token = credential.get_token("https://cognitiveservices.azure.com/.default")
        self.set("AZURE_TOKEN", token.token, print_val=False)
        logger.info("Azure token acquired successfully")
        return token.token

    def list_env_vars(self) -> None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [HIDDEN]")
            else:
                logger.info(f"{var}: {self.get(var)}")

# ------------------------------
# AzureChatbot: For validation using Azure OpenAI (LangChain)
# ------------------------------
class AzureChatbot:
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        logger.info("Initializing chatbot...")
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()

    def _setup_chat_model(self) -> None:
        from langchain.chat_models import AzureChatOpenAI
        from langchain.chains import ConversationChain
        from langchain.memory import ConversationBufferMemory
        
        endpoint = self.env.get("AZURE_OPENAI_ENDPOINT")
        api_version = self.env.get("API_VERSION", "2024-02-01")
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))

        # Only Azure AD token usage
        if not self.env.token:
            raise ValueError("Missing Azure AD token. Please set SECURED_ENDPOINTS=True or provide credentials.")
        
        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            openai_api_version=api_version,
            azure_endpoint=endpoint,
            azure_ad_token=self.env.token
        )
        
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        logger.info("Chat model initialized successfully")

    def validate_matches(self, query: str, matches: List[Dict[str, Any]]) -> str:
        prompt = f"""You are an expert validation agent. Given the query:
"{query}"
and the following candidate matches:
"""
        for idx, match in enumerate(matches, start=1):
            prompt += f"\n{idx}. Name: {match.get('name')}\n   Definition: {match.get('definition')}\n"
        prompt += "\nIf these matches are satisfactory, simply reply OK. Otherwise, suggest the best matching candidate from our knowledge base in the format:\nName: <name>\nDefinition: <definition>\nExplanation: <brief explanation>."
        try:
            response = self.conversation.predict(input=prompt)
            return response.strip()
        except Exception as e:
            logger.error(f"Validation agent error: {str(e)}")
            return "Validation failed"

    def chat(self, message: str) -> str:
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            response = self.conversation.predict(input=message)
            return response
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

# ------------------------------
# VectorStoreManager: Manage local ChromaDB storage
# ------------------------------
class VectorStoreManager:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(name="knowledge_base")
        logger.info("ChromaDB collection initialized locally")

    def add_documents(self, documents: List[Dict[str, Any]], embeddings: List[List[float]]):
        if len(documents) != len(embeddings):
            raise ValueError("Documents and embeddings count mismatch")
        ids = [str(uuid.uuid4()) for _ in documents]
        metadatas = [{"name": doc["name"], "definition": doc["definition"]} for doc in documents]
        # We'll store doc content as "name||definition" or just name+definition with space
        # but the user wants a single string for each row, so let's do "name definition"
        contents = [doc["name"] + " " + doc["definition"] for doc in documents]
        self.collection.add(ids=ids, documents=contents, embeddings=embeddings, metadatas=metadatas)
        logger.info(f"Added {len(documents)} documents to the vector store")

    def query(self, query_embedding: List[float], n_results: int = 4) -> List[Dict[str, Any]]:
        results = self.collection.query(query_embedding, n_results=n_results)
        matches = []
        if results and "metadatas" in results:
            for meta in results["metadatas"]:
                matches.append(meta)
        return matches

# ------------------------------
# CSV Processing
# ------------------------------
def load_csv_as_documents(csv_path: str) -> List[Dict[str, Any]]:
    df = read_csv_flexible(csv_path)
    if not {"name", "definition"}.issubset(set(df.columns)):
        raise ValueError("CSV must contain 'name' and 'definition' columns")
    documents = []
    for _, row in df.iterrows():
        # We store row by row as name and definition
        documents.append({
            "name": str(row["name"]),
            "definition": str(row["definition"])
        })
    return documents

def save_results_to_csv(results: List[Dict[str, Any]], output_path: str) -> None:
    df = pd.DataFrame(results)
    df.to_csv(output_path, index=False)
    logger.info(f"Results saved to {output_path}")

# ------------------------------
# Main Function: Workflow
# ------------------------------
def main():
    """
    Main script that:
      1) Initializes environment (OSEnv) for proxy, SSL, and AD token
      2) Creates a chatbot for validation
      3) Imports your existing EmbeddingClient from 'ooai_client_sdk.azopenaiembrdding'
         which has generate_embeddings(text: str) -> List[float]
      4) For each row in knowledge CSV, do name + " " + definition => generate_embeddings
      5) Store embeddings in local ChromaDB
      6) For each row in query CSV, do name + " " + definition => generate_embeddings => top-4 matches => chatbot validation
      7) Save results to final CSV
    """
    try:
        # If in Jupyter or environment lacking __file__, fallback to current working dir
        if '__file__' in globals():
            base_path = Path(__file__).resolve().parent
        else:
            base_path = Path.cwd()
        base_dir = base_path / "env"

        config_path = str(base_dir / "config.env")
        creds_path = str(base_dir / "credentials.env")
        cert_path = str(base_dir / "cacert.pem")
        for f in [config_path, creds_path, cert_path]:
            if not os.path.exists(f):
                print(f"Missing required file: {f}")
                return
        
        logger.info("Initializing environment (OSEnv)...")
        env = OSEnv(config_path, creds_path, cert_path)
        
        logger.info("Initializing chatbot agent (for match validation)...")
        chatbot = AzureChatbot(config_path, creds_path, cert_path)
        
        logger.info("Importing your EmbeddingClient from 'ooai_client_sdk.azopenaiembrdding' (AD-based only)...")
        # from ooai_client_sdk.azopenaiembrdding import EmbeddingClient
        embedding_client = EmbeddingClient()  # Adjust if your snippet requires arguments
        
        logger.info("Initializing local ChromaDB vector store...")
        vector_store = VectorStoreManager(persist_dir="./chroma_db")
        
        # Step 1: Knowledge Base Ingestion
        kb_csv = input("Enter path to knowledge base CSV: ").strip()
        kb_csv = remove_invisible_chars(kb_csv)
        kb_documents = load_csv_as_documents(kb_csv)

        kb_embeddings = []
        for doc in kb_documents:
            # Combine name + " " + definition into one string
            text_for_embedding = doc["name"] + " " + doc["definition"]
            # Generate embedding for each row individually
            emb = embedding_client.generate_embeddings(text_for_embedding)
            if emb is None or len(emb) == 0:
                logger.error(f"Failed to get embedding for: {text_for_embedding[:30]}...")
                kb_embeddings.append([])
            else:
                kb_embeddings.append(emb)
        
        # Store docs + embeddings
        vector_store.add_documents(kb_documents, kb_embeddings)
        
        # Step 2: Query CSV -> Vector matching
        query_csv = input("Enter path to query CSV: ").strip()
        query_csv = remove_invisible_chars(query_csv)
        query_df = read_csv_flexible(query_csv)
        if not {"name", "definition"}.issubset(set(query_df.columns)):
            raise ValueError("Query CSV must contain 'name' and 'definition' columns")
        
        results = []
        for index, row in query_df.iterrows():
            text_for_embedding = str(row["name"]) + " " + str(row["definition"])
            logger.info(f"Processing query {index+1}: {row['name']}")
            q_emb = embedding_client.generate_embeddings(text_for_embedding)
            if q_emb is None or len(q_emb) == 0:
                logger.error(f"Failed to generate embedding for query: {text_for_embedding[:30]}...")
                matches = []
                validation_response = "Embedding error"
            else:
                matches = vector_store.query(q_emb, n_results=4)
                # Validate/improve the match using the chatbot
                validation_response = chatbot.validate_matches(text_for_embedding, matches)
            
            results.append({
                "query_name": row["name"],
                "query_definition": row["definition"],
                "matches": json.dumps(matches),
                "validation": validation_response
            })
        
        # Step 3: Save results
        output_csv = input("Enter path for output CSV (e.g., output.csv): ").strip()
        output_csv = remove_invisible_chars(output_csv)
        save_results_to_csv(results, output_csv)
        print("Workflow completed successfully.")
        
    except Exception as e:
        logger.exception(f"Unexpected error: {str(e)}")
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()
