import os
from langchain.chat_models import AzureChatOpenAI
from langchain_community.graphs.index_creator import GraphIndexCreator
from langchain_community.chains.graph_qa.base import GraphQAChain
from langchain.vectorstores import Chroma
from langchain.agents import Tool, ZeroShotAgent, AgentExecutor
from chromadb.config import Settings

# Import your existing code (without modifying it):
from genai_env_setup import OSEnv           # we rely on OSEnv to set AZURE_TOKEN
from azoai_embedding_client import EmbeddingClient, Document as AzoaiDocument

from langchain.docstore.document import Document as LC_Document
from langchain.embeddings.base import Embeddings

class EmbeddingClientLangChainAdapter(Embeddings):
    def __init__(self, embedding_client: EmbeddingClient, embeddings_model: str = "text-embedding-3-large"):
        self.embedding_client = embedding_client
        self.embeddings_model = embeddings_model

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        embeddings = []
        for txt in texts:
            doc = AzoaiDocument(text=txt, id="")
            updated_doc = self.embedding_client.generate_embeddings(doc, embeddings_model=self.embeddings_model)
            embeddings.append(updated_doc.embedding)
        return embeddings

    def embed_query(self, text: str) -> list[float]:
        doc = AzoaiDocument(text=text, id="")
        updated_doc = self.embedding_client.generate_embeddings(doc, embeddings_model=self.embeddings_model)
        return updated_doc.embedding

class SemanticSearchAgent:
    """
    Example snippet that shows how to fix self.llm = AzureChatOpenAI(...)
    so that it uses AZURE_TOKEN from environment as a Bearer token.
    """
    def __init__(self, kb, env: OSEnv):
        self.kb = kb
        self.env = env

        # 1) Let OSEnv set environment variables (including AZURE_TOKEN)
        self.env.set_azure_token()

        # 2) Create your embedding adapter
        embedding_client = EmbeddingClient()
        self.embedding = EmbeddingClientLangChainAdapter(embedding_client)

        # 3) Build a Chroma vector store
        CHROMA_SETTINGS = Settings(
            anonymized_telemetry=False,
            persist_directory="chromadb-data"
        )
        from langchain.vectorstores import Chroma
        self.vs = Chroma.from_documents(
            documents=self.kb.docs,
            embedding=self.embedding,
            collection_name="kb_collection",
            client_settings=CHROMA_SETTINGS
        )

        # 4) FIX: Pass the Bearer token as an HTTP header
        #    We use openai_api_base (not azure_endpoint) to avoid conflict.
        #    openai_api_key is unused, but must be a string. 
        #    The 'headers' dict includes "Authorization": "Bearer <token>".
        self.llm = AzureChatOpenAI(
            openai_api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),  # e.g. "https://YOUR-RESOURCE.openai.azure.com"
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"), 
            deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
            openai_api_key="unused",  # We rely on the token, not an API key
            headers={"Authorization": f"Bearer {os.getenv('AZURE_TOKEN')}"}
        )

        # 5) Build a knowledge graph
        graph_creator = GraphIndexCreator(llm=self.llm)
        main_graph = graph_creator.graph_type()  # empty NetworkxEntityGraph
        for doc in self.kb.docs:
            partial_graph = graph_creator.from_text(doc.page_content)
            for triple in partial_graph.get_triples():
                main_graph.add_triple(triple)
        self.graph = main_graph

        # 6) GraphQAChain
        self.graph_qa_chain = GraphQAChain.from_llm(self.llm, graph=self.graph)

        # 7) Tools for the agent
        self.tools = [
            Tool(
                name="GraphQATool",
                func=self._graph_qa,
                description="Use this to query the knowledge graph..."
            ),
            Tool(
                name="VectorStoreSearch",
                func=self._vectorstore_search,
                description="Use this for semantic search with confidence..."
            ),
        ]

        # 8) Create an Agent
        prefix = """You are an AI assistant with access to the following tools:"""
        suffix = """Begin!"""
        prompt = ZeroShotAgent.create_prompt(
            self.tools,
            prefix=prefix,
            suffix=suffix,
            input_variables=["input"]
        )
        agent = ZeroShotAgent(llm=self.llm, tools=self.tools, prompt=prompt, verbose=True)
        self.agent_executor = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=self.tools,
            verbose=True
        )

    def _graph_qa(self, query: str) -> str:
        return self.graph_qa_chain.run(query)

    def _vectorstore_search(self, query: str) -> str:
        results = self.vs.similarity_search_with_score(query, k=3)
        if not results:
            return "No relevant matches found."
        lines = []
        for idx, (doc, score) in enumerate(results, start=1):
            confidence = max(0.0, min(1.0, 1.0 - score))
            rating = "Green" if confidence >= 0.8 else ("Amber" if confidence >= 0.5 else "Red")
            reason = f"Confidence {confidence:.2f}, which is {rating}. Matched: {doc.page_content}"
            lines.append(
                f"Match #{idx}\n"
                f"Name: {doc.metadata.get('name','Unknown')}\n"
                f"ID: {doc.metadata.get('id','No ID')}\n"
                f"Confidence: {confidence:.2f}\n"
                f"Rating: {rating}\n"
                f"Reason: {reason}\n"
            )
        return "\n".join(lines)

    def run_query(self, query: str) -> str:
        return self.agent_executor.run(query)
