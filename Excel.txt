# ... [Keep all existing imports and utility functions] ...

# Add these additional imports
from langchain.output_parsers import PydanticOutputParser
from langchain.schema import BaseOutputParser
from typing import Tuple
import re
try:
    from langgraph.graph import END, StateGraph
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False

# ================================
# Pydantic Models for Analysis Results
# ================================
class RegexResult(BaseModel):
    name: str
    regex: str
    reason: str
    confidence: int
    status: str

    @field_validator('confidence')
    def validate_confidence(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('Confidence must be between 0-100')
        return v

    @field_validator('status')
    def validate_status(cls, v):
        if v not in ('RED', 'AMBER', 'GREEN'):
            raise ValueError('Status must be RED, AMBER, or GREEN')
        return v

class TermAnalysisResult(BaseModel):
    results: List[RegexResult]
    summary: Dict[str, int]

# ================================
# Regex Processing Components
# ================================
class RegexOutputParser(PydanticOutputParser):
    def parse(self, text: str) -> RegexResult:
        try:
            pattern = (
                r"Regex:\s*(?P<regex>.+?)\n"
                r"Reason:\s*(?P<reason>.+?)\n"
                r"Confidence:\s*(?P<confidence>\d+).*?\n"
                r"Status:\s*(?P<status>RED|AMBER|GREEN)"
            )
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                raise ValueError("Could not parse LLM output")
            
            return RegexResult(
                name="",  # Will be filled later
                regex=match.group("regex").strip(),
                reason=match.group("reason").strip(),
                confidence=int(match.group("confidence")),
                status=match.group("status").strip()
            )
        except Exception as e:
            raise ValueError(f"Failed to parse output: {e}")

class TermAnalyzer:
    def __init__(self, chatbot: AzureChatbot):
        self.chatbot = chatbot
        self.parser = RegexOutputParser(pydantic_object=RegexResult)
        
        self.prompt_template = PromptTemplate(
            input_variables=["name", "definition", "related_terms"],
            template="""
            Generate a regex pattern for the technical term '{name}' which is defined as: {definition}.
            Consider these related terms for context but DO NOT include their names in the pattern:
            {related_terms}

            Requirements:
            1. Match common variations (case-insensitive, abbreviations, typos)
            2. Avoid false positives with similar terms
            3. Output format:
               Regex: <pattern>
               Reason: <generation reason>
               Confidence: <0-100 score>
               Status: <RED|AMBER|GREEN>
            """
        )
        
        self.chain = LLMChain(
            llm=self.chatbot.llm,
            prompt=self.prompt_template,
            output_parser=self.parser
        )

    def analyze_terms(self, input_path: str) -> TermAnalysisResult:
        with open(input_path) as f:
            terms = json.load(f)
        
        results = []
        for term in terms:
            try:
                related_terms_info = "\n".join(
                    [f"- {rt['definition']} (Example: {rt['example']})" 
                     for rt in term['related_terms']]
                )
                
                result = self.chain.invoke({
                    "name": term["name"],
                    "definition": term["definition"],
                    "related_terms": related_terms_info
                })
                
                # Validate and adjust confidence
                try:
                    re.compile(result.regex)
                except re.error:
                    result.confidence = max(0, result.confidence - 50)
                    result.status = "RED"
                    result.reason += " (Invalid regex pattern)"
                
                results.append(RegexResult(
                    name=term["name"],
                    regex=result.regex,
                    reason=result.reason,
                    confidence=result.confidence,
                    status=result.status
                ))
                
            except Exception as e:
                results.append(RegexResult(
                    name=term["name"],
                    regex="",
                    reason=f"Processing error: {str(e)}",
                    confidence=0,
                    status="RED"
                ))
        
        return self._create_final_result(results)

    def _create_final_result(self, results: List[RegexResult]) -> TermAnalysisResult:
        status_counts = {"RED": 0, "AMBER": 0, "GREEN": 0}
        for res in results:
            status_counts[res.status] += 1
        
        return TermAnalysisResult(
            results=results,
            summary={
                "total_terms": len(results),
                "red_status": status_counts["RED"],
                "amber_status": status_counts["AMBER"],
                "green_status": status_counts["GREEN"],
                "average_confidence": sum(r.confidence for r in results) // len(results)
            }
        )

# ================================
# LangGraph Workflow (if available)
# ================================
if LANGGRAPH_AVAILABLE:
    class AnalysisWorkflow:
        def __init__(self, analyzer: TermAnalyzer):
            self.analyzer = analyzer
            self.workflow = StateGraph(update_state_type=dict)
            
            self.workflow.add_node("validate_input", self.validate_input)
            self.workflow.add_node("process_terms", self.process_terms)
            self.workflow.add_node("finalize_output", self.finalize_output)
            
            self.workflow.set_entry_point("validate_input")
            self.workflow.add_edge("validate_input", "process_terms")
            self.workflow.add_edge("process_terms", "finalize_output")
            self.workflow.add_edge("finalize_output", END)

        def validate_input(self, state: dict) -> dict:
            if not Path(state["input_path"]).exists():
                raise ValueError("Input file does not exist")
            return state

        def process_terms(self, state: dict) -> dict:
            state["result"] = self.analyzer.analyze_terms(state["input_path"])
            return state

        def finalize_output(self, state: dict) -> dict:
            result = state["result"]
            logger.info(f"Processed {len(result.results)} terms")
            return state

        def run(self, input_path: str) -> TermAnalysisResult:
            return self.workflow.invoke({"input_path": input_path})["result"]

# ================================
# Application Class
# ================================
class Application:
    def __init__(self):
        self.env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.chatbot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        self.analyzer = TermAnalyzer(self.chatbot)
        
        if LANGGRAPH_AVAILABLE:
            self.workflow = AnalysisWorkflow(self.analyzer)
        else:
            self.workflow = None

    def run_analysis(self, input_path: str) -> TermAnalysisResult:
        if self.workflow:
            return self.workflow.run(input_path)
        return self.analyzer.analyze_terms(input_path)

# ================================
# Main Execution
# ================================
if __name__ == "__main__":
    try:
        app = Application()
        result = app.run_analysis("input.json")
        
        print(json.dumps(result.dict(), indent=2))
        with open("output.json", "w") as f:
            json.dump(result.dict(), f, indent=2)
            
        logger.info("Analysis completed successfully")
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        sys.exit(1)
