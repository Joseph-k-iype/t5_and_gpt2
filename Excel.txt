import os
import re
import uuid
import json
import logging
import chardet
import pandas as pd
import requests
import chromadb
from chromadb.config import Settings
from pathlib import Path
from typing import Optional, List, Dict, Any

from dotenv import load_dotenv, dotenv_values
from azure.identity import ClientSecretCredential
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# Import your Document + EmbeddingClient from your snippet
# We'll assume you have something like:
#   class Document(BaseModel):
#       text: str
#       embedding: List[float] = []
#   class EmbeddingClient:
#       def generate_embeddings(self, doc: Document) -> Document: ...
from ooai_client_sdk.azopenaiembrdding import Document, EmbeddingClient

# ------------------------------
# Logging Configuration
# ------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ------------------------------
# Helper Functions
# ------------------------------
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def remove_invisible_chars(s: str) -> str:
    """Remove invisible directional characters like \u202a, \u202b, etc."""
    return re.sub(r'[\u202a\u202b\u202c\u202d\u202e]', '', s).strip()

def guess_file_encoding(file_path: str, num_bytes: int = 4096) -> Optional[str]:
    """Use chardet to guess the file's encoding."""
    with open(file_path, 'rb') as f:
        raw_data = f.read(num_bytes)
    guess = chardet.detect(raw_data)
    encoding = guess.get('encoding', None)
    if encoding:
        logger.info(f"chardet guessed encoding='{encoding}' for {file_path}")
    else:
        logger.warning(f"chardet could not guess encoding for {file_path}")
    return encoding

def read_csv_flexible(csv_path: str) -> pd.DataFrame:
    """Try reading CSV with chardet guess, then fallback encodings, skipping bad lines."""
    enc = guess_file_encoding(csv_path)
    if enc:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            return df
        except Exception as e:
            logger.warning(f"Reading with guessed encoding '{enc}' failed: {e}")

    fallback_encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']
    for enc in fallback_encodings:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            logger.info(f"Successfully read CSV with fallback encoding='{enc}'")
            return df
        except Exception as e:
            logger.warning(f"Failed reading with encoding={enc}: {e}")

    raise ValueError("Unable to read CSV in any known encoding or chardet guess.")

# ------------------------------
# OSEnv Class: Environment, Proxy, Certificate, and Token
# ------------------------------
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")

        # If PROXY_ENABLED is True, set the proxy environment
        if self.get("PROXY_ENABLED", "False") == "True":
            self.set_proxy()
            logger.info("Proxy configured")

        # If SECURED_ENDPOINTS is True, get an Azure AD token
        if self.get("SECURED_ENDPOINTS", "False") == "True":
            logger.info("Securing endpoints using Azure AD")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        if not os.path.isabs(certificate_path):
            certificate_path = os.path.abspath(certificate_path)
        if not is_file_readable(certificate_path):
            raise Exception("Certificate file missing or not readable")
        self.set("REQUESTS_CA_BUNDLE", certificate_path)
        self.set("SSL_CERT_FILE", certificate_path)
        self.set("CURL_CA_BUNDLE", certificate_path)
        logger.info(f"Certificate path set to: {certificate_path}")

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        if not os.path.isabs(dotenvfile):
            dotenvfile = os.path.abspath(dotenvfile)
        if is_file_readable(dotenvfile):
            logger.info(f"Loading environment variables from {dotenvfile}")
            from dotenv import dotenv_values
            temp_dict = dotenv_values(dotenvfile)
            for k, v in temp_dict.items():
                self.set(k, v, print_val)
            del temp_dict

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        return os.environ.get(var_name, default)

    def set_proxy(self) -> None:
        # We do not set NO_PROXY so that Azure calls also go through the proxy.
        from azure.identity import ClientSecretCredential
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            raise ValueError("Missing proxy credentials for AD_USERNAME, AD_USER_PW, HTTPS_PROXY_DOMAIN.")
        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)
        logger.info("Proxy environment variables set (NO_PROXY not used).")

    def get_azure_token(self) -> str:
        from azure.identity import ClientSecretCredential
        tenant_id = self.get("AZURE_TENANT_ID")
        client_id = self.get("AZURE_CLIENT_ID")
        client_secret = self.get("AZURE_CLIENT_SECRET")
        credential = ClientSecretCredential(tenant_id, client_id, client_secret)
        token = credential.get_token("https://cognitiveservices.azure.com/.default")
        self.set("AZURE_TOKEN", token.token, print_val=False)
        logger.info("Azure token acquired successfully")
        return token.token

    def list_env_vars(self) -> None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [HIDDEN]")
            else:
                logger.info(f"{var}: {self.get(var)}")

# ------------------------------
# AzureChatbot: Using new AzureChatOpenAI param style
# ------------------------------
class AzureChatbot:
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        logger.info("Initializing chatbot...")
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()

    def _setup_chat_model(self) -> None:
        from langchain.chat_models import AzureChatOpenAI
        from langchain.chains import ConversationChain
        from langchain.memory import ConversationBufferMemory
        
        # The new param names in LangChain
        openai_api_base = self.env.get("AZURE_OPENAI_ENDPOINT")  # e.g. "https://<your-resource>.openai.azure.com"
        openai_api_version = self.env.get("API_VERSION", "2023-05-15")
        deployment_name = self.env.get("MODEL_NAME", "gpt-4o-mini")  # Your Azure deployment name
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))

        if not self.env.token:
            raise ValueError("Missing Azure AD token. Please set SECURED_ENDPOINTS=True or provide credentials.")
        
        # For Azure AD usage:
        #   openai_api_type="azure_ad"
        #   openai_api_key=self.env.token
        self.llm = AzureChatOpenAI(
            openai_api_base=openai_api_base,
            openai_api_version=openai_api_version,
            openai_api_type="azure_ad",
            openai_api_key=self.env.token,     # the bearer token
            deployment_name=deployment_name,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        logger.info("Chat model initialized successfully")

    def validate_matches(self, query: str, matches: List[Dict[str, Any]]) -> str:
        prompt = f"""You are an expert validation agent. Given the query:
"{query}"
and the following candidate matches:
"""
        for idx, match in enumerate(matches, start=1):
            prompt += f"\n{idx}. Name: {match.get('name')}\n   Definition: {match.get('definition')}\n"
        prompt += "\nIf these matches are satisfactory, simply reply OK. Otherwise, suggest the best matching candidate from our knowledge base in the format:\nName: <name>\nDefinition: <definition>\nExplanation: <brief explanation>."
        
        try:
            response = self.conversation.predict(input=prompt)
            return response.strip()
        except Exception as e:
            logger.error(f"Validation agent error: {str(e)}")
            return "Validation failed"

    def chat(self, message: str) -> str:
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            response = self.conversation.predict(input=message)
            return response
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

# ------------------------------
# VectorStoreManager: Manage local ChromaDB storage
# ------------------------------
class VectorStoreManager:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(name="knowledge_base")
        logger.info("ChromaDB collection initialized locally")

    def add_documents(self, documents: List[Dict[str, Any]], embeddings: List[List[float]]):
        if len(documents) != len(embeddings):
            raise ValueError("Documents and embeddings count mismatch")
        ids = [str(uuid.uuid4()) for _ in documents]
        metadatas = [{"name": doc["name"], "definition": doc["definition"]} for doc in documents]
        contents = [doc["name"] + " " + doc["definition"] for doc in documents]
        self.collection.add(ids=ids, documents=contents, embeddings=embeddings, metadatas=metadatas)
        logger.info(f"Added {len(documents)} documents to the vector store")

    def query(self, query_embedding: List[float], n_results: int = 4) -> List[Dict[str, Any]]:
        results = self.collection.query(query_embedding, n_results=n_results)
        matches = []
        if results and "metadatas" in results:
            for meta in results["metadatas"]:
                matches.append(meta)
        return matches

# ------------------------------
# CSV Processing
# ------------------------------
def load_csv_as_documents(csv_path: str) -> List[Dict[str, Any]]:
    df = read_csv_flexible(csv_path)
    if not {"name", "definition"}.issubset(set(df.columns)):
        raise ValueError("CSV must contain 'name' and 'definition' columns")
    documents = []
    for _, row in df.iterrows():
        documents.append({
            "name": str(row["name"]),
            "definition": str(row["definition"])
        })
    return documents

def save_results_to_csv(results: List[Dict[str, Any]], output_path: str) -> None:
    df = pd.DataFrame(results)
    df.to_csv(output_path, index=False)
    logger.info(f"Results saved to {output_path}")

# ------------------------------
# Main Function: Workflow
# ------------------------------
def main():
    """
    Main script that:
      1) Initializes environment (OSEnv) for proxy, SSL, and AD token
      2) Creates an AzureChatbot with the new param style for AzureChatOpenAI
         (openai_api_base, openai_api_type, openai_api_version, openai_api_key, deployment_name)
      3) Uses an EmbeddingClient for row-by-row embedding
      4) Ingests knowledge CSV, stores embeddings in ChromaDB
      5) Processes query CSV, top-4 matches, chatbot validation
      6) Saves final results to CSV
    """
    try:
        if '__file__' in globals():
            base_path = Path(__file__).resolve().parent
        else:
            base_path = Path.cwd()
        base_dir = base_path / "env"

        config_path = str(base_dir / "config.env")
        creds_path = str(base_dir / "credentials.env")
        cert_path = str(base_dir / "cacert.pem")
        for f in [config_path, creds_path, cert_path]:
            if not os.path.exists(f):
                print(f"Missing required file: {f}")
                return
        
        logger.info("Initializing environment (OSEnv)...")
        env = OSEnv(config_path, creds_path, cert_path)
        
        logger.info("Initializing chatbot agent with new param style (for match validation)...")
        chatbot = AzureChatbot(config_path, creds_path, cert_path)
        
        logger.info("Importing your EmbeddingClient from snippet (Pydantic-based).")
        from ooai_client_sdk.azopenaiembrdding import Document, EmbeddingClient
        embedding_client = EmbeddingClient()  # or pass constructor args if needed
        
        logger.info("Initializing local ChromaDB vector store...")
        vector_store = VectorStoreManager(persist_dir="./chroma_db")
        
        # Step 1: Knowledge Base Ingestion
        kb_csv = input("Enter path to knowledge base CSV: ").strip()
        kb_csv = remove_invisible_chars(kb_csv)
        kb_docs = load_csv_as_documents(kb_csv)

        kb_embeddings = []
        for doc_info in kb_docs:
            combined_text = doc_info["name"] + " " + doc_info["definition"]
            # Create a Document if your EmbeddingClient expects it
            doc_obj = Document(text=combined_text)
            doc_with_emb = embedding_client.generate_embeddings(doc_obj)
            emb = doc_with_emb.embedding
            if not emb:
                logger.error(f"Failed to get embedding for: {combined_text[:30]}...")
                kb_embeddings.append([])
            else:
                kb_embeddings.append(emb)
        
        vector_store.add_documents(kb_docs, kb_embeddings)
        
        # Step 2: Query CSV -> Vector matching
        query_csv = input("Enter path to query CSV: ").strip()
        query_csv = remove_invisible_chars(query_csv)
        query_df = read_csv_flexible(query_csv)
        if not {"name", "definition"}.issubset(set(query_df.columns)):
            raise ValueError("Query CSV must contain 'name' and 'definition' columns")
        
        results = []
        for index, row in query_df.iterrows():
            combined_text = str(row["name"]) + " " + str(row["definition"])
            doc_obj = Document(text=combined_text)
            doc_with_emb = embedding_client.generate_embeddings(doc_obj)
            emb = doc_with_emb.embedding
            if not emb:
                logger.error(f"Failed to generate embedding for query: {combined_text[:30]}...")
                matches = []
                validation_response = "Embedding error"
            else:
                matches = vector_store.query(emb, n_results=4)
                # Validate/improve the match using the chatbot
                validation_response = chatbot.validate_matches(combined_text, matches)
            
            results.append({
                "query_name": row["name"],
                "query_definition": row["definition"],
                "matches": json.dumps(matches),
                "validation": validation_response
            })
        
        # Step 3: Save results
        output_csv = input("Enter path for output CSV (e.g., output.csv): ").strip()
        output_csv = remove_invisible_chars(output_csv)
        save_results_to_csv(results, output_csv)
        print("Workflow completed successfully.")
        
    except Exception as e:
        logger.exception(f"Unexpected error: {str(e)}")
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()
