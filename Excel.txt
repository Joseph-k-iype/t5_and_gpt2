#!/usr/bin/env python3
"""
Problem Statement:
-----------------
We have two CSV files. 
1) The first CSV has:
   - name: the original label or identifier for a concept
   - definition: a descriptive explanation or context for that concept

2) The second CSV has:
   - pbt-name: the business-approved or "preferred" name for that concept
   - pbt-definition: the descriptive explanation or context for that PBT

We want to match each 'name' in the first CSV to the most semantically similar 'pbt-name'
from the second CSV, leveraging both 'definition' (from the first CSV) and
'pbt-definition' (from the second CSV) for contextual understanding.

The final goal is to find which "PBT-Name" is the best replacement or alignment 
for the original 'name', based on conceptual similarity.


Approach:
---------
1. Use Azure Active Directory (AAD) credentials (client ID, secret, tenant) to request a token for Azure OpenAI.
2. Configure OpenAI (via openai library) to use that bearer token and the Azure OpenAI endpoint.
3. Use LangChain's embeddings (text-embedding-ada-002 or text-embedding-3-large) to embed text.
4. Use LanceDB to store and query those embeddings.
5. For each 'name' in the first CSV, we find the top-k matching pbt-names, 
   filtering by a similarity threshold.

Prompt Structure:
-----------------
We integrate the following structured prompt for each data row:

"You are given the following information about a concept:

- Original Name: {name}
- Original Definition: {definition}

You also have a set of Preferred Business Terms (PBT), each with:
- PBT-Name: {pbt-name}
- PBT-Definition: {pbt-definition}

Determine which PBT-Name is the best semantic match for the Original Name based on both definitions."

We incorporate this prompt structure into the code's `_prepare_text()` method.
"""

import os
import time
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm

# Azure Identity for AAD auth
from azure.identity import ClientSecretCredential

# OpenAI for Azure usage
import openai

# LangChain & LanceDB
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
from langchain.docstore.document import Document
from lancedb import connect

###############################################################################
# Logging Setup
###############################################################################
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

###############################################################################
# OSEnv Class: Manages environment variables & .env files
###############################################################################
def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable.")
    return True

class OSEnv:
    """
    Production-ready environment manager that loads environment variables
    from .env files (config.env, credentials.env), sets custom certificate paths, etc.
    """
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        """Initialize with configuration, credentials, and optional certificate."""
        self.var_list = []
        
        # Load environment data
        self.bulk_set(config_file, print_val=True)
        logger.info(f"Loaded main configuration from {config_file}")
        
        self.bulk_set(creds_file, print_val=False)
        logger.info(f"Loaded credentials from {creds_file}")
        
        # Optional custom certificate
        self.set_certificate_path(certificate_path)

    def set_certificate_path(self, certificate_path: str) -> None:
        """Optionally set SSL certificate path if provided."""
        if certificate_path and os.path.exists(certificate_path):
            cert_path = str(Path(certificate_path))
            self.set("REQUESTS_CA_BUNDLE", cert_path, print_val=False)
            self.set("SSL_CERT_FILE", cert_path, print_val=False)
            self.set("CURL_CA_BUNDLE", cert_path, print_val=False)
            logger.info(f"SSL certificate path set to: {cert_path}")

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        """Load and set environment variables from a .env file."""
        if not os.path.exists(dotenvfile):
            logger.warning(f".env file not found: {dotenvfile}. Skipping.")
            return
        
        is_file_readable(str(dotenvfile))  # raises if not
        with open(dotenvfile, "r", encoding="utf-8") as f:
            lines = f.readlines()

        for line in lines:
            line = line.strip()
            if line and not line.startswith("#"):
                try:
                    key, val = line.split("=", 1)
                    key, val = key.strip(), val.strip().strip("'\"")
                    self.set(key, val, print_val)
                except ValueError:
                    continue

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        """Set an environment variable in the current process."""
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val and var_name not in ["AZURE_CLIENT_SECRET", "AD_USER_PW"]:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default_val: str = "") -> str:
        """Get an environment variable value."""
        return os.getenv(var_name, default_val)

###############################################################################
# SemanticMatcher Class
###############################################################################
class SemanticMatcher:
    """
    Performs semantic matching with:
     - Azure AAD-based token for OpenAI
     - LangChain embeddings
     - LanceDB vector search
     - Structured prompt for context
    """
    def __init__(self, env_setup: OSEnv):
        """
        1) Acquire AAD token
        2) Configure openai with that token
        3) Create embeddings & connect LanceDB
        """
        self.env = env_setup
        self._acquire_azure_token_via_aad()
        self._setup_openai()
        self._setup_embeddings()
        self._setup_lancedb()

    def _acquire_azure_token_via_aad(self):
        """Use azure.identity.ClientSecretCredential to get a token for Azure OpenAI."""
        tenant_id = self.env.get("AZURE_TENANT_ID")
        client_id = self.env.get("AZURE_CLIENT_ID")
        client_secret = self.env.get("AZURE_CLIENT_SECRET")

        if not all([tenant_id, client_id, client_secret]):
            raise ValueError("AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET must be set in environment.")

        credential = ClientSecretCredential(
            tenant_id=tenant_id,
            client_id=client_id,
            client_secret=client_secret
        )
        token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
        self.aad_token = token_obj.token
        logger.info("Acquired AAD token for Azure OpenAI.")

    def _setup_openai(self):
        """Configure openai to use Azure + the AAD token as api_key."""
        openai.api_type = "azure"
        openai.api_base = self.env.get("AZURE_OPENAI_ENDPOINT", "")
        openai.api_version = self.env.get("API_VERSION", "2023-03-15-preview")

        if not self.aad_token:
            raise ValueError("No AAD token found; cannot configure OpenAI.")
        openai.api_key = self.aad_token

    def _setup_embeddings(self):
        """
        Create a LangChain Embeddings object referencing your Azure embedding deployment.
        """
        deployment_name = self.env.get("AZURE_EMBEDDINGS_DEPLOYMENT", "my-embedding-deployment")
        model_name = self.env.get("AZURE_EMBEDDINGS_MODEL", "text-embedding-3-large")

        self.embeddings_model = OpenAIEmbeddings(
            deployment=deployment_name,
            model=model_name,
            openai_api_key=openai.api_key,
            openai_api_base=openai.api_base,
            openai_api_type=openai.api_type,
            openai_api_version=openai.api_version
        )
        logger.info(f"LangChain Embeddings set up for deployment: {deployment_name}, model: {model_name}")

    def _setup_lancedb(self):
        """Connect or create a local LanceDB store for vector embeddings."""
        self.db = connect("./lance_vectors_aad")
        self.table_name = "pbt_vectors_aad"
        logger.info("LanceDB connection established at ./lance_vectors_aad")

    def _prepare_text(self, name: str, definition: str) -> str:
        """
        Structured prompt for each data row, integrating the problem statement:
        We incorporate original name, original definition,
        and ask for the best matching PBT based on definitions.
        """
        # Notice the structured approach that references both 'name' & 'definition'
        prompt = (
            f"You are given the following information about a concept:\n\n"
            f"- Original Name: {name}\n"
            f"- Original Definition: {definition}\n\n"
            f"You also have a set of Preferred Business Terms (PBT), each with:\n"
            f"- PBT-Name\n"
            f"- PBT-Definition\n\n"
            f"Determine which PBT-Name is the best semantic match for the Original Name.\n"
        )
        return prompt

    ############################################################################
    # CSV Loading and Index Building
    ############################################################################
    def load_csv_data(self, source_csv: str, target_csv: str):
        """
        Load CSVs:
         - source.csv => [name, definition]
         - target.csv => [pbt-name, pbt-definition]
        """
        src_df = pd.read_csv(source_csv).fillna("")
        tgt_df = pd.read_csv(target_csv).fillna("")

        if not {"name", "definition"}.issubset(src_df.columns):
            raise ValueError("Source CSV must have 'name' and 'definition' columns.")
        if not {"pbt-name", "pbt-definition"}.issubset(tgt_df.columns):
            raise ValueError("Target CSV must have 'pbt-name' and 'pbt-definition' columns.")

        for df in [src_df, tgt_df]:
            for col in df.columns:
                df[col] = df[col].astype(str).str.strip()

        logger.info(f"Loaded {len(src_df)} source records and {len(tgt_df)} target records.")
        return src_df, tgt_df

    def index_target_data(self, target_df: pd.DataFrame):
        """
        Convert target rows into Documents with a specialized prompt, 
        storing them in LanceDB.
        """
        if self.table_name in self.db.table_names():
            logger.info(f"Table '{self.table_name}' exists; dropping to rebuild.")
            self.db.drop_table(self.table_name)

        docs = []
        for _, row in target_df.iterrows():
            # We incorporate the PBT name + definition into a more specialized text
            # so embeddings can reflect "pbt-name" + "pbt-definition" together:
            pbt_prompt = (
                f"PBT-Name: {row['pbt-name']}\n"
                f"PBT-Definition: {row['pbt-definition']}\n"
            )
            docs.append(
                Document(
                    page_content=pbt_prompt,
                    metadata={
                        "pbt-name": row["pbt-name"],
                        "pbt-definition": row["pbt-definition"]
                    }
                )
            )

        logger.info("Indexing target (PBT) documents in LanceDB...")
        LanceDB.from_documents(
            documents=docs,
            embedding=self.embeddings_model,
            connection=self.db,
            table_name=self.table_name
        )
        logger.info("Target data successfully indexed into LanceDB.")

    ############################################################################
    # Matching Source Rows
    ############################################################################
    def process_matches(self, source_df: pd.DataFrame, similarity_threshold=0.75, k=3):
        """
        For each source entry, generate a structured prompt, embed, 
        and do similarity search in LanceDB. 
        Return matches above the given similarity_threshold.
        """
        vector_store = LanceDB(
            connection=self.db,
            table_name=self.table_name,
            embedding=self.embeddings_model
        )

        all_matches = []
        for _, src_row in tqdm(source_df.iterrows(), total=len(source_df), desc="Matching Source Rows"):
            query_text = self._prepare_text(src_row["name"], src_row["definition"])
            results = vector_store.similarity_search_with_score(query_text, k=k)

            for doc, score in results:
                # Typically 'score' is in [0..1] if it's cosine similarity
                if score >= similarity_threshold:
                    all_matches.append({
                        "source_name": src_row["name"],
                        "source_definition": src_row["definition"],
                        "matched_pbt_name": doc.metadata["pbt-name"],
                        "matched_pbt_definition": doc.metadata["pbt-definition"],
                        "similarity_score": float(score),
                    })

        logger.info(f"Found {len(all_matches)} total matches (score >= {similarity_threshold}).")
        return all_matches

    def save_results(self, matches, output_file: str):
        """
        Save matches to CSV and JSON.
        """
        if not matches:
            logger.warning("No matches to save.")
            return

        df = pd.DataFrame(matches)
        df.to_csv(output_file, index=False)

        json_file = output_file.replace(".csv", ".json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(matches, f, indent=2, ensure_ascii=False)

        logger.info(f"Results saved to:\n  - {output_file}\n  - {json_file}")

###############################################################################
# Main Function
###############################################################################
def main():
    """
    Main Steps:
     1) Folders: One level up from src (env, data, output, logs).
     2) Load .env configs (Azure AD, Azure OpenAI).
     3) Create SemanticMatcher, index PBT data, match source data.
     4) Save results.
    """
    try:
        # We are in src/, so go one level up to find env, data, output, logs
        base_dir = Path(__file__).parent.parent
        
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"

        for directory in [data_dir, output_dir, log_dir]:
            directory.mkdir(exist_ok=True)

        # .env file paths
        config_file = env_dir / "config.env"
        creds_file = env_dir / "credentials.env"
        cert_file = env_dir / "cacert.pem"
        source_csv = data_dir / "source.csv"
        target_csv = data_dir / "target.csv"

        # Validate required files
        missing = []
        for f in [config_file, creds_file, source_csv, target_csv]:
            if not f.exists():
                missing.append(str(f))
        if missing:
            raise FileNotFoundError(f"Missing required files: {missing}")

        # Initialize environment
        logger.info("Initializing OSEnv and environment variables...")
        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))

        # Create semantic matcher
        matcher = SemanticMatcher(env_setup)

        # Load CSV data
        source_df, target_df = matcher.load_csv_data(str(source_csv), str(target_csv))

        # Index target data
        matcher.index_target_data(target_df)

        # Process matches
        matches = matcher.process_matches(
            source_df,
            similarity_threshold=0.75,  # threshold
            k=3                         # top-k
        )

        # Save results
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"matches_{timestamp}.csv"
        matcher.save_results(matches, str(output_file))

        print(f"\nProcess completed successfully! Results saved in {output_file}")

    except FileNotFoundError as e:
        logger.error(f"File Error: {e}")
    except ValueError as e:
        logger.error(f"Value Error: {e}")
    except Exception as e:
        logger.exception(f"Unexpected Error: {e}")
        raise

if __name__ == "__main__":
    main()
