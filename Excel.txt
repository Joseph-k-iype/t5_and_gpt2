#!/usr/bin/env python3
"""
CHUNKED PROMPT APPROACH:
------------------------
For each row in the 'source.csv' (columns: name, definition), we:
1) Break the 'target.csv' PBTs (pbt-name, pbt-definition) into CHUNK_SIZE pieces.
2) Ask AzureChatOpenAI which PBT is best in each chunk => get chunk winners.
3) If multiple chunk winners => do a 'tournament' until only 1 final PBT remains.
4) Store the model's final textual explanation or direct answer in 'model_answer'.

WHY:
----
This avoids hitting the model's maximum token limit if you have many PBT rows.

Usage:
------
1. Populate env/config.env & env/credentials.env with your settings 
   (MODEL_NAME, AZURE_OPENAI_ENDPOINT, etc.).
2. Place source.csv + target.csv in data/.
3. Run: python semantic_chat_match.py
4. Results -> output/chat_matches_<timestamp>.csv & .json
"""

import os
import csv
import time
import json
import logging
from pathlib import Path
from typing import List, Dict

import pandas as pd

# The Azure Identity import is used by OSEnv when SECURED_ENDPOINTS=True
from azure.identity import ClientSecretCredential

# We'll use your updated chunk approach via AzureChatOpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from dotenv import dotenv_values

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

#############################
# OSEnv Class (Unchanged)
#############################
def is_file_readable(filepath: str) -> bool:
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    ls = s.strip().lower()
    if ls == 'true':
        return True
    elif ls == 'false':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

class OSEnv:
    """Environment variable and certificate management class."""

    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        
        # Load main configuration
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        
        # Load credentials
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        
        # Set up certificates
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")
        
        # Proxy
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")
        
        # Possibly get Azure token
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints (AAD token)")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        try:
            cp_abs = os.path.abspath(certificate_path)
            if not is_file_readable(cp_abs):
                raise FileNotFoundError(f"Certificate file not found or not readable: {cp_abs}")
            self.set("REQUESTS_CA_BUNDLE", cp_abs)
            self.set("SSL_CERT_FILE", cp_abs)
            self.set("CURL_CA_BUNDLE", cp_abs)
            logger.info(f"Certificate path set to: {cp_abs}")
        except Exception as e:
            logger.error(f"Certificate configuration failed: {str(e)}")
            raise

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        try:
            df_abs = os.path.abspath(dotenvfile)
            if not is_file_readable(df_abs):
                return
            vals = dotenv_values(df_abs)
            for k, v in vals.items():
                self.set(k, v, print_val)
        except Exception as e:
            logger.error(f"Failed to load environment file {dotenvfile}: {str(e)}")
            raise

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default_val: str = "") -> str:
        return os.getenv(var_name, default_val)

    def set_proxy(self) -> None:
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            raise ValueError("Missing proxy credentials for PROXY_ENABLED=True")

        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)
        
        no_proxy_list = [
            'cognitiveservices.azure.com',
            'search.windows.net',
            'openai.azure.com',
            'core.windows.net',
            'azurewebsites.net'
        ]
        # Join them into NO_PROXY
        self.set("NO_PROXY", ",".join(no_proxy_list), print_val=False)

    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token_obj = credential.get_token("https://cognitiveservices.azure.com/.default")
            token_val = token_obj.token
            logger.info("Acquired Azure token successfully")
            return token_val
        except Exception as e:
            logger.error(f"Failed to get Azure token: {str(e)}")
            raise

#############################
# AzureChatbot with Chunking
#############################
class AzureChatbot:
    """
    A wrapper around AzureChatOpenAI that does:
      - multi-chunk approach for PBT lists
      - "tournament" style to avoid token overflow
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self._setup_chat_model()
        # Default chunk size = 50 (or define CHUNK_SIZE in config.env)
        self.chunk_size = int(self.env.get("CHUNK_SIZE","50"))

    def _setup_chat_model(self) -> None:
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))
        api_version = self.env.get("API_VERSION", "2024-02-01")
        azure_endpoint = self.env.get("AZURE_OPENAI_ENDPOINT")
        azure_token = self.env.token  # AAD token if SECURED_ENDPOINTS=True

        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            openai_api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token=azure_token
        )
        logger.info(f"Chat model '{model_name}' initialized (temp={temperature}, max_tokens={max_tokens}).")

    #############################
    # 1) Chunk-level picking
    #############################
    def _pick_best_in_chunk(self, name:str, definition:str, pbt_chunk:List[Dict]) -> str:
        """
        Prompt the model for a single chunk of PBTs:
          "Which pbt is best among these N pbt-names & definitions?"
        Return the raw model answer text (the user can parse it).
        """
        system_text = (
            "You are a helpful assistant. You must pick exactly one best matching PBT "
            "from the chunk. Provide a short justification."
        )
        user_text = (
            f"Original name: {name}\n"
            f"Original definition: {definition}\n\n"
            f"Potential PBTs in this chunk:\n"
        )
        for idx, pbt in enumerate(pbt_chunk, start=1):
            user_text += (f"({idx}) pbt-name: {pbt['pbt-name']}\n"
                          f"    pbt-definition: {pbt['pbt-definition']}\n\n")
        user_text += (
            "Out of these, which single pbt-name is the best match, and why?"
        )

        messages = [
            SystemMessage(content=system_text),
            HumanMessage(content=user_text)
        ]
        try:
            response = self.llm(messages)
            return response.content.strip()
        except Exception as e:
            logger.error(f"Error in _pick_best_in_chunk: {str(e)}")
            return f"ERROR picking chunk: {str(e)}"

    def _chunk_and_pick(self, name:str, definition:str, pbt_list:List[Dict]) -> List[str]:
        """
        Break pbt_list into chunk_size pieces, 
        pick 1 "winner" from each chunk => store the model's textual answer in chunk_answers.
        Return the list of chunk answers (strings).
        """
        chunk_answers = []
        # slice pbt_list into chunks
        for i in range(0, len(pbt_list), self.chunk_size):
            chunk = pbt_list[i : i+self.chunk_size]
            ans = self._pick_best_in_chunk(name, definition, chunk)
            chunk_answers.append(ans)
        return chunk_answers

    #############################
    # 2) "Tournament" approach
    #############################
    def match_single_record(self, 
                            name: str, 
                            definition: str, 
                            pbt_list: List[Dict[str, str]]) -> str:
        """
        For 1 row in source CSV, do multi-round chunk picking until we have 1 final best:
          1) chunk pbt_list => pick 1 from each => chunk_winners
          2) if multiple chunk_winners => treat them as new "pbt list", repeat
          3) final answer = last single chunk's answer
        Return the final textual reasoning from the model.
        
        (Note: This approach doesn't strictly unify all chunk picks 
               into 1 final text, but it ensures we never exceed tokens.)
        """
        # We'll store each "round" in a list so user can see the final round's text
        round_texts = []

        # Convert entire pbt_list into "fake PBT" items if we do multiple rounds.
        # But we won't parse each chunk's answer to get a single actual "pbt-definition". 
        # We'll treat the chunk's entire text as 1 "pbt" for next round.

        # Start with "virtual" PBT items = the actual pbt_list
        current_pbt_list = pbt_list

        round_index = 1
        while True:
            logger.info(f"Round {round_index}, PBT count={len(current_pbt_list)}")

            if len(current_pbt_list) <= self.chunk_size:
                # Just do one chunk pick => final
                final_answer = self._pick_best_in_chunk(name, definition, current_pbt_list)
                round_texts.append(f"(Final) => {final_answer}")
                break
            else:
                # multiple chunks
                chunk_answers = self._chunk_and_pick(name, definition, current_pbt_list)
                # Each chunk answer is a freeform text from the model
                # We'll treat each answer as a single "virtual" PBT item for next round
                new_pbt_list = []
                for idx, ans_text in enumerate(chunk_answers, start=1):
                    # We'll create a single pbt item out of the chunk answer
                    # so in next round, the user can pick from these "answers".
                    new_pbt_list.append({
                        "pbt-name": f"Round{round_index}_chunk{idx}_pick",
                        "pbt-definition": ans_text
                    })
                round_texts.append(f"(Round {round_index}) => {chunk_answers}")

                # if only 1 chunk answer => final
                if len(new_pbt_list) == 1:
                    final_answer = new_pbt_list[0]["pbt-definition"]
                    round_texts.append(f"(Final) => {final_answer}")
                    break
                else:
                    current_pbt_list = new_pbt_list
                    round_index += 1

        # Return the final textual reasoning
        return "\n".join(round_texts)

###########################
#   MAIN LOGIC
###########################
def main():
    """
    - Initialize environment
    - Load CSVs
    - For each row in source.csv, do chunk-based "tournament" matching
    - Save results
    """
    try:
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"

        for d in [env_dir, data_dir, output_dir, log_dir]:
            d.mkdir(exist_ok=True)

        # .env files
        config_file = env_dir / "config.env"
        creds_file = env_dir / "credentials.env"
        cert_file = env_dir / "cacert.pem"

        # CSV files
        source_csv = data_dir / "source.csv"
        target_csv = data_dir / "target.csv"

        # Check existence
        missing = []
        for f in [config_file, creds_file, cert_file, source_csv, target_csv]:
            if not f.exists():
                missing.append(str(f))
        if missing:
            print("Missing required files:")
            for m in missing:
                print(f"- {m}")
            return

        # 1) ENV
        logger.info("Initializing OSEnv and reading environment configs...")
        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))

        # 2) Chat model with chunking
        logger.info("Initializing Azure Chat model (chunk approach)...")
        chatbot = AzureChatbot(env_setup)

        # 3) Load data
        logger.info("Loading CSV data...")
        src_df = pd.read_csv(str(source_csv), dtype=str).fillna("")
        tgt_df = pd.read_csv(str(target_csv), dtype=str).fillna("")

        if not {"name", "definition"}.issubset(src_df.columns):
            raise ValueError("source.csv must have columns: name, definition")
        if not {"pbt-name", "pbt-definition"}.issubset(tgt_df.columns):
            raise ValueError("target.csv must have columns: pbt-name, pbt-definition")

        # Convert target DF to list of dicts
        pbt_list = tgt_df.to_dict('records')

        logger.info(f"Source CSV has {len(src_df)} rows, target CSV has {len(tgt_df)} PBT rows.")
        
        # 4) For each row in source, run chunk-based approach
        results = []
        for idx, row in src_df.iterrows():
            name_val = row["name"].strip()
            def_val = row["definition"].strip()

            logger.info(f"Processing row {idx+1}/{len(src_df)}: '{name_val}'")
            # Run tournament
            model_answer = chatbot.match_single_record(name_val, def_val, pbt_list)

            # Append to results
            results.append({
                "source_name": name_val,
                "source_definition": def_val,
                "model_answer": model_answer
            })
        
        # 5) Save results
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        out_file = output_dir / f"chat_matches_{timestamp}.csv"
        pd.DataFrame(results).to_csv(str(out_file), index=False)

        json_file = out_file.with_suffix(".json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nDone! Results saved to:\n  - {out_file}\n  - {json_file}")

    except Exception as e:
        logger.exception(f"Unexpected error: {str(e)}")
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    main()
