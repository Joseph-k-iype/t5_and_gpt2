import csv
import json

input_csv = 'input.csv'    # change this if your CSV has a different name or path
output_json = 'output.json'

data = []

with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        # Build the classification_regex array: only add patterns if they are non-empty
        regex_patterns = []
        for key in ["Main pattern", "alternative pattern 1", "alternative pattern 2", "alternative pattern 3"]:
            value = row.get(key, "").strip()
            if value:
                regex_patterns.append(value)
                
        # Construct the JSON object for the current row
        entry = {
            "classification_name": row.get("object_property", "").strip(),
            "classification_regex": regex_patterns,
            "classifierType": "data",
            "description": row.get("property definition", "").strip(),
            "enabled": True,
            "max_length": -1,
            "min_length": 0,
            "proximity_before": "",
            "proximity_after": "",
            "support_term_value": "",
            "nst_proximity_before": "",
            "nst_proximity_after": "",
            "negative_support_term_value": ""
        }
        data.append(entry)

# Write the JSON output to a file
with open(output_json, 'w', encoding='utf-8') as jsonfile:
    json.dump(data, jsonfile, indent=4)

print(f"Conversion complete. Output written to {output_json}")





import csv
import json
import math

# Name of the CSV file
input_csv = "data.csv"

# Dictionary to group data by 'name'
grouped_data = {}

with open(input_csv, mode="r", newline="", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        name = row["name"].strip()
        definition = row["definition"].strip()
        related_name = row["related term name"].strip()
        related_definition = row["related term definition"].strip()
        related_example = row["related term example"].strip()
        
        # If the name is not already in our grouped_data, add it with its definition and an empty related_terms list
        if name not in grouped_data:
            grouped_data[name] = {
                "name": name,
                "definition": definition,
                "related_terms": []
            }
        
        # Append the related term if provided (assuming empty strings mean no related term)
        if related_name:
            grouped_data[name]["related_terms"].append({
                "name": related_name,
                "definition": related_definition,
                "example": related_example
            })

# Convert the grouped data into a list of entries
data_list = list(grouped_data.values())
total_entries = len(data_list)

# Determine the chunk size for splitting into 5 JSON files
chunk_size = math.ceil(total_entries / 5)

for i in range(5):
    start_index = i * chunk_size
    end_index = start_index + chunk_size
    chunk = data_list[start_index:end_index]
    output_filename = f"data{i+1}.json"
    
    # Write each chunk to a separate JSON file with indentation for readability
    with open(output_filename, "w", encoding="utf-8") as outfile:
        json.dump(chunk, outfile, indent=4, ensure_ascii=False)

print("Conversion complete. 5 JSON files have been created.")




import pandas as pd
import networkx as nx

def load_data(file_path):
    """Load the CSV data and extract main terms."""
    df = pd.read_csv(file_path)
    main_terms = set(df['name'].unique())  # All unique main terms
    return df, main_terms

def build_filtered_graph(df, main_terms):
    """Build a graph with edges only between main terms."""
    # Filter rows where 'related term name' is a main term
    valid_edges = df[df['related term name'].isin(main_terms)]
    # Count edge weights (frequency of relationships)
    edge_weights = valid_edges.groupby(['name', 'related term name']).size().reset_index(name='weight')
    
    # Create a directed graph with weighted edges
    G = nx.DiGraph()
    for _, row in edge_weights.iterrows():
        source = row['name']
        target = row['related term name']
        weight = row['weight']
        G.add_edge(source, target, weight=weight)
    return G

def calculate_pagerank(G):
    """Compute PageRank scores for nodes in the graph."""
    return nx.pagerank(G, weight='weight')

def get_top_terms(pagerank_scores, top_n=30):
    """Return the top N terms by PageRank score."""
    sorted_terms = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)
    return [term for term, _ in sorted_terms[:top_n]]

def main(file_path):
    df, main_terms = load_data(file_path)
    G = build_filtered_graph(df, main_terms)
    pagerank_scores = calculate_pagerank(G)
    top_terms = get_top_terms(pagerank_scores, 30)
    return top_terms

# Example usage
if __name__ == "__main__":
    top_30_terms = main('terms.csv')  # Replace with your CSV path
    print("Top 30 Main Terms:")
    for term in top_30_terms:
        print(term)



class UniversalRegexGenerator(RegexGenerator):
    def process_batch(self, file_path: str) -> dict:
        """Process batch with incremental saving"""
        output_file = Path(file_path).with_name(f"{Path(file_path).stem}_results.json")
        
        # Initialize output file with empty array
        with open(output_file, 'w') as f:
            f.write("[]")
        
        processed_count = 0
        success_count = 0
        error_count = 0
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSON root should be an array of request objects")
            
            for idx, item in enumerate(data):
                processed_count += 1
                try:
                    result = self.process_request(item)
                    success_count += 1
                    self._save_incremental(output_file, result)
                except Exception as e:
                    error_count += 1
                    logger.error(f"Error processing item {idx}: {str(e)}")
                
                # Progress update
                print(f"\rProcessed: {processed_count} | Success: {success_count} | Errors: {error_count}", end="")
            
            print()  # New line after progress
            return {
                "processed_count": processed_count,
                "successful": success_count,
                "errors": error_count
            }
            
        except Exception as e:
            print(f"\nFatal error: {str(e)}")
            return {
                "processed_count": processed_count,
                "successful": success_count,
                "errors": error_count
            }

    def _save_incremental(self, output_file: Path, result: dict):
        """Append result to JSON array in output file"""
        try:
            # Read existing data
            with open(output_file, 'r+') as f:
                # Move to last character before closing bracket
                f.seek(-1, 2)
                
                # Append new result with proper formatting
                if f.tell() > 1:  # Not empty array
                    f.write(",\n    ")
                else:
                    f.write("\n    ")
                
                json.dump(result, f, indent=4)
                f.write("\n]")  # Close array
                
        except json.JSONDecodeError:
            # Handle case where file is corrupted
            logger.error("Output file corruption detected, resetting...")
            with open(output_file, 'w') as f:
                json.dump([result], f, indent=4)

    def _display_results(self, results: dict):
        """Display final summary"""
        print("\nFinal Processing Summary:")
        print(f"Total items processed: {results['processed_count']}")
        print(f"Successfully generated: {results['successful']}")
        print(f"Failed generations: {results['errors']}")
        print(f"\nResults saved to: {Path(self._current_output_file).resolve()}")











import sys
import json
from pathlib import Path

class UniversalRegexGenerator(RegexGenerator):
    def process_input(self):
        """Universal entry point with proper path validation"""
        # Get file path from command line or input prompt
        if len(sys.argv) > 1:
            file_path = sys.argv[1]
        else:
            file_path = input("Enter path to JSON file: ").strip()

        # Validate path exists
        if not Path(file_path).exists():
            print(f"Error: File not found - {file_path}")
            print("Please provide a valid file path.")
            sys.exit(1)

        # Process and display results
        try:
            results = self.process_batch(file_path)
            self._save_results(results, file_path)
            self._display_results(results)
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            sys.exit(1)

if __name__ == "__main__":
    generator = UniversalRegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    generator.process_input()









class UniversalRegexGenerator(RegexGenerator):
    def process_batch(self, file_path: str) -> dict:
        """Process batch from file path"""
        results = []
        errors = []
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSON root should be an array of request objects")
            
            for idx, item in enumerate(data):
                try:
                    # Validate item structure
                    if not isinstance(item, dict):
                        raise ValidationError("Each request must be a dictionary")
                    if 'name' not in item or 'definition' not in item:
                        raise ValidationError("Missing required fields (name/definition)")
                    
                    # Process valid request
                    result = self.process_request(item)
                    results.append(result)
                    
                except Exception as e:
                    errors.append({
                        "index": idx,
                        "error": str(e),
                        "input": item
                    })
            
            return {
                "processed_count": len(data),
                "successful": results,
                "errors": errors
            }
            
        except json.JSONDecodeError as e:
            return {
                "processed_count": 0,
                "successful": [],
                "errors": [{"error": f"Invalid JSON: {str(e)}", "file": file_path}]
            }
        except Exception as e:
            return {
                "processed_count": 0,
                "successful": [],
                "errors": [{"error": str(e), "file": file_path}]
            }

    def _save_results(self, results: dict, input_path: str):
        """Save results to file with proper naming"""
        try:
            input_path = Path(input_path)
            output_file = input_path.with_name(f"{input_path.stem}_results.json")
            
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"Results saved to: {output_file}")
            
        except Exception as e:
            print(f"Error saving results: {str(e)}")
            raise

    def _display_results(self, results: dict):
        """Improved results display"""
        print(f"\nProcessing Summary:")
        print(f"Total items: {results['processed_count']}")
        print(f"Successfully processed: {len(results['successful']}")
        print(f"Errors: {len(results['errors'])}")
        
        if results['errors']:
            print("\nFirst error:")
            error = results['errors'][0]
            msg = f"Item {error.get('index', '?')}: {error['error']}"
            if 'file' in error:
                msg += f"\nFile: {error['file']}"
            print(msg)

if __name__ == "__main__":
    generator = UniversalRegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        if not Path(file_path).exists():
            print(f"Error: File not found - {file_path}")
            sys.exit(1)
    else:
        file_path = input("Enter path to valid JSON file: ")
        if not Path(file_path).exists():
            print(f"Error: File not found - {file_path}")
            sys.exit(1)
    
    try:
        results = generator.process_batch(file_path)
        generator._save_results(results, file_path)
        generator._display_results(results)
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        sys.exit(1)









def _save_results(self, results: dict, input_path: str):
        """Save processing results to a JSON file"""
        try:
            output_file = Path(input_path).stem + "_results.json"
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"Results saved to: {output_file}")
        except Exception as e:
            print(f"Error saving results: {str(e)}")
            raise

    def process_input(self):
        """Universal entry point for all environments"""
        if len(sys.argv) > 1:
            file_path = sys.argv[1]
        else:
            file_path = input("Enter path to JSON file: ")
        
        try:
            results = self.process_batch(file_path)
            self._save_results(results, file_path)
            self._display_results(results)
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            sys.exit(1)









class UniversalRegexGenerator(RegexGenerator):
    def process_input(self):
        """Universal entry point for all environments"""
        if len(sys.argv) > 1:
            file_path = sys.argv[1]
        else:
            file_path = input("Enter path to JSON file: ")
        
        try:
            results = self.process_batch(file_path)
            self._save_results(results, file_path)
            self._display_results(results)
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            sys.exit(1)

    def _display_results(self, results: dict):
        """Display results in simple format"""
        print(f"\nProcessing Summary:")
        print(f"Total items: {results['processed_count']}")
        print(f"Successful: {len(results['successful'])}")
        print(f"Errors: {len(results['errors'])}")
        
        if results['successful']:
            print("\nFirst successful result:")
            first_success = results['successful'][0]
            print(f"Name: {first_success['name']}")
            print(f"Regex: {first_success['regex']}")
            print(f"Confidence: {first_success['confidence_score']:.2f}")
        
        if results['errors']:
            print("\nFirst error:")
            first_error = results['errors'][0]
            print(f"Item {first_error['index']}: {first_error['error']}")

if __name__ == "__main__":
    generator = UniversalRegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    generator.process_input()








class UniversalRegexGenerator(RegexGenerator):
    def process_input(self):
        """Universal entry point for both environments"""
        if self.is_notebook:
            file_path = input("Enter path to JSON file: ")
        else:
            file_path = sys.argv[1] if len(sys.argv) > 1 else input("Enter path to JSON file: ")
        
        try:
            results = self.process_batch(file_path)
            self._save_results(results, file_path)
            self._display_results(results)
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            sys.exit(1)

    def process_batch(self, file_path: str) -> dict:
        """Process batch from file path"""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSON root should be an array of request objects")
            
            results = []
            errors = []
            
            for idx, item in enumerate(data):
                try:
                    result = self.process_request(item)
                    results.append(result)
                except ValidationError as e:
                    errors.append({
                        "index": idx,
                        "error": f"Validation error: {str(e)}",
                        "input": item
                    })
                except Exception as e:
                    errors.append({
                        "index": idx,
                        "error": f"Processing error: {str(e)}",
                        "input": item
                    })
            
            return {
                "processed_count": len(results),
                "successful": results,
                "errors": errors
            }
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON format: {str(e)}")
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")

    def _display_results(self, results: dict):
        """Display results in simple format"""
        print(f"\nProcessing Summary:")
        print(f"Total items: {results['processed_count']}")
        print(f"Successful: {len(results['successful'])}")
        print(f"Errors: {len(results['errors'])}")
        
        if results['successful']:
            print("\nFirst successful result:")
            first_success = results['successful'][0]
            print(f"Name: {first_success['name']}")
            print(f"Regex: {first_success['regex']}")
            print(f"Confidence: {first_success['confidence_score']:.2f}")
        
        if results['errors']:
            print("\nFirst error:")
            first_error = results['errors'][0]
            print(f"Item {first_error['index']}: {first_error['error']}")

if __name__ == "__main__":
    generator = UniversalRegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    generator.process_input()








def _process_uploaded_file(self, uploaded_file):
    """Handle uploaded file content in notebook"""
    try:
        content = None
        
        # Debugging: Show full upload structure
        logger.debug(f"Full upload structure: {uploaded_file}")
        
        # Handle different Jupyter file upload formats
        if isinstance(uploaded_file, dict):
            # New format: {filename: {'content': bytes}}
            content = next(iter(uploaded_file.values()))['content']
        elif isinstance(uploaded_file, (list, tuple)):
            # Handle tuple of (name, Bunch) format
            if len(uploaded_file) > 0:
                first_item = uploaded_file[0]
                if isinstance(first_item, tuple) and len(first_item) >= 2:
                    # (name, Bunch) format
                    content = first_item[1].content
                else:
                    raise ValueError("Unexpected tuple structure in file upload")
        elif hasattr(uploaded_file, 'content'):
            # Direct content access for Bunch-like objects
            content = uploaded_file.content
        else:
            raise ValueError(f"Unsupported upload format: {type(uploaded_file)}")

        if not content:
            raise ValueError("Uploaded file appears empty")

        # Process the content
        results = self.process_batch_content(content)
        self._display_results(results)

    except Exception as e:
        with self.out:
            print(f"Error processing file: {str(e)}")
            print("\nUpload structure debug info:")
            print(f"Type: {type(uploaded_file)}")
            if isinstance(uploaded_file, (list, tuple)):
                print(f"First item type: {type(uploaded_file[0])}")
                if isinstance(uploaded_file[0], tuple):
                    print("First tuple contents types:")
                    print(f"1. {type(uploaded_file[0][0])}")
                    print(f"2. {type(uploaded_file[0][1])}")









def _process_uploaded_file(self, uploaded_file):
    """Handle uploaded file content in notebook"""
    try:
        content = None
        # Handle different Jupyter file upload formats
        if isinstance(uploaded_file, dict):
            # New format: {filename: {'content': bytes}}
            content = next(iter(uploaded_file.values()))['content']
        elif isinstance(uploaded_file, (list, tuple)):
            first_item = uploaded_file[0]
            # If first item is a Bunch (or has attribute 'content'), get its content attribute
            if hasattr(first_item, 'content'):
                content = first_item.content
            # Else, if it's a dict, access via key lookup
            elif isinstance(first_item, dict) and 'content' in first_item:
                content = first_item['content']
            else:
                raise ValueError(f"Unsupported tuple format: {type(first_item)}")
        elif hasattr(uploaded_file, 'content'):
            # Direct content access
            content = uploaded_file.content
        else:
            raise ValueError(f"Unsupported upload format: {type(uploaded_file)}")

        if not content:
            raise ValueError("Uploaded file appears empty")

        results = self.process_batch_content(content)
        self._display_results(results)

    except Exception as e:
        print(f"Error processing file: {str(e)}")
        print("\nUpload structure debug info:")
        print(f"Type: {type(uploaded_file)}")
        if isinstance(uploaded_file, (list, tuple)):
            print(f"First item type: {type(uploaded_file[0])}")















class UniversalRegexGenerator(RegexGenerator):
    # ... [keep previous methods unchanged] ...

    def process_batch_content(self, content: bytes) -> dict:
        """Common processing logic for both interfaces"""
        results = []
        errors = []
        
        try:
            # Decode and parse JSON
            decoded_content = content.decode('utf-8')
            data = json.loads(decoded_content)
            
            if not isinstance(data, list):
                raise ValueError("JSON root should be an array of request objects")
            
            for idx, item in enumerate(data):
                try:
                    # Validate request structure
                    if not isinstance(item, dict):
                        raise ValidationError("Request item must be a dictionary")
                        
                    # Fix field name mismatch
                    if 'related_terms' in item:
                        for rt in item['related_terms']:
                            if 'example' not in rt:
                                raise ValidationError("Related terms must contain 'example' field")
                    
                    result = self.process_request(item)
                    results.append(result)
                except ValidationError as e:
                    errors.append({
                        "index": idx,
                        "error": f"Validation error: {str(e)}",
                        "input": item
                    })
                except Exception as e:
                    errors.append({
                        "index": idx,
                        "error": f"Processing error: {str(e)}",
                        "input": item
                    })
                    
        except Exception as e:
            errors.append({
                "error": f"File parsing failed: {str(e)}",
                "content_sample": decoded_content[:100] if 'decoded_content' in locals() else str(content[:100])
            })
        
        return {
            "processed_count": len(results),
            "successful": results,
            "errors": errors
        }

    def _process_uploaded_file(self, uploaded_file):
        """Handle uploaded file content in notebook"""
        try:
            content = None
            
            # Handle different Jupyter file upload formats
            if isinstance(uploaded_file, dict):
                # New format: {filename: {'content': bytes}}
                content = next(iter(uploaded_file.values()))['content']
            elif isinstance(uploaded_file, (list, tuple)):
                # Old format: [(name, {'content': bytes})]
                content = uploaded_file[0][1]['content']
            elif hasattr(uploaded_file, 'content'):
                # Direct content access
                content = uploaded_file.content
            else:
                raise ValueError(f"Unsupported upload format: {type(uploaded_file)}")

            if not content:
                raise ValueError("Uploaded file appears empty")

            results = self.process_batch_content(content)
            self._display_results(results)

        except Exception as e:
            with self.out:
                print(f"Error processing file: {str(e)}")
                print("\nUpload structure debug info:")
                print(f"Type: {type(uploaded_file)}")
                if isinstance(uploaded_file, (list, tuple)):
                    print(f"First item type: {type(uploaded_file[0])}")












class UniversalRegexGenerator(RegexGenerator):
    def _process_uploaded_file(self, uploaded_file):
        """Handle uploaded file content in notebook"""
        try:
            # Debugging: Show upload structure
            logger.debug(f"Upload type: {type(uploaded_file)}")
            if hasattr(uploaded_file, 'keys'):
                logger.debug(f"Upload keys: {uploaded_file.keys()}")

            # Handle different Jupyter file upload formats
            content = None
            if isinstance(uploaded_file, dict):
                # New format: {filename: {'content': bytes}}
                for file_info in uploaded_file.values():
                    content = file_info['content']
                    break
            elif isinstance(uploaded_file, list):
                # Old format: [(name, {'content': bytes})]
                for _, file_info in uploaded_file:
                    content = file_info['content']
                    break
            else:
                # Fallback for unexpected formats
                content = getattr(uploaded_file, 'content', None)

            if not content:
                raise ValueError("No valid file content found in upload")

            # Process the content
            results = self.process_batch_content(content)
            self._display_results(results)

        except Exception as e:
            with self.out:
                print(f"Error processing file: {str(e)}")
                print("\nUpload structure debug info:")
                print(f"Type: {type(uploaded_file)}")
                if hasattr(uploaded_file, '__dict__'):
                    print(f"Attributes: {uploaded_file.__dict__.keys()}")
                logger.exception("File upload error structure")










def _process_uploaded_file(self, uploaded_file):
        """Handle uploaded file content in notebook"""
        try:
            # Handle different file upload formats
            if isinstance(uploaded_file, dict):
                # Newer Jupyter version format
                file_info = next(iter(uploaded_file.values()))
            elif isinstance(uploaded_file, list):
                # Older tuple-based format
                file_info = uploaded_file[0][1]
            else:
                raise ValueError("Unsupported file upload format")
            
            file_content = file_info['content']
            results = self.process_batch_content(file_content)
            self._display_results(results)
            
        except Exception as e:
            with out:
                print(f"Error processing file: {str(e)}")
                logger.exception("File processing error")











import sys
import json
from pathlib import Path
from IPython.display import display, JSON
import ipywidgets as widgets

class UniversalRegexGenerator(RegexGenerator):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.is_notebook = self._check_notebook()
        
    def _check_notebook(self) -> bool:
        """Check if running in Jupyter notebook"""
        try:
            from IPython import get_ipython
            return 'ipykernel' in str(get_ipython())
        except:
            return False
    
    def process_input(self):
        """Universal entry point that works in both environments"""
        if self.is_notebook:
            self._jupyter_interface()
        else:
            self._cli_interface()
    
    def _jupyter_interface(self):
        """Jupyter notebook interactive interface"""
        upload = widgets.FileUpload(
            description="Upload JSON",
            multiple=False,
            accept='.json'
        )
        
        out = widgets.Output()
        
        def on_process(button):
            out.clear_output()
            with out:
                if upload.value:
                    self._process_uploaded_file(upload.value)
        
        btn = widgets.Button(description="Process")
        btn.on_click(on_process)
        
        display(widgets.VBox([upload, btn, out]))
    
    def _process_uploaded_file(self, uploaded_file):
        """Handle uploaded file content in notebook"""
        file_content = next(iter(uploaded_file.values()))['content']
        results = self.process_batch_content(file_content)
        self._display_results(results)
    
    def _cli_interface(self):
        """Command-line interface handler"""
        if len(sys.argv) > 1:
            input_file = sys.argv[1]
        else:
            input_file = input("Enter input file path: ")
            
        results = self.process_batch(input_file)
        self._save_results(results, input_file)
        self._print_summary(results)
    
    def process_batch(self, file_path: str) -> dict:
        """Process batch from file path (CLI)"""
        with open(file_path, 'r') as f:
            content = f.read().encode('utf-8')
        return self.process_batch_content(content)
    
    def process_batch_content(self, content: bytes) -> dict:
        """Common processing logic for both interfaces"""
        # ... (keep previous process_batch_content implementation) ...
    
    def _display_results(self, results: dict):
        """Display results appropriately for environment"""
        if self.is_notebook:
            self._display_jupyter_results(results)
        else:
            self._display_cli_results(results)
    
    def _display_jupyter_results(self, results: dict):
        """Notebook-friendly display"""
        display(JSON(results))
        print(f"Processed {results['processed_count']} items")
        # ... (rest of Jupyter display logic) ...
    
    def _display_cli_results(self, results: dict):
        """CLI-friendly display"""
        print(json.dumps(results, indent=2))
    
    def _save_results(self, results: dict, input_path: str):
        """Save results to file"""
        output_file = Path(input_path).stem + "_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Results saved to {output_file}")

if __name__ == "__main__":
    # Initialize generator with configs
    generator = UniversalRegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    
    # Run appropriate interface
    if generator.is_notebook:
        generator._jupyter_interface()
    else:
        generator._cli_interface()










if __name__ == "__main__":
    import sys
    
    # Initialize generator
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )
    
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        input_file = input("Enter path to JSON file: ")
    
    try:
        # Process the batch file
        batch_result = generator.process_batch(input_file)
        
        # Save results
        output_file = Path(input_file).stem + "_results.json"
        with open(output_file, 'w') as f:
            json.dump(batch_result, f, indent=2)
            
        print(f"Processed {batch_result['processed_count']} items")
        print(f"Results saved to {output_file}")
        print(f"Errors: {len(batch_result['errors']}")
        
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        sys.exit(1)







print(f"Errors: {len(batch_result['errors']}")
print(f"Errors: {len(batch_result['errors'])}")







# ... [Keep all imports and utility classes unchanged] ...

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

class RegexGenerator:
    # ... [Keep __init__ and other initial methods unchanged] ...

    def validate_patterns(self, state: State) -> State:
        validation_results = []
        for idx, pattern in enumerate(state["candidates"]):
            try:
                # Validate syntax first
                is_valid = self._validate_syntax(pattern)
                validations = {
                    "syntax": is_valid,
                    "main_term": self._validate_main_term(pattern, state["name"]),
                    "coverage": self._calculate_coverage(pattern, state["all_terms"]),
                    "false_positives": self._check_false_positives(pattern, state)
                }
                
                # Calculate score
                score = self._calculate_score(validations)
                
                validation_results.append({
                    "pattern": pattern,
                    "score": score,
                    "validations": validations,
                    "reason": state["generation_reasons"][idx] if idx < len(state["generation_reasons"]) else "No reason provided"
                })
                
            except Exception as e:
                logger.error(f"Pattern validation failed: {e}")
                validation_results.append({
                    "pattern": pattern,
                    "score": 0.0,
                    "validations": {},
                    "reason": f"Validation error: {str(e)}"
                })
        
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    def select_pattern(self, state: State) -> State:
        if state["validation_results"]:
            best = state["validation_results"][0]
            if best["score"] >= 0.7 or best["validations"].get("main_term", False):
                state["best_pattern"] = best["pattern"]
                state["confidence"] = best["score"]
                state["reason"] = best["reason"]
                state["status"] = "complete"
                return state
        
        state["status"] = "retry" if state.get("iterations", 0) < 3 else "complete"
        state["iterations"] = state.get("iterations", 0) + 1
        return state

    # ... [Keep other helper methods unchanged] ...

    def process_request(self, input_data: Dict) -> Dict:
        try:
            request = RegexRequest(**input_data)
            state = State(
                name=request.name,
                definition=request.definition,
                related_terms=request.related_terms,
                synonyms=[],
                semantic_terms=[],
                keyword_terms=[],
                all_terms=[],
                candidates=[],
                generation_reasons=[],
                validation_results=[],
                best_pattern=None,
                confidence=None,
                reason=None,
                iterations=0,
                status=None
            )
            
            final_state = self.workflow.invoke(state)
            
            output = RegexOutput(
                name=final_state["name"],
                definition=final_state["definition"],
                regex=self._escape_for_json(final_state.get("best_pattern", "")),
                reason=final_state.get("reason", "Generation failed"),
                confidence_score=final_state.get("confidence", 0.0),
                validation_passes=[
                    f"{res['pattern']} (Score: {res['score']:.2f})" 
                    for res in final_state.get("validation_results", [])
                    if res.get("score", 0) >= 0.7
                ],
                validation_failures=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res.get("score", 0) < 0.7
                ],
                iterations=final_state.get("iterations", 0)
            )
            return output.model_dump()
        
        except ValidationError as e:
            logger.error(f"Validation error: {e}")
            return RegexOutput(
                name=input_data.get("name", ""),
                definition=input_data.get("definition", ""),
                regex="",
                reason=f"Validation error: {e}",
                confidence_score=0.0,
                validation_passes=[],
                validation_failures=[],
                iterations=0
            ).model_dump()

if __name__ == "__main__":
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )

    input_data = {
        "name": "email",
        "definition": "Electronic mail address",
        "related_terms": [
            {
                "name": "e-mail",
                "definition": "Alternative email format",
                "example": "user@domain.com"
            }
        ]
    }

    result = generator.process_request(input_data)
    print(json.dumps(result, indent=2))





















import os
import re
import json
import logging
import numpy as np
from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.retrievers import BM25Retriever
from langchain_community.chat_models import AzureChatOpenAI
from langchain_community.vectorstores import Chroma

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --------------------------
# Environment Configuration
# --------------------------
class OSEnv:
    # (Include your full OSEnv implementation here)

# --------------------------
# Data Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    example: str

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

class State(TypedDict):
    name: str
    definition: str
    related_terms: List[RelatedTerm]
    synonyms: List[str]
    semantic_terms: List[str]
    keyword_terms: List[str]
    all_terms: List[str]
    candidates: List[str]
    generation_reasons: List[str]
    validation_results: List[Dict]
    best_pattern: Optional[str]
    confidence: Optional[float]
    reason: Optional[str]
    iterations: int
    status: Optional[str]

# --------------------------
# Regex Generator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self._dummy_embedding_fn,
            persist_directory="./chroma_db"
        )

    def _dummy_embedding_fn(self, texts: List[str]) -> List[List[float]]:
        return [[0.0]*768 for _ in texts]

    def _build_workflow(self):
        workflow = StateGraph(State)
        workflow.add_node("enrich_context", self.enrich_context)
        workflow.add_node("generate_candidates", self.generate_candidates)
        workflow.add_node("validate_patterns", self.validate_patterns)
        workflow.add_node("select_pattern", self.select_pattern)
        
        workflow.add_edge("enrich_context", "generate_candidates")
        workflow.add_edge("generate_candidates", "validate_patterns")
        workflow.add_edge("validate_patterns", "select_pattern")
        
        workflow.add_conditional_edges(
            "select_pattern",
            self.retry_decision,
            {"retry": "generate_candidates", "complete": END}
        )
        workflow.set_entry_point("enrich_context")
        return workflow.compile()

    def retry_decision(self, state: State) -> str:
        return "retry" if state.get("status") == "retry" and state.get("iterations", 0) < 3 else "complete"

    # (Include all other methods: enrich_context, generate_candidates, 
    # validate_patterns, select_pattern, and helper methods here 
    # exactly as provided in previous corrected answers)

    def process_request(self, input_data: Dict) -> Dict:
        try:
            request = RegexRequest(**input_data)
            state = State(
                name=request.name,
                definition=request.definition,
                related_terms=request.related_terms,
                synonyms=[],
                semantic_terms=[],
                keyword_terms=[],
                all_terms=[],
                candidates=[],
                generation_reasons=[],
                validation_results=[],
                best_pattern=None,
                confidence=None,
                reason=None,
                iterations=0,
                status=None
            )
            
            final_state = self.workflow.invoke(state)
            
            output = RegexOutput(
                name=final_state["name"],
                definition=final_state["definition"],
                regex=self._escape_for_json(final_state.get("best_pattern", "")),
                reason=final_state.get("reason", "Generation failed"),
                confidence_score=final_state.get("confidence", 0.0),
                validation_passes=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] >= 0.7
                ],
                validation_failures=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] < 0.7
                ],
                iterations=final_state.get("iterations", 0)
            )
            return output.model_dump()
        
        except ValidationError as e:
            logger.error(f"Validation error: {e}")
            return RegexOutput(
                name=input_data.get("name", ""),
                definition=input_data.get("definition", ""),
                regex="",
                reason=f"Validation error: {e}",
                confidence_score=0.0,
                validation_passes=[],
                validation_failures=[],
                iterations=0
            ).model_dump()

    def _escape_for_json(self, pattern: str) -> str:
        """Properly escape regex for JSON output"""
        return json.dumps(pattern)[1:-1]  # Remove surrounding quotes

# --------------------------
# Usage Example
# --------------------------
if __name__ == "__main__":
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )

    input_data = {
        "name": "email",
        "definition": "Electronic mail address",
        "related_terms": [
            {
                "name": "e-mail",
                "definition": "Alternative email format",
                "example": "user@domain.com"
            }
        ]
    }

    result = generator.process_request(input_data)
    print(json.dumps(result, indent=2))
















import json
import re
from typing import Any

class RegexGenerator:
    # ... (keep previous methods unchanged) ...

    def validate_patterns(self, state: State) -> State:
        """Enhanced validation with regex syntax checking"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            try:
                # Check for proper regex syntax
                re.compile(pattern)
                
                # Check for common errors
                syntax_checks = {
                    "unclosed_groups": (r"\((?!\?:)", r"Count of '(' vs ')'"),
                    "unescaped_specials": (r"(?<!\\)[.^$*+?{}|\[\]\\]", "Unescaped special chars"),
                    "valid_boundaries": (r"\\b|\\B", "Valid word boundaries")
                }
                
                # Perform additional checks
                syntax_errors = []
                for check, (pattern, msg) in syntax_checks.items():
                    if re.search(pattern, pattern):
                        syntax_errors.append(msg)
                
                # Store validation results
                validation_results.append({
                    "pattern": pattern,
                    "raw_pattern": pattern.encode('unicode_escape').decode(),
                    "is_valid": True,
                    "syntax_errors": syntax_errors
                })
                
            except re.error as e:
                validation_results.append({
                    "pattern": pattern,
                    "raw_pattern": pattern.encode('unicode_escape').decode(),
                    "is_valid": False,
                    "error": str(e)
                })
        
        state["validation_results"] = validation_results
        return state

    def _prepare_for_json(self, pattern: str) -> str:
        """Convert regex pattern for proper JSON serialization"""
        return json.dumps(pattern)[1:-1]  # Remove surrounding quotes

    def process_request(self, input_data: Dict) -> Dict:
        """Final output processing with proper escaping"""
        result = super().process_request(input_data)
        
        # Convert pattern for JSON output
        if result.regex:
            result.regex = self._prepare_for_json(result.regex)
        
        return json.loads(result.json())








class RegexGenerator:
    # ... (keep previous initialization and other methods) ...

    def _build_workflow(self):
        workflow = StateGraph(State)
        
        workflow.add_node("enrich_context", self.enrich_context)
        workflow.add_node("generate_candidates", self.generate_candidates)
        workflow.add_node("validate_patterns", self.validate_patterns)
        workflow.add_node("select_pattern", self.select_pattern)
        
        workflow.add_edge("enrich_context", "generate_candidates")
        workflow.add_edge("generate_candidates", "validate_patterns")
        workflow.add_edge("validate_patterns", "select_pattern")
        
        # Add conditional edge with proper retry logic
        workflow.add_conditional_edges(
            "select_pattern",
            self.retry_decision,  # This now references the correct method
            {"retry": "generate_candidates", "complete": END}
        )
        
        workflow.set_entry_point("enrich_context")
        return workflow.compile()

    def retry_decision(self, state: State) -> str:
        """Determine whether to retry generation based on state"""
        if state.get("status") == "retry" and state.get("iterations", 0) < 3:
            return "retry"
        return "complete"

    def select_pattern(self, state: State) -> State:
        """Updated selection with status tracking"""
        if state["validation_results"]:
            best = state["validation_results"][0]
            
            # Acceptance criteria
            if best["score"] >= 0.7 or best["validations"].get("main_term", False):
                state["best_pattern"] = best["pattern"]
                state["confidence"] = best["score"]
                state["reason"] = best["reason"]
                state["status"] = "complete"
                return state
        
        # Set retry status if under max iterations
        if state.get("iterations", 0) < 3:
            state["status"] = "retry"
        else:
            state["status"] = "complete"
            
        state["iterations"] = state.get("iterations", 0) + 1
        return state






# --------------------------
# Enhanced generate_candidates Method
# --------------------------
def generate_candidates(self, state: State) -> State:
    """Robust candidate generation with response validation"""
    prompt = ChatPromptTemplate.from_template("""
    Generate 3 regex patterns for '{name}' ({definition}).
    Must include: {all_terms}
    Requirements:
    - Strictly match '{name}' exactly
    - Include all variations and related terms
    - Prevent false positives
    - Use proper anchoring and boundaries
    
    Output JSON format:
    {{
        "patterns": ["pattern1", "pattern2", "pattern3"],
        "reasons": ["reason1", "reason2", "reason3"]
    }}
    """)
    
    try:
        chain = prompt | self.llm | JsonOutputParser()
        response = chain.invoke({
            "name": state["name"],
            "definition": state["definition"],
            "all_terms": state["all_terms"]
        })
        
        # Validate response structure
        patterns = response.get("patterns", [])
        reasons = response.get("reasons", [])
        
        # Ensure equal number of patterns and reasons
        if len(patterns) != len(reasons):
            reasons = ["Auto-generated reason" for _ in patterns]
            
        # Validate patterns are strings
        valid_patterns = [str(p) for p in patterns if isinstance(p, str)]
        
        state["candidates"] = valid_patterns
        state["generation_reasons"] = reasons
        
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        state["candidates"] = []
        state["generation_reasons"] = []
    
    return state

# --------------------------
# Enhanced validate_patterns Method
# --------------------------
def validate_patterns(self, state: State) -> State:
    """Robust pattern validation with fallback handling"""
    validation_results = []
    
    for idx, pattern in enumerate(state["candidates"]):
        try:
            # Get reason safely
            reason = "No reason provided"
            if isinstance(state["generation_reasons"], list) and len(state["generation_reasons"]) > idx:
                reason = str(state["generation_reasons"][idx])
                
            # Validate pattern
            validations = {
                "syntax": self._validate_syntax(pattern),
                "main_term": self._validate_main_term(pattern, state["name"]),
                "coverage": self._calculate_coverage(pattern, state["all_terms"]),
                "false_positives": self._check_false_positives(pattern, state)
            }
            
            validation_results.append({
                "pattern": pattern,
                "score": self._calculate_score(validations),
                "reason": reason,
                "validations": validations
            })
            
        except Exception as e:
            logger.error(f"Pattern validation failed: {e}")
            validation_results.append({
                "pattern": pattern,
                "score": 0.0,
                "reason": f"Validation error: {str(e)}",
                "validations": {}
            })
    
    state["validation_results"] = sorted(
        validation_results, 
        key=lambda x: x["score"], 
        reverse=True
    )
    return state

# --------------------------
# Enhanced Validation Checks
# --------------------------
def _validate_main_term(self, pattern: str, main_term: str) -> bool:
    """Enhanced main term validation with normalization"""
    try:
        compiled = re.compile(pattern, re.IGNORECASE)
        return bool(compiled.fullmatch(main_term))
    except:
        return False

def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
    """Case-insensitive coverage calculation"""
    try:
        compiled = re.compile(pattern, re.IGNORECASE)
        matched = sum(1 for term in terms if compiled.fullmatch(str(term)))
        return matched / len(terms) if terms else 0.0
    except:
        return 0.0


def select_pattern(self, state: State) -> State:
    """Improved pattern selection with logging"""
    if state["validation_results"]:
        best = state["validation_results"][0]
        logger.debug(f"Best pattern score: {best['score']}")
        
        # Accept patterns with good main term matching even if coverage is lower
        if best["validations"].get("main_term", False) and best["score"] >= 0.6:
            state["best_pattern"] = best["pattern"]
            state["confidence"] = min(1.0, best["score"] * 1.2)  # Boost confidence
            state["reason"] = best["reason"]
            state["status"] = "complete"
            return state
    
    state["status"] = "retry" if state.get("iterations", 0) < 3 else "complete"
    state["iterations"] = state.get("iterations", 0) + 1
    return state



testing
# Manual test for the specific regex
pattern = r'\b([Ee]-?mail|email[_\\.address]*|EmailAddress|EMAIL|emails)\b'
test_cases = [
    "email", "Email", "EMAIL",
    "e-mail", "E-mail", "e-mail",
    "email_address", "email.address", 
    "EmailAddress", "emails"
]

compiled = re.compile(pattern, re.IGNORECASE)
for case in test_cases:
    print(f"{case}: {bool(compiled.fullmatch(case))}")

# All should return True for valid cases









from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
import re
import logging

logger = logging.getLogger(__name__)

# --------------------------
# Pydantic Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    example: str

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1)
    definition: str
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

class State(TypedDict):
    name: str
    definition: str
    related_terms: List[RelatedTerm]
    synonyms: List[str]
    semantic_terms: List[str]
    keyword_terms: List[str]
    all_terms: List[str]
    candidates: List[str]
    validation_results: List[Dict]
    best_pattern: Optional[str]
    confidence: Optional[float]
    reason: Optional[str]
    iterations: int
    status: Optional[str]

# --------------------------
# Regex Generator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        # ... (previous initialization code) ...

    def enrich_context(self, state: State) -> State:
        """Type-safe context enrichment with exhaustive validation"""
        state["synonyms"] = self._get_sanitized_variations(state["name"])
        
        # Process related terms with strict type checking
        for term in state["related_terms"]:
            if not isinstance(term, RelatedTerm):
                logger.error("Invalid related term type: %s", type(term))
                continue
                
            state["synonyms"].extend([
                self._str_safe(term.name),
                self._str_safe(term.example),
                self._str_safe(term.name).lower(),
                self._str_safe(term.name).replace("-", ""),
                self._str_safe(term.name).replace("_", "")
            ])

        # Semantic search with document validation
        try:
            semantic_results = self.knowledge_base.similarity_search(
                query=self._str_safe(state["definition"]),
                k=3
            )
            state["semantic_terms"] = [
                self._str_safe(doc.page_content) 
                for doc in semantic_results
                if hasattr(doc, 'page_content')
            ]
        except Exception as e:
            logger.error("Semantic search failed: %s", e)
            state["semantic_terms"] = []

        # Keyword search with input sanitation
        try:
            documents = [
                self._str_safe(doc.page_content) 
                for doc in self.knowledge_base.get().documents
                if hasattr(doc, 'page_content')
            ]
            keyword_retriever = BM25Retriever.from_texts(documents)
            keyword_results = keyword_retriever.invoke(
                self._str_safe(state["name"])
            )
            state["keyword_terms"] = [
                self._str_safe(res.page_content)
                for res in keyword_results
                if hasattr(res, 'page_content')
            ]
        except Exception as e:
            logger.error("Keyword search failed: %s", e)
            state["keyword_terms"] = []

        # Combine and sanitize all terms
        all_terms = (
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        )
        
        state["all_terms"] = list(set(
            filter(None, [
                self._str_safe(term).strip() 
                for term in all_terms
                if self._is_valid_term(term)
            ])
        ))
        
        logger.debug("Sanitized all_terms: %s", state["all_terms"])
        return state

    def _get_sanitized_variations(self, term: str) -> List[str]:
        """Generate linguistic variations with type safety"""
        base_term = self._str_safe(term)
        return [
            var for var in [
                base_term,
                base_term.lower(),
                base_term.upper(),
                base_term.title(),
                base_term.replace("-", ""),
                base_term.replace("_", ""),
                base_term.replace(" ", ""),
                f"{base_term}s",
                base_term[:-1] if base_term.endswith("s") else ""
            ]
            if self._is_valid_term(var)
        ]

    def _str_safe(self, value: any) -> str:
        """Convert value to string with fallback handling"""
        try:
            return str(value)
        except Exception as e:
            logger.warning("String conversion failed: %s", e)
            return ""

    def _is_valid_term(self, term: any) -> bool:
        """Validate term type and content"""
        if not isinstance(term, str):
            logger.warning("Invalid term type discarded: %s", type(term))
            return False
        return bool(term.strip())

    def validate_patterns(self, state: State) -> State:
        """Type-safe pattern validation"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            try:
                valid_pattern = self._str_safe(pattern)
                validations = {
                    "syntax": self._validate_syntax(valid_pattern),
                    "main_term": self._validate_main_term(valid_pattern, state["name"]),
                    "coverage": self._calculate_coverage(valid_pattern, state["all_terms"]),
                    "false_positives": self._check_false_positives(valid_pattern, state)
                }
                
                validation_results.append({
                    "pattern": valid_pattern,
                    "score": self._calculate_score(validations),
                    "reason": self._str_safe(state["generation_reasons"][idx]),
                    "validations": validations
                })
            except Exception as e:
                logger.error("Pattern validation failed: %s", e)
                
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    # ... (rest of the class methods with similar type safety checks) ...






# --------------------------
# Modified enrich_context Method
# --------------------------
def enrich_context(self, state: State) -> State:
    """Hybrid context enrichment with type safety checks"""
    # Initialize synonyms with string conversions
    state["synonyms"] = [str(var) for var in self._get_linguistic_variations(state["name"])]
    
    # Process related terms with explicit string casting
    for term in state["related_terms"]:
        term_name = str(term.name)
        term_example = str(term.example)
        state["synonyms"].extend([
            term_name,
            term_example,
            term_name.lower(),
            term_name.replace("-", ""),
            term_name.replace("_", "")
        ])
    
    # Semantic search with type safety
    try:
        semantic_results = self.knowledge_base.similarity_search(
            query=str(state["definition"]),  # Ensure string query
            k=3
        )
        state["semantic_terms"] = [str(doc.page_content) for doc in semantic_results]
    except Exception as e:
        logger.error(f"Semantic search failed: {e}")
        state["semantic_terms"] = []

    # Keyword search with type safety
    try:
        documents = [str(doc.page_content) for doc in self.knowledge_base.get().documents]
        keyword_retriever = BM25Retriever.from_texts(documents)
        keyword_results = keyword_retriever.invoke(str(state["name"]))  # Ensure string input
        state["keyword_terms"] = [str(res.page_content) for res in keyword_results]
    except Exception as e:
        logger.error(f"Keyword search failed: {e}")
        state["keyword_terms"] = []

    # Create all_terms with guaranteed string types
    all_terms = [
        *state["synonyms"],
        *state["semantic_terms"],
        *state["keyword_terms"]
    ]
    
    # Filter out empty strings and convert to hashable types
    state["all_terms"] = list(set(
        filter(None, [term.strip() for term in all_terms if isinstance(term, str)])
    ))
    
    return state

# --------------------------
# Updated Validation Methods
# --------------------------
def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
    """Safe coverage calculation with type checks"""
    try:
        compiled = re.compile(str(pattern))  # Ensure pattern is string
        matched = 0
        for term in terms:
            # Skip non-string terms
            if not isinstance(term, str):
                continue
            try:
                if compiled.fullmatch(term):
                    matched += 1
            except re.error:
                continue
        return matched / len(terms) if terms else 0.0
    except:
        return 0.0

def _check_false_positives(self, pattern: str, state: State) -> int:
    """Safe false positive check with type validation"""
    try:
        compiled = re.compile(str(pattern))  # Ensure pattern is string
        fp_count = 0
        for rt in state["related_terms"]:
            example = str(rt.example)  # Ensure example is string
            if not compiled.fullmatch(example):
                fp_count += 1
        return fp_count
    except:
        return len(state["related_terms"])







# --------------------------
# Updated Chroma Integration
# --------------------------
class ChromaEmbeddingWrapper:
    def __init__(self, embedding_client: EmbeddingClient):
        self.embedding_client = embedding_client
        
    def embed_query(self, text: str) -> List[float]:
        """Wrapper for single query embedding"""
        doc = MyDocument(text=text)
        embedded_doc = self.embedding_client.generate_embeddings(doc)
        return embedded_doc.embedding
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Wrapper for document batch embedding"""
        return [self.embed_query(text) for text in texts]

class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embedding_client = EmbeddingClient()
        self.embedding_wrapper = ChromaEmbeddingWrapper(self.embedding_client)
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self.embedding_wrapper,
            persist_directory="./chroma_db",
            client_settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

    def enrich_context(self, state: State) -> State:
        """Hybrid context enrichment with proper embedding handling"""
        state["synonyms"] = self._get_linguistic_variations(state["name"])
        
        # Add related terms and examples
        for term in state["related_terms"]:
            state["synonyms"].extend([
                term.name,
                term.example,
                term.name.lower(),
                term.name.replace("-", ""),
                term.name.replace("_", "")
            ])
        
        # Semantic search expansion
        try:
            semantic_results = self.knowledge_base.similarity_search(
                query=state["definition"],
                k=3
            )
            state["semantic_terms"] = [doc.page_content for doc in semantic_results]
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            state["semantic_terms"] = []

        # Keyword search expansion
        try:
            keyword_retriever = BM25Retriever.from_texts(
                [doc.page_content for doc in self.knowledge_base.get().documents]
            )
            keyword_results = keyword_retriever.invoke(state["name"])
            state["keyword_terms"] = [res.page_content for res in keyword_results]
        except Exception as e:
            logger.error(f"Keyword search failed: {e}")
            state["keyword_terms"] = []

        # Combine all terms
        state["all_terms"] = list(set(
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        ))
        return state

    # Rest of the class remains unchanged




# --------------------------
# Embedding Client (Your Existing Implementation)
# --------------------------
class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}

class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", 
                 embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(
            azure_endpoint=os.getenv("AZURE_ENDPOINT"),
            api_version=self.azure_api_version,
            azure_ad_token_provider=token_provider
        )
    
    def generate_embeddings(self, doc: MyDocument) -> MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc

# --------------------------
# Modified RegexGenerator Class
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embedding_client = EmbeddingClient()
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        # Create Chroma collection with your embedding client
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self._chroma_embedding_fn,
            persist_directory="./chroma_db",
            client_settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

    def _chroma_embedding_fn(self, texts: List[str]) -> List[List[float]]:
        """Adapter for Chroma to use your EmbeddingClient"""
        embeddings = []
        for text in texts:
            doc = MyDocument(text=text)
            embedded_doc = self.embedding_client.generate_embeddings(doc)
            embeddings.append(embedded_doc.embedding)
        return embeddings

    # Rest of the RegexGenerator class remains the same as previous implementation
    # ... [include all other methods unchanged] ...






import os
import re
import json
import numpy as np
from typing import List, Dict, Optional, TypedDict
from pydantic import BaseModel, Field, ValidationError
from langchain_community.retrievers import BM25Retriever
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import AzureOpenAIEmbeddings
from langchain_community.chat_models import AzureChatOpenAI

# --------------------------
# Environment Configuration
# --------------------------
class OSEnv:
    # (Include the full OSEnv implementation from previous code here)
    
# --------------------------
# Data Models
# --------------------------
class RelatedTerm(BaseModel):
    name: str = Field(..., min_length=1, description="Related term name")
    definition: str = Field(..., description="Description of related term")
    example: str = Field(..., description="Example usage of the term")

class RegexRequest(BaseModel):
    name: str = Field(..., min_length=1, description="Primary term name")
    definition: str = Field(..., description="Term definition")
    related_terms: List[RelatedTerm] = Field(default_factory=list)

class RegexOutput(BaseModel):
    name: str
    definition: str
    regex: str
    reason: str
    confidence_score: float
    validation_passes: List[str]
    validation_failures: List[str]
    iterations: int

# --------------------------
# Regex Generation System
# --------------------------
class RegexGenerator:
    def __init__(self, config_path: str, creds_path: str, cert_path: str):
        self.env = OSEnv(config_path, creds_path, cert_path)
        self.llm = AzureChatOpenAI(
            deployment_name=self.env.get("MODEL_NAME", "gpt-4"),
            temperature=0.3,
            max_tokens=500
        )
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment="text-embedding-3-large"
        )
        self.knowledge_base = self._init_knowledge_base()
        self.workflow = self._build_workflow()

    def _init_knowledge_base(self):
        return Chroma(
            collection_name="regex_patterns",
            embedding_function=self.embeddings,
            persist_directory="./chroma_db"
        )

    def _build_workflow(self):
        workflow = StateGraph(State)
        
        workflow.add_node("enrich_context", self.enrich_context)
        workflow.add_node("generate_candidates", self.generate_candidates)
        workflow.add_node("validate_patterns", self.validate_patterns)
        workflow.add_node("select_pattern", self.select_pattern)
        
        workflow.add_edge("enrich_context", "generate_candidates")
        workflow.add_edge("generate_candidates", "validate_patterns")
        workflow.add_edge("validate_patterns", "select_pattern")
        
        workflow.add_conditional_edges(
            "select_pattern",
            self.retry_decision,
            {"retry": "generate_candidates", "complete": END}
        )
        
        workflow.set_entry_point("enrich_context")
        return workflow.compile()

    # --------------------------
    # Workflow Components
    # --------------------------
    def enrich_context(self, state: State) -> State:
        """Hybrid context enrichment with semantic and related terms"""
        state["synonyms"] = self._get_linguistic_variations(state["name"])
        
        # Add related terms and examples
        for term in state["related_terms"]:
            state["synonyms"].extend([
                term.name,
                term.example,
                term.name.lower(),
                term.name.replace("-", ""),
                term.name.replace("_", "")
            ])
        
        # Semantic search expansion
        semantic_results = self.knowledge_base.similarity_search(
            query=state["definition"],
            k=3
        )
        state["semantic_terms"] = [doc.page_content for doc in semantic_results]
        
        # Keyword search expansion
        keyword_retriever = BM25Retriever.from_texts(
            [doc.page_content for doc in self.knowledge_base.get().documents]
        )
        keyword_results = keyword_retriever.invoke(state["name"])
        state["keyword_terms"] = [res.page_content for res in keyword_results]
        
        # Combine all terms
        state["all_terms"] = list(set(
            state["synonyms"] +
            state["semantic_terms"] +
            state["keyword_terms"]
        ))
        return state

    def generate_candidates(self, state: State) -> State:
        """Generate multiple regex candidates with LLM"""
        prompt = ChatPromptTemplate.from_template("""
        Generate 3 regex patterns for '{name}' ({definition}).
        Must include: {all_terms}
        Requirements:
        - Strictly match '{name}' exactly
        - Include all variations and related terms
        - Prevent false positives
        - Use proper anchoring and boundaries
        
        Output JSON with 'patterns' list and 'reasoning' for each pattern.
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        response = chain.invoke({
            "name": state["name"],
            "definition": state["definition"],
            "all_terms": state["all_terms"]
        })
        
        state["candidates"] = response.get("patterns", [])
        state["generation_reasons"] = response.get("reasoning", [])
        return state

    def validate_patterns(self, state: State) -> State:
        """Multi-dimensional pattern validation"""
        validation_results = []
        
        for idx, pattern in enumerate(state["candidates"]):
            validations = {
                "syntax": self._validate_syntax(pattern),
                "main_term": self._validate_main_term(pattern, state["name"]),
                "coverage": self._calculate_coverage(pattern, state["all_terms"]),
                "false_positives": self._check_false_positives(pattern, state)
            }
            
            validation_results.append({
                "pattern": pattern,
                "score": self._calculate_score(validations),
                "reason": state["generation_reasons"][idx],
                "validations": validations
            })
        
        state["validation_results"] = sorted(
            validation_results, 
            key=lambda x: x["score"], 
            reverse=True
        )
        return state

    def select_pattern(self, state: State) -> State:
        """Select best pattern or trigger retry"""
        if state["validation_results"]:
            best = state["validation_results"][0]
            if best["score"] >= 0.8:
                state["best_pattern"] = best["pattern"]
                state["confidence"] = best["score"]
                state["reason"] = best["reason"]
                state["status"] = "complete"
                return state
        
        state["status"] = "retry" if state.get("iterations", 0) < 3 else "complete"
        state["iterations"] = state.get("iterations", 0) + 1
        return state

    def retry_decision(self, state: State) -> str:
        return "retry" if state.get("status") == "retry" else "complete"

    # --------------------------
    # Validation Utilities
    # --------------------------
    def _validate_syntax(self, pattern: str) -> bool:
        try:
            re.compile(pattern)
            return True
        except re.error:
            return False

    def _validate_main_term(self, pattern: str, main_term: str) -> bool:
        try:
            return bool(re.fullmatch(pattern, main_term))
        except:
            return False

    def _calculate_coverage(self, pattern: str, terms: List[str]) -> float:
        try:
            matched = sum(1 for term in terms if re.fullmatch(pattern, term))
            return matched / len(terms)
        except:
            return 0.0

    def _check_false_positives(self, pattern: str, state: State) -> int:
        try:
            regex = re.compile(pattern)
            return sum(
                1 for rt in state["related_terms"]
                if not regex.fullmatch(rt.example)
            )
        except:
            return len(state["related_terms"])

    def _calculate_score(self, validations: Dict) -> float:
        weights = {
            "syntax": 0.3,
            "main_term": 0.25,
            "coverage": 0.35,
            "false_positives": -0.1
        }
        score = 0
        score += weights["syntax"] * validations["syntax"]
        score += weights["main_term"] * validations["main_term"]
        score += weights["coverage"] * validations["coverage"]
        score += weights["false_positives"] * validations["false_positives"]
        return max(0.0, min(1.0, score))

    # --------------------------
    # Helper Methods
    # --------------------------
    def _get_linguistic_variations(self, term: str) -> List[str]:
        return [
            term,
            term.lower(),
            term.upper(),
            term.title(),
            term.replace("-", ""),
            term.replace("_", ""),
            term.replace(" ", ""),
            term + "s",  # Plural
            term[:-1] if term.endswith("s") else ""  # Singular
        ]

    def process_request(self, input_data: Dict) -> RegexOutput:
        try:
            request = RegexRequest(**input_data)
            state = {
                "name": request.name,
                "definition": request.definition,
                "related_terms": request.related_terms,
                "iterations": 0
            }
            
            final_state = self.workflow.invoke(state)
            
            return RegexOutput(
                name=final_state["name"],
                definition=final_state["definition"],
                regex=final_state.get("best_pattern", ""),
                reason=final_state.get("reason", "Generation failed"),
                confidence_score=final_state.get("confidence", 0.0),
                validation_passes=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] >= 0.7
                ],
                validation_failures=[
                    f"{res['pattern']} (Score: {res['score']:.2f})"
                    for res in final_state.get("validation_results", [])
                    if res["score"] < 0.7
                ],
                iterations=final_state.get("iterations", 0)
            )
        except ValidationError as e:
            return self._error_output(input_data, f"Validation error: {e}")

    def _error_output(self, input_data: Dict, error: str) -> RegexOutput:
        return RegexOutput(
            name=input_data.get("name", ""),
            definition=input_data.get("definition", ""),
            regex="",
            reason=error,
            confidence_score=0.0,
            validation_passes=[],
            validation_failures=[],
            iterations=0
        )

# --------------------------
# Usage Example
# --------------------------
if __name__ == "__main__":
    # Initialize generator
    generator = RegexGenerator(
        config_path="env/config.env",
        creds_path="env/credentials.env",
        cert_path="env/cacert.pem"
    )

    # Sample input with related terms
    input_data = {
        "name": "email",
        "definition": "Electronic mail address",
        "related_terms": [
            {
                "name": "e-mail",
                "definition": "Alternative email format",
                "example": "user@domain.com"
            },
            {
                "name": "email_address",
                "definition": "Formal email identifier",
                "example": "name@organization.org"
            }
        ]
    }

    # Process request
    result = generator.process_request(input_data)
    
    print(json.dumps(result.dict(), indent=2))
