import os
import re
import uuid
import json
import logging
import pandas as pd
import networkx as nx
import chromadb
from chromadb.config import Settings
from pathlib import Path
from typing import Optional, List, Dict, Any

from dotenv import load_dotenv, dotenv_values
from azure.identity import ClientSecretCredential

# LangChain (community graph modules)
# You may need: pip install langchain[graph]
from langchain.indexes.graph import GraphIndexCreator
from langchain.chains import GraphQAChain
from langchain.graphs.networkx_graph import NetworkxEntityGraph
# For embeddings or chat models
from langchain.chat_models import AzureChatOpenAI
from langchain.llms import AzureOpenAI  # or use a generic LLM if needed
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# A pydantic-based Document + EmbeddingClient, if you have them:
from ooai_client_sdk.azopenaiembrdding import Document, EmbeddingClient

#########################
# Logging Setup
#########################
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

#########################
# Helper Functions
#########################
def stable_chunk_text(text: str, max_chars: int = 3000) -> List[str]:
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sent in sentences:
        if len(current_chunk) + len(sent) + 1 <= max_chars:
            current_chunk = (current_chunk + " " + sent).strip() if current_chunk else sent
        else:
            chunks.append(current_chunk)
            current_chunk = sent
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def average_embeddings(embeddings: List[List[float]]) -> List[float]:
    if not embeddings:
        return []
    dim = len(embeddings[0])
    sum_vec = [0.0] * dim
    for emb in embeddings:
        for i, val in enumerate(emb):
            sum_vec[i] += val
    return [v / len(embeddings) for v in sum_vec]

#########################
# OSEnv: environment + token
#########################
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        # Load environment, get Azure AD token if SECURED_ENDPOINTS=True
        # (Implementation omitted for brevity)
        ...

#########################
# VectorStoreManager: row-by-row ingestion
#########################
class VectorStoreManager:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(name="knowledge_base")

    def add_single_document(self, doc: Dict[str, Any], embedding: List[float]):
        if not embedding:
            return
        row_id = str(uuid.uuid4())
        meta = {"name": doc["name"], "definition": doc["definition"]}
        content = doc["name"] + " " + doc["definition"]
        self.collection.add(
            ids=[row_id],
            documents=[content],
            embeddings=[embedding],
            metadatas=[meta]
        )

    def query(self, query_embedding: List[float], n_results: int = 4) -> List[Dict[str, Any]]:
        results = self.collection.query(query_embedding, n_results=n_results)
        out = []
        if results and "metadatas" in results:
            for metas_for_one_query in results["metadatas"]:
                for meta in metas_for_one_query:
                    out.append(meta)
        return out

#########################
# Main
#########################
def main():
    """
    Illustrative script:
      1) Build a graph from CSV using GraphIndexCreator
      2) Row-by-row ingestion into Chroma (avoid big batches)
      3) Hybrid query: top-k from vector store + GraphQAChain
    """
    # 1) Load environment, tokens, etc.
    # env = OSEnv(...)

    # 2) Initialize EmbeddingClient, VectorStore
    embedding_client = EmbeddingClient()
    vector_store = VectorStoreManager()

    # 3) Read CSV => row-by-row => chunk => embed => add to vector store
    csv_path = "my_knowledge.csv"
    df = pd.read_csv(csv_path)
    for _, row in df.iterrows():
        name = str(row["name"])
        definition = str(row["definition"])
        text = name + " " + definition

        # chunk + embed
        chunks = stable_chunk_text(text, max_chars=3000)
        chunk_embs = []
        for ch in chunks:
            doc_obj = Document(text=ch)
            doc_with_emb = embedding_client.generate_embeddings(doc_obj)
            if doc_with_emb.embedding:
                chunk_embs.append(doc_with_emb.embedding)
        final_emb = average_embeddings(chunk_embs)

        vector_store.add_single_document({"name": name, "definition": definition}, final_emb)

    # 4) Build a knowledge graph from entire CSV text using GraphIndexCreator
    #    (One approach: combine all rows into a single big text)
    big_text = ""
    for _, row in df.iterrows():
        big_text += f"{row['name']} => {row['definition']}\n"
    # Alternatively, you might parse relationships row-by-row.

    # GraphIndexCreator: parse the text to build a knowledge graph
    from langchain.indexes.graph import GraphIndexCreator
    from langchain.graphs.networkx_graph import NetworkxEntityGraph
    # Suppose we have a chat model for the graph creation
    graph_llm = AzureOpenAI(temperature=0, ...)
    graph_creator = GraphIndexCreator(llm=graph_llm)
    # This will parse the big_text to extract entities & relations
    graph = graph_creator.from_text(big_text)

    # We build a GraphQAChain from that graph
    # The chain can do QA over the knowledge graph
    from langchain.chains import GraphQAChain
    # Suppose we have a separate Azure LLM for QA
    graph_qa_llm = AzureOpenAI(temperature=0, ...)
    graph_qa_chain = GraphQAChain.from_llm(llm=graph_qa_llm, graph=graph)

    # 5) Hybrid query:
    user_query = "What is the best name for X?"  # example
    # 5a) Vector approach
    # chunk + embed the query
    q_chunks = stable_chunk_text(user_query)
    q_embs = []
    for ch in q_chunks:
        doc_q = Document(text=ch)
        doc_q_emb = embedding_client.generate_embeddings(doc_q)
        if doc_q_emb.embedding:
            q_embs.append(doc_q_emb.embedding)
    final_q_emb = average_embeddings(q_embs)
    top_k_vector = vector_store.query(final_q_emb, n_results=4)

    # 5b) Graph approach
    # Use graph_qa_chain
    graph_answer = graph_qa_chain.run(user_query)

    # 5c) Merge or present results
    # For example, we can combine top_k_vector plus the graph answer in some agent
    # or just print them:
    print("Vector-based top-k results:")
    for idx, meta in enumerate(top_k_vector, start=1):
        print(f"{idx}. {meta}")

    print("\nGraph-based answer:", graph_answer)

    # A final agent or logic to unify them:
    # e.g. if top_k_vector is empty, rely on graph_answer, else compare

    print("Hybrid approach done.")

if __name__ == "__main__":
    main()
