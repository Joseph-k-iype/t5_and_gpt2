#!/usr/bin/env python3
"""
Improved version with better error handling and connection management:
1. Enhanced retry logic for API calls
2. Better certificate and proxy handling
3. Improved error messages and logging
4. Connection pooling optimization
5. Proper cleanup of resources
"""

import os
import time
import json
import logging
import warnings
import requests
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
from contextlib import contextmanager
from urllib3.exceptions import InsecureRequestWarning

# Azure Identity for SECURED_ENDPOINTS
from azure.identity import ClientSecretCredential
from azure.core.exceptions import AzureError

# LanceDB & OpenAI
import lancedb
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
from langchain.docstore.document import Document
import openai

# Suppress insecure warnings if using custom cert
warnings.filterwarnings("ignore", category=InsecureRequestWarning)

###########################
# Enhanced Logging
###########################
def setup_logging(log_dir: Path) -> logging.Logger:
    """Configure logging with both file and console handlers"""
    logger = logging.getLogger("embeddings_matcher")
    logger.setLevel(logging.INFO)
    
    # Prevent duplicate handlers
    if logger.handlers:
        return logger
        
    # File handler with timestamp
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    fh = logging.FileHandler(log_dir / f"matcher_{timestamp}.log")
    fh.setLevel(logging.INFO)
    
    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

###########################
# Enhanced Environment Management
###########################
class EnhancedEnvironment:
    """
    Enhanced environment manager with better error handling and validation
    """
    def __init__(self, config_file: Path, creds_file: Path, cert_file: Path):
        self.logger = logging.getLogger("embeddings_matcher")
        self.vars = {}
        self.token = None
        self._load_configs(config_file, creds_file)
        self._setup_cert(cert_file)
        self._setup_proxy()
        self._setup_azure()
    
    def _load_configs(self, config_file: Path, creds_file: Path):
        """Load and validate configuration files"""
        try:
            # Load config first
            if not config_file.exists():
                raise FileNotFoundError(f"Config file not found: {config_file}")
            self._load_env_file(config_file, is_config=True)
            
            # Then load credentials
            if not creds_file.exists():
                raise FileNotFoundError(f"Credentials file not found: {creds_file}")
            self._load_env_file(creds_file, is_config=False)
            
        except Exception as e:
            self.logger.error(f"Failed to load configuration: {str(e)}")
            raise

    def _load_env_file(self, path: Path, is_config: bool):
        """Load single environment file with enhanced error checking"""
        try:
            with open(path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip("'\"")
                            self.vars[key] = value
                            os.environ[key] = value
                            
                            if is_config:
                                # Mask sensitive values in logs
                                log_value = value if 'SECRET' not in key else '****'
                                self.logger.info(f"Loaded config: {key}={log_value}")
        
        except Exception as e:
            self.logger.error(f"Error loading file {path}: {str(e)}")
            raise

    def _setup_cert(self, cert_file: Path):
        """Configure SSL certificate with validation"""
        if not cert_file.exists():
            self.logger.warning(f"Certificate file not found: {cert_file}")
            return
            
        try:
            import ssl
            # Validate certificate
            ssl.create_default_context(cafile=str(cert_file))
            
            # Set certificate environment variables
            cert_vars = ["REQUESTS_CA_BUNDLE", "SSL_CERT_FILE", "CURL_CA_BUNDLE"]
            for var in cert_vars:
                os.environ[var] = str(cert_file)
            
            self.logger.info(f"Certificate configured: {cert_file}")
            
        except ssl.SSLError as e:
            self.logger.error(f"Invalid certificate file: {str(e)}")
            raise
        except Exception as e:
            self.logger.error(f"Error setting up certificate: {str(e)}")
            raise

    def _setup_proxy(self):
        """Configure proxy settings with enhanced validation"""
        if not self.get_bool("PROXY_ENABLED", False):
            return
            
        try:
            # Get proxy credentials
            user = self.get("AD_USERNAME")
            pw = self.get("AD_USER_PW")
            domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([user, pw, domain]):
                raise ValueError("Missing proxy credentials")
                
            # Set up proxy URL
            proxy_url = f"http://{user}:{pw}@{domain}"
            os.environ["HTTP_PROXY"] = proxy_url
            os.environ["HTTPS_PROXY"] = proxy_url
            
            # Configure NO_PROXY
            no_proxy = self.get("NO_PROXY", "")
            no_proxy_list = {d.strip() for d in no_proxy.split(",") if d.strip()}
            
            # Add required domains
            required_domains = {
                'cognitiveservices.azure.com',
                'search.windows.net', 
                'openai.azure.com',
                'core.windows.net',
                'openaipublic.blob.core.windows.net',
                'azurewebsites.net'
            }
            
            no_proxy_list.update(required_domains)
            os.environ["NO_PROXY"] = ",".join(no_proxy_list)
            
            self.logger.info("Proxy configured successfully")
            
        except Exception as e:
            self.logger.error(f"Error configuring proxy: {str(e)}")
            raise

    def _setup_azure(self):
        """Configure Azure authentication if needed"""
        if not self.get_bool("SECURED_ENDPOINTS", False):
            return
            
        try:
            tenant_id = self.get("AZURE_TENANT_ID")
            client_id = self.get("AZURE_CLIENT_ID")
            client_secret = self.get("AZURE_CLIENT_SECRET")
            
            if not all([tenant_id, client_id, client_secret]):
                raise ValueError("Missing Azure credentials")
                
            credential = ClientSecretCredential(tenant_id, client_id, client_secret)
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.token = token.token
            
            self.logger.info("Azure AD token acquired successfully")
            
        except Exception as e:
            self.logger.error(f"Error setting up Azure authentication: {str(e)}")
            raise

    def get(self, key: str, default: str = "") -> str:
        """Get environment variable with default"""
        return self.vars.get(key, os.getenv(key, default))

    def get_bool(self, key: str, default: bool = False) -> bool:
        """Get boolean environment variable"""
        val = self.get(key, str(default)).lower()
        return val == 'true'

###########################
# Enhanced OpenAI Session Management
###########################
@contextmanager
def openai_session(cert_path: Optional[str] = None, timeout: int = 300):
    """Context manager for OpenAI API session with proper cleanup"""
    session = None
    try:
        session = requests.Session()
        
        if cert_path:
            session.verify = cert_path
            
        # Configure retries
        retries = requests.adapters.Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"]
        )
        adapter = requests.adapters.HTTPAdapter(max_retries=retries)
        session.mount("https://", adapter)
        
        # Configure OpenAI
        openai.requestssession = session
        openai.timeout = timeout
        
        yield session
        
    finally:
        if session:
            session.close()

###########################
# Enhanced LanceDB Management
###########################
class EnhancedLanceDBIndex:
    """Enhanced LanceDB manager with better error handling and cleanup"""
    def __init__(self, env: EnhancedEnvironment, db_path: str = "./lance_vectors"):
        self.logger = logging.getLogger("embeddings_matcher")
        self.env = env
        self.db_path = db_path
        self.table_name = "pbt_vectors"
        self._setup_embeddings()
        
    def _setup_embeddings(self):
        """Configure OpenAI embeddings with proper error handling"""
        try:
            api_base = self.env.get("AZURE_OPENAI_ENDPOINT")
            if not api_base:
                raise ValueError("Missing AZURE_OPENAI_ENDPOINT")
                
            api_version = self.env.get("API_VERSION", "2023-03-15-preview")
            api_key = self.env.token if self.env.get_bool("SECURED_ENDPOINTS") else self.env.get("AZURE_API_KEY")
            
            model = self.env.get("AZURE_EMBEDDINGS_MODEL", "text-embedding-ada-002")
            deployment = self.env.get("AZURE_EMBEDDINGS_DEPLOYMENT")
            
            if not deployment:
                raise ValueError("Missing AZURE_EMBEDDINGS_DEPLOYMENT")
            
            self.embedding_fn = OpenAIEmbeddings(
                deployment=deployment,
                model=model,
                openai_api_type="azure",
                openai_api_key=api_key,
                openai_api_base=api_base,
                openai_api_version=api_version
            )
            
            self.logger.info(f"Embeddings configured: model={model}, deployment={deployment}")
            
        except Exception as e:
            self.logger.error(f"Failed to setup embeddings: {str(e)}")
            raise

    def index_target_data(self, target_df: pd.DataFrame):
        """Index target data with proper error handling and progress tracking"""
        try:
            db = lancedb.connect(self.db_path)
            
            if self.table_name in db.table_names():
                self.logger.info(f"Removing existing table: {self.table_name}")
                db.drop_table(self.table_name)
            
            docs = []
            for _, row in tqdm(target_df.iterrows(), total=len(target_df), desc="Preparing documents"):
                text = f"{row['pbt-name']} : {row['pbt-definition']}"
                docs.append(
                    Document(
                        page_content=text,
                        metadata={
                            "pbt-name": row["pbt-name"],
                            "pbt-definition": row["pbt-definition"]
                        }
                    )
                )
            
            LanceDB.from_documents(
                documents=docs,
                embedding=self.embedding_fn,
                connection=db,
                table_name=self.table_name
            )
            
            self.logger.info(f"Indexed {len(docs)} documents in LanceDB at {self.db_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to index target data: {str(e)}")
            raise

    def search_similar(self, text: str, top_k: int = 5) -> List[Dict]:
        """Search for similar documents with error handling"""
        try:
            db = lancedb.connect(self.db_path)
            store = LanceDB(
                connection=db,
                table_name=self.table_name,
                embedding=self.embedding_fn
            )
            
            results = store.similarity_search_with_score(text, k=top_k)
            
            return [{
                "pbt-name": doc.metadata["pbt-name"],
                "pbt-definition": doc.metadata["pbt-definition"],
                "similarity_score": float(score)
            } for doc, score in results]
            
        except Exception as e:
            self.logger.error(f"Error in similarity search: {str(e)}")
            raise

###########################
# Main Function
###########################
def main():
    """Enhanced main function with proper error handling and cleanup"""
    logger = None
    try:
        # Setup paths
        base_dir = Path(__file__).parent.parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"
        
        # Create directories
        for d in [env_dir, data_dir, output_dir, log_dir]:
            d.mkdir(exist_ok=True)
        
        # Setup logging
        logger = setup_logging(log_dir)
        
        # Initialize paths
        config_file = env_dir / "config.env"
        creds_file = env_dir / "credentials.env"
        cert_file = env_dir / "cacert.pem"
        source_csv = data_dir / "source.csv"
        target_csv = data_dir / "target.csv"
        
        # Validate files
        required_files = {
            "config.env": config_file,
            "credentials.env": creds_file,
            "cacert.pem": cert_file,
            "source.csv": source_csv,
            "target.csv": target_csv
        }
        
        missing = [name for name, path in required_files.items() if not path.exists()]
        if missing:
            raise FileNotFoundError(f"Missing required files: {', '.join(missing)}")
        
        # Initialize environment
        env = EnhancedEnvironment(config_file, creds_file, cert_file)
        
        # Setup OpenAI session
        with openai_session(cert_path=cert_file) as session:
            # Read and validate CSVs
            target_df = pd.read_csv(target_csv)
            if not {"pbt-name", "pbt-definition"}.issubset(target_df.columns):
                raise ValueError("target.csv missing required columns")
            
            source_df = pd.read_csv(source_csv)
            if not {"name", "definition"}.issubset(source_df.columns):
                raise ValueError("source.csv missing required columns")

            # Clean the dataframes
            for df in [target_df, source_df]:
                for col in df.columns:
                    if df[col].dtype == object:
                        df[col] = df[col].fillna("").astype(str).str.strip()

            # Initialize LanceDB
            indexer = EnhancedLanceDBIndex(env)
            
            # Index target data with progress tracking
            logger.info("Starting target data indexing...")
            indexer.index_target_data(target_df)
            
            # Get matching parameters
            top_k = int(env.get("TOP_K", "5"))
            similarity_threshold = float(env.get("SIMILARITY_THRESHOLD", "0.75"))
            
            # Process source data and find matches
            matches = []
            errors = []
            
            logger.info("Starting similarity matching...")
            with tqdm(total=len(source_df), desc="Processing") as pbar:
                for idx, row in source_df.iterrows():
                    try:
                        name_val = row["name"]
                        def_val = row["definition"]
                        text = f"{name_val} : {def_val}"
                        
                        # Get similar items
                        results = indexer.search_similar(text, top_k=top_k)
                        
                        # Filter by threshold
                        filtered_results = [
                            r for r in results 
                            if r["similarity_score"] >= similarity_threshold
                        ]
                        
                        # Add matches
                        for r in filtered_results:
                            matches.append({
                                "source_name": name_val,
                                "source_definition": def_val,
                                "pbt_name": r["pbt-name"],
                                "pbt_definition": r["pbt-definition"],
                                "similarity_score": r["similarity_score"]
                            })
                            
                    except Exception as e:
                        logger.error(f"Error processing row {idx}: {str(e)}")
                        errors.append({
                            "row_index": idx,
                            "source_name": name_val,
                            "error": str(e)
                        })
                    
                    pbar.update(1)
            
            # Generate output files
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            # Save matches
            matches_df = pd.DataFrame(matches)
            matches_csv = output_dir / f"matches_{timestamp}.csv"
            matches_json = output_dir / f"matches_{timestamp}.json"
            
            # Sort by similarity score
            matches_df = matches_df.sort_values(
                by=["source_name", "similarity_score"], 
                ascending=[True, False]
            )
            
            # Save as CSV
            matches_df.to_csv(matches_csv, index=False)
            
            # Save as JSON with formatting
            with open(matches_json, 'w', encoding='utf-8') as f:
                json.dump(
                    matches_df.to_dict(orient='records'),
                    f,
                    indent=2,
                    ensure_ascii=False
                )
            
            # Generate summary report
            summary = {
                "timestamp": timestamp,
                "total_source_rows": len(source_df),
                "total_target_rows": len(target_df),
                "total_matches_found": len(matches),
                "unique_source_items_matched": len(matches_df["source_name"].unique()),
                "similarity_threshold": similarity_threshold,
                "top_k": top_k,
                "errors": len(errors),
                "output_files": {
                    "csv": str(matches_csv),
                    "json": str(matches_json)
                }
            }
            
            # Save summary
            summary_file = output_dir / f"summary_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2)
            
            # Log completion
            logger.info("\nProcessing completed!")
            logger.info(f"Total matches found: {len(matches)}")
            logger.info(f"Unique source items matched: {len(matches_df['source_name'].unique())}")
            logger.info(f"Errors encountered: {len(errors)}")
            logger.info(f"\nOutput files:")
            logger.info(f"- Matches CSV: {matches_csv}")
            logger.info(f"- Matches JSON: {matches_json}")
            logger.info(f"- Summary: {summary_file}")
            
            # Log errors if any
            if errors:
                logger.warning("\nErrors encountered during processing:")
                for err in errors:
                    logger.warning(f"Row {err['row_index']} ({err['source_name']}): {err['error']}")
            
    except Exception as e:
        if logger:
            logger.exception("Fatal error occurred")
        else:
            print(f"Fatal error occurred before logger initialization: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
