import os
import sys
import json
import logging
import re
import asyncio
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List

# (Assume all your previously defined functions and classes are here,
# including clean_text, AzureChatbot, etc.)

# Synchronous processing function for one data item.
def process_item_sync(item: Dict[str, Any], bot: AzureChatbot) -> Dict[str, Any]:
    try:
        # Convert input fields to strings and clean them.
        cleaned_name = clean_text(str(item.get("name", "")))
        cleaned_definition = clean_text(str(item.get("definition", "")))
        cleaned_item = {"name": cleaned_name, "definition": cleaned_definition}

        # Call your chat completion API via the classification workflow.
        # This call is slow, so we want to run many of these concurrently.
        classification_result = bot.classify_data(cleaned_item)
        return {"input": cleaned_item, "result": classification_result}
    except Exception as e:
        logging.error(f"Error processing item {item}: {e}")
        return {"input": item, "error": str(e)}

# Asynchronous wrapper for the chat completion API call.
async def process_item_async(item: Dict[str, Any], bot: AzureChatbot, semaphore: asyncio.Semaphore, executor: ThreadPoolExecutor) -> Dict[str, Any]:
    async with semaphore:
        # Run the synchronous processing in the executor.
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(executor, process_item_sync, item, bot)
        return result

async def main_async():
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    logger = logging.getLogger(__name__)
    input_filename = "input_data.json"
    
    # Load the input JSON file (supporting either a single dict or a list).
    try:
        with open(input_filename, "r") as infile:
            input_data = json.load(infile)
        if isinstance(input_data, dict):
            input_data = [input_data]
        elif isinstance(input_data, list):
            for item in input_data:
                if "name" not in item or "definition" not in item:
                    raise ValueError("Each object in the JSON list must contain 'name' and 'definition' keys.")
        else:
            raise ValueError("Input JSON must be an object or a list of objects.")
    except Exception as e:
        logger.error(f"Error reading input file: {e}")
        sys.exit(1)
    
    # Initialize your chatbot (which internally sets up the classification workflow).
    bot = AzureChatbot(CONFIG_PATH, CREDS_PATH, CERT_PATH)
    
    # Define a semaphore to limit the number of concurrent chat completion calls.
    # Adjust max_concurrent to an appropriate value (e.g. 20) based on your rate limits.
    max_concurrent = 20
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # Use a ThreadPoolExecutor for running the blocking chat completion calls.
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        # Create asynchronous tasks for each data item.
        tasks = [process_item_async(item, bot, semaphore, executor) for item in input_data]
        results = await asyncio.gather(*tasks)
    
    # Save results to JSON.
    output_json_file = "isr_results.json"
    try:
        with open(output_json_file, "w") as outfile:
            json.dump(results, outfile, indent=2)
        logger.info(f"Classification results saved to {output_json_file}")
    except Exception as e:
        logger.error(f"Error saving JSON result: {e}")
    
    # Convert results to CSV using pandas.
    try:
        df = pd.json_normalize(results)
        output_csv_file = "isr_results.csv"
        df.to_csv(output_csv_file, index=False)
        logger.info(f"Classification results converted and saved to {output_csv_file}")
    except Exception as e:
        logger.error(f"Error converting result to CSV: {e}")

if __name__ == "__main__":
    asyncio.run(main_async())
