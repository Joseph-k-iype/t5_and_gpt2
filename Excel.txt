import os
import time
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
from langchain.schema import Document
from openai import AzureOpenAI

# Ensure OSEnv is defined & imported
# from ose_env import OSEnv  # Uncomment if OSEnv is in another file

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

class SemanticMatcher:
    """Handles semantic matching using LangChain, LanceDB, and Azure OpenAI."""

    def __init__(self, env: OSEnv):
        self.env = env
        self._setup_openai_client()
        self._setup_embedding_model()
        self._setup_vector_db()

    def _setup_openai_client(self):
        """Initialize Azure OpenAI Client."""
        self.client = AzureOpenAI(
            api_key=self.env.token,
            api_version=self.env.get("API_VERSION", "2024-02-01"),
            azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT")
        )

    def _setup_embedding_model(self):
        """Initialize LangChain's OpenAI Embeddings Model."""
        self.embeddings_model = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=self.env.token,
            openai_api_base=self.env.get("AZURE_OPENAI_ENDPOINT")
        )

    def _setup_vector_db(self):
        """Initialize LanceDB for efficient vector storage and retrieval."""
        self.lance_db = LanceDB("./lance_vectors")  # Path to store vectors

    def _prepare_text(self, name: str, description: str) -> str:
        """Format input text for embedding using prompt engineering."""
        prompt_template = (
            "You are given a financial term and its description. "
            "Match it with the most semantically similar Preferred Business Term (PBT).\n\n"
            "Financial Term: {name}\n"
            "Description: {description}\n"
            "Find the best matching PBT."
        )
        return prompt_template.format(name=name.strip(), description=description.strip())

    def load_csv_data(self, source_csv: str, target_csv: str) -> tuple:
        """Load and validate CSV files."""
        src_df, tgt_df = pd.read_csv(source_csv), pd.read_csv(target_csv)

        if not {"name", "definition"}.issubset(src_df.columns):
            raise ValueError("Source CSV must have 'name' and 'definition' columns")
        if not {"pbt-name", "pbt-definition"}.issubset(tgt_df.columns):
            raise ValueError("Target CSV must have 'pbt-name' and 'pbt-definition' columns")

        for df in [src_df, tgt_df]:
            df.fillna("", inplace=True)
            df = df.astype(str).applymap(str.strip)

        logger.info(f"Loaded {len(src_df)} source records and {len(tgt_df)} target records.")
        return src_df, tgt_df

    def index_target_data(self, tgt_df):
        """Embed target (PBT) definitions and store them in LanceDB."""
        docs = [
            Document(
                page_content=self._prepare_text(row["pbt-name"], row["pbt-definition"]),
                metadata={"pbt-name": row["pbt-name"], "pbt-definition": row["pbt-definition"]}
            )
            for _, row in tgt_df.iterrows()
        ]

        logger.info("Generating embeddings for target PBT data...")
        self.lance_db.from_documents(docs, self.embeddings_model)
        logger.info("Target data indexed successfully in LanceDB.")

    def process_matches(self, src_df, similarity_threshold=0.75):
        """Find semantic matches using LanceDB vector search."""
        matches = []
        
        for _, row in tqdm(src_df.iterrows(), total=len(src_df), desc="Matching terms"):
            query_text = self._prepare_text(row["name"], row["definition"])
            query_embedding = self.embeddings_model.embed_documents([query_text])

            # Search for the best matching PBT
            results = self.lance_db.similarity_search(query_embedding, k=3)

            for res in results:
                if res.score >= similarity_threshold:
                    matches.append({
                        "source_name": row["name"],
                        "source_definition": row["definition"],
                        "matched_pbt_name": res.metadata["pbt-name"],
                        "matched_pbt_definition": res.metadata["pbt-definition"],
                        "similarity_score": float(res.score),
                    })

        logger.info(f"Found {len(matches)} matches above threshold.")
        return matches

    def save_results(self, matches, output_file):
        """Save matched results to CSV and JSON files."""
        results_df = pd.DataFrame(matches)
        results_df.to_csv(output_file, index=False)

        json_file = output_file.replace(".csv", ".json")
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(matches, f, indent=2, ensure_ascii=False)

        logger.info(f"Results saved to {output_file} and {json_file}")

def main():
    """Run the semantic matching process."""
    try:
        base_dir = Path(__file__).parent
        env_dir = base_dir / "env"
        data_dir = base_dir / "data"
        output_dir = base_dir / "output"
        log_dir = base_dir / "logs"

        for directory in [data_dir, output_dir, log_dir]:
            directory.mkdir(exist_ok=True)

        config_file = env_dir / "config.env"
        creds_file = env_dir / "credentials.env"
        cert_file = env_dir / "cacert.pem"
        source_csv = data_dir / "source.csv"
        target_csv = data_dir / "target.csv"

        env_setup = OSEnv(str(config_file), str(creds_file), str(cert_file))
        matcher = SemanticMatcher(env_setup)

        # Load CSVs
        source_df, target_df = matcher.load_csv_data(str(source_csv), str(target_csv))

        # Index PBT Data
        matcher.index_target_data(target_df)

        # Process Matches
        matches = matcher.process_matches(source_df, similarity_threshold=0.75)

        # Save Results
        output_file = output_dir / f"matches_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        matcher.save_results(matches, str(output_file))

        print(f"\nProcess completed successfully! Results saved to {output_file}")

    except FileNotFoundError as e:
        logger.error(f"File Error: {str(e)}")
    except ValueError as e:
        logger.error(f"Validation Error: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected Error: {str(e)}")
        raise

if __name__ == "__main__":
    main()
