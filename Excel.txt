import logging
import uuid
from typing import Dict, Any

from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.vectorstores import Chroma
from chromadb.config import Settings

logger = logging.getLogger(__name__)

class AzureChatbot:
    """
    Full AzureChatbot class that:
      1) Loads environment via OSEnv
      2) Creates an AzureChatOpenAI LLM for normal conversation
      3) Builds a KnowledgeBase from CSV => docs + graph
      4) Creates a Chroma vector store for RAG
      5) Provides normal chat, rag_query, graph_search
      6) Integrates ISRClassificationChain to classify name + definition
         => {classification, reason, confidence, rating}
    """

    def __init__(self, config_file: str, creds_file: str, cert_file: str, csv_path: str,
                 env_cls, kb_cls, emb_cls, isr_chain_cls):
        """
        :param config_file: e.g. "config.env"
        :param creds_file: e.g. "credentials.env"
        :param cert_file: e.g. "cacert.pem"
        :param csv_path: e.g. "knowledgebase.csv"
        :param env_cls: reference to your OSEnv class
        :param kb_cls: reference to your KnowledgeBase class
        :param emb_cls: reference to your AzureOpenAIEmbeddings class
        :param isr_chain_cls: reference to your ISRClassificationChain class
        """
        logger.info("Initializing AzureChatbot with ISR classification capabilities.")
        # 1) Load environment
        self.env = env_cls(config_file, creds_file, cert_file)

        # 2) Create LLM for normal conversation
        self._setup_llm()

        # 3) Conversation memory
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)

        # 4) Build knowledge base => docs + graph
        self.kb = kb_cls(csv_path)
        self.graph = self.kb.graph

        # 5) Create vector store (Chroma) using your AzureOpenAIEmbeddings
        self._setup_vectorstore(emb_cls)

        # 6) Create an ISRClassificationChain instance
        self.isr_chain = isr_chain_cls(llm=self.llm)

        logger.info("AzureChatbot initialization complete.")

    def _setup_llm(self):
        """Create an AzureChatOpenAI model from environment variables."""
        from langchain.chat_models import AzureChatOpenAI

        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))
        api_version = self.env.get("API_VERSION", "2023-05-15")
        azure_endpoint = self.env.get("AZURE_OPENAI_ENDPOINT", "")
        azure_ad_token = self.env.token

        try:
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                openai_api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token=azure_ad_token
            )
            logger.info("LLM (AzureChatOpenAI) initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {str(e)}")
            raise

    def _setup_vectorstore(self, emb_cls):
        """Create a Chroma vector store with AzureOpenAIEmbeddings (cosine by default)."""
        from chromadb.config import Settings

        azure_embeddings_model = self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
        azure_api_version = self.env.get("EMBEDDINGS_API_VERSION", "2023-05-15")

        embedding = emb_cls(
            azure_api_version=azure_api_version,
            embeddings_model=azure_embeddings_model
        )
        # Cosine similarity by default
        chroma_settings = Settings(
            anonymized_telemetry=False,
            persist_directory="chromadb-data"
            # distance_metric="cosine" if you want to be explicit
        )
        try:
            self.vs = Chroma.from_documents(
                documents=self.kb.docs,
                embedding=embedding,
                collection_name="kb_collection",
                client_settings=chroma_settings
            )
            logger.info("Chroma vector store created successfully (cosine similarity).")
        except Exception as e:
            logger.error(f"Failed to set up vector store: {str(e)}")
            raise

    def list_env(self):
        """List environment variables from OSEnv."""
        self.env.list_env_vars()

    ###########################################################################
    # Normal Chat
    ###########################################################################
    def chat(self, message: str) -> str:
        """Basic conversation with memory."""
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            return self.conversation.predict(input=message)
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

    ###########################################################################
    # RAG query => direct vector search
    ###########################################################################
    def rag_query(self, query: str, k: int = 3) -> str:
        """Perform a direct vector store search, returning matches with confidence + rating."""
        try:
            results = self.vs.similarity_search_with_score(query, k=k)
            if not results:
                return "No relevant matches found."

            lines = []
            for idx, (doc, score) in enumerate(results, start=1):
                confidence = max(0.0, min(1.0, 1.0 - score))
                if confidence >= 0.8:
                    rating = "Green"
                elif confidence >= 0.5:
                    rating = "Amber"
                else:
                    rating = "Red"
                reason = f"Confidence={confidence:.2f}, rating={rating}, definition={doc.page_content}"
                lines.append(
                    f"Match #{idx}\n"
                    f"Name: {doc.metadata.get('name','Unknown')}\n"
                    f"ID: {doc.metadata.get('id','No ID')}\n"
                    f"Confidence: {confidence:.2f}\n"
                    f"Rating: {rating}\n"
                    f"Reason: {reason}\n"
                )
            return "\n".join(lines)
        except Exception as e:
            logger.error(f"Error in rag_query: {str(e)}")
            return f"An error occurred: {str(e)}"

    ###########################################################################
    # Graph search => naive substring
    ###########################################################################
    def graph_search(self, query: str) -> str:
        """Naive substring match in the knowledge base's networkx graph."""
        results = []
        q_lower = query.lower()
        for node, data in self.kb.graph.nodes(data=True):
            if q_lower in str(node).lower():
                node_info = f"Node: {node}, node_type={data.get('node_type','')}, node_id={data.get('node_id','')}"
                results.append(node_info)

        if not results:
            return "No matching nodes found in the graph."
        return "\n".join(results)

    ###########################################################################
    # ISR Classification
    ###########################################################################
    def classify_isr_item(self, name: str, definition: str) -> Dict[str, Any]:
        """
        Calls ISRClassificationChain on the given name + definition.
        Returns a dict with:
          {
            "classification": "...",
            "reason": "...",
            "confidence": float,
            "rating": "Green"/"Amber"/"Red"
          }
        """
        user_input = f"Name: {name}\nDefinition: {definition}"
        return self.isr_chain.classify_isr_data(user_input)
