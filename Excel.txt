import os
import csv
import json

# LangChain + Community imports
from langchain.docstore.document import Document
from langchain.chat_models import AzureChatOpenAI
from langchain.vectorstores import Chroma
from langchain.agents import Tool, ZeroShotAgent, AgentExecutor

# Graph integration (without directly importing networkx)
from langchain_community.graphs.index_creator import GraphIndexCreator
from langchain_community.chains.graph_qa.base import GraphQAChain

###############################################################################
# 1) OSEnv Class
###############################################################################
class OSEnv:
    """
    Example environment setup class.
    Adjust as needed to properly set your Azure AD token,
    OpenAI endpoint, etc.
    """
    def __init__(self, env_path: str, creds_path: str, cacert_path: str):
        self.env_path = env_path
        self.creds_path = creds_path
        self.cacert_path = cacert_path

    def set_azure_token(self):
        """
        Dummy method that sets environment variables
        for Azure AD-based authentication.
        Replace with your real logic.
        """
        # Example environment variables (replace with real ones)
        os.environ["AZURE_AD_TOKEN"] = "YOUR_AZURE_AD_TOKEN"
        os.environ["AZURE_OPENAI_ENDPOINT"] = "YOUR_AZURE_OPENAI_ENDPOINT"
        os.environ["AZURE_OPENAI_API_VERSION"] = "2023-05-15"
        os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"] = "YOUR_DEPLOYMENT_NAME"


def test_OSEnv():
    """Quick test for OSEnv class."""
    env = OSEnv("config/dev", "config/dev.creds", "cacert.pem")
    env.set_azure_token()
    assert "AZURE_AD_TOKEN" in os.environ, "AZURE_AD_TOKEN should be set"
    assert "AZURE_OPENAI_ENDPOINT" in os.environ, "AZURE_OPENAI_ENDPOINT should be set"
    print("test_OSEnv passed!")


###############################################################################
# 2) AzoEmbedding Class
###############################################################################
class AzoEmbedding:
    """
    Example custom embedding class that integrates with LangChain.
    Replace with your real embedding code from `azoembedding.py`.
    """
    def embed_documents(self, texts):
        """
        Return a list of embedding vectors, one per text.
        Dummy example: each text -> a 768-dim zero vector.
        """
        return [[0.0]*768 for _ in texts]

    def embed_query(self, text):
        """
        Return a single embedding vector for a query.
        Dummy example: a single 768-dim zero vector.
        """
        return [0.0]*768


def test_AzoEmbedding():
    """Quick test for AzoEmbedding class."""
    emb = AzoEmbedding()
    doc_vectors = emb.embed_documents(["Hello world", "Test doc"])
    query_vector = emb.embed_query("Some query")

    assert len(doc_vectors) == 2, "Should return two vectors for two texts"
    assert len(doc_vectors[0]) == 768, "Each doc vector should be 768-dim"
    assert len(query_vector) == 768, "Query vector should be 768-dim"
    print("test_AzoEmbedding passed!")


###############################################################################
# 3) KnowledgeBase Class
###############################################################################
class KnowledgeBase:
    """
    Loads CSV rows as Documents.
    Expects 'name' and 'definition' columns in the CSV.
    """
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.docs = self._load_csv_as_documents()

    def _load_csv_as_documents(self):
        docs = []
        with open(self.csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Combine 'name' + 'definition' into a single text block
                text = f"{row['name']}: {row['definition']}"
                metadata = {"name": row["name"]}
                docs.append(Document(page_content=text, metadata=metadata))
        return docs


def test_KnowledgeBase():
    """Quick test for KnowledgeBase class with a tiny CSV."""
    test_csv = "test_kb.csv"
    # Write a tiny CSV
    with open(test_csv, "w", encoding="utf-8") as f:
        f.write("name,definition\n")
        f.write("TestName,This is a test definition.\n")

    kb = KnowledgeBase(test_csv)
    assert len(kb.docs) == 1, "KnowledgeBase should load exactly 1 doc from test CSV"
    os.remove(test_csv)

    print("test_KnowledgeBase passed!")


###############################################################################
# 4) SemanticSearchAgent Class
###############################################################################
class SemanticSearchAgent:
    """
    - Creates a Graph index from CSV Documents (via GraphIndexCreator).
    - Creates a Chroma vector store for semantic search.
    - Defines a multi-tool agent:
        1) GraphQATool -> answers questions from the knowledge graph
        2) VectorStoreSearch -> returns top matches from the vector store
           with confidence, rating (R/A/G), and reason.
    """
    def __init__(self, kb: KnowledgeBase, env: OSEnv):
        self.kb = kb
        self.env = env

        # 1) Set up environment variables for Azure AD token, endpoints, etc.
        self.env.set_azure_token()

        # 2) Create the custom embedding
        self.embedding = AzoEmbedding()

        # 3) Build a Chroma vector store from the documents
        #    Telemetry disabled with "anonymized_telemetry=False"
        self.vs = Chroma.from_documents(
            documents=self.kb.docs,
            embedding=self.embedding,
            collection_name="kb_collection",
            client_settings={"anonymized_telemetry": False},
        )

        # 4) Build a graph index from the same documents
        self.graph = GraphIndexCreator().create_index(self.kb.docs)

        # 5) Create an LLM that uses Azure OpenAI with a Bearer token
        self.llm = AzureChatOpenAI(
            openai_api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
            openai_api_key="",  # Not used because we rely on Azure AD token
            headers={"Authorization": f"Bearer {os.getenv('AZURE_AD_TOKEN')}"}
        )

        # 6) Create a Graph QA chain for knowledge-graph queries
        self.graph_qa_chain = GraphQAChain.from_llm(self.llm, graph=self.graph)

        # 7) Define Tools for the agent
        self.tools = [
            Tool(
                name="GraphQATool",
                func=self._graph_qa,
                description=(
                    "Use this to query the knowledge graph for relationships, "
                    "hierarchies, or definitions found in the CSV-based knowledge."
                ),
            ),
            Tool(
                name="VectorStoreSearch",
                func=self._vectorstore_search,
                description=(
                    "Use this to find semantically similar definitions in the CSV-based knowledge. "
                    "It returns top matches with confidence, R/A/G rating, and a reason."
                ),
            ),
        ]

        # 8) Create an Agent that can call these Tools
        prefix = """You are an AI assistant with access to the following tools:"""
        suffix = """Begin!"""
        prompt = ZeroShotAgent.create_prompt(
            self.tools,
            prefix=prefix,
            suffix=suffix,
            input_variables=["input"]
        )

        agent = ZeroShotAgent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt,
            verbose=True
        )
        self.agent_executor = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=self.tools,
            verbose=True
        )

    def _graph_qa(self, query: str) -> str:
        """
        Use GraphQAChain to answer a question by traversing the knowledge graph.
        """
        return self.graph_qa_chain.run(query)

    def _vectorstore_search(self, query: str) -> str:
        """
        Search the Chroma vector store for the top 3 matches.
        Provide:
         - Confidence (0â€“1, from cosine similarity or 1 - distance)
         - Red/Amber/Green rating
         - Reason for the rating
        """
        results = self.vs.similarity_search_with_score(query, k=3)
        if not results:
            return "No relevant matches found."

        lines = []
        for idx, (doc, score) in enumerate(results, start=1):
            # If 'score' is a distance, confidence = 1 - distance
            confidence = max(0.0, min(1.0, 1.0 - score))

            # Determine R/A/G rating
            if confidence >= 0.8:
                rating = "Green"
            elif confidence >= 0.5:
                rating = "Amber"
            else:
                rating = "Red"

            # Basic reason text
            reason = (
                f"Confidence is {confidence:.2f}, which falls into the {rating} range. "
                f"Definition matched: {doc.page_content}"
            )

            line = (
                f"Match #{idx}\n"
                f"Name: {doc.metadata.get('name', 'Unknown')}\n"
                f"Confidence: {confidence:.2f}\n"
                f"Rating: {rating}\n"
                f"Reason: {reason}\n"
            )
            lines.append(line)

        return "\n".join(lines)

    def run_query(self, query: str) -> str:
        """
        Send the query to the multi-tool agent. The agent decides which tool(s) to call.
        """
        return self.agent_executor.run(query)


def test_SemanticSearchAgent():
    """
    Quick test to ensure we can instantiate the agent and call its tools
    without error. We'll use a tiny CSV and environment.
    """
    # Create a tiny CSV
    test_csv = "test_agent.csv"
    with open(test_csv, "w", encoding="utf-8") as f:
        f.write("name,definition\n")
        f.write("TestEntity,This is a definition for the test entity.\n")

    # Create minimal OSEnv + KnowledgeBase
    env = OSEnv("config/dev", "config/dev.creds", "cacert.pem")
    kb = KnowledgeBase(test_csv)

    # Instantiate the agent
    agent = SemanticSearchAgent(kb, env)

    # Test the internal vector store search
    vs_result = agent._vectorstore_search("test")
    assert vs_result is not None, "Vector store search should return a string"

    # Test the graph QA
    graph_result = agent._graph_qa("What is TestEntity?")
    assert graph_result is not None, "Graph QA should return a string"

    os.remove(test_csv)
    print("test_SemanticSearchAgent passed!")


###############################################################################
# 5) Main Entry Point
###############################################################################
def main():
    """
    Main flow:
      - Instantiate environment
      - Load knowledgebase.csv
      - Initialize agent
      - Read input.json with {name, definition}
      - Query agent
    """
    env = OSEnv("config/dev", "config/dev.creds", "cacert.pem")
    kb = KnowledgeBase("knowledgebase.csv")
    agent = SemanticSearchAgent(kb, env)

    # Load input.json
    with open("input.json", "r", encoding="utf-8") as jf:
        data = json.load(jf)

    # Construct a query from the JSON
    query = f"{data['name']} {data['definition']}"

    # Run the query
    result = agent.run_query(query)

    print("\n=== Final Agent Output ===")
    print(result)


###############################################################################
# Run tests and then main()
###############################################################################
if __name__ == "__main__":
    # Run quick tests
    test_OSEnv()
    test_AzoEmbedding()
    test_KnowledgeBase()
    test_SemanticSearchAgent()

    # Run main flow
    main()
