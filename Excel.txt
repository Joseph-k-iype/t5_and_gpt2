import os
import re
import uuid
import json
import logging
import pandas as pd
import chardet
from pathlib import Path
from typing import Optional, List, Dict, Any

from dotenv import load_dotenv, dotenv_values
import requests
import chromadb
from chromadb.config import Settings

from azure.identity import ClientSecretCredential, get_bearer_token_provider
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

import openai

# ------------------------------
# Logging Configuration
# ------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ------------------------------
# Helper Functions
# ------------------------------
def is_file_readable(filepath: str) -> bool:
    """Check if a file exists and is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str) -> bool:
    """Convert string to boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean string: {s}")

def stable_chunk_text(text: str, max_chars: int = 1000) -> List[str]:
    """Chunk text into smaller pieces at sentence boundaries."""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chars:
            current_chunk = (current_chunk + " " + sentence).strip() if current_chunk else sentence
        else:
            chunks.append(current_chunk)
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def remove_invisible_chars(s: str) -> str:
    """
    Remove invisible directional characters like \u202a, \u202b, \u202c, \u202d, \u202e,
    which can appear when copying Windows file paths from certain sources.
    """
    return re.sub(r'[\u202a\u202b\u202c\u202d\u202e]', '', s).strip()

def guess_file_encoding(file_path: str, num_bytes: int = 4096) -> Optional[str]:
    """
    Use chardet to guess the encoding of the file by reading a chunk of bytes.
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read(num_bytes)
    guess = chardet.detect(raw_data)
    encoding = guess.get('encoding', None)
    if encoding:
        logger.info(f"chardet guessed encoding='{encoding}' for {file_path}")
    else:
        logger.warning(f"chardet could not guess encoding for {file_path}")
    return encoding

def read_csv_flexible(csv_path: str) -> pd.DataFrame:
    """
    Attempt reading a CSV with chardet guess first, then fallback to a list of known encodings.
    We'll skip bad lines and replace errors to handle weird characters.
    """
    enc = guess_file_encoding(csv_path)
    if enc:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            return df
        except Exception as e:
            logger.warning(f"Reading with guessed encoding '{enc}' failed: {e}")

    fallback_encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']
    for enc in fallback_encodings:
        try:
            df = pd.read_csv(
                csv_path,
                encoding=enc,
                engine='python',
                on_bad_lines='skip',
                errors='replace'
            )
            logger.info(f"Successfully read CSV with fallback encoding='{enc}'")
            return df
        except Exception as e:
            logger.warning(f"Failed reading with encoding={enc}: {e}")

    raise ValueError("Unable to read CSV in any known encoding or chardet guess.")

# ------------------------------
# OSEnv Class: Environment, Proxy, Certificate, and Token Management
# ------------------------------
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        logger.info(f"Loaded main configuration from {config_file}")
        self.bulk_set(creds_file, False)
        logger.info(f"Loaded credentials from {creds_file}")
        self.set_certificate_path(certificate_path)
        logger.info("Certificate path configured")
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
            logger.info("Proxy configured")
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            logger.info("Securing endpoints using Azure AD")
            self.token = self.get_azure_token()
        else:
            self.token = None

    def set_certificate_path(self, certificate_path: str) -> None:
        if not os.path.isabs(certificate_path):
            certificate_path = os.path.abspath(certificate_path)
        if not is_file_readable(certificate_path):
            raise Exception("Certificate file missing or not readable")
        self.set("REQUESTS_CA_BUNDLE", certificate_path)
        self.set("SSL_CERT_FILE", certificate_path)
        self.set("CURL_CA_BUNDLE", certificate_path)
        logger.info(f"Certificate path set to: {certificate_path}")

    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        if not os.path.isabs(dotenvfile):
            dotenvfile = os.path.abspath(dotenvfile)
        if is_file_readable(dotenvfile):
            logger.info(f"Loading environment variables from {dotenvfile}")
            temp_dict = dotenv_values(dotenvfile)
            for k, v in temp_dict.items():
                self.set(k, v, print_val)
            del temp_dict

    def set(self, var_name: str, val: str, print_val: bool = True) -> None:
        os.environ[var_name] = val
        if var_name not in self.var_list:
            self.var_list.append(var_name)
        if print_val:
            logger.info(f"Set {var_name}={val}")

    def get(self, var_name: str, default: Optional[str] = None) -> Optional[str]:
        return os.environ.get(var_name, default)

    def set_proxy(self) -> None:
        ad_username = self.get("AD_USERNAME")
        ad_password = self.get("AD_USER_PW")
        proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
        if not all([ad_username, ad_password, proxy_domain]):
            raise ValueError("Missing proxy credentials")
        proxy_url = f"http://{ad_username}:{ad_password}@{proxy_domain}"
        self.set("HTTP_PROXY", proxy_url, print_val=False)
        self.set("HTTPS_PROXY", proxy_url, print_val=False)
        no_proxy_domains = [
            'cognitiveservices.azure.com',
            'search.windows.net',
            'openai.azure.com',
            'core.windows.net',
            'azurewebsites.net'
        ]
        self.set("NO_PROXY", ",".join(no_proxy_domains))
        logger.info("Proxy configuration completed")

    def get_azure_token(self) -> str:
        tenant_id = self.get("AZURE_TENANT_ID")
        client_id = self.get("AZURE_CLIENT_ID")
        client_secret = self.get("AZURE_CLIENT_SECRET")
        credential = ClientSecretCredential(tenant_id, client_id, client_secret)
        token = credential.get_token("https://cognitiveservices.azure.com/.default")
        self.set("AZURE_TOKEN", token.token, print_val=False)
        logger.info("Azure token acquired successfully")
        return token.token

    def list_env_vars(self) -> None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [HIDDEN]")
            else:
                logger.info(f"{var}: {self.get(var)}")

# ------------------------------
# AzureChatbot: For validation using Azure OpenAI (LangChain)
# ------------------------------
class AzureChatbot:
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        logger.info("Initializing chatbot...")
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()

    def _setup_chat_model(self) -> None:
        api_key = self.env.get("AZURE_OPENAI_API_KEY")
        endpoint = self.env.get("AZURE_OPENAI_ENDPOINT")
        api_version = self.env.get("API_VERSION", "2024-02-01")
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("MODEL_TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))

        if api_key:
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                openai_api_version=api_version,
                azure_endpoint=endpoint,
                openai_api_key=api_key
            )
        else:
            if not self.env.token:
                raise ValueError("Missing credentials: provide AZURE_OPENAI_API_KEY or set SECURED_ENDPOINTS to True.")
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                openai_api_version=api_version,
                azure_endpoint=endpoint,
                azure_ad_token=self.env.token
            )
        
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        logger.info("Chat model initialized successfully")

    def validate_matches(self, query: str, matches: List[Dict[str, Any]]) -> str:
        from langchain.chains import ConversationChain
        prompt = f"""You are an expert validation agent. Given the query:
"{query}"
and the following candidate matches:
"""
        for idx, match in enumerate(matches, start=1):
            prompt += f"\n{idx}. Name: {match.get('name')}\n   Definition: {match.get('definition')}\n"
        prompt += "\nIf these matches are satisfactory, simply reply OK. Otherwise, suggest the best matching candidate from our knowledge base in the format:\nName: <name>\nDefinition: <definition>\nExplanation: <brief explanation>."
        try:
            response = self.conversation.predict(input=prompt)
            return response.strip()
        except Exception as e:
            logger.error(f"Validation agent error: {str(e)}")
            return "Validation failed"

    def chat(self, message: str) -> str:
        if not message.strip():
            return "Please provide a non-empty message."
        try:
            response = self.conversation.predict(input=message)
            return response
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return f"An error occurred: {str(e)}"

# ------------------------------
# AzureEmbeddingClient: Using openai.Embedding with new "model" param
# ------------------------------
class AzureEmbeddingClient:
    """
    A client for generating embeddings via Azure OpenAI, using openai.Embedding.create
    with 'model=...' instead of 'engine=...' for openai>=1.0.0
    """
    def __init__(self, env: OSEnv):
        self.env = env
        self.api_version = self.env.get("EMBEDDINGS_API_VERSION", "2023-05-15")
        self.azure_endpoint = self.env.get("AZURE_OPENAI_EMBEDDINGS_ENDPOINT", "")
        if self.azure_endpoint.endswith("/"):
            self.azure_endpoint = self.azure_endpoint[:-1]
        if not self.azure_endpoint:
            raise ValueError("Missing AZURE_OPENAI_EMBEDDINGS_ENDPOINT in environment")

        self.deployment_name = self.env.get("EMBEDDINGS_DEPLOYMENT", "text-embedding-3-large")

        import requests
        self._session = requests.Session()
        self._session.trust_env = False
        ca_bundle = self.env.get("REQUESTS_CA_BUNDLE", "cacert.pem")
        self._session.verify = ca_bundle
        proxy_url = os.getenv("HTTPS_PROXY", "")
        if proxy_url:
            self._session.proxies = {"http": proxy_url, "https": proxy_url}
        
        import openai
        openai.requestssession = self._session

    def _configure_openai(self):
        import openai
        token = self.env.token
        api_key = self.env.get("AZURE_OPENAI_API_KEY")

        if api_key:
            openai.api_type = "azure"
            openai.api_key = api_key
        else:
            if not token:
                raise ValueError("Missing credentials for embeddings: no API key and no Azure AD token.")
            openai.api_type = "azure_ad"
            openai.api_key = token
        
        openai.api_version = self.api_version
        openai.api_base = self.azure_endpoint

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        import openai
        self._configure_openai()
        embeddings = []
        for text in texts:
            try:
                response = openai.Embedding.create(
                    input=text,
                    model=self.deployment_name  # Use 'model' param (NOT 'engine') in openai>=1.0.0
                )
                emb = response["data"][0]["embedding"]
                embeddings.append(emb)
            except Exception as e:
                logger.error(f"Error generating embedding for text '{text[:30]}...': {str(e)}")
                embeddings.append([])
        return embeddings

# ------------------------------
# VectorStoreManager: Manage local ChromaDB storage
# ------------------------------
class VectorStoreManager:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(name="knowledge_base")
        logger.info("ChromaDB collection initialized locally")

    def add_documents(self, documents: List[Dict[str, Any]], embeddings: List[List[float]]):
        if len(documents) != len(embeddings):
            raise ValueError("Documents and embeddings count mismatch")
        ids = [str(uuid.uuid4()) for _ in documents]
        metadatas = [{"name": doc["name"], "definition": doc["definition"]} for doc in documents]
        contents = [doc["name"] + "\n" + doc["definition"] for doc in documents]
        self.collection.add(ids=ids, documents=contents, embeddings=embeddings, metadatas=metadatas)
        logger.info(f"Added {len(documents)} documents to the vector store")

    def query(self, query_embedding: List[float], n_results: int = 4) -> List[Dict[str, Any]]:
        results = self.collection.query(query_embedding, n_results=n_results)
        matches = []
        if results and "metadatas" in results:
            for meta in results["metadatas"]:
                matches.append(meta)
        return matches

# ------------------------------
# CSV Processing
# ------------------------------
def load_csv_as_documents(csv_path: str) -> List[Dict[str, Any]]:
    df = read_csv_flexible(csv_path)
    if not {"name", "definition"}.issubset(set(df.columns)):
        raise ValueError("CSV must contain 'name' and 'definition' columns")
    documents = []
    for _, row in df.iterrows():
        combined = f"{row['name']}\n{row['definition']}"
        documents.append({"name": row["name"], "definition": row["definition"], "combined": combined})
    return documents

def save_results_to_csv(results: List[Dict[str, Any]], output_path: str) -> None:
    df = pd.DataFrame(results)
    df.to_csv(output_path, index=False)
    logger.info(f"Results saved to {output_path}")

# ------------------------------
# Main Function: Workflow
# ------------------------------
def main():
    try:
        # If we're in Jupyter or an environment lacking __file__, fallback to current working dir
        if '__file__' in globals():
            base_path = Path(__file__).resolve().parent
        else:
            base_path = Path.cwd()
        base_dir = base_path / "env"

        config_path = str(base_dir / "config.env")
        creds_path = str(base_dir / "credentials.env")
        cert_path = str(base_dir / "cacert.pem")
        for f in [config_path, creds_path, cert_path]:
            if not os.path.exists(f):
                print(f"Missing required file: {f}")
                return
        
        logger.info("Initializing environment...")
        env = OSEnv(config_path, creds_path, cert_path)
        
        logger.info("Initializing chatbot agent (for match validation)...")
        chatbot = AzureChatbot(config_path, creds_path, cert_path)
        
        logger.info("Initializing Azure embedding client (proxy aware) with openai>=1.0.0 usage...")
        embed_client = AzureEmbeddingClient(env)
        
        logger.info("Initializing local ChromaDB vector store...")
        vector_store = VectorStoreManager(persist_dir="./chroma_db")
        
        # Step 1: Knowledge Base Ingestion
        kb_csv = input("Enter path to knowledge base CSV: ").strip()
        kb_csv = remove_invisible_chars(kb_csv)
        kb_documents = load_csv_as_documents(kb_csv)
        combined_texts = [doc["combined"] for doc in kb_documents]
        logger.info("Generating embeddings for knowledge base documents...")
        kb_embeddings = embed_client.get_embeddings(combined_texts)
        vector_store.add_documents(kb_documents, kb_embeddings)
        
        # Step 2: Query CSV -> Vector matching
        query_csv = input("Enter path to query CSV: ").strip()
        query_csv = remove_invisible_chars(query_csv)
        query_df = read_csv_flexible(query_csv)
        if not {"name", "definition"}.issubset(set(query_df.columns)):
            raise ValueError("Query CSV must contain 'name' and 'definition' columns")
        
        results = []
        for index, row in query_df.iterrows():
            query_text = f"{row['name']}\n{row['definition']}"
            logger.info(f"Processing query {index+1}: {row['name']}")
            query_embedding = embed_client.get_embeddings([query_text])[0]
            matches = vector_store.query(query_embedding, n_results=4)
            
            # Validate/improve the match using the chatbot
            validation_response = chatbot.validate_matches(query_text, matches)
            
            results.append({
                "query_name": row["name"],
                "query_definition": row["definition"],
                "matches": json.dumps(matches),
                "validation": validation_response
            })
        
        # Step 3: Save results
        output_csv = input("Enter path for output CSV (e.g., output.csv): ").strip()
        output_csv = remove_invisible_chars(output_csv)
        save_results_to_csv(results, output_csv)
        print("Workflow completed successfully.")
        
    except Exception as e:
        logger.exception(f"Unexpected error: {str(e)}")
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()
