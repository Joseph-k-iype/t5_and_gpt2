#!/usr/bin/env python3
"""
Triple-Based RDF to FalkorDB Property Graph Converter

This module converts RDF data to FalkorDB property graphs using a single SPARQL query
that returns enriched triples with subject/object class information.

Query Format: SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass

Features:
- Single query approach with 6-variable triples
- Automatic node vs relationship detection based on object type
- Class-based node labeling
- Predicate-based relationship typing
- Streaming conversion with batching

Dependencies:
    pip install rdflib falkordb urllib3 requests
"""

import logging
import json
import time
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD
import falkordb

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TripleConfig:
    """Configuration for triple-based RDF conversion"""
    # The main SPARQL query with 6 variables
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Processing settings
    batch_size: int = 1000
    max_retries: int = 3
    timeout: int = 30
    
    # Transformation settings
    use_shortened_uris: bool = True
    preserve_uri_properties: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    # Filtering
    exclude_rdf_type_properties: bool = True
    literal_properties_only: Set[str] = None  # If set, only these predicates become properties
    
    # Monitoring
    export_stats: bool = True
    validate_conversion: bool = True

@dataclass
class ConversionStats:
    """Statistics tracking for conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0  # literal objects
    relationship_triples: int = 0  # URI objects
    
    # Created entities
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        # Convert sets to lists for JSON serialization
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        
        return result

class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
            'http://purl.obolibrary.org/obo/': 'obo'
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        # Check cache first
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        # Try namespace mapping
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        # Fallback: extract from URI structure
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        # Replace non-alphanumeric characters with underscores
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        # Remove leading/trailing underscores
        clean_name = clean_name.strip('_')
        # Ensure it starts with letter or underscore
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)

class NodeManager:
    """Manages node creation and properties"""
    
    def __init__(self, uri_processor: URIProcessor, config: TripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
    
    def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        if uri not in self.nodes:
            self._create_node(uri, class_uri)
    
    def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry"""
        labels = {self.config.default_node_label}
        
        # Add class-based label if available
        if class_uri and class_uri != str(RDF.Resource):
            class_label = self.uri_processor.process_uri(class_uri)
            labels.add(class_label)
        
        # Handle blank nodes
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'properties': {}
        }
        
        # Add URI properties if requested
        if self.config.preserve_uri_properties:
            node_data['properties']['uri'] = uri
        
        # Add processed identifier for easier querying
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        self.nodes[uri] = node_data
    
    def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        if subject_uri not in self.nodes:
            logger.warning(f"Node {subject_uri} not found when adding property")
            return
        
        # Skip rdf:type if configured to do so
        if (self.config.exclude_rdf_type_properties and 
            predicate_uri == str(RDF.type)):
            return
        
        prop_name = self.uri_processor.process_uri(predicate_uri)
        
        # Handle multiple values for the same property
        properties = self.nodes[subject_uri]['properties']
        if prop_name in properties:
            existing = properties[prop_name]
            if isinstance(existing, list):
                existing.append(literal_value)
            else:
                properties[prop_name] = [existing, literal_value]
        else:
            properties[prop_name] = literal_value
    
    def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        return self.nodes
    
    def clear(self):
        """Clear all nodes"""
        self.nodes.clear()

class FalkorDBManager:
    """Manages FalkorDB connection and operations"""
    
    def __init__(self, config: TripleConfig):
        self.config = config
        self.db = None
        self.graph = None
        self._connect()
    
    def _connect(self):
        """Establish connection to FalkorDB"""
        try:
            connect_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port
            }
            
            if self.config.falkordb_password:
                connect_kwargs['password'] = self.config.falkordb_password
            
            self.db = falkordb.FalkorDB(**connect_kwargs)
            self.graph = self.db.select_graph(self.config.graph_name)
            logger.info(f"Connected to FalkorDB at {self.config.falkordb_host}:{self.config.falkordb_port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def clear_graph(self):
        """Clear existing graph data"""
        try:
            self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    def create_indexes(self):
        """Create performance indexes"""
        if not self.config.create_indexes:
            return
        
        index_queries = [
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.uri)",
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.id)",
        ]
        
        for query in index_queries:
            try:
                self.graph.query(query)
                logger.debug(f"Created index: {query}")
            except Exception as e:
                logger.warning(f"Could not create index '{query}': {e}")
    
    def execute_query(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with error handling"""
        max_retries = self.config.max_retries
        
        for attempt in range(max_retries):
            try:
                return self.graph.query(query, params or {})
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"Query failed after {max_retries} attempts: {e}")
                    raise
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
                time.sleep(2 ** attempt)
    
    def create_node(self, uri: str, node_data: Dict[str, Any]) -> bool:
        """Create a node in FalkorDB"""
        try:
            # Prepare labels
            labels = [self.config.default_node_label if not label else label 
                     for label in node_data['labels']]
            labels_str = ':'.join(sorted(set(labels)))
            
            # Prepare properties
            properties = dict(node_data['properties'])
            
            # Build and execute query
            if properties:
                props_clause = ', '.join([f"{k}: ${k}" for k in properties.keys()])
                query = f"CREATE (n:{labels_str} {{{props_clause}}})"
                self.execute_query(query, properties)
            else:
                query = f"CREATE (n:{labels_str})"
                self.execute_query(query)
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating node {uri}: {e}")
            return False
    
    def create_relationship(self, subject_uri: str, predicate_uri: str, object_uri: str) -> bool:
        """Create a relationship in FalkorDB"""
        try:
            # Process predicate for relationship type
            from urllib.parse import urlparse
            parsed = urlparse(predicate_uri)
            if parsed.fragment:
                rel_type = parsed.fragment
            else:
                rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
            
            # Clean relationship type
            rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
            if not rel_type:
                rel_type = 'RELATED_TO'
            
            # Create relationship with optional properties
            rel_props = {}
            if self.config.preserve_uri_properties:
                rel_props['predicate_uri'] = predicate_uri
            
            if rel_props:
                props_clause = '{' + ', '.join([f"{k}: ${k}" for k in rel_props.keys()]) + '}'
                query = f"""
                MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
                CREATE (s)-[:{rel_type} {props_clause}]->(o)
                """
            else:
                query = f"""
                MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
                CREATE (s)-[:{rel_type}]->(o)
                """
            
            params = {
                'subject_uri': subject_uri,
                'object_uri': object_uri,
                **rel_props
            }
            
            self.execute_query(query, params)
            return True
            
        except Exception as e:
            logger.error(f"Error creating relationship {subject_uri} -> {object_uri}: {e}")
            return False
    
    def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}

class TripleBasedConverter:
    """Main converter using 6-variable triple query"""
    
    def __init__(self, config: TripleConfig):
        self.config = config
        self.stats = ConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = NodeManager(self.uri_processor, config)
        self.falkordb_manager = FalkorDBManager(config)
        self.rdf_graph = None
        
        # Processing state
        self.relationships_to_create: List[Tuple[str, str, str]] = []
        
        self._setup_rdf_connection()
    
    def _setup_rdf_connection(self):
        """Setup RDF graph connection to SPARQL endpoint"""
        try:
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"Connected to SPARQL endpoint: {self.config.sparql_endpoint}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def convert(self) -> ConversionStats:
        """Main conversion method"""
        try:
            logger.info("Starting triple-based RDF to FalkorDB conversion...")
            
            # Clear existing data
            self.falkordb_manager.clear_graph()
            
            # Execute the main query and process results
            self._execute_and_process_query()
            
            # Create nodes in FalkorDB
            self._create_nodes_in_falkordb()
            
            # Create relationships in FalkorDB
            self._create_relationships_in_falkordb()
            
            # Create indexes
            self.falkordb_manager.create_indexes()
            
            # Validate if requested
            if self.config.validate_conversion:
                self._validate_conversion()
            
            # Finalize statistics
            self._finalize_stats()
            
            logger.info("Conversion completed successfully!")
            return self.stats
            
        except Exception as e:
            logger.error(f"Conversion failed: {e}")
            raise
    
    def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("Executing triples query...")
            results = list(self.rdf_graph.query(self.config.triples_query))
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"Retrieved {len(results)} triples, processing in batches...")
            
            # Process in batches
            batch_size = self.config.batch_size
            for i in range(0, len(results), batch_size):
                batch = results[i:i+batch_size]
                self._process_triple_batch(batch)
                
                progress = (i + len(batch)) / len(results) * 100
                logger.info(f"Processing progress: {progress:.1f}%")
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
    
    def _process_triple_batch(self, batch: List[Tuple]):
        """Process a batch of 6-variable triples"""
        for triple in batch:
            try:
                if len(triple) >= 6:
                    subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                    self._process_single_triple(subject, subject_class, predicate, predicate_class, obj, object_class)
                    self.stats.processed_triples += 1
                else:
                    logger.warning(f"Invalid triple format (expected 6 variables): {triple}")
            except Exception as e:
                logger.warning(f"Error processing triple {triple}: {e}")
                self.stats.processing_errors += 1
    
    def _process_single_triple(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        # Track metadata
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        # Ensure subject node exists
        self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        # Determine if this creates a property or relationship
        if isinstance(obj, Literal):
            # Object is literal -> add as property
            literal_value = self._convert_literal_value(obj)
            self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            # Object is URI/BNode -> create relationship
            object_uri = str(obj)
            
            # Track object class
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            # Ensure object node exists
            self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            # Queue relationship for creation
            self.relationships_to_create.append((subject_uri, predicate_uri, object_uri))
            self.stats.relationship_triples += 1
        else:
            logger.warning(f"Unknown object type: {type(obj)} for {obj}")
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                elif literal.datatype in (XSD.dateTime, XSD.date, XSD.time):
                    return str(literal)  # Keep as string for FalkorDB
                else:
                    return str(literal)
            else:
                # Handle language tags
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _create_nodes_in_falkordb(self):
        """Create all nodes in FalkorDB"""
        nodes = self.node_manager.get_nodes()
        logger.info(f"Creating {len(nodes)} nodes in FalkorDB...")
        
        created_count = 0
        for uri, node_data in nodes.items():
            if self.falkordb_manager.create_node(uri, node_data):
                created_count += 1
        
        self.stats.unique_subjects = len(nodes)
        logger.info(f"Successfully created {created_count} nodes")
    
    def _create_relationships_in_falkordb(self):
        """Create all relationships in FalkorDB"""
        logger.info(f"Creating {len(self.relationships_to_create)} relationships in FalkorDB...")
        
        created_count = 0
        for subject_uri, predicate_uri, object_uri in self.relationships_to_create:
            if self.falkordb_manager.create_relationship(subject_uri, predicate_uri, object_uri):
                created_count += 1
        
        logger.info(f"Successfully created {created_count} relationships")
    
    def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("Validating conversion results...")
        
        falkor_stats = self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("No relationships were created despite processing relationship triples")
        
        logger.info(f"Validation complete: {falkor_stats['nodes']} nodes, {falkor_stats['relationships']} relationships")
    
    def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        # Get final counts from FalkorDB
        falkor_stats = self.falkordb_manager.get_graph_stats()
        self.stats.created_nodes = falkor_stats['nodes']
        self.stats.created_relationships = falkor_stats['relationships']
        
        # Calculate unique entities
        nodes = self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        # Count unique objects from relationships
        unique_objects = set()
        for _, _, obj_uri in self.relationships_to_create:
            unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            self.export_stats()
    
    def export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            with open(filename, 'w') as f:
                json.dump(self.stats.to_dict(), f, indent=2)
            logger.info(f"Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted graph"""
        return [
            "MATCH (n) RETURN labels(n) as node_labels, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN s.id, type(r), o.id LIMIT 10",
            "MATCH (n) WHERE size((n)--()) > 0 RETURN n.id, size((n)--()) as degree ORDER BY degree DESC LIMIT 10"
        ]

# Utility functions
def validate_sparql_endpoint(endpoint: str, username: str = None, password: str = None) -> bool:
    """Validate SPARQL endpoint connectivity"""
    try:
        auth = (username, password) if username and password else None
        store = sparqlstore.SPARQLStore(endpoint, auth=auth)
        
        g = Graph(store=store)
        list(g.query("SELECT * WHERE { ?s ?p ?o } LIMIT 1"))
        return True
    except Exception as e:
        logger.error(f"SPARQL endpoint validation failed: {e}")
        return False

def create_sample_query():
    """Create a sample 6-variable SPARQL query"""
    return """
    # Sample 6-variable triple query
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        
        # Get subject class (optional)
        OPTIONAL {
            ?subject a ?subjectClass .
        }
        
        # Get predicate class (optional) 
        OPTIONAL {
            ?predicate a ?predicateClass .
        }
        
        # Get object class (optional, only for URI objects)
        OPTIONAL {
            ?object a ?objectClass .
            FILTER(isURI(?object))
        }
        
        # Example filters (customize as needed)
        FILTER(?subject != ?object)  # Avoid self-loops
        
        # Optional: Filter by specific types
        # FILTER(?subjectClass IN (<http://xmlns.com/foaf/0.1/Person>, <http://schema.org/Organization>))
    }
    LIMIT 10000
    """

def main():
    """Example usage"""
    
    # Sample query
    sample_query = create_sample_query()
    
    config = TripleConfig(
        triples_query=sample_query,
        sparql_endpoint="https://dbpedia.org/sparql",
        username=None,
        password=None,
        
        falkordb_host='localhost',
        falkordb_port=6379,
        graph_name='triple_based_graph',
        
        batch_size=1000,
        use_shortened_uris=True,
        preserve_uri_properties=True,
        create_indexes=True,
        validate_conversion=True,
        export_stats=True
    )
    
    try:
        # Validate endpoint
        if not validate_sparql_endpoint(config.sparql_endpoint, config.username, config.password):
            logger.error("Cannot connect to SPARQL endpoint")
            return
        
        # Run conversion
        converter = TripleBasedConverter(config)
        stats = converter.convert()
        
        # Print results
        print("\n" + "="*60)
        print("TRIPLE-BASED CONVERSION COMPLETED")
        print("="*60)
        print(f"Total triples retrieved: {stats.total_triples_retrieved}")
        print(f"Property triples (literals): {stats.property_triples}")
        print(f"Relationship triples (URIs): {stats.relationship_triples}")
        print(f"Unique subjects: {stats.unique_subjects}")
        print(f"Unique objects: {stats.unique_objects}")
        print(f"Nodes created: {stats.created_nodes}")
        print(f"Relationships created: {stats.created_relationships}")
        
        print(f"\nDiscovered {len(stats.subject_classes)} subject classes")
        print(f"Discovered {len(stats.object_classes)} object classes") 
        print(f"Used {len(stats.predicates_used)} different predicates")
        
        if stats.end_time:
            duration = (stats.end_time - stats.start_time).total_seconds()
            print(f"\nTotal duration: {duration:.2f} seconds")
            print(f"Query execution: {stats.query_execution_time:.2f} seconds")
        
        print("\nSample Cypher queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            print(f"{i}. {query}")
        
    except Exception as e:
        logger.error(f"Conversion failed: {e}")
        raise

if __name__ == "__main__":
    main()
