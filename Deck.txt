#!/usr/bin/env python3
"""
TTL to FalkorDB Pipeline - Direct Access via Bind Mount
Converts TTL to CSV format then loads directly using bind mounts (no file copying)
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import sys
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
import shutil
from pathlib import Path
import platform
import subprocess

# Setup logging with UTF-8 encoding support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class TTLToFalkorDBConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to FalkorDB converter"""
        self.output_dir = Path(output_dir).resolve()  # Use absolute path
        self.nodes = {}  # node_id -> {node_type, properties}
        self.edges = []  # list of {source_id, target_id, edge_type, properties}
        self.node_id_map = {}  # URI -> unique_id
        self.next_id = 1
        self.is_windows = platform.system() == 'Windows'
        
        # Create output directory
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Output directory: {self.output_dir}")
        except Exception as e:
            logger.error(f"Failed to create output directory {self.output_dir}: {e}")
            raise
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        try:
            if isinstance(uri_or_literal, URIRef):
                uri_str = str(uri_or_literal)
                parsed = urlparse(uri_str)
                
                # Try fragment first (after #)
                if parsed.fragment:
                    name = parsed.fragment
                # Then try last path component
                elif parsed.path and parsed.path != '/':
                    name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
                # Fall back to netloc
                elif parsed.netloc:
                    name = parsed.netloc.replace('.', '_')
                else:
                    # Use hash of full URI as fallback
                    name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
                # Clean the name - only alphanumeric and underscore
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                # Ensure it starts with letter or underscore
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"n_{cleaned}"
                # Ensure it's not empty and has reasonable length
                if not cleaned or len(cleaned) < 1:
                    cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
                return cleaned[:50]  # Limit length
            else:
                # Handle literals or other types
                name = str(uri_or_literal)
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"l_{cleaned}"
                return (cleaned or "literal")[:50]
        except Exception as e:
            logger.warning(f"Error cleaning identifier for {uri_or_literal}: {e}")
            return f"error_{hashlib.md5(str(uri_or_literal).encode()).hexdigest()[:8]}"
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique ID for a resource"""
        try:
            uri = str(resource)
            if uri not in self.node_id_map:
                # Use hash-based IDs to avoid integer overflow issues
                hash_obj = hashlib.md5(uri.encode('utf-8'))
                node_id = f"n_{hash_obj.hexdigest()[:8]}"
                
                # Ensure uniqueness
                original_id = node_id
                counter = 1
                while node_id in [node['id'] for node in self.nodes.values()]:
                    node_id = f"{original_id}_{counter}"
                    counter += 1
                
                self.node_id_map[uri] = node_id
            return self.node_id_map[uri]
        except Exception as e:
            logger.error(f"Error creating node ID for {resource}: {e}")
            # Fallback to simple hash
            fallback_id = f"error_{hashlib.md5(str(resource).encode()).hexdigest()[:8]}"
            return fallback_id
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {}
        try:
            uri_str = str(uri)
            properties['uri'] = uri_str
            
            # Extract namespace and local name
            parsed = urlparse(uri_str)
            if parsed.fragment:
                properties['local_name'] = parsed.fragment
                properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
            elif parsed.path and parsed.path != '/':
                parts = [p for p in parsed.path.strip('/').split('/') if p]
                if parts:
                    properties['local_name'] = parts[-1]
                    properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        except Exception as e:
            logger.warning(f"Error extracting properties from URI {uri}: {e}")
            properties['uri'] = str(uri)
        
        return properties
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        try:
            if value is None:
                return ''
            
            if isinstance(value, bool):
                return 'true' if value else 'false'
            
            if isinstance(value, (list, dict)):
                # Convert to JSON string for complex data
                return json.dumps(value, ensure_ascii=False)
            
            # Convert to string and clean
            str_value = str(value).strip()
            
            # Replace problematic characters for CSV
            str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
            str_value = re.sub(r'\s+', ' ', str_value)  # Normalize whitespace
            
            # Escape quotes by doubling them (CSV standard)
            if '"' in str_value:
                str_value = str_value.replace('"', '""')
            
            # Limit string length to prevent memory issues
            if len(str_value) > 500:
                logger.warning(f"Very long string truncated (length: {len(str_value)})")
                str_value = str_value[:500] + "..."
            
            return str_value
        except Exception as e:
            logger.warning(f"Error sanitizing value {value}: {e}")
            return str(value) if value is not None else ''
    
    def process_literal_value(self, literal: Literal) -> str:
        """Process literal value and return cleaned string"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                # Handle common XSD datatypes
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int_val = int(literal)
                        # Keep reasonable sized integers
                        if abs(int_val) <= 2147483647:
                            return str(int_val)
                        else:
                            return str(literal)  # Keep as string if too large
                    except ValueError:
                        return str(literal)
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:
                            return str(float_val)
                        else:
                            return str(literal)
                    except ValueError:
                        return str(literal)
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower()
                    
                else:
                    return str(literal)
            else:
                return str(literal)
                
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal)
    
    def convert_ttl_to_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL file to CSV format for FalkorDB LOAD CSV"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Parse TTL file
        graph = Graph()
        try:
            logger.info("Parsing TTL file...")
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Successfully parsed TTL file. Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found in the TTL file!")
            return [], None
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"Processing only first {max_triples} of {total_triples} triples")
            total_triples = max_triples
        
        logger.info("Processing triples and building data structures...")
        processed_count = 0
        try:
            with tqdm(total=total_triples, desc="Processing triples") as pbar:
                for subject, predicate, obj in graph:
                    if max_triples and processed_count >= max_triples:
                        break
                    
                    try:
                        pbar.update(1)
                        processed_count += 1
                        
                        # Get or create subject node
                        subject_id = self.get_or_create_node_id(subject)
                        subject_type = self.clean_identifier(subject)
                        predicate_clean = self.clean_identifier(predicate)
                        
                        # Initialize subject node if not exists
                        if subject_id not in self.nodes:
                            if isinstance(subject, URIRef):
                                base_props = self.extract_properties_from_uri(subject)
                            else:  # BNode
                                base_props = {
                                    'uri': str(subject),
                                    'resource_type': 'blank_node'
                                }
                            
                            self.nodes[subject_id] = {
                                'id': subject_id,
                                'node_type': subject_type,
                                'properties': base_props
                            }
                        
                        # Handle object
                        if isinstance(obj, Literal):
                            # Add as property to subject node
                            value = self.process_literal_value(obj)
                            prop_name = predicate_clean
                            
                            # Store the value as a property
                            self.nodes[subject_id]['properties'][prop_name] = value
                            
                            # Store language if present
                            if obj.language:
                                lang_prop = f"{prop_name}_lang"
                                self.nodes[subject_id]['properties'][lang_prop] = obj.language
                        
                        else:
                            # Object is a resource - create edge
                            object_id = self.get_or_create_node_id(obj)
                            object_type = self.clean_identifier(obj)
                            
                            # Initialize object node if not exists
                            if object_id not in self.nodes:
                                if isinstance(obj, URIRef):
                                    base_props = self.extract_properties_from_uri(obj)
                                else:  # BNode
                                    base_props = {
                                        'uri': str(obj),
                                        'resource_type': 'blank_node'
                                    }
                                
                                self.nodes[object_id] = {
                                    'id': object_id,
                                    'node_type': object_type,
                                    'properties': base_props
                                }
                            
                            # Create edge
                            edge = {
                                'source_id': subject_id,
                                'target_id': object_id,
                                'edge_type': predicate_clean,
                                'properties': {
                                    'predicate_uri': str(predicate)
                                }
                            }
                            self.edges.append(edge)
                    except Exception as e:
                        logger.warning(f"Error processing triple {processed_count}: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error during triple processing: {e}")
            raise
        
        logger.info(f"Data processing complete:")
        logger.info(f"  Nodes: {len(self.nodes):,}")
        logger.info(f"  Edges: {len(self.edges):,}")
        
        # Write CSV files
        try:
            csv_files = self.write_csv_files()
        except Exception as e:
            logger.error(f"Error writing CSV files: {e}")
            raise
        
        return csv_files
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names from all nodes
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        
        all_properties = sorted(all_properties)
        
        try:
            with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: id, node_type, then all properties
                headers = ['id', 'node_type'] + all_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for node in self.nodes.values():
                    row = [
                        node['id'],
                        node['node_type']
                    ]
                    
                    # Add property values
                    for prop in all_properties:
                        value = node['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.nodes):,} nodes to {nodes_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing nodes file {nodes_file}: {e}")
            raise
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        # Get all possible edge property names
        all_edge_properties = set()
        for edge in self.edges:
            all_edge_properties.update(edge['properties'].keys())
        
        all_edge_properties = sorted(all_edge_properties)
        
        try:
            with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: source_id, target_id, edge_type, then all properties
                headers = ['source_id', 'target_id', 'edge_type'] + all_edge_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for edge in self.edges:
                    row = [
                        edge['source_id'],
                        edge['target_id'],
                        edge['edge_type']
                    ]
                    
                    # Add property values
                    for prop in all_edge_properties:
                        value = edge['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.edges):,} edges to {edges_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing edges file {edges_file}: {e}")
            raise
        
        return csv_files
    
    def generate_bind_mount_docker_command(self, container_name="falkordb_direct"):
        """Generate Docker command to run FalkorDB with bind mount"""
        if self.is_windows:
            # Windows path format for Docker
            mount_path = str(self.output_dir).replace('\\', '/')
            if ':' in mount_path:
                # Convert C:/path to /c/path format for Docker
                drive, rest = mount_path.split(':', 1)
                mount_path = f"/{drive.lower()}{rest}"
        else:
            mount_path = str(self.output_dir)
        
        docker_cmd = [
            "docker", "run", "-d",
            "--name", container_name,
            "-p", "6379:6379",
            "-p", "3000:3000",
            "-v", f"{self.output_dir}:/var/lib/falkordb/import",
            "falkordb/falkordb:latest"
        ]
        
        return docker_cmd, container_name
    
    def start_falkordb_with_bind_mount(self, container_name="falkordb_direct"):
        """Start FalkorDB container with csv_output directory bind mounted"""
        logger.info("Starting FalkorDB with bind mount to csv_output directory...")
        logger.info(f"Mounting: {self.output_dir} -> /var/lib/falkordb/import")
        
        try:
            # Check if container already exists
            result = subprocess.run([
                'docker', 'ps', '-a', '--filter', f'name={container_name}', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if container_name in result.stdout:
                logger.info(f"Stopping and removing existing container: {container_name}")
                subprocess.run(['docker', 'stop', container_name], capture_output=True)
                subprocess.run(['docker', 'rm', container_name], capture_output=True)
            
            # Generate and run Docker command
            docker_cmd, _ = self.generate_bind_mount_docker_command(container_name)
            
            logger.info(f"Starting container: {' '.join(docker_cmd)}")
            result = subprocess.run(docker_cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"Failed to start container: {result.stderr}")
                return False, None
            
            # Wait for container to start
            time.sleep(3)
            
            # Verify container is running
            verify_result = subprocess.run([
                'docker', 'ps', '--filter', f'name={container_name}', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if container_name in verify_result.stdout:
                logger.info(f"FalkorDB container started: {container_name}")
                logger.info("CSV files are directly accessible in container import directory")
                logger.info("Web interface: http://localhost:3000")
                return True, container_name
            else:
                logger.error("Container failed to start properly")
                return False, None
                
        except Exception as e:
            logger.error(f"Error starting Docker container: {e}")
            return False, None
    
    def generate_cypher_commands(self):
        """Generate Cypher commands for bind mount approach"""
        cypher_file = self.output_dir / "load_data.cypher"
        
        try:
            with open(cypher_file, 'w', encoding='utf-8') as f:
                f.write("// FalkorDB LOAD CSV Commands - Bind Mount Approach\n")
                f.write("// CSV files are directly accessible via bind mount\n")
                f.write(f"// Files mounted from: {self.output_dir}\n\n")
                
                f.write("// Clear existing data (optional)\n")
                f.write("// MATCH (n) DETACH DELETE n;\n\n")
                
                f.write("// Load all nodes with generic Entity label\n")
                f.write("LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row\n")
                f.write("CREATE (n:Entity)\n")
                f.write("SET n = row;\n\n")
                
                f.write("// Load all edges with generic CONNECTED_TO relationship\n")
                f.write("LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row\n")
                f.write("MATCH (source:Entity {id: row.source_id})\n")
                f.write("MATCH (target:Entity {id: row.target_id})\n")
                f.write("CREATE (source)-[r:CONNECTED_TO]->(target)\n")
                f.write("SET r = row\n")
                f.write("REMOVE r.source_id, r.target_id, r.edge_type;\n\n")
                
                f.write("// Verify data loaded\n")
                f.write("MATCH (n) RETURN count(n) AS node_count;\n")
                f.write("MATCH ()-[r]->() RETURN count(r) AS edge_count;\n")
                f.write("MATCH (n) RETURN n.node_type, count(*) AS count ORDER BY count DESC LIMIT 10;\n")
            
            logger.info(f"Generated Cypher commands in {cypher_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing Cypher file {cypher_file}: {e}")
            raise
        
        return str(cypher_file)
    
    def test_falkordb_connection(self, host: str, port: int, password: Optional[str] = None):
        """Test connection to FalkorDB"""
        logger.info(f"Testing connection to FalkorDB at {host}:{port}...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test basic connection
            response = r.ping()
            if response:
                logger.info("Redis connection successful")
            else:
                logger.error("Redis ping failed")
                return False
            
            # Test FalkorDB module
            try:
                modules = r.module_list()
                falkor_loaded = any('graph' in str(module).lower() for module in modules)
                if falkor_loaded:
                    logger.info("FalkorDB module is loaded")
                else:
                    logger.warning("FalkorDB module not detected in module list")
                    logger.info(f"Available modules: {modules}")
            except Exception as e:
                logger.warning(f"Could not check modules: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    def execute_cypher_commands(self, graph_name: str, 
                              host: str = '127.0.0.1', port: int = 6379, 
                              password: Optional[str] = None):
        """Execute Cypher commands on FalkorDB"""
        logger.info(f"Executing Cypher commands on graph '{graph_name}'...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test connection
            r.ping()
            logger.info("Connected to FalkorDB")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            return False
        
        try:
            # Load nodes
            logger.info("Loading nodes...")
            nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row
            CREATE (n:Entity)
            SET n = row
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
                logger.info(f"Nodes loaded: {result}")
            except Exception as e:
                logger.error(f"Failed to load nodes: {e}")
                if "error opening csv uri" in str(e).lower():
                    logger.error("CSV files not accessible. Check bind mount setup.")
                return False
            
            # Load edges
            logger.info("Loading edges...")
            edges_query = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            MATCH (source:Entity {id: row.source_id})
            MATCH (target:Entity {id: row.target_id})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = row
            REMOVE r.source_id, r.target_id
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_query)
                logger.info(f"Edges loaded: {result}")
            except Exception as e:
                logger.error(f"Failed to load edges: {e}")
                return False
            
            # Verify data
            try:
                count_query = "MATCH (n) RETURN count(n) AS node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"Total nodes in graph: {result}")
                
                edge_count_query = "MATCH ()-[r]->() RETURN count(r) AS edge_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, edge_count_query)
                logger.info(f"Total edges in graph: {result}")
            except Exception as e:
                logger.warning(f"Could not verify data counts: {e}")
            
            logger.info("Cypher commands executed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to execute Cypher commands: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Convert TTL to FalkorDB - Bind Mount (No File Copying)')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host (default: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port (default: 6379)')
    parser.add_argument('--password', help='FalkorDB password (default: none)')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--csv_only', action='store_true', help='Only convert to CSV, skip loading')
    parser.add_argument('--test_connection', action='store_true', help='Test FalkorDB connection only')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process for testing')
    parser.add_argument('--execute_cypher', action='store_true', help='Execute Cypher commands automatically')
    parser.add_argument('--start_docker', action='store_true', help='Start FalkorDB container with bind mount')
    parser.add_argument('--container_name', default='falkordb_direct', help='Docker container name')
    parser.add_argument('--show_docker_cmd', action='store_true', help='Show Docker command and exit')
    
    args = parser.parse_args()
    
    # Validate input
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        sys.exit(1)
    
    try:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        logger.info(f"Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    except Exception as e:
        logger.error(f"Cannot access TTL file: {e}")
        sys.exit(1)
    
    # Create converter
    try:
        converter = TTLToFalkorDBConverter(args.output_dir)
    except Exception as e:
        logger.error(f"Failed to initialize converter: {e}")
        sys.exit(1)
    
    # Show Docker command and exit if requested
    if args.show_docker_cmd:
        docker_cmd, container_name = converter.generate_bind_mount_docker_command(args.container_name)
        logger.info("Docker command to start FalkorDB with bind mount:")
        logger.info(" ".join(docker_cmd))
        logger.info(f"\nThis mounts {converter.output_dir} as the import directory")
        logger.info("No file copying needed - direct access to csv_output/")
        return
    
    # Start Docker if requested
    if args.start_docker:
        try:
            success, container_name = converter.start_falkordb_with_bind_mount(args.container_name)
            if success:
                logger.info("FalkorDB container started successfully!")
                logger.info(f"Container: {container_name}")
                logger.info("You can now run --execute_cypher to load data")
            else:
                logger.error("Failed to start FalkorDB container")
                sys.exit(1)
        except Exception as e:
            logger.error(f"Error starting Docker: {e}")
            sys.exit(1)
        return
    
    # Test connection only if requested
    if args.test_connection:
        logger.info("Connection test mode")
        try:
            success = converter.test_falkordb_connection(args.host, args.port, args.password)
            if success:
                logger.info("Connection test passed!")
                sys.exit(0)
            else:
                logger.error("Connection test failed!")
                sys.exit(1)
        except Exception as e:
            logger.error(f"Connection test error: {e}")
            sys.exit(1)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        try:
            csv_files = converter.convert_ttl_to_csv(args.ttl_file, args.max_triples)
            if not csv_files:
                logger.error("No CSV files were generated")
                sys.exit(1)
        except Exception as e:
            logger.error(f"TTL to CSV conversion failed: {e}")
            sys.exit(1)
        
        conversion_time = time.time() - start_time
        logger.info(f"TTL->CSV conversion completed in {conversion_time:.1f}s")
        
        # Generate Cypher commands
        try:
            cypher_file = converter.generate_cypher_commands()
        except Exception as e:
            logger.error(f"Failed to generate Cypher commands: {e}")
            sys.exit(1)
        
        if args.csv_only:
            logger.info("CSV-only mode: Files generated successfully")
            logger.info(f"CSV files in: {args.output_dir}/")
            logger.info(f"Cypher commands: {cypher_file}")
            logger.info("\nNext steps:")
            logger.info("1. Start FalkorDB with bind mount:")
            docker_cmd, _ = converter.generate_bind_mount_docker_command(args.container_name)
            logger.info(f"   {' '.join(docker_cmd)}")
            logger.info("2. Or use: python script.py data.ttl --start_docker")
            logger.info("3. Then load data: python script.py data.ttl --execute_cypher")
            return
        
        # Execute Cypher commands if requested
        if args.execute_cypher:
            try:
                success = converter.execute_cypher_commands(
                    args.graph_name, args.host, args.port, args.password
                )
                
                total_time = time.time() - start_time
                
                if success:
                    logger.info(f"SUCCESS! Total time: {total_time:.1f}s")
                    logger.info(f"Performance: {file_size_mb/total_time:.1f} MB/s")
                    logger.info(f"Data loaded into graph: {args.graph_name}")
                    logger.info(f"CSV files remain in: {converter.output_dir}")
                    logger.info("Web interface: http://localhost:3000")
                else:
                    logger.error("Failed to execute Cypher commands")
                    logger.error("Make sure FalkorDB container is running with bind mount")
                    logger.error("Use --start_docker flag to set up the container")
                    sys.exit(1)
            except Exception as e:
                logger.error(f"Error executing Cypher commands: {e}")
                sys.exit(1)
        else:
            logger.info("CSV files and Cypher commands generated")
            logger.info(f"Files in: {args.output_dir}/")
            logger.info(f"Commands in: {cypher_file}")
            logger.info("\nTo load data:")
            logger.info("1. Start FalkorDB with bind mount:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --start_docker")
            logger.info("2. Load data:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --execute_cypher")
            
    except KeyboardInterrupt:
        logger.error("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        logger.exception("Full error details:")
        sys.exit(1)

if __name__ == "__main__":
    main()
