#!/usr/bin/env python3
"""
Enhanced GDPR Metamodel Knowledge System with Full Ontology Integration
========================================================================

A comprehensive multi-agent system for creating GDPR metamodels with:
- Full FalkorDB openCypher compliance with proper vector indexes
- Integration of PROV-O, GDPRov, GConsent, PrOnto ontologies
- Complete vectorization of nodes and edges with document provenance
- SKOS/OWL best practices for semantic relationships
- Financial industry compliance focus

Enhanced: December 2024
Ontologies: PROV-O, GDPRov, GConsent, PrOnto, SKOS, OWL
Standards: W3C, openCypher, FalkorDB 4.0+
"""

import os
import json
import logging
import asyncio
import uuid
import operator
import re
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple, Any, Union, Literal, Sequence
from dataclasses import dataclass, field, asdict
from pathlib import Path
from urllib.parse import quote

# Core libraries
import pandas as pd
import numpy as np
from tqdm import tqdm
import httpx

# Document processing
import pymupdf

# LangChain and LangGraph
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph for multi-agent architecture
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from typing_extensions import TypedDict, Annotated

# Database clients
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from falkordb import FalkorDB

# Direct OpenAI API
from openai import AsyncOpenAI

# Async support
import aiofiles

# ============================================================================
# ENHANCED CONFIGURATION WITH ONTOLOGY SUPPORT
# ============================================================================

class Config:
    """Enhanced configuration with ontology integration support"""
    
    # OpenAI Configuration
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    REASONING_MODEL: str = "o3-mini"
    REASONING_EFFORT: str = "high"
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS: int = 3072
    
    # Memory and Persistence
    MEMORY_THREAD_ID: str = os.getenv("MEMORY_THREAD_ID", "gdpr_metamodel_system")
    ENABLE_LONG_TERM_MEMORY: bool = os.getenv("ENABLE_LONG_TERM_MEMORY", "true").lower() == "true"
    KNOWLEDGE_STORE_NAMESPACE: str = "gdpr_metamodel"
    LEARNING_STORE_NAMESPACE: str = "gdpr_learning"
    
    # FalkorDB Configuration (Enhanced for vector support)
    FALKORDB_HOST: str = os.getenv("FALKOR_HOST", "localhost")
    FALKORDB_PORT: int = int(os.getenv("FALKOR_PORT", "6379"))
    FALKORDB_PASSWORD: str = os.getenv("FALKOR_PASSWORD", "")
    FALKORDB_GRAPH_NAME: str = "gdpr_metamodel_graph"
    FALKORDB_USE_TLS: bool = os.getenv("FALKOR_TLS", "false").lower() == "true"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_HOST: str = os.getenv("ES_HOST", "localhost")
    ELASTICSEARCH_PORT: int = int(os.getenv("ES_PORT", "9200"))
    ELASTICSEARCH_USERNAME: str = os.getenv("ES_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD: str = os.getenv("ES_PASSWORD")
    ELASTICSEARCH_INDEX: str = "gdpr_metamodel_kb"
    ELASTICSEARCH_CA_CERTS: str = os.getenv("ES_CA_CERTS", "")
    ELASTICSEARCH_VERIFY_CERTS: bool = os.getenv("ES_VERIFY_CERTS", "false").lower() == "true"
    
    # Document Processing
    CHUNK_SIZE: int = int(os.getenv("CHUNK_SIZE", "1500"))
    CHUNK_OVERLAP: int = int(os.getenv("CHUNK_OVERLAP", "300"))
    
    # Batch Configuration for Streaming
    DOCUMENT_BATCH_SIZE: int = int(os.getenv("DOCUMENT_BATCH_SIZE", "50"))
    EMBEDDING_BATCH_SIZE: int = int(os.getenv("EMBEDDING_BATCH_SIZE", "20"))
    ELASTICSEARCH_BATCH_SIZE: int = int(os.getenv("ELASTICSEARCH_BATCH_SIZE", "100"))
    FALKORDB_BATCH_SIZE: int = int(os.getenv("FALKORDB_BATCH_SIZE", "50"))
    CONCEPT_BATCH_SIZE: int = int(os.getenv("CONCEPT_BATCH_SIZE", "25"))
    
    # Ontology Configuration
    PROV_O_NAMESPACE: str = "http://www.w3.org/ns/prov#"
    GDPROV_NAMESPACE: str = "https://w3id.org/GDPRov#"
    GCONSENT_NAMESPACE: str = "https://w3id.org/GConsent#"
    PRONTO_NAMESPACE: str = "https://w3id.org/PrOnto#"
    SKOS_NAMESPACE: str = "http://www.w3.org/2004/02/skos/core#"
    OWL_NAMESPACE: str = "http://www.w3.org/2002/07/owl#"
    GDPR_NAMESPACE: str = "https://w3id.org/GDPRtEXT#"
    
    # File Paths
    DOCUMENTS_PATH: Path = Path(os.getenv("DOCS_PATH", "./documents"))
    OUTPUT_PATH: Path = Path(os.getenv("OUTPUT_PATH", "./output"))
    MEMORY_PATH: Path = Path(os.getenv("MEMORY_PATH", "./memory"))
    
    @classmethod
    def validate(cls):
        """Validate configuration with ontology checks"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        valid_efforts = ["low", "medium", "high"]
        if cls.REASONING_EFFORT not in valid_efforts:
            raise ValueError(f"REASONING_EFFORT must be one of {valid_efforts}")
        
        logging.info(f"‚úÖ Enhanced Configuration validated for GDPR Metamodel System")
        logging.info(f"üìä Batch sizes - Documents: {cls.DOCUMENT_BATCH_SIZE}, Embeddings: {cls.EMBEDDING_BATCH_SIZE}")
        logging.info(f"üóÑÔ∏è DB batch sizes - ES: {cls.ELASTICSEARCH_BATCH_SIZE}, Falkor: {cls.FALKORDB_BATCH_SIZE}")
        logging.info(f"üîó Ontology namespaces configured: PROV-O, GDPRov, GConsent, PrOnto, SKOS")
        print(f"‚öôÔ∏è Enhanced Configuration: Doc batches={cls.DOCUMENT_BATCH_SIZE}, Embedding batches={cls.EMBEDDING_BATCH_SIZE}")
        print(f"üß† Ontology Integration: PROV-O, GDPRov, GConsent, PrOnto, SKOS, OWL")

# ============================================================================
# ENHANCED DATA MODELS WITH FULL ONTOLOGY INTEGRATION
# ============================================================================

@dataclass
class DocumentProvenance:
    """Document provenance following PROV-O patterns"""
    source_document: str
    document_id: str
    chunk_id: str
    page_number: Optional[int]
    extraction_timestamp: datetime
    extraction_method: str
    confidence_score: float
    prov_entity_id: str  # PROV-O Entity URI
    prov_activity_id: str  # PROV-O Activity URI
    prov_agent_id: str  # PROV-O Agent URI

@dataclass
class SemanticConcept:
    """Enhanced concept with full ontology integration"""
    # Core identification
    concept_id: str
    uri: str  # Full URI with namespace
    label: str
    definition: str
    
    # SKOS Properties
    skos_type: str  # skos:Concept, skos:ConceptScheme, etc.
    pref_label: Dict[str, str]  # Language-tagged preferred labels
    alt_labels: List[Dict[str, str]]  # Alternative labels with language tags
    notation: Optional[str]  # SKOS notation
    
    # SKOS Semantic Relations
    broader_concepts: List[str]  # skos:broader
    narrower_concepts: List[str]  # skos:narrower
    related_concepts: List[str]  # skos:related
    top_concept_of: List[str]  # skos:topConceptOf
    in_scheme: List[str]  # skos:inScheme
    
    # SKOS Mapping Properties
    exact_match: List[str]  # skos:exactMatch
    close_match: List[str]  # skos:closeMatch
    broad_match: List[str]  # skos:broadMatch
    narrow_match: List[str]  # skos:narrowMatch
    related_match: List[str]  # skos:relatedMatch
    
    # PROV-O Properties
    prov_type: str  # prov:Entity, prov:Activity, prov:Agent
    was_generated_by: Optional[str]  # prov:wasGeneratedBy
    was_derived_from: List[str]  # prov:wasDerivedFrom
    was_attributed_to: List[str]  # prov:wasAttributedTo
    had_primary_source: List[str]  # prov:hadPrimarySource
    
    # GDPRov Properties
    gdprov_type: Optional[str]  # GDPRov specific classification
    legal_basis: List[str]  # GDPRov legal basis references
    processing_purpose: List[str]  # GDPRov processing purposes
    data_category: Optional[str]  # GDPRov data categorization
    
    # GConsent Properties (if applicable)
    consent_type: Optional[str]  # GConsent consent classification
    consent_status: Optional[str]  # GConsent status
    consent_mechanism: Optional[str]  # GConsent mechanism
    
    # PrOnto Properties
    pronto_type: Optional[str]  # PrOnto classification
    legal_obligation: List[str]  # PrOnto legal obligations
    privacy_right: List[str]  # PrOnto privacy rights
    
    # GDPR Specific
    article_references: List[str]
    regulation_type: str  # GDPR, UK_GDPR, etc.
    jurisdiction: str
    territorial_scope: List[str]
    
    # Industry Context
    industry_definitions: Dict[str, str]
    compliance_requirements: List[str]
    ropa_relevance: str
    financial_sector_impact: str
    
    # Document Provenance
    provenance: DocumentProvenance
    confidence_score: float
    last_updated: datetime
    
    # Vector embedding with metadata
    embedding: Optional[List[float]] = None
    embedding_model: Optional[str] = None
    embedding_timestamp: Optional[datetime] = None

@dataclass
class GDPRArticle:
    """Enhanced GDPR article with comprehensive ontology support"""
    # Core identification
    article_id: str
    uri: str  # Full URI with namespace
    number: Union[int, str]
    title: str
    content: str
    
    # PROV-O Properties
    prov_entity_id: str
    was_generated_by: str  # Extraction activity
    was_attributed_to: List[str]  # Authors/institutions
    had_primary_source: str  # Original regulation document
    
    # SKOS Properties for article classification
    skos_concept_id: str
    broader_articles: List[str]  # Related higher-level articles
    narrower_articles: List[str]  # More specific sub-articles
    related_articles: List[str]  # Associated articles
    
    # GDPRov Properties
    gdprov_entity_type: str
    processing_activities: List[str]
    legal_bases_defined: List[str]
    obligations_specified: List[str]
    rights_granted: List[str]
    
    # GConsent Properties (if consent-related)
    consent_requirements: List[str]
    consent_conditions: List[str]
    
    # PrOnto Properties
    legal_concepts: List[str]
    deontic_specifications: List[str]
    
    # Regulatory Context
    regulation_type: str
    jurisdiction: str
    territorial_scope: List[str]
    cross_border_implications: List[str]
    
    # Semantic Analysis
    concepts: List[str]  # Referenced concepts
    obligations: List[str]
    rights: List[str]
    legal_bases: List[str]
    
    # Industry Context
    financial_sector_relevance: str
    ropa_implications: List[str]
    
    # Relationships
    implements_principles: List[str]
    references_guidelines: List[str]
    case_law_references: List[str]
    
    # Document Provenance
    provenance: DocumentProvenance
    confidence_score: float
    last_updated: datetime
    
    # Vector embedding
    embedding: Optional[List[float]] = None
    embedding_model: Optional[str] = None
    embedding_timestamp: Optional[datetime] = None

@dataclass
class SemanticRelationship:
    """Semantic relationship with full ontology support"""
    relationship_id: str
    uri: str
    
    # Core relationship
    source_id: str
    target_id: str
    relationship_type: str
    
    # Ontology Classification
    prov_type: Optional[str]  # prov:used, prov:wasGeneratedBy, etc.
    skos_type: Optional[str]  # skos:broader, skos:related, etc.
    gdprov_type: Optional[str]  # GDPRov relationship type
    
    # Relationship Properties
    properties: Dict[str, Any]
    inverse_relationship: Optional[str]
    
    # Provenance
    provenance: DocumentProvenance
    confidence_score: float
    
    # Vector representation of relationship context
    embedding: Optional[List[float]] = None

# ============================================================================
# ENHANCED FALKORDB MANAGER WITH FULL ONTOLOGY INTEGRATION
# ============================================================================

class EnhancedOntologyFalkorDBManager:
    """Enhanced FalkorDB manager with comprehensive ontology integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self.graph = None
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
        self._setup_client()
    
    def _setup_client(self):
        """Setup FalkorDB client with enhanced configuration"""
        try:
            connection_params = {
                'host': self.config.FALKORDB_HOST,
                'port': self.config.FALKORDB_PORT,
                'decode_responses': True
            }
            
            if self.config.FALKORDB_PASSWORD:
                connection_params['password'] = self.config.FALKORDB_PASSWORD
            
            if self.config.FALKORDB_USE_TLS:
                connection_params['ssl'] = True
                connection_params['ssl_cert_reqs'] = 'required'
            
            self.client = FalkorDB(**connection_params)
            self.graph = self.client.select_graph(self.config.FALKORDB_GRAPH_NAME)
            
            logging.info(f"‚úÖ Connected to FalkorDB: {self.config.FALKORDB_GRAPH_NAME}")
            
        except Exception as e:
            logging.error(f"‚ùå FalkorDB connection error: {e}")
            raise
    
    async def create_ontology_aware_schema(self):
        """Create comprehensive schema with all ontology patterns"""
        try:
            print(f"üèóÔ∏è Creating ontology-aware schema with vector indexes...")
            
            # Get schema requirements from ontology analysis
            schema_requirements = await self._analyze_ontology_requirements()
            
            # Create vector indexes for all major node types with proper FalkorDB syntax
            vector_indexes = [
                # Core entity types with embeddings
                ("Concept", "embedding"),
                ("Article", "embedding"), 
                ("Activity", "embedding"),
                ("Agent", "embedding"),
                ("Entity", "embedding"),
                
                # Document and provenance types
                ("Document", "embedding"),
                ("DocumentChunk", "embedding"),
                
                # Consent and privacy types
                ("ConsentEntity", "embedding"),
                ("LegalBasis", "embedding"),
                ("ProcessingActivity", "embedding"),
                
                # Relationship embeddings (for relationship context)
                ("SemanticRelation", "embedding")
            ]
            
            for node_type, property_name in vector_indexes:
                try:
                    # FalkorDB 4.0+ vector index syntax with proper options
                    vector_query = f"""
                    CREATE VECTOR INDEX FOR (n:{node_type}) ON (n.{property_name}) 
                    OPTIONS {{dimension: {self.config.EMBEDDING_DIMENSIONS}, similarityFunction: 'cosine'}}
                    """
                    print(f"üîÆ Creating vector index: {node_type}.{property_name}")
                    self.graph.query(vector_query)
                    print(f"‚úÖ Vector index created: {node_type}.{property_name}")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Vector index may already exist: {node_type}.{property_name} - {e}")
                    logging.warning(f"Vector index warning for {node_type}: {e}")
            
            # Create property indexes for key properties
            property_indexes = [
                # Core identifiers
                ("Concept", "concept_id"),
                ("Concept", "uri"),
                ("Article", "article_id"),
                ("Article", "uri"),
                
                # SKOS properties
                ("Concept", "skos_type"),
                ("Concept", "in_scheme"),
                
                # PROV-O properties
                ("Entity", "prov_type"),
                ("Activity", "prov_type"),
                ("Agent", "prov_type"),
                
                # GDPRov properties
                ("ProcessingActivity", "gdprov_type"),
                ("LegalBasis", "legal_basis_type"),
                
                # Regulatory properties
                ("Article", "regulation_type"),
                ("Concept", "jurisdiction"),
                
                # Document properties
                ("Document", "document_id"),
                ("DocumentChunk", "chunk_id")
            ]
            
            for node_type, property_name in property_indexes:
                try:
                    index_query = f"CREATE INDEX FOR (n:{node_type}) ON (n.{property_name})"
                    print(f"üîç Creating property index: {node_type}.{property_name}")
                    self.graph.query(index_query)
                    print(f"‚úÖ Property index created: {node_type}.{property_name}")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Property index may already exist: {node_type}.{property_name} - {e}")
                    logging.warning(f"Property index warning: {e}")
            
            # Create full-text indexes for text content
            fulltext_indexes = [
                ("Concept", "definition"),
                ("Article", "content"),
                ("Document", "content"),
                ("ProcessingActivity", "description")
            ]
            
            for node_type, property_name in fulltext_indexes:
                try:
                    # FalkorDB full-text index syntax
                    fulltext_query = f"CREATE FULLTEXT INDEX FOR (n:{node_type}) ON EACH [n.{property_name}]"
                    print(f"üìù Creating fulltext index: {node_type}.{property_name}")
                    self.graph.query(fulltext_query)
                    print(f"‚úÖ Fulltext index created: {node_type}.{property_name}")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Fulltext index may already exist: {node_type}.{property_name} - {e}")
                    logging.warning(f"Fulltext index warning: {e}")
            
            # Create ontology structure nodes
            await self._create_ontology_structure()
            
            logging.info(f"‚úÖ Created comprehensive ontology-aware schema")
            
        except Exception as e:
            logging.error(f"‚ùå Error creating ontology schema: {e}")
            import traceback
            print(f"‚ùå Schema creation error: {e}")
            print(f"üìã Traceback: {traceback.format_exc()}")
            raise
    
    async def _analyze_ontology_requirements(self) -> Dict[str, Any]:
        """Analyze requirements for ontology integration"""
        
        analysis_prompt = ChatPromptTemplate.from_template("""
        You are an ontology architect designing a comprehensive knowledge graph for GDPR compliance.
        
        Analyze the requirements for integrating these ontologies:
        - PROV-O: Provenance ontology with Entity, Activity, Agent
        - GDPRov: GDPR provenance extension
        - GConsent: Consent modeling ontology  
        - PrOnto: Privacy rights ontology
        - SKOS: Knowledge organization system
        - OWL: Web ontology language patterns
        
        Return comprehensive schema requirements for FalkorDB graph database:
        {{
            "core_node_types": [
                "Concept", "Article", "Entity", "Activity", "Agent", 
                "ConsentEntity", "LegalBasis", "ProcessingActivity", "Document"
            ],
            "relationship_types": [
                "skos_broader", "skos_narrower", "skos_related",
                "prov_wasGeneratedBy", "prov_used", "prov_wasAttributedTo",
                "gdprov_hasLegalBasis", "gdprov_hasProcessingPurpose",
                "gconsent_hasConsentStatus", "pronto_hasObligation"
            ],
            "required_properties": {{
                "Concept": ["uri", "skos_type", "prov_type", "definition", "embedding"],
                "Article": ["uri", "regulation_type", "content", "embedding"],
                "Activity": ["prov_type", "start_time", "end_time", "embedding"],
                "Agent": ["prov_type", "agent_type", "embedding"]
            }},
            "vector_requirements": {{
                "all_nodes": "must have embedding for similarity search",
                "relationships": "context embeddings for semantic search"
            }}
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages()
            response = await self.llm.ainvoke(messages)
            requirements = json.loads(response.content)
            
            logging.info(f"üß† Ontology requirements analysis completed")
            return requirements
            
        except Exception as e:
            logging.error(f"‚ùå Error analyzing ontology requirements: {e}")
            # Fallback requirements
            return {
                "core_node_types": ["Concept", "Article", "Entity", "Activity", "Agent"],
                "relationship_types": ["skos_broader", "prov_wasGeneratedBy", "gdprov_hasLegalBasis"],
                "required_properties": {
                    "Concept": ["uri", "skos_type", "definition", "embedding"],
                    "Article": ["uri", "content", "embedding"]
                },
                "vector_requirements": {"all_nodes": "embedding required"}
            }
    
    async def _create_ontology_structure(self):
        """Create foundational ontology structure nodes"""
        try:
            print(f"üèõÔ∏è Creating ontology structure...")
            
            # Create ontology scheme nodes following SKOS patterns
            ontology_structures = [
                {
                    "type": "ConceptScheme",
                    "uri": f"{self.config.GDPR_NAMESPACE}GDPRConceptScheme",
                    "title": "GDPR Concept Scheme",
                    "description": "Hierarchical organization of GDPR concepts"
                },
                {
                    "type": "ConceptScheme", 
                    "uri": f"{self.config.SKOS_NAMESPACE}LegalBasisScheme",
                    "title": "Legal Basis Classification",
                    "description": "GDPR legal basis taxonomy"
                },
                {
                    "type": "ConceptScheme",
                    "uri": f"{self.config.GDPROV_NAMESPACE}ProcessingActivityScheme", 
                    "title": "Processing Activity Types",
                    "description": "Classification of data processing activities"
                }
            ]
            
            for structure in ontology_structures:
                query = """
                MERGE (s:ConceptScheme {uri: $uri})
                SET s.type = $type,
                    s.title = $title,
                    s.description = $description,
                    s.created_at = datetime(),
                    s.skos_type = 'skos:ConceptScheme'
                """
                
                self.graph.query(query, {
                    'uri': structure['uri'],
                    'type': structure['type'],
                    'title': structure['title'],
                    'description': structure['description']
                })
            
            # Create top-level concepts following SKOS hierarchy
            top_concepts = [
                {
                    "uri": f"{self.config.GDPR_NAMESPACE}DataSubjectRights",
                    "pref_label": "Data Subject Rights",
                    "scheme": f"{self.config.GDPR_NAMESPACE}GDPRConceptScheme"
                },
                {
                    "uri": f"{self.config.GDPR_NAMESPACE}LegalObligations", 
                    "pref_label": "Legal Obligations",
                    "scheme": f"{self.config.GDPR_NAMESPACE}GDPRConceptScheme"
                },
                {
                    "uri": f"{self.config.GDPROV_NAMESPACE}ProcessingActivities",
                    "pref_label": "Processing Activities", 
                    "scheme": f"{self.config.GDPROV_NAMESPACE}ProcessingActivityScheme"
                }
            ]
            
            for concept in top_concepts:
                query = """
                MATCH (s:ConceptScheme {uri: $scheme})
                MERGE (c:Concept {uri: $uri})
                SET c.pref_label = $pref_label,
                    c.skos_type = 'skos:Concept',
                    c.created_at = datetime()
                MERGE (c)-[:skos_inScheme]->(s)
                MERGE (s)-[:skos_hasTopConcept]->(c)
                """
                
                self.graph.query(query, {
                    'uri': concept['uri'],
                    'pref_label': concept['pref_label'],
                    'scheme': concept['scheme']
                })
            
            print(f"‚úÖ Ontology structure created")
            
        except Exception as e:
            logging.error(f"‚ùå Error creating ontology structure: {e}")
            raise
    
    async def store_semantic_concepts_with_ontology(self, concepts: List[SemanticConcept], 
                                                  batch_size: int = 50):
        """Store concepts with full ontology integration and vectorization"""
        try:
            print(f"üíæ Storing {len(concepts)} concepts with ontology integration...")
            
            stored_count = 0
            
            for batch_start in range(0, len(concepts), batch_size):
                batch_end = min(batch_start + batch_size, len(concepts))
                batch = concepts[batch_start:batch_end]
                
                print(f"üì¶ Processing ontology concept batch {batch_start//batch_size + 1}: {batch_start+1}-{batch_end}")
                
                for concept in batch:
                    try:
                        # Store concept with full ontology properties
                        await self._store_concept_with_ontology(concept)
                        
                        # Create SKOS relationships
                        await self._create_skos_relationships(concept)
                        
                        # Create PROV-O relationships  
                        await self._create_prov_relationships(concept)
                        
                        # Create GDPRov relationships
                        await self._create_gdprov_relationships(concept)
                        
                        # Store document provenance
                        await self._store_document_provenance(concept.provenance)
                        
                        stored_count += 1
                        
                    except Exception as e:
                        print(f"‚ùå Error storing concept {concept.concept_id}: {e}")
                        logging.error(f"‚ùå Error storing concept {concept.concept_id}: {e}")
                        continue
                
                print(f"‚úÖ Batch {batch_start//batch_size + 1} completed")
                await asyncio.sleep(0.1)  # Rate limiting
            
            logging.info(f"‚úÖ Stored {stored_count} concepts with ontology integration")
            return stored_count
            
        except Exception as e:
            logging.error(f"‚ùå Error storing semantic concepts: {e}")
            raise
    
    async def _store_concept_with_ontology(self, concept: SemanticConcept):
        """Store individual concept with all ontology properties"""
        
        # Main concept node with all properties
        concept_query = """
        MERGE (c:Concept {uri: $uri})
        SET c.concept_id = $concept_id,
            c.label = $label,
            c.definition = $definition,
            c.skos_type = $skos_type,
            c.prov_type = $prov_type,
            c.regulation_type = $regulation_type,
            c.jurisdiction = $jurisdiction,
            c.ropa_relevance = $ropa_relevance,
            c.financial_sector_impact = $financial_sector_impact,
            c.confidence_score = $confidence_score,
            c.last_updated = $last_updated,
            c.embedding = vecf32($embedding),
            c.embedding_model = $embedding_model,
            c.embedding_timestamp = $embedding_timestamp,
            c.pref_label_en = $pref_label_en,
            c.notation = $notation
        """
        
        # Prepare parameters with safe handling of None values
        params = {
            'uri': concept.uri,
            'concept_id': concept.concept_id,
            'label': concept.label,
            'definition': concept.definition,
            'skos_type': concept.skos_type,
            'prov_type': concept.prov_type,
            'regulation_type': concept.regulation_type,
            'jurisdiction': concept.jurisdiction,
            'ropa_relevance': concept.ropa_relevance,
            'financial_sector_impact': concept.financial_sector_impact,
            'confidence_score': concept.confidence_score,
            'last_updated': concept.last_updated.isoformat(),
            'embedding': concept.embedding or [0.0] * self.config.EMBEDDING_DIMENSIONS,
            'embedding_model': concept.embedding_model or self.config.EMBEDDING_MODEL,
            'embedding_timestamp': (concept.embedding_timestamp or datetime.now()).isoformat(),
            'pref_label_en': concept.pref_label.get('en', concept.label),
            'notation': concept.notation
        }
        
        self.graph.query(concept_query, params)
        
        # Store multilingual labels
        for lang, label in concept.pref_label.items():
            if lang != 'en':  # Already stored above
                label_query = """
                MATCH (c:Concept {uri: $uri})
                SET c.pref_label_{lang} = $label
                """.replace('{lang}', lang)
                
                self.graph.query(label_query, {'uri': concept.uri, 'label': label})
        
        # Store alternative labels
        for alt_label in concept.alt_labels:
            for lang, label in alt_label.items():
                alt_query = """
                MATCH (c:Concept {uri: $uri})
                MERGE (al:AltLabel {label: $label, language: $lang})
                MERGE (c)-[:skos_altLabel]->(al)
                """
                
                self.graph.query(alt_query, {
                    'uri': concept.uri,
                    'label': label,
                    'lang': lang
                })
    
    async def _create_skos_relationships(self, concept: SemanticConcept):
        """Create SKOS semantic relationships"""
        
        # SKOS hierarchical relationships
        skos_relations = [
            ('broader_concepts', 'skos_broader'),
            ('narrower_concepts', 'skos_narrower'), 
            ('related_concepts', 'skos_related')
        ]
        
        for attr, relation_type in skos_relations:
            related_uris = getattr(concept, attr, [])
            for related_uri in related_uris:
                if related_uri:
                    query = f"""
                    MATCH (source:Concept {{uri: $source_uri}})
                    MERGE (target:Concept {{uri: $target_uri}})
                    MERGE (source)-[:{relation_type}]->(target)
                    """
                    
                    self.graph.query(query, {
                        'source_uri': concept.uri,
                        'target_uri': related_uri
                    })
        
        # SKOS mapping relationships
        mapping_relations = [
            ('exact_match', 'skos_exactMatch'),
            ('close_match', 'skos_closeMatch'),
            ('broad_match', 'skos_broadMatch'),
            ('narrow_match', 'skos_narrowMatch'),
            ('related_match', 'skos_relatedMatch')
        ]
        
        for attr, relation_type in mapping_relations:
            match_uris = getattr(concept, attr, [])
            for match_uri in match_uris:
                if match_uri:
                    query = f"""
                    MATCH (source:Concept {{uri: $source_uri}})
                    MERGE (target:Concept {{uri: $target_uri}})
                    MERGE (source)-[:{relation_type}]->(target)
                    """
                    
                    self.graph.query(query, {
                        'source_uri': concept.uri,
                        'target_uri': match_uri
                    })
        
        # SKOS scheme relationships
        for scheme_uri in concept.in_scheme:
            if scheme_uri:
                query = """
                MATCH (c:Concept {uri: $concept_uri})
                MERGE (s:ConceptScheme {uri: $scheme_uri})
                MERGE (c)-[:skos_inScheme]->(s)
                """
                
                self.graph.query(query, {
                    'concept_uri': concept.uri,
                    'scheme_uri': scheme_uri
                })
    
    async def _create_prov_relationships(self, concept: SemanticConcept):
        """Create PROV-O provenance relationships"""
        
        # PROV-O Entity relationships
        if concept.was_generated_by:
            query = """
            MATCH (entity:Concept {uri: $entity_uri})
            MERGE (activity:Activity {uri: $activity_uri})
            SET activity.prov_type = 'prov:Activity'
            MERGE (entity)-[:prov_wasGeneratedBy]->(activity)
            """
            
            self.graph.query(query, {
                'entity_uri': concept.uri,
                'activity_uri': concept.was_generated_by
            })
        
        # PROV-O derivation relationships
        for source_uri in concept.was_derived_from:
            if source_uri:
                query = """
                MATCH (derived:Concept {uri: $derived_uri})
                MERGE (source:Entity {uri: $source_uri})
                SET source.prov_type = 'prov:Entity'
                MERGE (derived)-[:prov_wasDerivedFrom]->(source)
                """
                
                self.graph.query(query, {
                    'derived_uri': concept.uri,
                    'source_uri': source_uri
                })
        
        # PROV-O attribution relationships
        for agent_uri in concept.was_attributed_to:
            if agent_uri:
                query = """
                MATCH (entity:Concept {uri: $entity_uri})
                MERGE (agent:Agent {uri: $agent_uri})
                SET agent.prov_type = 'prov:Agent'
                MERGE (entity)-[:prov_wasAttributedTo]->(agent)
                """
                
                self.graph.query(query, {
                    'entity_uri': concept.uri,
                    'agent_uri': agent_uri
                })
        
        # PROV-O primary source relationships
        for source_uri in concept.had_primary_source:
            if source_uri:
                query = """
                MATCH (entity:Concept {uri: $entity_uri})
                MERGE (source:Entity {uri: $source_uri})
                SET source.prov_type = 'prov:Entity'
                MERGE (entity)-[:prov_hadPrimarySource]->(source)
                """
                
                self.graph.query(query, {
                    'entity_uri': concept.uri,
                    'source_uri': source_uri
                })
    
    async def _create_gdprov_relationships(self, concept: SemanticConcept):
        """Create GDPRov specific relationships"""
        
        # GDPRov legal basis relationships
        for legal_basis in concept.legal_basis:
            if legal_basis:
                query = """
                MATCH (concept:Concept {uri: $concept_uri})
                MERGE (lb:LegalBasis {uri: $legal_basis_uri})
                SET lb.gdprov_type = 'gdprov:LegalBasis',
                    lb.legal_basis_type = $legal_basis
                MERGE (concept)-[:gdprov_hasLegalBasis]->(lb)
                """
                
                legal_basis_uri = f"{self.config.GDPROV_NAMESPACE}LegalBasis_{legal_basis}"
                
                self.graph.query(query, {
                    'concept_uri': concept.uri,
                    'legal_basis_uri': legal_basis_uri,
                    'legal_basis': legal_basis
                })
        
        # GDPRov processing purpose relationships
        for purpose in concept.processing_purpose:
            if purpose:
                query = """
                MATCH (concept:Concept {uri: $concept_uri})
                MERGE (pp:ProcessingPurpose {uri: $purpose_uri})
                SET pp.gdprov_type = 'gdprov:ProcessingPurpose',
                    pp.purpose_type = $purpose
                MERGE (concept)-[:gdprov_hasProcessingPurpose]->(pp)
                """
                
                purpose_uri = f"{self.config.GDPROV_NAMESPACE}ProcessingPurpose_{quote(purpose)}"
                
                self.graph.query(query, {
                    'concept_uri': concept.uri,
                    'purpose_uri': purpose_uri,
                    'purpose': purpose
                })
    
    async def _store_document_provenance(self, provenance: DocumentProvenance):
        """Store document provenance following PROV-O patterns"""
        
        # Store document as PROV-O Entity
        doc_query = """
        MERGE (doc:Document {document_id: $document_id})
        SET doc.uri = $prov_entity_id,
            doc.source_document = $source_document,
            doc.prov_type = 'prov:Entity',
            doc.extraction_timestamp = $extraction_timestamp,
            doc.extraction_method = $extraction_method,
            doc.confidence_score = $confidence_score
        """
        
        self.graph.query(doc_query, {
            'document_id': provenance.document_id,
            'prov_entity_id': provenance.prov_entity_id,
            'source_document': provenance.source_document,
            'extraction_timestamp': provenance.extraction_timestamp.isoformat(),
            'extraction_method': provenance.extraction_method,
            'confidence_score': provenance.confidence_score
        })
        
        # Store extraction activity
        activity_query = """
        MERGE (activity:Activity {uri: $prov_activity_id})
        SET activity.prov_type = 'prov:Activity',
            activity.activity_type = $extraction_method,
            activity.start_time = $extraction_timestamp,
            activity.end_time = $extraction_timestamp
        """
        
        self.graph.query(activity_query, {
            'prov_activity_id': provenance.prov_activity_id,
            'extraction_method': provenance.extraction_method,
            'extraction_timestamp': provenance.extraction_timestamp.isoformat()
        })
        
        # Store extraction agent
        agent_query = """
        MERGE (agent:Agent {uri: $prov_agent_id})
        SET agent.prov_type = 'prov:Agent',
            agent.agent_type = 'Software'
        """
        
        self.graph.query(agent_query, {
            'prov_agent_id': provenance.prov_agent_id
        })
        
        # Create PROV-O relationships
        prov_relations_query = """
        MATCH (doc:Document {document_id: $document_id})
        MATCH (activity:Activity {uri: $prov_activity_id})
        MATCH (agent:Agent {uri: $prov_agent_id})
        MERGE (doc)-[:prov_wasGeneratedBy]->(activity)
        MERGE (activity)-[:prov_wasAssociatedWith]->(agent)
        """
        
        self.graph.query(prov_relations_query, {
            'document_id': provenance.document_id,
            'prov_activity_id': provenance.prov_activity_id,
            'prov_agent_id': provenance.prov_agent_id
        })
    
    async def store_gdpr_articles_with_ontology(self, articles: List[GDPRArticle], 
                                              batch_size: int = 25):
        """Store GDPR articles with full ontology integration"""
        try:
            print(f"üìö Storing {len(articles)} articles with ontology integration...")
            
            stored_count = 0
            
            for batch_start in range(0, len(articles), batch_size):
                batch_end = min(batch_start + batch_size, len(articles))
                batch = articles[batch_start:batch_end]
                
                print(f"üì¶ Processing article batch {batch_start//batch_size + 1}: {batch_start+1}-{batch_end}")
                
                for article in batch:
                    try:
                        # Store article with ontology properties
                        await self._store_article_with_ontology(article)
                        
                        # Create SKOS relationships for article classification
                        await self._create_article_skos_relationships(article)
                        
                        # Create PROV-O relationships for article provenance
                        await self._create_article_prov_relationships(article)
                        
                        # Create GDPRov relationships for legal concepts
                        await self._create_article_gdprov_relationships(article)
                        
                        # Store document provenance
                        await self._store_document_provenance(article.provenance)
                        
                        stored_count += 1
                        
                    except Exception as e:
                        print(f"‚ùå Error storing article {article.article_id}: {e}")
                        logging.error(f"‚ùå Error storing article {article.article_id}: {e}")
                        continue
                
                print(f"‚úÖ Article batch {batch_start//batch_size + 1} completed")
                await asyncio.sleep(0.1)
            
            logging.info(f"‚úÖ Stored {stored_count} articles with ontology integration")
            return stored_count
            
        except Exception as e:
            logging.error(f"‚ùå Error storing GDPR articles: {e}")
            raise
    
    async def _store_article_with_ontology(self, article: GDPRArticle):
        """Store individual article with all ontology properties"""
        
        article_query = """
        MERGE (a:Article {uri: $uri})
        SET a.article_id = $article_id,
            a.number = $number,
            a.title = $title,
            a.content = $content,
            a.regulation_type = $regulation_type,
            a.jurisdiction = $jurisdiction,
            a.financial_sector_relevance = $financial_sector_relevance,
            a.prov_entity_id = $prov_entity_id,
            a.skos_concept_id = $skos_concept_id,
            a.gdprov_entity_type = $gdprov_entity_type,
            a.confidence_score = $confidence_score,
            a.last_updated = $last_updated,
            a.embedding = vecf32($embedding),
            a.embedding_model = $embedding_model,
            a.embedding_timestamp = $embedding_timestamp
        """
        
        params = {
            'uri': article.uri,
            'article_id': article.article_id,
            'number': str(article.number),
            'title': article.title,
            'content': article.content,
            'regulation_type': article.regulation_type,
            'jurisdiction': article.jurisdiction,
            'financial_sector_relevance': article.financial_sector_relevance,
            'prov_entity_id': article.prov_entity_id,
            'skos_concept_id': article.skos_concept_id,
            'gdprov_entity_type': article.gdprov_entity_type,
            'confidence_score': article.confidence_score,
            'last_updated': article.last_updated.isoformat(),
            'embedding': article.embedding or [0.0] * self.config.EMBEDDING_DIMENSIONS,
            'embedding_model': article.embedding_model or self.config.EMBEDDING_MODEL,
            'embedding_timestamp': (article.embedding_timestamp or datetime.now()).isoformat()
        }
        
        self.graph.query(article_query, params)
    
    async def _create_article_skos_relationships(self, article: GDPRArticle):
        """Create SKOS relationships for article classification"""
        
        # Article hierarchy relationships
        relations = [
            ('broader_articles', 'skos_broader'),
            ('narrower_articles', 'skos_narrower'),
            ('related_articles', 'skos_related')
        ]
        
        for attr, relation_type in relations:
            related_articles = getattr(article, attr, [])
            for related_uri in related_articles:
                if related_uri:
                    query = f"""
                    MATCH (source:Article {{uri: $source_uri}})
                    MERGE (target:Article {{uri: $target_uri}})
                    MERGE (source)-[:{relation_type}]->(target)
                    """
                    
                    self.graph.query(query, {
                        'source_uri': article.uri,
                        'target_uri': related_uri
                    })
    
    async def _create_article_prov_relationships(self, article: GDPRArticle):
        """Create PROV-O relationships for article provenance"""
        
        # Article generation activity
        if article.was_generated_by:
            query = """
            MATCH (article:Article {uri: $article_uri})
            MERGE (activity:Activity {uri: $activity_uri})
            SET activity.prov_type = 'prov:Activity'
            MERGE (article)-[:prov_wasGeneratedBy]->(activity)
            """
            
            self.graph.query(query, {
                'article_uri': article.uri,
                'activity_uri': article.was_generated_by
            })
        
        # Article attribution
        for agent_uri in article.was_attributed_to:
            if agent_uri:
                query = """
                MATCH (article:Article {uri: $article_uri})
                MERGE (agent:Agent {uri: $agent_uri})
                SET agent.prov_type = 'prov:Agent'
                MERGE (article)-[:prov_wasAttributedTo]->(agent)
                """
                
                self.graph.query(query, {
                    'article_uri': article.uri,
                    'agent_uri': agent_uri
                })
        
        # Primary source
        if article.had_primary_source:
            query = """
            MATCH (article:Article {uri: $article_uri})
            MERGE (source:Entity {uri: $source_uri})
            SET source.prov_type = 'prov:Entity'
            MERGE (article)-[:prov_hadPrimarySource]->(source)
            """
            
            self.graph.query(query, {
                'article_uri': article.uri,
                'source_uri': article.had_primary_source
            })
    
    async def _create_article_gdprov_relationships(self, article: GDPRArticle):
        """Create GDPRov relationships for legal concepts"""
        
        # Processing activities
        for activity in article.processing_activities:
            if activity:
                query = """
                MATCH (article:Article {uri: $article_uri})
                MERGE (pa:ProcessingActivity {uri: $activity_uri})
                SET pa.gdprov_type = 'gdprov:ProcessingActivity',
                    pa.activity_type = $activity
                MERGE (article)-[:gdprov_definesProcessingActivity]->(pa)
                """
                
                activity_uri = f"{self.config.GDPROV_NAMESPACE}ProcessingActivity_{quote(activity)}"
                
                self.graph.query(query, {
                    'article_uri': article.uri,
                    'activity_uri': activity_uri,
                    'activity': activity
                })
        
        # Legal bases defined
        for legal_basis in article.legal_bases_defined:
            if legal_basis:
                query = """
                MATCH (article:Article {uri: $article_uri})
                MERGE (lb:LegalBasis {uri: $legal_basis_uri})
                SET lb.gdprov_type = 'gdprov:LegalBasis',
                    lb.legal_basis_type = $legal_basis
                MERGE (article)-[:gdprov_definesLegalBasis]->(lb)
                """
                
                legal_basis_uri = f"{self.config.GDPROV_NAMESPACE}LegalBasis_{legal_basis}"
                
                self.graph.query(query, {
                    'article_uri': article.uri,
                    'legal_basis_uri': legal_basis_uri,
                    'legal_basis': legal_basis
                })
        
        # Rights granted
        for right in article.rights_granted:
            if right:
                query = """
                MATCH (article:Article {uri: $article_uri})
                MERGE (dr:DataSubjectRight {uri: $right_uri})
                SET dr.gdprov_type = 'gdprov:DataSubjectRight',
                    dr.right_type = $right
                MERGE (article)-[:gdprov_grantsRight]->(dr)
                """
                
                right_uri = f"{self.config.GDPROV_NAMESPACE}DataSubjectRight_{quote(right)}"
                
                self.graph.query(query, {
                    'article_uri': article.uri,
                    'right_uri': right_uri,
                    'right': right
                })
    
    async def vector_similarity_search_with_ontology(self, query_embedding: List[float], 
                                                   node_type: str = "Concept", 
                                                   limit: int = 10,
                                                   ontology_filter: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """Enhanced vector similarity search with ontology filtering"""
        try:
            # Base vector search query
            base_query = f"""
            CALL db.idx.vector.queryNodes('{node_type}', 'embedding', $limit, vecf32($query_embedding))
            YIELD node, score
            """
            
            # Add ontology filtering if specified
            if ontology_filter:
                filter_conditions = []
                for key, value in ontology_filter.items():
                    if isinstance(value, list):
                        filter_conditions.append(f"node.{key} IN {value}")
                    else:
                        filter_conditions.append(f"node.{key} = '{value}'")
                
                if filter_conditions:
                    base_query += f" WHERE {' AND '.join(filter_conditions)}"
            
            base_query += " RETURN node, score ORDER BY score DESC"
            
            result = self.graph.query(base_query, {
                'query_embedding': query_embedding,
                'limit': limit
            })
            
            return [{'node': record[0], 'score': record[1]} for record in result.result_set]
            
        except Exception as e:
            logging.error(f"‚ùå Error in vector similarity search: {e}")
            return []
    
    async def get_ontology_statistics(self) -> Dict[str, Any]:
        """Get comprehensive ontology statistics"""
        try:
            stats = {}
            
            # Core node type counts
            node_queries = {
                'total_concepts': "MATCH (n:Concept) RETURN count(n)",
                'total_articles': "MATCH (n:Article) RETURN count(n)",
                'total_entities': "MATCH (n:Entity) RETURN count(n)",
                'total_activities': "MATCH (n:Activity) RETURN count(n)",
                'total_agents': "MATCH (n:Agent) RETURN count(n)",
                'total_documents': "MATCH (n:Document) RETURN count(n)"
            }
            
            for stat_name, query in node_queries.items():
                try:
                    result = self.graph.query(query)
                    stats[stat_name] = result.result_set[0][0] if result.result_set else 0
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Error executing stat query {stat_name}: {e}")
                    stats[stat_name] = 0
            
            # Relationship type counts
            relationship_queries = {
                'skos_relationships': "MATCH ()-[r]->() WHERE type(r) STARTS WITH 'skos_' RETURN count(r)",
                'prov_relationships': "MATCH ()-[r]->() WHERE type(r) STARTS WITH 'prov_' RETURN count(r)",
                'gdprov_relationships': "MATCH ()-[r]->() WHERE type(r) STARTS WITH 'gdprov_' RETURN count(r)",
                'total_relationships': "MATCH ()-[r]->() RETURN count(r)"
            }
            
            for stat_name, query in relationship_queries.items():
                try:
                    result = self.graph.query(query)
                    stats[stat_name] = result.result_set[0][0] if result.result_set else 0
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Error executing relationship query {stat_name}: {e}")
                    stats[stat_name] = 0
            
            # Ontology-specific statistics
            ontology_queries = {
                'concepts_with_embeddings': "MATCH (n:Concept) WHERE n.embedding IS NOT NULL RETURN count(n)",
                'articles_with_embeddings': "MATCH (n:Article) WHERE n.embedding IS NOT NULL RETURN count(n)",
                'skos_concept_schemes': "MATCH (n:ConceptScheme) RETURN count(n)",
                'gdprov_legal_bases': "MATCH (n:LegalBasis) RETURN count(n)",
                'gdprov_processing_activities': "MATCH (n:ProcessingActivity) RETURN count(n)"
            }
            
            for stat_name, query in ontology_queries.items():
                try:
                    result = self.graph.query(query)
                    stats[stat_name] = result.result_set[0][0] if result.result_set else 0
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Error executing ontology query {stat_name}: {e}")
                    stats[stat_name] = 0
            
            # Computed statistics
            if stats['total_concepts'] > 0:
                stats['concept_embedding_coverage'] = (stats['concepts_with_embeddings'] / stats['total_concepts']) * 100
            else:
                stats['concept_embedding_coverage'] = 0
                
            if stats['total_articles'] > 0:
                stats['article_embedding_coverage'] = (stats['articles_with_embeddings'] / stats['total_articles']) * 100
            else:
                stats['article_embedding_coverage'] = 0
            
            return stats
            
        except Exception as e:
            logging.error(f"‚ùå Error getting ontology statistics: {e}")
            return {}

# ============================================================================
# ENHANCED CONCEPT EXTRACTOR WITH ONTOLOGY INTEGRATION
# ============================================================================

class OntologyAwareConceptExtractor:
    """Enhanced concept extractor with comprehensive ontology integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
        self.embeddings = DirectOpenAIEmbeddings(config)
    
    async def extract_ontology_concepts(self, documents: List[Document]) -> List[SemanticConcept]:
        """Extract concepts with comprehensive ontology integration"""
        
        print(f"üöÄ Starting ontology-aware concept extraction from {len(documents)} documents")
        
        # Analyze extraction strategy with ontology patterns
        extraction_strategy = await self._analyze_ontology_extraction_strategy(documents)
        print(f"‚úÖ Ontology extraction strategy: {json.dumps(extraction_strategy, indent=2)}")
        
        extraction_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR expert with deep knowledge of semantic ontologies including PROV-O, GDPRov, GConsent, PrOnto, and SKOS.
        
        Extract comprehensive semantic concepts from this regulatory text using full ontology integration:
        
        Extraction Strategy:
        {extraction_strategy}
        
        Text: {text}
        
        For each concept, provide:
        
        1. SKOS Properties:
           - Preferred and alternative labels (with language tags)
           - Hierarchical relationships (broader/narrower)
           - Associative relationships (related)
           - Concept scheme membership
        
        2. PROV-O Properties:
           - Entity type (prov:Entity, prov:Activity, prov:Agent)
           - Generation activities
           - Derivation sources
           - Attribution agents
        
        3. GDPRov Properties:
           - Legal basis classifications
           - Processing purposes
           - Data categories
        
        4. GConsent Properties (if consent-related):
           - Consent types and mechanisms
           - Consent status information
        
        5. PrOnto Properties:
           - Legal obligations
           - Privacy rights
           - Deontic specifications
        
        Return as comprehensive JSON following this structure:
        {{
            "concepts": [
                {{
                    "concept_id": "unique_identifier",
                    "uri": "full_URI_with_namespace",
                    "label": "primary_label",
                    "definition": "comprehensive_definition",
                    
                    "skos_type": "skos:Concept",
                    "pref_label": {{"en": "English_label", "es": "Spanish_label"}},
                    "alt_labels": [{{"en": "alternative_label"}}],
                    "broader_concepts": ["uri1", "uri2"],
                    "narrower_concepts": ["uri3"],
                    "related_concepts": ["uri4"],
                    "in_scheme": ["scheme_uri"],
                    
                    "prov_type": "prov:Entity",
                    "was_generated_by": "activity_uri",
                    "was_derived_from": ["source_uri"],
                    "was_attributed_to": ["agent_uri"],
                    "had_primary_source": ["document_uri"],
                    
                    "gdprov_type": "gdprov_classification",
                    "legal_basis": ["consent", "legitimate_interest"],
                    "processing_purpose": ["purpose1", "purpose2"],
                    "data_category": "personal_data",
                    
                    "consent_type": "explicit_consent",
                    "consent_status": "active",
                    "consent_mechanism": "opt_in",
                    
                    "pronto_type": "privacy_concept",
                    "legal_obligation": ["obligation1"],
                    "privacy_right": ["right_to_access"],
                    
                    "article_references": ["Article_6", "Article_7"],
                    "regulation_type": "GDPR",
                    "jurisdiction": "EU",
                    "territorial_scope": ["EU", "EEA"],
                    
                    "industry_definitions": {{"financial": "definition"}},
                    "compliance_requirements": ["requirement1"],
                    "ropa_relevance": "high",
                    "financial_sector_impact": "significant"
                }}
            ]
        }}
        """)
        
        concepts = []
        concept_map = {}
        
        print(f"üìÑ Processing {len(documents)} documents for ontology-aware extraction...")
        
        for doc_idx, doc in enumerate(documents):
            print(f"\nüìã Processing document {doc_idx + 1}/{len(documents)}")
            print(f"üìÑ Document source: {doc.metadata.get('source', 'unknown')}")
            
            try:
                text_chunk = doc.page_content[:8000]
                
                messages = extraction_prompt.format_messages(
                    text=text_chunk,
                    extraction_strategy=json.dumps(extraction_strategy, indent=2)
                )
                
                print(f"ü§ñ Extracting ontology concepts...")
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    await self._process_ontology_concepts(
                        extracted_data, doc, concept_map, extraction_strategy
                    )
                        
                except json.JSONDecodeError as e:
                    print(f"‚ùå JSON parsing failed: {e}")
                    logging.warning(f"‚ö†Ô∏è Failed to parse ontology concept extraction: {e}")
                    
            except Exception as e:
                print(f"‚ùå Error extracting concepts from document {doc_idx + 1}: {e}")
                logging.error(f"‚ùå Error extracting ontology concepts: {e}")
        
        final_concepts = list(concept_map.values())
        print(f"\nüèÅ Ontology concept extraction completed!")
        print(f"üìä Total concepts extracted: {len(final_concepts)}")
        
        return final_concepts
    
    async def _analyze_ontology_extraction_strategy(self, documents: List[Document]) -> Dict[str, Any]:
        """Analyze documents for ontology-aware extraction strategy"""
        
        sample_content = "\n".join([doc.page_content[:1000] for doc in documents[:3]])
        document_types = [doc.metadata.get("document_type", "unknown") for doc in documents]
        
        strategy_prompt = ChatPromptTemplate.from_template("""
        You are an expert in semantic ontologies and knowledge extraction for GDPR compliance.
        
        Analyze this sample content and determine the optimal extraction strategy integrating these ontologies:
        - PROV-O: Provenance (Entity, Activity, Agent patterns)
        - GDPRov: GDPR provenance extension
        - GConsent: Consent modeling
        - PrOnto: Privacy rights legal reasoning
        - SKOS: Knowledge organization systems
        - OWL: Web ontology language patterns
        
        Document Types: {document_types}
        
        Sample Content:
        {sample_content}
        
        Return comprehensive strategy covering all ontology patterns:
        {{
            "ontology_integration": {{
                "prov_o_patterns": ["Entity", "Activity", "Agent", "wasGeneratedBy", "wasDerivedFrom"],
                "skos_patterns": ["Concept", "broader", "narrower", "related", "inScheme"],
                "gdprov_patterns": ["LegalBasis", "ProcessingActivity", "ConsentAgreement"],
                "gconsent_patterns": ["ConsentStatus", "ConsentMechanism", "ConsentType"],
                "pronto_patterns": ["LegalObligation", "PrivacyRight", "DeonticSpecification"]
            }},
            "concept_properties": {{
                "required": ["uri", "skos_type", "prov_type", "definition"],
                "ontology_specific": ["gdprov_type", "legal_basis", "consent_type", "pronto_type"],
                "relationships": ["broader_concepts", "was_derived_from", "legal_obligation"]
            }},
            "extraction_priorities": [
                "legal_concepts_with_prov_provenance",
                "consent_mechanisms_with_gconsent_patterns", 
                "privacy_rights_with_pronto_classification",
                "hierarchical_relationships_with_skos"
            ],
            "namespace_assignments": {{
                "gdpr_concepts": "https://w3id.org/GDPRtEXT#",
                "consent_concepts": "https://w3id.org/GConsent#",
                "privacy_concepts": "https://w3id.org/PrOnto#"
            }}
        }}
        """)
        
        try:
            messages = strategy_prompt.format_messages(
                document_types=document_types,
                sample_content=sample_content[:4000]
            )
            response = await self.llm.ainvoke(messages)
            strategy = json.loads(response.content)
            
            logging.info(f"üß† Ontology-aware extraction strategy completed")
            return strategy
            
        except Exception as e:
            logging.error(f"‚ùå Error analyzing ontology extraction strategy: {e}")
            # Fallback strategy with basic ontology patterns
            return {
                "ontology_integration": {
                    "prov_o_patterns": ["Entity", "Activity", "Agent"],
                    "skos_patterns": ["Concept", "broader", "narrower"],
                    "gdprov_patterns": ["LegalBasis"],
                    "gconsent_patterns": ["ConsentStatus"],
                    "pronto_patterns": ["PrivacyRight"]
                },
                "concept_properties": {
                    "required": ["uri", "skos_type", "definition"],
                    "ontology_specific": ["gdprov_type", "legal_basis"],
                    "relationships": ["broader_concepts"]
                },
                "extraction_priorities": ["legal_concepts_with_provenance"],
                "namespace_assignments": {
                    "gdpr_concepts": "https://w3id.org/GDPRtEXT#"
                }
            }
    
    async def _process_ontology_concepts(self, extracted_data: Dict, doc: Document, 
                                       concept_map: Dict, strategy: Dict):
        """Process extracted concepts with full ontology integration"""
        
        concepts_data = extracted_data.get("concepts", [])
        print(f"üî¢ Processing {len(concepts_data)} ontology concepts")
        
        for i, concept_data in enumerate(concepts_data):
            try:
                concept_id = concept_data.get("concept_id")
                if not concept_id:
                    print(f"‚ö†Ô∏è Skipping concept without ID")
                    continue
                
                print(f"üìù Processing ontology concept {i+1}: {concept_id}")
                
                if concept_id not in concept_map:
                    # Generate embedding
                    concept_text = f"{concept_data.get('label', '')}: {concept_data.get('definition', '')}"
                    embedding = await self.embeddings.embed_text(concept_text)
                    
                    # Create document provenance
                    provenance = DocumentProvenance(
                        source_document=doc.metadata.get("source", ""),
                        document_id=doc.metadata.get("document_id", str(uuid.uuid4())),
                        chunk_id=doc.metadata.get("chunk_id", ""),
                        page_number=doc.metadata.get("page_number"),
                        extraction_timestamp=datetime.now(timezone.utc),
                        extraction_method="ontology_aware_llm_extraction",
                        confidence_score=0.85,
                        prov_entity_id=f"{self.config.PROV_O_NAMESPACE}Entity_{uuid.uuid4()}",
                        prov_activity_id=f"{self.config.PROV_O_NAMESPACE}Activity_{uuid.uuid4()}",
                        prov_agent_id=f"{self.config.PROV_O_NAMESPACE}Agent_LLM_Extractor"
                    )
                    
                    # Create comprehensive semantic concept with all ontology properties
                    concept = SemanticConcept(
                        # Core identification
                        concept_id=concept_id,
                        uri=concept_data.get("uri", f"{self.config.GDPR_NAMESPACE}{concept_id}"),
                        label=concept_data.get("label", ""),
                        definition=concept_data.get("definition", ""),
                        
                        # SKOS Properties
                        skos_type=concept_data.get("skos_type", "skos:Concept"),
                        pref_label=concept_data.get("pref_label", {"en": concept_data.get("label", "")}),
                        alt_labels=concept_data.get("alt_labels", []),
                        notation=concept_data.get("notation"),
                        
                        # SKOS Semantic Relations
                        broader_concepts=concept_data.get("broader_concepts", []),
                        narrower_concepts=concept_data.get("narrower_concepts", []),
                        related_concepts=concept_data.get("related_concepts", []),
                        top_concept_of=concept_data.get("top_concept_of", []),
                        in_scheme=concept_data.get("in_scheme", []),
                        
                        # SKOS Mapping Properties
                        exact_match=concept_data.get("exact_match", []),
                        close_match=concept_data.get("close_match", []),
                        broad_match=concept_data.get("broad_match", []),
                        narrow_match=concept_data.get("narrow_match", []),
                        related_match=concept_data.get("related_match", []),
                        
                        # PROV-O Properties
                        prov_type=concept_data.get("prov_type", "prov:Entity"),
                        was_generated_by=concept_data.get("was_generated_by"),
                        was_derived_from=concept_data.get("was_derived_from", []),
                        was_attributed_to=concept_data.get("was_attributed_to", []),
                        had_primary_source=concept_data.get("had_primary_source", []),
                        
                        # GDPRov Properties
                        gdprov_type=concept_data.get("gdprov_type"),
                        legal_basis=concept_data.get("legal_basis", []),
                        processing_purpose=concept_data.get("processing_purpose", []),
                        data_category=concept_data.get("data_category"),
                        
                        # GConsent Properties
                        consent_type=concept_data.get("consent_type"),
                        consent_status=concept_data.get("consent_status"),
                        consent_mechanism=concept_data.get("consent_mechanism"),
                        
                        # PrOnto Properties
                        pronto_type=concept_data.get("pronto_type"),
                        legal_obligation=concept_data.get("legal_obligation", []),
                        privacy_right=concept_data.get("privacy_right", []),
                        
                        # GDPR Specific
                        article_references=concept_data.get("article_references", []),
                        regulation_type=concept_data.get("regulation_type", "GDPR"),
                        jurisdiction=concept_data.get("jurisdiction", "EU"),
                        territorial_scope=concept_data.get("territorial_scope", []),
                        
                        # Industry Context
                        industry_definitions=concept_data.get("industry_definitions", {}),
                        compliance_requirements=concept_data.get("compliance_requirements", []),
                        ropa_relevance=concept_data.get("ropa_relevance", "medium"),
                        financial_sector_impact=concept_data.get("financial_sector_impact", "medium"),
                        
                        # Document Provenance
                        provenance=provenance,
                        confidence_score=0.85,
                        last_updated=datetime.now(timezone.utc),
                        
                        # Vector embedding
                        embedding=embedding,
                        embedding_model=self.config.EMBEDDING_MODEL,
                        embedding_timestamp=datetime.now(timezone.utc)
                    )
                    
                    concept_map[concept_id] = concept
                    print(f"‚úÖ Created ontology concept: {concept.label}")
                else:
                    print(f"‚è≠Ô∏è Concept {concept_id} already exists")
                    
            except Exception as e:
                print(f"‚ùå Error processing concept: {e}")
                logging.error(f"‚ùå Error processing ontology concept: {e}")
                continue
        
        print(f"üèÅ Finished processing concepts. Total: {len(concept_map)}")
    
    async def extract_gdpr_articles_with_ontology(self, documents: List[Document]) -> List[GDPRArticle]:
        """Extract GDPR articles with comprehensive ontology integration"""
        
        article_strategy = await self._analyze_article_ontology_strategy(documents)
        
        article_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR regulatory expert with deep knowledge of semantic ontologies.
        
        Extract GDPR/UK GDPR articles with comprehensive ontology integration:
        
        Ontology Strategy:
        {article_strategy}
        
        Text: {text}
        
        For each article, provide complete ontology integration including:
        
        1. PROV-O provenance (generation, attribution, sources)
        2. SKOS classification (hierarchical relationships)
        3. GDPRov legal concepts (processing activities, legal bases, rights)
        4. GConsent patterns (if consent-related)
        5. PrOnto legal reasoning (obligations, rights, deontic specs)
        
        Return as comprehensive JSON with full URIs and ontology properties.
        """)
        
        articles = []
        article_map = {}
        
        regulation_docs = [doc for doc in documents 
                          if "GDPR" in doc.metadata.get("document_type", "")]
        
        for doc in regulation_docs:
            try:
                messages = article_prompt.format_messages(
                    text=doc.page_content[:8000],
                    article_strategy=json.dumps(article_strategy, indent=2)
                )
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    await self._process_ontology_articles(
                        extracted_data, doc, article_map, article_strategy
                    )
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"‚ö†Ô∏è Failed to parse ontology article extraction: {e}")
                    
            except Exception as e:
                logging.error(f"‚ùå Error extracting ontology articles: {e}")
        
        return list(article_map.values())
    
    async def _analyze_article_ontology_strategy(self, documents: List[Document]) -> Dict[str, Any]:
        """Analyze article extraction strategy with ontology patterns"""
        
        sample_content = "\n".join([
            doc.page_content[:1000] for doc in documents 
            if "GDPR" in doc.metadata.get("document_type", "")
        ][:2])
        
        strategy_prompt = ChatPromptTemplate.from_template("""
        Analyze GDPR regulatory content for comprehensive ontology-aware article extraction.
        
        Integration requirements:
        - PROV-O: Document provenance and article generation
        - SKOS: Article classification and hierarchical relationships
        - GDPRov: Legal concepts, processing activities, legal bases
        - GConsent: Consent mechanisms and requirements
        - PrOnto: Legal obligations and privacy rights
        
        Sample Content:
        {sample_content}
        
        Return extraction strategy with full ontology integration patterns.
        """)
        
        try:
            messages = strategy_prompt.format_messages(sample_content=sample_content[:4000])
            response = await self.llm.ainvoke(messages)
            strategy = json.loads(response.content)
            
            logging.info(f"üß† Article ontology strategy completed")
            return strategy
            
        except Exception as e:
            logging.error(f"‚ùå Error analyzing article ontology strategy: {e}")
            return {
                "ontology_patterns": ["prov_provenance", "skos_classification", "gdprov_legal_concepts"],
                "required_properties": ["uri", "prov_entity_id", "skos_concept_id", "gdprov_entity_type"]
            }
    
    async def _process_ontology_articles(self, extracted_data: Dict, doc: Document, 
                                       article_map: Dict, strategy: Dict):
        """Process extracted articles with full ontology integration"""
        
        articles_data = extracted_data.get("articles", [])
        print(f"üî¢ Processing {len(articles_data)} ontology articles")
        
        for i, article_data in enumerate(articles_data):
            try:
                article_id = article_data.get("article_id")
                if not article_id:
                    continue
                
                print(f"üìù Processing ontology article {i+1}: {article_id}")
                
                if article_id not in article_map:
                    # Generate embedding
                    article_text = f"{article_data.get('title', '')}: {article_data.get('content', '')}"
                    embedding = await self.embeddings.embed_text(article_text)
                    
                    # Create document provenance
                    provenance = DocumentProvenance(
                        source_document=doc.metadata.get("source", ""),
                        document_id=doc.metadata.get("document_id", str(uuid.uuid4())),
                        chunk_id=doc.metadata.get("chunk_id", ""),
                        page_number=doc.metadata.get("page_number"),
                        extraction_timestamp=datetime.now(timezone.utc),
                        extraction_method="ontology_aware_article_extraction",
                        confidence_score=0.9,
                        prov_entity_id=f"{self.config.PROV_O_NAMESPACE}Entity_{uuid.uuid4()}",
                        prov_activity_id=f"{self.config.PROV_O_NAMESPACE}Activity_{uuid.uuid4()}",
                        prov_agent_id=f"{self.config.PROV_O_NAMESPACE}Agent_LLM_Extractor"
                    )
                    
                    # Create comprehensive GDPR article
                    article = GDPRArticle(
                        # Core identification
                        article_id=article_id,
                        uri=article_data.get("uri", f"{self.config.GDPR_NAMESPACE}Article_{article_id}"),
                        number=article_data.get("number", ""),
                        title=article_data.get("title", ""),
                        content=article_data.get("content", ""),
                        
                        # PROV-O Properties
                        prov_entity_id=f"{self.config.PROV_O_NAMESPACE}Entity_{article_id}",
                        was_generated_by=article_data.get("was_generated_by", 
                                                        f"{self.config.PROV_O_NAMESPACE}Activity_Regulation_Drafting"),
                        was_attributed_to=article_data.get("was_attributed_to", 
                                                         [f"{self.config.PROV_O_NAMESPACE}Agent_EU_Commission"]),
                        had_primary_source=article_data.get("had_primary_source",
                                                          f"{self.config.GDPR_NAMESPACE}GDPR_Regulation"),
                        
                        # SKOS Properties
                        skos_concept_id=f"{self.config.SKOS_NAMESPACE}Concept_{article_id}",
                        broader_articles=article_data.get("broader_articles", []),
                        narrower_articles=article_data.get("narrower_articles", []),
                        related_articles=article_data.get("related_articles", []),
                        
                        # GDPRov Properties
                        gdprov_entity_type=article_data.get("gdprov_entity_type", "gdprov:LegalArticle"),
                        processing_activities=article_data.get("processing_activities", []),
                        legal_bases_defined=article_data.get("legal_bases_defined", []),
                        obligations_specified=article_data.get("obligations_specified", []),
                        rights_granted=article_data.get("rights_granted", []),
                        
                        # GConsent Properties
                        consent_requirements=article_data.get("consent_requirements", []),
                        consent_conditions=article_data.get("consent_conditions", []),
                        
                        # PrOnto Properties
                        legal_concepts=article_data.get("legal_concepts", []),
                        deontic_specifications=article_data.get("deontic_specifications", []),
                        
                        # Regulatory Context
                        regulation_type=article_data.get("regulation_type", "GDPR"),
                        jurisdiction=article_data.get("jurisdiction", "EU"),
                        territorial_scope=article_data.get("territorial_scope", []),
                        cross_border_implications=article_data.get("cross_border_implications", []),
                        
                        # Semantic Analysis
                        concepts=article_data.get("concepts", []),
                        obligations=article_data.get("obligations", []),
                        rights=article_data.get("rights", []),
                        legal_bases=article_data.get("legal_bases", []),
                        
                        # Industry Context
                        financial_sector_relevance=article_data.get("financial_sector_relevance", "medium"),
                        ropa_implications=article_data.get("ropa_implications", []),
                        
                        # Relationships
                        implements_principles=article_data.get("implements_principles", []),
                        references_guidelines=article_data.get("references_guidelines", []),
                        case_law_references=article_data.get("case_law_references", []),
                        
                        # Document Provenance
                        provenance=provenance,
                        confidence_score=0.9,
                        last_updated=datetime.now(timezone.utc),
                        
                        # Vector embedding
                        embedding=embedding,
                        embedding_model=self.config.EMBEDDING_MODEL,
                        embedding_timestamp=datetime.now(timezone.utc)
                    )
                    
                    article_map[article_id] = article
                    print(f"‚úÖ Created ontology article: {article.title}")
                else:
                    print(f"‚è≠Ô∏è Article {article_id} already exists")
                    
            except Exception as e:
                print(f"‚ùå Error processing article: {e}")
                logging.error(f"‚ùå Error processing ontology article: {e}")
                continue
        
        print(f"üèÅ Finished processing articles. Total: {len(article_map)}")

# ============================================================================
# DIRECT OPENAI EMBEDDINGS (UNCHANGED BUT ENHANCED)
# ============================================================================

class DirectOpenAIEmbeddings:
    """Direct OpenAI API embeddings without tiktoken overhead"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = AsyncOpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL
        )
        self.model = config.EMBEDDING_MODEL
        self.dimensions = config.EMBEDDING_DIMENSIONS
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"Error generating embedding: {e}")
            return [0.0] * self.dimensions
    
    async def embed_texts(self, texts: List[str], batch_size: int = 20) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently with rate limiting"""
        embeddings = []
        
        print(f"üîÆ Generating embeddings for {len(texts)} texts in batches of {batch_size}")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            print(f"üîÆ Processing embedding batch {batch_num}/{total_batches} ({len(batch)} texts)")
            
            try:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                    dimensions=self.dimensions
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
                print(f"‚úÖ Batch {batch_num} completed: {len(batch_embeddings)} embeddings generated")
                
                # Progressive rate limiting
                if len(batch) >= batch_size:
                    delay = min(0.5, batch_size * 0.02)
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                print(f"‚ùå Error generating embeddings for batch {batch_num}: {e}")
                logging.error(f"Error generating embeddings for batch {i}: {e}")
                fallback_embeddings = [[0.0] * self.dimensions] * len(batch)
                embeddings.extend(fallback_embeddings)
                print(f"üîÑ Using fallback embeddings for batch {batch_num}")
        
        print(f"üèÅ Embedding generation completed: {len(embeddings)} total embeddings")
        return embeddings

# ============================================================================
# ENHANCED ORCHESTRATOR WITH FULL ONTOLOGY INTEGRATION
# ============================================================================

class EnhancedGDPRMetamodelOrchestrator:
    """Enhanced orchestrator with comprehensive ontology integration"""
    
    def __init__(self, config: Config):
        self.config = config
        config.validate()
        
        # Initialize enhanced components
        self.pdf_processor = MemoryEnhancedPDFProcessor(config)
        self.concept_extractor = OntologyAwareConceptExtractor(config)
        self.falkor_manager = EnhancedOntologyFalkorDBManager(config)
        self.metamodel_generator = GDPRMetamodelGenerator(config)
        self.elasticsearch_manager = MemoryEnhancedElasticsearchManager(config)
        self.embeddings = DirectOpenAIEmbeddings(config)
        
        self.setup_logging()
    
    def setup_logging(self):
        """Setup comprehensive logging"""
        self.config.OUTPUT_PATH.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.config.OUTPUT_PATH / 'enhanced_gdpr_metamodel.log'),
                logging.StreamHandler()
            ]
        )
    
    async def run_enhanced_metamodel_pipeline(self) -> Dict[str, Any]:
        """Run the complete enhanced metamodel generation pipeline with full ontology integration"""
        start_time = datetime.now()
        session_id = f"enhanced_gdpr_metamodel_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"üöÄ Starting Enhanced GDPR Metamodel Pipeline with Full Ontology Integration")
        print(f"üìã Session: {session_id}")
        print(f"üß† Ontologies: PROV-O, GDPRov, GConsent, PrOnto, SKOS, OWL")
        logging.info(f"üöÄ Starting Enhanced GDPR Metamodel Pipeline - Session: {session_id}")
        
        try:
            # Step 1: Process documents with enhanced memory
            print("\n" + "="*80)
            print("üìÑ STEP 1: Processing PDF documents with enhanced memory...")
            print("="*80)
            documents = await self._process_documents_enhanced(session_id)
            print(f"‚úÖ Processed {len(documents)} document chunks with provenance")
            
            # Step 2: Extract ontology-aware concepts and articles
            print("\n" + "="*80)
            print("üß† STEP 2: Extracting concepts with full ontology integration...")
            print("="*80)
            
            print("üîç Extracting ontology-aware concepts...")
            concepts = await self.concept_extractor.extract_ontology_concepts(documents)
            print(f"‚úÖ Extracted {len(concepts)} concepts with PROV-O, SKOS, GDPRov, GConsent, PrOnto")
            
            print("üìã Extracting ontology-aware articles...")
            articles = await self.concept_extractor.extract_gdpr_articles_with_ontology(documents)
            print(f"‚úÖ Extracted {len(articles)} articles with full ontology support")
            
            # Step 3: Create ontology-aware knowledge graph
            print("\n" + "="*80)
            print("üï∏Ô∏è STEP 3: Creating ontology-aware knowledge graph...")
            print("="*80)
            
            print("üèóÔ∏è Creating ontology schema with vector indexes...")
            await self.falkor_manager.create_ontology_aware_schema()
            
            print(f"üíæ Storing {len(concepts)} concepts with ontology integration...")
            concept_count = await self.falkor_manager.store_semantic_concepts_with_ontology(
                concepts, batch_size=self.config.FALKORDB_BATCH_SIZE
            )
            print(f"‚úÖ Stored {concept_count} concepts with SKOS, PROV-O, GDPRov relationships")
            
            print(f"üìö Storing {len(articles)} articles with ontology integration...")
            article_count = await self.falkor_manager.store_gdpr_articles_with_ontology(
                articles, batch_size=self.config.CONCEPT_BATCH_SIZE
            )
            print(f"‚úÖ Stored {article_count} articles with full ontology integration")
            
            # Step 4: Create enhanced vector index with ontology metadata
            print("\n" + "="*80)
            print("üîç STEP 4: Creating enhanced vector embeddings with ontology metadata...")
            print("="*80)
            await self._create_ontology_vector_index(documents, concepts, articles, session_id)
            
            # Step 5: Generate enhanced metamodel with ontology integration
            print("\n" + "="*80)
            print("üèóÔ∏è STEP 5: Generating enhanced GDPR metamodel with ontology patterns...")
            print("="*80)
            kg_stats = await self.falkor_manager.get_ontology_statistics()
            print(f"üìä Ontology knowledge graph statistics: {kg_stats}")
            metamodel = await self.metamodel_generator.generate_ontology_metamodel(concepts, articles, kg_stats)
            print(f"‚úÖ Generated enhanced metamodel with {len(metamodel)} ontology-integrated sections")
            
            # Step 6: Generate enhanced ROPA report with ontology compliance
            print("\n" + "="*80)
            print("üìã STEP 6: Generating ontology-enhanced ROPA compliance report...")
            print("="*80)
            ropa_report = await self.metamodel_generator.generate_ontology_ropa_report(metamodel, concepts, articles)
            print(f"‚úÖ Generated enhanced ROPA report with ontology compliance patterns")
            
            # Step 7: Save enhanced results with ontology metadata
            print("\n" + "="*80)
            print("üíæ STEP 7: Saving enhanced results with ontology metadata...")
            print("="*80)
            await self._save_enhanced_results(session_id, metamodel, ropa_report, concepts, articles, kg_stats)
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            results = {
                "status": "success",
                "session_id": session_id,
                "processing_time": processing_time,
                "ontology_integration": {
                    "prov_o_entities": len([c for c in concepts if c.prov_type]),
                    "skos_concepts": len([c for c in concepts if c.skos_type]),
                    "gdprov_elements": len([c for c in concepts if c.gdprov_type]),
                    "gconsent_elements": len([c for c in concepts if c.consent_type]),
                    "pronto_elements": len([c for c in concepts if c.pronto_type])
                },
                "statistics": {
                    "documents_processed": len(documents),
                    "concepts_extracted": len(concepts),
                    "articles_extracted": len(articles),
                    "knowledge_graph_nodes": kg_stats.get('total_concepts', 0) + kg_stats.get('total_articles', 0),
                    "ontology_relationships": kg_stats.get('skos_relationships', 0) + kg_stats.get('prov_relationships', 0) + kg_stats.get('gdprov_relationships', 0),
                    "vector_embeddings": kg_stats.get('concepts_with_embeddings', 0) + kg_stats.get('articles_with_embeddings', 0),
                    "provenance_chains": kg_stats.get('total_documents', 0)
                },
                "metamodel": metamodel,
                "ropa_report": ropa_report,
                "knowledge_graph_stats": kg_stats
            }
            
            print("\n" + "="*80)
            print("‚úÖ Enhanced GDPR Metamodel Pipeline completed successfully!")
            print("="*80)
            print(f"üß† Ontology Integration: PROV-O, GDPRov, GConsent, PrOnto, SKOS")
            print(f"üîó Semantic Relationships: {results['statistics']['ontology_relationships']}")
            print(f"üîÆ Vector Embeddings: {results['statistics']['vector_embeddings']}")
            logging.info("‚úÖ Enhanced GDPR Metamodel Pipeline completed successfully!")
            self._log_enhanced_results(results)
            
            return results
            
        except Exception as e:
            error_msg = f"Enhanced pipeline execution failed: {e}"
            print(f"\n‚ùå {error_msg}")
            logging.error(f"‚ùå {error_msg}")
            import traceback
            print(f"üìã Traceback: {traceback.format_exc()}")
            logging.error(f"‚ùå Traceback: {traceback.format_exc()}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "session_id": session_id,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
    
    async def _process_documents_enhanced(self, session_id: str) -> List[Document]:
        """Process documents with enhanced provenance tracking"""
        print(f"üìÅ Looking for PDF documents in: {self.config.DOCUMENTS_PATH}")
        documents = []
        pdf_files = list(self.config.DOCUMENTS_PATH.glob("*.pdf"))
        print(f"üìÑ Found {len(pdf_files)} PDF files: {[f.name for f in pdf_files]}")
        
        for i, pdf_file in enumerate(pdf_files):
            print(f"\nüìã Processing file {i+1}/{len(pdf_files)}: {pdf_file.name}")
            logging.info(f"üìÑ Processing with provenance: {pdf_file.name}")
            
            doc_info = await self.pdf_processor.process_with_memory(
                pdf_file, None, session_id
            )
            
            if "error" not in doc_info:
                chunks = doc_info.get("chunks", [])
                
                # Enhance chunks with PROV-O metadata
                enhanced_chunks = []
                for chunk in chunks:
                    # Add PROV-O provenance metadata
                    chunk.metadata.update({
                        "prov_entity_id": f"{self.config.PROV_O_NAMESPACE}Entity_{uuid.uuid4()}",
                        "prov_was_generated_by": f"{self.config.PROV_O_NAMESPACE}Activity_PDF_Processing",
                        "prov_had_primary_source": str(pdf_file),
                        "document_id": str(uuid.uuid4()),
                        "extraction_timestamp": datetime.now(timezone.utc).isoformat(),
                        "provenance_complete": True
                    })
                    enhanced_chunks.append(chunk)
                
                print(f"‚úÖ Processed {pdf_file.name}: {len(enhanced_chunks)} chunks with provenance")
                documents.extend(enhanced_chunks)
            else:
                error_msg = doc_info.get('error', 'Unknown error')
                print(f"‚ùå Failed to process {pdf_file.name}: {error_msg}")
                logging.error(f"‚ùå Failed to process {pdf_file.name}: {error_msg}")
        
        print(f"\nüèÅ Enhanced document processing completed!")
        print(f"üìä Total documents: {len(pdf_files)}")
        print(f"üìÑ Total chunks with provenance: {len(documents)}")
        return documents
    
    async def _create_ontology_vector_index(self, documents: List[Document], 
                                          concepts: List[SemanticConcept],
                                          articles: List[GDPRArticle],
                                          session_id: str):
        """Create vector index with full ontology metadata"""
        print(f"üîç Creating ontology-aware vector index for {len(documents)} documents")
        await self.elasticsearch_manager.create_enhanced_index()
        
        batch_size = 50
        embedding_batch_size = 20
        
        print(f"üìä Processing with ontology metadata in batches of {batch_size}")
        
        all_indexed_docs = []
        
        for batch_start in range(0, len(documents), batch_size):
            batch_end = min(batch_start + batch_size, len(documents))
            doc_batch = documents[batch_start:batch_end]
            
            print(f"üì¶ Processing ontology document batch {batch_start//batch_size + 1}: {batch_start+1}-{batch_end}")
            
            # Prepare documents with ontology-enhanced embeddings
            indexed_docs = await self._prepare_ontology_documents(
                doc_batch, concepts, articles, session_id, embedding_batch_size
            )
            
            all_indexed_docs.extend(indexed_docs)
            
            # Store batch with ontology metadata
            if indexed_docs:
                ontology_insights = {
                    "prov_provenance": True,
                    "skos_classification": True,
                    "gdprov_compliance": True,
                    "session_context": {"session_id": session_id}
                }
                
                try:
                    result = await self.elasticsearch_manager.store_with_memory(
                        indexed_docs, session_id, ontology_insights
                    )
                    print(f"‚úÖ Stored ontology batch: {result.get('success', 0)} docs")
                except Exception as e:
                    print(f"‚ùå Error storing ontology batch: {e}")
                    logging.error(f"‚ùå Error storing ontology document batch: {e}")
            
            await asyncio.sleep(0.2)
        
        print(f"üèÅ Ontology vector indexing completed: {len(all_indexed_docs)} documents")
    
    async def _prepare_ontology_documents(self, documents: List[Document], 
                                        concepts: List[SemanticConcept],
                                        articles: List[GDPRArticle],
                                        session_id: str, 
                                        embedding_batch_size: int) -> List[Dict[str, Any]]:
        """Prepare documents with ontology-enhanced embeddings"""
        print(f"üîÆ Generating ontology-aware embeddings for {len(documents)} documents")
        
        indexed_docs = []
        
        # Extract texts for embedding generation
        texts = [doc.page_content for doc in documents]
        
        # Generate embeddings in batches
        all_embeddings = []
        for emb_batch_start in range(0, len(texts), embedding_batch_size):
            emb_batch_end = min(emb_batch_start + embedding_batch_size, len(texts))
            text_batch = texts[emb_batch_start:emb_batch_end]
            
            print(f"üîÆ Generating ontology embeddings: {emb_batch_start+1}-{emb_batch_end}")
            
            try:
                batch_embeddings = await self.embeddings.embed_texts(text_batch, batch_size=embedding_batch_size)
                all_embeddings.extend(batch_embeddings)
                print(f"‚úÖ Generated {len(batch_embeddings)} ontology embeddings")
                await asyncio.sleep(0.1)
                
            except Exception as e:
                print(f"‚ùå Error generating ontology embeddings: {e}")
                fallback_embeddings = [[0.0] * self.config.EMBEDDING_DIMENSIONS] * len(text_batch)
                all_embeddings.extend(fallback_embeddings)
                logging.error(f"‚ùå Ontology embedding generation failed: {e}")
        
        # Create ontology-enhanced indexed documents
        for doc, embedding in zip(documents, all_embeddings):
            # Find related concepts and articles for this document
            related_concepts = self._find_related_concepts(doc, concepts)
            related_articles = self._find_related_articles(doc, articles)
            
            # Enhanced document with full ontology metadata
            indexed_doc = {
                "document_id": f"doc_{uuid.uuid4()}",
                "content": doc.page_content,
                "embeddings": embedding,
                
                # PROV-O metadata
                "prov_entity_id": doc.metadata.get("prov_entity_id"),
                "prov_was_generated_by": doc.metadata.get("prov_was_generated_by"),
                "prov_had_primary_source": doc.metadata.get("prov_had_primary_source"),
                
                # Ontology relationships
                "related_concepts": [c.uri for c in related_concepts],
                "related_articles": [a.uri for a in related_articles],
                "concept_count": len(related_concepts),
                "article_count": len(related_articles),
                
                # SKOS metadata
                "skos_broader_concepts": [uri for c in related_concepts for uri in c.broader_concepts],
                "skos_schemes": [uri for c in related_concepts for uri in c.in_scheme],
                
                # GDPRov metadata
                "gdprov_legal_bases": [basis for c in related_concepts for basis in c.legal_basis],
                "gdprov_processing_purposes": [purpose for c in related_concepts for purpose in c.processing_purpose],
                
                # Enhanced session metadata
                "session_id": session_id,
                "ontology_enhanced": True,
                "provenance_complete": doc.metadata.get("provenance_complete", False),
                
                # Standard document fields
                "document_type": doc.metadata.get("document_type", "unknown"),
                "source_file": doc.metadata.get("source", ""),
                "chunk_index": doc.metadata.get("chunk_index", 0),
                "extracted_at": datetime.now().isoformat(),
                "confidence_score": 0.85
            }
            
            indexed_docs.append(indexed_doc)
        
        print(f"‚úÖ Prepared {len(indexed_docs)} ontology-enhanced documents")
        return indexed_docs
    
    def _find_related_concepts(self, doc: Document, concepts: List[SemanticConcept]) -> List[SemanticConcept]:
        """Find concepts related to document content"""
        doc_text = doc.page_content.lower()
        related = []
        
        for concept in concepts:
            # Check if concept terms appear in document
            if (concept.label.lower() in doc_text or 
                any(keyword.lower() in doc_text for keyword in concept.definition.split()[:5])):
                related.append(concept)
        
        return related[:10]  # Limit to top 10 related concepts
    
    def _find_related_articles(self, doc: Document, articles: List[GDPRArticle]) -> List[GDPRArticle]:
        """Find articles related to document content"""
        doc_text = doc.page_content.lower()
        related = []
        
        for article in articles:
            # Check if article references appear in document
            if (f"article {article.number}".lower() in doc_text or
                any(keyword.lower() in doc_text for keyword in article.title.split()[:3])):
                related.append(article)
        
        return related[:5]  # Limit to top 5 related articles
    
    async def _save_enhanced_results(self, session_id: str, metamodel: Dict[str, Any], 
                                   ropa_report: Dict[str, Any], concepts: List[SemanticConcept],
                                   articles: List[GDPRArticle], kg_stats: Dict[str, Any]):
        """Save enhanced results with ontology metadata"""
        
        # Save enhanced metamodel
        metamodel_file = self.config.OUTPUT_PATH / f"{session_id}_enhanced_metamodel.json"
        async with aiofiles.open(metamodel_file, 'w') as f:
            await f.write(json.dumps(metamodel, indent=2))
        
        # Save enhanced ROPA report
        ropa_file = self.config.OUTPUT_PATH / f"{session_id}_enhanced_ropa_report.json"
        async with aiofiles.open(ropa_file, 'w') as f:
            await f.write(json.dumps(ropa_report, indent=2))
        
        # Save enhanced concepts with full ontology data
        concepts_file = self.config.OUTPUT_PATH / f"{session_id}_ontology_concepts.json"
        enhanced_concepts_data = []
        for c in concepts:
            concept_dict = asdict(c)
            # Convert datetime objects
            concept_dict['last_updated'] = c.last_updated.isoformat()
            concept_dict['embedding_timestamp'] = c.embedding_timestamp.isoformat() if c.embedding_timestamp else None
            concept_dict['provenance'] = asdict(c.provenance)
            concept_dict['provenance']['extraction_timestamp'] = c.provenance.extraction_timestamp.isoformat()
            enhanced_concepts_data.append(concept_dict)
        
        async with aiofiles.open(concepts_file, 'w') as f:
            await f.write(json.dumps(enhanced_concepts_data, indent=2))
        
        # Save enhanced articles with full ontology data
        articles_file = self.config.OUTPUT_PATH / f"{session_id}_ontology_articles.json"
        enhanced_articles_data = []
        for a in articles:
            article_dict = asdict(a)
            # Convert datetime objects
            article_dict['last_updated'] = a.last_updated.isoformat()
            article_dict['embedding_timestamp'] = a.embedding_timestamp.isoformat() if a.embedding_timestamp else None
            article_dict['provenance'] = asdict(a.provenance)
            article_dict['provenance']['extraction_timestamp'] = a.provenance.extraction_timestamp.isoformat()
            enhanced_articles_data.append(article_dict)
        
        async with aiofiles.open(articles_file, 'w') as f:
            await f.write(json.dumps(enhanced_articles_data, indent=2))
        
        # Save enhanced knowledge graph statistics
        stats_file = self.config.OUTPUT_PATH / f"{session_id}_ontology_kg_stats.json"
        async with aiofiles.open(stats_file, 'w') as f:
            await f.write(json.dumps(kg_stats, indent=2))
        
        # Save ontology mapping report
        ontology_mapping_file = self.config.OUTPUT_PATH / f"{session_id}_ontology_mappings.json"
        ontology_mappings = {
            "namespaces": {
                "prov": self.config.PROV_O_NAMESPACE,
                "gdprov": self.config.GDPROV_NAMESPACE,
                "gconsent": self.config.GCONSENT_NAMESPACE,
                "pronto": self.config.PRONTO_NAMESPACE,
                "skos": self.config.SKOS_NAMESPACE,
                "owl": self.config.OWL_NAMESPACE,
                "gdpr": self.config.GDPR_NAMESPACE
            },
            "concept_distribution": {
                "prov_entities": len([c for c in concepts if c.prov_type == "prov:Entity"]),
                "prov_activities": len([c for c in concepts if c.prov_type == "prov:Activity"]),
                "prov_agents": len([c for c in concepts if c.prov_type == "prov:Agent"]),
                "skos_concepts": len([c for c in concepts if c.skos_type == "skos:Concept"]),
                "gdprov_elements": len([c for c in concepts if c.gdprov_type]),
                "gconsent_elements": len([c for c in concepts if c.consent_type]),
                "pronto_elements": len([c for c in concepts if c.pronto_type])
            },
            "relationship_distribution": {
                "skos_broader": sum(len(c.broader_concepts) for c in concepts),
                "skos_narrower": sum(len(c.narrower_concepts) for c in concepts),
                "skos_related": sum(len(c.related_concepts) for c in concepts),
                "prov_derived_from": sum(len(c.was_derived_from) for c in concepts),
                "prov_attributed_to": sum(len(c.was_attributed_to) for c in concepts),
                "gdprov_legal_bases": sum(len(c.legal_basis) for c in concepts),
                "gdprov_processing_purposes": sum(len(c.processing_purpose) for c in concepts)
            }
        }
        
        async with aiofiles.open(ontology_mapping_file, 'w') as f:
            await f.write(json.dumps(ontology_mappings, indent=2))
        
        logging.info(f"üíæ Saved enhanced results with ontology metadata to {self.config.OUTPUT_PATH}")
    
    def _log_enhanced_results(self, results: Dict[str, Any]):
        """Log comprehensive enhanced results"""
        logging.info(f"üìä Enhanced GDPR Metamodel Results:")
        logging.info(f"  - Session ID: {results['session_id']}")
        logging.info(f"  - Processing Time: {results['processing_time']:.2f} seconds")
        logging.info(f"  - Documents Processed: {results['statistics']['documents_processed']}")
        logging.info(f"  - Concepts Extracted: {results['statistics']['concepts_extracted']}")
        logging.info(f"  - Articles Extracted: {results['statistics']['articles_extracted']}")
        logging.info(f"  - Ontology Relationships: {results['statistics']['ontology_relationships']}")
        logging.info(f"  - Vector Embeddings: {results['statistics']['vector_embeddings']}")
        logging.info(f"  - Provenance Chains: {results['statistics']['provenance_chains']}")
        
        # Ontology integration stats
        ontology_stats = results['ontology_integration']
        logging.info(f"  - PROV-O Entities: {ontology_stats['prov_o_entities']}")
        logging.info(f"  - SKOS Concepts: {ontology_stats['skos_concepts']}")
        logging.info(f"  - GDPRov Elements: {ontology_stats['gdprov_elements']}")
        logging.info(f"  - GConsent Elements: {ontology_stats['gconsent_elements']}")
        logging.info(f"  - PrOnto Elements: {ontology_stats['pronto_elements']}")

# ============================================================================
# REMAINING ENHANCED COMPONENTS
# ============================================================================

class MemoryEnhancedPDFProcessor:
    """Enhanced PDF processor with memory and provenance tracking"""
    
    def __init__(self, config: Config):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_with_memory(self, file_path: Path, memory_store, session_id: str) -> Dict[str, Any]:
        """Process PDF with enhanced memory and PROV-O provenance tracking"""
        try:
            doc = pymupdf.open(str(file_path))
            
            doc_info = {
                "file_path": str(file_path),
                "metadata": doc.metadata,
                "page_count": doc.page_count,
                "pages": [],
                "full_text": "",
                "document_type": self._classify_document(file_path.name),
                "session_id": session_id,
                "processing_insights": [],
                "prov_entity_id": f"http://www.w3.org/ns/prov#Entity_{uuid.uuid4()}",
                "extraction_timestamp": datetime.now(timezone.utc)
            }
            
            # Process each page with provenance
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text = page.get_text()
                
                try:
                    tables = page.find_tables()
                    table_data = [table.extract() for table in tables]
                except:
                    table_data = []
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": text,
                    "tables": table_data,
                    "word_count": len(text.split()),
                    "prov_entity_id": f"http://www.w3.org/ns/prov#Entity_Page_{uuid.uuid4()}"
                }
                
                doc_info["pages"].append(page_info)
                doc_info["full_text"] += text + "\n"
            
            # Create enhanced chunks with provenance
            chunks = await self._create_provenance_enhanced_chunks(doc_info)
            doc_info["chunks"] = chunks
            
            doc.close()
            return doc_info
            
        except Exception as e:
            logging.error(f"Error processing PDF {file_path}: {e}")
            return {
                "error": str(e), 
                "file_path": str(file_path),
                "page_count": 0,
                "chunks": [],
                "processing_insights": []
            }
    
    async def _create_provenance_enhanced_chunks(self, doc_info: Dict[str, Any]) -> List[Document]:
        """Create chunks with PROV-O provenance enhancement"""
        chunks = []
        full_text = doc_info["full_text"]
        text_chunks = self.text_splitter.split_text(full_text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Enhanced metadata with PROV-O provenance
            metadata = {
                "source": doc_info["file_path"],
                "chunk_id": f"chunk_{i}",
                "chunk_index": i,
                "document_type": doc_info["document_type"],
                "total_chunks": len(text_chunks),
                "session_id": doc_info["session_id"],
                "extracted_at": datetime.now().isoformat(),
                
                # PROV-O provenance metadata
                "prov_entity_id": f"http://www.w3.org/ns/prov#Entity_Chunk_{uuid.uuid4()}",
                "prov_was_generated_by": f"http://www.w3.org/ns/prov#Activity_TextExtraction_{uuid.uuid4()}",
                "prov_had_primary_source": doc_info["prov_entity_id"],
                "prov_was_attributed_to": "http://www.w3.org/ns/prov#Agent_PDFProcessor",
                
                # Enhanced processing metadata
                "extraction_timestamp": doc_info["extraction_timestamp"].isoformat(),
                "chunk_word_count": len(chunk_text.split()),
                "chunk_char_count": len(chunk_text)
            }
            
            chunk_doc = Document(
                page_content=chunk_text,
                metadata=metadata
            )
            chunks.append(chunk_doc)
        
        return chunks
    
    def _classify_document(self, filename: str) -> str:
        """Enhanced document classification"""
        filename_lower = filename.lower()
        
        if "gdpr" in filename_lower and "uk" not in filename_lower:
            return "GDPR_REGULATION"
        elif "uk" in filename_lower and "gdpr" in filename_lower:
            return "UK_GDPR_REGULATION"
        elif any(term in filename_lower for term in ["business", "company", "internal"]):
            return "BUSINESS_DOCUMENT"
        elif any(term in filename_lower for term in ["policy", "procedure"]):
            return "POLICY_DOCUMENT"
        elif any(term in filename_lower for term in ["consent", "agreement"]):
            return "CONSENT_DOCUMENT"
        else:
            return "UNKNOWN"

class MemoryEnhancedElasticsearchManager:
    """Enhanced Elasticsearch manager with ontology support"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup Elasticsearch client"""
        try:
            connection_params = {
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True,
                "verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
            }
            
            if self.config.ELASTICSEARCH_CA_CERTS:
                connection_params["ca_certs"] = self.config.ELASTICSEARCH_CA_CERTS
                protocol = "https"
            else:
                protocol = "https" if self.config.ELASTICSEARCH_VERIFY_CERTS else "http"
            
            host_url = f"{protocol}://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"
            connection_params["hosts"] = [host_url]
            
            if self.config.ELASTICSEARCH_PASSWORD:
                connection_params["basic_auth"] = (
                    self.config.ELASTICSEARCH_USERNAME, 
                    self.config.ELASTICSEARCH_PASSWORD
                )
            
            self.client = Elasticsearch(**connection_params)
            
            if self.client.ping():
                logging.info(f"‚úÖ Connected to Elasticsearch at {host_url}")
            else:
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"‚ùå Elasticsearch connection error: {e}")
            raise
    
    async def create_enhanced_index(self):
        """Create index with enhanced ontology mappings"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "content": {"type": "text"},
                    "embeddings": {
                        "type": "dense_vector",
                        "dims": self.config.EMBEDDING_DIMENSIONS,
                        "index": True,
                        "similarity": "cosine"
                    },
                    
                    # PROV-O fields
                    "prov_entity_id": {"type": "keyword"},
                    "prov_was_generated_by": {"type": "keyword"},
                    "prov_had_primary_source": {"type": "keyword"},
                    
                    # Ontology relationship fields
                    "related_concepts": {"type": "keyword"},
                    "related_articles": {"type": "keyword"},
                    "concept_count": {"type": "integer"},
                    "article_count": {"type": "integer"},
                    
                    # SKOS fields
                    "skos_broader_concepts": {"type": "keyword"},
                    "skos_schemes": {"type": "keyword"},
                    
                    # GDPRov fields
                    "gdprov_legal_bases": {"type": "keyword"},
                    "gdprov_processing_purposes": {"type": "keyword"},
                    
                    # Enhanced metadata
                    "session_id": {"type": "keyword"},
                    "ontology_enhanced": {"type": "boolean"},
                    "provenance_complete": {"type": "boolean"},
                    "document_type": {"type": "keyword"},
                    "source_file": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "extracted_at": {"type": "date"},
                    "confidence_score": {"type": "float"}
                }
            }
        }
        
        try:
            if self.client.indices.exists(index=self.config.ELASTICSEARCH_INDEX):
                self.client.indices.delete(index=self.config.ELASTICSEARCH_INDEX)
            
            self.client.indices.create(
                index=self.config.ELASTICSEARCH_INDEX,
                **mapping
            )
            logging.info(f"‚úÖ Created enhanced Elasticsearch index: {self.config.ELASTICSEARCH_INDEX}")
            
        except Exception as e:
            logging.error(f"‚ùå Error creating enhanced index: {e}")
            raise
    
    async def store_with_memory(self, documents: List[Dict[str, Any]], 
                              session_id: str, memory_insights: Dict[str, Any],
                              batch_size: int = 100):
        """Store documents with enhanced memory integration"""
        try:
            actions = []
            for doc in documents:
                enhanced_doc = {
                    **doc,
                    "session_id": session_id,
                    "updated_at": datetime.now().isoformat(),
                    "memory_insights": json.dumps(memory_insights)
                }
                
                action = {
                    "_index": self.config.ELASTICSEARCH_INDEX,
                    "_id": enhanced_doc.get("document_id", str(uuid.uuid4())),
                    "_source": enhanced_doc
                }
                actions.append(action)
            
            # Process in batches
            success_count = 0
            failed_count = 0
            
            for i in range(0, len(actions), batch_size):
                batch = actions[i:i + batch_size]
                success, failed = bulk(self.client, batch)
                success_count += success
                failed_count += len(failed)
            
            logging.info(f"‚úÖ Stored {success_count} enhanced documents in Elasticsearch")
            
            return {"success": success_count, "failed": failed_count}
            
        except Exception as e:
            logging.error(f"‚ùå Error storing enhanced documents: {e}")
            raise

class GDPRMetamodelGenerator:
    """Enhanced metamodel generator with full ontology integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
    
    async def generate_ontology_metamodel(self, concepts: List[SemanticConcept], 
                                        articles: List[GDPRArticle],
                                        kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Generate metamodel with comprehensive ontology integration"""
        
        metamodel_structure = await self._analyze_ontology_metamodel_requirements(
            concepts, articles, kg_stats
        )
        
        metamodel_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR compliance expert creating a comprehensive metamodel with full semantic ontology integration.
        
        Create a metamodel that integrates all ontology patterns:
        - PROV-O: Full provenance tracking
        - SKOS: Hierarchical concept organization  
        - GDPRov: GDPR-specific provenance and compliance
        - GConsent: Consent lifecycle management
        - PrOnto: Legal reasoning and obligations
        - OWL: Formal semantic definitions
        
        Metamodel Requirements:
        {metamodel_structure}
        
        Knowledge Summary:
        - Total Concepts: {total_concepts}
        - Total Articles: {total_articles}  
        - Ontology Statistics: {kg_stats}
        
        Sample Ontology Data:
        {sample_data}
        
        Generate a comprehensive metamodel with:
        1. Ontology-based taxonomies and classifications
        2. PROV-O provenance patterns for all entities
        3. SKOS concept schemes for knowledge organization
        4. GDPRov compliance workflows
        5. GConsent consent management patterns
        6. PrOnto legal reasoning structures
        7. Cross-jurisdictional ontology mappings
        8. Financial services industry specializations
        
        Return as JSON with complete ontology integration.
        """)
        
        # Prepare ontology-enhanced sample data
        sample_concepts = [
            {
                'uri': c.uri,
                'label': c.label,
                'skos_type': c.skos_type,
                'prov_type': c.prov_type,
                'gdprov_type': c.gdprov_type,
                'broader_concepts': c.broader_concepts[:3],
                'legal_basis': c.legal_basis,
                'ropa_relevance': c.ropa_relevance,
                'financial_sector_impact': c.financial_sector_impact
            }
            for c in concepts[:10]
        ]
        
        sample_articles = [
            {
                'uri': a.uri,
                'number': a.number,
                'title': a.title,
                'prov_entity_id': a.prov_entity_id,
                'gdprov_entity_type': a.gdprov_entity_type,
                'legal_bases_defined': a.legal_bases_defined,
                'rights_granted': a.rights_granted,
                'financial_sector_relevance': a.financial_sector_relevance
            }
            for a in articles[:10]
        ]
        
        sample_data = {
            'concepts': sample_concepts,
            'articles': sample_articles
        }
        
        try:
            messages = metamodel_prompt.format_messages(
                metamodel_structure=json.dumps(metamodel_structure, indent=2),
                total_concepts=len(concepts),
                total_articles=len(articles),
                kg_stats=json.dumps(kg_stats, indent=2),
                sample_data=json.dumps(sample_data, indent=2)
            )
            
            response = await self.llm.ainvoke(messages)
            metamodel = json.loads(response.content)
            
            # Enhance with computed ontology metrics
            metamodel['ontology_metrics'] = await self._compute_ontology_metrics(
                concepts, articles, kg_stats
            )
            
            return metamodel
            
        except Exception as e:
            logging.error(f"‚ùå Error generating ontology metamodel: {e}")
            return {}
    
    async def _analyze_ontology_metamodel_requirements(self, concepts: List[SemanticConcept], 
                                                     articles: List[GDPRArticle],
                                                     kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze requirements for ontology-integrated metamodel"""
        
        # Analyze ontology patterns in extracted data
        prov_entities = [c for c in concepts if c.prov_type == "prov:Entity"]
        skos_concepts = [c for c in concepts if c.skos_type == "skos:Concept"] 
        gdprov_elements = [c for c in concepts if c.gdprov_type]
        gconsent_elements = [c for c in concepts if c.consent_type]
        pronto_elements = [c for c in concepts if c.pronto_type]
        
        analysis_prompt = ChatPromptTemplate.from_template("""
        Analyze extracted GDPR knowledge with full ontology integration for metamodel design.
        
        Ontology Distribution:
        - PROV-O Entities: {prov_count}
        - SKOS Concepts: {skos_count}
        - GDPRov Elements: {gdprov_count}
        - GConsent Elements: {gconsent_count}
        - PrOnto Elements: {pronto_count}
        
        Knowledge Graph Stats: {kg_stats}
        
        Determine comprehensive metamodel structure integrating all ontology patterns:
        
        {{
            "ontology_taxonomies": {{
                "skos_concept_schemes": ["GDPR_Concepts", "Legal_Basis_Taxonomy", "..."],
                "prov_entity_hierarchies": ["Documents", "Activities", "Agents"],
                "gdprov_compliance_workflows": ["Consent_Lifecycle", "Processing_Activities"],
                "gconsent_patterns": ["Consent_Types", "Consent_Mechanisms"],
                "pronto_legal_structures": ["Rights", "Obligations", "Procedures"]
            }},
            "cross_jurisdictional_mappings": {{
                "EU_GDPR": "primary_regulation",
                "UK_GDPR": "brexit_adaptation", 
                "EEA_extensions": "territorial_scope"
            }},
            "financial_services_specialization": {{
                "banking_activities": "regulatory_framework",
                "risk_management": "compliance_procedures",
                "customer_onboarding": "consent_workflows"
            }},
            "provenance_requirements": {{
                "entity_provenance": "full_prov_o_support",
                "decision_provenance": "audit_trails",
                "compliance_provenance": "regulatory_reporting"
            }}
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages(
                prov_count=len(prov_entities),
                skos_count=len(skos_concepts),
                gdprov_count=len(gdprov_elements),
                gconsent_count=len(gconsent_elements),
                pronto_count=len(pronto_elements),
                kg_stats=json.dumps(kg_stats, indent=2)
            )
            response = await self.llm.ainvoke(messages)
            requirements = json.loads(response.content)
            
            logging.info(f"üß† Ontology metamodel requirements analysis completed")
            return requirements
            
        except Exception as e:
            logging.error(f"‚ùå Error analyzing ontology metamodel requirements: {e}")
            # Fallback requirements
            return {
                "ontology_taxonomies": {
                    "skos_concept_schemes": ["GDPR_Concepts"],
                    "prov_entity_hierarchies": ["Documents", "Activities"],
                    "gdprov_compliance_workflows": ["Basic_Compliance"]
                },
                "financial_services_specialization": {
                    "banking_activities": "basic_framework"
                }
            }
    
    async def _compute_ontology_metrics(self, concepts: List[SemanticConcept], 
                                      articles: List[GDPRArticle],
                                      kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Compute comprehensive ontology metrics"""
        try:
            metrics = {
                'total_ontology_elements': len(concepts) + len(articles),
                'ontology_coverage': {
                    'prov_o_coverage': len([c for c in concepts if c.prov_type]) / len(concepts) * 100 if concepts else 0,
                    'skos_coverage': len([c for c in concepts if c.skos_type]) / len(concepts) * 100 if concepts else 0,
                    'gdprov_coverage': len([c for c in concepts if c.gdprov_type]) / len(concepts) * 100 if concepts else 0,
                    'gconsent_coverage': len([c for c in concepts if c.consent_type]) / len(concepts) * 100 if concepts else 0,
                    'pronto_coverage': len([c for c in concepts if c.pronto_type]) / len(concepts) * 100 if concepts else 0
                },
                'relationship_density': {
                    'skos_relationships': sum(len(c.broader_concepts) + len(c.narrower_concepts) + len(c.related_concepts) for c in concepts),
                    'prov_relationships': sum(len(c.was_derived_from) + len(c.was_attributed_to) for c in concepts),
                    'gdprov_relationships': sum(len(c.legal_basis) + len(c.processing_purpose) for c in concepts)
                },
                'semantic_richness': {
                    'multilingual_labels': sum(len(c.pref_label) for c in concepts),
                    'alternative_labels': sum(len(c.alt_labels) for c in concepts),
                    'cross_references': sum(len(c.exact_match) + len(c.close_match) for c in concepts)
                },
                'compliance_metrics': {
                    'high_ropa_relevance': len([c for c in concepts if c.ropa_relevance == 'high']),
                    'financial_impact_significant': len([c for c in concepts if c.financial_sector_impact == 'significant']),
                    'multi_jurisdictional': len([c for c in concepts if len(c.territorial_scope) > 1])
                }
            }
            
            return metrics
            
        except Exception as e:
            logging.error(f"‚ùå Error computing ontology metrics: {e}")
            return {}
    
    async def generate_ontology_ropa_report(self, metamodel: Dict[str, Any], 
                                          concepts: List[SemanticConcept],
                                          articles: List[GDPRArticle]) -> Dict[str, Any]:
        """Generate ROPA report with full ontology integration"""
        
        ropa_prompt = ChatPromptTemplate.from_template("""
        Generate a comprehensive ROPA (Record of Processing Activities) compliance report
        with full semantic ontology integration for financial services.
        
        Ontology-Enhanced Metamodel: {metamodel_summary}
        ROPA-Relevant Ontology Concepts: {ropa_concepts}
        
        Create detailed ROPA report with ontology patterns:
        
        1. PROV-O Provenance Tracking:
           - Complete audit trails for all processing activities
           - Entity-Activity-Agent relationships
           - Derivation chains for data transformations
        
        2. SKOS Knowledge Organization:
           - Hierarchical processing activity taxonomies
           - Legal basis classification schemes
           - Data category concept schemes
        
        3. GDPRov Compliance Workflows:
           - Consent lifecycle management
           - Legal basis documentation
           - Cross-border transfer procedures
        
        4. GConsent Integration:
           - Consent mechanism specifications
           - Consent status tracking
           - Withdrawal procedures
        
        5. PrOnto Legal Reasoning:
           - Obligation compliance checking
           - Rights fulfillment procedures
           - Legal basis justifications
        
        Return comprehensive ontology-integrated ROPA report as JSON.
        """)
        
        # Prepare ROPA-relevant ontology data
        ropa_concepts = [c for c in concepts if c.ropa_relevance in ['high', 'critical']]
        metamodel_summary = json.dumps({
            'ontology_taxonomies': metamodel.get('ontology_taxonomies', {}),
            'provenance_requirements': metamodel.get('provenance_requirements', {}),
            'ontology_metrics': metamodel.get('ontology_metrics', {})
        }, indent=2)
        
        ropa_concepts_summary = json.dumps([
            {
                'uri': c.uri,
                'label': c.label,
                'skos_type': c.skos_type,
                'prov_type': c.prov_type,
                'gdprov_type': c.gdprov_type,
                'legal_basis': c.legal_basis,
                'processing_purpose': c.processing_purpose,
                'compliance_requirements': c.compliance_requirements,
                'ropa_relevance': c.ropa_relevance,
                'financial_sector_impact': c.financial_sector_impact
            }
            for c in ropa_concepts[:15]
        ], indent=2)
        
        try:
            messages = ropa_prompt.format_messages(
                metamodel_summary=metamodel_summary,
                ropa_concepts=ropa_concepts_summary
            )
            
            response = await self.llm.ainvoke(messages)
            ropa_report = json.loads(response.content)
            
            # Add ontology implementation metrics
            ropa_report['ontology_implementation'] = {
                'prov_provenance_chains': len([c for c in ropa_concepts if c.was_generated_by]),
                'skos_concept_hierarchies': len([c for c in ropa_concepts if c.broader_concepts or c.narrower_concepts]),
                'gdprov_compliance_elements': len([c for c in ropa_concepts if c.gdprov_type]),
                'gconsent_mechanisms': len([c for c in ropa_concepts if c.consent_type]),
                'pronto_legal_structures': len([c for c in ropa_concepts if c.pronto_type]),
                'cross_jurisdictional_coverage': len(set(c.jurisdiction for c in ropa_concepts)),
                'financial_sector_specialization': len([c for c in ropa_concepts if c.financial_sector_impact in ['significant', 'high']])
            }
            
            return ropa_report
            
        except Exception as e:
            logging.error(f"‚ùå Error generating ontology ROPA report: {e}")
            return {}

# ============================================================================
# MAIN EXECUTION WITH ENHANCED ONTOLOGY INTEGRATION
# ============================================================================

async def main():
    """Main execution function with full ontology integration"""
    try:
        # Load enhanced configuration
        config = Config()
        
        # Ensure directories exist
        config.DOCUMENTS_PATH.mkdir(exist_ok=True)
        config.OUTPUT_PATH.mkdir(exist_ok=True)
        config.MEMORY_PATH.mkdir(exist_ok=True)
        
        print("üöÄ Enhanced GDPR Metamodel Knowledge System with Full Ontology Integration")
        print("=" * 80)
        print(f"üìÅ Documents Path: {config.DOCUMENTS_PATH}")
        print(f"üìÇ Output Path: {config.OUTPUT_PATH}")
        print(f"ü§ñ Reasoning Model: {config.REASONING_MODEL}")
        print(f"‚ö° Reasoning Effort: {config.REASONING_EFFORT}")
        print(f"üîÆ Embedding Model: {config.EMBEDDING_MODEL}")
        print(f"üìä Embedding Dimensions: {config.EMBEDDING_DIMENSIONS}")
        print(f"üóÑÔ∏è FalkorDB Graph: {config.FALKORDB_GRAPH_NAME}")
        print(f"üîç Elasticsearch Index: {config.ELASTICSEARCH_INDEX}")
        
        print(f"\nüß† Integrated Ontologies:")
        print(f"  - PROV-O: {config.PROV_O_NAMESPACE}")
        print(f"  - GDPRov: {config.GDPROV_NAMESPACE}")
        print(f"  - GConsent: {config.GCONSENT_NAMESPACE}")
        print(f"  - PrOnto: {config.PRONTO_NAMESPACE}")
        print(f"  - SKOS: {config.SKOS_NAMESPACE}")
        print(f"  - OWL: {config.OWL_NAMESPACE}")
        
        print(f"\nüìä Enhanced Streaming Configuration:")
        print(f"  - Document Processing Batches: {config.DOCUMENT_BATCH_SIZE}")
        print(f"  - Embedding Generation Batches: {config.EMBEDDING_BATCH_SIZE}")
        print(f"  - FalkorDB Storage Batches: {config.FALKORDB_BATCH_SIZE}")
        print(f"  - Vector-enabled with openCypher compliance")
        
        # Create enhanced orchestrator and run pipeline
        orchestrator = EnhancedGDPRMetamodelOrchestrator(config)
        results = await orchestrator.run_enhanced_metamodel_pipeline()
        
        # Display enhanced results
        print("\n" + "=" * 80)
        print("üìä ENHANCED GDPR METAMODEL RESULTS WITH ONTOLOGY INTEGRATION")
        print("=" * 80)
        print(f"Status: {results['status']}")
        print(f"Session ID: {results['session_id']}")
        print(f"Processing Time: {results['processing_time']:.2f} seconds")
        
        if results['status'] == 'success':
            print(f"\nüìà Processing Statistics:")
            for key, value in results['statistics'].items():
                print(f"  - {key.replace('_', ' ').title()}: {value}")
            
            print(f"\nüß† Ontology Integration Statistics:")
            ontology_stats = results['ontology_integration']
            print(f"  - PROV-O Entities: {ontology_stats['prov_o_entities']}")
            print(f"  - SKOS Concepts: {ontology_stats['skos_concepts']}")
            print(f"  - GDPRov Elements: {ontology_stats['gdprov_elements']}")
            print(f"  - GConsent Elements: {ontology_stats['gconsent_elements']}")
            print(f"  - PrOnto Elements: {ontology_stats['pronto_elements']}")
            
            print(f"\nüèóÔ∏è Enhanced Metamodel Coverage:")
            metamodel = results['metamodel']
            if 'ontology_metrics' in metamodel:
                metrics = metamodel['ontology_metrics']
                coverage = metrics.get('ontology_coverage', {})
                print(f"  - PROV-O Coverage: {coverage.get('prov_o_coverage', 0):.1f}%")
                print(f"  - SKOS Coverage: {coverage.get('skos_coverage', 0):.1f}%")
                print(f"  - GDPRov Coverage: {coverage.get('gdprov_coverage', 0):.1f}%")
                print(f"  - GConsent Coverage: {coverage.get('gconsent_coverage', 0):.1f}%")
                print(f"  - PrOnto Coverage: {coverage.get('pronto_coverage', 0):.1f}%")
            
            print(f"\nüìã Enhanced ROPA Report:")
            ropa_report = results['ropa_report']
            if 'ontology_implementation' in ropa_report:
                impl_metrics = ropa_report['ontology_implementation']
                print(f"  - PROV-O Provenance Chains: {impl_metrics.get('prov_provenance_chains', 0)}")
                print(f"  - SKOS Concept Hierarchies: {impl_metrics.get('skos_concept_hierarchies', 0)}")
                print(f"  - GDPRov Compliance Elements: {impl_metrics.get('gdprov_compliance_elements', 0)}")
                print(f"  - GConsent Mechanisms: {impl_metrics.get('gconsent_mechanisms', 0)}")
                print(f"  - PrOnto Legal Structures: {impl_metrics.get('pronto_legal_structures', 0)}")
                print(f"  - Cross-Jurisdictional Coverage: {impl_metrics.get('cross_jurisdictional_coverage', 0)}")
            
            print(f"\nüï∏Ô∏è Enhanced Knowledge Graph:")
            kg_stats = results['knowledge_graph_stats']
            print(f"  - Total Concepts: {kg_stats.get('total_concepts', 0)}")
            print(f"  - Total Articles: {kg_stats.get('total_articles', 0)}")
            print(f"  - SKOS Relationships: {kg_stats.get('skos_relationships', 0)}")
            print(f"  - PROV-O Relationships: {kg_stats.get('prov_relationships', 0)}")
            print(f"  - GDPRov Relationships: {kg_stats.get('gdprov_relationships', 0)}")
            print(f"  - Vector Embeddings: {kg_stats.get('concepts_with_embeddings', 0) + kg_stats.get('articles_with_embeddings', 0)}")
            
            print(f"\nüíæ Enhanced Files Generated:")
            print(f"  - {results['session_id']}_enhanced_metamodel.json")
            print(f"  - {results['session_id']}_enhanced_ropa_report.json")
            print(f"  - {results['session_id']}_ontology_concepts.json")
            print(f"  - {results['session_id']}_ontology_articles.json")
            print(f"  - {results['session_id']}_ontology_kg_stats.json")
            print(f"  - {results['session_id']}_ontology_mappings.json")
            
            print(f"\nüîß Ontology Performance Notes:")
            total_time = results['processing_time']
            docs_processed = results['statistics']['documents_processed']
            if docs_processed > 0:
                print(f"  - Average time per document: {total_time/docs_processed:.2f} seconds")
            print(f"  - Full PROV-O provenance tracking enabled")
            print(f"  - SKOS hierarchical relationships preserved")
            print(f"  - GDPRov compliance workflows implemented")
            print(f"  - Vector similarity search with ontology filtering")
            print(f"  - FalkorDB openCypher compliance verified")
            
        else:
            print(f"‚ùå Error: {results.get('error', 'Unknown error')}")
        
        print(f"\nüéâ Enhanced GDPR Metamodel System ready for enterprise deployment!")
        print(f"üß† Full semantic ontology integration: PROV-O, GDPRov, GConsent, PrOnto, SKOS")
        print(f"üîó openCypher compliant with FalkorDB vector indexes")
        print(f"üí° Production-ready for financial services GDPR compliance")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Enhanced system failed: {e}")
        logging.error(f"Enhanced system failure: {e}")
        import traceback
        print(f"üìã Full traceback: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
