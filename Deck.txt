import rdflib
from rdflib import ConjunctiveGraph, Namespace, URIRef, Literal
from rdflib.plugins.stores import sparqlstore
import falkordb
from typing import Dict, List, Set, Tuple
import json

class RDFAnalyzer:
    """Utility class to analyze RDF graph structure before conversion"""
    
    def __init__(self, sparql_endpoint: str, username: str = None, password: str = None):
        auth = (username, password) if username and password else None
        self.store = sparqlstore.SPARQLUpdateStore(sparql_endpoint, auth=auth)
        self.graph = ConjunctiveGraph(store=self.store)
    
    def analyze_schema(self) -> Dict:
        """Analyze the RDF schema to understand its structure"""
        
        analysis = {
            'classes': {},
            'properties': {},
            'namespaces': {},
            'statistics': {}
        }
        
        # Get all classes
        classes_query = """
        SELECT ?class (COUNT(?instance) as ?count) WHERE {
            ?instance a ?class .
        } GROUP BY ?class ORDER BY DESC(?count)
        """
        
        for result in self.graph.query(classes_query):
            class_uri = str(result[0])
            count = int(result[1])
            analysis['classes'][class_uri] = {
                'count': count,
                'short_name': self._get_short_name(class_uri)
            }
        
        # Get all properties
        properties_query = """
        SELECT ?prop (COUNT(*) as ?count) WHERE {
            ?s ?prop ?o .
        } GROUP BY ?prop ORDER BY DESC(?count)
        """
        
        for result in self.graph.query(properties_query):
            prop_uri = str(result[0])
            count = int(result[1])
            analysis['properties'][prop_uri] = {
                'count': count,
                'short_name': self._get_short_name(prop_uri)
            }
        
        # Get namespaces
        for prefix, namespace in self.graph.namespaces():
            analysis['namespaces'][str(prefix)] = str(namespace)
        
        # Get statistics
        stats_queries = {
            'total_triples': "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }",
            'unique_subjects': "SELECT (COUNT(DISTINCT ?s) as ?count) WHERE { ?s ?p ?o }",
            'unique_predicates': "SELECT (COUNT(DISTINCT ?p) as ?count) WHERE { ?s ?p ?o }",
            'unique_objects': "SELECT (COUNT(DISTINCT ?o) as ?count) WHERE { ?s ?p ?o }"
        }
        
        for stat_name, query in stats_queries.items():
            result = list(self.graph.query(query))[0]
            analysis['statistics'][stat_name] = int(result[0])
        
        return analysis
    
    def _get_short_name(self, uri: str) -> str:
        """Get short name for URI"""
        if '#' in uri:
            return uri.split('#')[-1]
        elif '/' in uri:
            return uri.split('/')[-1]
        return uri
    
    def get_property_ranges(self, property_uri: str) -> Dict:
        """Analyze the range of values for a specific property"""
        
        range_query = f"""
        SELECT ?o (COUNT(*) as ?count) WHERE {{
            ?s <{property_uri}> ?o .
        }} GROUP BY ?o ORDER BY DESC(?count) LIMIT 100
        """
        
        ranges = {'literals': [], 'uris': [], 'datatypes': {}}
        
        for result in self.graph.query(range_query):
            obj = result[0]
            count = int(result[1])
            
            if isinstance(obj, Literal):
                ranges['literals'].append({'value': str(obj), 'count': count})
                datatype = str(obj.datatype) if obj.datatype else 'string'
                ranges['datatypes'][datatype] = ranges['datatypes'].get(datatype, 0) + count
            else:
                ranges['uris'].append({'value': str(obj), 'count': count})
        
        return ranges

class BatchProcessor:
    """Process RDF data in batches for large graphs"""
    
    def __init__(self, sparql_endpoint: str, falkordb_graph, batch_size: int = 1000,
                 username: str = None, password: str = None):
        auth = (username, password) if username and password else None
        self.store = sparqlstore.SPARQLUpdateStore(sparql_endpoint, auth=auth)
        self.rdf_graph = ConjunctiveGraph(store=self.store)
        self.falkor_graph = falkordb_graph
        self.batch_size = batch_size
    
    def process_in_batches(self):
        """Process RDF data in batches"""
        
        # Get total count
        count_query = "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o }"
        total_triples = int(list(self.rdf_graph.query(count_query))[0][0])
        
        print(f"Processing {total_triples} triples in batches of {self.batch_size}")
        
        offset = 0
        while offset < total_triples:
            batch_query = f"""
            SELECT ?s ?p ?o WHERE {{
                ?s ?p ?o .
            }} LIMIT {self.batch_size} OFFSET {offset}
            """
            
            batch_results = list(self.rdf_graph.query(batch_query))
            self._process_batch(batch_results)
            
            offset += self.batch_size
            progress = min(100, (offset / total_triples) * 100)
            print(f"Progress: {progress:.1f}% ({offset}/{total_triples})")
    
    def _process_batch(self, batch_results: List[Tuple]):
        """Process a batch of triples"""
        
        # Collect nodes and relationships from batch
        nodes_in_batch = set()
        relationships_in_batch = []
        
        for s, p, o in batch_results:
            if isinstance(s, URIRef):
                nodes_in_batch.add(str(s))
            
            if isinstance(o, URIRef):
                nodes_in_batch.add(str(o))
                relationships_in_batch.append((str(s), str(p), str(o)))
        
        # Create nodes for this batch
        for node_uri in nodes_in_batch:
            self._create_node_if_not_exists(node_uri)
        
        # Create relationships for this batch
        for s, p, o in relationships_in_batch:
            self._create_relationship(s, p, o)
    
    def _create_node_if_not_exists(self, uri: str):
        """Create node if it doesn't exist"""
        check_query = "MATCH (n {uri: $uri}) RETURN n"
        result = self.falkor_graph.query(check_query, {'uri': uri})
        
        if not result.result_set:
            # Node doesn't exist, create it
            create_query = "CREATE (n:Resource {uri: $uri})"
            self.falkor_graph.query(create_query, {'uri': uri})
    
    def _create_relationship(self, subject: str, predicate: str, obj: str):
        """Create relationship between nodes"""
        rel_type = predicate.split('/')[-1].split('#')[-1]
        rel_query = f"""
        MATCH (s {{uri: $subject}}), (o {{uri: $object}})
        CREATE (s)-[:{rel_type}]->(o)
        """
        self.falkor_graph.query(rel_query, {'subject': subject, 'object': obj})

class SchemaMapper:
    """Map RDF schema to property graph schema"""
    
    def __init__(self, schema_analysis: Dict):
        self.schema = schema_analysis
    
    def generate_mapping_config(self) -> Dict:
        """Generate configuration for schema mapping"""
        
        config = {
            'node_labels': {},
            'relationship_types': {},
            'property_mappings': {}
        }
        
        # Map RDF classes to node labels
        for class_uri, class_info in self.schema['classes'].items():
            config['node_labels'][class_uri] = {
                'label': class_info['short_name'],
                'count': class_info['count']
            }
        
        # Map RDF properties to relationship types or node properties
        for prop_uri, prop_info in self.schema['properties'].items():
            short_name = prop_info['short_name']
            
            if prop_uri == 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type':
                # Skip rdf:type as it's handled by node labels
                continue
            
            config['relationship_types'][prop_uri] = {
                'type': short_name,
                'count': prop_info['count']
            }
        
        return config
    
    def save_mapping_config(self, filename: str, config: Dict):
        """Save mapping configuration to file"""
        with open(filename, 'w') as f:
            json.dump(config, f, indent=2)
    
    def load_mapping_config(self, filename: str) -> Dict:
        """Load mapping configuration from file"""
        with open(filename, 'r') as f:
            return json.load(f)

# Usage example combining all utilities
def comprehensive_conversion(sparql_endpoint: str, username: str = None, password: str = None):
    """Complete conversion workflow with analysis and optimization"""
    
    # Step 1: Analyze the RDF schema
    print("Step 1: Analyzing RDF schema...")
    analyzer = RDFAnalyzer(sparql_endpoint, username, password)
    schema_analysis = analyzer.analyze_schema()
    
    print(f"Found {len(schema_analysis['classes'])} classes and {len(schema_analysis['properties'])} properties")
    print(f"Total triples: {schema_analysis['statistics']['total_triples']}")
    
    # Step 2: Generate mapping configuration
    print("Step 2: Generating schema mapping...")
    mapper = SchemaMapper(schema_analysis)
    mapping_config = mapper.generate_mapping_config()
    mapper.save_mapping_config('rdf_mapping.json', mapping_config)
    
    # Step 3: Setup FalkorDB
    print("Step 3: Setting up FalkorDB...")
    db = falkordb.FalkorDB()
    falkor_graph = db.select_graph('comprehensive_rdf_graph')
    falkor_graph.query("MATCH (n) DETACH DELETE n")  # Clear existing data
    
    # Step 4: Process in batches if large dataset
    print("Step 4: Converting data...")
    total_triples = schema_analysis['statistics']['total_triples']
    
    if total_triples > 10000:  # Use batch processing for large datasets
        processor = BatchProcessor(sparql_endpoint, falkor_graph, batch_size=1000, 
                                 username=username, password=password)
        processor.process_in_batches()
    else:
        # Use the comprehensive converter for smaller datasets
        from rdf_to_falkordb_converter import RDFToFalkorDBConverter
        converter = RDFToFalkorDBConverter(sparql_endpoint, username=username, password=password)
        converter.convert()
    
    print("Conversion completed!")
    
    return schema_analysis, mapping_config

if __name__ == "__main__":
    # Run comprehensive conversion
    endpoint = "your_sparql_endpoint_here"
    
    comprehensive_conversion(
        sparql_endpoint=endpoint,
        username="username",  # or None
        password="password"   # or None
    )
