#!/usr/bin/env python3
"""
Complete Optimized Async RDF to FalkorDB Converter with Automatic Deduplication
Dependencies: pip install rdflib falkordb redis asyncio tqdm
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

from tqdm.asyncio import tqdm
from tqdm import tqdm as tqdm_sync

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def clean_label_name(label: str) -> str:
    """Clean label name to ensure valid Cypher identifier"""
    if not label:
        return 'Resource'
    
    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', label)
    
    if clean_label and clean_label[0].isdigit():
        clean_label = f"_{clean_label}"
    
    if not clean_label:
        clean_label = 'Resource'
    elif len(clean_label) > 50:
        clean_label = clean_label[:50]
    
    return clean_label


@dataclass
class OptimizedAsyncTripleConfig:
    """Configuration for async triple-based RDF conversion"""
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    append_to_existing_graph: bool = True
    clear_existing_graph: bool = False
    handle_duplicates: bool = True
    skip_existing_nodes: bool = True
    skip_existing_relationships: bool = True
    
    batch_size: int = 2000
    max_concurrent_batches: int = 3
    connection_pool_size: int = 10
    sparql_timeout: int = 7200
    falkordb_timeout: Optional[int] = 300
    max_retries: int = 5
    retry_delay: int = 3
    
    preserve_uri_properties: bool = False
    disable_relationship_properties: bool = True
    group_relationships_by_type: bool = True
    use_bulk_relationship_creation: bool = True
    
    use_shortened_uris: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    exclude_rdf_type_properties: bool = False
    validate_conversion: bool = False
    export_stats: bool = True
    progress_update_interval: int = 50


@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    initial_nodes: int = 0
    initial_relationships: int = 0
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    skipped_nodes: int = 0
    skipped_relationships: int = 0
    final_nodes: int = 0
    final_relationships: int = 0
    
    relationship_creation_time: float = 0.0
    node_creation_time: float = 0.0
    relationship_creation_rate: float = 0.0
    
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    relationship_types_count: Dict[str, int] = None
    
    append_mode: bool = False
    graph_was_cleared: bool = False
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
        if self.relationship_types_count is None:
            self.relationship_types_count = {}
    
    def get_incremental_summary(self) -> str:
        """Get a summary of incremental changes"""
        if self.append_mode:
            return (f"Added {self.created_nodes:,} nodes and {self.created_relationships:,} relationships. "
                   f"Graph now has {self.final_nodes:,} nodes and {self.final_relationships:,} relationships total.")
        else:
            return (f"Created {self.created_nodes:,} nodes and {self.created_relationships:,} relationships "
                   f"in new graph.")


class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)


class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: OptimizedAsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry with proper RDF class as primary label"""
        labels = set()
        primary_label = None
        
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            clean_class_label = clean_label_name(class_label)
            labels.add(clean_class_label)
            primary_label = clean_class_label
        else:
            primary_label = clean_label_name(self.config.default_node_label)
            labels.add(primary_label)
        
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'primary_label': primary_label,
            'properties': {}
        }
        
        node_data['properties']['uri'] = uri
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        if self.config.preserve_uri_properties and class_uri:
            node_data['properties']['rdf_type'] = class_uri
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()


class OptimizedAsyncFalkorDBManager:
    """Optimized FalkorDB manager with corrected syntax for FalkorDB compatibility"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB"""
        try:
            clean_default_label = clean_label_name(self.config.default_node_label)
            if clean_default_label != self.config.default_node_label:
                logger.warning(f"Default label '{self.config.default_node_label}' will be cleaned to '{clean_default_label}'")
            
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
                'socket_connect_timeout': 30,
            }
            
            if self.config.falkordb_timeout:
                pool_kwargs['timeout'] = self.config.falkordb_timeout
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            logger.info(f"‚úÖ FalkorDB connection established to graph '{self.config.graph_name}'")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("üóëÔ∏è  Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def get_initial_graph_stats(self) -> Dict[str, int]:
        """Get initial graph statistics before processing"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            if node_count > 0 or rel_count > 0:
                logger.info(f"üìä Existing graph: {node_count:,} nodes, {rel_count:,} relationships")
            else:
                logger.info("üìä Starting with empty graph")
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting initial statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with async retry logic"""
        for attempt in range(self.config.max_retries):
            try:
                return await self.graph.query(query, params or {})
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
    
    async def create_index_safely(self, label: str) -> bool:
        """Create an index safely with proper error handling"""
        try:
            query = f"CREATE INDEX FOR (n:{label}) ON (n.uri)"
            await self.execute_query_with_retry(query)
            logger.info(f"‚úÖ Created index for label '{label}'")
            return True
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info(f"‚ÑπÔ∏è  Index for '{label}' already exists")
                return True
            else:
                logger.warning(f"Could not create index for label '{label}': {e}")
                return False
    
    async def create_indexes(self, discovered_labels: Dict[str, int] = None):
        """Create indexes with robust duplicate handling"""
        if not self.config.create_indexes:
            return
        
        logger.info("Creating performance indexes for discovered RDF classes...")
        
        clean_default_label = clean_label_name(self.config.default_node_label)
        
        if not discovered_labels:
            logger.warning("No discovered labels provided - creating fallback index only")
            await self.create_index_safely(clean_default_label)
            return
        
        top_labels = sorted(discovered_labels.items(), key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"Creating indexes for top {len(top_labels)} RDF classes:")
        
        created_indexes = set()
        
        for label, count in top_labels:
            logger.info(f"  - {label}: {count:,} nodes")
            if await self.create_index_safely(label):
                created_indexes.add(label)
        
        if clean_default_label not in created_indexes:
            await self.create_index_safely(clean_default_label)
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """Create nodes with URI-based merging"""
        if not nodes_batch:
            return 0
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        try:
            nodes_by_label = defaultdict(list)
            
            for uri, node_data in nodes_batch:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                nodes_by_label[primary_label].append({
                    'uri': uri,
                    'properties': properties
                })
            
            total_created = 0
            
            for label, nodes_data in nodes_by_label.items():
                if use_merge:
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    MERGE (n:{label} {{uri: node_data.uri}})
                    SET n += node_data.properties
                    RETURN count(n) as total_processed
                    """
                else:
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    CREATE (n:{label})
                    SET n += node_data.properties
                    RETURN count(n) as total_created
                    """
                
                result = await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
                batch_processed = result.result_set[0][0] if result.result_set else len(nodes_data)
                total_created += batch_processed
            
            return total_created
                
        except Exception as e:
            logger.error(f"Batch node creation failed: {e}")
            return await self._create_nodes_individual_fallback(nodes_batch, use_merge)
    
    async def _create_nodes_individual_fallback(self, nodes_batch: List[Tuple[str, Dict[str, Any]]], use_merge: bool = False) -> int:
        """Fallback to create nodes individually"""
        created_count = 0
        
        for uri, node_data in nodes_batch:
            try:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                if use_merge:
                    query = f"""
                    MERGE (n:{primary_label} {{uri: $uri}})
                    SET n += $properties
                    """
                else:
                    query = f"""
                    CREATE (n:{primary_label})
                    SET n += $properties
                    """
                
                await self.execute_query_with_retry(query, {
                    'uri': uri,
                    'properties': properties
                })
                created_count += 1
                
            except Exception as e:
                logger.warning(f"Failed to create individual node {uri}: {e}")
                continue
        
        return created_count
    
    async def create_relationships_ultra_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """Ultra-fast relationship creation optimized for 2M+ edges"""
        if not relationships_by_type:
            return 0
        
        total_relationships = sum(len(rels) for rels in relationships_by_type.values())
        logger.info(f"üöÄ Creating {total_relationships:,} relationships using ultra-fast approach")
        
        start_time = time.time()
        total_created = 0
        
        if self.config.handle_duplicates:
            logger.info("‚ö° Using MERGE strategy (handles duplicates)")
            semaphore = asyncio.Semaphore(1)
            
            async def create_relationships_for_type(rel_type: str, relationships: List[Tuple[str, str, str]]):
                async with semaphore:
                    return await self._create_relationships_bulk_by_type(rel_type, relationships)
            
            tasks = []
            for rel_type, relationships in relationships_by_type.items():
                task = asyncio.create_task(create_relationships_for_type(rel_type, relationships))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, int):
                    total_created += result
                else:
                    rel_type = list(relationships_by_type.keys())[i]
                    logger.error(f"Failed to create relationships for type {rel_type}: {result}")
        else:
            logger.info("üöÑ Using CREATE strategy (maximum speed)")
            total_created = await self._create_relationships_mega_fast(relationships_by_type)
        
        duration = time.time() - start_time
        rate = total_created / duration if duration > 0 else 0
        
        logger.info(f"üéâ Relationship creation completed:")
        logger.info(f"  Created: {total_created:,} relationships")
        logger.info(f"  Time: {duration:.2f} seconds ({duration/60:.1f} minutes)")
        logger.info(f"  Rate: {rate:.1f} relationships/second")
        
        return total_created
    
    async def _create_relationships_mega_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """MEGA-FAST: Process all relationships with tqdm progress bars"""
        
        all_relationships = []
        for rel_type, relationships in relationships_by_type.items():
            for subject_uri, predicate_uri, object_uri in relationships:
                all_relationships.append((subject_uri, predicate_uri, object_uri, rel_type))
        
        logger.info(f"üöÑ MEGA-FAST MODE: Processing {len(all_relationships):,} relationships")
        
        mega_batch_size = min(100000, max(50000, len(all_relationships) // 20))
        logger.info(f"üì¶ Using mega-batch size: {mega_batch_size:,} relationships per batch")
        
        total_created = 0
        total_batches = (len(all_relationships) + mega_batch_size - 1) // mega_batch_size
        
        with tqdm_sync(total=total_batches, desc="üöÑ Creating mega-batches", 
                      unit="mega-batch", ncols=100, colour="red") as pbar:
            
            for i in range(0, len(all_relationships), mega_batch_size):
                batch = all_relationships[i:i+mega_batch_size]
                batch_num = (i // mega_batch_size) + 1
                
                pbar.set_description(f"üöÑ Mega-batch {batch_num}/{total_batches}")
                
                try:
                    created = await self._execute_mega_batch_query(batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'rate': f"{total_created/(time.time() - self.config.batch_size):.0f}/s" if hasattr(self.config, 'batch_size') else "N/A",
                        'batch_size': f"{len(batch):,}"
                    })
                    
                except Exception as e:
                    logger.error(f"Mega-batch {batch_num} failed: {e}")
                    created = await self._fallback_mega_batch(batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'status': 'fallback'
                    })
        
        return total_created
    
    async def _create_relationships_bulk_by_type(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Create all relationships of a specific type with tqdm progress bar"""
        if not relationships:
            return 0
        
        logger.info(f"Creating {len(relationships):,} relationships of type '{rel_type}'")
        
        if len(relationships) > 500000:
            batch_size = self.config.batch_size * 10
        elif len(relationships) > 100000:
            batch_size = self.config.batch_size * 5
        elif len(relationships) > 10000:
            batch_size = self.config.batch_size * 2
        else:
            batch_size = self.config.batch_size
        
        logger.info(f"üì¶ Using adaptive batch size: {batch_size:,} relationships per batch")
        
        total_created = 0
        total_batches = (len(relationships) + batch_size - 1) // batch_size
        
        batches = []
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            batches.append(batch)
        
        with tqdm_sync(total=len(batches), desc=f"‚ö° Creating {rel_type}", 
                      unit="batch", ncols=100, colour="yellow") as pbar:
            
            for batch_idx, batch in enumerate(batches):
                batch_num = batch_idx + 1
                
                try:
                    created = await self._execute_relationship_batch_query(rel_type, batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'batch': f"{batch_num}/{total_batches}"
                    })
                    
                except Exception as e:
                    logger.error(f"Failed to create relationship batch for {rel_type}: {e}")
                    created = await self._create_relationships_smaller_batches(rel_type, batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'status': 'fallback',
                        'batch': f"{batch_num}/{total_batches}"
                    })
        
        logger.info(f"‚úÖ Completed {rel_type}: {total_created:,} relationships created")
        return total_created
    
    async def _execute_mega_batch_query(self, batch: List[Tuple[str, str, str, str]]) -> int:
        """Execute a mega-batch of relationships with optimized Cypher"""
        
        grouped_batch = defaultdict(list)
        for subject_uri, predicate_uri, object_uri, rel_type in batch:
            grouped_batch[rel_type].append({
                'subject_uri': subject_uri,
                'object_uri': object_uri
            })
        
        total_created = 0
        
        for rel_type, rel_data in grouped_batch.items():
            clean_rel_type = self._clean_relationship_type(rel_type)
            
            query = f"""
            UNWIND $batch_data AS rel
            MATCH (s {{uri: rel.subject_uri}})
            MATCH (o {{uri: rel.object_uri}})
            CREATE (s)-[:{clean_rel_type}]->(o)
            """
            
            await self.execute_query_with_retry(query, {'batch_data': rel_data})
            total_created += len(rel_data)
        
        return total_created
    
    async def _fallback_mega_batch(self, batch: List[Tuple[str, str, str, str]]) -> int:
        """Fallback for failed mega-batches: process in smaller chunks"""
        fallback_batch_size = 10000
        total_created = 0
        
        for i in range(0, len(batch), fallback_batch_size):
            small_batch = batch[i:i+fallback_batch_size]
            try:
                created = await self._execute_mega_batch_query(small_batch)
                total_created += created
            except Exception as e:
                logger.warning(f"Fallback batch failed: {e}")
                continue
        
        return total_created
    
    async def _execute_relationship_batch_query(self, rel_type: str, batch: List[Tuple[str, str, str]]) -> int:
        """Execute optimized batch relationship creation with URI-based merging"""
        batch_data = []
        for subject_uri, predicate_uri, object_uri in batch:
            rel_data = {
                'subject_uri': subject_uri,
                'object_uri': object_uri
            }
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                rel_data['predicate_uri'] = predicate_uri
            batch_data.append(rel_data)
        
        clean_rel_type = self._clean_relationship_type(rel_type)
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        if use_merge:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[:{clean_rel_type}]->(o)
                """
        else:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
        
        await self.execute_query_with_retry(query, {'batch_data': batch_data})
        return len(batch)
    
    def _clean_relationship_type(self, rel_type: str) -> str:
        """Clean relationship type to ensure valid Cypher identifier"""
        clean_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if clean_type and clean_type[0].isdigit():
            clean_type = f"_{clean_type}"
        
        if not clean_type:
            clean_type = 'RELATED_TO'
        elif len(clean_type) > 50:
            clean_type = clean_type[:50]
        
        return clean_type
    
    async def _create_relationships_smaller_batches(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Fallback: create relationships in much smaller batches"""
        batch_size = 100
        total_created = 0
        clean_rel_type = self._clean_relationship_type(rel_type)
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            try:
                batch_data = []
                for subject_uri, predicate_uri, object_uri in batch:
                    batch_data.append({
                        'subject_uri': subject_uri,
                        'object_uri': object_uri
                    })
                
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
                
                await self.execute_query_with_retry(query, {'batch_data': batch_data})
                total_created += len(batch)
                
            except Exception as e:
                logger.warning(f"Small batch failed for {rel_type}: {e}")
                for subject_uri, predicate_uri, object_uri in batch:
                    try:
                        query = """
                        MATCH (s {uri: $subject_uri})
                        MATCH (o {uri: $object_uri})
                        CREATE (s)-[:RELATED_TO]->(o)
                        """
                        await self.execute_query_with_retry(query, {
                            'subject_uri': subject_uri,
                            'object_uri': object_uri
                        })
                        total_created += 1
                    except Exception:
                        continue
        
        return total_created
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")


class OptimizedAsyncTripleBasedConverter:
    """Main async converter with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = OptimizedAsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        self.relationships_by_type: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        self.total_relationships = 0
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
    
    def _reset_conversion_state(self):
        """Reset converter state for fresh conversion"""
        logger.info("üîÑ Resetting converter state for fresh conversion...")
        
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.relationships_by_type.clear()
        self.total_relationships = 0
        
        logger.info("‚úÖ Converter state reset complete")
    
    def _setup_fresh_rdf_connection(self):
        """Setup a fresh RDF graph connection to SPARQL endpoint"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                try:
                    if hasattr(self.rdf_graph.store, 'close'):
                        self.rdf_graph.store.close()
                except:
                    pass
            
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"‚úÖ Fresh SPARQL connection established to: {self.config.sparql_endpoint}")
            
            test_query = "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o } LIMIT 1"
            try:
                list(self.rdf_graph.query(test_query))
                logger.info("‚úÖ SPARQL connection test successful")
            except Exception as test_error:
                logger.warning(f"‚ö†Ô∏è  SPARQL connection test failed: {test_error}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def _cleanup_rdf_connection(self):
        """Clean up RDF connection resources"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                if hasattr(self.rdf_graph.store, 'close'):
                    self.rdf_graph.store.close()
                    logger.info("üßπ Cleaned up SPARQL connection")
                self.rdf_graph = None
        except Exception as e:
            logger.warning(f"Error cleaning up SPARQL connection: {e}")
    
    def _execute_sparql_query_with_retry(self):
        """Execute SPARQL query with connection retry logic"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                if attempt == 0:
                    logger.info("Setting up initial SPARQL connection...")
                else:
                    logger.info(f"SPARQL query attempt {attempt + 1}/{max_attempts} - setting up fresh connection...")
                
                self._setup_fresh_rdf_connection()
                
                if self.rdf_graph is None:
                    raise Exception("Failed to establish SPARQL connection - rdf_graph is None")
                
                logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
                logger.info(f"Query preview: {self.config.triples_query[:200]}...")
                
                start_time = time.time()
                query_result = self.rdf_graph.query(self.config.triples_query)
                results = list(query_result)
                execution_time = time.time() - start_time
                
                logger.info(f"‚úÖ SPARQL query completed in {execution_time:.2f}s, retrieved {len(results)} triples")
                return results
                
            except Exception as e:
                logger.error(f"SPARQL query attempt {attempt + 1} failed: {e}")
                
                if attempt < max_attempts - 1:
                    self._cleanup_rdf_connection()
                    retry_wait = 5 * (attempt + 1)
                    logger.info(f"Retrying in {retry_wait} seconds...")
                    time.sleep(retry_wait)
                else:
                    logger.error(f"‚ùå SPARQL query failed after {max_attempts} attempts")
                    raise
        
        raise Exception("SPARQL query failed - should not reach here")
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method"""
        try:
            self._reset_conversion_state()
            self.stats.append_mode = self.config.append_to_existing_graph
            
            if self.config.append_to_existing_graph:
                logger.info("üìà Starting incremental RDF to FalkorDB conversion (append mode)...")
            else:
                logger.info("üöÄ Starting fresh RDF to FalkorDB conversion...")
            
            await self.falkordb_manager.connect()
            
            initial_stats = await self.falkordb_manager.get_initial_graph_stats()
            self.stats.initial_nodes = initial_stats['nodes']
            self.stats.initial_relationships = initial_stats['relationships']
            
            if self.config.clear_existing_graph:
                logger.info("üóëÔ∏è  Clearing existing graph as requested...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            elif not self.config.append_to_existing_graph:
                logger.info("üóëÔ∏è  Clearing graph for fresh conversion...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            
            await self.node_manager.clear()
            
            await self._execute_and_process_query()
            
            node_start = time.time()
            await self._create_nodes_in_falkordb_async()
            self.stats.node_creation_time = time.time() - node_start
            
            rel_start = time.time()
            await self._create_relationships_ultra_fast()
            self.stats.relationship_creation_time = time.time() - rel_start
            
            if self.stats.relationship_creation_time > 0:
                self.stats.relationship_creation_rate = self.stats.created_relationships / self.stats.relationship_creation_time
            
            await self._create_indexes_with_discovered_labels()
            
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            await self._finalize_stats()
            
            if self.config.append_to_existing_graph:
                logger.info("‚úÖ Incremental conversion completed successfully!")
                logger.info(f"üìä {self.stats.get_incremental_summary()}")
            else:
                logger.info("‚úÖ Fresh conversion completed successfully!")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"‚ùå Conversion failed: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("üìä Executing triples query with fresh connection...")
            
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(None, self._execute_sparql_query_with_retry)
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"üìà Retrieved {len(results):,} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("‚ö° Processing triples in optimized async batches...")
            
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with tqdm progress bar"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"üîÑ Processing {len(results):,} triples in {total_batches} batches of {batch_size}")
        
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        completed = 0
        failed = 0
        
        with tqdm_sync(total=len(tasks), desc="üîÑ Processing triple batches", 
                      unit="batch", ncols=100, colour="blue") as pbar:
            
            for future in asyncio.as_completed(tasks):
                try:
                    await future
                    completed += 1
                    self.stats.completed_batches = completed
                    pbar.update(1)
                    pbar.set_postfix({
                        'completed': f"{completed}/{total_batches}",
                        'triples': f"{self.stats.processed_triples:,}"
                    })
                    
                except Exception as e:
                    failed += 1
                    self.stats.failed_batches = failed
                    pbar.update(1)
                    pbar.set_postfix({
                        'completed': f"{completed}/{total_batches}",
                        'failed': failed,
                        'triples': f"{self.stats.processed_triples:,}"
                    })
                    logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"‚úÖ Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        if isinstance(obj, Literal):
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            object_uri = str(obj)
            
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            rel_type = self._get_relationship_type(predicate_uri)
            self.relationships_by_type[rel_type].append((subject_uri, predicate_uri, object_uri))
            self.total_relationships += 1
            self.stats.relationship_triples += 1
            
            self.stats.relationship_types_count[rel_type] = self.stats.relationship_types_count.get(rel_type, 0) + 1
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _get_relationship_type(self, predicate_uri: str) -> str:
        """Extract and clean relationship type from predicate URI"""
        parsed = urlparse(predicate_uri)
        if parsed.fragment:
            rel_type = parsed.fragment
        else:
            rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
        
        rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if not rel_type or rel_type.isdigit():
            rel_type = 'RELATED_TO'
        elif rel_type[0].isdigit():
            rel_type = f"_{rel_type}"
        
        if len(rel_type) > 50:
            rel_type = rel_type[:50]
        
        return rel_type
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB with tqdm progress bar"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"üèóÔ∏è  Creating {len(nodes):,} nodes in FalkorDB...")
        
        if not nodes:
            logger.warning("No nodes to create")
            return
        
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        total_created = 0
        
        with tqdm_sync(total=len(tasks), desc="üèóÔ∏è  Creating node batches", 
                      unit="batch", ncols=100, colour="green") as pbar:
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, int):
                    total_created += result
                    pbar.update(1)
                    pbar.set_postfix({
                        'nodes_created': f"{total_created:,}"
                    })
                else:
                    pbar.update(1)
                    pbar.set_postfix({
                        'nodes_created': f"{total_created:,}",
                        'errors': "Some failed"
                    })
                    logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"‚úÖ Successfully created {total_created:,} nodes")
        self.stats.created_nodes = total_created
    
    async def _create_relationships_ultra_fast(self):
        """Create all relationships using ultra-fast optimized approach"""
        if not self.relationships_by_type:
            logger.info("No relationships to create")
            return
        
        logger.info(f"‚ö° Creating {self.total_relationships:,} relationships using ultra-fast approach...")
        
        logger.info("üìä Relationship type distribution:")
        for rel_type, count in sorted(self.stats.relationship_types_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  {rel_type}: {count:,} relationships")
        
        created_count = await self.falkordb_manager.create_relationships_ultra_fast(self.relationships_by_type)
        self.stats.created_relationships = created_count
        
        logger.info(f"‚úÖ Successfully created {created_count:,} relationships")
    
    async def _create_indexes_with_discovered_labels(self):
        """Collect discovered labels and create indexes for them - non-blocking"""
        try:
            nodes = await self.node_manager.get_nodes()
            label_counts = Counter()
            
            for node_data in nodes.values():
                primary_label = node_data.get('primary_label')
                if primary_label:
                    label_counts[primary_label] += 1
            
            if not label_counts:
                logger.warning("No nodes with labels found - creating fallback indexes only")
                await self.falkordb_manager.create_indexes()
                return
            
            logger.info(f"Discovered {len(label_counts)} RDF class labels from {sum(label_counts.values()):,} nodes")
            
            discovered_labels = dict(label_counts)
            await self.falkordb_manager.create_indexes(discovered_labels)
            
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info("‚ÑπÔ∏è  Index already exists - this is normal when running multiple queries")
                logger.info("üí° Conversion will continue successfully without recreating indexes")
            else:
                logger.warning(f"Index creation failed but conversion will continue: {e}")
                logger.info("üí° Tip: Indexes improve query performance but are not required for functionality")
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("üîç Validating conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("‚ö†Ô∏è  No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("‚ö†Ô∏è  No relationships were created despite processing relationship triples")
        
        logger.info(f"‚úÖ Validation complete: {falkor_stats['nodes']:,} nodes, {falkor_stats['relationships']:,} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        final_falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.final_nodes = final_falkor_stats['nodes']
        self.stats.final_relationships = final_falkor_stats['relationships']
        
        if self.config.append_to_existing_graph and not self.stats.graph_was_cleared:
            actual_nodes_created = self.stats.final_nodes - self.stats.initial_nodes
            actual_rels_created = self.stats.final_relationships - self.stats.initial_relationships
            
            if actual_nodes_created >= 0:
                self.stats.created_nodes = actual_nodes_created
            if actual_rels_created >= 0:
                self.stats.created_relationships = actual_rels_created
                
            nodes_processed = len(await self.node_manager.get_nodes())
            self.stats.skipped_nodes = max(0, nodes_processed - actual_nodes_created)
            self.stats.skipped_relationships = max(0, self.total_relationships - actual_rels_created)
        else:
            self.stats.created_nodes = self.stats.final_nodes
            self.stats.created_relationships = self.stats.final_relationships
        
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        unique_objects = set()
        for rel_list in self.relationships_by_type.values():
            for _, _, obj_uri in rel_list:
                unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)


def create_optimized_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph", 
                          append_mode: bool = True) -> OptimizedAsyncTripleConfig:
    """Create an optimized configuration for fast edge loading with incremental support"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        append_to_existing_graph=append_mode,
        clear_existing_graph=False,
        handle_duplicates=True,
        batch_size=2000,
        max_concurrent_batches=3,
        connection_pool_size=10,
        sparql_timeout=7200,
        falkordb_timeout=300,
        preserve_uri_properties=False,
        disable_relationship_properties=True,
        group_relationships_by_type=True,
        use_bulk_relationship_creation=True,
        use_shortened_uris=True,
        create_indexes=True,
        validate_conversion=False,
        export_stats=True,
        progress_update_interval=25,
    )


def create_ultra_fast_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration optimized for 2M+ edges - MAXIMUM SPEED"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        append_to_existing_graph=True,
        clear_existing_graph=False,
        handle_duplicates=False,
        batch_size=50000,
        max_concurrent_batches=1,
        connection_pool_size=5,
        sparql_timeout=7200,
        falkordb_timeout=1800,
        preserve_uri_properties=False,
        disable_relationship_properties=True,
        group_relationships_by_type=True,
        use_bulk_relationship_creation=True,
        use_shortened_uris=False,
        create_indexes=False,
        validate_conversion=False,
        export_stats=False,
        progress_update_interval=1,
        max_retries=2,
        retry_delay=1,
    )


def create_append_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for appending to existing graph with robust duplicate handling"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=True)
    config.append_to_existing_graph = True
    config.clear_existing_graph = False
    config.handle_duplicates = True
    config.skip_existing_nodes = True
    config.skip_existing_relationships = True
    return config


def create_safe_multi_query_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration specifically for multiple query runs with maximum safety"""
    config = create_append_config(sparql_endpoint, triples_query, graph_name)
    config.max_retries = 3
    config.retry_delay = 2
    config.validate_conversion = False
    config.export_stats = False
    config.create_indexes = True
    return config


async def clear_all_indexes(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """Clear all indexes from a graph - useful for fresh starts"""
    
    logger.info(f"üßπ Clearing all indexes from graph '{graph_name}'...")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        try:
            result = await falkordb_manager.execute_query_with_retry("CALL db.indexes()")
            if result.result_set:
                for row in result.result_set:
                    if len(row) > 0:
                        index_info = str(row[0])
                        if ":" in index_info and "(" in index_info:
                            label_part = index_info.split("(")[0].replace("INDEX", "").replace(":", "").strip()
                            if label_part:
                                try:
                                    drop_query = f"DROP INDEX FOR (n:{label_part}) ON (n.uri)"
                                    await falkordb_manager.execute_query_with_retry(drop_query)
                                    logger.info(f"‚úÖ Dropped index for label '{label_part}'")
                                except Exception as e:
                                    logger.debug(f"Could not drop index for {label_part}: {e}")
        except Exception as e:
            logger.info(f"Index clearing completed with some errors (this is normal): {e}")
        
        logger.info("‚úÖ Index clearing completed")
        
    except Exception as e:
        logger.warning(f"Index clearing failed: {e}")
    finally:
        await falkordb_manager.close()


async def deduplicate_graph(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """Automatic deduplication: Remove duplicate nodes and relationships with progress tracking"""
    
    logger.info(f"üßπ Starting automatic deduplication of graph '{graph_name}'...")
    logger.info("‚ú® This ensures your final graph has no duplicates!")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        initial_stats = await falkordb_manager.get_graph_stats()
        logger.info(f"üìä Before deduplication: {initial_stats['nodes']:,} nodes, {initial_stats['relationships']:,} relationships")
        
        dedup_steps = [
            ("üîç Scanning for duplicate nodes", "duplicate node detection"),
            ("üßπ Removing duplicate nodes", "node deduplication"),
            ("üîç Scanning for duplicate relationships", "duplicate relationship detection"),
            ("üßπ Removing duplicate relationships", "relationship deduplication"),
            ("üìä Finalizing cleanup", "cleanup finalization")
        ]
        
        with tqdm_sync(total=len(dedup_steps), desc="üßπ Deduplicating graph", 
                      unit="step", ncols=100, colour="magenta") as pbar:
            
            pbar.set_description(dedup_steps[0][0])
            pbar.update(1)
            
            pbar.set_description(dedup_steps[1][0])
            dedup_nodes_query = """
            MATCH (n)
            WITH n.uri as uri, collect(n) as nodes
            WHERE size(nodes) > 1
            WITH uri, nodes, 
                 [node in nodes | size(keys(node))] as prop_counts,
                 range(0, size(nodes)-1) as indices
            WITH uri, nodes, 
                 [i in indices | {node: nodes[i], props: prop_counts[i]}] as node_info
            WITH uri, node_info, 
                 reduce(max_props = -1, info in node_info | 
                       CASE WHEN info.props > max_props THEN info.props ELSE max_props END) as max_prop_count
            WITH uri, [info in node_info WHERE info.props = max_prop_count][0].node as keeper,
                 [info in node_info WHERE info.props < max_prop_count | info.node] as to_delete
            UNWIND to_delete as duplicate_node
            DETACH DELETE duplicate_node
            """
            
            try:
                await falkordb_manager.execute_query_with_retry(dedup_nodes_query)
                logger.info("‚úÖ Node deduplication completed")
            except Exception as e:
                logger.warning(f"Node deduplication query failed (may be no duplicates): {e}")
            
            pbar.update(1)
            
            pbar.set_description(dedup_steps[2][0])
            pbar.update(1)
            
            pbar.set_description(dedup_steps[3][0])
            dedup_rels_query = """
            MATCH (a)-[r]->(b)
            WITH a, b, type(r) as rel_type, collect(r) as rels
            WHERE size(rels) > 1
            WITH a, b, rel_type, rels[1..] as duplicates
            UNWIND duplicates as duplicate_rel
            DELETE duplicate_rel
            """
            
            try:
                await falkordb_manager.execute_query_with_retry(dedup_rels_query)
                logger.info("‚úÖ Relationship deduplication completed")
            except Exception as e:
                logger.warning(f"Relationship deduplication query failed (may be no duplicates): {e}")
            
            pbar.update(1)
            
            pbar.set_description(dedup_steps[4][0])
            final_stats = await falkordb_manager.get_graph_stats()
            pbar.update(1)
        
        logger.info(f"üìä After deduplication: {final_stats['nodes']:,} nodes, {final_stats['relationships']:,} relationships")
        
        nodes_removed = initial_stats['nodes'] - final_stats['nodes']
        rels_removed = initial_stats['relationships'] - final_stats['relationships']
        
        if nodes_removed > 0 or rels_removed > 0:
            logger.info(f"üßπ DEDUPLICATION RESULTS:")
            logger.info(f"   ‚úÖ Removed {nodes_removed:,} duplicate nodes")
            logger.info(f"   ‚úÖ Removed {rels_removed:,} duplicate relationships")
            logger.info(f"   ‚ú® Your graph is now completely deduplicated!")
        else:
            logger.info("‚ú® No duplicates found - your graph was already perfectly clean!")
        
        return {
            'initial_nodes': initial_stats['nodes'],
            'initial_relationships': initial_stats['relationships'],
            'final_nodes': final_stats['nodes'],
            'final_relationships': final_stats['relationships'],
            'nodes_removed': nodes_removed,
            'relationships_removed': rels_removed
        }
        
    except Exception as e:
        logger.error(f"‚ùå Deduplication failed: {e}")
        raise
    finally:
        await falkordb_manager.close()


async def run_multiple_queries_with_deduplication(sparql_endpoint: str, queries_list: List[str], 
                                                graph_name: str = "merged_graph", clear_indexes_first: bool = False):
    """
    Run multiple queries with automatic deduplication and progress tracking
    
    Automatic deduplication: After all queries complete, the system will:
       1. Merge duplicate nodes (same URI) 
       2. Remove duplicate relationships
       3. Ensure your final graph is completely clean
    """
    
    if clear_indexes_first:
        logger.info("üßπ Clearing existing indexes to avoid conflicts...")
        await clear_all_indexes(graph_name)
    
    logger.info(f"üîÑ Running {len(queries_list)} queries with automatic deduplication...")
    logger.info(f"üìä Target graph: '{graph_name}'")
    logger.info("‚ú® AUTOMATIC DEDUPLICATION will run after all queries complete!")
    
    all_stats = []
    
    with tqdm_sync(total=len(queries_list), desc="üöÄ Processing queries", 
                  unit="query", ncols=100, colour="cyan") as query_pbar:
        
        for i, query in enumerate(queries_list, 1):
            query_pbar.set_description(f"üöÄ Query {i}/{len(queries_list)}")
            logger.info(f"\n{'='*60}")
            logger.info(f"üöÄ QUERY {i}/{len(queries_list)}")
            logger.info(f"{'='*60}")
            logger.info(f"Query preview: {query[:200]}...")
            
            try:
                config = create_safe_multi_query_config(sparql_endpoint, query, graph_name)
                converter = OptimizedAsyncTripleBasedConverter(config)
                stats = await converter.convert()
                all_stats.append(stats)
                
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'total_edges': f"{sum(s.created_relationships for s in all_stats):,}"
                })
                
                logger.info(f"‚úÖ Query {i} completed: {stats.get_incremental_summary()}")
                
            except Exception as e:
                error_msg = str(e).lower()
                if "already indexed" in error_msg:
                    logger.warning(f"‚ö†Ô∏è  Query {i} hit index conflict: {e}")
                    logger.info("üí° This is usually not critical - data may still be processed")
                else:
                    logger.error(f"‚ùå Query {i} failed: {e}")
                
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'failed': f"{i - len(all_stats)}"
                })
                continue
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üéâ ALL QUERIES COMPLETED")
    logger.info(f"{'='*80}")
    
    if all_stats:
        final_stats = all_stats[-1]
        logger.info(f"üìä Pre-deduplication: {final_stats.final_nodes:,} nodes, {final_stats.final_relationships:,} relationships")
        
        total_processed = sum(s.processed_triples for s in all_stats)
        total_new_nodes = sum(s.created_nodes for s in all_stats)
        total_new_relationships = sum(s.created_relationships for s in all_stats)
        
        logger.info(f"üìà Processing summary:")
        logger.info(f"   Total triples processed: {total_processed:,}")
        logger.info(f"   Total new nodes added: {total_new_nodes:,}")
        logger.info(f"   Total new relationships added: {total_new_relationships:,}")
        logger.info(f"   Successful queries: {len(all_stats)}/{len(queries_list)}")
        
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® AUTOMATIC DEDUPLICATION STARTING")
        logger.info(f"{'='*80}")
        logger.info("üîÑ Running final deduplication to ensure a perfectly clean graph...")
        
        dedup_results = await deduplicate_graph(graph_name)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® DEDUPLICATION COMPLETED")
        logger.info(f"{'='*80}")
        logger.info(f"üìä Final clean graph: {dedup_results['final_nodes']:,} nodes, {dedup_results['final_relationships']:,} relationships")
        
        if dedup_results['nodes_removed'] > 0 or dedup_results['relationships_removed'] > 0:
            logger.info(f"üßπ Cleaned up {dedup_results['nodes_removed']:,} duplicate nodes and {dedup_results['relationships_removed']:,} duplicate relationships")
        else:
            logger.info("‚ú® Graph was already perfectly clean - no duplicates found!")
        
        return all_stats
    else:
        logger.error("‚ùå No queries completed successfully")
        return []


async def run_ultra_fast_conversion(sparql_endpoint: str, queries_list: List[str], 
                                 graph_name: str = "ultra_fast_graph") -> List[AsyncConversionStats]:
    """
    Ultra-fast mode: Optimized for 2M+ edges with automatic deduplication
    
    This mode sacrifices some safety features for maximum performance:
    - Uses massive batch sizes (50K-100K edges per batch)
    - Skips duplicate checking during load (for maximum speed)
    - Minimal indexing during load (creates indexes after)
    - Single-threaded processing for stability with large batches
    
    Automatic deduplication: Still runs comprehensive deduplication at the end!
    
    Best for: Clean data with 2M+ edges where speed is critical
    """
    
    logger.info("üöÑ ULTRA-FAST MODE: Optimized for 2M+ edges")
    logger.info("=" * 60)
    logger.info("‚ö° Ultra-fast optimizations enabled:")
    logger.info("   üöÑ Mega-batch processing (50K-100K edges/batch)")
    logger.info("   ‚ö° CREATE-only mode during load (maximum speed)")
    logger.info("   üéØ Single-threaded for stability")
    logger.info("   üìä Minimal overhead during processing")
    logger.info("   üèóÔ∏è  Indexes created after loading")
    logger.info("   ‚ú® AUTOMATIC DEDUPLICATION at the end")
    
    all_stats = []
    
    with tqdm_sync(total=len(queries_list), desc="üöÑ Ultra-fast queries", 
                  unit="query", ncols=100, colour="red") as query_pbar:
        
        for i, query in enumerate(queries_list, 1):
            query_pbar.set_description(f"üöÑ Ultra-fast query {i}/{len(queries_list)}")
            logger.info(f"\n{'='*60}")
            logger.info(f"üöÑ ULTRA-FAST QUERY {i}/{len(queries_list)}")
            logger.info(f"{'='*60}")
            logger.info(f"Query preview: {query[:200]}...")
            
            try:
                config = create_ultra_fast_config(sparql_endpoint, query, graph_name)
                converter = OptimizedAsyncTripleBasedConverter(config)
                stats = await converter.convert()
                all_stats.append(stats)
                
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'total_edges': f"{sum(s.created_relationships for s in all_stats):,}",
                    'rate': f"{stats.relationship_creation_rate:.0f}/s" if stats.relationship_creation_rate > 0 else "N/A"
                })
                
                logger.info(f"üöÑ Ultra-fast query {i} completed: {stats.get_incremental_summary()}")
                
                if stats.relationship_creation_rate > 0:
                    logger.info(f"‚ö° Edge creation rate: {stats.relationship_creation_rate:.0f} relationships/second")
                
            except Exception as e:
                logger.error(f"‚ùå Ultra-fast query {i} failed: {e}")
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'failed': f"{i - len(all_stats)}"
                })
                continue
    
    if all_stats:
        logger.info("\nüèóÔ∏è  Creating indexes after ultra-fast loading...")
        try:
            final_config = create_ultra_fast_config(sparql_endpoint, queries_list[0], graph_name)
            final_config.create_indexes = True
            
            falkordb_manager = OptimizedAsyncFalkorDBManager(final_config)
            await falkordb_manager.connect()
            
            final_stats = await falkordb_manager.get_graph_stats()
            logger.info(f"üìä Pre-deduplication: {final_stats['nodes']:,} nodes, {final_stats['relationships']:,} relationships")
            
            await falkordb_manager.create_index_safely("Resource")
            await falkordb_manager.close()
            logger.info("‚úÖ Post-loading indexing completed")
            
        except Exception as e:
            logger.warning(f"Post-loading indexing failed: {e}")
    
    if all_stats:
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® AUTOMATIC DEDUPLICATION STARTING")
        logger.info(f"{'='*80}")
        logger.info("üîÑ Running comprehensive deduplication on ultra-fast loaded data...")
        
        dedup_results = await deduplicate_graph(graph_name)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® ULTRA-FAST + DEDUPLICATION COMPLETED")
        logger.info(f"{'='*80}")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üöÑ ULTRA-FAST MODE COMPLETED")
    logger.info(f"{'='*80}")
    
    if all_stats:
        total_edges = sum(s.created_relationships for s in all_stats)
        total_time = sum((s.end_time - s.start_time).total_seconds() for s in all_stats if s.end_time)
        
        logger.info(f"üìä Ultra-fast results:")
        logger.info(f"   Total edges created: {total_edges:,}")
        logger.info(f"   Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
        logger.info(f"   Overall edge rate: {total_edges/total_time:.0f} edges/second")
        logger.info(f"   Successful queries: {len(all_stats)}/{len(queries_list)}")
        
        if 'dedup_results' in locals():
            logger.info(f"   Final clean graph: {dedup_results['final_nodes']:,} nodes, {dedup_results['final_relationships']:,} relationships")
            if dedup_results['nodes_removed'] > 0 or dedup_results['relationships_removed'] > 0:
                logger.info(f"   Removed duplicates: {dedup_results['nodes_removed']:,} nodes, {dedup_results['relationships_removed']:,} relationships")
            else:
                logger.info(f"   ‚ú® No duplicates found - data was perfectly clean!")
        
        edge_rate = total_edges / total_time if total_time > 0 else 0
        if edge_rate > 10000:
            logger.info("üöÄ OUTSTANDING: Ultra-high-speed edge processing (>10K edges/sec)")
        elif edge_rate > 5000:
            logger.info("üéâ EXCELLENT: High-speed edge processing (>5K edges/sec)")
        elif edge_rate > 2000:
            logger.info("‚úÖ GOOD: Fast edge processing (>2K edges/sec)")
        else:
            logger.info("‚ö†Ô∏è  MODERATE: Consider optimizing system resources")
        
        return all_stats
    else:
        logger.error("‚ùå No ultra-fast queries completed successfully")
        return []


async def run_queries_simple(endpoint: str, queries: List[str], graph_name: str = "my_graph") -> bool:
    """
    Simple interface: Run multiple queries with all error handling built-in
    
    This function handles ALL potential issues automatically:
    - Index conflicts
    - SPARQL connection errors
    - URI merging
    - Deduplication
    - Error recovery
    """
    
    logger.info("üöÄ STARTING SIMPLE MULTI-QUERY INTERFACE")
    logger.info("=" * 60)
    logger.info("‚ú® Automatic features enabled:")
    logger.info("   ‚úÖ Index conflict prevention")
    logger.info("   ‚úÖ URI-based node merging") 
    logger.info("   ‚úÖ Duplicate relationship prevention")
    logger.info("   ‚úÖ Error recovery (failed queries don't stop others)")
    logger.info("   ‚úÖ Final deduplication cleanup")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        success = len(stats_list) > 0
        
        if success:
            final_stats = stats_list[-1]
            logger.info("\nüéâ SIMPLE INTERFACE: SUCCESS!")
            logger.info(f"üìä Your graph '{graph_name}' has {final_stats.final_nodes:,} nodes and {final_stats.final_relationships:,} relationships")
            logger.info(f"‚úÖ {len(stats_list)}/{len(queries)} queries completed successfully")
            
            logger.info("\nüîç To explore your data, connect to FalkorDB and run:")
            logger.info(f"   MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10")
            logger.info(f"   MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10")
            
        else:
            logger.error("\n‚ùå SIMPLE INTERFACE: All queries failed")
            logger.info("üí° Check your SPARQL endpoint and query syntax")
            
        return success
        
    except Exception as e:
        logger.error(f"‚ùå Simple interface failed: {e}")
        
        if "already indexed" in str(e).lower():
            logger.info("üí° Index error detected. The system should handle this automatically.")
            logger.info("üí° If this persists, try using a different graph_name")
        elif "sparql" in str(e).lower() or "endpoint" in str(e).lower():
            logger.info("üí° SPARQL connection issue. Check your endpoint URL and network connectivity")
        else:
            logger.info("üí° Unexpected error. Check your query syntax and FalkorDB connection")
        
        return False


async def example_multiple_queries():
    """Example of running multiple queries with automatic merging and deduplication"""
    
    endpoint = "https://dbpedia.org/sparql"
    graph_name = "multi_query_graph"
    
    queries = [
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/A"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/B"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(?predicate = <http://dbpedia.org/ontology/birthPlace>)
        }
        LIMIT 30
        """
    ]
    
    logger.info("üöÄ EXAMPLE: Multiple Queries with Automatic Merging")
    logger.info("=" * 60)
    logger.info("üí° This example shows how to:")
    logger.info("   1. Run multiple SPARQL queries")
    logger.info("   2. Automatically merge results into one graph")
    logger.info("   3. Handle duplicate data seamlessly")
    logger.info("   4. Perform final deduplication")
    logger.info("   5. Handle index conflicts gracefully")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        if stats_list:
            logger.info("\nüéâ SUCCESS: All queries completed and merged!")
            logger.info(f"üìä Final graph '{graph_name}' is ready for use")
            
            logger.info("\nüîç Sample queries to explore your merged data:")
            sample_queries = [
                f"MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10",
                f"MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10", 
                f"MATCH (n) RETURN n.uri, labels(n) LIMIT 5"
            ]
            
            for i, query in enumerate(sample_queries, 1):
                logger.info(f"   {i}. {query}")
        
        return stats_list
        
    except Exception as e:
        logger.error(f"‚ùå Multiple queries example failed: {e}")
        logger.info("üí° If you're getting index errors, try setting clear_indexes_first=True")
        raise


if __name__ == "__main__":
    asyncio.run(example_multiple_queries())
