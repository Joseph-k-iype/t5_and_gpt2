#!/usr/bin/env python3
"""
Multiple Approaches for Large RDF to FalkorDB Conversion
=======================================================

This provides several proven methods to handle 17GB+ TTL files:
1. Apache Jena RIOT + Custom Parser (Recommended)
2. File Splitting + Batch Processing  
3. External Tool Chain + CSV Import
4. Database-to-Database Transfer
5. Streaming Line Parser (No RDFLib)

Requirements:
    pip install falkordb psutil tqdm pandas
    # Optional: Apache Jena for RIOT tool
"""

import subprocess
import tempfile
import shutil
import csv
import json
import time
import logging
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any, Iterator
from dataclasses import dataclass
import re
import os
import gc

# Core libraries
import psutil
from tqdm import tqdm
import pandas as pd

# FalkorDB
import falkordb


@dataclass
class ConversionConfig:
    """Configuration for multiple conversion approaches."""
    # Method selection
    method: str = "auto"  # auto, jena, split, csv, streaming, db2db
    
    # File handling
    chunk_size: int = 1000000        # Lines per chunk when splitting
    temp_dir: Optional[str] = None   # Temporary directory
    keep_temp_files: bool = False    # Keep intermediate files
    
    # Database
    host: str = "localhost"
    port: int = 6379
    password: Optional[str] = None
    graph_name: str = "knowledge_graph"
    
    # Performance
    max_memory_mb: int = 4000
    batch_size: int = 1000
    parallel_workers: int = 4
    
    # Safety
    max_errors: int = 100000         # Max errors before stopping
    skip_long_lines: bool = True     # Skip extremely long lines
    max_line_length: int = 50000     # Max line length to process


class Method1_JenaRiotConverter:
    """Method 1: Use Apache Jena RIOT for robust parsing + custom loader."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        self.temp_dir = Path(config.temp_dir or tempfile.mkdtemp())
        
    def check_jena_available(self) -> bool:
        """Check if Apache Jena RIOT is available."""
        try:
            result = subprocess.run(['riot', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def convert(self, ttl_file: str) -> bool:
        """Convert using Jena RIOT + custom processing."""
        if not self.check_jena_available():
            print("❌ Apache Jena RIOT not found. Install with:")
            print("   wget https://archive.apache.org/dist/jena/binaries/apache-jena-4.9.0.tar.gz")
            print("   tar -xzf apache-jena-4.9.0.tar.gz")
            print("   export PATH=$PATH:$(pwd)/apache-jena-4.9.0/bin")
            return False
        
        try:
            print(f"🔧 Using Apache Jena RIOT to convert {ttl_file}")
            
            # Convert TTL to N-Triples using RIOT (very robust)
            nt_file = self.temp_dir / "converted.nt"
            
            cmd = [
                'riot', 
                '--output=NT',  # N-Triples output
                '--strict',     # Strict parsing
                str(ttl_file)
            ]
            
            print("⏳ Converting to N-Triples with RIOT...")
            with open(nt_file, 'w', encoding='utf-8') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)
            
            if result.returncode != 0:
                print(f"❌ RIOT conversion failed: {result.stderr}")
                return False
            
            print(f"✅ RIOT conversion complete: {nt_file}")
            
            # Now process the clean N-Triples
            return self._process_ntriples(nt_file)
            
        except Exception as e:
            print(f"❌ Jena RIOT method failed: {e}")
            return False
    
    def _process_ntriples(self, nt_file: Path) -> bool:
        """Process clean N-Triples file."""
        try:
            db = falkordb.FalkorDB(host=self.config.host, port=self.config.port)
            graph = db.select_graph(self.config.graph_name)
            
            # Simple N-Triples parser
            nodes = {}
            edges = []
            properties = defaultdict(list)
            
            print("📊 Processing N-Triples...")
            
            with open(nt_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(tqdm(f, desc="Processing"), 1):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    try:
                        # Simple N-Triples parsing
                        parts = self._parse_ntriple_line(line)
                        if parts:
                            subject, predicate, obj = parts
                            self._process_triple_simple(subject, predicate, obj, nodes, edges, properties)
                        
                        # Batch insert every 10000 triples
                        if line_num % 10000 == 0:
                            self._insert_batch(graph, nodes, edges, properties)
                            nodes.clear()
                            edges.clear()
                            properties.clear()
                            gc.collect()
                    
                    except Exception as e:
                        if line_num % 10000 == 0:
                            print(f"⚠️  Errors at line {line_num}: {e}")
                        continue
            
            # Final batch
            self._insert_batch(graph, nodes, edges, properties)
            print("✅ N-Triples processing complete")
            return True
            
        except Exception as e:
            print(f"❌ N-Triples processing failed: {e}")
            return False
    
    def _parse_ntriple_line(self, line: str) -> Optional[Tuple[str, str, str]]:
        """Simple N-Triples line parser."""
        try:
            # Remove trailing dot
            if line.endswith(' .'):
                line = line[:-2]
            
            # Split by whitespace, handling quoted literals
            parts = []
            current_part = ""
            in_quotes = False
            
            i = 0
            while i < len(line):
                char = line[i]
                if char == '"' and (i == 0 or line[i-1] != '\\'):
                    in_quotes = not in_quotes
                    current_part += char
                elif char == ' ' and not in_quotes:
                    if current_part:
                        parts.append(current_part)
                        current_part = ""
                else:
                    current_part += char
                i += 1
            
            if current_part:
                parts.append(current_part)
            
            if len(parts) >= 3:
                return parts[0], parts[1], ' '.join(parts[2:])
            
            return None
            
        except Exception:
            return None
    
    def _process_triple_simple(self, subject: str, predicate: str, obj: str, 
                              nodes: dict, edges: list, properties: dict):
        """Process a triple with simple logic."""
        try:
            # Clean URIs
            subject = subject.strip('<>')
            predicate = predicate.strip('<>')
            
            # Generate IDs
            subj_id = hashlib.md5(subject.encode()).hexdigest()[:12]
            
            # Ensure subject node exists
            if subj_id not in nodes:
                nodes[subj_id] = {'id': subj_id, 'uri': subject, 'labels': ['Resource']}
            
            # Handle different object types
            if obj.startswith('"'):
                # Literal - add as property
                literal_value = obj.strip('"').split('"')[0]  # Simple literal extraction
                prop_name = predicate.split('/')[-1].split('#')[-1]
                properties[subj_id].append({'property': prop_name, 'value': literal_value})
            else:
                # URI - create edge
                obj = obj.strip('<>')
                obj_id = hashlib.md5(obj.encode()).hexdigest()[:12]
                
                if obj_id not in nodes:
                    nodes[obj_id] = {'id': obj_id, 'uri': obj, 'labels': ['Resource']}
                
                edge_type = predicate.split('/')[-1].split('#')[-1]
                edges.append({'source': subj_id, 'target': obj_id, 'type': edge_type})
                
        except Exception:
            pass  # Skip problematic triples
    
    def _insert_batch(self, graph, nodes: dict, edges: list, properties: dict):
        """Insert a batch of data."""
        try:
            # Insert nodes
            if nodes:
                node_list = list(nodes.values())
                query = """
                UNWIND $nodes AS node
                MERGE (n:Resource {id: node.id})
                SET n.uri = node.uri
                """
                graph.query(query, {'nodes': node_list})
            
            # Insert edges
            if edges:
                for edge in edges:
                    try:
                        query = f"""
                        MATCH (a {{id: $source}}), (b {{id: $target}})
                        MERGE (a)-[r:{edge['type']}]->(b)
                        """
                        graph.query(query, edge)
                    except:
                        continue
            
            # Set properties
            for node_id, props in properties.items():
                for prop in props:
                    try:
                        query = f"""
                        MATCH (n {{id: $node_id}})
                        SET n.{prop['property']} = $value
                        """
                        graph.query(query, {'node_id': node_id, 'value': prop['value']})
                    except:
                        continue
                        
        except Exception as e:
            print(f"Batch insert error: {e}")


class Method2_FileSplitter:
    """Method 2: Split large file into smaller chunks and process each."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        self.temp_dir = Path(config.temp_dir or tempfile.mkdtemp())
        
    def convert(self, ttl_file: str) -> bool:
        """Convert by splitting file into manageable chunks."""
        try:
            print(f"📂 Splitting {ttl_file} into chunks...")
            
            # Split file
            chunk_files = self._split_file(ttl_file)
            
            print(f"📊 Processing {len(chunk_files)} chunks...")
            
            # Process each chunk
            db = falkordb.FalkorDB(host=self.config.host, port=self.config.port)
            graph = db.select_graph(self.config.graph_name)
            
            for i, chunk_file in enumerate(chunk_files):
                print(f"🔄 Processing chunk {i+1}/{len(chunk_files)}: {chunk_file}")
                self._process_chunk_with_rdflib(chunk_file, graph)
                
                if not self.config.keep_temp_files:
                    chunk_file.unlink()
            
            print("✅ File splitting method complete")
            return True
            
        except Exception as e:
            print(f"❌ File splitting method failed: {e}")
            return False
    
    def _split_file(self, ttl_file: str) -> List[Path]:
        """Split TTL file into smaller chunks."""
        chunk_files = []
        chunk_num = 0
        
        with open(ttl_file, 'r', encoding='utf-8', errors='ignore') as infile:
            current_chunk = []
            
            for line_num, line in enumerate(infile, 1):
                if self.config.skip_long_lines and len(line) > self.config.max_line_length:
                    continue
                
                current_chunk.append(line)
                
                if len(current_chunk) >= self.config.chunk_size:
                    chunk_file = self._write_chunk(current_chunk, chunk_num)
                    chunk_files.append(chunk_file)
                    current_chunk = []
                    chunk_num += 1
            
            # Write final chunk
            if current_chunk:
                chunk_file = self._write_chunk(current_chunk, chunk_num)
                chunk_files.append(chunk_file)
        
        return chunk_files
    
    def _write_chunk(self, lines: List[str], chunk_num: int) -> Path:
        """Write chunk to file."""
        chunk_file = self.temp_dir / f"chunk_{chunk_num:06d}.ttl"
        
        with open(chunk_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        
        return chunk_file
    
    def _process_chunk_with_rdflib(self, chunk_file: Path, graph):
        """Process chunk using RDFLib (smaller files are safer)."""
        try:
            import rdflib
            
            g = rdflib.Graph()
            g.parse(str(chunk_file), format='turtle')
            
            # Simple processing
            for subject, predicate, obj in g:
                try:
                    # Basic triple insertion
                    subj_id = hashlib.md5(str(subject).encode()).hexdigest()[:12]
                    
                    if isinstance(obj, rdflib.Literal):
                        # Property
                        prop_name = str(predicate).split('/')[-1].split('#')[-1]
                        query = f"""
                        MERGE (n:Resource {{id: $id}})
                        SET n.uri = $uri, n.{prop_name} = $value
                        """
                        graph.query(query, {
                            'id': subj_id,
                            'uri': str(subject),
                            'value': str(obj)
                        })
                    else:
                        # Relationship
                        obj_id = hashlib.md5(str(obj).encode()).hexdigest()[:12]
                        edge_type = str(predicate).split('/')[-1].split('#')[-1]
                        
                        # Create nodes and edge
                        graph.query("MERGE (n:Resource {id: $id}) SET n.uri = $uri", 
                                   {'id': subj_id, 'uri': str(subject)})
                        graph.query("MERGE (n:Resource {id: $id}) SET n.uri = $uri", 
                                   {'id': obj_id, 'uri': str(obj)})
                        
                        query = f"""
                        MATCH (a {{id: $source}}), (b {{id: $target}})
                        MERGE (a)-[r:{edge_type}]->(b)
                        """
                        graph.query(query, {'source': subj_id, 'target': obj_id})
                        
                except Exception:
                    continue  # Skip problematic triples
                    
        except Exception as e:
            print(f"Chunk processing error: {e}")


class Method3_CSVPipeline:
    """Method 3: Convert to CSV first, then use FalkorDB bulk loader."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        self.temp_dir = Path(config.temp_dir or tempfile.mkdtemp())
    
    def convert(self, ttl_file: str) -> bool:
        """Convert via CSV pipeline."""
        try:
            print("🔄 Converting TTL to CSV format...")
            
            # Step 1: Convert TTL to N-Triples if possible
            nt_file = self._ttl_to_ntriples(ttl_file)
            
            # Step 2: Parse N-Triples to CSV
            nodes_csv, edges_csv = self._ntriples_to_csv(nt_file)
            
            # Step 3: Load CSV into FalkorDB
            return self._load_csv_to_falkordb(nodes_csv, edges_csv)
            
        except Exception as e:
            print(f"❌ CSV pipeline method failed: {e}")
            return False
    
    def _ttl_to_ntriples(self, ttl_file: str) -> Path:
        """Convert TTL to N-Triples using available tools."""
        nt_file = self.temp_dir / "converted.nt"
        
        # Try rapper first
        if shutil.which('rapper'):
            cmd = ['rapper', '-i', 'turtle', '-o', 'ntriples', ttl_file]
            with open(nt_file, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE)
            if result.returncode == 0:
                return nt_file
        
        # Try riot
        if shutil.which('riot'):
            cmd = ['riot', '--output=NT', ttl_file]
            with open(nt_file, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE)
            if result.returncode == 0:
                return nt_file
        
        # Fallback: copy original file
        print("⚠️  No RDF converter found, using original file")
        return Path(ttl_file)
    
    def _ntriples_to_csv(self, nt_file: Path) -> Tuple[Path, Path]:
        """Convert N-Triples to CSV files."""
        nodes_csv = self.temp_dir / "nodes.csv"
        edges_csv = self.temp_dir / "edges.csv"
        
        nodes = {}
        edges = []
        
        print("📊 Parsing to CSV format...")
        
        with open(nt_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in tqdm(f, desc="Converting to CSV"):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                try:
                    # Simple N-Triples parsing
                    if line.endswith(' .'):
                        line = line[:-2]
                    
                    # Very basic parsing - just split and clean
                    parts = line.split(' ', 2)
                    if len(parts) >= 3:
                        subject = parts[0].strip('<>')
                        predicate = parts[1].strip('<>')
                        obj = parts[2]
                        
                        subj_id = hashlib.md5(subject.encode()).hexdigest()[:12]
                        
                        # Add subject node
                        if subj_id not in nodes:
                            nodes[subj_id] = {
                                'id': subj_id,
                                'uri': subject,
                                'label': 'Resource'
                            }
                        
                        # Handle object
                        if obj.startswith('"'):
                            # Literal - extract value
                            literal_value = obj.split('"')[1] if '"' in obj[1:] else obj
                            prop_name = predicate.split('/')[-1].split('#')[-1]
                            nodes[subj_id][prop_name] = literal_value
                        else:
                            # URI - create edge
                            obj = obj.strip('<>')
                            obj_id = hashlib.md5(obj.encode()).hexdigest()[:12]
                            
                            if obj_id not in nodes:
                                nodes[obj_id] = {
                                    'id': obj_id,
                                    'uri': obj,
                                    'label': 'Resource'
                                }
                            
                            edge_type = predicate.split('/')[-1].split('#')[-1]
                            edges.append({
                                'source': subj_id,
                                'target': obj_id,
                                'type': edge_type
                            })
                
                except Exception:
                    continue  # Skip problematic lines
        
        # Write CSV files
        self._write_nodes_csv(nodes, nodes_csv)
        self._write_edges_csv(edges, edges_csv)
        
        return nodes_csv, edges_csv
    
    def _write_nodes_csv(self, nodes: dict, csv_file: Path):
        """Write nodes to CSV."""
        if not nodes:
            return
        
        # Get all possible columns
        all_columns = set()
        for node in nodes.values():
            all_columns.update(node.keys())
        
        columns = ['id', 'uri', 'label'] + sorted(all_columns - {'id', 'uri', 'label'})
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=columns)
            writer.writeheader()
            
            for node in nodes.values():
                row = {col: node.get(col, '') for col in columns}
                writer.writerow(row)
    
    def _write_edges_csv(self, edges: list, csv_file: Path):
        """Write edges to CSV."""
        if not edges:
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['source', 'target', 'type'])
            writer.writeheader()
            writer.writerows(edges)
    
    def _load_csv_to_falkordb(self, nodes_csv: Path, edges_csv: Path) -> bool:
        """Load CSV files into FalkorDB."""
        try:
            db = falkordb.FalkorDB(host=self.config.host, port=self.config.port)
            graph = db.select_graph(self.config.graph_name)
            
            print("📥 Loading nodes from CSV...")
            # Load nodes
            df_nodes = pd.read_csv(nodes_csv)
            
            for _, row in tqdm(df_nodes.iterrows(), total=len(df_nodes), desc="Loading nodes"):
                try:
                    query = f"MERGE (n:{row['label']} {{id: $id}}) SET n.uri = $uri"
                    params = {'id': row['id'], 'uri': row['uri']}
                    
                    # Add other properties
                    for col in df_nodes.columns:
                        if col not in ['id', 'uri', 'label'] and pd.notna(row[col]):
                            query += f", n.{col} = ${col}"
                            params[col] = row[col]
                    
                    graph.query(query, params)
                except Exception:
                    continue
            
            print("🔗 Loading edges from CSV...")
            # Load edges
            df_edges = pd.read_csv(edges_csv)
            
            for _, row in tqdm(df_edges.iterrows(), total=len(df_edges), desc="Loading edges"):
                try:
                    query = f"""
                    MATCH (a {{id: $source}}), (b {{id: $target}})
                    MERGE (a)-[r:{row['type']}]->(b)
                    """
                    graph.query(query, {
                        'source': row['source'],
                        'target': row['target']
                    })
                except Exception:
                    continue
            
            print("✅ CSV loading complete")
            return True
            
        except Exception as e:
            print(f"❌ CSV loading failed: {e}")
            return False


class Method4_StreamingParser:
    """Method 4: Custom streaming parser with no RDFLib dependency."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        
    def convert(self, ttl_file: str) -> bool:
        """Convert using custom streaming parser."""
        try:
            print("🔄 Using custom streaming parser...")
            
            db = falkordb.FalkorDB(host=self.config.host, port=self.config.port)
            graph = db.select_graph(self.config.graph_name)
            
            batch_nodes = []
            batch_edges = []
            line_count = 0
            error_count = 0
            
            with open(ttl_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in tqdm(f, desc="Streaming parse"):
                    line_count += 1
                    
                    try:
                        # Skip very long lines
                        if len(line) > self.config.max_line_length:
                            continue
                        
                        # Basic TTL line parsing
                        result = self._parse_ttl_line(line.strip())
                        if result:
                            subject, predicate, obj = result
                            
                            # Convert to graph elements
                            nodes, edges = self._convert_triple(subject, predicate, obj)
                            batch_nodes.extend(nodes)
                            batch_edges.extend(edges)
                        
                        # Batch insert
                        if len(batch_nodes) >= self.config.batch_size:
                            self._insert_batch(graph, batch_nodes, batch_edges)
                            batch_nodes.clear()
                            batch_edges.clear()
                        
                        # Error checking
                        if error_count > self.config.max_errors:
                            print(f"❌ Too many errors ({error_count}), stopping")
                            return False
                    
                    except Exception as e:
                        error_count += 1
                        if error_count % 10000 == 0:
                            print(f"⚠️  Errors: {error_count} at line {line_count}")
                        continue
            
            # Final batch
            if batch_nodes or batch_edges:
                self._insert_batch(graph, batch_nodes, batch_edges)
            
            print(f"✅ Streaming parser complete. Processed {line_count:,} lines with {error_count:,} errors")
            return True
            
        except Exception as e:
            print(f"❌ Streaming parser failed: {e}")
            return False
    
    def _parse_ttl_line(self, line: str) -> Optional[Tuple[str, str, str]]:
        """Very basic TTL line parsing."""
        if not line or line.startswith('#') or line.startswith('@'):
            return None
        
        try:
            # Handle simple cases: <subject> <predicate> <object> .
            if line.endswith(' .'):
                line = line[:-2]
            
            # Very basic parsing - this won't handle all TTL syntax
            parts = []
            current = ""
            in_brackets = False
            in_quotes = False
            
            for char in line:
                if char == '<' and not in_quotes:
                    in_brackets = True
                    current += char
                elif char == '>' and in_brackets and not in_quotes:
                    in_brackets = False
                    current += char
                elif char == '"' and not in_brackets:
                    in_quotes = not in_quotes
                    current += char
                elif char == ' ' and not in_brackets and not in_quotes:
                    if current.strip():
                        parts.append(current.strip())
                        current = ""
                else:
                    current += char
            
            if current.strip():
                parts.append(current.strip())
            
            if len(parts) >= 3:
                return parts[0], parts[1], ' '.join(parts[2:])
            
            return None
            
        except Exception:
            return None
    
    def _convert_triple(self, subject: str, predicate: str, obj: str) -> Tuple[List[dict], List[dict]]:
        """Convert triple to nodes and edges."""
        nodes = []
        edges = []
        
        try:
            # Clean URIs
            subject = subject.strip('<>')
            predicate = predicate.strip('<>')
            
            # Generate IDs
            subj_id = hashlib.md5(subject.encode()).hexdigest()[:12]
            
            # Subject node
            nodes.append({
                'id': subj_id,
                'uri': subject,
                'label': 'Resource'
            })
            
            # Handle object
            if obj.startswith('"'):
                # Literal - this is very basic literal parsing
                literal_value = obj.split('"')[1] if '"' in obj[1:] else obj.strip('"')
                prop_name = predicate.split('/')[-1].split('#')[-1]
                
                # Add property to node (simplified)
                nodes[-1][prop_name] = literal_value
            else:
                # URI object
                obj = obj.strip('<>')
                obj_id = hashlib.md5(obj.encode()).hexdigest()[:12]
                
                # Object node
                nodes.append({
                    'id': obj_id,
                    'uri': obj,
                    'label': 'Resource'
                })
                
                # Edge
                edge_type = predicate.split('/')[-1].split('#')[-1]
                edges.append({
                    'source': subj_id,
                    'target': obj_id,
                    'type': edge_type
                })
        
        except Exception:
            pass  # Skip problematic triples
        
        return nodes, edges
    
    def _insert_batch(self, graph, nodes: list, edges: list):
        """Insert batch of nodes and edges."""
        try:
            # Insert nodes
            for node in nodes:
                try:
                    query = f"MERGE (n:{node['label']} {{id: $id}}) SET n.uri = $uri"
                    params = {'id': node['id'], 'uri': node['uri']}
                    
                    # Add properties
                    for key, value in node.items():
                        if key not in ['id', 'uri', 'label']:
                            query += f", n.{key} = ${key}"
                            params[key] = value
                    
                    graph.query(query, params)
                except Exception:
                    continue
            
            # Insert edges
            for edge in edges:
                try:
                    query = f"""
                    MATCH (a {{id: $source}}), (b {{id: $target}})
                    MERGE (a)-[r:{edge['type']}]->(b)
                    """
                    graph.query(query, {
                        'source': edge['source'],
                        'target': edge['target']
                    })
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"Batch insert error: {e}")


class MultiMethodConverter:
    """Main converter that tries multiple methods."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        
    def convert(self, ttl_file: str) -> bool:
        """Try multiple conversion methods in order of reliability."""
        
        print(f"🎯 Converting {ttl_file} using multi-method approach")
        print(f"File size: {Path(ttl_file).stat().st_size / (1024**3):.2f} GB")
        
        methods = self._get_methods_to_try()
        
        for method_name, method_obj in methods:
            print(f"\n{'='*60}")
            print(f"🔄 Trying Method: {method_name}")
            print(f"{'='*60}")
            
            try:
                if method_obj.convert(ttl_file):
                    print(f"✅ SUCCESS: {method_name} completed successfully!")
                    return True
                else:
                    print(f"❌ FAILED: {method_name} did not complete successfully")
            except Exception as e:
                print(f"❌ ERROR: {method_name} failed with exception: {e}")
            
            print(f"⏭️  Moving to next method...")
        
        print(f"\n❌ All methods failed. Consider:")
        print("1. Converting TTL to N-Triples format first")
        print("2. Splitting the file manually into smaller chunks")
        print("3. Using a different RDF processing tool")
        
        return False
    
    def _get_methods_to_try(self) -> List[Tuple[str, Any]]:
        """Get list of methods to try based on config."""
        
        if self.config.method == "jena":
            return [("Apache Jena RIOT", Method1_JenaRiotConverter(self.config))]
        elif self.config.method == "split":
            return [("File Splitting", Method2_FileSplitter(self.config))]
        elif self.config.method == "csv":
            return [("CSV Pipeline", Method3_CSVPipeline(self.config))]
        elif self.config.method == "streaming":
            return [("Streaming Parser", Method4_StreamingParser(self.config))]
        else:
            # Auto mode - try all methods in order of reliability
            return [
                ("Apache Jena RIOT", Method1_JenaRiotConverter(self.config)),
                ("CSV Pipeline", Method3_CSVPipeline(self.config)),
                ("File Splitting", Method2_FileSplitter(self.config)),
                ("Streaming Parser", Method4_StreamingParser(self.config)),
            ]


def main():
    """Main function with method selection."""
    
    import argparse
    
    parser = argparse.ArgumentParser(description="Multiple approaches for large RDF conversion")
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--method', choices=['auto', 'jena', 'split', 'csv', 'streaming'], 
                       default='auto', help='Conversion method')
    parser.add_argument('--host', default='localhost', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--graph-name', default='knowledge_graph', help='Graph name')
    parser.add_argument('--temp-dir', help='Temporary directory')
    parser.add_argument('--keep-temp', action='store_true', help='Keep temporary files')
    
    args = parser.parse_args()
    
    # Configuration
    config = ConversionConfig(
        method=args.method,
        host=args.host,
        port=args.port,
        graph_name=args.graph_name,
        temp_dir=args.temp_dir,
        keep_temp_files=args.keep_temp,
        chunk_size=500000,      # Smaller chunks for 17GB files
        max_memory_mb=6000,     # Higher memory limit
        batch_size=500,         # Smaller batches for stability
        max_errors=500000,      # Allow more errors for large files
        max_line_length=100000  # Skip extremely long lines
    )
    
    # Convert
    converter = MultiMethodConverter(config)
    success = converter.convert(args.ttl_file)
    
    if success:
        print(f"\n🎉 Conversion completed successfully!")
        print(f"Graph '{config.graph_name}' created in FalkorDB")
    else:
        print(f"\n❌ Conversion failed with all methods")
        
    return success


if __name__ == "__main__":
    main()
