#!/usr/bin/env python3
"""
RDF Structure Preserving TTL to FalkorDB Converter
Maps RDF triples exactly to property graph model:
- Subjects/Objects (URIs/BNodes) -> Nodes
- Predicates -> Edge types (when linking URIs) OR Properties (when object is literal)
- Preserves exact RDF structure in property graph format
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import sys
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
from pathlib import Path
from collections import defaultdict

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RDFToPropertyGraphConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the RDF to Property Graph converter"""
        self.output_dir = Path(output_dir).resolve()
        
        # Core RDF -> Property Graph mapping
        self.nodes = {}  # node_id -> {uri, properties}
        self.edges = []  # {source_id, target_id, predicate_uri, predicate_label}
        self.uri_to_id = {}  # URI/BNode -> unique_id
        
        # Statistics
        self.stats = {
            'total_triples': 0,
            'literal_triples': 0,  # Subject-Predicate-Literal (become properties)
            'resource_triples': 0,  # Subject-Predicate-Object (become edges)
            'unique_subjects': 0,
            'unique_objects': 0,
            'unique_predicates': 0
        }
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_uri_for_label(self, uri_str: str) -> str:
        """Extract clean label from URI for display purposes"""
        try:
            parsed = urlparse(uri_str)
            
            # Try fragment first (after #)
            if parsed.fragment:
                name = parsed.fragment
            # Then try last path component
            elif parsed.path and parsed.path != '/':
                path_parts = [p for p in parsed.path.strip('/').split('/') if p]
                if path_parts:
                    name = path_parts[-1]
                else:
                    name = parsed.netloc or 'unknown'
            # Fall back to netloc
            elif parsed.netloc:
                name = parsed.netloc.replace('.', '_')
            else:
                # Use hash as fallback
                name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
            
            # Clean for use as identifier
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"uri_{cleaned}"
            if not cleaned:
                cleaned = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
            return cleaned[:50]  # Limit length
            
        except Exception as e:
            logger.warning(f"Error cleaning URI {uri_str}: {e}")
            return f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique node ID for a URI or blank node"""
        uri_str = str(resource)
        
        if uri_str not in self.uri_to_id:
            # Create unique ID based on URI hash
            hash_obj = hashlib.md5(uri_str.encode('utf-8'))
            base_id = f"n_{hash_obj.hexdigest()[:12]}"
            
            # Ensure uniqueness
            node_id = base_id
            counter = 1
            while node_id in self.uri_to_id.values():
                node_id = f"{base_id}_{counter}"
                counter += 1
            
            self.uri_to_id[uri_str] = node_id
        
        return self.uri_to_id[uri_str]
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        if value is None:
            return ''
        
        if isinstance(value, bool):
            return 'true' if value else 'false'
        
        if isinstance(value, (list, dict)):
            return json.dumps(value, ensure_ascii=False)
        
        str_value = str(value).strip()
        
        # Handle newlines and normalize whitespace
        str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
        str_value = re.sub(r'\s+', ' ', str_value)
        
        # Escape quotes for CSV
        if '"' in str_value:
            str_value = str_value.replace('"', '""')
        
        # Limit length to prevent issues
        if len(str_value) > 1000:
            logger.warning(f"Very long value truncated (length: {len(str_value)})")
            str_value = str_value[:1000] + "..."
        
        return str_value
    
    def process_literal_value(self, literal: Literal) -> tuple:
        """Process literal and return (value, datatype, language)"""
        try:
            value = str(literal)
            datatype = str(literal.datatype) if literal.datatype else None
            language = literal.language if literal.language else None
            
            # Try to convert typed literals to appropriate Python types
            if datatype:
                if 'integer' in datatype.lower() or 'int' in datatype.lower():
                    try:
                        int_val = int(literal)
                        if abs(int_val) <= 2147483647:  # SQL INT range
                            return int_val, datatype, language
                    except ValueError:
                        pass
                        
                elif 'decimal' in datatype.lower() or 'double' in datatype.lower() or 'float' in datatype.lower():
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:  # Reasonable range
                            return float_val, datatype, language
                    except ValueError:
                        pass
                        
                elif 'boolean' in datatype.lower():
                    return str(literal).lower() in ('true', '1'), datatype, language
            
            return value, datatype, language
            
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal), None, None
    
    def convert_ttl_to_property_graph(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL to property graph model preserving RDF structure"""
        logger.info(f"Converting {ttl_file_path} to property graph format...")
        logger.info("🔄 Preserving exact RDF structure in property graph model")
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Parse RDF graph
        rdf_graph = Graph()
        try:
            logger.info("📖 Parsing RDF graph...")
            rdf_graph.parse(ttl_file_path, format='turtle')
            total_triples = len(rdf_graph)
            self.stats['total_triples'] = total_triples
            logger.info(f"✅ Parsed {total_triples:,} RDF triples")
        except Exception as e:
            logger.error(f"❌ Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("⚠️ No triples found in TTL file!")
            return []
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"🎯 Processing only first {max_triples:,} of {total_triples:,} triples")
            total_triples = max_triples
            self.stats['total_triples'] = max_triples
        
        # Collect all unique subjects, predicates, objects for statistics
        unique_subjects = set()
        unique_predicates = set()
        unique_objects = set()
        
        logger.info("🔍 Processing RDF triples into property graph model...")
        processed_count = 0
        
        with tqdm(total=total_triples, desc="Converting RDF triples") as pbar:
            for subject, predicate, obj in rdf_graph:
                if max_triples and processed_count >= max_triples:
                    break
                
                try:
                    pbar.update(1)
                    processed_count += 1
                    
                    # Statistics
                    unique_subjects.add(str(subject))
                    unique_predicates.add(str(predicate))
                    unique_objects.add(str(obj))
                    
                    # Get subject node ID (always create node for subject)
                    subject_id = self.get_or_create_node_id(subject)
                    
                    # Initialize subject node if not exists
                    if subject_id not in self.nodes:
                        subject_uri = str(subject)
                        self.nodes[subject_id] = {
                            'id': subject_id,
                            'uri': subject_uri,
                            'type': 'BlankNode' if isinstance(subject, BNode) else 'URI',
                            'label': self.clean_uri_for_label(subject_uri),
                            'properties': {}
                        }
                    
                    # Process based on object type
                    if isinstance(obj, Literal):
                        # Object is literal -> Add as property to subject node
                        self.stats['literal_triples'] += 1
                        
                        predicate_uri = str(predicate)
                        predicate_label = self.clean_uri_for_label(predicate_uri)
                        
                        value, datatype, language = self.process_literal_value(obj)
                        
                        # Store the property
                        prop_key = predicate_label
                        
                        # Handle multiple values for same predicate
                        if prop_key in self.nodes[subject_id]['properties']:
                            # Convert to list if not already
                            existing = self.nodes[subject_id]['properties'][prop_key]
                            if not isinstance(existing, list):
                                existing = [existing]
                            existing.append(value)
                            self.nodes[subject_id]['properties'][prop_key] = existing
                        else:
                            self.nodes[subject_id]['properties'][prop_key] = value
                        
                        # Store metadata about the property
                        if datatype:
                            self.nodes[subject_id]['properties'][f"{prop_key}_datatype"] = datatype
                        if language:
                            self.nodes[subject_id]['properties'][f"{prop_key}_language"] = language
                    
                    else:
                        # Object is URI/BNode -> Create edge between subject and object
                        self.stats['resource_triples'] += 1
                        
                        # Get object node ID
                        object_id = self.get_or_create_node_id(obj)
                        
                        # Initialize object node if not exists
                        if object_id not in self.nodes:
                            object_uri = str(obj)
                            self.nodes[object_id] = {
                                'id': object_id,
                                'uri': object_uri,
                                'type': 'BlankNode' if isinstance(obj, BNode) else 'URI',
                                'label': self.clean_uri_for_label(object_uri),
                                'properties': {}
                            }
                        
                        # Create edge with predicate as relationship type
                        predicate_uri = str(predicate)
                        predicate_label = self.clean_uri_for_label(predicate_uri)
                        
                        edge = {
                            'source_id': subject_id,
                            'target_id': object_id,
                            'predicate_uri': predicate_uri,
                            'predicate_label': predicate_label,
                            'edge_type': predicate_label  # For compatibility
                        }
                        
                        self.edges.append(edge)
                
                except Exception as e:
                    logger.warning(f"⚠️ Error processing triple {processed_count}: {e}")
                    continue
        
        # Update statistics
        self.stats['unique_subjects'] = len(unique_subjects)
        self.stats['unique_predicates'] = len(unique_predicates)
        self.stats['unique_objects'] = len(unique_objects)
        
        logger.info("✅ RDF to Property Graph conversion complete!")
        logger.info(f"📊 Statistics:")
        logger.info(f"   Total triples processed: {processed_count:,}")
        logger.info(f"   Literal triples (properties): {self.stats['literal_triples']:,}")
        logger.info(f"   Resource triples (edges): {self.stats['resource_triples']:,}")
        logger.info(f"   Unique subjects: {self.stats['unique_subjects']:,}")
        logger.info(f"   Unique predicates: {self.stats['unique_predicates']:,}")
        logger.info(f"   Unique objects: {self.stats['unique_objects']:,}")
        logger.info(f"   Property graph nodes: {len(self.nodes):,}")
        logger.info(f"   Property graph edges: {len(self.edges):,}")
        
        # Write CSV files
        csv_files = self.write_csv_files()
        return csv_files
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        logger.info("💾 Writing property graph to CSV files...")
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names from all nodes
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        all_properties = sorted(all_properties)
        
        # Write nodes
        with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
            # Headers: core node fields + all discovered properties
            headers = ['id', 'uri', 'type', 'label'] + all_properties
            writer = csv.writer(csvfile, **self.csv_params)
            writer.writerow(headers)
            
            for node in self.nodes.values():
                row = [
                    node['id'],
                    node['uri'],
                    node['type'],
                    node['label']
                ]
                
                # Add property values
                for prop in all_properties:
                    value = node['properties'].get(prop, '')
                    row.append(self.sanitize_csv_value(value))
                
                writer.writerow(row)
        
        logger.info(f"✅ Written {len(self.nodes):,} nodes to {nodes_file.name}")
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
            headers = ['source_id', 'target_id', 'predicate_uri', 'predicate_label', 'edge_type']
            writer = csv.writer(csvfile, **self.csv_params)
            writer.writerow(headers)
            
            for edge in self.edges:
                row = [
                    edge['source_id'],
                    edge['target_id'],
                    edge['predicate_uri'],
                    edge['predicate_label'],
                    edge['edge_type']
                ]
                writer.writerow(row)
        
        logger.info(f"✅ Written {len(self.edges):,} edges to {edges_file.name}")
        
        # Write conversion statistics
        stats_file = self.output_dir / "conversion_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2)
        logger.info(f"📊 Written conversion statistics to {stats_file.name}")
        
        return csv_files
    
    def load_to_falkordb(self, graph_name: str, host: str = '127.0.0.1', 
                        port: int = 6379, password: Optional[str] = None):
        """Load the property graph to FalkorDB preserving RDF structure"""
        logger.info(f"🚀 Loading property graph to FalkorDB (graph: {graph_name})...")
        
        try:
            import redis
        except ImportError:
            logger.error("❌ redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Connect to FalkorDB
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            r.ping()
            logger.info(f"✅ Connected to FalkorDB at {host}:{port}")
            
        except Exception as e:
            logger.error(f"❌ Failed to connect to FalkorDB: {e}")
            return False
        
        # Check CSV files exist
        nodes_file = self.output_dir / "nodes.csv"
        edges_file = self.output_dir / "edges.csv"
        
        if not nodes_file.exists() or not edges_file.exists():
            logger.error("❌ CSV files not found. Run conversion first.")
            return False
        
        # Configure import folder
        try:
            logger.info(f"🔧 Configuring import folder: {self.output_dir}")
            r.execute_command("GRAPH.CONFIG", "SET", "IMPORT_FOLDER", str(self.output_dir))
            logger.info("✅ Import folder configured")
        except Exception as e:
            logger.warning(f"⚠️ Could not set import folder: {e}")
        
        try:
            # Load nodes preserving RDF structure
            logger.info("📥 Loading nodes (RDF subjects/objects as graph nodes)...")
            nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row
            CREATE (n:Resource)
            SET n = row
            """
            
            result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
            logger.info(f"✅ Nodes loaded: {result}")
            
            # Create index for faster edge loading
            logger.info("🔍 Creating index on Resource.id...")
            index_query = "CREATE INDEX FOR (n:Resource) ON (n.id)"
            try:
                r.execute_command("GRAPH.QUERY", graph_name, index_query)
                logger.info("✅ Index created")
            except Exception as e:
                logger.warning(f"⚠️ Index creation failed: {e}")
            
            # Load edges preserving predicate relationships
            logger.info("🔗 Loading edges (RDF predicates as graph relationships)...")
            edges_query = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            CALL {
                WITH row
                MATCH (source:Resource {id: row.source_id}), (target:Resource {id: row.target_id})
                CALL apoc.create.relationship(source, row.predicate_label, {predicate_uri: row.predicate_uri}, target) YIELD rel
                RETURN rel
            } IN TRANSACTIONS OF 1000 ROWS
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_query)
                logger.info(f"✅ Edges loaded with dynamic relationship types: {result}")
            except Exception as e:
                logger.warning(f"⚠️ Dynamic relationship loading failed: {e}")
                logger.info("🔄 Falling back to generic relationship type...")
                
                # Fallback: use generic relationship type
                edges_fallback_query = """
                LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
                MATCH (source:Resource {id: row.source_id})
                MATCH (target:Resource {id: row.target_id})
                CREATE (source)-[r:PREDICATE]->(target)
                SET r = {predicate_uri: row.predicate_uri, predicate_label: row.predicate_label}
                """
                
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_fallback_query)
                logger.info(f"✅ Edges loaded with generic PREDICATE type: {result}")
            
            # Verify loaded data
            try:
                # Count nodes
                count_query = "MATCH (n:Resource) RETURN count(n) AS node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"📊 Total nodes in graph: {result}")
                
                # Count edges
                edge_count_query = "MATCH ()-[r]->() RETURN count(r) AS edge_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, edge_count_query)
                logger.info(f"📊 Total edges in graph: {result}")
                
                # Show relationship types
                rel_types_query = "MATCH ()-[r]->() RETURN type(r) AS rel_type, count(*) AS count ORDER BY count DESC LIMIT 10"
                result = r.execute_command("GRAPH.QUERY", graph_name, rel_types_query)
                logger.info(f"📊 Top relationship types: {result}")
                
            except Exception as e:
                logger.warning(f"⚠️ Could not verify data: {e}")
            
            logger.info("🎉 SUCCESS! RDF structure preserved in property graph!")
            logger.info(f"   🔗 Graph name: {graph_name}")
            logger.info(f"   📁 CSV files: {self.output_dir}")
            logger.info(f"   🌐 FalkorDB: {host}:{port}")
            
            return True
            
        except Exception as e:
            logger.error(f"❌ Failed to load data: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Convert TTL to Property Graph preserving exact RDF structure'
    )
    parser.add_argument('ttl_file', nargs='?', help='Path to TTL file')
    parser.add_argument('--convert', action='store_true', help='Convert TTL to property graph CSV')
    parser.add_argument('--load', action='store_true', help='Load CSV to FalkorDB')
    parser.add_argument('--both', action='store_true', help='Convert and load')
    parser.add_argument('--graph_name', default='rdf_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--password', help='FalkorDB password')
    parser.add_argument('--output_dir', default='csv_output', help='CSV output directory')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process')
    
    args = parser.parse_args()
    
    # Default action
    if not any([args.convert, args.load, args.both]):
        if args.ttl_file:
            args.both = True
        else:
            parser.print_help()
            print("\n🔄 RDF Structure Preserving Converter")
            print("Maps RDF triples exactly to property graph:")
            print("  • Subjects/Objects (URIs/BNodes) → Graph Nodes")
            print("  • Predicates → Edge Types (URI links) OR Properties (literals)")
            print("  • Preserves exact RDF semantics")
            print("\nExamples:")
            print("  python script.py data.ttl --convert    # Convert TTL to property graph CSV")
            print("  python script.py --load               # Load CSV to FalkorDB")
            print("  python script.py data.ttl             # Convert and load (default)")
            return
    
    # Create converter
    converter = RDFToPropertyGraphConverter(args.output_dir)
    start_time = time.time()
    
    # Convert if requested
    if args.convert or args.both:
        if not args.ttl_file:
            logger.error("❌ TTL file required for conversion")
            sys.exit(1)
        
        if not os.path.exists(args.ttl_file):
            logger.error(f"❌ TTL file not found: {args.ttl_file}")
            sys.exit(1)
        
        try:
            file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
            logger.info(f"🎯 Converting TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
            
            csv_files = converter.convert_ttl_to_property_graph(args.ttl_file, args.max_triples)
            
            conversion_time = time.time() - start_time
            logger.info(f"✅ Conversion completed in {conversion_time:.1f}s")
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            sys.exit(1)
    
    # Load if requested
    if args.load or args.both:
        try:
            success = converter.load_to_falkordb(
                args.graph_name, args.host, args.port, args.password
            )
            
            if success:
                total_time = time.time() - start_time
                logger.info(f"🎉 COMPLETE! Total time: {total_time:.1f}s")
                logger.info(f"🔗 RDF structure preserved in property graph!")
            else:
                logger.error("❌ Failed to load to FalkorDB")
                sys.exit(1)
                
        except Exception as e:
            logger.error(f"❌ Loading failed: {e}")
            sys.exit(1)

if __name__ == "__main__":
    main()
