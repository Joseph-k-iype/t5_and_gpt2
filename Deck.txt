#!/usr/bin/env python3
"""
Contextual Case Mapping System using LangChain, LangGraph, and OpenAI o3-mini
Uses ReAct agent pattern for intelligent matching with reasoning and confidence scores.
"""

import json
import csv
import os
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from pathlib import Path

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from langchain.tools import BaseTool
from langchain.pydantic_v1 import BaseModel, Field

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.sqlite import SqliteSaver

# Additional imports
import sqlite3
from functools import lru_cache
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class MappingResult:
    """Result structure for case mapping"""
    case_id: str
    case_text: str
    matched_name: str
    confidence_score: float
    reasoning: str
    definition_used: str

class CaseMatchingTool(BaseTool):
    """Tool for matching cases to names using contextual analysis"""
    
    name = "case_matching_tool"
    description = "Analyzes a case text and matches it to the most appropriate name based on definitions"
    
    def __init__(self, names_definitions: Dict[str, str]):
        super().__init__()
        self.names_definitions = names_definitions
    
    class CaseMatchingInput(BaseModel):
        case_text: str = Field(description="The case text to analyze and match")
        
    args_schema = CaseMatchingInput
    
    def _run(self, case_text: str) -> str:
        """Execute the case matching logic"""
        results = []
        
        for name, definition in self.names_definitions.items():
            # Create a prompt for semantic similarity analysis
            analysis_prompt = f"""
            Analyze the semantic similarity between:
            
            Case: "{case_text}"
            
            Name: "{name}"
            Definition: "{definition}"
            
            Provide a relevance score (0-100) and brief reasoning for how well the case matches this name/definition.
            Format: Score: [number] | Reasoning: [explanation]
            """
            
            results.append({
                'name': name,
                'definition': definition,
                'analysis_prompt': analysis_prompt
            })
        
        return json.dumps(results, indent=2)

class ConfidenceAnalysisTool(BaseTool):
    """Tool for analyzing confidence and providing final reasoning"""
    
    name = "confidence_analysis_tool"
    description = "Analyzes matching results and provides confidence scores with detailed reasoning"
    
    class ConfidenceInput(BaseModel):
        matching_results: str = Field(description="JSON string of matching analysis results")
        case_text: str = Field(description="Original case text")
        
    args_schema = ConfidenceInput
    
    def _run(self, matching_results: str, case_text: str) -> str:
        """Analyze confidence and provide reasoning"""
        try:
            results = json.loads(matching_results)
            
            confidence_prompt = f"""
            Based on the matching analysis for case: "{case_text}"
            
            Analysis Results: {json.dumps(results, indent=2)}
            
            Determine:
            1. The best matching name
            2. Confidence score (0-100)
            3. Detailed reasoning for the match
            4. Why other options were less suitable
            
            Return in JSON format:
            {{
                "best_match": "name",
                "confidence": score,
                "reasoning": "detailed explanation",
                "alternatives_considered": ["other names considered"]
            }}
            """
            
            return confidence_prompt
            
        except json.JSONDecodeError:
            return f"Error: Invalid JSON in matching results: {matching_results}"

class ContextualMapper:
    """Main class for contextual case mapping using LangGraph ReAct agent"""
    
    def __init__(self, openai_api_key: str = None):
        """Initialize the mapper with OpenAI API key"""
        self.api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass it directly.")
        
        # Initialize o3-mini model
        self.llm = ChatOpenAI(
            model="o3-mini",
            api_key=self.api_key,
            temperature=0.1,  # Low temperature for consistent reasoning
            max_tokens=2048
        )
        
        self.names_definitions = {}
        self.cases = []
        self.tools = []
        self.graph = None
        
    def load_json_definitions(self, json_file_path: str) -> None:
        """Load name-definition mappings from JSON file"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
                
            # Handle different JSON structures
            if isinstance(data, list):
                # If it's a list of objects with name and definition
                for item in data:
                    if 'name' in item and 'definition' in item:
                        self.names_definitions[item['name']] = item['definition']
            elif isinstance(data, dict):
                # If it's a direct name->definition mapping
                if all(isinstance(v, str) for v in data.values()):
                    self.names_definitions = data
                else:
                    # If it's a dict with name/definition structure
                    for key, value in data.items():
                        if isinstance(value, dict) and 'definition' in value:
                            self.names_definitions[key] = value['definition']
                        elif isinstance(value, str):
                            self.names_definitions[key] = value
            
            logger.info(f"Loaded {len(self.names_definitions)} name-definition pairs")
            
        except FileNotFoundError:
            raise FileNotFoundError(f"JSON file not found: {json_file_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON format in {json_file_path}: {e}")
    
    def load_csv_cases(self, csv_file_path: str) -> None:
        """Load cases from CSV file"""
        try:
            with open(csv_file_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                self.cases = []
                
                for row in reader:
                    if 'id' in row and 'case' in row:
                        self.cases.append({
                            'id': row['id'].strip(),
                            'case': row['case'].strip()
                        })
                
            logger.info(f"Loaded {len(self.cases)} cases from CSV")
            
        except FileNotFoundError:
            raise FileNotFoundError(f"CSV file not found: {csv_file_path}")
        except Exception as e:
            raise ValueError(f"Error reading CSV file {csv_file_path}: {e}")
    
    def setup_react_agent(self) -> None:
        """Setup the ReAct agent using LangGraph"""
        if not self.names_definitions:
            raise ValueError("Names and definitions must be loaded before setting up the agent")
        
        # Initialize tools
        self.tools = [
            CaseMatchingTool(self.names_definitions),
            ConfidenceAnalysisTool()
        ]
        
        # Create LangGraph state
        class AgentState(BaseModel):
            messages: List[BaseMessage] = Field(default_factory=list)
            case_text: str = ""
            matching_results: str = ""
            final_result: str = ""
        
        # Define the agent workflow
        def call_model(state: AgentState):
            """Call the LLM with current state"""
            messages = state.messages
            response = self.llm.invoke(messages)
            return {"messages": [response]}
        
        def should_continue(state: AgentState):
            """Determine if the agent should continue or end"""
            last_message = state.messages[-1]
            
            # If the last message contains a tool call, continue to tools
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                return "tools"
            
            # If we have a final result, end
            if "final_result" in str(last_message.content).lower():
                return "end"
            
            return "end"
        
        # Create the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", ToolNode(self.tools))
        
        # Add edges
        workflow.set_entry_point("agent")
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                "end": END
            }
        )
        workflow.add_edge("tools", "agent")
        
        # Create checkpointer for memory
        checkpointer = SqliteSaver.from_conn_string(":memory:")
        
        # Compile the graph
        self.graph = workflow.compile(checkpointer=checkpointer)
    
    def match_case_to_name(self, case_id: str, case_text: str) -> MappingResult:
        """Match a single case to the most appropriate name"""
        if not self.graph:
            self.setup_react_agent()
        
        # Create the initial prompt for the ReAct agent
        system_prompt = f"""
        You are an expert at contextual matching. You have access to tools to:
        1. Analyze semantic similarity between cases and name/definition pairs
        2. Calculate confidence scores and provide detailed reasoning
        
        Your task is to match this case to the most appropriate name:
        Case: "{case_text}"
        
        Available names and definitions:
        {json.dumps(self.names_definitions, indent=2)}
        
        Use the tools systematically:
        1. First use case_matching_tool to analyze the case against all names
        2. Then use confidence_analysis_tool to determine the best match with reasoning
        
        Provide your final answer in this exact JSON format:
        {{
            "matched_name": "best_matching_name",
            "confidence_score": confidence_as_float_0_to_1,
            "reasoning": "detailed_explanation_of_why_this_match_was_chosen",
            "definition_used": "the_definition_that_led_to_this_match"
        }}
        """
        
        initial_message = HumanMessage(content=system_prompt)
        
        # Run the agent
        config = {"configurable": {"thread_id": f"case_{case_id}"}}
        
        try:
            final_state = self.graph.invoke(
                {"messages": [initial_message], "case_text": case_text},
                config
            )
            
            # Extract the final result from the last message
            last_message = final_state["messages"][-1].content
            
            # Try to parse JSON result from the response
            result_data = self._extract_json_result(last_message)
            
            return MappingResult(
                case_id=case_id,
                case_text=case_text,
                matched_name=result_data.get('matched_name', 'Unknown'),
                confidence_score=float(result_data.get('confidence_score', 0.0)),
                reasoning=result_data.get('reasoning', 'No reasoning provided'),
                definition_used=result_data.get('definition_used', 'No definition found')
            )
            
        except Exception as e:
            logger.error(f"Error processing case {case_id}: {e}")
            return MappingResult(
                case_id=case_id,
                case_text=case_text,
                matched_name='Error',
                confidence_score=0.0,
                reasoning=f'Error during processing: {str(e)}',
                definition_used='N/A'
            )
    
    def _extract_json_result(self, text: str) -> Dict[str, Any]:
        """Extract JSON result from agent response"""
        try:
            # Look for JSON in the text
            start_idx = text.find('{')
            end_idx = text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = text[start_idx:end_idx]
                return json.loads(json_str)
            
        except json.JSONDecodeError:
            pass
        
        # Fallback: try to extract information using patterns
        return {
            'matched_name': 'Unknown',
            'confidence_score': 0.5,
            'reasoning': 'Could not parse agent response',
            'definition_used': 'N/A'
        }
    
    def process_all_cases(self) -> List[MappingResult]:
        """Process all loaded cases and return mapping results"""
        if not self.cases:
            raise ValueError("No cases loaded. Use load_csv_cases() first.")
        
        if not self.names_definitions:
            raise ValueError("No definitions loaded. Use load_json_definitions() first.")
        
        results = []
        total_cases = len(self.cases)
        
        logger.info(f"Processing {total_cases} cases...")
        
        for i, case in enumerate(self.cases, 1):
            logger.info(f"Processing case {i}/{total_cases}: {case['id']}")
            
            result = self.match_case_to_name(case['id'], case['case'])
            results.append(result)
            
            # Log progress
            if i % 5 == 0 or i == total_cases:
                logger.info(f"Completed {i}/{total_cases} cases")
        
        return results
    
    def save_results(self, results: List[MappingResult], output_file: str) -> None:
        """Save mapping results to a JSON file"""
        output_data = []
        
        for result in results:
            output_data.append({
                'case_id': result.case_id,
                'case_text': result.case_text,
                'matched_name': result.matched_name,
                'confidence_score': result.confidence_score,
                'reasoning': result.reasoning,
                'definition_used': result.definition_used,
                'timestamp': datetime.now().isoformat()
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results saved to {output_file}")
    
    def print_summary(self, results: List[MappingResult]) -> None:
        """Print a summary of the mapping results"""
        if not results:
            print("No results to summarize.")
            return
        
        print("\n" + "="*50)
        print("CONTEXTUAL MAPPING SUMMARY")
        print("="*50)
        
        # Overall statistics
        total_cases = len(results)
        avg_confidence = sum(r.confidence_score for r in results) / total_cases
        high_confidence = sum(1 for r in results if r.confidence_score >= 0.8)
        
        print(f"Total Cases Processed: {total_cases}")
        print(f"Average Confidence: {avg_confidence:.2f}")
        print(f"High Confidence Matches (≥0.8): {high_confidence}")
        print(f"Success Rate: {(high_confidence/total_cases)*100:.1f}%")
        
        # Name distribution
        name_counts = {}
        for result in results:
            name_counts[result.matched_name] = name_counts.get(result.matched_name, 0) + 1
        
        print(f"\nName Distribution:")
        for name, count in sorted(name_counts.items()):
            print(f"  {name}: {count} cases")
        
        # Sample results
        print(f"\nSample Results:")
        for i, result in enumerate(results[:3], 1):
            print(f"\n{i}. Case ID: {result.case_id}")
            print(f"   Case: {result.case_text[:100]}...")
            print(f"   Matched: {result.matched_name}")
            print(f"   Confidence: {result.confidence_score:.2f}")
            print(f"   Reasoning: {result.reasoning[:150]}...")

def main():
    """Main function to run the contextual mapping"""
    # Configuration
    JSON_FILE = "names_definitions.json"  # Update with your JSON file path
    CSV_FILE = "cases.csv"                # Update with your CSV file path
    OUTPUT_FILE = "mapping_results.json"
    
    # Initialize the mapper
    try:
        mapper = ContextualMapper()
        
        # Load data
        print("Loading data files...")
        mapper.load_json_definitions(JSON_FILE)
        mapper.load_csv_cases(CSV_FILE)
        
        # Process cases
        print("Setting up ReAct agent...")
        results = mapper.process_all_cases()
        
        # Save and display results
        mapper.save_results(results, OUTPUT_FILE)
        mapper.print_summary(results)
        
        print(f"\nProcessing complete! Results saved to {OUTPUT_FILE}")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        print(f"Error: {e}")

if __name__ == "__main__":
    main()
