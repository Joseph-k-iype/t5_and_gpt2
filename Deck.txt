"""
Cross-Platform Timeout Solutions for FalkorDB
============================================

This module provides timeout solutions that work on ALL platforms:
- Windows (no signal support)
- Linux/macOS (full signal support)
- Docker containers
- Cloud environments

Solutions included:
1. Threading-based timeouts (works everywhere)
2. Connection-level timeouts (built into Redis/FalkorDB)
3. Asyncio-based timeouts (modern Python)
4. Process-based timeouts (ultimate fallback)
5. Simple time monitoring (basic but reliable)
"""

import os
import sys
import time
import logging
import threading
import subprocess
import json
import tempfile
from typing import Dict, List, Set, Tuple, Any, Optional, Callable
from collections import defaultdict
from queue import Queue, Empty
from contextlib import contextmanager
import platform

# Core dependencies
import falkordb
from rdflib import Graph, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, XSD

# Check Python version for asyncio support
PYTHON_VERSION = sys.version_info
HAS_ASYNCIO = PYTHON_VERSION >= (3, 7)

if HAS_ASYNCIO:
    import asyncio
    try:
        import aioredis
        HAS_AIOREDIS = True
    except ImportError:
        HAS_AIOREDIS = False
else:
    HAS_AIOREDIS = False


class ThreadingTimeoutHandler:
    """
    Threading-based timeout handler that works on ALL platforms.
    Most reliable cross-platform solution.
    """
    
    def __init__(self, default_timeout: int = 60):
        self.default_timeout = default_timeout
        self.logger = logging.getLogger(__name__)
    
    def execute_with_timeout(self, func: Callable, args: tuple = (), 
                           kwargs: dict = None, timeout: int = None) -> Any:
        """
        Execute function with timeout using threading.
        Works on Windows, Linux, macOS, and all Python versions.
        """
        kwargs = kwargs or {}
        timeout = timeout or self.default_timeout
        
        result_queue = Queue(maxsize=1)
        exception_queue = Queue(maxsize=1)
        
        def target():
            try:
                result = func(*args, **kwargs)
                result_queue.put(result)
            except Exception as e:
                exception_queue.put(e)
        
        # Start thread
        thread = threading.Thread(target=target, daemon=True)
        thread.start()
        
        # Wait with timeout
        thread.join(timeout=timeout)
        
        if thread.is_alive():
            # Timeout occurred
            self.logger.warning(f"Query timed out after {timeout} seconds")
            # Note: We can't actually kill the thread, but we can abandon it
            raise TimeoutError(f"Operation timed out after {timeout} seconds")
        
        # Check for exceptions
        try:
            exception = exception_queue.get_nowait()
            raise exception
        except Empty:
            pass
        
        # Get result
        try:
            return result_queue.get_nowait()
        except Empty:
            raise RuntimeError("No result returned from threaded operation")


class ConnectionTimeoutHandler:
    """
    Use Redis/FalkorDB built-in connection timeouts.
    Most efficient and reliable method.
    """
    
    def __init__(self, 
                 host: str = "localhost",
                 port: int = 6379,
                 password: Optional[str] = None,
                 socket_timeout: int = 60,
                 socket_connect_timeout: int = 30,
                 socket_keepalive: bool = True,
                 socket_keepalive_options: dict = None):
        
        self.connection_params = {
            'host': host,
            'port': port,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': socket_connect_timeout,
            'socket_keepalive': socket_keepalive,
            'socket_keepalive_options': socket_keepalive_options or {},
            'retry_on_timeout': True,
            'health_check_interval': 30
        }
        
        if password:
            self.connection_params['password'] = password
        
        self.logger = logging.getLogger(__name__)
        self._connection = None
    
    def get_connection(self) -> falkordb.FalkorDB:
        """Get connection with timeout settings."""
        if self._connection is None:
            self.logger.info("Creating FalkorDB connection with timeout settings...")
            try:
                self._connection = falkordb.FalkorDB(**self.connection_params)
                self.logger.info("‚úÖ Connection established with built-in timeouts")
            except Exception as e:
                self.logger.error(f"‚ùå Failed to create connection: {e}")
                raise
        
        return self._connection
    
    def execute_query(self, graph_name: str, query: str) -> Any:
        """Execute query with connection-level timeout."""
        db = self.get_connection()
        graph = db.select_graph(graph_name)
        
        try:
            return graph.query(query)
        except Exception as e:
            # Reset connection on timeout/error
            self._connection = None
            raise


class ProcessTimeoutHandler:
    """
    Ultimate fallback: Execute queries in separate processes.
    Guaranteed to work but has overhead.
    """
    
    def __init__(self, connection_params: dict):
        self.connection_params = connection_params
        self.logger = logging.getLogger(__name__)
    
    def execute_with_timeout(self, graph_name: str, query: str, timeout: int = 60) -> Any:
        """Execute query in separate process with timeout."""
        
        # Create temporary files for communication
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as query_file:
            query_data = {
                'connection_params': self.connection_params,
                'graph_name': graph_name,
                'query': query
            }
            json.dump(query_data, query_file)
            query_file_path = query_file.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as result_file:
            result_file_path = result_file.name
        
        try:
            # Create subprocess to execute query
            subprocess_code = f'''
import json
import sys
import falkordb

# Load query data
with open("{query_file_path}", "r") as f:
    data = json.load(f)

try:
    # Connect and execute
    db = falkordb.FalkorDB(**data["connection_params"])
    graph = db.select_graph(data["graph_name"])
    result = graph.query(data["query"])
    
    # Save result
    result_data = {{
        "success": True,
        "result": str(result.result_set) if hasattr(result, "result_set") else str(result)
    }}
    
    with open("{result_file_path}", "w") as f:
        json.dump(result_data, f)

except Exception as e:
    # Save error
    result_data = {{
        "success": False,
        "error": str(e)
    }}
    
    with open("{result_file_path}", "w") as f:
        json.dump(result_data, f)
'''
            
            # Execute with timeout
            process = subprocess.Popen([
                sys.executable, '-c', subprocess_code
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            try:
                stdout, stderr = process.communicate(timeout=timeout)
                
                # Read result
                with open(result_file_path, 'r') as f:
                    result_data = json.load(f)
                
                if result_data['success']:
                    return result_data['result']
                else:
                    raise Exception(result_data['error'])
                    
            except subprocess.TimeoutExpired:
                process.kill()
                process.communicate()  # Clean up
                raise TimeoutError(f"Query timed out after {timeout} seconds")
        
        finally:
            # Cleanup temp files
            try:
                os.unlink(query_file_path)
                os.unlink(result_file_path)
            except:
                pass


if HAS_ASYNCIO and HAS_AIOREDIS:
    class AsyncTimeoutHandler:
        """
        Modern asyncio-based timeout handler.
        Most efficient for high-concurrency scenarios.
        """
        
        def __init__(self, connection_params: dict):
            self.connection_params = connection_params
            self.logger = logging.getLogger(__name__)
            self._pool = None
        
        async def get_connection_pool(self):
            """Get async Redis connection pool."""
            if self._pool is None:
                self._pool = aioredis.ConnectionPool.from_url(
                    f"redis://{self.connection_params['host']}:{self.connection_params['port']}",
                    password=self.connection_params.get('password'),
                    socket_timeout=self.connection_params.get('socket_timeout', 60),
                    socket_connect_timeout=self.connection_params.get('socket_connect_timeout', 30)
                )
            return self._pool
        
        async def execute_with_timeout(self, graph_name: str, query: str, timeout: int = 60) -> Any:
            """Execute query with asyncio timeout."""
            pool = await self.get_connection_pool()
            
            async with aioredis.Redis(connection_pool=pool) as redis:
                try:
                    # Execute with asyncio timeout
                    result = await asyncio.wait_for(
                        redis.execute_command("GRAPH.QUERY", graph_name, query),
                        timeout=timeout
                    )
                    return result
                except asyncio.TimeoutError:
                    raise TimeoutError(f"Query timed out after {timeout} seconds")


class SimpleTimeMonitor:
    """
    Simple time-based monitoring without actual timeout enforcement.
    Works everywhere, provides warnings about long-running queries.
    """
    
    def __init__(self, warning_threshold: int = 30):
        self.warning_threshold = warning_threshold
        self.logger = logging.getLogger(__name__)
    
    def execute_with_monitoring(self, func: Callable, args: tuple = (), 
                              kwargs: dict = None, expected_timeout: int = 60) -> Any:
        """Execute function with time monitoring and warnings."""
        kwargs = kwargs or {}
        start_time = time.time()
        
        # Start monitoring thread
        stop_monitoring = threading.Event()
        monitor_thread = threading.Thread(
            target=self._monitor_execution, 
            args=(start_time, expected_timeout, stop_monitoring),
            daemon=True
        )
        monitor_thread.start()
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            stop_monitoring.set()
            execution_time = time.time() - start_time
            
            if execution_time > expected_timeout:
                self.logger.warning(f"Query took {execution_time:.1f}s (exceeded expected {expected_timeout}s)")
            elif execution_time > self.warning_threshold:
                self.logger.info(f"Query took {execution_time:.1f}s")
    
    def _monitor_execution(self, start_time: float, expected_timeout: int, stop_event: threading.Event):
        """Monitor execution in background thread."""
        while not stop_event.is_set():
            elapsed = time.time() - start_time
            
            if elapsed > self.warning_threshold and elapsed % 30 < 1:  # Log every 30 seconds
                self.logger.info(f"Query still running after {elapsed:.0f}s...")
            
            if elapsed > expected_timeout and elapsed % 60 < 1:  # Warn every minute after timeout
                self.logger.warning(f"Query exceeded expected timeout ({elapsed:.0f}s > {expected_timeout}s)")
            
            time.sleep(1)


class CrossPlatformFalkorDBConverter:
    """
    RDF converter with cross-platform timeout handling that works on ALL systems.
    """
    
    def __init__(self,
                 # Connection parameters
                 falkor_host: str = "localhost",
                 falkor_port: int = 6379,
                 falkor_password: Optional[str] = None,
                 
                 # Timeout settings
                 socket_timeout: int = 60,
                 query_timeout: int = 120,
                 connection_timeout: int = 30,
                 
                 # Processing settings
                 chunk_size: int = 2000,
                 node_batch_size: int = 25,
                 rel_batch_size: int = 10,
                 
                 # Timeout method selection
                 timeout_method: str = "auto"):  # auto, threading, connection, process, asyncio, monitor
        
        # Store settings
        self.connection_params = {
            'host': falkor_host,
            'port': falkor_port,
            'password': falkor_password,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': connection_timeout,
            'socket_keepalive': True
        }
        
        self.query_timeout = query_timeout
        self.chunk_size = chunk_size
        self.node_batch_size = node_batch_size
        self.rel_batch_size = rel_batch_size
        
        # Initialize timeout handler based on platform and preference
        self.timeout_method = self._select_timeout_method(timeout_method)
        self.timeout_handler = self._create_timeout_handler()
        
        # Data structures
        self.entity_index = {}
        self.entity_counter = 0
        
        # Statistics
        self.stats = {
            'queries_executed': 0,
            'queries_timed_out': 0,
            'total_query_time': 0,
            'platform': platform.system(),
            'timeout_method': self.timeout_method
        }
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"üîß Initialized cross-platform converter (method: {self.timeout_method})")
    
    def _select_timeout_method(self, preferred: str) -> str:
        """Select best available timeout method for current platform."""
        if preferred != "auto":
            return preferred
        
        # Auto-select based on platform and capabilities
        system = platform.system().lower()
        
        if HAS_ASYNCIO and HAS_AIOREDIS:
            return "asyncio"
        elif system == "windows":
            return "connection"  # Most reliable on Windows
        else:
            return "threading"   # Good balance for Unix systems
    
    def _create_timeout_handler(self):
        """Create appropriate timeout handler."""
        if self.timeout_method == "connection":
            return ConnectionTimeoutHandler(**self.connection_params)
        elif self.timeout_method == "threading":
            return ThreadingTimeoutHandler(self.query_timeout)
        elif self.timeout_method == "process":
            return ProcessTimeoutHandler(self.connection_params)
        elif self.timeout_method == "asyncio" and HAS_ASYNCIO and HAS_AIOREDIS:
            return AsyncTimeoutHandler(self.connection_params)
        elif self.timeout_method == "monitor":
            return SimpleTimeMonitor()
        else:
            # Fallback to connection timeout
            self.logger.warning(f"Unknown timeout method '{self.timeout_method}', using connection timeout")
            return ConnectionTimeoutHandler(**self.connection_params)
    
    def execute_query_with_timeout(self, graph_name: str, query: str, timeout: int = None) -> Any:
        """Execute query with the selected timeout method."""
        timeout = timeout or self.query_timeout
        start_time = time.time()
        
        try:
            if isinstance(self.timeout_handler, ConnectionTimeoutHandler):
                result = self.timeout_handler.execute_query(graph_name, query)
            
            elif isinstance(self.timeout_handler, ThreadingTimeoutHandler):
                # Use threading timeout
                db = falkordb.FalkorDB(**self.connection_params)
                graph = db.select_graph(graph_name)
                
                result = self.timeout_handler.execute_with_timeout(
                    func=graph.query,
                    args=(query,),
                    timeout=timeout
                )
            
            elif isinstance(self.timeout_handler, ProcessTimeoutHandler):
                result = self.timeout_handler.execute_with_timeout(graph_name, query, timeout)
            
            elif hasattr(self.timeout_handler, 'execute_with_timeout'):  # AsyncTimeoutHandler
                if HAS_ASYNCIO:
                    # Run async method in event loop
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        result = loop.run_until_complete(
                            self.timeout_handler.execute_with_timeout(graph_name, query, timeout)
                        )
                    finally:
                        loop.close()
                else:
                    raise RuntimeError("Asyncio not available")
            
            elif isinstance(self.timeout_handler, SimpleTimeMonitor):
                # Use monitoring without actual timeout
                db = falkordb.FalkorDB(**self.connection_params)
                graph = db.select_graph(graph_name)
                
                result = self.timeout_handler.execute_with_monitoring(
                    func=graph.query,
                    args=(query,),
                    expected_timeout=timeout
                )
            
            else:
                raise RuntimeError(f"Unknown timeout handler type: {type(self.timeout_handler)}")
            
            # Update statistics
            execution_time = time.time() - start_time
            self.stats['queries_executed'] += 1
            self.stats['total_query_time'] += execution_time
            
            return result
            
        except TimeoutError as e:
            self.stats['queries_timed_out'] += 1
            self.logger.warning(f"Query timed out: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            raise
    
    def process_file_cross_platform(self, file_path: str, graph_name: str = "cross_platform_graph") -> Dict[str, Any]:
        """Process RDF file with cross-platform timeout handling."""
        start_time = time.time()
        
        self.logger.info(f"üöÄ Processing {file_path} with {self.timeout_method} timeout method")
        
        # Parse RDF file
        self.logger.info("üìñ Parsing RDF file...")
        graph = Graph()
        graph.parse(file_path, format='turtle')
        triples = list(graph)
        
        self.logger.info(f"üìä Processing {len(triples):,} triples in chunks of {self.chunk_size}")
        
        # Clear existing data
        try:
            self.execute_query_with_timeout(graph_name, "MATCH (n) DETACH DELETE n", timeout=120)
            self.logger.info("üóëÔ∏è Cleared existing data")
        except Exception as e:
            self.logger.warning(f"Could not clear existing data: {e}")
        
        # Process in chunks
        total_nodes = 0
        total_relationships = 0
        
        for chunk_idx in range(0, len(triples), self.chunk_size):
            chunk_end = min(chunk_idx + self.chunk_size, len(triples))
            chunk_triples = triples[chunk_idx:chunk_end]
            
            self.logger.info(f"üì¶ Processing chunk {chunk_idx//self.chunk_size + 1}: "
                           f"triples {chunk_idx:,}-{chunk_end:,}")
            
            # Convert chunk to nodes and relationships
            nodes, relationships = self._process_triple_chunk(chunk_triples)
            
            # Upload with timeout handling
            nodes_uploaded = self._upload_nodes_safe(graph_name, nodes)
            rels_uploaded = self._upload_relationships_safe(graph_name, relationships)
            
            total_nodes += nodes_uploaded
            total_relationships += rels_uploaded
            
            progress = chunk_end / len(triples) * 100
            self.logger.info(f"‚úÖ Chunk completed: {nodes_uploaded} nodes, {rels_uploaded} relationships "
                           f"(progress: {progress:.1f}%)")
        
        # Final statistics
        total_time = time.time() - start_time
        
        final_stats = {
            'total_time': total_time,
            'total_triples': len(triples),
            'total_nodes': total_nodes,
            'total_relationships': total_relationships,
            'throughput': len(triples) / total_time if total_time > 0 else 0,
            'timeout_method': self.timeout_method,
            'platform': self.stats['platform'],
            'query_stats': {
                'queries_executed': self.stats['queries_executed'],
                'queries_timed_out': self.stats['queries_timed_out'],
                'avg_query_time': self.stats['total_query_time'] / self.stats['queries_executed'] 
                                 if self.stats['queries_executed'] > 0 else 0
            }
        }
        
        self.logger.info("üéâ Cross-platform processing completed!")
        self.logger.info(f"‚è±Ô∏è Total time: {total_time:.1f}s")
        self.logger.info(f"üìä Processed: {len(triples):,} triples")
        self.logger.info(f"üèóÔ∏è Created: {total_nodes:,} nodes, {total_relationships:,} relationships")
        self.logger.info(f"üöÄ Throughput: {final_stats['throughput']:.1f} triples/second")
        self.logger.info(f"‚ö†Ô∏è Query timeouts: {self.stats['queries_timed_out']}")
        
        return final_stats
    
    def _process_triple_chunk(self, triples: List[Tuple]) -> Tuple[List[Dict], List[Tuple]]:
        """Process triples into nodes and relationships."""
        nodes = {}
        relationships = []
        
        for subject, predicate, obj in triples:
            subject_str = str(subject)
            subject_id = self._get_entity_id(subject_str)
            
            # Create subject node
            if subject_id not in nodes:
                nodes[subject_id] = {
                    'id': subject_id,
                    'uri': subject_str,
                    'labels': ['Resource']
                }
            
            # Handle object
            if isinstance(obj, (URIRef, BNode)):
                obj_str = str(obj)
                obj_id = self._get_entity_id(obj_str)
                
                # Create object node
                if obj_id not in nodes:
                    nodes[obj_id] = {
                        'id': obj_id,
                        'uri': obj_str,
                        'labels': ['Resource']
                    }
                
                # Create relationship
                rel_type = str(predicate).split('/')[-1].split('#')[-1]
                relationships.append((subject_id, obj_id, rel_type, str(predicate)))
            
            elif isinstance(obj, Literal):
                # Add as property
                prop_name = str(predicate).split('/')[-1].split('#')[-1]
                nodes[subject_id][prop_name] = str(obj)
        
        return list(nodes.values()), relationships
    
    def _get_entity_id(self, entity_str: str) -> int:
        """Get or create entity ID."""
        if entity_str not in self.entity_index:
            self.entity_index[entity_str] = self.entity_counter
            self.entity_counter += 1
        return self.entity_index[entity_str]
    
    def _upload_nodes_safe(self, graph_name: str, nodes: List[Dict]) -> int:
        """Upload nodes with safe batching."""
        uploaded = 0
        
        for i in range(0, len(nodes), self.node_batch_size):
            batch_end = min(i + self.node_batch_size, len(nodes))
            batch_nodes = nodes[i:batch_end]
            
            try:
                # Build safe queries
                queries = []
                for node in batch_nodes:
                    labels_str = ':'.join(node.get('labels', ['Resource']))
                    props = [f"entity_id: {node['id']}"]
                    
                    for key, value in node.items():
                        if key not in ['id', 'labels']:
                            if isinstance(value, str):
                                safe_value = value.replace("'", "\\'")[:200]
                                props.append(f"{key}: '{safe_value}'")
                    
                    props_str = '{' + ', '.join(props) + '}'
                    queries.append(f"CREATE (:{labels_str} {props_str})")
                
                # Execute with timeout
                if queries:
                    combined_query = ' '.join(queries)
                    self.execute_query_with_timeout(graph_name, combined_query, 
                                                  timeout=max(30, len(queries) * 2))
                    uploaded += len(queries)
                    
            except Exception as e:
                self.logger.warning(f"Node batch failed: {e}")
                # Try individual uploads
                for node in batch_nodes:
                    try:
                        labels_str = ':'.join(node.get('labels', ['Resource']))
                        query = f"CREATE (:{labels_str} {{entity_id: {node['id']}}})"
                        self.execute_query_with_timeout(graph_name, query, timeout=30)
                        uploaded += 1
                    except:
                        continue
        
        return uploaded
    
    def _upload_relationships_safe(self, graph_name: str, relationships: List[Tuple]) -> int:
        """Upload relationships one by one (safest approach)."""
        uploaded = 0
        
        for source_id, target_id, rel_type, predicate_uri in relationships:
            try:
                # Sanitize relationship type
                safe_rel_type = ''.join(c if c.isalnum() or c == '_' else '_' for c in rel_type)
                if not safe_rel_type:
                    safe_rel_type = "RELATED"
                
                query = f"""
                MATCH (a) WHERE a.entity_id = {source_id}
                MATCH (b) WHERE b.entity_id = {target_id}
                CREATE (a)-[:{safe_rel_type}]->(b)
                """
                
                self.execute_query_with_timeout(graph_name, query, timeout=30)
                uploaded += 1
                
            except Exception as e:
                self.logger.debug(f"Relationship failed: {e}")
                continue
        
        return uploaded


def main():
    """Command-line interface for cross-platform converter."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Cross-Platform RDF to FalkorDB Converter")
    parser.add_argument("rdf_file", help="Path to RDF file")
    parser.add_argument("--graph-name", default="cross_platform_graph", help="FalkorDB graph name")
    parser.add_argument("--timeout-method", default="auto", 
                       choices=["auto", "connection", "threading", "process", "asyncio", "monitor"],
                       help="Timeout method to use")
    parser.add_argument("--chunk-size", type=int, default=2000, help="Chunk size")
    parser.add_argument("--socket-timeout", type=int, default=60, help="Socket timeout")
    parser.add_argument("--query-timeout", type=int, default=120, help="Query timeout")
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    logger.info(f"üñ•Ô∏è Platform: {platform.system()}")
    logger.info(f"üêç Python: {platform.python_version()}")
    logger.info(f"üîß Timeout method: {args.timeout_method}")
    
    # Create converter
    converter = CrossPlatformFalkorDBConverter(
        timeout_method=args.timeout_method,
        chunk_size=args.chunk_size,
        socket_timeout=args.socket_timeout,
        query_timeout=args.query_timeout
    )
    
    try:
        # Process file
        stats = converter.process_file_cross_platform(args.rdf_file, args.graph_name)
        
        logger.info("‚úÖ Processing completed successfully!")
        logger.info(f"üìä Final stats: {stats}")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Processing failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
