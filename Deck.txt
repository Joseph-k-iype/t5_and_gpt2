#!/usr/bin/env python3
"""
Next-Generation GDPR Search Engine with Advanced RAG & Multi-Agent Architecture
===============================================================================

A state-of-the-art command-line search engine featuring:
- Advanced RAG techniques (CRAG, Self-RAG, Adaptive RAG)
- Enhanced GraphRAG with FalkorDB knowledge graphs 
- Agentic Elasticsearch with hybrid search
- Multi-agent architecture with LangGraph coordination
- Real-time evaluation and correction mechanisms
- Custom OpenAI embeddings integration

Author: Enhanced with 2025 SOTA techniques
Date: June 2025
Framework: LangChain v0.3+ + LangGraph v0.4+ + FalkorDB + Elasticsearch
Python: 3.10+ (Cross-platform compatible)
"""

import asyncio
import json
import logging
import os
import sys
from pathlib import Path
import uuid
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Literal, Union, Sequence, Annotated
from dataclasses import dataclass, field, asdict
import argparse
import traceback
from enum import Enum
import pickle
import hashlib
from abc import ABC, abstractmethod

# Environment variable loading
from dotenv import load_dotenv
load_dotenv()

# Core libraries
import pandas as pd
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.markdown import Markdown
import click

# Enterprise web search (no API keys required)
import requests
from bs4 import BeautifulSoup
import feedparser
import wikipediaapi
from urllib.parse import urljoin, urlparse, quote
import ssl

# LangChain v0.3+ imports with proper structure
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool, BaseTool
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory

# Enhanced LangChain imports for advanced RAG
from langchain_community.chains.graph_qa.falkordb import FalkorDBQAChain
from langchain_community.graphs import FalkorDBGraph
from langchain_elasticsearch import ElasticsearchStore
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.ensemble import EnsembleRetriever

# LangGraph v0.4+ imports with latest API
from langgraph.graph import StateGraph, START, END, MessagesState, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import create_react_agent, ToolNode
from langgraph.types import Command
from typing_extensions import TypedDict

# Database clients
from elasticsearch import Elasticsearch
from falkordb import FalkorDB

# Async support
import aiofiles
import httpx

# OpenAI for direct API access
import openai

# ============================================================================
# CUSTOM EMBEDDINGS IMPLEMENTATION
# ============================================================================

class CustomOpenAIEmbeddings:
    """Custom OpenAI embeddings using direct API access"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-large", dimensions: int = 3072):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.dimensions = dimensions
    
    async def embed_query(self, text: str) -> List[float]:
        """Generate embedding for a single query"""
        try:
            response = await self.client.embeddings.create(
                input=text,
                model=self.model,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logging.error(f"❌ OpenAI embedding error: {e}")
            # Return zero vector as fallback
            return [0.0] * self.dimensions
    
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple documents"""
        try:
            response = await self.client.embeddings.create(
                input=texts,
                model=self.model,
                dimensions=self.dimensions
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logging.error(f"❌ OpenAI embedding error: {e}")
            # Return zero vectors as fallback
            return [[0.0] * self.dimensions for _ in texts]

# ============================================================================
# ENTERPRISE WEB SEARCH (NO API KEYS REQUIRED)
# ============================================================================

class EnterpriseWebSearch:
    """Enterprise web search without external API keys - Windows compatible"""
    
    def __init__(self, config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': getattr(config, 'USER_AGENT', 'GDPR-Search-Engine/1.0 (Enterprise)'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # Configure corporate proxy with authentication
        http_proxy = getattr(config, 'HTTP_PROXY', '')
        https_proxy = getattr(config, 'HTTPS_PROXY', '')
        
        if http_proxy or https_proxy:
            proxies = {}
            if http_proxy:
                proxies['http'] = http_proxy
            if https_proxy:
                proxies['https'] = https_proxy
            
            self.session.proxies.update(proxies)
            logging.info("Corporate proxy configured")
        
        # Initialize Wikipedia API
        enable_wikipedia = getattr(config, 'ENABLE_WIKIPEDIA_SEARCH', True)
        if enable_wikipedia:
            try:
                self.wiki = wikipediaapi.Wikipedia(
                    language='en',
                    user_agent=getattr(config, 'USER_AGENT', 'GDPR-Search-Engine/1.0 (Enterprise)')
                )
            except Exception as e:
                logging.warning(f"Wikipedia API initialization failed: {e}")
                self.wiki = None
        else:
            self.wiki = None
        
        # GDPR-specific RSS feeds (no API keys required)
        self.gdpr_rss_feeds = [
            "https://edpb.europa.eu/news/feed",  # European Data Protection Board
            "https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/feed/",  # UK ICO
            "https://www.cnil.fr/en/rss.xml",  # French CNIL
            "https://www.bfdi.bund.de/SiteGlobals/Functions/RSSFeed/RSSNewsletter_EN.xml",  # German DPA
        ]
        
        # GDPR authority websites for direct scraping
        self.gdpr_authorities = {
            "EDPB": "https://edpb.europa.eu",
            "UK_ICO": "https://ico.org.uk", 
            "EU_Commission": "https://commission.europa.eu",
            "CNIL": "https://www.cnil.fr/en",
            "BfDI": "https://www.bfdi.bund.de/EN"
        }
    
    async def search(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """Comprehensive web search using multiple sources"""
        all_results = []
        
        try:
            # 1. DuckDuckGo Instant Answer (no API key)
            web_search_enabled = getattr(self.config, 'WEB_SEARCH_ENABLED', True)
            if web_search_enabled:
                ddg_results = await self._duckduckgo_search(query, max_results=3)
                all_results.extend(ddg_results)
            
            # 2. Wikipedia search
            enable_wikipedia = getattr(self.config, 'ENABLE_WIKIPEDIA_SEARCH', True)
            if enable_wikipedia and self.wiki:
                wiki_results = await self._wikipedia_search(query, max_results=2)
                all_results.extend(wiki_results)
            
            # 3. RSS feeds from GDPR authorities
            enable_rss = getattr(self.config, 'ENABLE_RSS_FEEDS', True)
            if enable_rss:
                rss_results = await self._rss_search(query, max_results=3)
                all_results.extend(rss_results)
            
            # 4. Direct scraping of GDPR authority sites
            enable_scraping = getattr(self.config, 'ENABLE_WEB_SCRAPING', True)
            if enable_scraping:
                authority_results = await self._authority_search(query, max_results=2)
                all_results.extend(authority_results)
            
            # Sort by relevance score and return top results
            all_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            return all_results[:max_results]
            
        except Exception as e:
            logging.error(f"Enterprise web search error: {e}")
            return []
    
    async def _duckduckgo_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search using DuckDuckGo Instant Answer API (no key required)"""
        results = []
        
        try:
            # DuckDuckGo Instant Answer API
            ddg_url = "https://api.duckduckgo.com/"
            params = {
                'q': f"GDPR {query}",
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            timeout = getattr(self.config, 'REQUEST_TIMEOUT', 10)
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.session.get(ddg_url, params=params, timeout=timeout)
            )
            response.raise_for_status()
            
            data = response.json()
            
            # Process instant answer
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', 'DuckDuckGo Result'),
                    'content': data['AbstractText'],
                    'url': data.get('AbstractURL', ''),
                    'source': 'DuckDuckGo',
                    'relevance_score': 0.8,
                    'type': 'instant_answer'
                })
            
            # Process related topics
            for topic in data.get('RelatedTopics', [])[:max_results-1]:
                if isinstance(topic, dict) and topic.get('Text'):
                    results.append({
                        'title': topic.get('Text', '')[:100] + '...',
                        'content': topic.get('Text', ''),
                        'url': topic.get('FirstURL', ''),
                        'source': 'DuckDuckGo',
                        'relevance_score': 0.7,
                        'type': 'related_topic'
                    })
            
        except Exception as e:
            logging.error(f"DuckDuckGo search error: {e}")
        
        return results
    
    async def _wikipedia_search(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """Search Wikipedia for GDPR-related information"""
        results = []
        
        if not self.wiki:
            return results
        
        try:
            # Search Wikipedia
            search_query = f"GDPR {query}"
            search_results = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.wiki.search(search_query, results=max_results * 2)
            )
            
            for title in search_results[:max_results]:
                try:
                    page = await asyncio.get_event_loop().run_in_executor(
                        None, lambda: self.wiki.page(title)
                    )
                    
                    if page.exists():
                        # Calculate relevance based on GDPR keyword density
                        content = page.summary[:1000]
                        gdpr_keywords = ['gdpr', 'data protection', 'privacy', 'personal data', 'controller', 'processor']
                        relevance_score = sum(content.lower().count(keyword) for keyword in gdpr_keywords) / 10
                        relevance_score = min(relevance_score, 1.0)
                        
                        if relevance_score > 0.1:  # Only include relevant results
                            results.append({
                                'title': page.title,
                                'content': content,
                                'url': page.fullurl,
                                'source': 'Wikipedia',
                                'relevance_score': relevance_score,
                                'type': 'encyclopedia'
                            })
                            
                except Exception as e:
                    logging.warning(f"Error processing Wikipedia page {title}: {e}")
                    continue
                    
        except Exception as e:
            logging.error(f"Wikipedia search error: {e}")
        
        return results
    
    async def _rss_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search RSS feeds from GDPR authorities"""
        results = []
        query_lower = query.lower()
        
        for feed_url in self.gdpr_rss_feeds:
            try:
                # Parse RSS feed
                feed_data = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: feedparser.parse(feed_url)
                )
                
                for entry in feed_data.entries[:10]:  # Check last 10 entries per feed
                    title = entry.get('title', '')
                    summary = entry.get('summary', entry.get('description', ''))
                    content = f"{title} {summary}".lower()
                    
                    # Calculate relevance
                    query_words = query_lower.split()
                    relevance_score = sum(1 for word in query_words if word in content) / len(query_words)
                    
                    if relevance_score > 0.3:  # Only include relevant news
                        results.append({
                            'title': title,
                            'content': summary[:500],
                            'url': entry.get('link', ''),
                            'source': f"RSS: {urlparse(feed_url).netloc}",
                            'relevance_score': relevance_score,
                            'type': 'news',
                            'published': entry.get('published', '')
                        })
                        
                        if len(results) >= max_results:
                            break
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                logging.warning(f"Error parsing RSS feed {feed_url}: {e}")
                continue
        
        return results
    
    async def _authority_search(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """Search GDPR authority websites directly"""
        results = []
        
        for authority_name, base_url in self.gdpr_authorities.items():
            try:
                # Search specific authority site
                search_results = await self._scrape_authority_site(authority_name, base_url, query)
                results.extend(search_results)
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                logging.warning(f"Error searching {authority_name}: {e}")
                continue
        
        return results[:max_results]
    
    async def _scrape_authority_site(self, authority_name: str, base_url: str, query: str) -> List[Dict[str, Any]]:
        """Scrape specific GDPR authority website"""
        results = []
        
        try:
            # Common search patterns for different authorities
            search_patterns = {
                "EDPB": "/search?q={query}",
                "UK_ICO": "/search?q={query}",
                "EU_Commission": "/search/?queryText={query}",
                "CNIL": "/en/search?search_api_fulltext={query}",
                "BfDI": "/EN/search?query={query}"
            }
            
            search_path = search_patterns.get(authority_name, "/search?q={query}")
            search_url = urljoin(base_url, search_path.format(query=query.replace(' ', '+')))
            
            timeout = getattr(self.config, 'REQUEST_TIMEOUT', 10)
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.session.get(search_url, timeout=timeout)
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract search results (common patterns)
            result_selectors = [
                '.search-result',
                '.result-item', 
                '.search-item',
                'article',
                '.news-item'
            ]
            
            for selector in result_selectors:
                elements = soup.select(selector)[:3]  # Max 3 per authority
                
                for element in elements:
                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'a'])
                    content_elem = element.find(['p', '.summary', '.excerpt'])
                    link_elem = element.find('a', href=True)
                    
                    if title_elem and content_elem:
                        title = title_elem.get_text(strip=True)
                        content = content_elem.get_text(strip=True)[:300]
                        url = urljoin(base_url, link_elem['href']) if link_elem else base_url
                        
                        # Calculate relevance
                        query_words = query.lower().split()
                        text_content = f"{title} {content}".lower()
                        relevance_score = sum(1 for word in query_words if word in text_content) / len(query_words)
                        
                        if relevance_score > 0.2:
                            results.append({
                                'title': title,
                                'content': content,
                                'url': url,
                                'source': f"{authority_name} Official",
                                'relevance_score': relevance_score + 0.2,  # Boost official sources
                                'type': 'official_document'
                            })
                
                if results:  # If we found results with this selector, break
                    break
                    
        except Exception as e:
            logging.warning(f"Error scraping {authority_name}: {e}")
        
        return results

# ============================================================================
# ENHANCED CONFIGURATION WITH 2025 STANDARDS
# ============================================================================

@dataclass
class AdvancedSearchEngineConfig:
    """Enhanced configuration with 2025 SOTA features"""
    
    # OpenAI Configuration - Enterprise Standard
    OPENAI_API_KEY: str = field(default_factory=lambda: os.getenv("OPENAI_API_KEY", ""))
    OPENAI_BASE_URL: str = field(default_factory=lambda: os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"))
    REASONING_MODEL: str = "o3-mini"  # Latest reasoning model
    REASONING_EFFORT: str = field(default_factory=lambda: os.getenv("REASONING_EFFORT", "medium"))
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS: int = 3072
    
    # Advanced RAG Configuration
    ENABLE_CRAG: bool = field(default_factory=lambda: os.getenv("ENABLE_CRAG", "true").lower() == "true")
    ENABLE_SELF_RAG: bool = field(default_factory=lambda: os.getenv("ENABLE_SELF_RAG", "true").lower() == "true")
    ENABLE_ADAPTIVE_RAG: bool = field(default_factory=lambda: os.getenv("ENABLE_ADAPTIVE_RAG", "true").lower() == "true")
    ENABLE_MULTIMODAL: bool = field(default_factory=lambda: os.getenv("ENABLE_MULTIMODAL", "false").lower() == "true")
    
    # Retrieval Enhancement Settings
    RETRIEVAL_CONFIDENCE_THRESHOLD: float = 0.7
    MAX_RETRIEVAL_ITERATIONS: int = 3
    ENABLE_QUERY_EXPANSION: bool = True
    ENABLE_HIERARCHICAL_CHUNKING: bool = True
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    
    # FalkorDB Enhanced Configuration
    FALKORDB_HOST: str = field(default_factory=lambda: os.getenv("FALKOR_HOST", "localhost"))
    FALKORDB_PORT: int = field(default_factory=lambda: int(os.getenv("FALKOR_PORT", "6379")))
    FALKORDB_PASSWORD: str = field(default_factory=lambda: os.getenv("FALKOR_PASSWORD", ""))
    FALKORDB_GRAPH_NAME: str = "gdpr_metamodel_graph"
    FALKORDB_USE_TLS: bool = field(default_factory=lambda: os.getenv("FALKOR_TLS", "false").lower() == "true")
    
    # Elasticsearch Enhanced Configuration
    ELASTICSEARCH_HOST: str = field(default_factory=lambda: os.getenv("ES_HOST", "localhost"))
    ELASTICSEARCH_PORT: int = field(default_factory=lambda: int(os.getenv("ES_PORT", "9200")))
    ELASTICSEARCH_USERNAME: str = field(default_factory=lambda: os.getenv("ES_USERNAME", "elastic"))
    ELASTICSEARCH_PASSWORD: str = field(default_factory=lambda: os.getenv("ES_PASSWORD", ""))
    ELASTICSEARCH_INDEX: str = "gdpr_metamodel_kb"
    ELASTICSEARCH_CA_CERTS: str = field(default_factory=lambda: os.getenv("ES_CA_CERTS", ""))
    ELASTICSEARCH_VERIFY_CERTS: bool = field(default_factory=lambda: os.getenv("ES_VERIFY_CERTS", "false").lower() == "true")
    ENABLE_HYBRID_SEARCH: bool = True
    
    # Web Search Configuration
    WEB_SEARCH_ENABLED: bool = field(default_factory=lambda: os.getenv("WEB_SEARCH_ENABLED", "true").lower() == "true")
    ENABLE_WIKIPEDIA_SEARCH: bool = field(default_factory=lambda: os.getenv("ENABLE_WIKIPEDIA_SEARCH", "true").lower() == "true")
    ENABLE_RSS_FEEDS: bool = field(default_factory=lambda: os.getenv("ENABLE_RSS_FEEDS", "true").lower() == "true")
    ENABLE_WEB_SCRAPING: bool = field(default_factory=lambda: os.getenv("ENABLE_WEB_SCRAPING", "true").lower() == "true")
    USER_AGENT: str = "GDPR-Search-Engine/1.0 (Enterprise)"
    REQUEST_TIMEOUT: int = 10
    
    # Corporate Proxy Configuration (Windows format)
    HTTP_PROXY: str = field(default_factory=lambda: os.getenv("HTTP_PROXY", ""))
    HTTPS_PROXY: str = field(default_factory=lambda: os.getenv("HTTPS_PROXY", ""))
    NO_PROXY: str = field(default_factory=lambda: os.getenv("NO_PROXY", "localhost,127.0.0.1"))
    
    # Agent System Configuration
    MAX_AGENT_ITERATIONS: int = 5
    AGENT_REFLECTION_ENABLED: bool = True
    ENABLE_AGENT_MEMORY: bool = True
    ENABLE_COORDINATED_RETRIEVAL: bool = True
    
    # Evaluation and Feedback
    ENABLE_REAL_TIME_EVALUATION: bool = True
    CONFIDENCE_THRESHOLD: float = 0.8
    HALLUCINATION_DETECTION: bool = True
    FEEDBACK_LEARNING_RATE: float = 0.1
    
    # Performance and Scalability
    MAX_SEARCH_RESULTS: int = 15
    MAX_CONVERSATION_HISTORY: int = 30
    ENABLE_CACHING: bool = True
    CACHE_TTL: int = 3600  # 1 hour
    
    # Enterprise Features
    ENABLE_AUDIT_LOGGING: bool = True
    ENABLE_METRICS_COLLECTION: bool = True
    PRIVACY_MODE: bool = field(default_factory=lambda: os.getenv("PRIVACY_MODE", "false").lower() == "true")
    
    # Storage paths (cross-platform)
    SESSION_STORAGE_PATH: Path = field(default_factory=lambda: Path("./sessions"))
    FEEDBACK_STORAGE_PATH: Path = field(default_factory=lambda: Path("./feedback"))
    CACHE_STORAGE_PATH: Path = field(default_factory=lambda: Path("./cache"))
    LOGS_PATH: Path = field(default_factory=lambda: Path("./logs"))
    METRICS_PATH: Path = field(default_factory=lambda: Path("./metrics"))
    
    def validate(self):
        """Enhanced validation with comprehensive checks"""
        if not self.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required. Please set it in your .env file.")
        
        # Validate reasoning effort
        valid_efforts = ["low", "medium", "high"]
        if self.REASONING_EFFORT not in valid_efforts:
            raise ValueError(f"REASONING_EFFORT must be one of {valid_efforts}")
        
        # Validate thresholds
        if not 0.0 <= self.RETRIEVAL_CONFIDENCE_THRESHOLD <= 1.0:
            raise ValueError("RETRIEVAL_CONFIDENCE_THRESHOLD must be between 0.0 and 1.0")
        
        if not 0.0 <= self.CONFIDENCE_THRESHOLD <= 1.0:
            raise ValueError("CONFIDENCE_THRESHOLD must be between 0.0 and 1.0")
        
        # Create directories (cross-platform)
        try:
            for path in [self.SESSION_STORAGE_PATH, self.FEEDBACK_STORAGE_PATH, 
                        self.CACHE_STORAGE_PATH, self.LOGS_PATH, self.METRICS_PATH]:
                path.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logging.error(f"Failed to create directories: {e}")
            raise

# ============================================================================
# ADVANCED RAG TECHNIQUES IMPLEMENTATION
# ============================================================================

class RetrievalEvaluator:
    """Lightweight retrieval evaluator for CRAG implementation"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            model_kwargs={"reasoning_effort": config.REASONING_EFFORT}
        )
        
    async def evaluate_retrieval_quality(self, query: str, documents: List[Document]) -> Dict[str, Any]:
        """Evaluate the quality and relevance of retrieved documents"""
        
        if not documents:
            return {
                "confidence": 0.0,
                "action": "incorrect",
                "reasoning": "No documents retrieved"
            }
        
        # Create evaluation prompt
        doc_contents = "\n\n".join([f"Document {i+1}: {doc.page_content[:300]}..." 
                                   for i, doc in enumerate(documents[:3])])
        
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a retrieval quality evaluator for GDPR compliance queries.

Evaluate the relevance and accuracy of retrieved documents for the given query.

Classification:
- "correct": Documents are highly relevant and accurate
- "ambiguous": Documents are partially relevant but need clarification
- "incorrect": Documents are irrelevant or inaccurate

Provide a confidence score (0.0-1.0) and brief reasoning.

Respond in JSON format:
{
    "confidence": float,
    "action": "correct|ambiguous|incorrect", 
    "reasoning": "brief explanation"
}"""),
            ("human", f"""Query: {query}

Retrieved Documents:
{doc_contents}

Evaluate the retrieval quality:""")
        ])
        
        try:
            response = await self.llm.ainvoke(evaluation_prompt.format_messages())
            evaluation = json.loads(response.content.strip())
            
            # Ensure valid response
            if "confidence" not in evaluation:
                evaluation["confidence"] = 0.5
            if "action" not in evaluation:
                evaluation["action"] = "ambiguous"
            if "reasoning" not in evaluation:
                evaluation["reasoning"] = "Evaluation incomplete"
                
            return evaluation
            
        except Exception as e:
            logging.error(f"Retrieval evaluation error: {e}")
            return {
                "confidence": 0.5,
                "action": "ambiguous",
                "reasoning": f"Evaluation failed: {str(e)}"
            }

class CorrectiveRAG:
    """Corrective RAG (CRAG) implementation with self-correction"""
    
    def __init__(self, config: AdvancedSearchEngineConfig, web_search_tool, vector_search_tool):
        self.config = config
        self.evaluator = RetrievalEvaluator(config)
        self.web_search_tool = web_search_tool
        self.vector_search_tool = vector_search_tool
        
    async def corrective_retrieve(self, query: str, initial_documents: List[Document]) -> Dict[str, Any]:
        """Perform corrective retrieval with quality evaluation"""
        
        # Step 1: Evaluate initial retrieval
        evaluation = await self.evaluator.evaluate_retrieval_quality(query, initial_documents)
        
        corrected_documents = initial_documents.copy()
        correction_steps = []
        
        # Step 2: Apply corrective actions based on evaluation
        if evaluation["action"] == "correct":
            # Documents are good, use as-is
            correction_steps.append("Initial retrieval was sufficient")
            
        elif evaluation["action"] == "ambiguous":
            # Try to get additional context
            correction_steps.append("Retrieving additional context for ambiguous results")
            
            # Perform additional vector search with expanded query
            expanded_query = await self._expand_query(query)
            additional_results = await self.vector_search_tool.ainvoke({"query": expanded_query})
            
            if additional_results.get("success") and additional_results.get("results"):
                additional_docs = [Document(page_content=str(result.get("data", ""))) 
                                 for result in additional_results["results"][:3]]
                corrected_documents.extend(additional_docs)
                correction_steps.append(f"Added {len(additional_docs)} additional documents")
                
        elif evaluation["action"] == "incorrect":
            # Try web search for current information
            correction_steps.append("Initial retrieval was poor, trying web search")
            
            web_results = await self.web_search_tool.ainvoke({"query": query})
            
            if web_results.get("success") and web_results.get("results"):
                web_docs = [Document(
                    page_content=result.get("content", ""),
                    metadata={"source": result.get("url", ""), "title": result.get("title", "")}
                ) for result in web_results["results"][:3]]
                
                corrected_documents = web_docs  # Replace with web results
                correction_steps.append(f"Replaced with {len(web_docs)} web search results")
            else:
                correction_steps.append("Web search failed, using original documents")
        
        # Step 3: Re-evaluate if we made corrections
        if len(correction_steps) > 1:
            final_evaluation = await self.evaluator.evaluate_retrieval_quality(query, corrected_documents)
        else:
            final_evaluation = evaluation
        
        return {
            "documents": corrected_documents,
            "initial_evaluation": evaluation,
            "final_evaluation": final_evaluation,
            "correction_steps": correction_steps,
            "confidence": final_evaluation["confidence"]
        }
    
    async def _expand_query(self, query: str) -> str:
        """Expand query for better retrieval"""
        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a query expansion expert. Expand the given GDPR query with related terms and synonyms. Return only the expanded query."),
            ("human", f"Original query: {query}\n\nExpanded query:")
        ])
        
        try:
            llm = ChatOpenAI(model=self.config.REASONING_MODEL)
            response = await llm.ainvoke(expansion_prompt.format_messages())
            return response.content.strip()
        except Exception as e:
            logging.error(f"Query expansion error: {e}")
            return query

class SelfReflectiveRAG:
    """Self-RAG implementation with reflection tokens"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            model_kwargs={"reasoning_effort": config.REASONING_EFFORT}
        )
        
    async def generate_with_reflection(self, query: str, documents: List[Document], 
                                     conversation_context: str = "") -> Dict[str, Any]:
        """Generate response with self-reflection and critique"""
        
        # Step 1: Initial generation
        initial_response = await self._generate_initial_response(query, documents, conversation_context)
        
        # Step 2: Self-critique
        critique = await self._self_critique(query, initial_response, documents)
        
        # Step 3: Reflection and potential revision
        if critique["needs_revision"]:
            revised_response = await self._revise_response(query, initial_response, critique, documents)
            final_response = revised_response
            reflection_steps = ["Generated initial response", "Self-critique identified issues", "Revised response"]
        else:
            final_response = initial_response
            reflection_steps = ["Generated initial response", "Self-critique confirmed quality"]
        
        return {
            "response": final_response,
            "initial_response": initial_response,
            "critique": critique,
            "reflection_steps": reflection_steps,
            "confidence": critique.get("confidence", 0.7)
        }
    
    async def _generate_initial_response(self, query: str, documents: List[Document], 
                                       conversation_context: str) -> str:
        """Generate initial response from documents"""
        
        doc_context = "\n\n".join([f"Document {i+1}: {doc.page_content}" 
                                  for i, doc in enumerate(documents[:5])])
        
        generation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a GDPR compliance expert. Generate a comprehensive response based on the provided documents.

Key requirements:
- Be accurate and cite specific GDPR articles when relevant
- Provide practical guidance
- Acknowledge uncertainty when information is incomplete
- Use clear, professional language"""),
            ("human", f"""Conversation Context:
{conversation_context}

Question: {query}

Available Documents:
{doc_context}

Response:""")
        ])
        
        try:
            response = await self.llm.ainvoke(generation_prompt.format_messages())
            return response.content.strip()
        except Exception as e:
            logging.error(f"Initial generation error: {e}")
            return "I apologize, but I encountered an error while generating the response."
    
    async def _self_critique(self, query: str, response: str, documents: List[Document]) -> Dict[str, Any]:
        """Perform self-critique on the generated response"""
        
        critique_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a response quality critic for GDPR compliance. Evaluate the response quality.

Evaluation criteria:
1. Accuracy: Is the information factually correct?
2. Completeness: Does it fully answer the question?
3. Relevance: Is it directly relevant to the query?
4. Clarity: Is it clear and well-structured?
5. Citation: Are GDPR articles properly referenced?

Respond in JSON format:
{
    "needs_revision": boolean,
    "confidence": float (0.0-1.0),
    "issues": ["list of specific issues if any"],
    "strengths": ["list of strengths"],
    "suggestions": ["improvement suggestions if needed"]
}"""),
            ("human", f"""Query: {query}

Response to evaluate:
{response}

Provide your critique:""")
        ])
        
        try:
            critique_response = await self.llm.ainvoke(critique_prompt.format_messages())
            critique = json.loads(critique_response.content.strip())
            
            # Ensure valid structure
            required_keys = ["needs_revision", "confidence", "issues", "strengths", "suggestions"]
            for key in required_keys:
                if key not in critique:
                    critique[key] = [] if key in ["issues", "strengths", "suggestions"] else (False if key == "needs_revision" else 0.7)
            
            return critique
            
        except Exception as e:
            logging.error(f"Self-critique error: {e}")
            return {
                "needs_revision": False,
                "confidence": 0.7,
                "issues": [],
                "strengths": ["Response generated successfully"],
                "suggestions": []
            }
    
    async def _revise_response(self, query: str, initial_response: str, 
                             critique: Dict, documents: List[Document]) -> str:
        """Revise response based on critique"""
        
        doc_context = "\n\n".join([f"Document {i+1}: {doc.page_content}" 
                                  for i, doc in enumerate(documents[:5])])
        
        revision_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are revising a GDPR compliance response based on critique feedback.

Address the identified issues while maintaining the strengths. Focus on:
- Fixing factual inaccuracies
- Adding missing information
- Improving clarity and structure
- Ensuring proper GDPR citations"""),
            ("human", f"""Original Query: {query}

Initial Response:
{initial_response}

Critique Issues: {critique.get('issues', [])}
Suggestions: {critique.get('suggestions', [])}

Available Documents:
{doc_context}

Revised Response:""")
        ])
        
        try:
            response = await self.llm.ainvoke(revision_prompt.format_messages())
            return response.content.strip()
        except Exception as e:
            logging.error(f"Response revision error: {e}")
            return initial_response

class AdaptiveRAG:
    """Adaptive RAG with query complexity routing"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(model=config.REASONING_MODEL)
        
    async def analyze_query_complexity(self, query: str) -> Dict[str, Any]:
        """Analyze query complexity and determine retrieval strategy"""
        
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a query complexity analyzer for GDPR compliance queries.

Analyze the query and classify its complexity and type:

Complexity levels:
- simple: Basic factual questions
- moderate: Questions requiring synthesis of 2-3 concepts
- complex: Multi-step reasoning, comparisons, or analysis

Query types:
- factual: Looking for specific facts or definitions
- procedural: How-to questions or process explanations
- analytical: Comparing, evaluating, or analyzing concepts
- hybrid: Combining multiple aspects

Retrieval strategies:
- single_vector: Simple vector search sufficient
- multi_vector: Multiple vector searches needed
- graph_traversal: Relationship exploration required
- web_enhanced: Current information needed
- comprehensive: All retrieval methods required

Respond in JSON format:
{
    "complexity": "simple|moderate|complex",
    "query_type": "factual|procedural|analytical|hybrid",
    "retrieval_strategy": "single_vector|multi_vector|graph_traversal|web_enhanced|comprehensive",
    "reasoning": "explanation of the analysis"
}"""),
            ("human", f"Analyze this GDPR query: {query}")
        ])
        
        try:
            response = await self.llm.ainvoke(analysis_prompt.format_messages())
            analysis = json.loads(response.content.strip())
            
            # Validate response structure
            defaults = {
                "complexity": "moderate",
                "query_type": "factual", 
                "retrieval_strategy": "single_vector",
                "reasoning": "Default analysis applied"
            }
            
            for key, default_value in defaults.items():
                if key not in analysis:
                    analysis[key] = default_value
            
            return analysis
            
        except Exception as e:
            logging.error(f"Query complexity analysis error: {e}")
            return {
                "complexity": "moderate",
                "query_type": "factual",
                "retrieval_strategy": "single_vector", 
                "reasoning": f"Analysis failed: {str(e)}"
            }

# ============================================================================
# ENHANCED DATABASE MANAGERS WITH ADVANCED FEATURES
# ============================================================================

class AdvancedFalkorDBManager:
    """Enhanced FalkorDB manager with QA chains and openCypher queries"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.client = None
        self.graph = None
        self.qa_chain = None
        self._setup_client()
        self._setup_qa_chain()
    
    def _setup_client(self):
        """Setup FalkorDB client with enhanced configuration"""
        try:
            connection_params = {
                'host': self.config.FALKORDB_HOST,
                'port': self.config.FALKORDB_PORT,
                'decode_responses': True
            }
            
            if self.config.FALKORDB_PASSWORD:
                connection_params['password'] = self.config.FALKORDB_PASSWORD
            
            if self.config.FALKORDB_USE_TLS:
                connection_params['ssl'] = True
                connection_params['ssl_cert_reqs'] = 'required'
            
            self.client = FalkorDB(**connection_params)
            self.graph = self.client.select_graph(self.config.FALKORDB_GRAPH_NAME)
            
            # Initialize FalkorDB Graph for LangChain
            self.langchain_graph = FalkorDBGraph(
                database=self.config.FALKORDB_GRAPH_NAME,
                host=self.config.FALKORDB_HOST,
                port=self.config.FALKORDB_PORT,
                password=self.config.FALKORDB_PASSWORD,
                ssl=self.config.FALKORDB_USE_TLS
            )
            
            logging.info(f"✅ Connected to FalkorDB: {self.config.FALKORDB_GRAPH_NAME}")
            
        except Exception as e:
            logging.error(f"❌ FalkorDB connection error: {e}")
            raise
    
    def _setup_qa_chain(self):
        """Setup enhanced FalkorDB QA chain"""
        try:
            llm = ChatOpenAI(
                model=self.config.REASONING_MODEL,
                model_kwargs={"reasoning_effort": self.config.REASONING_EFFORT}
            )
            
            # Create FalkorDB QA Chain with custom prompts
            self.qa_chain = FalkorDBQAChain.from_llm(
                llm=llm,
                graph=self.langchain_graph,
                verbose=True,
                return_intermediate_steps=True
            )
            
            logging.info("✅ FalkorDB QA Chain initialized")
            
        except Exception as e:
            logging.error(f"❌ FalkorDB QA Chain setup error: {e}")
            self.qa_chain = None
    
    async def enhanced_graph_search(self, query: str, search_type: str = "adaptive") -> List[Dict[str, Any]]:
        """Enhanced graph search with multiple strategies"""
        try:
            if search_type == "qa_chain":
                return await self._qa_chain_search(query)
            elif search_type == "multi_hop":
                return await self._multi_hop_search(query)
            elif search_type == "pattern_matching":
                return await self._pattern_matching_search(query)
            elif search_type == "adaptive":
                return await self._adaptive_graph_search(query)
            else:
                return await self._semantic_graph_search(query)
                
        except Exception as e:
            logging.error(f"❌ Enhanced graph search error: {e}")
            return []
    
    async def _qa_chain_search(self, query: str) -> List[Dict[str, Any]]:
        """Use FalkorDB QA Chain for natural language queries"""
        if not self.qa_chain:
            return []
        
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.qa_chain.invoke({"query": query})
            )
            
            return [{
                "type": "qa_chain_result",
                "data": {
                    "query": query,
                    "result": result.get("result", ""),
                    "intermediate_steps": result.get("intermediate_steps", [])
                },
                "score": 0.9,
                "source": "FalkorDB QA Chain"
            }]
            
        except Exception as e:
            logging.error(f"❌ QA Chain search error: {e}")
            return []
    
    async def _multi_hop_search(self, query: str) -> List[Dict[str, Any]]:
        """Multi-hop reasoning through graph relationships using openCypher"""
        try:
            # openCypher query for multi-hop traversal without LIMIT
            multi_hop_query = """
            MATCH path = (start)-[*1..3]-(end)
            WHERE start.label CONTAINS $query OR start.definition CONTAINS $query
            WITH path, start, end, length(path) as path_length
            ORDER BY path_length ASC
            RETURN start, end, path, path_length
            """
            
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.graph.query(multi_hop_query, {'query': query.lower()})
            )
            
            results = []
            for record in result.result_set:
                results.append({
                    "type": "multi_hop_path",
                    "data": {
                        "start_node": record[0],
                        "end_node": record[1], 
                        "path": record[2],
                        "path_length": record[3]
                    },
                    "score": 0.85,
                    "reasoning_type": "multi_hop"
                })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Multi-hop search error: {e}")
            return []
    
    async def _pattern_matching_search(self, query: str) -> List[Dict[str, Any]]:
        """Pattern matching search using openCypher patterns"""
        try:
            # Pattern matching for GDPR concepts
            pattern_queries = [
                # Find concepts and their broader/narrower relationships
                """
                MATCH (c:Concept)-[:skos_broader]->(broader)
                WHERE c.label CONTAINS $query OR c.definition CONTAINS $query
                RETURN c, broader, 'broader' as relationship_type
                """,
                
                # Find articles referencing concepts
                """
                MATCH (a:Article)-[:references]->(c:Concept)
                WHERE c.label CONTAINS $query OR a.title CONTAINS $query
                RETURN a, c, 'references' as relationship_type
                """,
                
                # Find related concepts through shared properties
                """
                MATCH (c1:Concept)-[:skos_related]-(c2:Concept)
                WHERE c1.label CONTAINS $query OR c1.definition CONTAINS $query
                RETURN c1, c2, 'related' as relationship_type
                """
            ]
            
            results = []
            for i, cypher_query in enumerate(pattern_queries):
                try:
                    result = await asyncio.get_event_loop().run_in_executor(
                        None, lambda q=cypher_query: self.graph.query(q, {'query': query.lower()})
                    )
                    
                    for record in result.result_set:
                        results.append({
                            "type": f"pattern_match_{i}",
                            "data": {
                                "node1": record[0],
                                "node2": record[1] if len(record) > 1 else None,
                                "relationship": record[2] if len(record) > 2 else "unknown"
                            },
                            "score": 0.8,
                            "source": "FalkorDB Pattern"
                        })
                        
                except Exception as pattern_error:
                    logging.warning(f"Pattern query {i} failed: {pattern_error}")
                    continue
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Pattern matching search error: {e}")
            return []
    
    async def _adaptive_graph_search(self, query: str) -> List[Dict[str, Any]]:
        """Adaptive search combining multiple approaches"""
        all_results = []
        
        # Try QA Chain first for natural language understanding
        qa_results = await self._qa_chain_search(query)
        all_results.extend(qa_results)
        
        # Add semantic search
        semantic_results = await self._semantic_graph_search(query)
        all_results.extend(semantic_results)
        
        # Add multi-hop if query seems complex
        if len(query.split()) > 3:
            multi_hop_results = await self._multi_hop_search(query)
            all_results.extend(multi_hop_results)
        
        # Add pattern matching for relationship queries
        if any(word in query.lower() for word in ['related', 'connected', 'similar', 'relationship']):
            pattern_results = await self._pattern_matching_search(query)
            all_results.extend(pattern_results)
        
        # Sort by score and remove duplicates
        unique_results = []
        seen_content = set()
        
        for result in sorted(all_results, key=lambda x: x.get('score', 0), reverse=True):
            content_hash = hashlib.md5(str(result.get('data', '')).encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        return unique_results[:self.config.MAX_SEARCH_RESULTS]
    
    async def _semantic_graph_search(self, query: str) -> List[Dict[str, Any]]:
        """Enhanced semantic search with openCypher"""
        results = []
        try:
            # Enhanced semantic query with multiple node types - no LIMIT
            semantic_query = """
            MATCH (n)
            WHERE n.label CONTAINS $query 
               OR n.definition CONTAINS $query
               OR n.description CONTAINS $query
               OR n.content CONTAINS $query
            OPTIONAL MATCH (n)-[r]-(related)
            WITH n, collect({type: type(r), related: related}) as relationships
            RETURN n, relationships, 
                   CASE 
                       WHEN n.label CONTAINS $query THEN 1.0
                       WHEN n.definition CONTAINS $query THEN 0.8
                       ELSE 0.6
                   END as relevance_score
            ORDER BY relevance_score DESC
            """
            
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.graph.query(semantic_query, {'query': query.lower()})
            )
            
            for record in result.result_set:
                node = record[0]
                relationships = record[1] if len(record) > 1 else []
                relevance_score = record[2] if len(record) > 2 else 0.5
                
                results.append({
                    'type': 'semantic_concept',
                    'data': node,
                    'relationships': relationships,
                    'score': relevance_score,
                    'source': 'FalkorDB Semantic'
                })
                
        except Exception as e:
            logging.error(f"❌ Semantic graph search error: {e}")
        
        return results

class AdvancedElasticsearchManager:
    """Enhanced Elasticsearch manager with agentic capabilities"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.client = None
        self.embeddings = None
        self._setup_client()
        self._setup_embeddings()
    
    def _setup_client(self):
        """Setup Elasticsearch client with enhanced configuration"""
        try:
            connection_params = {
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True,
                "verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
            }
            
            if self.config.ELASTICSEARCH_CA_CERTS:
                connection_params["ca_certs"] = self.config.ELASTICSEARCH_CA_CERTS
                protocol = "https"
            else:
                protocol = "https" if self.config.ELASTICSEARCH_VERIFY_CERTS else "http"
            
            host_url = f"{protocol}://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"
            connection_params["hosts"] = [host_url]
            
            if self.config.ELASTICSEARCH_PASSWORD:
                connection_params["basic_auth"] = (
                    self.config.ELASTICSEARCH_USERNAME, 
                    self.config.ELASTICSEARCH_PASSWORD
                )
            
            self.client = Elasticsearch(**connection_params)
            
            if self.client.ping():
                logging.info(f"✅ Connected to Elasticsearch at {host_url}")
            else:
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"❌ Elasticsearch connection error: {e}")
            raise
    
    def _setup_embeddings(self):
        """Setup custom OpenAI embeddings"""
        try:
            self.embeddings = CustomOpenAIEmbeddings(
                api_key=self.config.OPENAI_API_KEY,
                model=self.config.EMBEDDING_MODEL,
                dimensions=self.config.EMBEDDING_DIMENSIONS
            )
            logging.info("✅ Custom OpenAI embeddings initialized")
        except Exception as e:
            logging.error(f"❌ Embeddings setup error: {e}")
            raise
    
    async def agentic_vector_search(self, query: str, search_strategy: str = "hybrid") -> List[Dict[str, Any]]:
        """Agentic vector search with multiple strategies"""
        try:
            if search_strategy == "hybrid":
                return await self._hybrid_search(query)
            elif search_strategy == "multi_query":
                return await self._multi_query_search(query)
            elif search_strategy == "contextual_compression":
                return await self._contextual_compression_search(query)
            else:
                return await self._dense_vector_search(query)
                
        except Exception as e:
            logging.error(f"❌ Agentic vector search error: {e}")
            return []
    
    async def _hybrid_search(self, query: str) -> List[Dict[str, Any]]:
        """Hybrid search combining dense vector and sparse text search"""
        try:
            # Dense vector search
            vector_results = await self._dense_vector_search(query)
            
            # Sparse keyword search
            keyword_results = await self._keyword_search(query)
            
            # Combine and rerank results
            combined_results = []
            
            # Add vector results with higher weight for semantic relevance
            for i, result in enumerate(vector_results[:7]):
                result['hybrid_score'] = result.get('score', 0.7) * 0.7 + (1.0 - i * 0.1)
                result['search_type'] = 'vector'
                combined_results.append(result)
            
            # Add keyword results with weight for exact matches
            for i, result in enumerate(keyword_results[:5]):
                result['hybrid_score'] = result.get('score', 0.6) * 0.3 + (0.8 - i * 0.1)
                result['search_type'] = 'keyword'
                combined_results.append(result)
            
            # Sort by hybrid score and remove duplicates
            unique_results = []
            seen_ids = set()
            
            for result in sorted(combined_results, key=lambda x: x.get('hybrid_score', 0), reverse=True):
                result_id = result.get('id') or hash(str(result.get('data', '')))
                if result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
            
            return unique_results[:self.config.MAX_SEARCH_RESULTS]
            
        except Exception as e:
            logging.error(f"❌ Hybrid search error: {e}")
            return await self._dense_vector_search(query)
    
    async def _dense_vector_search(self, query: str) -> List[Dict[str, Any]]:
        """Dense vector search using embeddings"""
        try:
            # Generate query embedding
            query_embedding = await self.embeddings.embed_query(query)
            
            # Elasticsearch vector search
            search_body = {
                "size": self.config.MAX_SEARCH_RESULTS,
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embeddings') + 1.0",
                            "params": {"query_vector": query_embedding}
                        }
                    }
                },
                "_source": {
                    "excludes": ["embeddings"]
                }
            }
            
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.client.search(
                    index=self.config.ELASTICSEARCH_INDEX,
                    body=search_body
                )
            )
            
            results = []
            for hit in response['hits']['hits']:
                results.append({
                    'type': 'dense_vector',
                    'data': hit['_source'],
                    'score': hit['_score'],
                    'id': hit['_id'],
                    'source': 'Elasticsearch Dense Vector'
                })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Dense vector search error: {e}")
            return []
    
    async def _keyword_search(self, query: str) -> List[Dict[str, Any]]:
        """Keyword-based search for exact matches"""
        try:
            search_body = {
                "size": self.config.MAX_SEARCH_RESULTS,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["content^2", "title^1.5", "document_type", "source_file"],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                }
            }
            
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.client.search(
                    index=self.config.ELASTICSEARCH_INDEX,
                    body=search_body
                )
            )
            
            results = []
            for hit in response['hits']['hits']:
                results.append({
                    'type': 'keyword_match',
                    'data': hit['_source'],
                    'score': hit['_score'] / 10.0,  # Normalize score
                    'id': hit['_id'],
                    'source': 'Elasticsearch Keyword'
                })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Keyword search error: {e}")
            return []
    
    async def _multi_query_search(self, query: str) -> List[Dict[str, Any]]:
        """Multi-query search for comprehensive results"""
        try:
            # Generate multiple query variations
            query_variations = await self._generate_query_variations(query)
            
            all_results = []
            for variation in query_variations:
                variation_results = await self._dense_vector_search(variation)
                for result in variation_results:
                    result['query_variation'] = variation
                all_results.extend(variation_results)
            
            # Deduplicate and score
            unique_results = []
            seen_content = set()
            
            for result in all_results:
                content_hash = hashlib.md5(str(result.get('data', '')).encode()).hexdigest()
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)
            
            return sorted(unique_results, key=lambda x: x.get('score', 0), reverse=True)[:self.config.MAX_SEARCH_RESULTS]
            
        except Exception as e:
            logging.error(f"❌ Multi-query search error: {e}")
            return await self._dense_vector_search(query)
    
    async def _generate_query_variations(self, query: str) -> List[str]:
        """Generate query variations for multi-query search"""
        try:
            llm = ChatOpenAI(model=self.config.REASONING_MODEL)
            
            variation_prompt = ChatPromptTemplate.from_messages([
                ("system", """Generate 3 alternative phrasings of the given GDPR query to improve search recall.
Each variation should maintain the original intent but use different terminology.

Return only the 3 variations, one per line, without numbers or formatting."""),
                ("human", f"Original query: {query}")
            ])
            
            response = await llm.ainvoke(variation_prompt.format_messages())
            variations = [line.strip() for line in response.content.strip().split('\n') if line.strip()]
            
            return [query] + variations[:3]  # Original query + up to 3 variations
            
        except Exception as e:
            logging.error(f"Query variation generation error: {e}")
            return [query]
    
    async def _contextual_compression_search(self, query: str) -> List[Dict[str, Any]]:
        """Contextual compression search for focused results"""
        try:
            # Get initial results
            initial_results = await self._dense_vector_search(query)
            
            if not initial_results:
                return []
            
            # Create documents for compression
            documents = []
            for result in initial_results[:10]:  # Compress top 10
                doc_content = str(result.get('data', {}).get('content', ''))
                if doc_content:
                    documents.append(Document(
                        page_content=doc_content,
                        metadata=result.get('data', {})
                    ))
            
            # Compress documents using LLM
            compressed_docs = await self._compress_documents(query, documents)
            
            # Convert back to result format
            compressed_results = []
            for i, doc in enumerate(compressed_docs):
                compressed_results.append({
                    'type': 'contextual_compressed',
                    'data': {
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    },
                    'score': 0.9 - (i * 0.05),  # Decrease score by rank
                    'id': f"compressed_{i}",
                    'source': 'Elasticsearch Compressed'
                })
            
            return compressed_results
            
        except Exception as e:
            logging.error(f"❌ Contextual compression search error: {e}")
            return await self._dense_vector_search(query)
    
    async def _compress_documents(self, query: str, documents: List[Document]) -> List[Document]:
        """Compress documents to extract relevant portions"""
        try:
            llm = ChatOpenAI(model=self.config.REASONING_MODEL)
            
            compressed_docs = []
            for doc in documents:
                compression_prompt = ChatPromptTemplate.from_messages([
                    ("system", """You are a document compression expert. Extract the most relevant portions of the document for the given query.

Requirements:
- Keep only content directly relevant to the query
- Maintain important context and details
- Remove irrelevant sections
- Preserve key facts and citations
- Maximum 300 words per document"""),
                    ("human", f"""Query: {query}

Document to compress:
{doc.page_content}

Compressed relevant content:""")
                ])
                
                try:
                    response = await llm.ainvoke(compression_prompt.format_messages())
                    compressed_content = response.content.strip()
                    
                    if compressed_content and len(compressed_content) > 50:
                        compressed_docs.append(Document(
                            page_content=compressed_content,
                            metadata=doc.metadata
                        ))
                        
                except Exception as comp_error:
                    logging.warning(f"Document compression failed: {comp_error}")
                    # Keep original if compression fails
                    compressed_docs.append(doc)
            
            return compressed_docs
            
        except Exception as e:
            logging.error(f"Document compression error: {e}")
            return documents

# ============================================================================
# ENHANCED MULTI-AGENT TOOLS WITH ADVANCED CAPABILITIES
# ============================================================================

# Global instances for enhanced tool injection
falkor_manager: Optional[AdvancedFalkorDBManager] = None
es_manager: Optional[AdvancedElasticsearchManager] = None
web_search_instance = None
corrective_rag: Optional[CorrectiveRAG] = None
self_reflective_rag: Optional[SelfReflectiveRAG] = None
adaptive_rag: Optional[AdaptiveRAG] = None

@tool
def enhanced_graph_search_tool(query: str, search_type: str = "adaptive") -> Dict[str, Any]:
    """Enhanced graph search with QA chains and openCypher"""
    try:
        if falkor_manager is None:
            return {
                "success": False,
                "error": "Graph database not available",
                "results": []
            }
        
        # Create a new event loop for this task
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(falkor_manager.enhanced_graph_search(query, search_type))
            loop.close()
        except Exception as e:
            logging.error(f"Graph search execution error: {e}")
            results = []
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "search_type": search_type,
            "capabilities": ["qa_chain", "openCypher", "multi_hop", "semantic"]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def agentic_vector_search_tool(query: str, search_strategy: str = "hybrid") -> Dict[str, Any]:
    """Agentic vector search with hybrid capabilities"""
    try:
        if es_manager is None:
            return {
                "success": False,
                "error": "Vector database not available",
                "results": []
            }
        
        # Create a new event loop for this task
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(es_manager.agentic_vector_search(query, search_strategy))
            loop.close()
        except Exception as e:
            logging.error(f"Vector search execution error: {e}")
            results = []
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "search_strategy": search_strategy,
            "capabilities": ["hybrid", "multi_query", "contextual_compression"]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool 
def corrective_retrieval_tool(query: str) -> Dict[str, Any]:
    """Corrective RAG (CRAG) with quality evaluation and self-correction"""
    try:
        if corrective_rag is None:
            return {
                "success": False,
                "error": "Corrective RAG not available",
                "results": []
            }
        
        # Get initial vector search results
        initial_search = agentic_vector_search_tool.invoke({"query": query, "search_strategy": "hybrid"})
        
        if not initial_search.get("success"):
            return initial_search
        
        # Convert to documents for evaluation
        initial_docs = []
        for result in initial_search.get("results", []):
            content = str(result.get("data", {}).get("content", ""))
            if content:
                initial_docs.append(Document(page_content=content))
        
        # Run corrective retrieval with new event loop
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            correction_result = loop.run_until_complete(corrective_rag.corrective_retrieve(query, initial_docs))
            loop.close()
        except Exception as e:
            logging.error(f"Corrective RAG execution error: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }
        
        return {
            "success": True,
            "results": [{"type": "corrective_rag", "data": doc.page_content, "metadata": doc.metadata} 
                       for doc in correction_result["documents"]],
            "initial_evaluation": correction_result["initial_evaluation"],
            "final_evaluation": correction_result["final_evaluation"],
            "correction_steps": correction_result["correction_steps"],
            "confidence": correction_result["confidence"]
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def adaptive_routing_tool(query: str) -> Dict[str, Any]:
    """Adaptive query routing based on complexity analysis"""
    try:
        if adaptive_rag is None:
            return {
                "success": False,
                "error": "Adaptive RAG not available",
                "results": []
            }
        
        # Analyze query complexity with new event loop
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            analysis = loop.run_until_complete(adaptive_rag.analyze_query_complexity(query))
            loop.close()
        except Exception as e:
            logging.error(f"Adaptive routing execution error: {e}")
            analysis = {
                "complexity": "moderate",
                "query_type": "factual",
                "retrieval_strategy": "single_vector",
                "reasoning": f"Analysis failed: {str(e)}"
            }
        
        # Route to appropriate search strategy
        strategy_map = {
            "single_vector": "hybrid",
            "multi_vector": "multi_query", 
            "graph_traversal": "adaptive",
            "web_enhanced": "hybrid",
            "comprehensive": "adaptive"
        }
        
        retrieval_strategy = analysis.get("retrieval_strategy", "single_vector")
        search_strategy = strategy_map.get(retrieval_strategy, "hybrid")
        
        # Execute appropriate search
        if retrieval_strategy == "graph_traversal":
            search_result = enhanced_graph_search_tool.invoke({
                "query": query, 
                "search_type": "adaptive"
            })
        else:
            search_result = agentic_vector_search_tool.invoke({
                "query": query,
                "search_strategy": search_strategy
            })
        
        return {
            "success": True,
            "query_analysis": analysis,
            "selected_strategy": search_strategy,
            "results": search_result.get("results", []),
            "routing_reasoning": analysis.get("reasoning", "")
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def web_search_tool(query: str) -> Dict[str, Any]:
    """Enhanced web search for latest GDPR information"""
    try:
        if web_search_instance is None:
            return {
                "success": False,
                "error": "Web search not available",
                "results": []
            }
        
        # Run web search with new event loop
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(web_search_instance.search(f"GDPR {query}"))
            loop.close()
        except Exception as e:
            logging.error(f"Web search execution error: {e}")
            results = []
        
        return {
            "success": True,
            "results": results,
            "count": len(results),
            "sources": list(set(r.get('source', 'Unknown') for r in results))
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def comprehensive_search_tool(query: str) -> Dict[str, Any]:
    """Comprehensive search combining all available methods"""
    try:
        # Start with adaptive routing
        routing_result = adaptive_routing_tool.invoke({"query": query})
        
        # Get graph results
        graph_result = enhanced_graph_search_tool.invoke({
            "query": query,
            "search_type": "adaptive"
        })
        
        # Get vector results with corrective RAG
        corrective_result = corrective_retrieval_tool.invoke({"query": query})
        
        # Get web results for current information
        web_result = web_search_tool.invoke({"query": query})
        
        return {
            "success": True,
            "adaptive_routing": routing_result,
            "graph_search": graph_result,
            "corrective_rag": corrective_result,
            "web_search": web_result,
            "total_sources": 4,
            "comprehensive": True
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

# ============================================================================
# ENHANCED MULTI-AGENT ARCHITECTURE WITH ADVANCED COORDINATION
# ============================================================================

class AdvancedSearchState(TypedDict):
    """Enhanced state for advanced multi-agent search system"""
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    query_analysis: Optional[Dict[str, Any]]
    routing_decision: Optional[str]
    
    # Search results from different methods
    graph_results: Optional[List[Dict]]
    vector_results: Optional[List[Dict]]
    web_results: Optional[List[Dict]]
    corrective_results: Optional[Dict[str, Any]]
    
    # Advanced RAG results
    self_rag_results: Optional[Dict[str, Any]]
    adaptive_routing: Optional[Dict[str, Any]]
    
    # Evaluation and reflection
    retrieval_evaluations: List[Dict[str, Any]]
    confidence_scores: Dict[str, float]
    reflection_steps: List[str]
    
    # Final synthesis
    synthesized_response: Optional[str]
    final_confidence: Optional[float]
    requires_feedback: bool
    
    # Agent coordination
    active_agent: Optional[str]
    agent_history: List[str]
    iteration_count: int
    
    # Performance metrics
    search_latency: Dict[str, float]
    total_documents: int

class EnhancedGDPRSearchAgents:
    """Advanced multi-agent system with 2025 SOTA techniques"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        
        # Enhanced LLM with reasoning capabilities
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            model_kwargs={
                "reasoning_effort": config.REASONING_EFFORT
            }
        )
        
        # Initialize advanced RAG components
        self.corrective_rag = None
        self.self_reflective_rag = None
        self.adaptive_rag = None
        
        # Enhanced tools with advanced capabilities
        self.tools = [
            enhanced_graph_search_tool,
            agentic_vector_search_tool,
            corrective_retrieval_tool,
            adaptive_routing_tool,
            web_search_tool,
            comprehensive_search_tool
        ]
        
        # Enhanced memory and state management
        self.memory = MemorySaver()
        self.store = InMemoryStore()
        
        # Build the enhanced agent graph
        self.search_graph = self._build_enhanced_search_graph()
    
    def initialize_advanced_rag(self, falkor_mgr, es_mgr, web_search):
        """Initialize advanced RAG components after database setup"""
        global corrective_rag, self_reflective_rag, adaptive_rag
        
        self.corrective_rag = CorrectiveRAG(self.config, web_search_tool, agentic_vector_search_tool)
        self.self_reflective_rag = SelfReflectiveRAG(self.config)
        self.adaptive_rag = AdaptiveRAG(self.config)
        
        # Set global instances
        corrective_rag = self.corrective_rag
        self_reflective_rag = self.self_reflective_rag
        adaptive_rag = self.adaptive_rag
    
    def _build_enhanced_search_graph(self) -> StateGraph:
        """Build enhanced multi-agent search graph with advanced coordination"""
        
        workflow = StateGraph(AdvancedSearchState)
        
        # Enhanced agent nodes
        workflow.add_node("query_analyzer", self._query_analyzer_agent)
        workflow.add_node("adaptive_router", self._adaptive_router_agent)
        workflow.add_node("graph_specialist", self._enhanced_graph_specialist)
        workflow.add_node("vector_specialist", self._enhanced_vector_specialist)
        workflow.add_node("web_researcher", self._enhanced_web_researcher)
        workflow.add_node("corrective_agent", self._corrective_rag_agent)
        workflow.add_node("reflection_agent", self._self_reflection_agent)
        workflow.add_node("synthesis_coordinator", self._synthesis_coordinator)
        workflow.add_node("quality_evaluator", self._quality_evaluator)
        workflow.add_node("response_optimizer", self._response_optimizer)
        
        # Enhanced routing with adaptive decision making
        workflow.add_edge(START, "query_analyzer")
        workflow.add_edge("query_analyzer", "adaptive_router")
        
        # Adaptive routing to specialists
        workflow.add_conditional_edges(
            "adaptive_router",
            self._route_to_specialists,
            {
                "graph_only": "graph_specialist",
                "vector_only": "vector_specialist",
                "web_only": "web_researcher", 
                "graph_vector": "graph_specialist",
                "comprehensive": "corrective_agent",
                "simple": "vector_specialist"
            }
        )
        
        # Specialist coordination paths
        workflow.add_edge("graph_specialist", "synthesis_coordinator")
        workflow.add_edge("vector_specialist", "corrective_agent")
        workflow.add_edge("web_researcher", "synthesis_coordinator")
        workflow.add_edge("corrective_agent", "reflection_agent")
        workflow.add_edge("reflection_agent", "synthesis_coordinator")
        
        # Enhanced evaluation and optimization
        workflow.add_edge("synthesis_coordinator", "quality_evaluator")
        
        workflow.add_conditional_edges(
            "quality_evaluator",
            self._evaluate_quality,
            {
                "optimize": "response_optimizer",
                "retry_graph": "graph_specialist",
                "retry_vector": "vector_specialist", 
                "retry_web": "web_researcher",
                "complete": END
            }
        )
        
        workflow.add_edge("response_optimizer", END)
        
        return workflow.compile(checkpointer=self.memory, store=self.store)
    
    def _query_analyzer_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced query analysis with complexity assessment"""
        
        messages = state.get("messages", [])
        if not messages:
            return state
        
        query = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
        state["query"] = query
        
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced GDPR query analyzer using reasoning capabilities.

Analyze the query for:
1. Intent classification (factual, procedural, analytical, comparative)
2. Complexity level (simple, moderate, complex, expert)
3. Domain specificity (general GDPR, specific articles, case law, practical implementation)
4. Information recency needs (static knowledge vs current developments)
5. Reasoning requirements (direct lookup, multi-hop, synthesis, analysis)

Consider edge cases and ambiguous queries. Use chain-of-thought reasoning.

Return JSON:
{
    "intent": "factual|procedural|analytical|comparative|hybrid",
    "complexity": "simple|moderate|complex|expert", 
    "domain": "general|articles|case_law|implementation|cross_domain",
    "recency": "static|recent|current|mixed",
    "reasoning": "direct|multi_hop|synthesis|analysis|creative",
    "confidence": float,
    "key_concepts": ["list", "of", "concepts"],
    "reasoning_chain": "step by step analysis"
}"""),
            ("human", f"Analyze this GDPR query: {query}")
        ])
        
        try:
            response = self.llm.invoke(analysis_prompt.format_messages())
            response_text = response.content.strip()
            
            # Clean up response - remove any markdown formatting or extra text
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            # Try to find JSON in the response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                analysis = json.loads(json_text)
            else:
                # Fallback if no JSON found
                raise ValueError("No valid JSON found in response")
            
            # Validate and set defaults
            defaults = {
                "intent": "factual",
                "complexity": "moderate",
                "domain": "general", 
                "recency": "static",
                "reasoning": "direct",
                "confidence": 0.7,
                "key_concepts": [],
                "reasoning_chain": "Default analysis"
            }
            
            for key, default in defaults.items():
                if key not in analysis:
                    analysis[key] = default
            
            state["query_analysis"] = analysis
            state["active_agent"] = "query_analyzer"
            state["agent_history"] = ["query_analyzer"]
            state["iteration_count"] = 0
            
            # Add analysis message
            analysis_msg = AIMessage(
                content=f"Query analyzed: {analysis['complexity']} {analysis['intent']} query about {analysis['domain']} GDPR topics",
                name="query_analyzer"
            )
            state["messages"] = state.get("messages", []) + [analysis_msg]
            
            logging.info(f"🔍 Query Analysis: {analysis['complexity']} {analysis['intent']} query (confidence: {analysis['confidence']:.2f})")
            
        except Exception as e:
            logging.error(f"❌ Query analysis error: {e}")
            state["query_analysis"] = {
                "intent": "factual",
                "complexity": "moderate", 
                "domain": "general",
                "recency": "static",
                "reasoning": "direct",
                "confidence": 0.5,
                "key_concepts": [],
                "reasoning_chain": f"Analysis failed: {str(e)}"
            }
        
        return state
    
    def _adaptive_router_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced adaptive routing based on query analysis"""
        
        analysis = state.get("query_analysis", {})
        query = state.get("query", "")
        
        # Advanced routing logic based on analysis
        complexity = analysis.get("complexity", "moderate")
        intent = analysis.get("intent", "factual")
        domain = analysis.get("domain", "general")
        reasoning = analysis.get("reasoning", "direct")
        recency = analysis.get("recency", "static")
        
        # Sophisticated routing decision
        if complexity == "expert" or reasoning == "analysis":
            routing_decision = "comprehensive"
        elif domain == "articles" or reasoning == "multi_hop":
            routing_decision = "graph_vector"
        elif recency in ["recent", "current"]:
            routing_decision = "web_only"
        elif intent == "factual" and complexity == "simple":
            routing_decision = "simple"
        elif domain == "case_law" or intent == "comparative":
            routing_decision = "graph_only"
        else:
            routing_decision = "vector_only"
        
        # Enhanced reasoning for routing
        routing_reasoning = self._generate_routing_reasoning(analysis, routing_decision)
        
        state["routing_decision"] = routing_decision
        state["adaptive_routing"] = {
            "decision": routing_decision,
            "reasoning": routing_reasoning,
            "analysis_basis": analysis
        }
        state["active_agent"] = "adaptive_router"
        state["agent_history"].append("adaptive_router")
        
        # Add routing message
        routing_msg = AIMessage(
            content=f"Routing to {routing_decision} strategy: {routing_reasoning}",
            name="adaptive_router"
        )
        state["messages"] = state.get("messages", []) + [routing_msg]
        
        logging.info(f"🧭 Adaptive Routing: {routing_decision} ({routing_reasoning})")
        
        return state
    
    def _generate_routing_reasoning(self, analysis: Dict, decision: str) -> str:
        """Generate reasoning for routing decision"""
        complexity = analysis.get("complexity", "moderate")
        intent = analysis.get("intent", "factual") 
        domain = analysis.get("domain", "general")
        reasoning = analysis.get("reasoning", "direct")
        
        reasoning_map = {
            "comprehensive": f"Complex {intent} query requiring multi-source analysis",
            "graph_vector": f"Domain-specific query needing relationship exploration", 
            "web_only": f"Current information needed for {domain} topics",
            "simple": f"Simple {intent} query with direct answer available",
            "graph_only": f"Relationship-heavy query in {domain} domain",
            "vector_only": f"Standard {complexity} query suitable for document search"
        }
        
        return reasoning_map.get(decision, "Standard routing applied")
    
    def _enhanced_graph_specialist(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced graph specialist with QA chains and openCypher"""
        
        query = state.get("query", "")
        analysis = state.get("query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Determine graph search strategy based on analysis
            if analysis.get("reasoning") == "multi_hop":
                search_type = "multi_hop"
            elif analysis.get("domain") == "articles":
                search_type = "qa_chain"
            elif "relationship" in query.lower() or "connected" in query.lower():
                search_type = "pattern_matching"
            else:
                search_type = "adaptive"
            
            # Execute enhanced graph search
            results = enhanced_graph_search_tool.invoke({
                "query": query,
                "search_type": search_type
            })
            
            state["graph_results"] = results.get("results", [])
            state["active_agent"] = "graph_specialist"
            state["agent_history"].append("graph_specialist")
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            if "search_latency" not in state:
                state["search_latency"] = {}
            state["search_latency"]["graph"] = end_time - start_time
            
            # Add specialist message
            specialist_msg = AIMessage(
                content=f"Graph search completed: {len(state['graph_results'])} results using {search_type} strategy",
                name="graph_specialist"
            )
            state["messages"] = state.get("messages", []) + [specialist_msg]
            
            logging.info(f"📊 Graph Specialist: {len(state['graph_results'])} results ({search_type})")
            
        except Exception as e:
            logging.error(f"❌ Graph specialist error: {e}")
            state["graph_results"] = []
        
        return state
    
    def _enhanced_vector_specialist(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced vector specialist with agentic search"""
        
        query = state.get("query", "")
        analysis = state.get("query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Determine search strategy based on analysis
            if analysis.get("complexity") == "expert":
                search_strategy = "multi_query"
            elif analysis.get("intent") == "comparative":
                search_strategy = "contextual_compression"
            else:
                search_strategy = "hybrid"
            
            # Execute agentic vector search
            results = agentic_vector_search_tool.invoke({
                "query": query,
                "search_strategy": search_strategy
            })
            
            state["vector_results"] = results.get("results", [])
            state["active_agent"] = "vector_specialist"
            state["agent_history"].append("vector_specialist")
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            if "search_latency" not in state:
                state["search_latency"] = {}
            state["search_latency"]["vector"] = end_time - start_time
            
            # Add specialist message
            specialist_msg = AIMessage(
                content=f"Vector search completed: {len(state['vector_results'])} results using {search_strategy} strategy",
                name="vector_specialist"
            )
            state["messages"] = state.get("messages", []) + [specialist_msg]
            
            logging.info(f"🔍 Vector Specialist: {len(state['vector_results'])} results ({search_strategy})")
            
        except Exception as e:
            logging.error(f"❌ Vector specialist error: {e}")
            state["vector_results"] = []
        
        return state
    
    def _enhanced_web_researcher(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced web researcher with authority focus"""
        
        query = state.get("query", "")
        analysis = state.get("query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Execute web search
            results = web_search_tool.invoke({"query": query})
            
            state["web_results"] = results.get("results", [])
            state["active_agent"] = "web_researcher"
            state["agent_history"].append("web_researcher")
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            if "search_latency" not in state:
                state["search_latency"] = {}
            state["search_latency"]["web"] = end_time - start_time
            
            # Add researcher message
            researcher_msg = AIMessage(
                content=f"Web research completed: {len(state['web_results'])} results from {len(results.get('sources', []))} sources",
                name="web_researcher"
            )
            state["messages"] = state.get("messages", []) + [researcher_msg]
            
            logging.info(f"🌐 Web Researcher: {len(state['web_results'])} results")
            
        except Exception as e:
            logging.error(f"❌ Web researcher error: {e}")
            state["web_results"] = []
        
        return state
    
    def _corrective_rag_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Corrective RAG agent with quality evaluation"""
        
        query = state.get("query", "")
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Execute corrective retrieval
            results = corrective_retrieval_tool.invoke({"query": query})
            
            state["corrective_results"] = results
            state["active_agent"] = "corrective_agent"
            state["agent_history"].append("corrective_agent")
            
            # Record evaluation
            if "retrieval_evaluations" not in state:
                state["retrieval_evaluations"] = []
            
            if results.get("success"):
                state["retrieval_evaluations"].append({
                    "agent": "corrective_rag",
                    "initial_evaluation": results.get("initial_evaluation", {}),
                    "final_evaluation": results.get("final_evaluation", {}),
                    "correction_steps": results.get("correction_steps", [])
                })
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            if "search_latency" not in state:
                state["search_latency"] = {}
            state["search_latency"]["corrective"] = end_time - start_time
            
            # Add corrective message
            corrective_msg = AIMessage(
                content=f"Corrective RAG completed: {len(results.get('correction_steps', []))} correction steps applied",
                name="corrective_agent"
            )
            state["messages"] = state.get("messages", []) + [corrective_msg]
            
            logging.info(f"🔧 Corrective RAG: {len(results.get('correction_steps', []))} corrections")
            
        except Exception as e:
            logging.error(f"❌ Corrective RAG error: {e}")
            state["corrective_results"] = {"success": False, "error": str(e)}
        
        return state
    
    def _self_reflection_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Self-reflection agent with critique capabilities"""
        
        query = state.get("query", "")
        
        # Collect all available documents for reflection
        all_documents = []
        
        # From vector results
        for result in state.get("vector_results", []):
            content = str(result.get("data", {}).get("content", ""))
            if content:
                all_documents.append(Document(page_content=content))
        
        # From corrective results
        corrective_results = state.get("corrective_results", {})
        if corrective_results.get("success"):
            for result in corrective_results.get("results", []):
                content = result.get("data", "")
                if content:
                    all_documents.append(Document(page_content=content))
        
        # From graph results
        for result in state.get("graph_results", []):
            content = str(result.get("data", ""))
            if content:
                all_documents.append(Document(page_content=content))
        
        try:
            if self_reflective_rag and all_documents:
                # Execute self-reflective generation
                conversation_context = self._build_conversation_context(state)
                
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            asyncio.run, 
                            self_reflective_rag.generate_with_reflection(query, all_documents, conversation_context)
                        )
                        reflection_result = future.result()
                else:
                    reflection_result = asyncio.run(
                        self_reflective_rag.generate_with_reflection(query, all_documents, conversation_context)
                    )
                
                state["self_rag_results"] = reflection_result
                state["reflection_steps"] = reflection_result.get("reflection_steps", [])
                
                # Record confidence
                if "confidence_scores" not in state:
                    state["confidence_scores"] = {}
                state["confidence_scores"]["self_reflection"] = reflection_result.get("confidence", 0.7)
                
            else:
                state["self_rag_results"] = {"response": "Self-reflection not available", "confidence": 0.5}
                state["reflection_steps"] = ["Self-reflection skipped - no documents available"]
            
            state["active_agent"] = "reflection_agent"
            state["agent_history"].append("reflection_agent")
            
            # Add reflection message
            reflection_msg = AIMessage(
                content=f"Self-reflection completed: {len(state.get('reflection_steps', []))} reflection steps",
                name="reflection_agent"
            )
            state["messages"] = state.get("messages", []) + [reflection_msg]
            
            logging.info(f"🤔 Self-Reflection: {len(state.get('reflection_steps', []))} steps")
            
        except Exception as e:
            logging.error(f"❌ Self-reflection error: {e}")
            state["self_rag_results"] = {"response": f"Reflection failed: {str(e)}", "confidence": 0.3}
            state["reflection_steps"] = [f"Reflection error: {str(e)}"]
        
        return state
    
    def _synthesis_coordinator(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced synthesis coordinator with multi-source integration"""
        
        query = state.get("query", "")
        analysis = state.get("query_analysis", {})
        
        # Collect all available information
        synthesis_context = self._build_synthesis_context(state)
        
        synthesis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced GDPR synthesis coordinator using reasoning capabilities.

Synthesize information from multiple sources to create a comprehensive, accurate response.

Sources available:
- Graph knowledge (relationships, concepts, hierarchies)
- Vector documents (content, articles, guidance)
- Web information (current developments, cases)
- Corrective retrieval (quality-assessed information)
- Self-reflection analysis (critique and validation)

Requirements:
1. Integrate information from all available sources
2. Prioritize authoritative GDPR sources
3. Highlight consensus and flag disagreements
4. Provide practical, actionable guidance
5. Cite specific GDPR articles when relevant
6. Acknowledge uncertainty and limitations
7. Use clear, professional language
8. Structure response logically

Use chain-of-thought reasoning to build your synthesis."""),
            ("human", f"""Query: {query}

Query Analysis: {analysis}

Available Information:
{synthesis_context}

Provide a comprehensive synthesized response:""")
        ])
        
        try:
            response = self.llm.invoke(synthesis_prompt.format_messages())
            synthesized_response = response.content.strip()
            
            state["synthesized_response"] = synthesized_response
            state["active_agent"] = "synthesis_coordinator"
            state["agent_history"].append("synthesis_coordinator")
            
            # Calculate document count
            total_docs = (
                len(state.get("graph_results", [])) +
                len(state.get("vector_results", [])) +
                len(state.get("web_results", [])) +
                len(state.get("corrective_results", {}).get("results", []))
            )
            state["total_documents"] = total_docs
            
            # Add synthesis message
            synthesis_msg = AIMessage(
                content=f"Synthesis completed: Integrated information from {total_docs} sources",
                name="synthesis_coordinator"
            )
            state["messages"] = state.get("messages", []) + [synthesis_msg]
            
            logging.info(f"🔄 Synthesis: Integrated {total_docs} sources")
            
        except Exception as e:
            logging.error(f"❌ Synthesis error: {e}")
            state["synthesized_response"] = "I apologize, but I encountered an error while synthesizing the response. Please try rephrasing your question."
        
        return state
    
    def _quality_evaluator(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced quality evaluator with comprehensive assessment"""
        
        query = state.get("query", "")
        response = state.get("synthesized_response", "")
        analysis = state.get("query_analysis", {})
        
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced quality evaluator for GDPR responses using reasoning capabilities.

Evaluate the response across multiple dimensions:

1. Accuracy: Factual correctness and GDPR compliance
2. Completeness: Full coverage of the query requirements
3. Clarity: Clear, understandable language and structure
4. Authority: Proper citations and authoritative sources
5. Recency: Current and up-to-date information when needed
6. Practicality: Actionable guidance and real-world applicability
7. Consistency: Internal consistency and logical flow

Based on query analysis: {analysis}

Decision options:
- "complete": Response meets all quality standards
- "optimize": Good response that can be improved
- "retry_graph": Need better relationship/concept information
- "retry_vector": Need more document content
- "retry_web": Need more current information

Use chain-of-thought reasoning for evaluation.

Respond in JSON:
{
    "decision": "complete|optimize|retry_graph|retry_vector|retry_web",
    "overall_confidence": float,
    "dimension_scores": {
        "accuracy": float,
        "completeness": float, 
        "clarity": float,
        "authority": float,
        "recency": float,
        "practicality": float,
        "consistency": float
    },
    "strengths": ["list of strengths"],
    "weaknesses": ["list of weaknesses"], 
    "improvement_suggestions": ["specific suggestions"],
    "reasoning_chain": "step by step evaluation"
}"""),
            ("human", f"""Query: {query}

Response to evaluate:
{response}

Sources used:
- Graph results: {len(state.get('graph_results', []))}
- Vector results: {len(state.get('vector_results', []))}
- Web results: {len(state.get('web_results', []))}
- Corrective steps: {len(state.get('corrective_results', {}).get('correction_steps', []))}
- Reflection steps: {len(state.get('reflection_steps', []))}

Provide comprehensive evaluation:""")
        ])
        
        try:
            response_eval = self.llm.invoke(evaluation_prompt.format_messages())
            evaluation = json.loads(response_eval.content.strip())
            
            # Validate evaluation structure
            required_keys = ["decision", "overall_confidence", "dimension_scores", "strengths", "weaknesses", "improvement_suggestions", "reasoning_chain"]
            for key in required_keys:
                if key not in evaluation:
                    if key == "decision":
                        evaluation[key] = "complete"
                    elif key == "overall_confidence":
                        evaluation[key] = 0.7
                    elif key == "dimension_scores":
                        evaluation[key] = {}
                    else:
                        evaluation[key] = []
            
            state["final_confidence"] = evaluation["overall_confidence"]
            
            # Record comprehensive evaluation
            if "retrieval_evaluations" not in state:
                state["retrieval_evaluations"] = []
            
            state["retrieval_evaluations"].append({
                "agent": "quality_evaluator",
                "evaluation": evaluation,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })
            
            # Determine if feedback is required
            if evaluation["overall_confidence"] < self.config.CONFIDENCE_THRESHOLD:
                state["requires_feedback"] = True
            
            state["active_agent"] = "quality_evaluator"
            state["agent_history"].append("quality_evaluator")
            
            # Add evaluation message
            eval_msg = AIMessage(
                content=f"Quality evaluation: {evaluation['decision']} (confidence: {evaluation['overall_confidence']:.2f})",
                name="quality_evaluator"
            )
            state["messages"] = state.get("messages", []) + [eval_msg]
            
            logging.info(f"⚖️ Quality Evaluation: {evaluation['decision']} (confidence: {evaluation['overall_confidence']:.2f})")
            
        except Exception as e:
            logging.error(f"❌ Quality evaluation error: {e}")
            evaluation = {
                "decision": "complete",
                "overall_confidence": 0.7,
                "reasoning_chain": f"Evaluation failed: {str(e)}"
            }
            state["final_confidence"] = 0.7
        
        return state
    
    def _response_optimizer(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Response optimizer for final enhancement"""
        
        query = state.get("query", "")
        current_response = state.get("synthesized_response", "")
        evaluation = state.get("retrieval_evaluations", [])[-1] if state.get("retrieval_evaluations") else {}
        
        # Get improvement suggestions from evaluation
        suggestions = evaluation.get("evaluation", {}).get("improvement_suggestions", [])
        weaknesses = evaluation.get("evaluation", {}).get("weaknesses", [])
        
        optimization_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a response optimizer for GDPR compliance responses.

Enhance the response based on evaluation feedback while maintaining accuracy.

Focus areas:
1. Address identified weaknesses
2. Implement improvement suggestions
3. Enhance clarity and structure
4. Strengthen authoritative citations
5. Improve practical applicability
6. Ensure GDPR compliance accuracy

Maintain the core information while optimizing presentation and completeness."""),
            ("human", f"""Query: {query}

Current Response:
{current_response}

Identified Weaknesses:
{weaknesses}

Improvement Suggestions:
{suggestions}

Optimized Response:""")
        ])
        
        try:
            response = self.llm.invoke(optimization_prompt.format_messages())
            optimized_response = response.content.strip()
            
            state["synthesized_response"] = optimized_response
            state["active_agent"] = "response_optimizer"
            state["agent_history"].append("response_optimizer")
            
            # Boost confidence slightly for optimization
            if state.get("final_confidence"):
                state["final_confidence"] = min(state["final_confidence"] + 0.1, 1.0)
            
            # Add optimizer message
            optimizer_msg = AIMessage(
                content=f"Response optimized: Enhanced based on {len(suggestions)} suggestions",
                name="response_optimizer"
            )
            state["messages"] = state.get("messages", []) + [optimizer_msg]
            
            logging.info(f"✨ Response Optimizer: Enhanced with {len(suggestions)} improvements")
            
        except Exception as e:
            logging.error(f"❌ Response optimization error: {e}")
            # Keep original response if optimization fails
            pass
        
        return state
    
    def _build_conversation_context(self, state: AdvancedSearchState) -> str:
        """Build conversation context for self-reflection"""
        messages = state.get("messages", [])
        if len(messages) <= 1:
            return ""
        
        context_parts = []
        for msg in messages[-5:]:  # Last 5 messages
            if hasattr(msg, 'content') and hasattr(msg, 'name'):
                context_parts.append(f"{msg.name}: {msg.content}")
            elif hasattr(msg, 'content'):
                context_parts.append(f"User: {msg.content}")
        
        return "\n".join(context_parts)
    
    def _build_synthesis_context(self, state: AdvancedSearchState) -> str:
        """Build comprehensive synthesis context"""
        context_parts = []
        
        # Graph results
        graph_results = state.get("graph_results", [])
        if graph_results:
            context_parts.append("**Graph Knowledge:**")
            for i, result in enumerate(graph_results[:3]):
                result_type = result.get('type', 'concept')
                data = result.get('data', {})
                context_parts.append(f"- {result_type}: {str(data)[:200]}...")
        
        # Vector results
        vector_results = state.get("vector_results", [])
        if vector_results:
            context_parts.append("\n**Document Content:**")
            for i, result in enumerate(vector_results[:3]):
                content = result.get('data', {}).get('content', '')
                context_parts.append(f"- {content[:200]}...")
        
        # Web results
        web_results = state.get("web_results", [])
        if web_results:
            context_parts.append("\n**Current Information:**")
            for i, result in enumerate(web_results[:3]):
                title = result.get('title', 'Web Result')
                content = result.get('content', '')
                context_parts.append(f"- {title}: {content[:200]}...")
        
        # Corrective results
        corrective_results = state.get("corrective_results", {})
        if corrective_results.get("success"):
            context_parts.append("\n**Quality-Assessed Information:**")
            correction_steps = corrective_results.get("correction_steps", [])
            context_parts.append(f"- Applied {len(correction_steps)} quality corrections")
            for result in corrective_results.get("results", [])[:2]:
                content = result.get("data", "")
                context_parts.append(f"- {content[:200]}...")
        
        # Self-reflection results
        self_rag = state.get("self_rag_results", {})
        if self_rag.get("response"):
            context_parts.append("\n**Self-Reflection Analysis:**")
            context_parts.append(f"- {self_rag['response'][:300]}...")
            if self_rag.get("critique"):
                critique = self_rag["critique"]
                if critique.get("strengths"):
                    context_parts.append(f"- Strengths: {', '.join(critique['strengths'][:3])}")
                if critique.get("issues"):
                    context_parts.append(f"- Issues addressed: {', '.join(critique['issues'][:3])}")
        
        return "\n".join(context_parts)
    
    def _route_to_specialists(self, state: AdvancedSearchState) -> str:
        """Enhanced routing logic based on adaptive analysis"""
        routing_decision = state.get("routing_decision", "vector_only")
        iteration_count = state.get("iteration_count", 0)
        
        # Prevent infinite loops
        if iteration_count >= self.config.MAX_AGENT_ITERATIONS:
            return "simple"
        
        return routing_decision
    
    def _evaluate_quality(self, state: AdvancedSearchState) -> str:
        """Enhanced quality evaluation routing"""
        evaluations = state.get("retrieval_evaluations", [])
        if not evaluations:
            return "complete"
        
        latest_eval = evaluations[-1]
        evaluation = latest_eval.get("evaluation", {})
        decision = evaluation.get("decision", "complete")
        iteration_count = state.get("iteration_count", 0)
        
        # Prevent infinite loops
        if iteration_count >= self.config.MAX_AGENT_ITERATIONS:
            return "complete"
        
        # Increment iteration count for retry decisions
        if decision.startswith("retry_"):
            state["iteration_count"] = iteration_count + 1
        
        return decision

# ============================================================================
# PERFORMANCE MONITORING AND METRICS
# ============================================================================

class PerformanceMetrics:
    """Advanced performance monitoring and metrics collection"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.metrics_storage = config.METRICS_PATH
        self.session_metrics = {}
    
    async def record_search_metrics(self, session_id: str, query: str, 
                                  state: AdvancedSearchState, response_time: float):
        """Record comprehensive search metrics"""
        try:
            metrics = {
                "session_id": session_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query": query,
                "response_time": response_time,
                "total_documents": state.get("total_documents", 0),
                "final_confidence": state.get("final_confidence", 0.0),
                "agent_path": state.get("agent_history", []),
                "iteration_count": state.get("iteration_count", 0),
                "search_latencies": state.get("search_latency", {}),
                "retrieval_evaluations": len(state.get("retrieval_evaluations", [])),
                "query_analysis": state.get("query_analysis", {}),
                "routing_decision": state.get("routing_decision", ""),
                "requires_feedback": state.get("requires_feedback", False)
            }
            
            # Store in session metrics
            if session_id not in self.session_metrics:
                self.session_metrics[session_id] = []
            self.session_metrics[session_id].append(metrics)
            
            # Save to file
            await self._save_metrics(metrics)
            
        except Exception as e:
            logging.error(f"❌ Metrics recording error: {e}")
    
    async def _save_metrics(self, metrics: Dict):
        """Save metrics to storage"""
        try:
            metrics_file = self.metrics_storage / f"metrics_{datetime.now().strftime('%Y%m%d')}.jsonl"
            
            async with aiofiles.open(metrics_file, 'a', encoding='utf-8') as f:
                await f.write(json.dumps(metrics) + '\n')
                
        except Exception as e:
            logging.error(f"❌ Metrics save error: {e}")
    
    async def get_performance_summary(self, session_id: str = None) -> Dict[str, Any]:
        """Get performance summary for analysis"""
        try:
            if session_id and session_id in self.session_metrics:
                metrics_list = self.session_metrics[session_id]
            else:
                # Load all metrics for today
                metrics_list = await self._load_daily_metrics()
            
            if not metrics_list:
                return {"message": "No metrics available"}
            
            # Calculate summary statistics
            response_times = [m.get("response_time", 0) for m in metrics_list]
            confidences = [m.get("final_confidence", 0) for m in metrics_list]
            document_counts = [m.get("total_documents", 0) for m in metrics_list]
            
            summary = {
                "total_queries": len(metrics_list),
                "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
                "avg_confidence": sum(confidences) / len(confidences) if confidences else 0,
                "avg_documents": sum(document_counts) / len(document_counts) if document_counts else 0,
                "feedback_rate": sum(1 for m in metrics_list if m.get("requires_feedback")) / len(metrics_list) * 100,
                "agent_usage": self._analyze_agent_usage(metrics_list),
                "routing_patterns": self._analyze_routing_patterns(metrics_list)
            }
            
            return summary
            
        except Exception as e:
            logging.error(f"❌ Performance summary error: {e}")
            return {"error": str(e)}
    
    async def _load_daily_metrics(self) -> List[Dict]:
        """Load metrics for current day"""
        try:
            metrics_file = self.metrics_storage / f"metrics_{datetime.now().strftime('%Y%m%d')}.jsonl"
            
            if not metrics_file.exists():
                return []
            
            metrics_list = []
            async with aiofiles.open(metrics_file, 'r', encoding='utf-8') as f:
                async for line in f:
                    if line.strip():
                        metrics_list.append(json.loads(line.strip()))
            
            return metrics_list
            
        except Exception as e:
            logging.error(f"❌ Daily metrics load error: {e}")
            return []
    
    def _analyze_agent_usage(self, metrics_list: List[Dict]) -> Dict[str, int]:
        """Analyze agent usage patterns"""
        agent_counts = {}
        
        for metrics in metrics_list:
            for agent in metrics.get("agent_path", []):
                agent_counts[agent] = agent_counts.get(agent, 0) + 1
        
        return agent_counts
    
    def _analyze_routing_patterns(self, metrics_list: List[Dict]) -> Dict[str, int]:
        """Analyze routing decision patterns"""
        routing_counts = {}
        
        for metrics in metrics_list:
            routing = metrics.get("routing_decision", "unknown")
            routing_counts[routing] = routing_counts.get(routing, 0) + 1
        
        return routing_counts

# ============================================================================
# ENHANCED CONVERSATION MANAGEMENT WITH ADVANCED MEMORY
# ============================================================================

class AdvancedConversationManager:
    """Enhanced conversation manager with advanced memory and context handling"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.session_storage = config.SESSION_STORAGE_PATH
        self.current_session = None
        self.conversation_embeddings = None
        
        # Initialize conversation embeddings for semantic memory
        if config.ENABLE_AGENT_MEMORY:
            try:
                self.conversation_embeddings = CustomOpenAIEmbeddings(
                    api_key=config.OPENAI_API_KEY,
                    model=config.EMBEDDING_MODEL,
                    dimensions=config.EMBEDDING_DIMENSIONS
                )
            except Exception as e:
                logging.warning(f"Conversation embeddings initialization failed: {e}")
    
    async def start_enhanced_session(self, user_id: str = None, session_context: Dict = None) -> str:
        """Start enhanced conversation session with context"""
        session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        session_data = {
            "session_id": session_id,
            "user_id": user_id or "anonymous",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "session_context": session_context or {},
            "conversation_history": [],
            "search_history": [],
            "feedback_history": [],
            "user_preferences": {},
            "conversation_summary": "",
            "semantic_memory": [],
            "performance_metrics": {
                "total_queries": 0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "satisfaction_score": 0.0
            }
        }
        
        await self._save_session(session_data)
        self.current_session = session_data
        
        logging.info(f"🆕 Enhanced session started: {session_id}")
        return session_id
    
    async def load_session(self, session_id: str) -> bool:
        """Load existing conversation session"""
        try:
            session_file = self.session_storage / f"{session_id}.json"
            
            if not session_file.exists():
                return False
            
            async with aiofiles.open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.loads(await f.read())
            
            self.current_session = session_data
            logging.info(f"📂 Loaded session: {session_id}")
            return True
            
        except Exception as e:
            logging.error(f"❌ Error loading session {session_id}: {e}")
            return False
    
    async def list_sessions(self, console: Console, limit: int = 10):
        """List recent sessions with enhanced information"""
        try:
            session_files = sorted(
                self.session_storage.glob("session_*.json"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )[:limit]
            
            if not session_files:
                console.print("📂 [yellow]No sessions found.[/yellow]")
                return
            
            console.print(f"\n📂 [bold blue]Recent Sessions (last {limit}):[/bold blue]")
            
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("Session ID", style="cyan")
            table.add_column("Start Time", style="green")
            table.add_column("Queries", style="yellow")
            table.add_column("Avg Confidence", style="blue")
            table.add_column("User", style="white")
            
            for file in session_files:
                try:
                    async with aiofiles.open(file, 'r', encoding='utf-8') as f:
                        data = json.loads(await f.read())
                    
                    session_id = data.get("session_id", "Unknown")
                    start_time = data.get("start_time", "Unknown")
                    queries = data.get("performance_metrics", {}).get("total_queries", 0)
                    avg_confidence = data.get("performance_metrics", {}).get("avg_confidence", 0.0)
                    user_id = data.get("user_id", "anonymous")
                    
                    # Format timestamp
                    try:
                        dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M")
                    except:
                        formatted_time = start_time
                    
                    table.add_row(
                        session_id, 
                        formatted_time, 
                        str(queries), 
                        f"{avg_confidence:.2f}",
                        user_id
                    )
                    
                except Exception as e:
                    logging.error(f"❌ Error reading session file {file}: {e}")
            
            console.print(table)
            
        except Exception as e:
            console.print(f"❌ [red]Error listing sessions: {e}[/red]")
    
    async def add_enhanced_interaction(self, query: str, response: str, 
                                     search_state: AdvancedSearchState, 
                                     response_time: float):
        """Add enhanced interaction with semantic memory"""
        if not self.current_session:
            return
        
        interaction = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "query": query,
            "response": response,
            "response_time": response_time,
            "confidence": search_state.get("final_confidence", 0.0),
            "agent_path": search_state.get("agent_history", []),
            "total_documents": search_state.get("total_documents", 0),
            "query_analysis": search_state.get("query_analysis", {}),
            "retrieval_evaluations": search_state.get("retrieval_evaluations", []),
            "requires_feedback": search_state.get("requires_feedback", False)
        }
        
        # Add to conversation history
        self.current_session["conversation_history"].append(interaction)
        
        # Update performance metrics
        await self._update_session_metrics(interaction)
        
        # Add to semantic memory if enabled
        if self.conversation_embeddings and self.config.ENABLE_AGENT_MEMORY:
            await self._add_to_semantic_memory(query, response, interaction)
        
        # Keep conversation history manageable
        if len(self.current_session["conversation_history"]) > self.config.MAX_CONVERSATION_HISTORY:
            await self._compress_conversation_history()
        
        await self._save_session(self.current_session)
    
    async def _update_session_metrics(self, interaction: Dict):
        """Update session performance metrics"""
        metrics = self.current_session["performance_metrics"]
        current_count = metrics["total_queries"]
        
        # Update averages incrementally
        metrics["total_queries"] = current_count + 1
        metrics["avg_response_time"] = (
            (metrics["avg_response_time"] * current_count + interaction["response_time"]) / 
            metrics["total_queries"]
        )
        metrics["avg_confidence"] = (
            (metrics["avg_confidence"] * current_count + interaction["confidence"]) / 
            metrics["total_queries"]
        )
    
    async def _add_to_semantic_memory(self, query: str, response: str, interaction: Dict):
        """Add interaction to semantic memory for future retrieval"""
        try:
            # Create embedding for the interaction
            interaction_text = f"Query: {query}\nResponse: {response[:500]}"
            
            embedding = await self.conversation_embeddings.embed_query(interaction_text)
            
            memory_entry = {
                "timestamp": interaction["timestamp"],
                "query": query,
                "response_summary": response[:200] + "..." if len(response) > 200 else response,
                "embedding": embedding,
                "confidence": interaction["confidence"],
                "topics": interaction.get("query_analysis", {}).get("key_concepts", [])
            }
            
            self.current_session["semantic_memory"].append(memory_entry)
            
            # Keep semantic memory manageable (last 50 interactions)
            if len(self.current_session["semantic_memory"]) > 50:
                self.current_session["semantic_memory"] = self.current_session["semantic_memory"][-50:]
                
        except Exception as e:
            logging.warning(f"Semantic memory addition failed: {e}")
    
    async def get_relevant_conversation_context(self, current_query: str, max_context: int = 3) -> str:
        """Get relevant conversation context using semantic similarity"""
        if not self.current_session or not self.config.ENABLE_AGENT_MEMORY:
            return self._get_basic_conversation_context()
        
        try:
            semantic_memory = self.current_session.get("semantic_memory", [])
            if not semantic_memory or not self.conversation_embeddings:
                return self._get_basic_conversation_context()
            
            # Get embedding for current query
            query_embedding = await self.conversation_embeddings.embed_query(current_query)
            
            # Calculate similarities
            similarities = []
            for i, memory in enumerate(semantic_memory):
                if "embedding" in memory:
                    # Simple cosine similarity calculation
                    dot_product = sum(a * b for a, b in zip(query_embedding, memory["embedding"]))
                    norm_a = sum(a * a for a in query_embedding) ** 0.5
                    norm_b = sum(b * b for b in memory["embedding"]) ** 0.5
                    similarity = dot_product / (norm_a * norm_b) if norm_a * norm_b > 0 else 0.0
                    similarities.append((i, similarity, memory))
            
            # Sort by similarity and get top contexts
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            context_parts = []
            for i, (_, similarity, memory) in enumerate(similarities[:max_context]):
                if similarity > 0.3:  # Threshold for relevance
                    context_parts.append(
                        f"Previous Q: {memory['query']}\n"
                        f"Previous A: {memory['response_summary']}"
                    )
            
            return "\n\n".join(context_parts) if context_parts else self._get_basic_conversation_context()
            
        except Exception as e:
            logging.warning(f"Semantic context retrieval failed: {e}")
            return self._get_basic_conversation_context()
    
    def _get_basic_conversation_context(self) -> str:
        """Get basic conversation context from recent history"""
        if not self.current_session:
            return ""
        
        history = self.current_session.get("conversation_history", [])
        if not history:
            return ""
        
        # Get last 3 interactions for basic context
        recent_history = history[-3:]
        context_parts = []
        
        for interaction in recent_history:
            context_parts.append(f"Q: {interaction['query']}")
            context_parts.append(f"A: {interaction['response'][:200]}...")
        
        return "\n".join(context_parts)
    
    async def _compress_conversation_history(self):
        """Compress old conversation history using summarization"""
        try:
            history = self.current_session["conversation_history"]
            if len(history) <= self.config.MAX_CONVERSATION_HISTORY:
                return
            
            # Keep recent interactions, summarize older ones
            recent_count = self.config.MAX_CONVERSATION_HISTORY // 2
            recent_history = history[-recent_count:]
            old_history = history[:-recent_count]
            
            # Create summary of old interactions
            old_interactions_text = "\n\n".join([
                f"Q: {interaction['query']}\nA: {interaction['response'][:300]}"
                for interaction in old_history[-10:]  # Summarize last 10 of the old ones
            ])
            
            summary_prompt = ChatPromptTemplate.from_messages([
                ("system", "Summarize the key topics, questions, and insights from this GDPR conversation history. Keep important details and context."),
                ("human", f"Conversation history to summarize:\n\n{old_interactions_text}")
            ])
            
            llm = ChatOpenAI(model=self.config.REASONING_MODEL)
            response = await llm.ainvoke(summary_prompt.format_messages())
            
            # Update session with compressed history
            self.current_session["conversation_summary"] = response.content.strip()
            self.current_session["conversation_history"] = recent_history
            
            logging.info(f"📝 Compressed conversation history: {len(old_history)} → summary + {len(recent_history)} recent")
            
        except Exception as e:
            logging.error(f"❌ Conversation compression error: {e}")
    
    async def _save_session(self, session_data: Dict):
        """Enhanced session saving with compression"""
        try:
            session_file = self.session_storage / f"{session_data['session_id']}.json"
            
            # Compress large sessions
            if self.config.ENABLE_CACHING:
                session_size = len(json.dumps(session_data))
                if session_size > 100000:  # 100KB threshold
                    await self._compress_session_data(session_data)
            
            async with aiofiles.open(session_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(session_data, indent=2, ensure_ascii=False, default=str))
                
        except Exception as e:
            logging.error(f"❌ Enhanced session save error: {e}")
    
    async def _compress_session_data(self, session_data: Dict):
        """Compress session data to reduce storage"""
        # Remove embeddings from semantic memory for storage
        if "semantic_memory" in session_data:
            for memory in session_data["semantic_memory"]:
                if "embedding" in memory:
                    del memory["embedding"]
        
        # Truncate very long responses in history
        for interaction in session_data.get("conversation_history", []):
            if len(interaction.get("response", "")) > 2000:
                interaction["response"] = interaction["response"][:2000] + "... [truncated for storage]"

# ============================================================================
# MAIN ENHANCED SEARCH ENGINE APPLICATION
# ============================================================================

class EnhancedGDPRSearchEngineCLI:
    """Next-generation GDPR search engine with advanced capabilities"""
    
    def __init__(self):
        self.config = AdvancedSearchEngineConfig()
        self.console = Console()
        
        # Core components
        self.falkor_manager = None
        self.es_manager = None
        self.web_search_instance = None
        self.search_agents = None
        
        # Advanced components
        self.conversation_manager = None
        self.performance_metrics = None
        
        # Advanced RAG components
        self.corrective_rag = None
        self.self_reflective_rag = None
        self.adaptive_rag = None
    
    async def initialize(self):
        """Initialize all enhanced components"""
        try:
            self.console.print("🚀 [bold blue]Initializing Next-Generation GDPR Search Engine...[/bold blue]")
            
            # Validate configuration
            self.config.validate()
            self.console.print("✅ Enhanced configuration validated")
            
            # Initialize components with progress tracking
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:
                
                # Database connections
                task1 = progress.add_task("🗄️ Connecting to Enhanced FalkorDB...", total=1)
                self.falkor_manager = AdvancedFalkorDBManager(self.config)
                progress.update(task1, completed=1)
                
                task2 = progress.add_task("🔍 Connecting to Agentic Elasticsearch...", total=1)
                self.es_manager = AdvancedElasticsearchManager(self.config)
                progress.update(task2, completed=1)
                
                # Enhanced systems
                task3 = progress.add_task("🤖 Initializing Advanced Multi-Agent System...", total=1)
                self.search_agents = EnhancedGDPRSearchAgents(self.config)
                progress.update(task3, completed=1)
                
                task4 = progress.add_task("💬 Initializing Enhanced Conversation Manager...", total=1)
                self.conversation_manager = AdvancedConversationManager(self.config)
                progress.update(task4, completed=1)
                
                task5 = progress.add_task("📈 Setting up Performance Monitoring...", total=1)
                self.performance_metrics = PerformanceMetrics(self.config)
                progress.update(task5, completed=1)
                
                # Web search (if enabled)
                if self.config.WEB_SEARCH_ENABLED:
                    task6 = progress.add_task("🌐 Initializing Enterprise Web Search...", total=1)
                    self.web_search_instance = EnterpriseWebSearch(self.config)
                    progress.update(task6, completed=1)
            
            # Set global instances for tool injection
            global falkor_manager, es_manager, web_search_instance
            falkor_manager = self.falkor_manager
            es_manager = self.es_manager
            web_search_instance = self.web_search_instance
            
            # Initialize advanced RAG components
            self.search_agents.initialize_advanced_rag(
                self.falkor_manager, 
                self.es_manager, 
                self.web_search_instance
            )
            
            self.console.print("🎉 [bold green]Next-generation system initialization complete![/bold green]")
            self.console.print(f"🔧 [cyan]Enabled features: CRAG={self.config.ENABLE_CRAG}, Self-RAG={self.config.ENABLE_SELF_RAG}, Adaptive={self.config.ENABLE_ADAPTIVE_RAG}[/cyan]")
            
            return True
            
        except Exception as e:
            self.console.print(f"❌ [bold red]Initialization failed: {e}[/bold red]")
            logging.error(f"❌ Initialization error: {e}")
            traceback.print_exc()
            return False
    
    async def run_enhanced_interactive_mode(self):
        """Run enhanced interactive mode with advanced features"""
        
        self.console.print("\n" + "="*90)
        self.console.print("🔍 [bold blue]Next-Generation GDPR Search Engine - Enhanced Interactive Mode[/bold blue]")
        self.console.print("="*90)
        
        # Start enhanced session
        session_id = await self.conversation_manager.start_enhanced_session()
        self.console.print(f"📝 Enhanced session started: [cyan]{session_id}[/cyan]")
        
        # Display enhanced commands
        self._display_enhanced_commands()
        
        self.console.print("\n💬 [bold]Ask your GDPR questions or use advanced commands above.[/bold]")
        self.console.print("💡 [italic]Pro tip: Try complex queries - the system uses advanced reasoning and multiple sources![/italic]")
        
        while True:
            try:
                # Get user input with enhanced prompt
                query = Prompt.ask(
                    "\n🤔 [bold cyan]Your GDPR question[/bold cyan]",
                    default="",
                    show_default=False
                )
                
                if not query.strip():
                    continue
                
                # Handle enhanced commands
                if query.startswith("/"):
                    await self._handle_enhanced_command(query)
                    continue
                
                # Process enhanced search query
                await self._process_enhanced_search_query(query, session_id)
                
            except KeyboardInterrupt:
                self.console.print("\n👋 [yellow]Thank you for using the enhanced GDPR search engine![/yellow]")
                
                # Display session summary
                await self._display_session_summary(session_id)
                break
                
            except Exception as e:
                self.console.print(f"❌ [red]Error: {e}[/red]")
                logging.error(f"❌ Enhanced interactive mode error: {e}")
                traceback.print_exc()
    
    def _display_enhanced_commands(self):
        """Display enhanced command options"""
        commands_table = Table(show_header=True, header_style="bold magenta", title="Enhanced Commands")
        commands_table.add_column("Command", style="cyan", width=25)
        commands_table.add_column("Description", style="white", width=50)
        
        commands = [
            ("/help", "Show this enhanced help message"),
            ("/sessions", "List recent sessions with metrics"),
            ("/load <session_id>", "Load previous session with context"),
            ("/performance", "View system performance metrics"),
            ("/clear", "Start new session"),
            ("/debug", "Toggle debug mode"),
            ("/quit", "Exit the application")
        ]
        
        for cmd, desc in commands:
            commands_table.add_row(cmd, desc)
        
        self.console.print(commands_table)
    
    async def _handle_enhanced_command(self, command: str):
        """Handle enhanced CLI commands"""
        parts = command.split()
        cmd = parts[0].lower()
        
        if cmd == "/help":
            self._display_enhanced_commands()
            
        elif cmd == "/sessions":
            await self.conversation_manager.list_sessions(self.console)
            
        elif cmd == "/load" and len(parts) > 1:
            session_id = parts[1]
            if await self.conversation_manager.load_session(session_id):
                self.console.print(f"✅ [green]Loaded session: {session_id}[/green]")
            else:
                self.console.print(f"❌ [red]Session not found: {session_id}[/red]")
            
        elif cmd == "/performance":
            await self._show_performance_metrics()
            
        elif cmd == "/clear":
            session_id = await self.conversation_manager.start_enhanced_session()
            self.console.print(f"🆕 [green]New enhanced session started: {session_id}[/green]")
            
        elif cmd == "/debug":
            current_level = logging.getLogger().level
            new_level = logging.DEBUG if current_level != logging.DEBUG else logging.INFO
            logging.getLogger().setLevel(new_level)
            level_name = "DEBUG" if new_level == logging.DEBUG else "INFO"
            self.console.print(f"🐛 [yellow]Debug mode: {level_name}[/yellow]")
            
        elif cmd == "/quit":
            raise KeyboardInterrupt()
            
        else:
            self.console.print(f"❓ [yellow]Unknown command: {command}[/yellow]")
            self._display_enhanced_commands()
    
    async def _process_enhanced_search_query(self, query: str, session_id: str):
        """Process enhanced search query with advanced multi-agent system"""
        
        start_time = asyncio.get_event_loop().time()
        
        self.console.print(f"\n🔍 [bold]Processing with advanced AI: [cyan]{query}[/cyan][/bold]")
        
        # Get relevant conversation context
        conversation_context = await self.conversation_manager.get_relevant_conversation_context(query)
        
        # Create enhanced initial state
        initial_state = AdvancedSearchState(
            messages=[HumanMessage(content=query)],
            query=query,
            query_analysis=None,
            routing_decision=None,
            graph_results=None,
            vector_results=None,
            web_results=None,
            corrective_results=None,
            self_rag_results=None,
            adaptive_routing=None,
            retrieval_evaluations=[],
            confidence_scores={},
            reflection_steps=[],
            synthesized_response=None,
            final_confidence=None,
            requires_feedback=False,
            active_agent=None,
            agent_history=[],
            iteration_count=0,
            search_latency={},
            total_documents=0
        )
        
        try:
            # Process with enhanced multi-agent system
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:
                
                search_task = progress.add_task("🤖 Advanced multi-agent processing...", total=1)
                
                # Run the enhanced agent graph
                config = {"configurable": {"thread_id": session_id}}
                result_state = await self.search_agents.search_graph.ainvoke(
                    initial_state,
                    config=config
                )
                
                progress.update(search_task, completed=1)
            
            # Calculate total response time
            end_time = asyncio.get_event_loop().time()
            total_response_time = end_time - start_time
            
            # Display enhanced results
            await self._display_enhanced_search_results(result_state, query, session_id, total_response_time)
            
            # Record metrics and add to conversation
            await self.performance_metrics.record_search_metrics(
                session_id, query, result_state, total_response_time
            )
            
            await self.conversation_manager.add_enhanced_interaction(
                query, 
                result_state.get("synthesized_response", "No response generated"),
                result_state,
                total_response_time
            )
            
        except Exception as e:
            self.console.print(f"❌ [red]Enhanced search processing error: {e}[/red]")
            logging.error(f"❌ Enhanced search processing error: {e}")
            traceback.print_exc()
    
    async def _display_enhanced_search_results(self, result_state: AdvancedSearchState, 
                                            query: str, session_id: str, response_time: float):
        """Display comprehensive enhanced search results"""
        
        self.console.print("\n" + "="*90)
        self.console.print("📋 [bold blue]Enhanced Search Results[/bold blue]")
        self.console.print("="*90)
        
        # Performance metrics
        confidence = result_state.get("final_confidence", 0.0)
        agent_path = result_state.get("agent_history", [])
        total_docs = result_state.get("total_documents", 0)
        
        metrics_table = Table(title="Performance Metrics", show_header=True, header_style="bold cyan")
        metrics_table.add_column("Metric", style="cyan")
        metrics_table.add_column("Value", style="green")
        
        metrics_table.add_row("Response Time", f"{response_time:.2f}s")
        metrics_table.add_row("Confidence Score", f"{confidence:.2f}")
        metrics_table.add_row("Agent Path", " → ".join(agent_path))
        metrics_table.add_row("Total Sources", str(total_docs))
        metrics_table.add_row("Iterations", str(result_state.get("iteration_count", 0)))
        
        self.console.print(metrics_table)
        
        # Query analysis
        if result_state.get("query_analysis"):
            analysis = result_state["query_analysis"]
            self.console.print(f"\n🔍 [bold]Query Analysis:[/bold]")
            self.console.print(f"• Type: {analysis.get('intent', 'unknown')} ({analysis.get('complexity', 'moderate')} complexity)")
            self.console.print(f"• Domain: {analysis.get('domain', 'general')}")
            self.console.print(f"• Reasoning: {analysis.get('reasoning', 'direct')}")
            if analysis.get('key_concepts'):
                self.console.print(f"• Key concepts: {', '.join(analysis['key_concepts'])}")
        
        # Main response
        if result_state.get("synthesized_response"):
            self.console.print(f"\n💬 [bold blue]Response:[/bold blue]")
            response_panel = Panel(
                Markdown(result_state["synthesized_response"]),
                title="🤖 Enhanced GDPR Assistant Response",
                border_style="blue"
            )
            self.console.print(response_panel)
        
        # Smart feedback prompt based on confidence and complexity
        await self._smart_feedback_prompt(result_state, query, session_id)
    
    async def _smart_feedback_prompt(self, result_state: AdvancedSearchState, 
                                   query: str, session_id: str):
        """Smart feedback prompting based on result characteristics"""
        
        confidence = result_state.get("final_confidence", 0.7)
        requires_feedback = result_state.get("requires_feedback", False)
        agent_count = len(result_state.get("agent_history", []))
        
        # Determine feedback likelihood based on various factors
        should_request_feedback = (
            requires_feedback or 
            confidence < 0.7 or 
            agent_count > 4 or
            result_state.get("iteration_count", 0) > 2
        )
        
        if should_request_feedback:
            # Customize feedback request based on situation
            if confidence < 0.6:
                prompt_text = "🔄 This response had low confidence - your feedback would be very valuable!"
            elif agent_count > 5:
                prompt_text = "🤖 This query used advanced multi-agent coordination - how did we do?"
            elif result_state.get("corrective_results", {}).get("correction_steps"):
                prompt_text = "🔧 We applied quality corrections - was the final result better?"
            else:
                prompt_text = "📝 Would you like to provide feedback to help improve future responses?"
            
            request_feedback = Confirm.ask(f"\n{prompt_text}", default=False)
            
            if request_feedback:
                self.console.print("📝 [green]Thank you for your willingness to provide feedback![/green]")
                self.console.print("💡 [cyan]Feedback collection system would be implemented here.[/cyan]")
    
    async def _show_performance_metrics(self):
        """Show system performance metrics"""
        try:
            summary = await self.performance_metrics.get_performance_summary()
            
            self.console.print("\n" + "="*70)
            self.console.print("📈 [bold blue]System Performance Metrics[/bold blue]")
            self.console.print("="*70)
            
            if "error" in summary:
                self.console.print(f"❌ [red]Error: {summary['error']}[/red]")
                return
            
            if "message" in summary:
                self.console.print(f"📊 [yellow]{summary['message']}[/yellow]")
                return
            
            # Performance table
            perf_table = Table(show_header=True, header_style="bold magenta")
            perf_table.add_column("Metric", style="cyan")
            perf_table.add_column("Value", style="green")
            
            perf_table.add_row("Total Queries", str(summary.get("total_queries", 0)))
            perf_table.add_row("Avg Response Time", f"{summary.get('avg_response_time', 0):.2f}s")
            perf_table.add_row("Avg Confidence", f"{summary.get('avg_confidence', 0):.2f}")
            perf_table.add_row("Avg Documents", f"{summary.get('avg_documents', 0):.1f}")
            perf_table.add_row("Feedback Rate", f"{summary.get('feedback_rate', 0):.1f}%")
            
            self.console.print(perf_table)
            
            # Agent usage
            agent_usage = summary.get("agent_usage", {})
            if agent_usage:
                self.console.print(f"\n🤖 [bold cyan]Agent Usage[/bold cyan]")
                for agent, count in sorted(agent_usage.items(), key=lambda x: x[1], reverse=True):
                    self.console.print(f"• {agent}: {count} times")
            
            # Routing patterns
            routing_patterns = summary.get("routing_patterns", {})
            if routing_patterns:
                self.console.print(f"\n🧭 [bold cyan]Routing Patterns[/bold cyan]")
                for pattern, count in sorted(routing_patterns.items(), key=lambda x: x[1], reverse=True):
                    self.console.print(f"• {pattern}: {count} times")
                    
        except Exception as e:
            self.console.print(f"❌ [red]Error showing performance metrics: {e}[/red]")
    
    async def _display_session_summary(self, session_id: str):
        """Display session summary on exit"""
        try:
            summary = await self.performance_metrics.get_performance_summary(session_id)
            
            if summary and "total_queries" in summary:
                self.console.print("\n" + "="*60)
                self.console.print("📊 [bold blue]Session Summary[/bold blue]")
                self.console.print("="*60)
                
                self.console.print(f"• Queries processed: {summary['total_queries']}")
                self.console.print(f"• Average response time: {summary['avg_response_time']:.2f}s")
                self.console.print(f"• Average confidence: {summary['avg_confidence']:.2f}")
                self.console.print(f"• Feedback provided: {summary['feedback_rate']:.1f}%")
                
        except Exception as e:
            logging.error(f"Session summary error: {e}")

# ============================================================================
# ENHANCED CLI COMMAND INTERFACE
# ============================================================================

@click.group()
@click.option('--debug', is_flag=True, help='Enable debug logging')
@click.option('--config-file', help='Path to configuration file')
def cli(debug, config_file):
    """Next-Generation GDPR Search Engine with Advanced RAG & Multi-Agent Architecture"""
    if debug:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    if config_file:
        # Load configuration from file if specified
        logging.info(f"Loading configuration from {config_file}")

@cli.command()
@click.option('--session-id', help='Load specific session')
@click.option('--user-id', help='User identifier for personalization')
@click.option('--enable-crag/--disable-crag', default=True, help='Enable/disable Corrective RAG')
@click.option('--enable-self-rag/--disable-self-rag', default=True, help='Enable/disable Self-Reflective RAG')
@click.option('--reasoning-effort', type=click.Choice(['low', 'medium', 'high']), default='medium', help='Reasoning effort level')
def interactive(session_id, user_id, enable_crag, enable_self_rag, reasoning_effort):
    """Start enhanced interactive search mode with advanced features"""
    async def run_enhanced_interactive():
        engine = EnhancedGDPRSearchEngineCLI()
        
        # Override config with CLI options
        engine.config.ENABLE_CRAG = enable_crag
        engine.config.ENABLE_SELF_RAG = enable_self_rag
        engine.config.REASONING_EFFORT = reasoning_effort
        
        if await engine.initialize():
            if session_id:
                success = await engine.conversation_manager.load_session(session_id)
                if success:
                    engine.console.print(f"✅ [green]Loaded session: {session_id}[/green]")
                else:
                    engine.console.print(f"❌ [red]Session not found: {session_id}[/red]")
            
            await engine.run_enhanced_interactive_mode()
        else:
            engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
    
    asyncio.run(run_enhanced_interactive())

@cli.command()
@click.argument('query')
@click.option('--reasoning-effort', type=click.Choice(['low', 'medium', 'high']), default='medium', help='Reasoning effort level')
@click.option('--enable-web/--disable-web', default=True, help='Enable/disable web search')
def search(query, reasoning_effort, enable_web):
    """Perform a single enhanced search query"""
    async def run_enhanced_search():
        engine = EnhancedGDPRSearchEngineCLI()
        
        # Override config
        engine.config.REASONING_EFFORT = reasoning_effort
        engine.config.WEB_SEARCH_ENABLED = enable_web
        
        if not await engine.initialize():
            engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
            return
        
        # Start session for single query
        session_id = await engine.conversation_manager.start_enhanced_session()
        
        # Process query
        await engine._process_enhanced_search_query(query, session_id)
    
    asyncio.run(run_enhanced_search())

@cli.command()
@click.option('--limit', default=10, help='Number of sessions to show')
@click.option('--show-metrics/--hide-metrics', default=True, help='Show performance metrics')
def sessions(limit, show_metrics):
    """List recent conversation sessions with enhanced information"""
    async def list_enhanced_sessions():
        config = AdvancedSearchEngineConfig()
        conversation_manager = AdvancedConversationManager(config)
        console = Console()
        
        await conversation_manager.list_sessions(console, limit)
        
        if show_metrics:
            performance_metrics = PerformanceMetrics(config)
            summary = await performance_metrics.get_performance_summary()
            
            if summary and "total_queries" in summary:
                console.print(f"\n📊 [bold cyan]Today's Performance[/bold cyan]")
                console.print(f"• Total queries: {summary['total_queries']}")
                console.print(f"• Average response time: {summary['avg_response_time']:.2f}s")
                console.print(f"• Average confidence: {summary['avg_confidence']:.2f}")
    
    asyncio.run(list_enhanced_sessions())

@cli.command()
@click.option('--session-id', help='Show metrics for specific session')
def metrics(session_id):
    """View system performance metrics and analytics"""
    async def show_enhanced_metrics():
        config = AdvancedSearchEngineConfig()
        performance_metrics = PerformanceMetrics(config)
        console = Console()
        
        summary = await performance_metrics.get_performance_summary(session_id)
        
        if "error" in summary:
            console.print(f"❌ [red]Error: {summary['error']}[/red]")
            return
        
        console.print("\n📈 [bold blue]Performance Analytics[/bold blue]")
        
        # Display comprehensive metrics
        if "total_queries" in summary:
            metrics_table = Table(show_header=True, header_style="bold magenta")
            metrics_table.add_column("Metric", style="cyan")
            metrics_table.add_column("Value", style="green")
            
            for key, value in summary.items():
                if isinstance(value, (int, float)):
                    if isinstance(value, float):
                        metrics_table.add_row(key.replace('_', ' ').title(), f"{value:.2f}")
                    else:
                        metrics_table.add_row(key.replace('_', ' ').title(), str(value))
            
            console.print(metrics_table)
    
    asyncio.run(show_enhanced_metrics())

@cli.command()
def benchmark():
    """Run system benchmark and performance tests"""
    async def run_benchmark():
        console = Console()
        console.print("🚀 [bold blue]Running Enhanced System Benchmark[/bold blue]")
        
        engine = EnhancedGDPRSearchEngineCLI()
        
        if not await engine.initialize():
            console.print("❌ [red]Failed to initialize for benchmark[/red]")
            return
        
        # Test queries of varying complexity
        test_queries = [
            "What is GDPR?",  # Simple
            "How to implement data subject rights in practice?",  # Moderate
            "Compare GDPR compliance requirements for controllers vs processors in cross-border data transfers involving third countries with adequacy decisions",  # Complex
        ]
        
        session_id = await engine.conversation_manager.start_enhanced_session()
        
        benchmark_results = []
        
        for i, query in enumerate(test_queries, 1):
            console.print(f"\n🧪 Test {i}/3: {query[:50]}...")
            
            start_time = asyncio.get_event_loop().time()
            await engine._process_enhanced_search_query(query, session_id)
            end_time = asyncio.get_event_loop().time()
            
            response_time = end_time - start_time
            benchmark_results.append({
                "query": query,
                "response_time": response_time,
                "complexity": ["Simple", "Moderate", "Complex"][i-1]
            })
        
        # Display benchmark results
        console.print("\n📊 [bold blue]Benchmark Results[/bold blue]")
        
        benchmark_table = Table(show_header=True, header_style="bold magenta")
        benchmark_table.add_column("Complexity", style="cyan")
        benchmark_table.add_column("Response Time", style="green")
        benchmark_table.add_column("Query Preview", style="white")
        
        for result in benchmark_results:
            benchmark_table.add_row(
                result["complexity"],
                f"{result['response_time']:.2f}s",
                result["query"][:40] + "..."
            )
        
        console.print(benchmark_table)
        
        avg_time = sum(r["response_time"] for r in benchmark_results) / len(benchmark_results)
        console.print(f"\n⚡ Average response time: {avg_time:.2f}s")
    
    asyncio.run(run_benchmark())

if __name__ == "__main__":
    # Enhanced entry point with better error handling
    import sys
    
    try:
        if len(sys.argv) == 1:
            # No arguments, run enhanced interactive mode by default
            async def main():
                engine = EnhancedGDPRSearchEngineCLI()
                if await engine.initialize():
                    await engine.run_enhanced_interactive_mode()
                else:
                    engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
                    sys.exit(1)
            
            asyncio.run(main())
        else:
            # Run CLI commands
            cli()
    except KeyboardInterrupt:
        print("\n👋 Goodbye!")
    except Exception as e:
        console = Console()
        console.print(f"❌ [red]Fatal error: {e}[/red]")
        logging.error(f"Fatal error: {e}")
        traceback.print_exc()
        sys.exit(1)
