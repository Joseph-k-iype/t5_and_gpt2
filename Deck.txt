import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
import time
import datetime
from typing import Optional, Any, Dict, List, Union
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI, BadRequestError
from pydantic import BaseModel
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.docstore import Document as LC_DOCUMENT
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
from pydantic import BaseModel, ValidationError, field_validator

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        # Set credential for Azure OpenAI
        self.credential = self._get_credential()
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"), 
                client_id=self.get("AZURE_CLIENT_ID"), 
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if is_file_readable(dotenvfile):
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
                del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            # Don't raise here, file might not exist
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## Enhanced Mapping Response Model
class MappingResult(BaseModel):
    bde_id: str
    bde_description: str
    best_match_pbt: str
    best_match_definition: str
    confidence_score: float  # 0.0 to 1.0 scale
    reasoning: str
    alternative_matches: List[Dict[str, Any]] = []


## Azure Batch Processor Class
class AzureBatchCSVMapper:
    def __init__(self, config_file: str = CONFIG_PATH, creds_file: str = CREDS_PATH, cert_file: str = CERT_PATH):
        """Initialize the Azure Batch CSV Mapper"""
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.client = self._setup_azure_client()
        self.deployment_name = self.env.get("BATCH_DEPLOYMENT_NAME", "gpt-4o-mini-global-batch")
        
    def _setup_azure_client(self) -> AzureOpenAI:
        """Setup Azure OpenAI client for batch processing"""
        try:
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            
            return AzureOpenAI(
                azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT"),
                azure_ad_token_provider=token_provider,
                api_version="2025-03-01-preview"  # Latest API version supporting batch
            )
        except Exception as e:
            logger.error(f"Error setting up Azure OpenAI client: {e}")
            raise
    
    def read_csv_files(self, bde_csv_path: str, pbt_csv_path: str) -> tuple:
        """Read and validate CSV files"""
        try:
            # Read BDE CSV
            bde_df = pd.read_csv(bde_csv_path)
            if 'bde' not in bde_df.columns or 'description' not in bde_df.columns:
                raise ValueError("BDE CSV must contain 'bde' and 'description' columns")
            
            # Read PBT CSV
            pbt_df = pd.read_csv(pbt_csv_path)
            if 'pbt' not in pbt_df.columns or 'definition' not in pbt_df.columns:
                raise ValueError("PBT CSV must contain 'pbt' and 'definition' columns")
            
            logger.info(f"Loaded {len(bde_df)} BDE records and {len(pbt_df)} PBT records")
            return bde_df, pbt_df
            
        except Exception as e:
            logger.error(f"Error reading CSV files: {e}")
            raise
    
    def create_batch_jsonl(self, bde_df: pd.DataFrame, pbt_df: pd.DataFrame, output_file: str = "batch_mapping.jsonl") -> str:
        """Create JSONL file for batch processing"""
        try:
            # Convert PBT data to a formatted string for context
            pbt_context = []
            for _, row in pbt_df.iterrows():
                pbt_context.append(f"PBT: {row['pbt']} - Definition: {row['definition']}")
            
            pbt_context_str = "\n".join(pbt_context)
            
            # Create system prompt for mapping
            system_prompt = f"""You are an expert data analyst tasked with mapping business data elements (BDE) to their most appropriate process-based taxonomy (PBT) categories based on semantic similarity.

Available PBT options:
{pbt_context_str}

Instructions:
1. Analyze the BDE description carefully
2. Find the most semantically similar PBT based on the definition
3. Provide a confidence score (0.0-1.0) where 1.0 is perfect match
4. Explain your reasoning for the match
5. If confidence is below 0.7, suggest up to 2 alternative matches

Respond in valid JSON format matching this structure:
{{
    "bde_id": "string",
    "bde_description": "string", 
    "best_match_pbt": "string",
    "best_match_definition": "string",
    "confidence_score": 0.0,
    "reasoning": "string",
    "alternative_matches": [
        {{"pbt": "string", "definition": "string", "confidence": 0.0}}
    ]
}}"""

            # Create JSONL entries
            batch_requests = []
            for idx, row in bde_df.iterrows():
                custom_id = f"bde-mapping-{idx}"
                
                user_prompt = f"Map this BDE to the most appropriate PBT:\nBDE ID: {row['bde']}\nBDE Description: {row['description']}"
                
                request = {
                    "custom_id": custom_id,
                    "method": "POST", 
                    "url": "/chat/completions",
                    "body": {
                        "model": self.deployment_name,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "response_format": {"type": "json_object"},
                        "temperature": 0.1,
                        "max_tokens": 800
                    }
                }
                batch_requests.append(request)
            
            # Write to JSONL file
            with open(output_file, 'w', encoding='utf-8') as f:
                for request in batch_requests:
                    f.write(json.dumps(request, ensure_ascii=False) + '\n')
            
            logger.info(f"Created batch JSONL file: {output_file} with {len(batch_requests)} requests")
            return output_file
            
        except Exception as e:
            logger.error(f"Error creating batch JSONL: {e}")
            raise
    
    def upload_batch_file(self, jsonl_file: str) -> str:
        """Upload JSONL file for batch processing"""
        try:
            with open(jsonl_file, "rb") as f:
                file_response = self.client.files.create(
                    file=f,
                    purpose="batch",
                    extra_body={
                        "expires_after": {"seconds": 1209600, "anchor": "created_at"}  # 14 days
                    }
                )
            
            logger.info(f"Uploaded file: {file_response.id}")
            return file_response.id
            
        except Exception as e:
            logger.error(f"Error uploading batch file: {e}")
            raise
    
    def create_batch_job(self, file_id: str) -> str:
        """Create and submit batch job"""
        try:
            batch_response = self.client.batches.create(
                input_file_id=file_id,
                endpoint="/chat/completions",
                completion_window="24h",
                extra_body={
                    "output_expires_after": {"seconds": 1209600, "anchor": "created_at"}  # 14 days
                }
            )
            
            logger.info(f"Created batch job: {batch_response.id}")
            return batch_response.id
            
        except BadRequestError as e:
            if 'token_limit_exceeded' in str(e):
                logger.warning("Token limit exceeded. Consider splitting your batch into smaller files.")
                # You could implement retry logic with exponential backoff here
            raise
        except Exception as e:
            logger.error(f"Error creating batch job: {e}")
            raise
    
    def monitor_batch_job(self, batch_id: str, check_interval: int = 60) -> dict:
        """Monitor batch job progress"""
        try:
            status = "validating"
            while status not in ("completed", "failed", "canceled", "expired"):
                time.sleep(check_interval)
                
                batch_response = self.client.batches.retrieve(batch_id)
                status = batch_response.status
                
                logger.info(f"{datetime.datetime.now()} Batch ID: {batch_id}, Status: {status}")
                
                if batch_response.status == "failed":
                    if batch_response.errors:
                        for error in batch_response.errors.data:
                            logger.error(f"Error code {error.code}: {error.message}")
                    break
            
            return batch_response
            
        except Exception as e:
            logger.error(f"Error monitoring batch job: {e}")
            raise
    
    def download_results(self, batch_response, output_file: str = "mapping_results.jsonl") -> str:
        """Download and save batch results"""
        try:
            output_file_id = batch_response.output_file_id
            if not output_file_id:
                output_file_id = batch_response.error_file_id
                if not output_file_id:
                    raise ValueError("No output or error file available")
            
            file_response = self.client.files.content(output_file_id)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(file_response.text)
            
            logger.info(f"Downloaded results to: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"Error downloading results: {e}")
            raise
    
    def parse_results_to_csv(self, results_file: str, output_csv: str = "bde_pbt_mappings.csv") -> pd.DataFrame:
        """Parse JSONL results and convert to CSV"""
        try:
            mappings = []
            
            with open(results_file, 'r', encoding='utf-8') as f:
                for line in f:
                    result = json.loads(line.strip())
                    
                    if result.get('response') and result['response'].get('body'):
                        response_body = result['response']['body']
                        if response_body.get('choices'):
                            content = response_body['choices'][0]['message']['content']
                            
                            try:
                                mapping_data = json.loads(content)
                                mappings.append({
                                    'custom_id': result['custom_id'],
                                    'bde_id': mapping_data.get('bde_id', ''),
                                    'bde_description': mapping_data.get('bde_description', ''),
                                    'best_match_pbt': mapping_data.get('best_match_pbt', ''),
                                    'best_match_definition': mapping_data.get('best_match_definition', ''),
                                    'confidence_score': mapping_data.get('confidence_score', 0.0),
                                    'reasoning': mapping_data.get('reasoning', ''),
                                    'alternative_matches': json.dumps(mapping_data.get('alternative_matches', []))
                                })
                            except json.JSONDecodeError as je:
                                logger.warning(f"Could not parse mapping content for {result['custom_id']}: {je}")
                    
                    elif result.get('error'):
                        logger.error(f"Error in result {result['custom_id']}: {result['error']}")
            
            # Convert to DataFrame and save
            df = pd.DataFrame(mappings)
            df.to_csv(output_csv, index=False)
            
            logger.info(f"Parsed {len(mappings)} mappings to: {output_csv}")
            return df
            
        except Exception as e:
            logger.error(f"Error parsing results: {e}")
            raise
    
    def run_complete_mapping(self, bde_csv_path: str, pbt_csv_path: str, output_dir: str = "batch_output") -> str:
        """Run the complete BDE to PBT mapping process"""
        try:
            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            
            # Step 1: Read CSV files
            logger.info("Step 1: Reading CSV files...")
            bde_df, pbt_df = self.read_csv_files(bde_csv_path, pbt_csv_path)
            
            # Step 2: Create batch JSONL
            logger.info("Step 2: Creating batch JSONL...")
            jsonl_file = os.path.join(output_dir, "batch_mapping.jsonl")
            self.create_batch_jsonl(bde_df, pbt_df, jsonl_file)
            
            # Step 3: Upload file
            logger.info("Step 3: Uploading batch file...")
            file_id = self.upload_batch_file(jsonl_file)
            
            # Step 4: Create batch job
            logger.info("Step 4: Creating batch job...")
            batch_id = self.create_batch_job(file_id)
            
            # Step 5: Monitor progress
            logger.info("Step 5: Monitoring batch job progress...")
            batch_response = self.monitor_batch_job(batch_id)
            
            # Step 6: Download results
            logger.info("Step 6: Downloading results...")
            results_file = os.path.join(output_dir, "mapping_results.jsonl")
            self.download_results(batch_response, results_file)
            
            # Step 7: Parse to CSV
            logger.info("Step 7: Parsing results to CSV...")
            output_csv = os.path.join(output_dir, "bde_pbt_mappings.csv")
            final_df = self.parse_results_to_csv(results_file, output_csv)
            
            logger.info(f"âœ… Complete mapping process finished! Results saved to: {output_csv}")
            return output_csv
            
        except Exception as e:
            logger.error(f"Error in complete mapping process: {e}")
            raise


# Example usage and configuration
def main():
    """Example usage of the Azure Batch CSV Mapper"""
    try:
        # Initialize the mapper
        mapper = AzureBatchCSVMapper()
        
        # Example CSV file paths (update these with your actual file paths)
        bde_csv_path = "bde_data.csv"  # Contains columns: bde, description
        pbt_csv_path = "pbt_data.csv"  # Contains columns: pbt, definition
        
        # Run complete mapping process
        result_file = mapper.run_complete_mapping(bde_csv_path, pbt_csv_path)
        
        print(f"Mapping completed successfully! Results available at: {result_file}")
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise


if __name__ == "__main__":
    main()
