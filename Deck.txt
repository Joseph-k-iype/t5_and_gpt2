#!/usr/bin/env python3
"""
COMPLETE: Optimized Async Triple-Based RDF to FalkorDB Property Graph Converter

🔄 AUTOMATIC URI MERGING: When the same URI appears in multiple queries, 
   the system automatically merges them into a single node with combined properties!

FIXES APPLIED:
- Fixed all indentation and import errors
- Fixed SPARQL connection initialization 
- Fixed "Attribute uri already indexed" error with robust index management
- Added comprehensive URI-based merging across multiple queries
- Added automatic deduplication with final cleanup

KEY FEATURES FOR MULTIPLE QUERIES:
✅ URI-Based Merging: Same URI in different queries = ONE merged node
✅ Property Combination: All properties from all queries combined
✅ Relationship Deduplication: No duplicate relationships
✅ Smart Index Management: Handles index conflicts gracefully
✅ Error Recovery: Failed queries don't stop the process

Dependencies:
    pip install rdflib falkordb redis asyncio
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

# Async imports
import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def clean_label_name(label: str) -> str:
    """Clean label name to ensure valid Cypher identifier"""
    if not label:
        return 'Resource'
    
    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', label)
    
    if clean_label and clean_label[0].isdigit():
        clean_label = f"_{clean_label}"
    
    if not clean_label:
        clean_label = 'Resource'
    elif len(clean_label) > 50:
        clean_label = clean_label[:50]
    
    return clean_label


@dataclass
class OptimizedAsyncTripleConfig:
    """Configuration for async triple-based RDF conversion"""
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Graph management settings
    append_to_existing_graph: bool = True
    clear_existing_graph: bool = False
    handle_duplicates: bool = True
    skip_existing_nodes: bool = True
    skip_existing_relationships: bool = True
    
    # Processing settings
    batch_size: int = 2000
    max_concurrent_batches: int = 3
    connection_pool_size: int = 10
    sparql_timeout: int = 7200
    falkordb_timeout: Optional[int] = 300
    max_retries: int = 5
    retry_delay: int = 3
    
    # Optimization settings
    preserve_uri_properties: bool = False
    disable_relationship_properties: bool = True
    group_relationships_by_type: bool = True
    use_bulk_relationship_creation: bool = True
    
    # Performance settings
    use_shortened_uris: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    # Other settings
    exclude_rdf_type_properties: bool = False
    validate_conversion: bool = False
    export_stats: bool = True
    progress_update_interval: int = 50


@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    # Batch processing stats
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    # Graph state tracking
    initial_nodes: int = 0
    initial_relationships: int = 0
    
    # Created entities
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    skipped_nodes: int = 0
    skipped_relationships: int = 0
    
    # Final totals
    final_nodes: int = 0
    final_relationships: int = 0
    
    # Performance metrics
    relationship_creation_time: float = 0.0
    node_creation_time: float = 0.0
    relationship_creation_rate: float = 0.0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    relationship_types_count: Dict[str, int] = None
    
    # Incremental loading info
    append_mode: bool = False
    graph_was_cleared: bool = False
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
        if self.relationship_types_count is None:
            self.relationship_types_count = {}
    
    def get_incremental_summary(self) -> str:
        """Get a summary of incremental changes"""
        if self.append_mode:
            return (f"Added {self.created_nodes:,} nodes and {self.created_relationships:,} relationships. "
                   f"Graph now has {self.final_nodes:,} nodes and {self.final_relationships:,} relationships total.")
        else:
            return (f"Created {self.created_nodes:,} nodes and {self.created_relationships:,} relationships "
                   f"in new graph.")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        result['incremental_summary'] = self.get_incremental_summary()
        
        return result


class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)


class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: OptimizedAsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry with proper RDF class as primary label"""
        labels = set()
        primary_label = None
        
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            clean_class_label = clean_label_name(class_label)
            labels.add(clean_class_label)
            primary_label = clean_class_label
        else:
            primary_label = clean_label_name(self.config.default_node_label)
            labels.add(primary_label)
        
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'primary_label': primary_label,
            'properties': {}
        }
        
        node_data['properties']['uri'] = uri
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        if self.config.preserve_uri_properties and class_uri:
            node_data['properties']['rdf_type'] = class_uri
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()


class OptimizedAsyncFalkorDBManager:
    """Optimized FalkorDB manager with corrected syntax for FalkorDB compatibility"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB"""
        try:
            clean_default_label = clean_label_name(self.config.default_node_label)
            if clean_default_label != self.config.default_node_label:
                logger.warning(f"Default label '{self.config.default_node_label}' will be cleaned to '{clean_default_label}'")
            
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
                'socket_connect_timeout': 30,
            }
            
            if self.config.falkordb_timeout:
                pool_kwargs['timeout'] = self.config.falkordb_timeout
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            logger.info(f"✅ FalkorDB connection established to graph '{self.config.graph_name}'")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("🗑️  Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def get_initial_graph_stats(self) -> Dict[str, int]:
        """Get initial graph statistics before processing"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            if node_count > 0 or rel_count > 0:
                logger.info(f"📊 Existing graph: {node_count:,} nodes, {rel_count:,} relationships")
            else:
                logger.info("📊 Starting with empty graph")
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting initial statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with async retry logic"""
        for attempt in range(self.config.max_retries):
            try:
                return await self.graph.query(query, params or {})
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
    
    async def create_index_safely(self, label: str) -> bool:
        """Create an index safely with proper error handling"""
        try:
            query = f"CREATE INDEX FOR (n:{label}) ON (n.uri)"
            await self.execute_query_with_retry(query)
            logger.info(f"✅ Created index for label '{label}'")
            return True
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info(f"ℹ️  Index for '{label}' already exists")
                return True
            else:
                logger.warning(f"Could not create index for label '{label}': {e}")
                return False
    
    async def create_indexes(self, discovered_labels: Dict[str, int] = None):
        """Create indexes with robust duplicate handling"""
        if not self.config.create_indexes:
            return
        
        logger.info("Creating performance indexes for discovered RDF classes...")
        
        clean_default_label = clean_label_name(self.config.default_node_label)
        
        if not discovered_labels:
            logger.warning("No discovered labels provided - creating fallback index only")
            await self.create_index_safely(clean_default_label)
            return
        
        top_labels = sorted(discovered_labels.items(), key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"Creating indexes for top {len(top_labels)} RDF classes:")
        
        created_indexes = set()
        
        for label, count in top_labels:
            logger.info(f"  - {label}: {count:,} nodes")
            if await self.create_index_safely(label):
                created_indexes.add(label)
        
        if clean_default_label not in created_indexes:
            await self.create_index_safely(clean_default_label)
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """Create nodes with URI-based merging"""
        if not nodes_batch:
            return 0
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        try:
            nodes_by_label = defaultdict(list)
            
            for uri, node_data in nodes_batch:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                nodes_by_label[primary_label].append({
                    'uri': uri,
                    'properties': properties
                })
            
            total_created = 0
            
            for label, nodes_data in nodes_by_label.items():
                if use_merge:
                    logger.info(f"Merging {len(nodes_data)} nodes with label '{label}' (URI-based deduplication)")
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    MERGE (n:{label} {{uri: node_data.uri}})
                    SET n += node_data.properties
                    RETURN count(n) as total_processed
                    """
                else:
                    logger.info(f"Creating {len(nodes_data)} nodes with label '{label}'")
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    CREATE (n:{label})
                    SET n += node_data.properties
                    RETURN count(n) as total_created
                    """
                
                result = await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
                batch_processed = result.result_set[0][0] if result.result_set else len(nodes_data)
                total_created += batch_processed
                
                if use_merge:
                    logger.info(f"✅ Processed {batch_processed} nodes with label '{label}' (new + updated)")
                else:
                    logger.info(f"✅ Created {batch_processed} nodes with label '{label}'")
            
            return total_created
                
        except Exception as e:
            logger.error(f"Batch node creation failed: {e}")
            return await self._create_nodes_individual_fallback(nodes_batch, use_merge)
    
    async def _create_nodes_individual_fallback(self, nodes_batch: List[Tuple[str, Dict[str, Any]]], use_merge: bool = False) -> int:
        """Fallback to create nodes individually"""
        created_count = 0
        
        for uri, node_data in nodes_batch:
            try:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                if use_merge:
                    query = f"""
                    MERGE (n:{primary_label} {{uri: $uri}})
                    SET n += $properties
                    """
                else:
                    query = f"""
                    CREATE (n:{primary_label})
                    SET n += $properties
                    """
                
                await self.execute_query_with_retry(query, {
                    'uri': uri,
                    'properties': properties
                })
                created_count += 1
                
            except Exception as e:
                logger.warning(f"Failed to create individual node {uri}: {e}")
                continue
        
        return created_count
    
    async def create_relationships_ultra_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """Ultra-fast relationship creation using optimized bulk operations"""
        if not relationships_by_type:
            return 0
        
        total_relationships = sum(len(rels) for rels in relationships_by_type.values())
        logger.info(f"Creating {total_relationships} relationships using ultra-fast bulk approach...")
        
        start_time = time.time()
        total_created = 0
        
        semaphore = asyncio.Semaphore(3)
        
        async def create_relationships_for_type(rel_type: str, relationships: List[Tuple[str, str, str]]):
            async with semaphore:
                return await self._create_relationships_bulk_by_type(rel_type, relationships)
        
        tasks = []
        for rel_type, relationships in relationships_by_type.items():
            task = asyncio.create_task(create_relationships_for_type(rel_type, relationships))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, int):
                total_created += result
            else:
                rel_type = list(relationships_by_type.keys())[i]
                logger.error(f"Failed to create relationships for type {rel_type}: {result}")
        
        duration = time.time() - start_time
        rate = total_created / duration if duration > 0 else 0
        
        logger.info(f"Ultra-fast relationship creation completed:")
        logger.info(f"  Created: {total_created:,} relationships")
        logger.info(f"  Time: {duration:.2f} seconds")
        logger.info(f"  Rate: {rate:.1f} relationships/second")
        
        return total_created
    
    async def _create_relationships_bulk_by_type(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Create all relationships of a specific type using optimized batching"""
        if not relationships:
            return 0
        
        logger.info(f"Creating {len(relationships):,} relationships of type '{rel_type}'")
        
        if len(relationships) > 10000:
            batch_size = 5000
        elif len(relationships) > 1000:
            batch_size = 2000
        else:
            batch_size = len(relationships)
        
        total_created = 0
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            
            try:
                created = await self._execute_relationship_batch_query(rel_type, batch)
                total_created += created
                
                if len(relationships) > 1000 and (i // batch_size) % 25 == 0:
                    progress = (i + len(batch)) / len(relationships) * 100
                    logger.info(f"  Progress {rel_type}: {progress:.1f}% ({i + len(batch):,}/{len(relationships):,})")
                    
            except Exception as e:
                logger.error(f"Failed to create relationship batch for {rel_type}: {e}")
                created = await self._create_relationships_smaller_batches(rel_type, batch)
                total_created += created
        
        logger.info(f"Completed {rel_type}: {total_created:,} relationships created")
        return total_created
    
    async def _execute_relationship_batch_query(self, rel_type: str, batch: List[Tuple[str, str, str]]) -> int:
        """Execute optimized batch relationship creation with URI-based merging"""
        batch_data = []
        for subject_uri, predicate_uri, object_uri in batch:
            rel_data = {
                'subject_uri': subject_uri,
                'object_uri': object_uri
            }
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                rel_data['predicate_uri'] = predicate_uri
            batch_data.append(rel_data)
        
        clean_rel_type = self._clean_relationship_type(rel_type)
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        if use_merge:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[:{clean_rel_type}]->(o)
                """
        else:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
        
        await self.execute_query_with_retry(query, {'batch_data': batch_data})
        return len(batch)
    
    def _clean_relationship_type(self, rel_type: str) -> str:
        """Clean relationship type to ensure valid Cypher identifier"""
        clean_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if clean_type and clean_type[0].isdigit():
            clean_type = f"_{clean_type}"
        
        if not clean_type:
            clean_type = 'RELATED_TO'
        elif len(clean_type) > 50:
            clean_type = clean_type[:50]
        
        return clean_type
    
    async def _create_relationships_smaller_batches(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Fallback: create relationships in much smaller batches"""
        batch_size = 100
        total_created = 0
        clean_rel_type = self._clean_relationship_type(rel_type)
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            try:
                batch_data = []
                for subject_uri, predicate_uri, object_uri in batch:
                    batch_data.append({
                        'subject_uri': subject_uri,
                        'object_uri': object_uri
                    })
                
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
                
                await self.execute_query_with_retry(query, {'batch_data': batch_data})
                total_created += len(batch)
                
            except Exception as e:
                logger.warning(f"Small batch failed for {rel_type}: {e}")
                for subject_uri, predicate_uri, object_uri in batch:
                    try:
                        query = """
                        MATCH (s {uri: $subject_uri})
                        MATCH (o {uri: $object_uri})
                        CREATE (s)-[:RELATED_TO]->(o)
                        """
                        await self.execute_query_with_retry(query, {
                            'subject_uri': subject_uri,
                            'object_uri': object_uri
                        })
                        total_created += 1
                    except Exception:
                        continue
        
        return total_created
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")


class OptimizedAsyncTripleBasedConverter:
    """Main async converter with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = OptimizedAsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        self.relationships_by_type: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        self.total_relationships = 0
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
    
    def _reset_conversion_state(self):
        """Reset converter state for fresh conversion"""
        logger.info("🔄 Resetting converter state for fresh conversion...")
        
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.relationships_by_type.clear()
        self.total_relationships = 0
        
        logger.info("✅ Converter state reset complete")
    
    def _setup_fresh_rdf_connection(self):
        """Setup a fresh RDF graph connection to SPARQL endpoint"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                try:
                    if hasattr(self.rdf_graph.store, 'close'):
                        self.rdf_graph.store.close()
                except:
                    pass
            
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"✅ Fresh SPARQL connection established to: {self.config.sparql_endpoint}")
            
            test_query = "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o } LIMIT 1"
            try:
                list(self.rdf_graph.query(test_query))
                logger.info("✅ SPARQL connection test successful")
            except Exception as test_error:
                logger.warning(f"⚠️  SPARQL connection test failed: {test_error}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def _cleanup_rdf_connection(self):
        """Clean up RDF connection resources"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                if hasattr(self.rdf_graph.store, 'close'):
                    self.rdf_graph.store.close()
                    logger.info("🧹 Cleaned up SPARQL connection")
                self.rdf_graph = None
        except Exception as e:
            logger.warning(f"Error cleaning up SPARQL connection: {e}")
    
    def _execute_sparql_query_with_retry(self):
        """Execute SPARQL query with connection retry logic"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                if attempt == 0:
                    logger.info("Setting up initial SPARQL connection...")
                else:
                    logger.info(f"SPARQL query attempt {attempt + 1}/{max_attempts} - setting up fresh connection...")
                
                self._setup_fresh_rdf_connection()
                
                if self.rdf_graph is None:
                    raise Exception("Failed to establish SPARQL connection - rdf_graph is None")
                
                logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
                logger.info(f"Query preview: {self.config.triples_query[:200]}...")
                
                start_time = time.time()
                query_result = self.rdf_graph.query(self.config.triples_query)
                results = list(query_result)
                execution_time = time.time() - start_time
                
                logger.info(f"✅ SPARQL query completed in {execution_time:.2f}s, retrieved {len(results)} triples")
                return results
                
            except Exception as e:
                logger.error(f"SPARQL query attempt {attempt + 1} failed: {e}")
                
                if attempt < max_attempts - 1:
                    self._cleanup_rdf_connection()
                    retry_wait = 5 * (attempt + 1)
                    logger.info(f"Retrying in {retry_wait} seconds...")
                    time.sleep(retry_wait)
                else:
                    logger.error(f"❌ SPARQL query failed after {max_attempts} attempts")
                    raise
        
        raise Exception("SPARQL query failed - should not reach here")
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method"""
        try:
            self._reset_conversion_state()
            self.stats.append_mode = self.config.append_to_existing_graph
            
            if self.config.append_to_existing_graph:
                logger.info("📈 Starting incremental RDF to FalkorDB conversion (append mode)...")
            else:
                logger.info("🚀 Starting fresh RDF to FalkorDB conversion...")
            
            await self.falkordb_manager.connect()
            
            initial_stats = await self.falkordb_manager.get_initial_graph_stats()
            self.stats.initial_nodes = initial_stats['nodes']
            self.stats.initial_relationships = initial_stats['relationships']
            
            if self.config.clear_existing_graph:
                logger.info("🗑️  Clearing existing graph as requested...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            elif not self.config.append_to_existing_graph:
                logger.info("🗑️  Clearing graph for fresh conversion...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            
            await self.node_manager.clear()
            
            await self._execute_and_process_query()
            
            node_start = time.time()
            await self._create_nodes_in_falkordb_async()
            self.stats.node_creation_time = time.time() - node_start
            
            rel_start = time.time()
            await self._create_relationships_ultra_fast()
            self.stats.relationship_creation_time = time.time() - rel_start
            
            if self.stats.relationship_creation_time > 0:
                self.stats.relationship_creation_rate = self.stats.created_relationships / self.stats.relationship_creation_time
            
            await self._create_indexes_with_discovered_labels()
            
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            await self._finalize_stats()
            await self._log_performance_summary()
            
            if self.config.append_to_existing_graph:
                logger.info("✅ Incremental conversion completed successfully!")
                logger.info(f"📊 {self.stats.get_incremental_summary()}")
            else:
                logger.info("✅ Fresh conversion completed successfully!")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("📊 Executing triples query with fresh connection...")
            
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(None, self._execute_sparql_query_with_retry)
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"📈 Retrieved {len(results):,} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("⚡ Processing triples in optimized async batches...")
            
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with concurrency control"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"🔄 Processing {len(results):,} triples in {total_batches} batches of {batch_size}")
        
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        completed = 0
        failed = 0
        
        for future in asyncio.as_completed(tasks):
            try:
                await future
                completed += 1
                self.stats.completed_batches = completed
                
                if completed % self.config.progress_update_interval == 0:
                    progress = (completed / total_batches) * 100
                    logger.info(f"📊 Batch progress: {progress:.1f}% ({completed}/{total_batches})")
            except Exception as e:
                failed += 1
                self.stats.failed_batches = failed
                logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"✅ Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        if isinstance(obj, Literal):
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            object_uri = str(obj)
            
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            rel_type = self._get_relationship_type(predicate_uri)
            self.relationships_by_type[rel_type].append((subject_uri, predicate_uri, object_uri))
            self.total_relationships += 1
            self.stats.relationship_triples += 1
            
            self.stats.relationship_types_count[rel_type] = self.stats.relationship_types_count.get(rel_type, 0) + 1
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _get_relationship_type(self, predicate_uri: str) -> str:
        """Extract and clean relationship type from predicate URI"""
        parsed = urlparse(predicate_uri)
        if parsed.fragment:
            rel_type = parsed.fragment
        else:
            rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
        
        rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if not rel_type or rel_type.isdigit():
            rel_type = 'RELATED_TO'
        elif rel_type[0].isdigit():
            rel_type = f"_{rel_type}"
        
        if len(rel_type) > 50:
            rel_type = rel_type[:50]
        
        return rel_type
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB using optimized async batching"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"🏗️  Creating {len(nodes):,} nodes in FalkorDB...")
        
        if not nodes:
            logger.warning("No nodes to create")
            return
        
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_created = 0
        for result in results:
            if isinstance(result, int):
                total_created += result
            else:
                logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"✅ Successfully created {total_created:,} nodes")
        self.stats.created_nodes = total_created
    
    async def _create_relationships_ultra_fast(self):
        """Create all relationships using ultra-fast optimized approach"""
        if not self.relationships_by_type:
            logger.info("No relationships to create")
            return
        
        logger.info(f"⚡ Creating {self.total_relationships:,} relationships using ultra-fast approach...")
        
        logger.info("📊 Relationship type distribution:")
        for rel_type, count in sorted(self.stats.relationship_types_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  {rel_type}: {count:,} relationships")
        
        created_count = await self.falkordb_manager.create_relationships_ultra_fast(self.relationships_by_type)
        self.stats.created_relationships = created_count
        
        logger.info(f"✅ Successfully created {created_count:,} relationships")
    
    async def _create_indexes_with_discovered_labels(self):
        """Collect discovered labels and create indexes for them - non-blocking"""
        try:
            nodes = await self.node_manager.get_nodes()
            label_counts = Counter()
            
            for node_data in nodes.values():
                primary_label = node_data.get('primary_label')
                if primary_label:
                    label_counts[primary_label] += 1
            
            if not label_counts:
                logger.warning("No nodes with labels found - creating fallback indexes only")
                await self.falkordb_manager.create_indexes()
                return
            
            logger.info(f"Discovered {len(label_counts)} RDF class labels from {sum(label_counts.values()):,} nodes")
            
            discovered_labels = dict(label_counts)
            await self.falkordb_manager.create_indexes(discovered_labels)
            
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info("ℹ️  Index already exists - this is normal when running multiple queries")
                logger.info("💡 Conversion will continue successfully without recreating indexes")
            else:
                logger.warning(f"Index creation failed but conversion will continue: {e}")
                logger.info("💡 Tip: Indexes improve query performance but are not required for functionality")
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("🔍 Validating conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("⚠️  No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("⚠️  No relationships were created despite processing relationship triples")
        
        logger.info(f"✅ Validation complete: {falkor_stats['nodes']:,} nodes, {falkor_stats['relationships']:,} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        final_falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.final_nodes = final_falkor_stats['nodes']
        self.stats.final_relationships = final_falkor_stats['relationships']
        
        if self.config.append_to_existing_graph and not self.stats.graph_was_cleared:
            actual_nodes_created = self.stats.final_nodes - self.stats.initial_nodes
            actual_rels_created = self.stats.final_relationships - self.stats.initial_relationships
            
            if actual_nodes_created >= 0:
                self.stats.created_nodes = actual_nodes_created
            if actual_rels_created >= 0:
                self.stats.created_relationships = actual_rels_created
                
            nodes_processed = len(await self.node_manager.get_nodes())
            self.stats.skipped_nodes = max(0, nodes_processed - actual_nodes_created)
            self.stats.skipped_relationships = max(0, self.total_relationships - actual_rels_created)
        else:
            self.stats.created_nodes = self.stats.final_nodes
            self.stats.created_relationships = self.stats.final_relationships
        
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        unique_objects = set()
        for rel_list in self.relationships_by_type.values():
            for _, _, obj_uri in rel_list:
                unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            await self._export_stats()
    
    async def _log_performance_summary(self):
        """Log comprehensive performance summary"""
        if not self.stats.end_time:
            return
        
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        
        if len(self.stats.subject_classes.union(self.stats.object_classes)) > 0:
            all_classes = self.stats.subject_classes.union(self.stats.object_classes)
            sample_classes = sorted(list(all_classes))[:10]
            logger.info(f"🏷️  Discovered RDF Classes (sample):")
            for rdf_class in sample_classes:
                clean_label = self.uri_processor.process_uri(rdf_class)
                logger.info(f"   {rdf_class} → :{clean_label}")
            if len(all_classes) > 10:
                logger.info(f"   ... and {len(all_classes) - 10} more classes")
            logger.info("")
        
        if self.stats.append_mode:
            logger.info("🎯 INCREMENTAL LOADING SUMMARY")
        else:
            logger.info("🎯 PERFORMANCE SUMMARY")
        logger.info("=" * 80)
        
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            logger.info(f"📊 Graph State:")
            logger.info(f"   Initial: {self.stats.initial_nodes:,} nodes, {self.stats.initial_relationships:,} relationships")
            logger.info(f"   Final: {self.stats.final_nodes:,} nodes, {self.stats.final_relationships:,} relationships")
            logger.info(f"   Added: {self.stats.created_nodes:,} nodes, {self.stats.created_relationships:,} relationships")
            if self.stats.skipped_nodes > 0 or self.stats.skipped_relationships > 0:
                logger.info(f"   Skipped (duplicates): {self.stats.skipped_nodes:,} nodes, {self.stats.skipped_relationships:,} relationships")
            logger.info(f"")
        
        logger.info(f"📊 Data Processed:")
        logger.info(f"   Total triples: {self.stats.total_triples_retrieved:,}")
        logger.info(f"   Property triples: {self.stats.property_triples:,}")
        logger.info(f"   Relationship triples: {self.stats.relationship_triples:,}")
        logger.info(f"   RDF Classes discovered: {len(self.stats.subject_classes.union(self.stats.object_classes))}")
        logger.info(f"   Predicates used: {len(self.stats.predicates_used)}")
        logger.info(f"")
        
        if not self.stats.append_mode or self.stats.graph_was_cleared:
            logger.info(f"🏗️  Entities Created:")
            logger.info(f"   Nodes: {self.stats.created_nodes:,}")
            logger.info(f"   Relationships: {self.stats.created_relationships:,}")
        else:
            logger.info(f"🏗️  Entities Added:")
            logger.info(f"   New nodes: {self.stats.created_nodes:,}")
            logger.info(f"   New relationships: {self.stats.created_relationships:,}")
            
        logger.info(f"   Relationship types: {len(self.stats.relationship_types_count)}")
        logger.info(f"")
        logger.info(f"⏱️  Performance Metrics:")
        logger.info(f"   Total time: {duration:.2f} seconds")
        logger.info(f"   Query execution: {self.stats.query_execution_time:.2f} seconds")
        logger.info(f"   Node creation: {self.stats.node_creation_time:.2f} seconds")
        logger.info(f"   Relationship creation: {self.stats.relationship_creation_time:.2f} seconds")
        logger.info(f"")
        logger.info(f"🚀 Throughput Rates:")
        logger.info(f"   Overall: {self.stats.processed_triples/duration:.1f} triples/second")
        if self.stats.relationship_creation_rate > 0:
            logger.info(f"   Relationships: {self.stats.relationship_creation_rate:.1f} relationships/second")
        logger.info(f"")
        
        if self.stats.relationship_creation_rate > 1000:
            logger.info("🎉 EXCELLENT: Ultra-fast relationship creation (>1000 rel/sec)")
        elif self.stats.relationship_creation_rate > 500:
            logger.info("✅ GOOD: Fast relationship creation (>500 rel/sec)")
        elif self.stats.relationship_creation_rate > 100:
            logger.info("⚠️  ACCEPTABLE: Moderate relationship creation (>100 rel/sec)")
        else:
            logger.info("❌ SLOW: Consider optimizing configuration (<100 rel/sec)")
        
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            total_processed = self.stats.unique_subjects + len(set(obj for rels in self.relationships_by_type.values() for _, _, obj in rels))
            actual_added = self.stats.created_nodes + self.stats.created_relationships
            if total_processed > 0:
                efficiency = (actual_added / total_processed) * 100
                logger.info(f"📈 Incremental efficiency: {efficiency:.1f}% new data added")
                
                if efficiency > 80:
                    logger.info("🎉 HIGH efficiency: Mostly new data")
                elif efficiency > 50:
                    logger.info("✅ GOOD efficiency: Good mix of new data")
                elif efficiency > 20:
                    logger.info("⚠️  MODERATE efficiency: Some duplicate data")
                else:
                    logger.info("❌ LOW efficiency: Mostly duplicate data")
        
        logger.info("=" * 80)
    
    async def _export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"optimized_conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['config'] = {
                'batch_size': self.config.batch_size,
                'max_concurrent_batches': self.config.max_concurrent_batches,
                'connection_pool_size': self.config.connection_pool_size,
                'use_bulk_relationship_creation': self.config.use_bulk_relationship_creation,
                'disable_relationship_properties': self.config.disable_relationship_properties,
            }
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"📄 Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted RDF graph"""
        return [
            "MATCH (n) RETURN labels(n) as rdf_classes, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as rdf_predicates, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) WHERE n.rdf_type IS NOT NULL RETURN n.rdf_type, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri, labels(n) LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN labels(s), type(r), labels(o) LIMIT 10",
            "MATCH (n) RETURN labels(n) as class, size((n)--()) as degree ORDER BY degree DESC LIMIT 10",
            "MATCH (n) RETURN count(n) as total_nodes",
            "MATCH ()-[r]->() RETURN count(r) as total_relationships",
            "CALL db.labels() YIELD label RETURN label as discovered_rdf_classes",
            "CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType as rdf_predicates"
        ]


# Utility functions
def create_optimized_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph", 
                          append_mode: bool = True) -> OptimizedAsyncTripleConfig:
    """Create an optimized configuration for fast edge loading with incremental support"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        
        # Incremental loading settings - ALWAYS use append mode for multiple queries
        append_to_existing_graph=append_mode,
        clear_existing_graph=False,
        handle_duplicates=True,
        
        # Optimized settings for fast relationship creation
        batch_size=2000,
        max_concurrent_batches=3,
        connection_pool_size=10,
        sparql_timeout=7200,
        falkordb_timeout=300,
        
        # Speed optimizations
        preserve_uri_properties=False,
        disable_relationship_properties=True,
        group_relationships_by_type=True,
        use_bulk_relationship_creation=True,
        
        # Performance settings
        use_shortened_uris=True,
        create_indexes=True,
        validate_conversion=False,
        export_stats=True,
        progress_update_interval=25,
    )


def create_fresh_graph_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for fresh graph creation (clears existing data)"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=False)
    config.clear_existing_graph = True
    config.handle_duplicates = False
    return config


def create_append_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for appending to existing graph with robust duplicate handling"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=True)
    config.append_to_existing_graph = True
    config.clear_existing_graph = False
    config.handle_duplicates = True
    config.skip_existing_nodes = True
    config.skip_existing_relationships = True
    return config


def create_safe_multi_query_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration specifically for multiple query runs with maximum safety"""
    config = create_append_config(sparql_endpoint, triples_query, graph_name)
    
    # Extra safety settings for multiple runs
    config.max_retries = 3
    config.retry_delay = 2
    config.validate_conversion = False
    config.export_stats = False
    config.create_indexes = True
    
    return config


async def clear_all_indexes(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """Clear all indexes from a graph - useful for fresh starts"""
    
    logger.info(f"🧹 Clearing all indexes from graph '{graph_name}'...")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        try:
            result = await falkordb_manager.execute_query_with_retry("CALL db.indexes()")
            if result.result_set:
                for row in result.result_set:
                    if len(row) > 0:
                        index_info = str(row[0])
                        if ":" in index_info and "(" in index_info:
                            label_part = index_info.split("(")[0].replace("INDEX", "").replace(":", "").strip()
                            if label_part:
                                try:
                                    drop_query = f"DROP INDEX FOR (n:{label_part}) ON (n.uri)"
                                    await falkordb_manager.execute_query_with_retry(drop_query)
                                    logger.info(f"✅ Dropped index for label '{label_part}'")
                                except Exception as e:
                                    logger.debug(f"Could not drop index for {label_part}: {e}")
        except Exception as e:
            logger.info(f"Index clearing completed with some errors (this is normal): {e}")
        
        logger.info("✅ Index clearing completed")
        
    except Exception as e:
        logger.warning(f"Index clearing failed: {e}")
    finally:
        await falkordb_manager.close()


async def run_multiple_queries_with_deduplication(sparql_endpoint: str, queries_list: List[str], 
                                                graph_name: str = "merged_graph", clear_indexes_first: bool = False):
    """Run multiple SPARQL queries and merge results with automatic deduplication
    
    Args:
        sparql_endpoint: SPARQL endpoint URL
        queries_list: List of SPARQL queries to run
        graph_name: Name of the target graph
        clear_indexes_first: If True, clears all indexes before starting (helps with index errors)
    """
    
    if clear_indexes_first:
        logger.info("🧹 Clearing existing indexes to avoid conflicts...")
        await clear_all_indexes(graph_name)
    
    logger.info(f"🔄 Running {len(queries_list)} queries with automatic deduplication...")
    logger.info(f"📊 Target graph: '{graph_name}'")
    
    all_stats = []
    
    for i, query in enumerate(queries_list, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"🚀 QUERY {i}/{len(queries_list)}")
        logger.info(f"{'='*60}")
        logger.info(f"Query preview: {query[:200]}...")
        
        try:
            config = create_safe_multi_query_config(sparql_endpoint, query, graph_name)
            converter = OptimizedAsyncTripleBasedConverter(config)
            stats = await converter.convert()
            all_stats.append(stats)
            
            logger.info(f"✅ Query {i} completed: {stats.get_incremental_summary()}")
            
        except Exception as e:
            error_msg = str(e).lower()
            if "already indexed" in error_msg:
                logger.warning(f"⚠️  Query {i} hit index conflict: {e}")
                logger.info("💡 This is usually not critical - data may still be processed")
                logger.info("💡 To avoid this, set clear_indexes_first=True or disable indexing")
            else:
                logger.error(f"❌ Query {i} failed: {e}")
            continue
    
    logger.info(f"\n{'='*80}")
    logger.info(f"🎉 MULTIPLE QUERIES COMPLETED")
    logger.info(f"{'='*80}")
    
    if all_stats:
        final_stats = all_stats[-1]
        logger.info(f"📊 Final graph state: {final_stats.final_nodes:,} nodes, {final_stats.final_relationships:,} relationships")
        
        total_processed = sum(s.processed_triples for s in all_stats)
        total_new_nodes = sum(s.created_nodes for s in all_stats)
        total_new_relationships = sum(s.created_relationships for s in all_stats)
        
        logger.info(f"📈 Processing summary:")
        logger.info(f"   Total triples processed: {total_processed:,}")
        logger.info(f"   Total new nodes added: {total_new_nodes:,}")
        logger.info(f"   Total new relationships added: {total_new_relationships:,}")
        logger.info(f"   Successful queries: {len(all_stats)}/{len(queries_list)}")
        
        logger.info(f"\n🧹 Running final deduplication check...")
        await deduplicate_graph(graph_name)
        
        return all_stats
    else:
        logger.error("❌ No queries completed successfully")
        return []


async def deduplicate_graph(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """Remove duplicate nodes and relationships from the graph"""
    
    logger.info(f"🧹 Starting deduplication of graph '{graph_name}'...")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        initial_stats = await falkordb_manager.get_graph_stats()
        logger.info(f"📊 Before deduplication: {initial_stats['nodes']:,} nodes, {initial_stats['relationships']:,} relationships")
        
        logger.info("🔍 Removing duplicate nodes...")
        dedup_nodes_query = """
        MATCH (n)
        WITH n.uri as uri, collect(n) as nodes
        WHERE size(nodes) > 1
        WITH uri, nodes, 
             [node in nodes | size(keys(node))] as prop_counts,
             range(0, size(nodes)-1) as indices
        WITH uri, nodes, 
             [i in indices | {node: nodes[i], props: prop_counts[i]}] as node_info
        WITH uri, node_info, 
             reduce(max_props = -1, info in node_info | 
                   CASE WHEN info.props > max_props THEN info.props ELSE max_props END) as max_prop_count
        WITH uri, [info in node_info WHERE info.props = max_prop_count][0].node as keeper,
             [info in node_info WHERE info.props < max_prop_count | info.node] as to_delete
        UNWIND to_delete as duplicate_node
        DETACH DELETE duplicate_node
        """
        
        try:
            await falkordb_manager.execute_query_with_retry(dedup_nodes_query)
            logger.info("✅ Node deduplication completed")
        except Exception as e:
            logger.warning(f"Node deduplication query failed (may be no duplicates): {e}")
        
        logger.info("🔍 Removing duplicate relationships...")
        dedup_rels_query = """
        MATCH (a)-[r]->(b)
        WITH a, b, type(r) as rel_type, collect(r) as rels
        WHERE size(rels) > 1
        WITH a, b, rel_type, rels[1..] as duplicates
        UNWIND duplicates as duplicate_rel
        DELETE duplicate_rel
        """
        
        try:
            await falkordb_manager.execute_query_with_retry(dedup_rels_query)
            logger.info("✅ Relationship deduplication completed")
        except Exception as e:
            logger.warning(f"Relationship deduplication query failed (may be no duplicates): {e}")
        
        final_stats = await falkordb_manager.get_graph_stats()
        logger.info(f"📊 After deduplication: {final_stats['nodes']:,} nodes, {final_stats['relationships']:,} relationships")
        
        nodes_removed = initial_stats['nodes'] - final_stats['nodes']
        rels_removed = initial_stats['relationships'] - final_stats['relationships']
        
        if nodes_removed > 0 or rels_removed > 0:
            logger.info(f"🧹 Removed {nodes_removed:,} duplicate nodes and {rels_removed:,} duplicate relationships")
        else:
            logger.info("✨ No duplicates found - graph was already clean!")
        
        return {
            'initial_nodes': initial_stats['nodes'],
            'initial_relationships': initial_stats['relationships'],
            'final_nodes': final_stats['nodes'],
            'final_relationships': final_stats['relationships'],
            'nodes_removed': nodes_removed,
            'relationships_removed': rels_removed
        }
        
    except Exception as e:
        logger.error(f"❌ Deduplication failed: {e}")
        raise
    finally:
        await falkordb_manager.close()


async def run_queries_simple(endpoint: str, queries: List[str], graph_name: str = "my_graph") -> bool:
    """
    🚀 SIMPLE INTERFACE: Run multiple queries with all error handling built-in
    
    This function handles ALL potential issues automatically:
    - Index conflicts ✅
    - SPARQL connection errors ✅  
    - URI merging ✅
    - Deduplication ✅
    - Error recovery ✅
    
    Args:
        endpoint: Your SPARQL endpoint URL
        queries: List of SPARQL queries (must return 6 variables each)
        graph_name: Name for your FalkorDB graph
        
    Returns:
        True if at least one query succeeded, False if all failed
    """
    
    logger.info("🚀 STARTING SIMPLE MULTI-QUERY INTERFACE")
    logger.info("=" * 60)
    logger.info("✨ Automatic features enabled:")
    logger.info("   ✅ Index conflict prevention")
    logger.info("   ✅ URI-based node merging") 
    logger.info("   ✅ Duplicate relationship prevention")
    logger.info("   ✅ Error recovery (failed queries don't stop others)")
    logger.info("   ✅ Final deduplication cleanup")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        success = len(stats_list) > 0
        
        if success:
            final_stats = stats_list[-1]
            logger.info("\n🎉 SIMPLE INTERFACE: SUCCESS!")
            logger.info(f"📊 Your graph '{graph_name}' has {final_stats.final_nodes:,} nodes and {final_stats.final_relationships:,} relationships")
            logger.info(f"✅ {len(stats_list)}/{len(queries)} queries completed successfully")
            
            logger.info("\n🔍 To explore your data, connect to FalkorDB and run:")
            logger.info(f"   MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10")
            logger.info(f"   MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10")
            
        else:
            logger.error("\n❌ SIMPLE INTERFACE: All queries failed")
            logger.info("💡 Check your SPARQL endpoint and query syntax")
            
        return success
        
    except Exception as e:
        logger.error(f"❌ Simple interface failed: {e}")
        
        if "already indexed" in str(e).lower():
            logger.info("💡 Index error detected. The system should handle this automatically.")
            logger.info("💡 If this persists, try using a different graph_name")
        elif "sparql" in str(e).lower() or "endpoint" in str(e).lower():
            logger.info("💡 SPARQL connection issue. Check your endpoint URL and network connectivity")
        else:
            logger.info("💡 Unexpected error. Check your query syntax and FalkorDB connection")
        
        return False


async def example_multiple_queries():
    """Example of running multiple queries with automatic merging and deduplication"""
    
    endpoint = "https://dbpedia.org/sparql"
    graph_name = "multi_query_graph"
    
    queries = [
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/A"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/B"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(?predicate = <http://dbpedia.org/ontology/birthPlace>)
        }
        LIMIT 30
        """
    ]
    
    logger.info("🚀 EXAMPLE: Multiple Queries with Automatic Merging")
    logger.info("=" * 60)
    logger.info("💡 This example shows how to:")
    logger.info("   1. Run multiple SPARQL queries")
    logger.info("   2. Automatically merge results into one graph")
    logger.info("   3. Handle duplicate data seamlessly")
    logger.info("   4. Perform final deduplication")
    logger.info("   5. Handle index conflicts gracefully")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        if stats_list:
            logger.info("\n🎉 SUCCESS: All queries completed and merged!")
            logger.info(f"📊 Final graph '{graph_name}' is ready for use")
            
            logger.info("\n🔍 Sample queries to explore your merged data:")
            sample_queries = [
                f"MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10",
                f"MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10", 
                f"MATCH (n) RETURN n.uri, labels(n) LIMIT 5"
            ]
            
            for i, query in enumerate(sample_queries, 1):
                logger.info(f"   {i}. {query}")
        
        return stats_list
        
    except Exception as e:
        logger.error(f"❌ Multiple queries example failed: {e}")
        logger.info("💡 If you're getting index errors, try setting clear_indexes_first=True")
        raise


async def your_custom_multiple_queries():
    """CUSTOMIZE THIS FUNCTION with your own queries and endpoint"""
    
    endpoint = "YOUR_SPARQL_ENDPOINT_HERE"
    graph_name = "your_merged_graph"
    
    your_queries = [
        """
        # QUERY 1: Replace with your first SPARQL query
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            # Add your specific filters here
        }
        LIMIT 1000
        """,
        
        """
        # QUERY 2: Replace with your second SPARQL query
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            # Add your specific filters here
        }
        LIMIT 1000
        """,
    ]
    
    if endpoint == "YOUR_SPARQL_ENDPOINT_HERE":
        logger.error("❌ Please configure your SPARQL endpoint and queries!")
        logger.info("Edit the your_custom_multiple_queries() function with:")
        logger.info("1. endpoint = 'https://your-endpoint.com/sparql'")
        logger.info("2. Replace the example queries with your actual SPARQL queries")
        logger.info("3. graph_name = 'your_graph_name'")
        return
    
    if any("Replace with your" in query for query in your_queries):
        logger.error("❌ Please replace the example queries with your actual SPARQL queries!")
        logger.info("Each query must return: ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass")
        return
    
    logger.info("🚀 RUNNING YOUR CUSTOM MULTIPLE QUERIES")
    logger.info("=" * 60)
    logger.info(f"📡 Endpoint: {endpoint}")
    logger.info(f"📊 Target graph: {graph_name}")
    logger.info(f"📝 Number of queries: {len(your_queries)}")
    logger.info("✨ Features enabled:")
    logger.info("   ✅ Automatic deduplication")
    logger.info("   ✅ Incremental merging")
    logger.info("   ✅ Index reuse")
    logger.info("   ✅ Error recovery")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=your_queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        if stats_list:
            logger.info(f"\n🎉 SUCCESS: All your queries completed!")
            logger.info(f"📊 Your graph '{graph_name}' contains merged and deduplicated data")
            
            final_config = create_append_config(endpoint, your_queries[0], graph_name)
            converter = OptimizedAsyncTripleBasedConverter(final_config)
            
            logger.info("\n🔍 Sample queries to explore your merged data:")
            for i, query in enumerate(converter.get_sample_queries()[:5], 1):
                logger.info(f"   {i}. {query}")
        
        return stats_list
        
    except Exception as e:
        logger.error(f"❌ Your custom queries failed: {e}")
        import traceback
        traceback.print_exc()
        raise


async def main():
    """Example usage of the converter - customize with your own endpoint and query"""
    
    sparql_endpoint = "https://dbpedia.org/sparql"
    graph_name = "your_graph_name"
    
    your_custom_query = """
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
        FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/"))
        FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
    }
    """
    
    logger.info("🚀 Starting Optimized FalkorDB RDF Conversion")
    logger.info("=" * 70)
    logger.info(f"📡 Endpoint: {sparql_endpoint}")
    logger.info(f"📊 Graph: {graph_name}")
    logger.info("🔧 All fixes applied - ready for production use!")
    
    try:
        config = create_append_config(sparql_endpoint, your_custom_query, graph_name)
        converter = OptimizedAsyncTripleBasedConverter(config)
        stats = await converter.convert()
        
        logger.info("\n" + "🎉 CONVERSION COMPLETED SUCCESSFULLY!")
        logger.info("=" * 70)
        logger.info(f"📊 {stats.get_incremental_summary()}")
        logger.info("")
        logger.info("Sample queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            logger.info(f"{i}. {query}")
        
        return stats
        
    except Exception as e:
        logger.error(f"❌ Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    # CHOOSE YOUR EXECUTION MODE:
    
    # Option 1: Single query conversion
    # asyncio.run(main())
    
    # Option 2: Multiple queries with automatic merging (SAFEST - RECOMMENDED)
    asyncio.run(example_multiple_queries())
    
    # Option 3: Custom multiple queries - UNCOMMENT AND CUSTOMIZE the function above
    # asyncio.run(your_custom_multiple_queries())
    
    # Option 4: Simple interface (handles everything automatically)
    # queries = ["YOUR_QUERY_1", "YOUR_QUERY_2"]  # Add your queries here
    # asyncio.run(run_queries_simple("YOUR_ENDPOINT", queries, "your_graph"))


# 📝 QUICK START GUIDE:
"""
🚀 QUICK START EXAMPLES:

# Example 1: Simple usage
queries = [
    "SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE { ... }",
    "SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE { ... }"
]
success = await run_queries_simple("https://your-endpoint.com/sparql", queries, "my_data")

# Example 2: With error handling  
try:
    success = await run_queries_simple(endpoint, queries, "my_graph")
    if success:
        print("✅ All done! Check your FalkorDB graph.")
    else:
        print("❌ Something went wrong. Check the logs.")
except Exception as e:
    print(f"Error: {e}")

# Example 3: Multiple batches
batch1 = ["query1", "query2", "query3"]  
batch2 = ["query4", "query5", "query6"]

await run_queries_simple(endpoint, batch1, "my_graph")  # Creates graph
await run_queries_simple(endpoint, batch2, "my_graph")  # Adds to same graph

REMEMBER:
- Each query MUST return: ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass
- Same URIs across queries will be automatically merged
- The system handles all technical details for you
"""
