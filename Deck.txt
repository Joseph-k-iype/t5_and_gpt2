#!/usr/bin/env python3
"""
GDPR Dynamic Multi-Agent Knowledge System - Memory-Enhanced Version
==================================================================

A sophisticated multi-agent system with deep memory capabilities for GDPR knowledge graphs:
- LangGraph memory/persistence for cross-session learning and incremental knowledge building
- Direct OpenAI API for embeddings (no tiktoken overhead)
- Long-term memory stores for concept evolution and document relationship learning
- Thread-based persistence for maintaining analysis context across runs
- Store-based cross-thread knowledge sharing

Author: AI Assistant
Created: June 2025
Enhanced: With LangGraph memory features and direct OpenAI API usage
"""

import os
import json
import logging
import asyncio
import uuid
import operator
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, Literal, Sequence
from dataclasses import dataclass, field, asdict
from pathlib import Path

# Core libraries
import pandas as pd
import numpy as np
from tqdm import tqdm
import httpx

# Document processing with PyMuPDF
import pymupdf

# LangChain and LangGraph - latest syntax with memory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph for multi-agent architecture with memory/persistence
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from typing_extensions import TypedDict, Annotated

# Elasticsearch 8.x client
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# FalkorDB for labeled property graphs
from falkordb import FalkorDB

# Direct OpenAI API client
from openai import AsyncOpenAI

# Async support
import aiofiles

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

class Config:
    """Enhanced configuration with memory and persistence settings"""
    
    # OpenAI Configuration - Direct API usage
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    REASONING_MODEL: str = "o3-mini"
    REASONING_EFFORT: str = "high"  # low, medium, high
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS: int = 3072
    
    # Memory and Persistence Configuration
    MEMORY_THREAD_ID: str = os.getenv("MEMORY_THREAD_ID", "gdpr_knowledge_system")
    ENABLE_LONG_TERM_MEMORY: bool = os.getenv("ENABLE_LONG_TERM_MEMORY", "true").lower() == "true"
    KNOWLEDGE_STORE_NAMESPACE: str = "gdpr_knowledge"
    LEARNING_STORE_NAMESPACE: str = "gdpr_learning"
    
    # Elasticsearch 8.x Configuration
    ELASTICSEARCH_HOST: str = os.getenv("ES_HOST", "localhost")
    ELASTICSEARCH_PORT: int = int(os.getenv("ES_PORT", "9200"))
    ELASTICSEARCH_USERNAME: str = os.getenv("ES_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD: str = os.getenv("ES_PASSWORD")
    ELASTICSEARCH_INDEX: str = "gdpr_knowledge_base"
    ELASTICSEARCH_CA_CERTS: str = os.getenv("ES_CA_CERTS", "")  # Path to .crt file
    ELASTICSEARCH_VERIFY_CERTS: bool = os.getenv("ES_VERIFY_CERTS", "false").lower() == "true"
    
    # FalkorDB Configuration
    FALKORDB_HOST: str = os.getenv("FALKOR_HOST", "localhost")
    FALKORDB_PORT: int = int(os.getenv("FALKOR_PORT", "6379"))
    FALKORDB_PASSWORD: str = os.getenv("FALKOR_PASSWORD", "")
    FALKORDB_GRAPH_NAME: str = "gdpr_knowledge_graph"
    
    # Document Processing
    CHUNK_SIZE: int = 1500
    CHUNK_OVERLAP: int = 300
    
    # File Paths
    DOCUMENTS_PATH: Path = Path(os.getenv("DOCS_PATH", "./documents"))
    OUTPUT_PATH: Path = Path(os.getenv("OUTPUT_PATH", "./output"))
    MEMORY_PATH: Path = Path(os.getenv("MEMORY_PATH", "./memory"))
    
    @classmethod
    def validate(cls):
        """Validate configuration"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        # Validate reasoning effort for o3-mini
        valid_efforts = ["low", "medium", "high"]
        if cls.REASONING_EFFORT not in valid_efforts:
            raise ValueError(f"REASONING_EFFORT must be one of {valid_efforts}, got: {cls.REASONING_EFFORT}")
        
        logging.info(f"âœ… Configuration validated:")
        logging.info(f"  - Model: {cls.REASONING_MODEL}")
        logging.info(f"  - Reasoning Effort: {cls.REASONING_EFFORT}")
        logging.info(f"  - Embedding Model: {cls.EMBEDDING_MODEL}")
        logging.info(f"  - ES Verify Certs: {cls.ELASTICSEARCH_VERIFY_CERTS}")
        if cls.ELASTICSEARCH_CA_CERTS:
            logging.info(f"  - ES CA Certs: {cls.ELASTICSEARCH_CA_CERTS}")

# ============================================================================
# ENHANCED STATE WITH MEMORY
# ============================================================================

class EnhancedAgentState(TypedDict):
    """Enhanced state with memory capabilities"""
    # Core processing state
    documents: Annotated[List[Document], operator.add]
    extracted_articles: Dict[str, Any]
    concepts: List[Dict[str, Any]]
    knowledge_graph_stats: Dict[str, Any]
    vector_index_stats: Dict[str, Any]
    current_agent: str
    iteration_count: int
    errors: Annotated[List[str], operator.add]
    
    # Memory and learning state
    session_id: str
    previous_knowledge: Dict[str, Any]
    learned_patterns: List[Dict[str, Any]]
    concept_evolution: Dict[str, List[Dict[str, Any]]]
    document_relationships: Dict[str, List[str]]
    processing_insights: List[Dict[str, Any]]
    
    # Enhanced metadata
    metadata: Dict[str, Any]

@dataclass
class MemoryEntry:
    """Enhanced memory entry for cross-session learning"""
    entry_id: str
    entry_type: str  # "concept", "article", "pattern", "insight"
    content: Dict[str, Any]
    confidence_score: float
    source_documents: List[str]
    created_at: datetime
    updated_at: datetime
    usage_count: int
    related_entries: List[str]

@dataclass
class GDPRArticle:
    """Enhanced GDPR Article with memory integration"""
    article_id: str
    number: Union[int, str]
    title: str
    content: str
    regulation_type: str
    key_concepts: List[str]
    obligations: List[str]
    rights: List[str]
    source_document: str
    confidence_score: float
    # Memory enhancements
    previous_versions: List[Dict[str, Any]] = field(default_factory=list)
    learning_history: List[Dict[str, Any]] = field(default_factory=list)
    related_articles: List[str] = field(default_factory=list)

@dataclass
class GDPRConcept:
    """Enhanced GDPR Concept with evolution tracking"""
    concept_id: str
    label: str
    definition: str
    category: str
    article_references: List[str]
    regulation_type: str
    related_concepts: List[str]
    source_document: str
    confidence_score: float
    # Memory enhancements
    definition_evolution: List[Dict[str, Any]] = field(default_factory=list)
    usage_patterns: List[Dict[str, Any]] = field(default_factory=list)
    context_variations: List[str] = field(default_factory=list)

# ============================================================================
# DIRECT OPENAI EMBEDDINGS MANAGER
# ============================================================================

class DirectOpenAIEmbeddings:
    """Direct OpenAI API embeddings without tiktoken overhead"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = AsyncOpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL
        )
        self.model = config.EMBEDDING_MODEL
        self.dimensions = config.EMBEDDING_DIMENSIONS
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for a single text using /v1/embeddings endpoint"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"Error generating embedding via /v1/embeddings: {e}")
            # Return zero vector as fallback
            return [0.0] * self.dimensions
    
    async def embed_texts(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently using /v1/embeddings endpoint"""
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            try:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                    dimensions=self.dimensions
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
                # Rate limiting courtesy
                if len(batch) >= batch_size:
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logging.error(f"Error generating embeddings for batch {i} via /v1/embeddings: {e}")
                # Add zero vectors as fallback
                embeddings.extend([[0.0] * self.dimensions] * len(batch))
        
        return embeddings

# ============================================================================
# MEMORY-ENHANCED MANAGERS
# ============================================================================

class MemoryEnhancedElasticsearchManager:
    """Elasticsearch manager with memory integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup Elasticsearch client with simplified SSL configuration"""
        try:
            # Build connection parameters
            connection_params = {
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True,
                "verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
            }
            
            # Add CA certificate path if provided
            if self.config.ELASTICSEARCH_CA_CERTS:
                connection_params["ca_certs"] = self.config.ELASTICSEARCH_CA_CERTS
                protocol = "https"
                logging.info(f"Using CA certificate: {self.config.ELASTICSEARCH_CA_CERTS}")
            else:
                # Auto-detect protocol based on port or verify_certs setting
                protocol = "https" if self.config.ELASTICSEARCH_VERIFY_CERTS else "http"
            
            # Build host URL
            host_url = f"{protocol}://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"
            connection_params["hosts"] = [host_url]
            
            # Add authentication if provided
            if self.config.ELASTICSEARCH_PASSWORD:
                connection_params["basic_auth"] = (
                    self.config.ELASTICSEARCH_USERNAME, 
                    self.config.ELASTICSEARCH_PASSWORD
                )
                logging.info(f"Using basic auth with user: {self.config.ELASTICSEARCH_USERNAME}")
            
            # Create client
            self.client = Elasticsearch(**connection_params)
            
            # Test connection
            if self.client.ping():
                logging.info(f"âœ… Connected to Memory-Enhanced Elasticsearch at {host_url}")
            else:
                logging.error("âŒ Failed to connect to Elasticsearch")
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"âŒ Elasticsearch connection error: {e}")
            raise
    
    async def create_enhanced_index(self):
        """Create index with memory-enhanced mappings"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "content": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer"
                    },
                    "embeddings": {
                        "type": "dense_vector",
                        "dims": self.config.EMBEDDING_DIMENSIONS,
                        "index": True,
                        "similarity": "cosine"
                    },
                    # Memory fields
                    "session_id": {"type": "keyword"},
                    "memory_version": {"type": "integer"},
                    "learning_insights": {"type": "text"},
                    "concept_evolution": {"type": "object"},
                    "processing_history": {"type": "object"},
                    
                    # Enhanced document fields
                    "document_type": {"type": "keyword"},
                    "regulation_type": {"type": "keyword"},
                    "article_reference": {"type": "keyword"},
                    "concepts": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "source_file": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "extracted_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
        
        try:
            if self.client.indices.exists(index=self.config.ELASTICSEARCH_INDEX):
                # Check if we need to update mapping for memory features
                current_mapping = self.client.indices.get_mapping(index=self.config.ELASTICSEARCH_INDEX)
                if "memory_version" not in str(current_mapping):
                    logging.info("ðŸ”„ Upgrading index for memory features...")
                    self.client.indices.delete(index=self.config.ELASTICSEARCH_INDEX)
            
            if not self.client.indices.exists(index=self.config.ELASTICSEARCH_INDEX):
                self.client.indices.create(
                    index=self.config.ELASTICSEARCH_INDEX,
                    **mapping
                )
                logging.info(f"âœ… Created memory-enhanced Elasticsearch index: {self.config.ELASTICSEARCH_INDEX}")
            
        except Exception as e:
            logging.error(f"âŒ Error creating enhanced index: {e}")
            raise
    
    async def store_with_memory(self, documents: List[Dict[str, Any]], session_id: str, memory_insights: Dict[str, Any]):
        """Store documents with memory integration"""
        try:
            actions = []
            for doc in documents:
                # Enhanced document with memory
                enhanced_doc = {
                    **doc,
                    "session_id": session_id,
                    "memory_version": 1,
                    "learning_insights": json.dumps(memory_insights.get("insights", [])),
                    "concept_evolution": memory_insights.get("concept_evolution", {}),
                    "processing_history": {
                        "processed_at": datetime.now().isoformat(),
                        "confidence_evolution": doc.get("confidence_score", 0.8)
                    },
                    "updated_at": datetime.now().isoformat()
                }
                
                action = {
                    "_index": self.config.ELASTICSEARCH_INDEX,
                    "_id": enhanced_doc.get("document_id", str(uuid.uuid4())),
                    "_source": enhanced_doc
                }
                actions.append(action)
            
            success, failed = bulk(self.client, actions)
            logging.info(f"âœ… Stored {success} documents with memory integration")
            
            return {"success": success, "failed": len(failed)}
            
        except Exception as e:
            logging.error(f"âŒ Error storing with memory: {e}")
            raise

# ============================================================================
# MEMORY-ENHANCED PDF PROCESSOR
# ============================================================================

class MemoryEnhancedPDFProcessor:
    """PDF processor with memory of previous analyses"""
    
    def __init__(self, config: Config):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_with_memory(self, file_path: Path, memory_store, session_id: str) -> Dict[str, Any]:
        """Process PDF with memory of previous analyses"""
        try:
            # Check if we've processed this document before
            file_key = str(file_path.name)
            previous_analysis = await self._get_previous_analysis(memory_store, file_key)
            
            # Open document with pymupdf
            doc = pymupdf.open(str(file_path))
            
            doc_info = {
                "file_path": str(file_path),
                "metadata": doc.metadata,
                "page_count": doc.page_count,
                "pages": [],
                "full_text": "",
                "document_type": self._classify_document(file_path.name),
                "session_id": session_id,
                "previous_analysis": previous_analysis,
                "processing_insights": []
            }
            
            # Process each page with memory awareness
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text = page.get_text()
                
                # Check if this page content has evolved from previous analysis
                page_insights = await self._analyze_page_evolution(text, previous_analysis, page_num)
                
                try:
                    tables = page.find_tables()
                    table_data = [table.extract() for table in tables]
                except:
                    table_data = []
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": text,
                    "tables": table_data,
                    "word_count": len(text.split()),
                    "insights": page_insights
                }
                
                doc_info["pages"].append(page_info)
                doc_info["full_text"] += text + "\n"
                doc_info["processing_insights"].extend(page_insights)
            
            # Create enhanced chunks with memory
            chunks = await self._create_memory_enhanced_chunks(doc_info)
            doc_info["chunks"] = chunks
            
            # Store analysis for future reference
            await self._store_analysis_memory(memory_store, file_key, doc_info)
            
            doc.close()
            return doc_info
            
        except Exception as e:
            logging.error(f"Error processing PDF with memory {file_path}: {e}")
            # Return a proper dictionary structure even on error
            return {
                "error": str(e), 
                "file_path": str(file_path),
                "page_count": 0,
                "chunks": [],
                "processing_insights": [],
                "previous_analysis": {}
            }
    
    async def _get_previous_analysis(self, memory_store, file_key: str) -> Dict[str, Any]:
        """Retrieve previous analysis from memory"""
        try:
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "document_analysis")
            items = await memory_store.asearch(namespace, query=file_key)
            if items:
                return items[0].value
            return {}
        except:
            return {}
    
    async def _analyze_page_evolution(self, current_text: str, previous_analysis: Dict[str, Any], page_num: int) -> List[Dict[str, Any]]:
        """Analyze how page content has evolved"""
        insights = []
        
        if previous_analysis and "pages" in previous_analysis:
            prev_pages = previous_analysis["pages"]
            if page_num < len(prev_pages):
                prev_text = prev_pages[page_num].get("text", "")
                
                # Simple change detection
                if prev_text != current_text:
                    text_diff = len(current_text) - len(prev_text)
                    insights.append({
                        "type": "content_evolution",
                        "change": "content_modified",
                        "text_diff": text_diff,
                        "significance": "high" if abs(text_diff) > 100 else "low"
                    })
        
        return insights
    
    async def _create_memory_enhanced_chunks(self, doc_info: Dict[str, Any]) -> List[Document]:
        """Create chunks with memory enhancement"""
        chunks = []
        full_text = doc_info["full_text"]
        text_chunks = self.text_splitter.split_text(full_text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Enhanced metadata with memory
            metadata = {
                "source": doc_info["file_path"],
                "chunk_id": f"chunk_{i}",
                "chunk_index": i,
                "document_type": doc_info["document_type"],
                "total_chunks": len(text_chunks),
                "session_id": doc_info["session_id"],
                "processing_insights": doc_info["processing_insights"],
                "has_previous_analysis": bool(doc_info["previous_analysis"]),
                "extracted_at": datetime.now().isoformat()
            }
            
            chunk_doc = Document(
                page_content=chunk_text,
                metadata=metadata
            )
            chunks.append(chunk_doc)
        
        return chunks
    
    async def _store_analysis_memory(self, memory_store, file_key: str, doc_info: Dict[str, Any]):
        """Store analysis in memory for future use"""
        try:
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "document_analysis")
            memory_entry = {
                "file_key": file_key,
                "analysis_summary": {
                    "page_count": doc_info["page_count"],
                    "document_type": doc_info["document_type"],
                    "total_insights": len(doc_info["processing_insights"]),
                    "processed_at": datetime.now().isoformat()
                },
                "pages": doc_info["pages"][:5],  # Store sample pages
                "insights": doc_info["processing_insights"]
            }
            
            await memory_store.aput(namespace, file_key, memory_entry)
            logging.info(f"ðŸ’¾ Stored analysis memory for {file_key}")
            
        except Exception as e:
            logging.error(f"Error storing analysis memory: {e}")
    
    def _classify_document(self, filename: str) -> str:
        """Classify document type"""
        filename_lower = filename.lower()
        
        if "gdpr" in filename_lower and "uk" not in filename_lower:
            return "GDPR_REGULATION"
        elif "uk" in filename_lower and "gdpr" in filename_lower:
            return "UK_GDPR_REGULATION"
        elif any(term in filename_lower for term in ["business", "company", "internal"]):
            return "BUSINESS_DOCUMENT"
        else:
            return "UNKNOWN"

# ============================================================================
# MEMORY-ENHANCED MULTI-AGENT COMPONENTS
# ============================================================================

class MemoryEnhancedDocumentAnalyzer:
    """Document analyzer with cross-session memory"""
    
    def __init__(self, config: Config):
        self.config = config
        self.pdf_processor = MemoryEnhancedPDFProcessor(config)
    
    async def analyze_with_memory(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Analyze documents with memory enhancement"""
        logging.info("ðŸ§  MemoryEnhancedDocumentAnalyzer: Starting analysis with memory")
        
        try:
            # Get memory store from config
            memory_store = config.get("configurable", {}).get("store")
            session_id = state["session_id"]
            
            logging.info(f"ðŸ§  MemoryEnhancedDocumentAnalyzer: Session ID: {session_id}")
            logging.info(f"ðŸ’¾ Memory store available: {memory_store is not None}")
            
            # Load previous knowledge
            previous_knowledge = await self._load_previous_knowledge(memory_store, session_id)
            state["previous_knowledge"] = previous_knowledge
            
            documents = []
            pdf_files = list(self.config.DOCUMENTS_PATH.glob("*.pdf"))
            logging.info(f"ðŸ“ Found {len(pdf_files)} PDF files to process")
            
            for pdf_file in pdf_files:
                logging.info(f"ðŸ“„ Processing with memory: {pdf_file.name}")
                
                doc_info = await self.pdf_processor.process_with_memory(
                    pdf_file, memory_store, session_id
                )
                
                # Ensure doc_info is always a dictionary
                if not isinstance(doc_info, dict):
                    logging.error(f"Unexpected doc_info type for {pdf_file}: {type(doc_info)}")
                    doc_info = {"error": f"Invalid response type: {type(doc_info)}", "file_path": str(pdf_file)}
                
                if "error" not in doc_info:
                    chunks_to_add = doc_info.get("chunks", [])
                    documents.extend(chunks_to_add)
                    state["metadata"][str(pdf_file)] = {
                        "processing_status": "completed",
                        "page_count": doc_info.get("page_count", 0),
                        "chunk_count": len(chunks_to_add),
                        "memory_insights": len(doc_info.get("processing_insights", [])),
                        "has_previous_analysis": bool(doc_info.get("previous_analysis", {}))
                    }
                else:
                    error_msg = doc_info.get('error', 'Unknown error')
                    state["errors"].append(f"Failed to process {pdf_file}: {error_msg}")
                    state["metadata"][str(pdf_file)] = {
                        "processing_status": "failed",
                        "error": error_msg
                    }
            
            state["documents"] = documents
            state["current_agent"] = "concept_extractor"
            
            # Store processing insights
            processing_insights = {
                "total_documents": len(pdf_files),
                "successful_documents": len([d for d in state["metadata"].values() 
                                           if d.get("processing_status") == "completed"]),
                "memory_enhanced_count": len([d for d in state["metadata"].values() 
                                            if d.get("has_previous_analysis")]),
                "session_id": session_id,
                "processed_at": datetime.now().isoformat()
            }
            
            state["processing_insights"] = [processing_insights]
            
            logging.info(f"âœ… Processed {len(documents)} chunks with memory enhancement")
            return state
            
        except Exception as e:
            error_msg = f"MemoryEnhancedDocumentAnalyzer error: {e}"
            logging.error(f"âŒ {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _load_previous_knowledge(self, memory_store, session_id: str) -> Dict[str, Any]:
        """Load previous knowledge from memory store"""
        try:
            if not memory_store:
                return {}
            
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "sessions")
            items = await memory_store.asearch(namespace, query=session_id)
            
            if items:
                return items[0].value
            return {}
            
        except Exception as e:
            logging.error(f"Error loading previous knowledge: {e}")
            return {}

class MemoryEnhancedConceptExtractor:
    """Concept extractor with learning and evolution tracking"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL,
            temperature=0.1
        )
    
    async def extract_with_memory(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Extract concepts with memory and evolution tracking"""
        logging.info("ðŸ§  MemoryEnhancedConceptExtractor: Starting extraction with memory")
        
        try:
            memory_store = config.get("configurable", {}).get("store")
            session_id = state["session_id"]
            
            # Load previous concepts for evolution tracking
            previous_concepts = await self._load_previous_concepts(memory_store)
            
            # Extract articles with memory awareness
            articles = await self._extract_articles_with_memory(
                state["documents"], previous_concepts, memory_store
            )
            state["extracted_articles"] = {article.article_id: asdict(article) for article in articles}
            
            # Extract concepts with evolution tracking
            concepts = await self._extract_concepts_with_evolution(
                state["documents"], previous_concepts, memory_store
            )
            state["concepts"] = [asdict(concept) for concept in concepts]
            
            # Track concept evolution
            concept_evolution = await self._track_concept_evolution(
                concepts, previous_concepts, memory_store
            )
            state["concept_evolution"] = concept_evolution
            
            # Store learning patterns
            await self._store_learning_patterns(memory_store, session_id, concepts, articles)
            
            state["current_agent"] = "knowledge_builder"
            
            logging.info(f"âœ… Extracted {len(articles)} articles and {len(concepts)} concepts with memory")
            return state
            
        except Exception as e:
            error_msg = f"MemoryEnhancedConceptExtractor error: {e}"
            logging.error(f"âŒ {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _load_previous_concepts(self, memory_store) -> Dict[str, Any]:
        """Load previously extracted concepts"""
        try:
            if not memory_store:
                return {}
            
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "concepts")
            items = await memory_store.asearch(namespace, query="all_concepts")
            
            if items:
                return items[0].value
            return {}
            
        except Exception as e:
            logging.error(f"Error loading previous concepts: {e}")
            return {}
    
    async def _extract_articles_with_memory(self, documents: List[Document], 
                                           previous_concepts: Dict[str, Any], 
                                           memory_store) -> List[GDPRArticle]:
        """Extract articles with memory of previous extractions"""
        extraction_prompt = ChatPromptTemplate.from_template("""
        You are an expert GDPR analyst with access to previous knowledge.
        
        Previous concepts found: {previous_concepts}
        
        Analyze this regulatory text and extract articles, building on previous knowledge:
        
        Text: {text}
        
        Return JSON with articles, noting any evolution from previous analysis:
        {{
            "articles": [
                {{
                    "article_number": "5",
                    "title": "Article title", 
                    "content": "Full text",
                    "regulation_type": "GDPR",
                    "key_concepts": ["concept1", "concept2"],
                    "obligations": ["obligation1"],
                    "rights": ["right1"],
                    "evolution_notes": "How this differs from previous analysis"
                }}
            ]
        }}
        """)
        
        articles = []
        regulation_docs = [doc for doc in documents 
                          if "GDPR" in doc.metadata.get("document_type", "")]
        
        for doc in regulation_docs:
            try:
                previous_context = json.dumps(list(previous_concepts.keys())[:10])
                
                # Use /v1/chat/completions endpoint via LangChain ChatOpenAI
                # Use /v1/chat/completions endpoint via LangChain ChatOpenAI  
                messages = extraction_prompt.format_messages(
                    text=doc.page_content[:6000],
                    previous_concepts=previous_context
                )
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    for article_data in extracted_data.get("articles", []):
                        article = GDPRArticle(
                            article_id=f"{article_data['regulation_type'].lower()}_article_{article_data['article_number']}",
                            number=article_data["article_number"],
                            title=article_data["title"],
                            content=article_data["content"],
                            regulation_type=article_data["regulation_type"],
                            key_concepts=article_data.get("key_concepts", []),
                            obligations=article_data.get("obligations", []),
                            rights=article_data.get("rights", []),
                            source_document=doc.metadata.get("source", ""),
                            confidence_score=0.9,
                            learning_history=[{
                                "extracted_at": datetime.now().isoformat(),
                                "evolution_notes": article_data.get("evolution_notes", ""),
                                "session_id": doc.metadata.get("session_id", "")
                            }]
                        )
                        articles.append(article)
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"âš ï¸ Failed to parse article extraction: {e}")
                    
            except Exception as e:
                logging.error(f"âŒ Error extracting articles: {e}")
        
        return articles
    
    async def _extract_concepts_with_evolution(self, documents: List[Document], 
                                             previous_concepts: Dict[str, Any], 
                                             memory_store) -> List[GDPRConcept]:
        """Extract concepts with evolution tracking"""
        concept_extraction_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR expert with memory of previous concept definitions.
        
        Previous concepts: {previous_concepts}
        
        Analyze this text and extract GDPR concepts, noting evolution:
        
        Text: {text}
        
        Return JSON with concepts and their evolution:
        {{
            "concepts": [
                {{
                    "concept_id": "unique_id",
                    "label": "Concept name",
                    "definition": "Clear definition",
                    "category": "principle|legal_basis|right|obligation",
                    "article_references": ["article1"],
                    "regulation_type": "GDPR|UK_GDPR|BOTH",
                    "related_concepts": ["related1"],
                    "definition_evolution": "How definition evolved from previous",
                    "context_variations": ["context1", "context2"]
                }}
            ]
        }}
        """)
        
        concepts = []
        concept_map = {}
        
        for doc in documents:
            try:
                previous_context = json.dumps({k: v.get("definition", "") 
                                             for k, v in list(previous_concepts.items())[:5]})
                
                # Use /v1/chat/completions endpoint via LangChain ChatOpenAI
                messages = concept_extraction_prompt.format_messages(
                    text=doc.page_content[:6000],
                    previous_concepts=previous_context
                )
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    for concept_data in extracted_data.get("concepts", []):
                        concept_id = concept_data["concept_id"]
                        
                        if concept_id not in concept_map:
                            concept = GDPRConcept(
                                concept_id=concept_id,
                                label=concept_data["label"],
                                definition=concept_data["definition"],
                                category=concept_data["category"],
                                article_references=concept_data.get("article_references", []),
                                regulation_type=concept_data.get("regulation_type", "BOTH"),
                                related_concepts=concept_data.get("related_concepts", []),
                                source_document=doc.metadata.get("source", ""),
                                confidence_score=0.8,
                                definition_evolution=[{
                                    "version": 1,
                                    "definition": concept_data["definition"],
                                    "evolution_notes": concept_data.get("definition_evolution", ""),
                                    "updated_at": datetime.now().isoformat()
                                }],
                                context_variations=concept_data.get("context_variations", [])
                            )
                            concept_map[concept_id] = concept
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"âš ï¸ Failed to parse concept extraction: {e}")
                    
            except Exception as e:
                logging.error(f"âŒ Error extracting concepts: {e}")
        
        return list(concept_map.values())
    
    async def _track_concept_evolution(self, current_concepts: List[GDPRConcept], 
                                     previous_concepts: Dict[str, Any], 
                                     memory_store) -> Dict[str, List[Dict[str, Any]]]:
        """Track how concepts have evolved"""
        evolution = {}
        
        for concept in current_concepts:
            concept_id = concept.concept_id
            evolution[concept_id] = []
            
            if concept_id in previous_concepts:
                prev_def = previous_concepts[concept_id].get("definition", "")
                curr_def = concept.definition
                
                if prev_def != curr_def:
                    evolution[concept_id].append({
                        "change_type": "definition_updated",
                        "previous_definition": prev_def,
                        "current_definition": curr_def,
                        "updated_at": datetime.now().isoformat()
                    })
            else:
                evolution[concept_id].append({
                    "change_type": "new_concept",
                    "first_seen": datetime.now().isoformat()
                })
        
        return evolution
    
    async def _store_learning_patterns(self, memory_store, session_id: str, 
                                     concepts: List[GDPRConcept], 
                                     articles: List[GDPRArticle]):
        """Store learning patterns for future use"""
        try:
            if not memory_store:
                return
            
            # Store concepts
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "concepts")
            concepts_dict = {c.concept_id: asdict(c) for c in concepts}
            await memory_store.aput(namespace, "all_concepts", concepts_dict)
            
            # Store learning insights
            learning_namespace = (self.config.LEARNING_STORE_NAMESPACE, session_id)
            learning_data = {
                "session_id": session_id,
                "concepts_count": len(concepts),
                "articles_count": len(articles),
                "learning_patterns": [
                    {"pattern": "concept_evolution", "count": len(concepts)},
                    {"pattern": "article_extraction", "count": len(articles)}
                ],
                "stored_at": datetime.now().isoformat()
            }
            
            await memory_store.aput(learning_namespace, f"learning_{session_id}", learning_data)
            logging.info(f"ðŸ’¾ Stored learning patterns for session {session_id}")
            
        except Exception as e:
            logging.error(f"Error storing learning patterns: {e}")

class MemoryEnhancedVectorIndexer:
    """Vector indexer with direct OpenAI API and memory integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.elasticsearch_manager = MemoryEnhancedElasticsearchManager(config)
        self.embeddings = DirectOpenAIEmbeddings(config)
    
    async def create_with_memory(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Create vector index with memory integration"""
        logging.info("ðŸ” MemoryEnhancedVectorIndexer: Creating vector index with memory")
        
        try:
            memory_store = config.get("configurable", {}).get("store")
            session_id = state["session_id"]
            
            # Create enhanced index
            await self.elasticsearch_manager.create_enhanced_index()
            
            # Prepare documents with memory insights
            indexed_docs = await self._prepare_memory_enhanced_documents(
                state["documents"], state, memory_store
            )
            
            # Store with memory integration
            memory_insights = {
                "insights": state.get("processing_insights", []),
                "concept_evolution": state.get("concept_evolution", {}),
                "session_context": {
                    "session_id": session_id,
                    "total_concepts": len(state.get("concepts", [])),
                    "total_articles": len(state.get("extracted_articles", {}))
                }
            }
            
            result = await self.elasticsearch_manager.store_with_memory(
                indexed_docs, session_id, memory_insights
            )
            
            state["vector_index_stats"] = {
                "total_documents": len(indexed_docs),
                "successfully_indexed": result["success"],
                "failed": result["failed"],
                "embedding_dimensions": self.config.EMBEDDING_DIMENSIONS,
                "memory_enhanced": True,
                "session_id": session_id
            }
            
            state["current_agent"] = "completed"
            
            logging.info(f"âœ… Vector index created with memory: {result['success']} documents")
            return state
            
        except Exception as e:
            error_msg = f"MemoryEnhancedVectorIndexer error: {e}"
            logging.error(f"âŒ {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _prepare_memory_enhanced_documents(self, documents: List[Document], 
                                               state: EnhancedAgentState, 
                                               memory_store) -> List[Dict[str, Any]]:
        """Prepare documents with memory enhancement and direct embeddings"""
        indexed_docs = []
        
        # Extract texts for embedding
        texts = [doc.page_content for doc in documents]
        
        # Generate embeddings using direct OpenAI API (no tiktoken)
        logging.info(f"ðŸ”® Generating embeddings for {len(texts)} documents using direct OpenAI API")
        embeddings = await self.embeddings.embed_texts(texts, batch_size=50)
        
        for doc, embedding in zip(documents, embeddings):
            # Enhanced document with memory features
            indexed_doc = {
                "document_id": f"doc_{uuid.uuid4()}",
                "content": doc.page_content,
                "embeddings": embedding,
                
                # Memory features
                "session_id": state["session_id"],
                "memory_version": 1,
                "processing_insights": doc.metadata.get("processing_insights", []),
                "has_previous_analysis": doc.metadata.get("has_previous_analysis", False),
                
                # Standard document fields
                "document_type": doc.metadata.get("document_type", "unknown"),
                "source_file": doc.metadata.get("source", ""),
                "chunk_index": doc.metadata.get("chunk_index", 0),
                "extracted_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "regulation_type": self._determine_regulation_type(doc.page_content),
                "article_reference": self._extract_article_references(doc.page_content),
                "concepts": self._extract_concept_keywords(doc.page_content),
                "confidence_score": 0.8
            }
            
            indexed_docs.append(indexed_doc)
        
        return indexed_docs
    
    def _determine_regulation_type(self, content: str) -> str:
        """Determine regulation type from content"""
        content_lower = content.lower()
        if "uk gdpr" in content_lower:
            return "UK_GDPR"
        elif "gdpr" in content_lower:
            return "GDPR"
        return "unknown"
    
    def _extract_article_references(self, content: str) -> List[str]:
        """Extract article references from content"""
        import re
        patterns = [
            r"[Aa]rticle (\d+)",
            r"[Aa]rt\. (\d+)",
            r"[Aa]rticle (\d+\.\d+)"
        ]
        
        references = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            references.extend(matches)
        
        return list(set(references))
    
    def _extract_concept_keywords(self, content: str) -> List[str]:
        """Extract concept keywords from content"""
        keywords = []
        content_lower = content.lower()
        
        gdpr_terms = [
            "consent", "legitimate interest", "contract", "legal obligation",
            "personal data", "processing", "controller", "processor",
            "data subject", "data protection officer", "privacy by design",
            "data minimisation", "accountability", "transparency"
        ]
        
        for term in gdpr_terms:
            if term in content_lower:
                keywords.append(term)
        
        return keywords

# ============================================================================
# MEMORY-ENHANCED ORCHESTRATOR
# ============================================================================

class MemoryEnhancedGDPROrchestrator:
    """Multi-agent orchestrator with LangGraph memory and persistence"""
    
    def __init__(self, config: Config):
        self.config = config
        config.validate()
        
        # Initialize memory components
        self.memory_saver = MemorySaver()
        self.memory_store = InMemoryStore()
        
        # Initialize enhanced agents
        self.document_analyzer = MemoryEnhancedDocumentAnalyzer(config)
        self.concept_extractor = MemoryEnhancedConceptExtractor(config)
        self.vector_indexer = MemoryEnhancedVectorIndexer(config)
        
        # Setup logging
        self.setup_logging()
        
        # Create workflow with memory
        self.workflow = self._create_memory_enhanced_workflow()
    
    def setup_logging(self):
        """Setup logging with memory path"""
        self.config.MEMORY_PATH.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.config.OUTPUT_PATH / 'gdpr_memory_system.log'),
                logging.StreamHandler()
            ]
        )
    
    def _create_memory_enhanced_workflow(self) -> StateGraph:
        """Create LangGraph workflow with memory and persistence"""
        # Create StateGraph with enhanced state
        workflow = StateGraph(EnhancedAgentState)
        
        # Add memory-enhanced nodes
        workflow.add_node("document_analyzer", self._memory_document_analyzer_node)
        workflow.add_node("concept_extractor", self._memory_concept_extractor_node) 
        workflow.add_node("vector_indexer", self._memory_vector_indexer_node)
        
        # Define edges
        workflow.add_edge(START, "document_analyzer")
        workflow.add_edge("document_analyzer", "concept_extractor")
        workflow.add_edge("concept_extractor", "vector_indexer")
        workflow.add_edge("vector_indexer", END)
        
        # Compile with memory components
        return workflow.compile(
            checkpointer=self.memory_saver,
            store=self.memory_store
        )
    
    async def _memory_document_analyzer_node(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Memory-enhanced document analyzer node"""
        logging.info("ðŸ§  Executing MemoryEnhancedDocumentAnalyzer node")
        state["current_agent"] = "document_analyzer"
        state["iteration_count"] = state.get("iteration_count", 0) + 1
        
        try:
            return await self.document_analyzer.analyze_with_memory(state, config)
        except Exception as e:
            logging.error(f"âŒ MemoryEnhancedDocumentAnalyzer node failed: {e}")
            state["errors"] = state.get("errors", []) + [f"DocumentAnalyzer: {str(e)}"]
            return state
    
    async def _memory_concept_extractor_node(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Memory-enhanced concept extractor node"""
        logging.info("ðŸ§  Executing MemoryEnhancedConceptExtractor node")
        state["current_agent"] = "concept_extractor"
        state["iteration_count"] = state.get("iteration_count", 0) + 1
        
        try:
            return await self.concept_extractor.extract_with_memory(state, config)
        except Exception as e:
            logging.error(f"âŒ MemoryEnhancedConceptExtractor node failed: {e}")
            state["errors"] = state.get("errors", []) + [f"ConceptExtractor: {str(e)}"]
            return state
    
    async def _memory_vector_indexer_node(self, state: EnhancedAgentState, config: RunnableConfig) -> EnhancedAgentState:
        """Memory-enhanced vector indexer node"""
        logging.info("ðŸ§  Executing MemoryEnhancedVectorIndexer node")
        state["current_agent"] = "vector_indexer"
        state["iteration_count"] = state.get("iteration_count", 0) + 1
        
        try:
            return await self.vector_indexer.create_with_memory(state, config)
        except Exception as e:
            logging.error(f"âŒ MemoryEnhancedVectorIndexer node failed: {e}")
            state["errors"] = state.get("errors", []) + [f"VectorIndexer: {str(e)}"]
            return state
    
    async def run_memory_enhanced_pipeline(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Run the complete memory-enhanced multi-agent pipeline"""
        start_time = datetime.now()
        
        # Use provided session_id or generate new one
        if session_id is None:
            session_id = f"gdpr_session_{uuid.uuid4().hex[:8]}"
        
        logging.info(f"ðŸš€ Starting Memory-Enhanced GDPR Pipeline - Session: {session_id}")
        
        try:
            # Initialize enhanced state with memory features
            initial_state: EnhancedAgentState = {
                # Core processing state
                "documents": [],
                "extracted_articles": {},
                "concepts": [],
                "knowledge_graph_stats": {},
                "vector_index_stats": {},
                "current_agent": "",
                "iteration_count": 0,
                "errors": [],
                
                # Memory and learning state
                "session_id": session_id,
                "previous_knowledge": {},
                "learned_patterns": [],
                "concept_evolution": {},
                "document_relationships": {},
                "processing_insights": [],
                
                # Enhanced metadata
                "metadata": {
                    "pipeline_start": start_time.isoformat(),
                    "session_id": session_id,
                    "memory_enabled": True,
                    "config": {
                        "reasoning_model": self.config.REASONING_MODEL,
                        "reasoning_effort": self.config.REASONING_EFFORT,
                        "embedding_model": self.config.EMBEDDING_MODEL,
                        "embedding_dimensions": self.config.EMBEDDING_DIMENSIONS,
                        "memory_thread_id": self.config.MEMORY_THREAD_ID,
                        "elasticsearch_ca_certs": bool(self.config.ELASTICSEARCH_CA_CERTS),
                        "elasticsearch_verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
                    }
                }
            }
            
            # Configuration for memory
            run_config = {
                "configurable": {
                    "thread_id": self.config.MEMORY_THREAD_ID,
                    "store": self.memory_store
                }
            }
            
            # Execute workflow with memory
            logging.info("ðŸ”„ Executing memory-enhanced multi-agent workflow...")
            final_state = await self.workflow.ainvoke(initial_state, run_config)
            
            # Calculate processing time
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            # Generate enhanced results
            results = {
                "status": "success" if not final_state["errors"] else "completed_with_errors",
                "processing_time": processing_time,
                "session_id": session_id,
                "memory_enabled": True,
                
                "statistics": {
                    "documents_processed": len(final_state["documents"]),
                    "articles_extracted": len(final_state["extracted_articles"]),
                    "concepts_identified": len(final_state["concepts"]),
                    "agent_iterations": final_state["iteration_count"],
                    "memory_insights": len(final_state.get("processing_insights", [])),
                    "concept_evolutions": len(final_state.get("concept_evolution", {}))
                },
                
                "memory_features": {
                    "previous_knowledge_loaded": bool(final_state.get("previous_knowledge")),
                    "concept_evolution_tracked": len(final_state.get("concept_evolution", {})),
                    "learning_patterns_stored": len(final_state.get("learned_patterns", [])),
                    "processing_insights": final_state.get("processing_insights", [])
                },
                
                "knowledge_graph_stats": final_state.get("knowledge_graph_stats", {}),
                "vector_index_stats": final_state.get("vector_index_stats", {}),
                "errors": final_state["errors"]
            }
            
            # Store session results in memory for future reference
            await self._store_session_memory(session_id, results)
            
            # Log completion
            if final_state["errors"]:
                logging.warning(f"âš ï¸ Pipeline completed with {len(final_state['errors'])} errors")
                for error in final_state["errors"]:
                    logging.warning(f"  - {error}")
            else:
                logging.info("âœ… Memory-enhanced pipeline completed successfully!")
            
            logging.info(f"ðŸ“Š Enhanced Results:")
            logging.info(f"  - Processing Time: {processing_time:.2f} seconds")
            logging.info(f"  - Session ID: {session_id}")
            logging.info(f"  - Documents: {results['statistics']['documents_processed']}")
            logging.info(f"  - Articles: {results['statistics']['articles_extracted']}")
            logging.info(f"  - Concepts: {results['statistics']['concepts_identified']}")
            logging.info(f"  - Memory Insights: {results['statistics']['memory_insights']}")
            logging.info(f"  - Concept Evolutions: {results['statistics']['concept_evolutions']}")
            
            return results
            
        except Exception as e:
            error_msg = f"Memory-enhanced pipeline execution failed: {e}"
            logging.error(f"âŒ {error_msg}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "session_id": session_id,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
    
    async def _store_session_memory(self, session_id: str, results: Dict[str, Any]):
        """Store session results in memory for future reference"""
        try:
            namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "sessions")
            session_data = {
                "session_id": session_id,
                "results_summary": results,
                "stored_at": datetime.now().isoformat()
            }
            
            await self.memory_store.aput(namespace, session_id, session_data)
            logging.info(f"ðŸ’¾ Stored session memory for {session_id}")
            
        except Exception as e:
            logging.error(f"Error storing session memory: {e}")
    
    async def get_memory_insights(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Get insights from stored memory"""
        try:
            insights = {
                "total_sessions": 0,
                "concept_evolution_history": [],
                "learning_patterns": [],
                "document_processing_history": []
            }
            
            # Get session insights
            session_namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "sessions")
            sessions = await self.memory_store.asearch(session_namespace, query="session")
            insights["total_sessions"] = len(sessions)
            
            # Get concept evolution
            concept_namespace = (self.config.KNOWLEDGE_STORE_NAMESPACE, "concepts")
            concepts = await self.memory_store.asearch(concept_namespace, query="concept")
            insights["concept_evolution_history"] = [item.value for item in concepts]
            
            return insights
            
        except Exception as e:
            logging.error(f"Error getting memory insights: {e}")
            return {"error": str(e)}

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Main execution function with memory enhancement"""
    try:
        # Load configuration
        config = Config()
        
        # Ensure directories exist
        config.DOCUMENTS_PATH.mkdir(exist_ok=True)
        config.OUTPUT_PATH.mkdir(exist_ok=True)
        config.MEMORY_PATH.mkdir(exist_ok=True)
        
        print("ðŸš€ GDPR Memory-Enhanced Multi-Agent Knowledge System")
        print("=" * 60)
        print(f"ðŸ“ Documents Path: {config.DOCUMENTS_PATH}")
        print(f"ðŸ“‚ Output Path: {config.OUTPUT_PATH}")
        print(f"ðŸ§  Memory Path: {config.MEMORY_PATH}")
        print(f"ðŸ¤– Reasoning Model: {config.REASONING_MODEL}")
        print(f"âš¡ Reasoning Effort: {config.REASONING_EFFORT}")
        print(f"ðŸ”® Embedding Model: {config.EMBEDDING_MODEL} (Direct API)")
        print(f"ðŸ“Š Embedding Dimensions: {config.EMBEDDING_DIMENSIONS}")
        print(f"ðŸ’¾ Memory Thread ID: {config.MEMORY_THREAD_ID}")
        print(f"ðŸ”„ Long-term Memory: {'Enabled' if config.ENABLE_LONG_TERM_MEMORY else 'Disabled'}")
        print(f"ðŸ” Elasticsearch SSL: {'CA Cert provided' if config.ELASTICSEARCH_CA_CERTS else 'No SSL'}")
        if config.ELASTICSEARCH_CA_CERTS:
            print(f"ðŸ“œ ES CA Certificate: {config.ELASTICSEARCH_CA_CERTS}")
        print(f"âœ… ES Verify Certs: {'Enabled' if config.ELASTICSEARCH_VERIFY_CERTS else 'Disabled'}")
        
        # Create memory-enhanced orchestrator
        orchestrator = MemoryEnhancedGDPROrchestrator(config)
        
        # Run memory-enhanced pipeline
        print("\nðŸ”„ Starting memory-enhanced multi-agent pipeline...")
        session_id = f"gdpr_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        results = await orchestrator.run_memory_enhanced_pipeline(session_id)
        
        # Display enhanced results
        print("\n" + "=" * 60)
        print("ðŸ“Š MEMORY-ENHANCED RESULTS")
        print("=" * 60)
        print(f"Status: {results['status']}")
        print(f"Session ID: {results['session_id']}")
        print(f"Processing Time: {results['processing_time']:.2f} seconds")
        print(f"Memory Enabled: {results['memory_enabled']}")
        
        print(f"\nðŸ“ˆ Processing Statistics:")
        for key, value in results['statistics'].items():
            print(f"  - {key.replace('_', ' ').title()}: {value}")
        
        print(f"\nðŸ§  Memory Features:")
        for key, value in results['memory_features'].items():
            print(f"  - {key.replace('_', ' ').title()}: {value}")
        
        if results.get('vector_index_stats'):
            print(f"\nðŸ” Vector Index:")
            for key, value in results['vector_index_stats'].items():
                print(f"  - {key.replace('_', ' ').title()}: {value}")
        
        if results.get('errors'):
            print(f"\nâš ï¸ Errors: {len(results['errors'])}")
            for error in results['errors']:
                print(f"  - {error}")
        
        # Show memory insights
        print(f"\nðŸ’¡ Getting memory insights...")
        insights = await orchestrator.get_memory_insights(session_id)
        if "error" not in insights:
            print(f"  - Total Sessions in Memory: {insights['total_sessions']}")
            print(f"  - Concept Evolution Entries: {len(insights['concept_evolution_history'])}")
            print(f"  - Learning Patterns: {len(insights['learning_patterns'])}")
        
        print(f"\nðŸŽ‰ Memory-enhanced system ready for incremental learning!")
        print(f"Run again with the same session ID to build on this knowledge.")
        
        return results
        
    except Exception as e:
        print(f"âŒ System failed: {e}")
        logging.error(f"System failure: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
