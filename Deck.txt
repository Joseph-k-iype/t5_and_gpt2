"""
Cross-Platform Timeout Solutions for FalkorDB
============================================

This module provides timeout solutions that work on ALL platforms:
- Windows (no signal support)
- Linux/macOS (full signal support)
- Docker containers
- Cloud environments

Solutions included:
1. Threading-based timeouts (works everywhere)
2. Connection-level timeouts (built into Redis/FalkorDB)
3. Asyncio-based timeouts (modern Python)
4. Process-based timeouts (ultimate fallback)
5. Simple time monitoring (basic but reliable)
"""

import os
import sys
import time
import logging
import threading
import subprocess
import json
import tempfile
from typing import Dict, List, Set, Tuple, Any, Optional, Callable
from collections import defaultdict
from queue import Queue, Empty
from contextlib import contextmanager
import platform

# Core dependencies
import falkordb
from rdflib import Graph, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, XSD

# Check Python version for asyncio support
PYTHON_VERSION = sys.version_info
HAS_ASYNCIO = PYTHON_VERSION >= (3, 7)

if HAS_ASYNCIO:
    import asyncio
    try:
        import aioredis
        HAS_AIOREDIS = True
    except ImportError:
        HAS_AIOREDIS = False
else:
    HAS_AIOREDIS = False


class ThreadingTimeoutHandler:
    """
    Threading-based timeout handler that works on ALL platforms.
    Most reliable cross-platform solution.
    """
    
    def __init__(self, default_timeout: int = 60):
        self.default_timeout = default_timeout
        self.logger = logging.getLogger(__name__)
    
    def execute_with_timeout(self, func: Callable, args: tuple = (), 
                           kwargs: dict = None, timeout: int = None) -> Any:
        """
        Execute function with timeout using threading.
        Works on Windows, Linux, macOS, and all Python versions.
        """
        kwargs = kwargs or {}
        timeout = timeout or self.default_timeout
        
        result_queue = Queue(maxsize=1)
        exception_queue = Queue(maxsize=1)
        
        def target():
            try:
                result = func(*args, **kwargs)
                result_queue.put(result)
            except Exception as e:
                exception_queue.put(e)
        
        # Start thread
        thread = threading.Thread(target=target, daemon=True)
        thread.start()
        
        # Wait with timeout
        thread.join(timeout=timeout)
        
        if thread.is_alive():
            # Timeout occurred
            self.logger.warning(f"Query timed out after {timeout} seconds")
            # Note: We can't actually kill the thread, but we can abandon it
            raise TimeoutError(f"Operation timed out after {timeout} seconds")
        
        # Check for exceptions
        try:
            exception = exception_queue.get_nowait()
            raise exception
        except Empty:
            pass
        
        # Get result
        try:
            return result_queue.get_nowait()
        except Empty:
            raise RuntimeError("No result returned from threaded operation")


class ConnectionTimeoutHandler:
    """
    Use Redis/FalkorDB built-in connection timeouts.
    Most efficient and reliable method.
    """
    
    def __init__(self, 
                 host: str = "localhost",
                 port: int = 6379,
                 password: Optional[str] = None,
                 socket_timeout: int = 60,
                 socket_connect_timeout: int = 30,
                 socket_keepalive: bool = True,
                 socket_keepalive_options: dict = None):
        
        self.connection_params = {
            'host': host,
            'port': port,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': socket_connect_timeout,
            'socket_keepalive': socket_keepalive,
            'socket_keepalive_options': socket_keepalive_options or {},
            'retry_on_timeout': True,
            'health_check_interval': 30
        }
        
        if password:
            self.connection_params['password'] = password
        
        self.logger = logging.getLogger(__name__)
        self._connection = None
    
    def get_connection(self) -> falkordb.FalkorDB:
        """Get connection with timeout settings."""
        if self._connection is None:
            self.logger.info("Creating FalkorDB connection with timeout settings...")
            try:
                self._connection = falkordb.FalkorDB(**self.connection_params)
                self.logger.info("✅ Connection established with built-in timeouts")
            except Exception as e:
                self.logger.error(f"❌ Failed to create connection: {e}")
                raise
        
        return self._connection
    
    def execute_query(self, graph_name: str, query: str) -> Any:
        """Execute query with connection-level timeout."""
        db = self.get_connection()
        graph = db.select_graph(graph_name)
        
        try:
            return graph.query(query)
        except Exception as e:
            # Reset connection on timeout/error
            self._connection = None
            raise


class ProcessTimeoutHandler:
    """
    Ultimate fallback: Execute queries in separate processes.
    Guaranteed to work but has overhead.
    """
    
    def __init__(self, connection_params: dict):
        self.connection_params = connection_params
        self.logger = logging.getLogger(__name__)
    
    def execute_with_timeout(self, graph_name: str, query: str, timeout: int = 60) -> Any:
        """Execute query in separate process with timeout."""
        
        # Create temporary files for communication
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as query_file:
            query_data = {
                'connection_params': self.connection_params,
                'graph_name': graph_name,
                'query': query
            }
            json.dump(query_data, query_file)
            query_file_path = query_file.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as result_file:
            result_file_path = result_file.name
        
        try:
            # Create subprocess to execute query
            subprocess_code = f'''
import json
import sys
import falkordb

# Load query data
with open("{query_file_path}", "r") as f:
    data = json.load(f)

try:
    # Connect and execute
    db = falkordb.FalkorDB(**data["connection_params"])
    graph = db.select_graph(data["graph_name"])
    result = graph.query(data["query"])
    
    # Save result
    result_data = {{
        "success": True,
        "result": str(result.result_set) if hasattr(result, "result_set") else str(result)
    }}
    
    with open("{result_file_path}", "w") as f:
        json.dump(result_data, f)

except Exception as e:
    # Save error
    result_data = {{
        "success": False,
        "error": str(e)
    }}
    
    with open("{result_file_path}", "w") as f:
        json.dump(result_data, f)
'''
            
            # Execute with timeout
            process = subprocess.Popen([
                sys.executable, '-c', subprocess_code
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            try:
                stdout, stderr = process.communicate(timeout=timeout)
                
                # Read result
                with open(result_file_path, 'r') as f:
                    result_data = json.load(f)
                
                if result_data['success']:
                    return result_data['result']
                else:
                    raise Exception(result_data['error'])
                    
            except subprocess.TimeoutExpired:
                process.kill()
                process.communicate()  # Clean up
                raise TimeoutError(f"Query timed out after {timeout} seconds")
        
        finally:
            # Cleanup temp files
            try:
                os.unlink(query_file_path)
                os.unlink(result_file_path)
            except:
                pass


if HAS_ASYNCIO and HAS_AIOREDIS:
    class AsyncTimeoutHandler:
        """
        Modern asyncio-based timeout handler.
        Most efficient for high-concurrency scenarios.
        """
        
        def __init__(self, connection_params: dict):
            self.connection_params = connection_params
            self.logger = logging.getLogger(__name__)
            self._pool = None
        
        async def get_connection_pool(self):
            """Get async Redis connection pool."""
            if self._pool is None:
                self._pool = aioredis.ConnectionPool.from_url(
                    f"redis://{self.connection_params['host']}:{self.connection_params['port']}",
                    password=self.connection_params.get('password'),
                    socket_timeout=self.connection_params.get('socket_timeout', 60),
                    socket_connect_timeout=self.connection_params.get('socket_connect_timeout', 30)
                )
            return self._pool
        
        async def execute_with_timeout(self, graph_name: str, query: str, timeout: int = 60) -> Any:
            """Execute query with asyncio timeout."""
            pool = await self.get_connection_pool()
            
            async with aioredis.Redis(connection_pool=pool) as redis:
                try:
                    # Execute with asyncio timeout
                    result = await asyncio.wait_for(
                        redis.execute_command("GRAPH.QUERY", graph_name, query),
                        timeout=timeout
                    )
                    return result
                except asyncio.TimeoutError:
                    raise TimeoutError(f"Query timed out after {timeout} seconds")


class SimpleTimeMonitor:
    """
    Simple time-based monitoring without actual timeout enforcement.
    Works everywhere, provides warnings about long-running queries.
    """
    
    def __init__(self, warning_threshold: int = 30):
        self.warning_threshold = warning_threshold
        self.logger = logging.getLogger(__name__)
    
    def execute_with_monitoring(self, func: Callable, args: tuple = (), 
                              kwargs: dict = None, expected_timeout: int = 60) -> Any:
        """Execute function with time monitoring and warnings."""
        kwargs = kwargs or {}
        start_time = time.time()
        
        # Start monitoring thread
        stop_monitoring = threading.Event()
        monitor_thread = threading.Thread(
            target=self._monitor_execution, 
            args=(start_time, expected_timeout, stop_monitoring),
            daemon=True
        )
        monitor_thread.start()
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            stop_monitoring.set()
            execution_time = time.time() - start_time
            
            if execution_time > expected_timeout:
                self.logger.warning(f"Query took {execution_time:.1f}s (exceeded expected {expected_timeout}s)")
            elif execution_time > self.warning_threshold:
                self.logger.info(f"Query took {execution_time:.1f}s")
    
    def _monitor_execution(self, start_time: float, expected_timeout: int, stop_event: threading.Event):
        """Monitor execution in background thread."""
        while not stop_event.is_set():
            elapsed = time.time() - start_time
            
            if elapsed > self.warning_threshold and elapsed % 30 < 1:  # Log every 30 seconds
                self.logger.info(f"Query still running after {elapsed:.0f}s...")
            
            if elapsed > expected_timeout and elapsed % 60 < 1:  # Warn every minute after timeout
                self.logger.warning(f"Query exceeded expected timeout ({elapsed:.0f}s > {expected_timeout}s)")
            
            time.sleep(1)


class CrossPlatformFalkorDBConverter:
    """
    RDF converter with cross-platform timeout handling and robust query sanitization.
    Fixes "invalid input" errors and works on ALL systems.
    """
    
    def __init__(self,
                 # Connection parameters
                 falkor_host: str = "localhost",
                 falkor_port: int = 6379,
                 falkor_password: Optional[str] = None,
                 
                 # Timeout settings
                 socket_timeout: int = 60,
                 query_timeout: int = 120,
                 connection_timeout: int = 30,
                 
                 # Processing settings
                 chunk_size: int = 1000,      # Smaller for reliability
                 node_batch_size: int = 10,   # Much smaller batches
                 rel_batch_size: int = 5,     # Very small rel batches
                 
                 # Timeout method selection
                 timeout_method: str = "auto",  # auto, threading, connection, process, asyncio, monitor
                 
                 # Query safety settings
                 enable_query_validation: bool = True,
                 max_property_length: int = 200,
                 max_label_length: int = 50):
        
        # Store settings
        self.connection_params = {
            'host': falkor_host,
            'port': falkor_port,
            'password': falkor_password,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': connection_timeout,
            'socket_keepalive': True
        }
        
        self.query_timeout = query_timeout
        self.chunk_size = chunk_size
        self.node_batch_size = node_batch_size
        self.rel_batch_size = rel_batch_size
        
        # Query safety settings
        self.enable_query_validation = enable_query_validation
        self.max_property_length = max_property_length
        self.max_label_length = max_label_length
        
        # Initialize timeout handler based on platform and preference
        self.timeout_method = self._select_timeout_method(timeout_method)
        self.timeout_handler = self._create_timeout_handler()
        
        # Data structures with safer defaults
        self.entity_index = {}
        self.entity_counter = 0
        self.safe_property_cache = {}  # Cache for sanitized properties
        self.safe_label_cache = {}     # Cache for sanitized labels
        
        # Enhanced statistics
        self.stats = {
            'queries_executed': 0,
            'queries_timed_out': 0,
            'queries_failed': 0,
            'queries_retried': 0,
            'total_query_time': 0,
            'platform': platform.system(),
            'timeout_method': self.timeout_method,
            'sanitization_fixes': 0,
            'invalid_queries_caught': 0
        }
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"🔧 Initialized cross-platform converter with query safety (method: {self.timeout_method})")
    
    def _select_timeout_method(self, preferred: str) -> str:
        """Select best available timeout method for current platform."""
        if preferred != "auto":
            return preferred
        
        # Auto-select based on platform and capabilities
        system = platform.system().lower()
        
        if HAS_ASYNCIO and HAS_AIOREDIS:
            return "asyncio"
        elif system == "windows":
            return "connection"  # Most reliable on Windows
        else:
            return "threading"   # Good balance for Unix systems
    
    def _create_timeout_handler(self):
        """Create appropriate timeout handler."""
        if self.timeout_method == "connection":
            return ConnectionTimeoutHandler(**self.connection_params)
        elif self.timeout_method == "threading":
            return ThreadingTimeoutHandler(self.query_timeout)
        elif self.timeout_method == "process":
            return ProcessTimeoutHandler(self.connection_params)
        elif self.timeout_method == "asyncio" and HAS_ASYNCIO and HAS_AIOREDIS:
            return AsyncTimeoutHandler(self.connection_params)
        elif self.timeout_method == "monitor":
            return SimpleTimeMonitor()
        else:
            # Fallback to connection timeout
            self.logger.warning(f"Unknown timeout method '{self.timeout_method}', using connection timeout")
            return ConnectionTimeoutHandler(**self.connection_params)
    
    def _sanitize_string_for_cypher(self, value: str, max_length: int = None) -> str:
        """
        Comprehensive string sanitization for Cypher queries.
        Fixes 'invalid input' errors by properly escaping all special characters.
        """
        if not isinstance(value, str):
            value = str(value)
        
        # Limit length
        if max_length:
            value = value[:max_length]
        
        # Remove or escape problematic characters
        # Replace backslashes first (must be done before other escapes)
        value = value.replace('\\', '\\\\')
        
        # Escape quotes
        value = value.replace("'", "\\'")
        value = value.replace('"', '\\"')
        
        # Remove control characters that can break queries
        import re
        # Remove control characters except newline and tab
        value = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', value)
        
        # Replace newlines and tabs with spaces
        value = value.replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
        
        # Remove multiple spaces
        value = re.sub(r'\s+', ' ', value).strip()
        
        # Ensure it doesn't end with backslash
        if value.endswith('\\'):
            value = value[:-1]
        
        return value
    
    def _sanitize_property_name(self, prop_name: str) -> str:
        """
        Sanitize property names to be valid Cypher identifiers.
        """
        if prop_name in self.safe_property_cache:
            return self.safe_property_cache[prop_name]
        
        if not isinstance(prop_name, str):
            prop_name = str(prop_name)
        
        # Remove URI prefixes and get meaningful part
        if prop_name.startswith('http'):
            if '#' in prop_name:
                prop_name = prop_name.split('#')[-1]
            elif '/' in prop_name:
                prop_name = prop_name.split('/')[-1]
        
        # Keep only alphanumeric and underscores
        import re
        safe_name = re.sub(r'[^\w]', '_', prop_name)
        
        # Ensure it starts with letter or underscore
        if safe_name and safe_name[0].isdigit():
            safe_name = f"prop_{safe_name}"
        
        # Ensure it's not empty
        if not safe_name:
            safe_name = "property"
        
        # Limit length
        safe_name = safe_name[:50]
        
        self.safe_property_cache[prop_name] = safe_name
        return safe_name
    
    def _sanitize_label_name(self, label: str) -> str:
        """
        Sanitize label names to be valid Cypher labels.
        """
        if label in self.safe_label_cache:
            return self.safe_label_cache[label]
        
        if not isinstance(label, str):
            label = str(label)
        
        # Remove URI prefixes
        if label.startswith('http'):
            if '#' in label:
                label = label.split('#')[-1]
            elif '/' in label:
                label = label.split('/')[-1]
        
        # Keep only alphanumeric and underscores
        import re
        safe_label = re.sub(r'[^\w]', '_', label)
        
        # Ensure it starts with letter
        if safe_label and safe_label[0].isdigit():
            safe_label = f"L_{safe_label}"
        
        # Ensure it's not empty
        if not safe_label:
            safe_label = "Resource"
        
        # Limit length
        safe_label = safe_label[:self.max_label_length]
        
        self.safe_label_cache[label] = safe_label
        return safe_label
    
    def _validate_cypher_query(self, query: str) -> bool:
        """
        Basic validation to catch common Cypher syntax errors before execution.
        """
        if not self.enable_query_validation:
            return True
        
        # Basic syntax checks
        if not query or not query.strip():
            return False
        
        # Check for unmatched quotes
        single_quotes = query.count("'") - query.count("\\'")
        double_quotes = query.count('"') - query.count('\\"')
        
        if single_quotes % 2 != 0 or double_quotes % 2 != 0:
            self.logger.warning("Query has unmatched quotes")
            return False
        
        # Check for unmatched brackets
        if query.count('(') != query.count(')'):
            self.logger.warning("Query has unmatched parentheses")
            return False
        
        if query.count('{') != query.count('}'):
            self.logger.warning("Query has unmatched braces")
            return False
        
        # Check for potentially problematic patterns
        problematic_patterns = [
            r'[^\x20-\x7E]',  # Non-printable characters
            r'\\(?![\\\'"])',  # Unescaped backslashes
        ]
        
        import re
        for pattern in problematic_patterns:
            if re.search(pattern, query):
                self.logger.warning(f"Query contains problematic pattern: {pattern}")
                return False
        
        return True
    
    def _build_safe_node_query(self, node: Dict) -> Optional[str]:
        """
        Build a completely safe CREATE query for a node with comprehensive error checking.
        """
        try:
            # Sanitize labels
            raw_labels = node.get('labels', ['Resource'])
            if isinstance(raw_labels, set):
                raw_labels = list(raw_labels)
            
            safe_labels = []
            for label in raw_labels[:3]:  # Limit to 3 labels max
                safe_label = self._sanitize_label_name(str(label))
                if safe_label and safe_label not in safe_labels:
                    safe_labels.append(safe_label)
            
            if not safe_labels:
                safe_labels = ['Resource']
            
            labels_str = ':'.join(safe_labels)
            
            # Build properties with comprehensive sanitization
            props = []
            
            # Always include entity_id
            entity_id = node.get('id', 0)
            if isinstance(entity_id, (int, float)) and not (isinstance(entity_id, float) and 
                                                          (entity_id != entity_id or abs(entity_id) == float('inf'))):
                props.append(f"entity_id: {int(entity_id)}")
            else:
                props.append(f"entity_id: {hash(str(entity_id)) % 1000000}")
            
            # Add other properties safely
            for key, value in node.items():
                if key in ['id', 'labels']:
                    continue
                
                # Skip if value is None or empty
                if value is None or value == '':
                    continue
                
                # Sanitize property name
                safe_key = self._sanitize_property_name(str(key))
                
                # Handle different value types
                if isinstance(value, str):
                    if len(value.strip()) == 0:
                        continue
                    safe_value = self._sanitize_string_for_cypher(value, self.max_property_length)
                    if safe_value:  # Only add if not empty after sanitization
                        props.append(f"{safe_key}: '{safe_value}'")
                
                elif isinstance(value, bool):
                    props.append(f"{safe_key}: {'true' if value else 'false'}")
                
                elif isinstance(value, (int, float)):
                    # Check for NaN, infinity
                    if isinstance(value, float) and (value != value or abs(value) == float('inf')):
                        continue
                    props.append(f"{safe_key}: {value}")
                
                elif isinstance(value, (list, set, tuple)):
                    # Convert to string representation
                    if len(value) > 0:
                        str_value = ', '.join(str(v) for v in list(value)[:5])  # Limit to 5 items
                        safe_value = self._sanitize_string_for_cypher(str_value, self.max_property_length)
                        if safe_value:
                            props.append(f"{safe_key}: '{safe_value}'")
                
                else:
                    # Convert other types to string
                    str_value = str(value)
                    if str_value and str_value != 'None':
                        safe_value = self._sanitize_string_for_cypher(str_value, self.max_property_length)
                        if safe_value:
                            props.append(f"{safe_key}: '{safe_value}'")
            
            # Build final query
            if props:
                props_str = '{' + ', '.join(props) + '}'
                query = f"CREATE (:{labels_str} {props_str})"
            else:
                # Fallback with minimal properties
                query = f"CREATE (:{labels_str} {{entity_id: {entity_id}}})"
            
            # Validate before returning
            if self._validate_cypher_query(query):
                return query
            else:
                self.stats['invalid_queries_caught'] += 1
                # Return minimal safe query as fallback
                return f"CREATE (:Resource {{entity_id: {entity_id}}})"
                
        except Exception as e:
            self.logger.debug(f"Failed to build node query: {e}")
            # Return minimal safe query
            entity_id = hash(str(node)) % 1000000
            return f"CREATE (:Resource {{entity_id: {entity_id}}})"
    
    def _build_safe_relationship_query(self, source_id: int, target_id: int, 
                                     rel_type: str, predicate_uri: str) -> str:
        """
        Build a completely safe CREATE query for a relationship.
        """
        try:
            # Sanitize relationship type
            safe_rel_type = self._sanitize_label_name(str(rel_type))
            if not safe_rel_type:
                safe_rel_type = "RELATED"
            
            # Sanitize predicate URI
            safe_predicate = self._sanitize_string_for_cypher(str(predicate_uri), 300)
            
            # Ensure source_id and target_id are valid integers
            try:
                source_id = int(source_id)
                target_id = int(target_id)
            except (ValueError, TypeError):
                source_id = hash(str(source_id)) % 1000000
                target_id = hash(str(target_id)) % 1000000
            
            # Build query with optional predicate property
            if safe_predicate:
                query = f"""
                MATCH (a) WHERE a.entity_id = {source_id}
                MATCH (b) WHERE b.entity_id = {target_id}
                CREATE (a)-[:{safe_rel_type} {{predicate_uri: '{safe_predicate}'}}]->(b)
                """
            else:
                query = f"""
                MATCH (a) WHERE a.entity_id = {source_id}
                MATCH (b) WHERE b.entity_id = {target_id}
                CREATE (a)-[:{safe_rel_type}]->(b)
                """
            
            # Validate query
            if self._validate_cypher_query(query):
                return query.strip()
            else:
                self.stats['invalid_queries_caught'] += 1
                # Return minimal safe query
                return f"""
                MATCH (a) WHERE a.entity_id = {source_id}
                MATCH (b) WHERE b.entity_id = {target_id}
                CREATE (a)-[:RELATED]->(b)
                """.strip()
                
        except Exception as e:
            self.logger.debug(f"Failed to build relationship query: {e}")
            # Return minimal safe query
            return f"""
            MATCH (a) WHERE a.entity_id = {source_id}
            MATCH (b) WHERE b.entity_id = {target_id}
            CREATE (a)-[:RELATED]->(b)
            """.strip()
    
    def execute_query_with_timeout(self, graph_name: str, query: str, timeout: int = None) -> Any:
        """Execute query with the selected timeout method and comprehensive error handling."""
        timeout = timeout or self.query_timeout
        start_time = time.time()
        max_retries = 3
        
        # Validate query first
        if not self._validate_cypher_query(query):
            self.stats['invalid_queries_caught'] += 1
            raise ValueError(f"Invalid query syntax detected")
        
        for attempt in range(max_retries):
            try:
                if isinstance(self.timeout_handler, ConnectionTimeoutHandler):
                    result = self.timeout_handler.execute_query(graph_name, query)
                
                elif isinstance(self.timeout_handler, ThreadingTimeoutHandler):
                    # Use threading timeout
                    db = falkordb.FalkorDB(**self.connection_params)
                    graph = db.select_graph(graph_name)
                    
                    result = self.timeout_handler.execute_with_timeout(
                        func=graph.query,
                        args=(query,),
                        timeout=timeout
                    )
                
                elif isinstance(self.timeout_handler, ProcessTimeoutHandler):
                    result = self.timeout_handler.execute_with_timeout(graph_name, query, timeout)
                
                elif hasattr(self.timeout_handler, 'execute_with_timeout'):  # AsyncTimeoutHandler
                    if HAS_ASYNCIO:
                        # Run async method in event loop
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            result = loop.run_until_complete(
                                self.timeout_handler.execute_with_timeout(graph_name, query, timeout)
                            )
                        finally:
                            loop.close()
                    else:
                        raise RuntimeError("Asyncio not available")
                
                elif isinstance(self.timeout_handler, SimpleTimeMonitor):
                    # Use monitoring without actual timeout
                    db = falkordb.FalkorDB(**self.connection_params)
                    graph = db.select_graph(graph_name)
                    
                    result = self.timeout_handler.execute_with_monitoring(
                        func=graph.query,
                        args=(query,),
                        expected_timeout=timeout
                    )
                
                else:
                    raise RuntimeError(f"Unknown timeout handler type: {type(self.timeout_handler)}")
                
                # Update statistics on success
                execution_time = time.time() - start_time
                self.stats['queries_executed'] += 1
                self.stats['total_query_time'] += execution_time
                
                if attempt > 0:
                    self.stats['queries_retried'] += 1
                
                return result
                
            except TimeoutError as e:
                self.stats['queries_timed_out'] += 1
                self.logger.warning(f"Query timed out on attempt {attempt + 1}: {e}")
                if attempt == max_retries - 1:
                    raise
                
            except Exception as e:
                self.stats['queries_failed'] += 1
                error_msg = str(e).lower()
                
                # Handle specific FalkorDB errors
                if 'invalid input' in error_msg:
                    self.logger.error(f"Query syntax error on attempt {attempt + 1}: {e}")
                    self.logger.error(f"Problematic query: {query[:200]}...")
                    # Don't retry syntax errors
                    raise ValueError(f"Query syntax error: {e}")
                
                elif 'connection' in error_msg or 'timeout' in error_msg:
                    self.logger.warning(f"Connection error on attempt {attempt + 1}: {e}")
                    if attempt == max_retries - 1:
                        raise
                
                else:
                    self.logger.warning(f"Query failed on attempt {attempt + 1}: {e}")
                    if attempt == max_retries - 1:
                        raise
                
                # Wait before retry with exponential backoff
                if attempt < max_retries - 1:
                    retry_delay = 2 ** attempt
                    self.logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
    
    def process_file_cross_platform(self, file_path: str, graph_name: str = "cross_platform_graph") -> Dict[str, Any]:
        """Process RDF file with cross-platform timeout handling."""
        start_time = time.time()
        
        self.logger.info(f"🚀 Processing {file_path} with {self.timeout_method} timeout method")
        
        # Parse RDF file
        self.logger.info("📖 Parsing RDF file...")
        graph = Graph()
        graph.parse(file_path, format='turtle')
        triples = list(graph)
        
        self.logger.info(f"📊 Processing {len(triples):,} triples in chunks of {self.chunk_size}")
        
        # Clear existing data
        try:
            self.execute_query_with_timeout(graph_name, "MATCH (n) DETACH DELETE n", timeout=120)
            self.logger.info("🗑️ Cleared existing data")
        except Exception as e:
            self.logger.warning(f"Could not clear existing data: {e}")
        
        # Process in chunks
        total_nodes = 0
        total_relationships = 0
        
        for chunk_idx in range(0, len(triples), self.chunk_size):
            chunk_end = min(chunk_idx + self.chunk_size, len(triples))
            chunk_triples = triples[chunk_idx:chunk_end]
            
            self.logger.info(f"📦 Processing chunk {chunk_idx//self.chunk_size + 1}: "
                           f"triples {chunk_idx:,}-{chunk_end:,}")
            
            # Convert chunk to nodes and relationships
            nodes, relationships = self._process_triple_chunk(chunk_triples)
            
            # Upload with timeout handling
            nodes_uploaded = self._upload_nodes_safe(graph_name, nodes)
            rels_uploaded = self._upload_relationships_safe(graph_name, relationships)
            
            total_nodes += nodes_uploaded
            total_relationships += rels_uploaded
            
            progress = chunk_end / len(triples) * 100
            self.logger.info(f"✅ Chunk completed: {nodes_uploaded} nodes, {rels_uploaded} relationships "
                           f"(progress: {progress:.1f}%)")
        
        # Final statistics
        total_time = time.time() - start_time
        
        final_stats = {
            'total_time': total_time,
            'total_triples': len(triples),
            'total_nodes': total_nodes,
            'total_relationships': total_relationships,
            'throughput': len(triples) / total_time if total_time > 0 else 0,
            'timeout_method': self.timeout_method,
            'platform': self.stats['platform'],
            'query_stats': {
                'queries_executed': self.stats['queries_executed'],
                'queries_timed_out': self.stats['queries_timed_out'],
                'avg_query_time': self.stats['total_query_time'] / self.stats['queries_executed'] 
                                 if self.stats['queries_executed'] > 0 else 0
            }
        }
        
        self.logger.info("🎉 Cross-platform processing completed!")
        self.logger.info(f"⏱️ Total time: {total_time:.1f}s")
        self.logger.info(f"📊 Processed: {len(triples):,} triples")
        self.logger.info(f"🏗️ Created: {total_nodes:,} nodes, {total_relationships:,} relationships")
        self.logger.info(f"🚀 Throughput: {final_stats['throughput']:.1f} triples/second")
        self.logger.info(f"⚠️ Query timeouts: {self.stats['queries_timed_out']}")
        
        return final_stats
    
    def _process_triple_chunk(self, triples: List[Tuple]) -> Tuple[List[Dict], List[Tuple]]:
        """Process triples into nodes and relationships."""
        nodes = {}
        relationships = []
        
        for subject, predicate, obj in triples:
            subject_str = str(subject)
            subject_id = self._get_entity_id(subject_str)
            
            # Create subject node
            if subject_id not in nodes:
                nodes[subject_id] = {
                    'id': subject_id,
                    'uri': subject_str,
                    'labels': ['Resource']
                }
            
            # Handle object
            if isinstance(obj, (URIRef, BNode)):
                obj_str = str(obj)
                obj_id = self._get_entity_id(obj_str)
                
                # Create object node
                if obj_id not in nodes:
                    nodes[obj_id] = {
                        'id': obj_id,
                        'uri': obj_str,
                        'labels': ['Resource']
                    }
                
                # Create relationship
                rel_type = str(predicate).split('/')[-1].split('#')[-1]
                relationships.append((subject_id, obj_id, rel_type, str(predicate)))
            
            elif isinstance(obj, Literal):
                # Add as property
                prop_name = str(predicate).split('/')[-1].split('#')[-1]
                nodes[subject_id][prop_name] = str(obj)
        
        return list(nodes.values()), relationships
    
    def _get_entity_id(self, entity_str: str) -> int:
        """Get or create entity ID."""
        if entity_str not in self.entity_index:
            self.entity_index[entity_str] = self.entity_counter
            self.entity_counter += 1
        return self.entity_index[entity_str]
    
    def _upload_nodes_safe(self, graph_name: str, nodes: List[Dict]) -> int:
        """Upload nodes with comprehensive safety checks and error handling."""
        uploaded = 0
        
        self.logger.info(f"🏗️ Uploading {len(nodes)} nodes with safe queries...")
        
        for i in range(0, len(nodes), self.node_batch_size):
            batch_end = min(i + self.node_batch_size, len(nodes))
            batch_nodes = nodes[i:batch_end]
            
            # Progress logging
            if i % (self.node_batch_size * 5) == 0:
                progress = i / len(nodes) * 100
                self.logger.info(f"📊 Node progress: {progress:.1f}% ({i}/{len(nodes)})")
            
            try:
                # Try batch upload first
                queries = []
                valid_nodes = []
                
                for node in batch_nodes:
                    query = self._build_safe_node_query(node)
                    if query:
                        queries.append(query)
                        valid_nodes.append(node)
                
                if queries:
                    # Execute batch with timeout
                    combined_query = ' '.join(queries)
                    
                    try:
                        self.execute_query_with_timeout(
                            graph_name, 
                            combined_query, 
                            timeout=max(30, len(queries) * 3)  # 3 seconds per query
                        )
                        uploaded += len(queries)
                        self.logger.debug(f"✅ Batch of {len(queries)} nodes uploaded successfully")
                        
                    except Exception as batch_error:
                        self.logger.warning(f"Batch upload failed: {batch_error}")
                        
                        # Fall back to individual uploads
                        self.logger.info("🔄 Falling back to individual node uploads...")
                        for node, query in zip(valid_nodes, queries):
                            try:
                                self.execute_query_with_timeout(graph_name, query, timeout=30)
                                uploaded += 1
                            except Exception as individual_error:
                                self.logger.debug(f"Individual node upload failed: {individual_error}")
                                
                                # Try minimal safe query as last resort
                                try:
                                    entity_id = node.get('id', hash(str(node)) % 1000000)
                                    minimal_query = f"CREATE (:Resource {{entity_id: {entity_id}}})"
                                    self.execute_query_with_timeout(graph_name, minimal_query, timeout=15)
                                    uploaded += 1
                                    self.stats['sanitization_fixes'] += 1
                                except:
                                    # Skip this node entirely
                                    continue
                
            except Exception as e:
                self.logger.error(f"Node batch processing failed: {e}")
                continue
        
        self.logger.info(f"✅ Node upload completed: {uploaded}/{len(nodes)} nodes uploaded")
        return uploaded
    
    def _upload_relationships_safe(self, graph_name: str, relationships: List[Tuple]) -> int:
        """Upload relationships with comprehensive safety checks."""
        uploaded = 0
        
        self.logger.info(f"🔗 Uploading {len(relationships)} relationships with safe queries...")
        
        # Upload relationships one by one for maximum safety
        for i, (source_id, target_id, rel_type, predicate_uri) in enumerate(relationships):
            # Progress logging
            if i % 1000 == 0 and i > 0:
                progress = i / len(relationships) * 100
                self.logger.info(f"🔗 Relationship progress: {progress:.1f}% ({i}/{len(relationships)})")
            
            try:
                # Build safe query
                query = self._build_safe_relationship_query(source_id, target_id, rel_type, predicate_uri)
                
                # Execute with timeout
                self.execute_query_with_timeout(graph_name, query, timeout=30)
                uploaded += 1
                
            except Exception as e:
                self.logger.debug(f"Relationship {i} failed ({source_id} -> {target_id}): {e}")
                
                # Try minimal relationship as fallback
                try:
                    # Ensure IDs are integers
                    safe_source = int(source_id) if isinstance(source_id, (int, float)) else hash(str(source_id)) % 1000000
                    safe_target = int(target_id) if isinstance(target_id, (int, float)) else hash(str(target_id)) % 1000000
                    
                    minimal_query = f"""
                    MATCH (a) WHERE a.entity_id = {safe_source}
                    MATCH (b) WHERE b.entity_id = {safe_target}
                    CREATE (a)-[:RELATED]->(b)
                    """
                    
                    self.execute_query_with_timeout(graph_name, minimal_query, timeout=15)
                    uploaded += 1
                    self.stats['sanitization_fixes'] += 1
                    
                except Exception as fallback_error:
                    self.logger.debug(f"Minimal relationship also failed: {fallback_error}")
                    continue
        
        self.logger.info(f"✅ Relationship upload completed: {uploaded}/{len(relationships)} relationships uploaded")
        return uploaded


def main():
    """Command-line interface for cross-platform converter."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Cross-Platform RDF to FalkorDB Converter")
    parser.add_argument("rdf_file", help="Path to RDF file")
    parser.add_argument("--graph-name", default="cross_platform_graph", help="FalkorDB graph name")
    parser.add_argument("--timeout-method", default="auto", 
                       choices=["auto", "connection", "threading", "process", "asyncio", "monitor"],
                       help="Timeout method to use")
    parser.add_argument("--chunk-size", type=int, default=2000, help="Chunk size")
    parser.add_argument("--socket-timeout", type=int, default=60, help="Socket timeout")
    parser.add_argument("--query-timeout", type=int, default=120, help="Query timeout")
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    logger.info(f"🖥️ Platform: {platform.system()}")
    logger.info(f"🐍 Python: {platform.python_version()}")
    logger.info(f"🔧 Timeout method: {args.timeout_method}")
    
    # Create converter
    converter = CrossPlatformFalkorDBConverter(
        timeout_method=args.timeout_method,
        chunk_size=args.chunk_size,
        socket_timeout=args.socket_timeout,
        query_timeout=args.query_timeout
    )
    
    try:
        # Process file
        stats = converter.process_file_cross_platform(args.rdf_file, args.graph_name)
        
        logger.info("✅ Processing completed successfully!")
        logger.info(f"📊 Final stats: {stats}")
        
        return 0
        
    except Exception as e:
        logger.error(f"❌ Processing failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
