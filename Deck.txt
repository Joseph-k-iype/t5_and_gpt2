# Complete FalkorDB Setup and Usage Guide

## 1. Environment Setup

### Prerequisites
```bash
# Python 3.8+ required
python --version

# Install pip if not available
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python get-pip.py
```

### Install Required Dependencies
```bash
# Create virtual environment (recommended)
python -m venv rdf_converter_env
source rdf_converter_env/bin/activate  # On Windows: rdf_converter_env\Scripts\activate

# Install core dependencies
pip install rdflib numpy scipy pandas

# Install database drivers
pip install falkordb neo4j

# Optional performance libraries
pip install pyarrow  # For better compression
pip install psutil   # For memory monitoring
```

## 2. FalkorDB Installation

### Option 1: Docker (Recommended)
```bash
# Pull and run FalkorDB
docker run -p 6379:6379 -it --rm falkordb/falkordb:latest

# Or with persistence
docker run -p 6379:6379 -v ./data:/data -it --rm falkordb/falkordb:latest
```

### Option 2: Local Installation
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install redis-server
wget https://github.com/FalkorDB/FalkorDB/releases/latest/download/falkordb.so
# Follow FalkorDB installation instructions

# macOS
brew install redis
# Download FalkorDB module and configure
```

### Option 3: Cloud Services
- **Redis Cloud**: Create FalkorDB instance
- **AWS/GCP/Azure**: Deploy FalkorDB container

## 3. Verify FalkorDB Installation

### Test Connection
```python
import falkordb

# Test connection
try:
    db = falkordb.FalkorDB(host='localhost', port=6379)
    graph = db.select_graph("test")
    result = graph.query("RETURN 'Hello FalkorDB' AS message")
    print("‚úÖ FalkorDB connection successful!")
    print(f"Result: {result.result_set}")
except Exception as e:
    print(f"‚ùå FalkorDB connection failed: {e}")
```

## 4. Prepare Sample RDF Data

### Option 1: Create Sample Data
```bash
# Create sample.ttl file
cat > sample.ttl << 'EOF'
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix ex: <http://example.org/> .

ex:person1 rdf:type foaf:Person ;
           foaf:name "Alice Johnson" ;
           foaf:age 30 ;
           foaf:email "alice@example.com" ;
           ex:worksAt ex:company1 .

ex:person2 rdf:type foaf:Person ;
           foaf:name "Bob Smith" ;
           foaf:age 25 ;
           foaf:email "bob@example.com" ;
           ex:worksAt ex:company1 .

ex:person3 rdf:type foaf:Person ;
           foaf:name "Carol Davis" ;
           foaf:age 35 ;
           foaf:email "carol@example.com" ;
           ex:worksAt ex:company2 .

ex:company1 rdf:type ex:Organization ;
            rdfs:label "Tech Corp" ;
            ex:industry "Technology" ;
            ex:location "San Francisco" .

ex:company2 rdf:type ex:Organization ;
            rdfs:label "Data Inc" ;
            ex:industry "Data Science" ;
            ex:location "New York" .

ex:person1 foaf:knows ex:person2 .
ex:person2 foaf:knows ex:person3 .
ex:person1 ex:hasSkill ex:skill1 .
ex:person2 ex:hasSkill ex:skill2 .

ex:skill1 rdf:type ex:Skill ;
          rdfs:label "Python Programming" ;
          ex:level "Expert" .

ex:skill2 rdf:type ex:Skill ;
          rdfs:label "Data Analysis" ;
          ex:level "Intermediate" .
EOF
```

### Option 2: Download Real RDF Data
```bash
# Download sample datasets
wget https://www.w3.org/People/Berners-Lee/card.rdf -O berners-lee.rdf
wget http://dbpedia.org/data/Berlin.ttl -O berlin.ttl

# Or use larger datasets
# wget http://downloads.dbpedia.org/2016-10/core-i18n/en/persondata_en.ttl.bz2
# bunzip2 persondata_en.ttl.bz2
```

## 5. Save the Converter Code

### Create the main converter file
```bash
# Save the integrated converter code as rdf_converter.py
# (Copy the UltraHighPerformanceRDFConverter code from the previous artifact)
```

### Create a simplified runner script
```python
# save as run_converter.py
import sys
import logging
from rdf_converter import UltraHighPerformanceRDFConverter

def main():
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    if len(sys.argv) < 2:
        print("Usage: python run_converter.py <rdf_file> [options]")
        print("Options:")
        print("  --falkor              Upload to FalkorDB")
        print("  --neo4j               Upload to Neo4j")  
        print("  --graph-name NAME     FalkorDB graph name (default: rdf_graph)")
        print("  --falkor-host HOST    FalkorDB host (default: localhost)")
        print("  --falkor-port PORT    FalkorDB port (default: 6379)")
        print("  --chunk-size SIZE     Processing chunk size (default: 10000)")
        return
    
    rdf_file = sys.argv[1]
    
    # Parse arguments
    upload_falkor = '--falkor' in sys.argv
    upload_neo4j = '--neo4j' in sys.argv
    
    # Get optional parameters
    graph_name = "rdf_graph"
    falkor_host = "localhost"
    falkor_port = 6379
    chunk_size = 10000
    
    # Parse named arguments
    args = sys.argv[2:]
    i = 0
    while i < len(args):
        if args[i] == '--graph-name' and i + 1 < len(args):
            graph_name = args[i + 1]
            i += 2
        elif args[i] == '--falkor-host' and i + 1 < len(args):
            falkor_host = args[i + 1]
            i += 2
        elif args[i] == '--falkor-port' and i + 1 < len(args):
            falkor_port = int(args[i + 1])
            i += 2
        elif args[i] == '--chunk-size' and i + 1 < len(args):
            chunk_size = int(args[i + 1])
            i += 2
        else:
            i += 1
    
    print(f"üöÄ Starting RDF conversion for: {rdf_file}")
    print(f"üìä Configuration:")
    print(f"   - Chunk size: {chunk_size}")
    print(f"   - Upload to FalkorDB: {upload_falkor}")
    print(f"   - FalkorDB host: {falkor_host}:{falkor_port}")
    print(f"   - Graph name: {graph_name}")
    
    # Create converter with optimizations
    converter = UltraHighPerformanceRDFConverter(
        chunk_size=chunk_size,
        expected_entities=100000,  # Adjust based on your data size
        falkor_host=falkor_host,
        falkor_port=falkor_port,
        enable_string_interning=True,
        enable_bloom_filter=True,
        enable_vectorization=True,
        log_level=logging.INFO
    )
    
    try:
        # Process the file
        stats = converter.process_file_ultra_performance(
            file_path=rdf_file,
            upload_to_falkor=upload_falkor,
            upload_to_neo4j=upload_neo4j,
            falkor_graph_name=graph_name
        )
        
        # Print results
        print(f"\n‚úÖ Processing completed successfully!")
        print(f"üìà Results:")
        print(f"   - Triples processed: {stats['total_triples']:,}")
        print(f"   - Nodes created: {stats['total_nodes']:,}")
        print(f"   - Relationships created: {stats['total_relationships']:,}")
        print(f"   - Processing time: {stats['total_time']:.2f} seconds")
        print(f"   - Throughput: {stats['triples_per_second']:,.0f} triples/second")
        
        # Show optimization stats
        opt_stats = stats['optimization_stats']
        print(f"\nüîß Optimization Performance:")
        print(f"   - Bloom filter hit rate: {opt_stats['bloom_hit_rate']:.1f}%")
        print(f"   - Total entities: {opt_stats['total_entities']:,}")
        print(f"   - Namespace cache size: {opt_stats['namespace_cache_size']:,}")
        
        if upload_falkor and stats.get('upload_results', {}).get('falkor_success'):
            print(f"\nüéØ FalkorDB upload successful!")
            print(f"   - Graph name: {graph_name}")
            print(f"   - Connection: {falkor_host}:{falkor_port}")
        
    except Exception as e:
        print(f"‚ùå Error during processing: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

## 6. Run the Converter

### Basic Usage
```bash
# Simple conversion (no database upload)
python run_converter.py sample.ttl

# Convert and upload to FalkorDB
python run_converter.py sample.ttl --falkor

# Custom configuration
python run_converter.py sample.ttl --falkor \
    --graph-name "my_knowledge_graph" \
    --falkor-host "localhost" \
    --falkor-port 6379 \
    --chunk-size 5000
```

### Advanced Usage Examples
```bash
# Large dataset with optimizations
python run_converter.py large_dataset.ttl --falkor \
    --graph-name "large_graph" \
    --chunk-size 50000

# Multiple uploads
python run_converter.py sample.ttl --falkor --neo4j \
    --graph-name "multi_db_graph"

# Remote FalkorDB
python run_converter.py sample.ttl --falkor \
    --falkor-host "your-falkor-server.com" \
    --falkor-port 6379 \
    --graph-name "remote_graph"
```

## 7. Verify Data in FalkorDB

### Query the Data
```python
# verify_data.py
import falkordb

def verify_falkordb_data(graph_name="rdf_graph", host="localhost", port=6379):
    """Verify the uploaded data in FalkorDB."""
    
    try:
        # Connect to FalkorDB
        db = falkordb.FalkorDB(host=host, port=port)
        graph = db.select_graph(graph_name)
        
        print(f"üîç Verifying data in graph '{graph_name}'")
        print("=" * 50)
        
        # Basic statistics
        node_count_result = graph.query("MATCH (n) RETURN count(n) AS node_count")
        node_count = node_count_result.result_set[0][0] if node_count_result.result_set else 0
        
        rel_count_result = graph.query("MATCH ()-[r]->() RETURN count(r) AS rel_count") 
        rel_count = rel_count_result.result_set[0][0] if rel_count_result.result_set else 0
        
        print(f"üìä Graph Statistics:")
        print(f"   - Total nodes: {node_count:,}")
        print(f"   - Total relationships: {rel_count:,}")
        
        # Sample nodes
        print(f"\nüîé Sample nodes:")
        sample_nodes = graph.query("MATCH (n) RETURN n LIMIT 5")
        for i, row in enumerate(sample_nodes.result_set):
            print(f"   Node {i+1}: {row[0]}")
        
        # Node types/labels
        print(f"\nüè∑Ô∏è  Node labels:")
        labels_result = graph.query("MATCH (n) RETURN DISTINCT labels(n) AS labels LIMIT 10")
        for row in labels_result.result_set:
            print(f"   Labels: {row[0]}")
        
        # Sample relationships
        print(f"\nüîó Sample relationships:")
        sample_rels = graph.query("MATCH (a)-[r]->(b) RETURN a, type(r), b LIMIT 5")
        for i, row in enumerate(sample_rels.result_set):
            print(f"   Rel {i+1}: {row[0]} -[{row[1]}]-> {row[2]}")
        
        # Advanced queries
        print(f"\nüîç Advanced queries:")
        
        # Find nodes with most connections
        degree_query = """
        MATCH (n)-[r]-()
        RETURN n, count(r) AS degree
        ORDER BY degree DESC
        LIMIT 5
        """
        degree_result = graph.query(degree_query)
        print(f"   Top connected nodes:")
        for row in degree_result.result_set:
            print(f"     {row[0]} (degree: {row[1]})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Verification failed: {e}")
        return False

# Run verification
if __name__ == "__main__":
    import sys
    graph_name = sys.argv[1] if len(sys.argv) > 1 else "rdf_graph"
    verify_falkordb_data(graph_name)
```

### Interactive Queries
```bash
# Run verification
python verify_data.py rdf_graph

# Or connect directly with redis-cli
redis-cli
> GRAPH.QUERY rdf_graph "MATCH (n) RETURN count(n)"
> GRAPH.QUERY rdf_graph "MATCH (n:foaf_Person) RETURN n.foaf_name LIMIT 5"
```

## 8. Monitoring and Performance

### Monitor Processing
```python
# monitor.py - Real-time monitoring script
import time
import psutil
import falkordb

def monitor_processing(graph_name="rdf_graph"):
    """Monitor the conversion process."""
    
    print("üîç Monitoring RDF conversion process...")
    
    while True:
        try:
            # System resources
            memory = psutil.virtual_memory()
            cpu = psutil.cpu_percent(interval=1)
            
            # FalkorDB stats
            db = falkordb.FalkorDB(host='localhost', port=6379)
            graph = db.select_graph(graph_name)
            
            try:
                node_count_result = graph.query("MATCH (n) RETURN count(n) AS count")
                node_count = node_count_result.result_set[0][0] if node_count_result.result_set else 0
                
                rel_count_result = graph.query("MATCH ()-[r]->() RETURN count(r) AS count")
                rel_count = rel_count_result.result_set[0][0] if rel_count_result.result_set else 0
            except:
                node_count = rel_count = 0
            
            print(f"\rüíª CPU: {cpu:5.1f}% | üíæ RAM: {memory.percent:5.1f}% | "
                  f"üìä Nodes: {node_count:,} | üîó Rels: {rel_count:,}", end="", flush=True)
            
            time.sleep(2)
            
        except KeyboardInterrupt:
            print(f"\n‚úÖ Monitoring stopped")
            break
        except Exception as e:
            print(f"\n‚ùå Monitor error: {e}")
            time.sleep(5)

if __name__ == "__main__":
    monitor_processing()
```

## 9. Troubleshooting

### Common Issues and Solutions

#### FalkorDB Connection Issues
```bash
# Check if FalkorDB is running
docker ps | grep falkordb
# or
redis-cli ping

# Check network connectivity
telnet localhost 6379

# Restart FalkorDB
docker restart <falkordb_container_id>
```

#### Memory Issues
```python
# Reduce chunk size for large files
converter = UltraHighPerformanceRDFConverter(
    chunk_size=1000,  # Smaller chunks
    max_memory_mb=2048,  # Lower memory limit
    expected_entities=50000  # Conservative estimate
)
```

#### Performance Tuning
```python
# For large datasets
converter = UltraHighPerformanceRDFConverter(
    chunk_size=100000,  # Larger chunks for better throughput
    node_batch_size=20000,  # Larger batches
    rel_batch_size=10000,
    max_memory_mb=16384,  # Use more RAM if available
    expected_entities=10000000  # Pre-size for large datasets
)
```

## 10. Production Deployment

### Docker Compose Setup
```yaml
# docker-compose.yml
version: '3.8'
services:
  falkordb:
    image: falkordb/falkordb:latest
    ports:
      - "6379:6379"
    volumes:
      - falkordb_data:/data
    restart: unless-stopped
  
  rdf-converter:
    build: .
    depends_on:
      - falkordb
    volumes:
      - ./data:/app/data
      - ./output:/app/output
    environment:
      - FALKOR_HOST=falkordb
      - FALKOR_PORT=6379

volumes:
  falkordb_data:
```

### Dockerfile for Converter
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Run converter
CMD ["python", "run_converter.py"]
```

This complete guide will get you up and running with the optimized RDF converter and FalkorDB integration. Start with the sample data to verify everything works, then move to your actual RDF datasets.
