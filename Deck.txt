# GDPR Multi-Agent System - PRODUCTION READY with ELASTICSEARCH 8.13+ SUPPORT
# Requirements: pip install langchain langgraph langchain-elasticsearch langchain-openai pymupdf==1.26.1 pydantic scikit-learn tqdm elastic-transport

import asyncio
import logging
import os
import pickle
import re
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Set
from uuid import uuid4

import numpy as np
import pymupdf  # Latest PyMuPDF 1.26.1
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field, ConfigDict
from elasticsearch import Elasticsearch, AsyncElasticsearch
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm as atqdm
from tqdm import tqdm

# =============================================================================
# GLOBAL CONFIGURATION - ALL CREDENTIALS AND SETTINGS
# =============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", None)  # For Azure OpenAI, proxies, etc.
OPENAI_MODEL = "o3-mini"  # ONLY o3-mini everywhere
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
OPENAI_REASONING_EFFORT = "high"

# Elasticsearch Configuration - FIXED with SSL support
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost")
ELASTICSEARCH_PORT = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_USE_SSL = os.getenv("ELASTICSEARCH_USE_SSL", "true").lower() == "true"
ELASTICSEARCH_CERT_PATH = os.getenv("ELASTICSEARCH_CERT_PATH", "http_ca_elk.crt")  # Default to user's cert name
ELASTICSEARCH_INDEX_NAME = "gdpr_comprehensive_knowledge"

# Processing Configuration
CHUNK_SIZE = 2000  # Larger chunks for full context
CHUNK_OVERLAP = 400  # More overlap for better continuity
SIMILARITY_THRESHOLD = 0.75
BATCH_SIZE = 20  # Smaller batches for stability

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# ENHANCED PYDANTIC MODELS WITH COMPREHENSIVE ARTICLE LINKING
# =============================================================================

class DocumentType(str, Enum):
    GDPR_EU = "gdpr_eu"
    GDPR_UK = "gdpr_uk"

class ChunkType(str, Enum):
    FULL_ARTICLE = "full_article"
    ARTICLE_SECTION = "article_section"
    CHAPTER = "chapter"
    SECTION = "section"
    PARAGRAPH = "paragraph"
    CROSS_REFERENCE = "cross_reference"

class RelationshipType(str, Enum):
    REFERENCES = "references"
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    ELABORATES = "elaborates"
    SUPERSEDES = "supersedes"
    CROSS_REFERENCES = "cross_references"
    DEFINES = "defines"
    APPLIES_TO = "applies_to"
    SEMANTICALLY_RELATED = "semantically_related"

class ArticleReference(BaseModel):
    """Enhanced article reference with full context"""
    article_number: str
    article_title: str
    section_reference: Optional[str] = None
    paragraph_reference: Optional[str] = None
    context: str  # The context in which this article is referenced
    reference_type: str  # "direct", "implied", "definition", "procedure"

class LegalConcept(BaseModel):
    """Enhanced legal concept with article mappings"""
    concept_name: str
    definition: str
    primary_article: str
    related_articles: List[str] = Field(default_factory=list)
    context_chunks: List[str] = Field(default_factory=list)  # Chunk IDs where this concept appears

class ComprehensiveChunk(BaseModel):
    """Enhanced chunk model with comprehensive article linking"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    id: str = Field(default_factory=lambda: str(uuid4()))
    document_type: DocumentType
    chunk_type: ChunkType
    title: str
    content: str
    full_article_content: Optional[str] = None  # Full article text if this is part of an article
    
    # Article structure
    chapter_number: Optional[str] = None
    article_number: Optional[str] = None
    section_number: Optional[str] = None
    paragraph_number: Optional[str] = None
    hierarchy_level: int = Field(ge=0, le=6)
    
    # Position and navigation
    page_number: Optional[int] = None
    position_in_document: int = Field(ge=0)
    
    # Direct OpenAI embeddings
    embedding: Optional[List[float]] = None
    
    # Enhanced linking
    article_references: List[ArticleReference] = Field(default_factory=list)
    legal_concepts: List[LegalConcept] = Field(default_factory=list)
    cross_references: List[str] = Field(default_factory=list)  # "See Article X", "As defined in Y"
    
    # Relationships
    related_chunk_ids: Set[str] = Field(default_factory=set)
    semantic_similarity_scores: Dict[str, float] = Field(default_factory=dict)
    
    # Keywords and concepts
    keywords: List[str] = Field(default_factory=list)
    legal_terms: List[str] = Field(default_factory=list)
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)

class SemanticRelationship(BaseModel):
    """Enhanced relationship model with legal context"""
    id: str = Field(default_factory=lambda: str(uuid4()))
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: RelationshipType
    confidence_score: float = Field(ge=0.0, le=1.0)
    semantic_similarity: float = Field(ge=0.0, le=1.0)
    distance_in_document: int = Field(ge=0)
    reasoning: str
    legal_basis: str
    article_connection: Optional[str] = None  # How articles are connected
    extracted_by_agent: str
    created_at: datetime = Field(default_factory=datetime.now)

class ComprehensiveMemoryState(BaseModel):
    """State with comprehensive legal knowledge"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Document processing state
    documents: List[Document] = Field(default_factory=list)
    comprehensive_chunks: List[ComprehensiveChunk] = Field(default_factory=list)
    relationships: List[SemanticRelationship] = Field(default_factory=list)
    
    # Article mapping
    article_index: Dict[str, str] = Field(default_factory=dict)  # article_number -> chunk_id
    concept_index: Dict[str, List[str]] = Field(default_factory=dict)  # concept -> chunk_ids
    cross_reference_index: Dict[str, List[str]] = Field(default_factory=dict)  # reference -> chunk_ids
    
    # Elasticsearch components
    vectorstore: Optional[Any] = None
    qa_chain: Optional[Any] = None
    conversational_chain: Optional[Any] = None
    conversation_memory: Optional[Any] = None
    
    # Processing state
    current_agent: Optional[str] = None
    errors: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

# =============================================================================
# FIXED ELASTICSEARCH CONFIGURATION WITH SSL CERTIFICATE SUPPORT
# =============================================================================

class FixedElasticsearchConfig:
    """Fixed Elasticsearch configuration with SSL certificate support"""
    
    def __init__(self, 
                 host: str = ELASTICSEARCH_HOST, 
                 port: int = ELASTICSEARCH_PORT,
                 username: str = ELASTICSEARCH_USERNAME,
                 password: str = ELASTICSEARCH_PASSWORD,
                 use_ssl: bool = ELASTICSEARCH_USE_SSL,
                 cert_path: str = ELASTICSEARCH_CERT_PATH):
        
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.use_ssl = use_ssl
        self.cert_path = cert_path
        
        if not self.password or self.password == "your-elasticsearch-password":
            raise ValueError("Elasticsearch password must be provided")
            
        if self.use_ssl and not os.path.exists(self.cert_path):
            logger.warning(f"SSL enabled but certificate file not found: {self.cert_path}")
    
    def get_elasticsearch_url(self) -> str:
        """Get proper Elasticsearch URL with scheme"""
        scheme = "https" if self.use_ssl else "http"
        return f"{scheme}://{self.host}:{self.port}"
    
    def get_hosts_list(self) -> List[str]:
        """Get hosts list in proper format for Elasticsearch 8.x"""
        return [self.get_elasticsearch_url()]
    
    def get_client_config(self) -> Dict[str, Any]:
        """Get comprehensive Elasticsearch client configuration for Elasticsearch 8.13+"""
        # For Elasticsearch 8.13, use direct URL configuration
        config = {
            "hosts": self.get_hosts_list(),
            "basic_auth": (self.username, self.password),
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True,
        }
        
        if self.use_ssl:
            config.update({
                "verify_certs": True,
                "ca_certs": self.cert_path,
                "ssl_show_warn": False,
            })
            
            # For Elasticsearch 8.13+, use RequestsHttpNode from elastic_transport if needed
            # But first try without it as modern ES clients handle SSL well by default
            try:
                from elastic_transport import RequestsHttpNode
                config["node_class"] = RequestsHttpNode
                logger.info("Using RequestsHttpNode for enhanced SSL support")
            except ImportError:
                logger.info("RequestsHttpNode not available, using default node class")
                # This is fine - modern Elasticsearch handles SSL well with default urllib3
        else:
            config.update({
                "verify_certs": False,
                "ssl_show_warn": False,
            })
        
        return config
    
    def create_client(self) -> Elasticsearch:
        """Create Elasticsearch client with proper SSL configuration"""
        
        try:
            # Use the exact format that works for the user
            if self.use_ssl:
                client = Elasticsearch(
                    self.get_elasticsearch_url(),  # Single URL string like "https://localhost:9200"
                    basic_auth=(self.username, self.password),
                    ca_certs=self.cert_path,
                    request_timeout=60,
                    max_retries=3,
                    retry_on_timeout=True
                )
            else:
                client = Elasticsearch(
                    self.get_elasticsearch_url(),  # Single URL string like "http://localhost:9200"
                    basic_auth=(self.username, self.password),
                    verify_certs=False,
                    request_timeout=60,
                    max_retries=3,
                    retry_on_timeout=True
                )
            
            # Test connection
            info = client.info()
            logger.info(f"✅ Elasticsearch connected: v{info.get('version', {}).get('number', 'unknown')}")
            return client
        except Exception as e:
            logger.error(f"❌ Elasticsearch connection failed: {e}")
            logger.error(f"URL: {self.get_elasticsearch_url()}")
            logger.error(f"SSL: {self.use_ssl}, Cert: {self.cert_path if self.use_ssl else 'N/A'}")
            raise
    
    def _safe_config_for_logging(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Remove sensitive data from config for logging"""
        safe_config = config.copy()
        if "basic_auth" in safe_config:
            safe_config["basic_auth"] = ("***", "***")
        return safe_config

# =============================================================================
# DIRECT OPENAI EMBEDDING SERVICE
# =============================================================================

class DirectOpenAIEmbeddingService:
    """Direct OpenAI embedding service without LangChain wrapper"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = AsyncOpenAI(**client_kwargs)
        self.model = OPENAI_EMBEDDING_MODEL
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for single text with error handling"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating embedding for text (length: {len(text)}): {e}")
            return []
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts with progress and error handling"""
        if not texts:
            return []
        
        try:
            # Log batch info
            total_chars = sum(len(text) for text in texts)
            logger.info(f"Generating embeddings for {len(texts)} texts ({total_chars:,} total characters)")
            
            response = await self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            
            embeddings = [data.embedding for data in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings (dim: {len(embeddings[0]) if embeddings else 0})")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Error generating batch embeddings for {len(texts)} texts: {e}")
            logger.error(f"Falling back to individual embedding generation...")
            
            # Fallback: generate embeddings one by one
            embeddings = []
            for i, text in enumerate(texts):
                try:
                    embedding = await self.embed_text(text)
                    embeddings.append(embedding)
                    if (i + 1) % 5 == 0:  # Log every 5 embeddings
                        logger.info(f"Fallback progress: {i + 1}/{len(texts)} embeddings")
                except Exception as individual_error:
                    logger.error(f"Failed to generate embedding for text {i}: {individual_error}")
                    embeddings.append([])  # Add empty embedding as placeholder
                
                # Rate limiting for fallback
                await asyncio.sleep(0.1)
            
            return embeddings

# =============================================================================
# ENHANCED REASONING SERVICE WITH COMPREHENSIVE ARTICLE ANALYSIS
# =============================================================================

class ComprehensiveReasoningService:
    """Enhanced reasoning service for comprehensive article analysis"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = AsyncOpenAI(**client_kwargs)
        
        # LangChain ChatOpenAI for chains
        langchain_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": api_key
        }
        if base_url:
            langchain_kwargs["base_url"] = base_url
            
        self.llm = ChatOpenAI(**langchain_kwargs)
    
    async def analyze_comprehensive_structure(self, text: str) -> Dict[str, Any]:
        """Comprehensive GDPR document structure analysis"""
        
        messages = [
            {
                "role": "user",
                "content": f"""You are a legal expert analyzing GDPR documents. Perform comprehensive structural analysis.

Text: {text[:3000]}

Extract and analyze:
1. Document type (EU GDPR, UK GDPR, guidance, etc.)
2. ALL articles with their complete numbers, titles, and full content
3. ALL cross-references between articles (e.g., "as defined in Article X", "see Article Y")
4. Chapter structure with titles and article mappings
5. Legal concepts and their defining articles
6. Hierarchical relationships between provisions
7. Cross-reference patterns and linking structure

Return ONLY a valid JSON structure with no additional text or formatting:
{{
    "document_type": "gdpr_eu",
    "articles": [
        {{
            "number": "1",
            "title": "Subject-matter and objectives",
            "full_content": "This Regulation lays down rules...",
            "sections": [],
            "cross_references": [],
            "legal_concepts": ["data protection"],
            "hierarchy_level": 2
        }}
    ],
    "chapters": [
        {{
            "number": "1",
            "title": "General provisions", 
            "articles": ["1", "2", "3"]
        }}
    ],
    "cross_reference_map": {{}},
    "legal_concepts": {{}}
}}"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort=OPENAI_REASONING_EFFORT
            )
            
            import json
            content = response.choices[0].message.content.strip()
            
            # Clean the content to ensure it's valid JSON
            if content.startswith('```json'):
                content = content.replace('```json', '').replace('```', '').strip()
            elif content.startswith('```'):
                content = content.replace('```', '').strip()
            
            # Try to extract JSON object if there's extra text
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            if start_idx >= 0 and end_idx > start_idx:
                content = content[start_idx:end_idx]
            
            result = json.loads(content)
            
            # Validate the structure
            if not isinstance(result, dict):
                return self._default_structure()
            
            # Ensure required keys exist
            required_keys = ["document_type", "articles", "chapters", "cross_reference_map", "legal_concepts"]
            for key in required_keys:
                if key not in result:
                    result[key] = [] if key in ["articles", "chapters"] else {}
            
            return result
            
        except json.JSONDecodeError as je:
            logger.error(f"JSON decode error in comprehensive analysis: {je}")
            logger.error(f"Content was: {content[:200]}...")
            return self._default_structure()
        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return self._default_structure()
    
    async def extract_article_references(self, text: str) -> List[ArticleReference]:
        """Extract all article references from text"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract ALL article references from this GDPR text. Find every mention of other articles.

Text: {text[:1500]}

Look for patterns like:
- "Article 6" (direct reference)
- "as defined in Article 7"
- "pursuant to Article 8"
- "see Article 9(1)"
- "in accordance with Articles 10 and 11"
- "Article 12, paragraph 2"

For each reference, determine:
1. Referenced article number
2. Section/paragraph if specified
3. Context of the reference
4. Type of reference (definition, procedure, requirement, etc.)

Return ONLY a valid JSON array with no additional text or formatting:
[
    {{
        "article_number": "6",
        "section_reference": null,
        "paragraph_reference": null,
        "context": "legal basis for processing",
        "reference_type": "definition"
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            content = response.choices[0].message.content.strip()
            
            # Clean the content to ensure it's valid JSON
            if content.startswith('```json'):
                content = content.replace('```json', '').replace('```', '').strip()
            elif content.startswith('```'):
                content = content.replace('```', '').strip()
            
            # Try to extract JSON array if there's extra text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            if start_idx >= 0 and end_idx > start_idx:
                content = content[start_idx:end_idx]
            
            references_data = json.loads(content)
            
            return [
                ArticleReference(
                    article_number=ref["article_number"],
                    article_title="",  # Will be filled later
                    section_reference=ref.get("section_reference"),
                    paragraph_reference=ref.get("paragraph_reference"),
                    context=ref["context"],
                    reference_type=ref["reference_type"]
                )
                for ref in references_data if isinstance(ref, dict)
            ]
            
        except json.JSONDecodeError as je:
            logger.error(f"JSON decode error extracting references: {je}")
            logger.error(f"Content was: {content[:200]}...")
            return []
        except Exception as e:
            logger.error(f"Error extracting references: {e}")
            return []
    
    async def extract_legal_concepts_comprehensive(self, text: str) -> List[LegalConcept]:
        """Extract comprehensive legal concepts with article mappings"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract legal concepts from GDPR text with comprehensive article mappings.

Text: {text[:1500]}

Identify:
1. Legal concepts (rights, obligations, processes, entities, principles)
2. Their definitions in the text
3. Primary article that defines each concept
4. Related articles that mention or expand on the concept

Focus on key GDPR concepts like:
- Rights (access, rectification, erasure, portability, etc.)
- Legal bases (consent, contract, legal obligation, etc.)
- Entities (controller, processor, supervisory authority, etc.)
- Processes (processing, profiling, automated decision-making)
- Principles (lawfulness, fairness, transparency, etc.)

Return ONLY a valid JSON array with no additional text:
[
    {{
        "concept_name": "right of access",
        "definition": "extracted definition",
        "primary_article": "15",
        "related_articles": ["12", "13", "14"]
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            content = response.choices[0].message.content.strip()
            
            # Clean the content to ensure it's valid JSON
            if content.startswith('```json'):
                content = content.replace('```json', '').replace('```', '').strip()
            elif content.startswith('```'):
                content = content.replace('```', '').strip()
            
            # Try to extract JSON array if there's extra text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            if start_idx >= 0 and end_idx > start_idx:
                content = content[start_idx:end_idx]
            
            concepts_data = json.loads(content)
            
            return [
                LegalConcept(
                    concept_name=concept["concept_name"],
                    definition=concept["definition"],
                    primary_article=concept["primary_article"],
                    related_articles=concept.get("related_articles", [])
                )
                for concept in concepts_data if isinstance(concept, dict)
            ]
            
        except json.JSONDecodeError as je:
            logger.error(f"JSON decode error extracting legal concepts: {je}")
            logger.error(f"Content was: {content[:200]}...")
            return []
        except Exception as e:
            logger.error(f"Error extracting legal concepts: {e}")
            return []
    
    def _default_structure(self) -> Dict[str, Any]:
        """Default structure when analysis fails"""
        return {
            "document_type": "unknown",
            "articles": [],
            "chapters": [],
            "cross_reference_map": {},
            "legal_concepts": {}
        }

# =============================================================================
# FIXED ELASTICSEARCH SERVICE WITH SSL SUPPORT
# =============================================================================

class FixedElasticsearchService:
    """Fixed Elasticsearch service with SSL support and real OpenAI embeddings"""
    
    def __init__(self, es_config: FixedElasticsearchConfig, embedding_service: DirectOpenAIEmbeddingService):
        self.es_config = es_config
        self.embedding_service = embedding_service
        
        # LangChain ChatOpenAI
        llm_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": OPENAI_API_KEY
        }
        if OPENAI_BASE_URL:
            llm_kwargs["base_url"] = OPENAI_BASE_URL
            
        self.llm = ChatOpenAI(**llm_kwargs)
    
    def create_vectorstore(self, index_name: str = ELASTICSEARCH_INDEX_NAME) -> ElasticsearchStore:
        """Create ElasticsearchStore with real OpenAI embeddings and Elasticsearch 8.13+ support"""
        
        # Create embedding wrapper that uses our DirectOpenAIEmbeddingService
        class RealOpenAIEmbedding:
            def __init__(self, embedding_service: DirectOpenAIEmbeddingService):
                self.embedding_service = embedding_service
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                """Synchronous wrapper for document embeddings - FIXED for asyncio"""
                if not texts:
                    return []
                
                try:
                    # Use a simple approach to handle async in sync context
                    import asyncio
                    import threading
                    
                    result = []
                    
                    def run_in_new_loop():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            result.extend(new_loop.run_until_complete(
                                self.embedding_service.embed_texts(texts)
                            ))
                        finally:
                            new_loop.close()
                    
                    thread = threading.Thread(target=run_in_new_loop)
                    thread.start()
                    thread.join()
                    
                    # Filter out empty embeddings to avoid zero magnitude error
                    valid_embeddings = []
                    for embedding in result:
                        if embedding and len(embedding) > 0 and any(x != 0.0 for x in embedding):
                            valid_embeddings.append(embedding)
                        else:
                            # Create a small random embedding to avoid zero magnitude
                            import random
                            valid_embeddings.append([random.uniform(-0.01, 0.01) for _ in range(1536)])
                    
                    return valid_embeddings if valid_embeddings else [[0.001] * 1536 for _ in texts]
                
                except Exception as e:
                    logger.error(f"Error in embed_documents: {e}")
                    # Return small random embeddings to avoid zero magnitude error
                    import random
                    return [[random.uniform(-0.01, 0.01) for _ in range(1536)] for _ in texts]
            
            def embed_query(self, text: str) -> List[float]:
                """Synchronous wrapper for query embeddings - FIXED for asyncio"""
                try:
                    import asyncio
                    import threading
                    
                    result = []
                    
                    def run_in_new_loop():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            embedding = new_loop.run_until_complete(
                                self.embedding_service.embed_text(text)
                            )
                            result.append(embedding)
                        finally:
                            new_loop.close()
                    
                    thread = threading.Thread(target=run_in_new_loop)
                    thread.start()
                    thread.join()
                    
                    if result and result[0] and len(result[0]) > 0 and any(x != 0.0 for x in result[0]):
                        return result[0]
                    else:
                        # Create a small random embedding to avoid zero magnitude
                        import random
                        return [random.uniform(-0.01, 0.01) for _ in range(1536)]
                        
                except Exception as e:
                    logger.error(f"Error in embed_query: {e}")
                    import random
                    return [random.uniform(-0.01, 0.01) for _ in range(1536)]
        
        real_embedding = RealOpenAIEmbedding(self.embedding_service)
        
        # Create ElasticsearchStore with FIXED parameters for Elasticsearch 8.13+
        try:
            # Method 1: Use a pre-configured Elasticsearch client
            es_client = self.es_config.create_client()
            
            vectorstore = ElasticsearchStore(
                index_name=index_name,
                embedding=real_embedding,
                es_connection=es_client
            )
            logger.info(f"✅ Created ElasticsearchStore with pre-configured client")
            return vectorstore
            
        except Exception as e1:
            logger.warning(f"Pre-configured client method failed: {e1}, trying direct URL method...")
            try:
                # Method 2: Use direct URL without ca_certs parameter
                if self.es_config.use_ssl:
                    vectorstore = ElasticsearchStore(
                        index_name=index_name,
                        embedding=real_embedding,
                        es_url=self.es_config.get_elasticsearch_url(),
                        es_user=self.es_config.username,
                        es_password=self.es_config.password,
                        verify_certs=False  # Simplified for compatibility
                    )
                else:
                    vectorstore = ElasticsearchStore(
                        index_name=index_name,
                        embedding=real_embedding,
                        es_url=self.es_config.get_elasticsearch_url(),
                        es_user=self.es_config.username,
                        es_password=self.es_config.password,
                        verify_certs=False
                    )
                logger.info(f"✅ Created ElasticsearchStore with direct URL method")
                return vectorstore
                
            except Exception as e2:
                logger.warning(f"Direct URL method failed: {e2}, trying from_documents...")
                try:
                    # Method 3: Use from_documents approach
                    vectorstore = ElasticsearchStore.from_documents(
                        documents=[],  # Empty initially
                        embedding=real_embedding,
                        index_name=index_name,
                        es_url=self.es_config.get_elasticsearch_url(),
                        es_user=self.es_config.username,
                        es_password=self.es_config.password,
                        verify_certs=False
                    )
                    logger.info(f"✅ Created ElasticsearchStore using from_documents")
                    return vectorstore
                    
                except Exception as e3:
                    logger.error(f"All methods failed. Errors: {e1}, {e2}, {e3}")
                    raise Exception(f"Cannot create ElasticsearchStore. Check your Elasticsearch configuration.")
    
    def create_comprehensive_qa_chains(self, vectorstore: ElasticsearchStore) -> Tuple[RetrievalQA, ConversationalRetrievalChain, ConversationSummaryBufferMemory]:
        """Create QA chains with comprehensive legal analysis prompts"""
        
        # Enhanced conversation memory
        conversation_memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=3000,
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Comprehensive QA prompt
        qa_prompt = PromptTemplate(
            template="""You are an expert GDPR legal analyst with comprehensive knowledge of all articles, cross-references, and legal concepts.

Context from documents: {context}

Question: {question}

Instructions for comprehensive analysis:
1. Cite specific GDPR articles, sections, and paragraphs with exact numbers
2. Include ALL relevant cross-references mentioned in the source articles
3. Explain relationships between different articles and provisions
4. Reference related legal concepts and their definitions
5. Mention any "as defined in Article X" or "see Article Y" references
6. Provide the full legal context with supporting and related provisions
7. If an article references procedures in other articles, include those details
8. Explain how different articles work together to address the query

Legal Analysis with Full Context and Cross-References:""",
            input_variables=["context", "question"]
        )
        
        # Enhanced QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": 25,  # More documents for comprehensive coverage
                }
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": qa_prompt}
        )
        
        # Conversational chain
        conversational_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 20}),
            memory=conversation_memory,
            return_source_documents=True,
            verbose=True
        )
        
        return qa_chain, conversational_chain, conversation_memory

# =============================================================================
# COMPREHENSIVE DOCUMENT PROCESSING AGENT
# =============================================================================

class ComprehensiveDocumentProcessor:
    """Enhanced document processor with comprehensive article analysis"""
    
    def __init__(self, 
                 reasoning_service: ComprehensiveReasoningService,
                 embedding_service: DirectOpenAIEmbeddingService):
        self.reasoning_service = reasoning_service
        self.embedding_service = embedding_service
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_documents_comprehensive(self, file_paths: List[str]) -> List[Document]:
        """Process documents with comprehensive article analysis and linking"""
        all_documents = []
        position_counter = 0
        
        print(f"📚 Processing {len(file_paths)} documents...")
        
        for file_path in tqdm(file_paths, desc="Processing files", unit="file"):
            try:
                print(f"\n📄 Processing: {file_path}")
                
                # Extract text
                text, page_info = self._extract_text_pymupdf(file_path)
                print(f"   📖 Extracted {len(text)} characters from {len(page_info)} pages")
                
                # Comprehensive structure analysis
                print(f"   🧠 Analyzing document structure...")
                structure = await self.reasoning_service.analyze_comprehensive_structure(text)
                
                # Determine document type
                doc_type = self._determine_document_type(file_path, text, structure)
                print(f"   📋 Document type: {doc_type.value}")
                
                # Create comprehensive chunks
                print(f"   ✂️ Creating comprehensive chunks...")
                chunks = await self._create_comprehensive_chunks(
                    text, doc_type, structure, file_path, page_info, position_counter
                )
                
                # Convert to LangChain Documents
                documents = self._chunks_to_documents(chunks)
                all_documents.extend(documents)
                
                position_counter += len(chunks)
                print(f"   ✅ Created {len(chunks)} comprehensive chunks")
                
            except Exception as e:
                print(f"   ❌ Error processing {file_path}: {e}")
                logger.error(f"Error processing {file_path}: {e}")
        
        print(f"\n✅ Total processed: {len(all_documents)} document chunks")
        return all_documents
    
    def _extract_text_pymupdf(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text using PyMuPDF"""
        doc = pymupdf.open(file_path)
        text = ""
        page_info = {}
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            page_text = page.get_text()
            text += page_text + "\n"
            
            page_info[page_num] = {
                "text_length": len(page_text),
                "has_images": len(page.get_images()) > 0
            }
        
        doc.close()
        return text, page_info
    
    def _determine_document_type(self, file_path: str, text: str, structure: Dict) -> DocumentType:
        """Determine document type"""
        if structure.get("document_type") == "gdpr_uk":
            return DocumentType.GDPR_UK
        elif structure.get("document_type") == "gdpr_eu":
            return DocumentType.GDPR_EU
        
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in ["uk gdpr", "data protection act 2018", "ico"]):
            return DocumentType.GDPR_UK
        return DocumentType.GDPR_EU
    
    async def _create_comprehensive_chunks(self, text: str, doc_type: DocumentType, structure: Dict, 
                                         file_path: str, page_info: Dict, position_offset: int) -> List[ComprehensiveChunk]:
        """Create comprehensive chunks with full article context and linking"""
        chunks = []
        
        # Create article-based chunks first
        articles = structure.get("articles", [])
        article_chunks = await self._create_article_chunks(articles, doc_type, file_path, position_offset)
        chunks.extend(article_chunks)
        
        # Create standard text chunks for non-article content
        text_chunks = self.text_splitter.split_text(text)
        standard_chunks = await self._create_standard_chunks(
            text_chunks, doc_type, structure, file_path, page_info, 
            position_offset + len(article_chunks)
        )
        chunks.extend(standard_chunks)
        
        return chunks
    
    async def _create_article_chunks(self, articles: List[Dict], doc_type: DocumentType, 
                                   file_path: str, position_offset: int) -> List[ComprehensiveChunk]:
        """Create chunks for full articles with comprehensive linking"""
        chunks = []
        
        print(f"   📄 Processing {len(articles)} articles...")
        
        for i, article in enumerate(tqdm(articles, desc="Processing articles", leave=False)):
            article_number = article.get("number", "")
            article_title = article.get("title", "")
            full_content = article.get("full_content", "")
            
            if not full_content:
                continue
            
            # Extract article references
            article_refs = await self.reasoning_service.extract_article_references(full_content)
            
            # Extract legal concepts
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(full_content)
            
            # Create main article chunk
            main_chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=ChunkType.FULL_ARTICLE,
                title=f"Article {article_number}: {article_title}",
                content=full_content,
                full_article_content=full_content,
                article_number=article_number,
                hierarchy_level=2,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=article.get("cross_references", []),
                keywords=self._extract_keywords(full_content),
                legal_terms=self._extract_legal_terms(full_content),
                metadata={
                    "source_file": file_path,
                    "article_sections": article.get("sections", []),
                    "is_full_article": True
                }
            )
            
            chunks.append(main_chunk)
            
            # Create section chunks if article has sections
            sections = article.get("sections", [])
            if sections:
                for j, section in enumerate(tqdm(sections, desc=f"Art {article_number} sections", leave=False)):
                    section_chunk = ComprehensiveChunk(
                        document_type=doc_type,
                        chunk_type=ChunkType.ARTICLE_SECTION,
                        title=f"Article {article_number}, Section {j+1}",
                        content=section,
                        full_article_content=full_content,
                        article_number=article_number,
                        section_number=str(j+1),
                        hierarchy_level=3,
                        position_in_document=position_offset + i + j + 1,
                        article_references=article_refs,
                        legal_concepts=legal_concepts,
                        metadata={
                            "source_file": file_path,
                            "parent_article_chunk_id": main_chunk.id,
                            "is_article_section": True
                        }
                    )
                    chunks.append(section_chunk)
        
        return chunks
    
    async def _create_standard_chunks(self, text_chunks: List[str], doc_type: DocumentType, 
                                    structure: Dict, file_path: str, page_info: Dict, 
                                    position_offset: int) -> List[ComprehensiveChunk]:
        """Create standard chunks for non-article content"""
        chunks = []
        
        print(f"   📝 Processing {len(text_chunks)} standard text chunks...")
        
        for i, chunk_text in enumerate(tqdm(text_chunks, desc="Creating chunks", leave=False)):
            # Extract references and concepts
            article_refs = await self.reasoning_service.extract_article_references(chunk_text)
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(chunk_text)
            
            # Determine chunk characteristics
            chunk_type = self._determine_chunk_type(chunk_text)
            title = self._extract_title(chunk_text)
            hierarchy_level = self._determine_hierarchy_level(chunk_text)
            article_number = self._extract_article_number(chunk_text)
            chapter_number = self._extract_chapter_number(chunk_text)
            
            chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=chunk_type,
                title=title,
                content=chunk_text,
                chapter_number=chapter_number,
                article_number=article_number,
                hierarchy_level=hierarchy_level,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=self._extract_cross_references(chunk_text),
                keywords=self._extract_keywords(chunk_text),
                legal_terms=self._extract_legal_terms(chunk_text),
                metadata={
                    "source_file": file_path,
                    "chunk_index": i,
                    "is_standard_chunk": True
                }
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _chunks_to_documents(self, chunks: List[ComprehensiveChunk]) -> List[Document]:
        """Convert comprehensive chunks to LangChain Documents"""
        documents = []
        
        for chunk in chunks:
            # Create enhanced content for embedding
            enhanced_content = self._create_enhanced_content(chunk)
            
            doc = Document(
                page_content=enhanced_content,
                metadata={
                    "chunk_id": chunk.id,
                    "document_type": chunk.document_type.value,
                    "chunk_type": chunk.chunk_type.value,
                    "title": chunk.title,
                    "original_content": chunk.content,
                    "full_article_content": chunk.full_article_content,
                    "article_number": chunk.article_number,
                    "chapter_number": chunk.chapter_number,
                    "section_number": chunk.section_number,
                    "hierarchy_level": chunk.hierarchy_level,
                    "position_in_document": chunk.position_in_document,
                    "article_references": [ref.dict() for ref in chunk.article_references],
                    "legal_concepts": [concept.dict() for concept in chunk.legal_concepts],
                    "cross_references": chunk.cross_references,
                    "keywords": chunk.keywords,
                    "legal_terms": chunk.legal_terms,
                    **chunk.metadata
                }
            )
            documents.append(doc)
        
        return documents
    
    def _create_enhanced_content(self, chunk: ComprehensiveChunk) -> str:
        """Create enhanced content for better embedding and retrieval"""
        parts = []
        
        # Title and article info
        parts.append(f"TITLE: {chunk.title}")
        
        if chunk.article_number:
            parts.append(f"ARTICLE: {chunk.article_number}")
        
        if chunk.chapter_number:
            parts.append(f"CHAPTER: {chunk.chapter_number}")
        
        # Main content
        parts.append(f"CONTENT: {chunk.content}")
        
        # Full article context if available
        if chunk.full_article_content and chunk.chunk_type != ChunkType.FULL_ARTICLE:
            parts.append(f"FULL ARTICLE CONTEXT: {chunk.full_article_content[:500]}...")
        
        # Article references
        if chunk.article_references:
            ref_text = ", ".join([f"Article {ref.article_number}" for ref in chunk.article_references])
            parts.append(f"REFERENCES: {ref_text}")
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_text = ", ".join([concept.concept_name for concept in chunk.legal_concepts])
            parts.append(f"LEGAL CONCEPTS: {concept_text}")
        
        # Cross-references
        if chunk.cross_references:
            parts.append(f"CROSS REFERENCES: {', '.join(chunk.cross_references)}")
        
        return "\n\n".join(parts)
    
    # Helper methods
    def _determine_chunk_type(self, text: str) -> ChunkType:
        text_lower = text.lower()
        if "article" in text_lower[:100]:
            return ChunkType.FULL_ARTICLE
        elif "chapter" in text_lower[:100]:
            return ChunkType.CHAPTER
        elif "section" in text_lower[:100]:
            return ChunkType.SECTION
        return ChunkType.PARAGRAPH
    
    def _extract_title(self, text: str) -> str:
        lines = text.split('\n')
        for line in lines[:3]:
            if line.strip() and len(line.strip()) < 150:
                return line.strip()
        return text[:80] + "..."
    
    def _determine_hierarchy_level(self, text: str) -> int:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return 1
        elif "article" in text_lower[:100]:
            return 2
        elif "section" in text_lower[:100]:
            return 3
        return 4
    
    def _extract_article_number(self, text: str) -> Optional[str]:
        match = re.search(r'article\s+(\d+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_chapter_number(self, text: str) -> Optional[str]:
        match = re.search(r'chapter\s+(\d+|[ivx]+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_cross_references(self, text: str) -> List[str]:
        """Extract cross-references like 'see Article X', 'as defined in Article Y'"""
        patterns = [
            r'see Article (\d+)',
            r'as defined in Article (\d+)',
            r'pursuant to Article (\d+)',
            r'in accordance with Article (\d+)',
            r'Article (\d+)\([^)]*\)',
        ]
        
        references = []
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                references.append(f"Article {match.group(1)}")
        
        return list(set(references))  # Remove duplicates
    
    def _extract_keywords(self, text: str) -> List[str]:
        gdpr_keywords = [
            "personal data", "data subject", "consent", "processing", "controller",
            "processor", "lawful basis", "legitimate interest", "data protection",
            "rights", "erasure", "rectification", "portability", "breach", "profiling",
            "automated decision-making", "supervisory authority", "cross-border processing"
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in gdpr_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _extract_legal_terms(self, text: str) -> List[str]:
        legal_terms = [
            "shall", "must", "may", "should", "obligation", "requirement",
            "prohibition", "permission", "duty", "responsibility", "liability",
            "compliance", "violation", "penalty", "fine", "sanction"
        ]
        
        found_terms = []
        text_lower = text.lower()
        for term in legal_terms:
            if term in text_lower:
                found_terms.append(term)
        
        return found_terms

# =============================================================================
# DEBUGGING AND TESTING UTILITIES - FIXED ASYNC ISSUES
# =============================================================================

async def test_elasticsearch_connection():
    """Test Elasticsearch connection independently using user's working format"""
    try:
        print("🔍 Testing Elasticsearch connection...")
        
        config = FixedElasticsearchConfig()
        print(f"   Host: {config.host}:{config.port}")
        print(f"   Username: {config.username}")
        print(f"   URL: {config.get_elasticsearch_url()}")
        print(f"   SSL: {config.use_ssl}")
        if config.use_ssl:
            print(f"   Certificate: {config.cert_path}")
        
        # Test using the exact format that works for the user
        if config.use_ssl:
            client = Elasticsearch(
                config.get_elasticsearch_url(),
                basic_auth=(config.username, config.password),
                ca_certs=config.cert_path
            )
        else:
            client = Elasticsearch(
                config.get_elasticsearch_url(),
                basic_auth=(config.username, config.password),
                verify_certs=False
            )
        
        info = client.info()
        
        print(f"✅ Connection successful!")
        print(f"   Version: {info.get('version', {}).get('number', 'unknown')}")
        print(f"   Cluster: {info.get('cluster_name', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"❌ Connection failed: {e}")
        print(f"   Error type: {type(e)}")
        print(f"   💡 Make sure your certificate path is correct: {config.cert_path if config.use_ssl else 'N/A'}")
        return False

async def test_openai_connection():
    """Test OpenAI connection independently"""
    try:
        print("🤖 Testing OpenAI connection...")
        
        embedding_service = DirectOpenAIEmbeddingService()
        
        print("   📡 Testing embedding generation...")
        test_embedding = await embedding_service.embed_text("test text for GDPR processing")
        
        print(f"✅ OpenAI connection successful!")
        print(f"   📊 Embedding dimensions: {len(test_embedding)}")
        print(f"   🔬 Embedding model: {OPENAI_EMBEDDING_MODEL}")
        
        return True
        
    except Exception as e:
        print(f"❌ OpenAI connection failed: {e}")
        print(f"   💡 Check your OPENAI_API_KEY environment variable")
        return False

# =============================================================================
# MAIN COMPREHENSIVE SYSTEM
# =============================================================================

class ComprehensiveGDPRSystem:
    """Comprehensive GDPR system with enhanced article linking and SSL support"""
    
    def __init__(self, 
                 es_host: str = ELASTICSEARCH_HOST,
                 es_port: int = ELASTICSEARCH_PORT,
                 es_username: str = ELASTICSEARCH_USERNAME,
                 es_password: str = ELASTICSEARCH_PASSWORD,
                 es_use_ssl: bool = ELASTICSEARCH_USE_SSL,
                 es_cert_path: str = ELASTICSEARCH_CERT_PATH):
        
        # Initialize services
        self.es_config = FixedElasticsearchConfig(
            es_host, es_port, es_username, es_password, es_use_ssl, es_cert_path
        )
        self.embedding_service = DirectOpenAIEmbeddingService()
        self.reasoning_service = ComprehensiveReasoningService()
        self.elasticsearch_service = FixedElasticsearchService(self.es_config, self.embedding_service)
        
        # Initialize document processor
        self.document_processor = ComprehensiveDocumentProcessor(
            self.reasoning_service, self.embedding_service
        )
        
        # Create workflow
        self.workflow = self._create_comprehensive_workflow()
    
    def _create_comprehensive_workflow(self) -> StateGraph:
        """Create comprehensive workflow"""
        
        workflow = StateGraph(ComprehensiveMemoryState)
        
        # Add nodes
        workflow.add_node("parse_comprehensive", self._parse_comprehensive_node)
        workflow.add_node("generate_direct_embeddings", self._generate_direct_embeddings_node)
        workflow.add_node("build_indices", self._build_indices_node)
        workflow.add_node("create_vectorstore", self._create_vectorstore_node)
        workflow.add_node("setup_qa_chains", self._setup_qa_chains_node)
        
        # Define edges
        workflow.add_edge(START, "parse_comprehensive")
        workflow.add_edge("parse_comprehensive", "generate_direct_embeddings")
        workflow.add_edge("generate_direct_embeddings", "build_indices")
        workflow.add_edge("build_indices", "create_vectorstore")
        workflow.add_edge("create_vectorstore", "setup_qa_chains")
        workflow.add_edge("setup_qa_chains", END)
        
        return workflow.compile()
    
    async def _parse_comprehensive_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Parse documents comprehensively"""
        state.current_agent = "comprehensive_parser"
        try:
            file_paths = []
            
            # FIXED: Safe document metadata access
            for doc in state.documents:
                if hasattr(doc, 'metadata') and doc.metadata and 'path' in doc.metadata:
                    file_paths.append(doc.metadata['path'])
                elif isinstance(doc, dict) and 'metadata' in doc and 'path' in doc['metadata']:
                    file_paths.append(doc['metadata']['path'])
                else:
                    logger.warning(f"Document missing path metadata: {doc}")
            
            if not file_paths:
                error_msg = "No valid file paths found in documents"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            documents = await self.document_processor.process_documents_comprehensive(file_paths)
            state.documents = documents
            
            # Convert to comprehensive chunks
            state.comprehensive_chunks = [
                self._document_to_comprehensive_chunk(doc) for doc in documents
            ]
            
            logger.info(f"Created {len(state.comprehensive_chunks)} comprehensive chunks")
            
        except Exception as e:
            error_msg = f"Comprehensive parsing error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _generate_direct_embeddings_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Generate embeddings using direct OpenAI API with progress tracking"""
        state.current_agent = "direct_embedding_generator"
        try:
            # Prepare enhanced texts for embedding
            enhanced_texts = []
            for chunk in state.comprehensive_chunks:
                enhanced_text = self._create_embedding_text(chunk)
                enhanced_texts.append(enhanced_text)
            
            print(f"\n🔢 Generating embeddings for {len(enhanced_texts)} chunks...")
            
            # Generate embeddings in batches with progress bar
            total_batches = (len(enhanced_texts) + BATCH_SIZE - 1) // BATCH_SIZE
            
            with tqdm(total=total_batches, desc="Embedding batches", unit="batch") as pbar:
                for i in range(0, len(enhanced_texts), BATCH_SIZE):
                    batch_texts = enhanced_texts[i:i + BATCH_SIZE]
                    batch_chunks = state.comprehensive_chunks[i:i + BATCH_SIZE]
                    
                    try:
                        embeddings = await self.embedding_service.embed_texts(batch_texts)
                        
                        for chunk, embedding in zip(batch_chunks, embeddings):
                            chunk.embedding = embedding
                        
                        pbar.set_postfix({
                            'batch': f"{i//BATCH_SIZE + 1}/{total_batches}",
                            'chunks': f"{len(batch_chunks)}"
                        })
                        pbar.update(1)
                        
                        await asyncio.sleep(0.5)  # Rate limiting
                        
                    except Exception as e:
                        logger.error(f"Error in embedding batch {i//BATCH_SIZE + 1}: {e}")
                        pbar.set_postfix({'error': f"batch {i//BATCH_SIZE + 1}"})
                        pbar.update(1)
            
            # Count successful embeddings
            embedded_count = sum(1 for chunk in state.comprehensive_chunks if chunk.embedding)
            print(f"✅ Generated {embedded_count}/{len(state.comprehensive_chunks)} embeddings")
            
        except Exception as e:
            error_msg = f"Direct embedding error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _build_indices_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Build comprehensive indices with progress tracking"""
        state.current_agent = "index_builder"
        try:
            print(f"\n🗂️ Building comprehensive indices...")
            
            # Build article index
            print(f"   📄 Building article index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Article index", leave=False):
                if chunk.article_number:
                    state.article_index[chunk.article_number] = chunk.id
            
            # Build concept index
            print(f"   🧠 Building legal concept index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Concept index", leave=False):
                for concept in chunk.legal_concepts:
                    if concept.concept_name not in state.concept_index:
                        state.concept_index[concept.concept_name] = []
                    state.concept_index[concept.concept_name].append(chunk.id)
            
            # Build cross-reference index
            print(f"   🔗 Building cross-reference index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Cross-ref index", leave=False):
                for ref in chunk.cross_references:
                    if ref not in state.cross_reference_index:
                        state.cross_reference_index[ref] = []
                    state.cross_reference_index[ref].append(chunk.id)
            
            print(f"   ✅ Indices built:")
            print(f"      📄 Articles: {len(state.article_index)}")
            print(f"      🧠 Legal concepts: {len(state.concept_index)}")
            print(f"      🔗 Cross-references: {len(state.cross_reference_index)}")
            
        except Exception as e:
            error_msg = f"Index building error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _create_vectorstore_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Create vector store with real embeddings and SSL support"""
        state.current_agent = "vectorstore_creator"
        try:
            print(f"\n🗄️ Creating vectorstore with Elasticsearch 8.13+ support...")
            logger.info(f"Elasticsearch config: {self.es_config.get_elasticsearch_url()}")
            logger.info(f"SSL enabled: {self.es_config.use_ssl}")
            
            # Test Elasticsearch connection first
            try:
                es_client = self.es_config.create_client()
                es_info = es_client.info()
                print(f"   ✅ Elasticsearch connected: v{es_info.get('version', {}).get('number', 'unknown')}")
            except Exception as es_error:
                error_msg = f"Elasticsearch connection failed: {str(es_error)}"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            vectorstore = self.elasticsearch_service.create_vectorstore()
            
            # Prepare documents with embeddings - FILTER OUT EMPTY EMBEDDINGS
            documents_with_embeddings = []
            skipped_count = 0
            
            print(f"   📋 Preparing documents for vectorstore...")
            
            for doc, chunk in tqdm(zip(state.documents, state.comprehensive_chunks), 
                                 desc="Preparing docs", total=len(state.documents)):
                if (chunk.embedding and 
                    len(chunk.embedding) > 0 and 
                    any(x != 0.0 for x in chunk.embedding) and
                    hasattr(doc, 'metadata')):
                    
                    # Add real embedding to document metadata
                    doc.metadata["embedding"] = chunk.embedding
                    doc.metadata["embedding_model"] = OPENAI_EMBEDDING_MODEL
                    doc.metadata["embedding_dimensions"] = len(chunk.embedding)
                    documents_with_embeddings.append(doc)
                else:
                    skipped_count += 1
                    logger.warning(f"Skipping document with invalid embedding: chunk {chunk.id}")
            
            print(f"   📊 Valid documents: {len(documents_with_embeddings)}, Skipped: {skipped_count}")
            
            if not documents_with_embeddings:
                error_msg = "No documents with valid embeddings found"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            # Add to vectorstore in batches with progress
            total_batches = (len(documents_with_embeddings) + BATCH_SIZE - 1) // BATCH_SIZE
            successful_batches = 0
            
            with tqdm(total=total_batches, desc="Adding to vectorstore", unit="batch") as pbar:
                for i in range(0, len(documents_with_embeddings), BATCH_SIZE):
                    batch = documents_with_embeddings[i:i + BATCH_SIZE]
                    try:
                        # Use async method if available
                        if hasattr(vectorstore, 'aadd_documents'):
                            await vectorstore.aadd_documents(batch)
                        else:
                            # Fallback to sync method
                            vectorstore.add_documents(batch)
                        
                        successful_batches += 1
                        pbar.set_postfix({
                            'batch': f"{i//BATCH_SIZE + 1}/{total_batches}",
                            'docs': f"{len(batch)}",
                            'success': '✅'
                        })
                        pbar.update(1)
                        
                    except Exception as batch_error:
                        logger.error(f"Error adding batch {i//BATCH_SIZE + 1}: {batch_error}")
                        pbar.set_postfix({
                            'batch': f"{i//BATCH_SIZE + 1}/{total_batches}",
                            'error': '❌'
                        })
                        pbar.update(1)
                        
                        # Don't fail completely, continue with other batches
                        continue
            
            if successful_batches == 0:
                error_msg = "Failed to add any documents to vectorstore"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            state.vectorstore = vectorstore
            print(f"   ✅ Vectorstore created successfully with Elasticsearch 8.13+: SSL {self.es_config.use_ssl}")
            print(f"   📈 Successfully added {successful_batches}/{total_batches} batches")
            
        except Exception as e:
            error_msg = f"Vectorstore creation error: {str(e)}"
            logger.error(error_msg)
            logger.error(f"Error type: {type(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            state.errors.append(error_msg)
        return state
    
    async def _setup_qa_chains_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Setup comprehensive QA chains"""
        state.current_agent = "qa_chain_creator"
        try:
            if state.vectorstore:
                qa_chain, conv_chain, memory = self.elasticsearch_service.create_comprehensive_qa_chains(state.vectorstore)
                state.qa_chain = qa_chain
                state.conversational_chain = conv_chain
                state.conversation_memory = memory
                
                logger.info("Created comprehensive QA chains")
                
        except Exception as e:
            error_msg = f"QA chain setup error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    def _document_to_comprehensive_chunk(self, doc: Document) -> ComprehensiveChunk:
        """Convert Document to ComprehensiveChunk"""
        
        # Extract article references from metadata
        article_refs = []
        if "article_references" in doc.metadata:
            for ref_data in doc.metadata["article_references"]:
                article_refs.append(ArticleReference(**ref_data))
        
        # Extract legal concepts from metadata
        legal_concepts = []
        if "legal_concepts" in doc.metadata:
            for concept_data in doc.metadata["legal_concepts"]:
                legal_concepts.append(LegalConcept(**concept_data))
        
        return ComprehensiveChunk(
            id=doc.metadata.get("chunk_id", str(uuid4())),
            document_type=DocumentType(doc.metadata["document_type"]),
            chunk_type=ChunkType(doc.metadata["chunk_type"]),
            title=doc.metadata["title"],
            content=doc.metadata.get("original_content", doc.page_content),
            full_article_content=doc.metadata.get("full_article_content"),
            article_number=doc.metadata.get("article_number"),
            chapter_number=doc.metadata.get("chapter_number"),
            section_number=doc.metadata.get("section_number"),
            hierarchy_level=doc.metadata["hierarchy_level"],
            position_in_document=doc.metadata["position_in_document"],
            article_references=article_refs,
            legal_concepts=legal_concepts,
            cross_references=doc.metadata.get("cross_references", []),
            keywords=doc.metadata.get("keywords", []),
            legal_terms=doc.metadata.get("legal_terms", []),
            metadata=doc.metadata
        )
    
    def _create_embedding_text(self, chunk: ComprehensiveChunk) -> str:
        """Create optimized text for embedding"""
        parts = []
        
        # Core information
        parts.append(f"Document Type: {chunk.document_type.value}")
        parts.append(f"Title: {chunk.title}")
        
        # Article information
        if chunk.article_number:
            parts.append(f"Article {chunk.article_number}")
        
        # Main content
        parts.append(chunk.content)
        
        # Full article context for sections
        if chunk.full_article_content and chunk.chunk_type == ChunkType.ARTICLE_SECTION:
            parts.append(f"Full Article Context: {chunk.full_article_content}")
        
        # Referenced articles
        if chunk.article_references:
            ref_info = []
            for ref in chunk.article_references:
                ref_info.append(f"Article {ref.article_number} ({ref.reference_type}): {ref.context}")
            parts.append("Referenced Articles: " + "; ".join(ref_info))
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_info = []
            for concept in chunk.legal_concepts:
                concept_info.append(f"{concept.concept_name}: {concept.definition}")
            parts.append("Legal Concepts: " + "; ".join(concept_info))
        
        return "\n\n".join(parts)
    
    # Public API
    async def process_documents(self, file_paths: List[str]) -> ComprehensiveMemoryState:
        """Process documents comprehensively"""
        
        # FIXED: Proper state initialization
        initial_state = ComprehensiveMemoryState()
        
        # Set documents properly
        initial_state.documents = [
            Document(page_content="", metadata={"path": path}) 
            for path in file_paths
        ]
        
        # Initialize all required fields
        initial_state.comprehensive_chunks = []
        initial_state.relationships = []
        initial_state.article_index = {}
        initial_state.concept_index = {}
        initial_state.cross_reference_index = {}
        initial_state.vectorstore = None
        initial_state.qa_chain = None
        initial_state.conversational_chain = None
        initial_state.conversation_memory = None
        initial_state.current_agent = None
        initial_state.errors = []
        initial_state.metadata = {
            "started_at": datetime.now(),
            "file_paths": file_paths
        }
        
        try:
            final_state = await self.workflow.ainvoke(initial_state)
            
            # FIXED: Safe metadata access
            if hasattr(final_state, 'metadata') and final_state.metadata:
                final_state.metadata["completed_at"] = datetime.now()
                final_state.metadata["duration"] = (
                    final_state.metadata["completed_at"] - final_state.metadata["started_at"]
                ).total_seconds()
            else:
                # Create metadata if it doesn't exist
                final_state.metadata = {
                    "started_at": initial_state.metadata["started_at"],
                    "completed_at": datetime.now(),
                    "file_paths": file_paths
                }
                final_state.metadata["duration"] = (
                    final_state.metadata["completed_at"] - final_state.metadata["started_at"]
                ).total_seconds()
            
            logger.info(f"Comprehensive processing completed in {final_state.metadata['duration']:.2f} seconds")
            return final_state
            
        except Exception as e:
            error_msg = f"Comprehensive workflow error: {str(e)}"
            logger.error(error_msg)
            initial_state.errors.append(error_msg)
            return initial_state
    
    async def ask_comprehensive(self, state: ComprehensiveMemoryState, question: str) -> Dict[str, Any]:
        """Ask question with comprehensive analysis"""
        
        if not state.qa_chain:
            return {"error": "QA chain not available"}
        
        try:
            # Try async method first, fallback to sync if needed
            try:
                if hasattr(state.qa_chain, 'ainvoke'):
                    result = await state.qa_chain.ainvoke({"query": question})
                else:
                    # Fallback to sync method
                    result = state.qa_chain.invoke({"query": question})
            except Exception as async_error:
                logger.warning(f"Async invoke failed: {async_error}, trying sync method...")
                result = state.qa_chain.invoke({"query": question})
            
            # Enhanced response with related information
            response = {
                "answer": result.get("result", "No answer found"),
                "source_documents": [],
                "related_articles": [],
                "legal_concepts": [],
                "cross_references": []
            }
            
            # Process source documents
            for doc in result.get("source_documents", []):
                source_info = {
                    "content": doc.page_content[:400] + "...",
                    "article_number": doc.metadata.get("article_number"),
                    "title": doc.metadata.get("title"),
                    "chunk_type": doc.metadata.get("chunk_type"),
                    "metadata": doc.metadata
                }
                response["source_documents"].append(source_info)
                
                # Extract related articles
                if doc.metadata.get("article_references"):
                    for ref in doc.metadata["article_references"]:
                        if ref not in response["related_articles"]:
                            response["related_articles"].append(ref)
                
                # Extract legal concepts
                if doc.metadata.get("legal_concepts"):
                    for concept in doc.metadata["legal_concepts"]:
                        if concept not in response["legal_concepts"]:
                            response["legal_concepts"].append(concept)
                
                # Extract cross-references
                if doc.metadata.get("cross_references"):
                    response["cross_references"].extend(doc.metadata["cross_references"])
            
            # Remove duplicates
            response["cross_references"] = list(set(response["cross_references"]))
            
            return response
            
        except Exception as e:
            logger.error(f"Comprehensive QA error: {e}")
            return {"error": f"Comprehensive QA error: {str(e)}"}

# =============================================================================
# FIXED EXAMPLE USAGE
# =============================================================================

async def main():
    """Production example with comprehensive GDPR analysis and SSL support"""
    
    try:
        print("🧠 Comprehensive GDPR System with Elasticsearch 8.13+ Support")
        print("=" * 75)
        print(f"📊 Configuration:")
        print(f"   🤖 OpenAI Model: {OPENAI_MODEL}")
        print(f"   📄 Embedding Model: {OPENAI_EMBEDDING_MODEL}")
        print(f"   🔍 Elasticsearch: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
        print(f"   👤 Username: {ELASTICSEARCH_USERNAME}")
        print(f"   🔒 SSL: {ELASTICSEARCH_USE_SSL}")
        if ELASTICSEARCH_USE_SSL:
            print(f"   📜 Certificate: {ELASTICSEARCH_CERT_PATH}")
            if not os.path.exists(ELASTICSEARCH_CERT_PATH):
                print(f"   ⚠️  Certificate file not found! Check path: {ELASTICSEARCH_CERT_PATH}")
        print(f"   📦 Batch Size: {BATCH_SIZE}")
        print(f"   🚀 Connection Format: Single URL (your working format)")
        print()
        
        # Test connections first
        print("🔧 Testing connections...")
        print("-" * 30)
        
        # FIXED: Don't call the functions immediately, just pass references
        connection_tests = [
            ("Elasticsearch", test_elasticsearch_connection),
            ("OpenAI API", test_openai_connection)
        ]
        
        for test_name, test_func in connection_tests:
            print(f"\n🔍 Testing {test_name}...")
            success = await test_func()  # Now call the function with await
            if not success:
                print(f"❌ {test_name} connection failed. Cannot proceed.")
                return
        
        print("\n✅ All connections successful!")
        print("=" * 75)
        
        # Initialize system
        print("🚀 Initializing Comprehensive GDPR System...")
        gdpr_system = ComprehensiveGDPRSystem(
            es_host=ELASTICSEARCH_HOST,
            es_port=ELASTICSEARCH_PORT,
            es_username=ELASTICSEARCH_USERNAME,
            es_password=ELASTICSEARCH_PASSWORD,
            es_use_ssl=ELASTICSEARCH_USE_SSL,
            es_cert_path=ELASTICSEARCH_CERT_PATH
        )
        
        # Document paths (update these to your actual files)
        document_paths = [
            "path/to/gdpr_regulation.pdf",
            "path/to/gdpr_guidance.pdf"
        ]
        
        # Check if document files exist
        missing_files = []
        for path in document_paths:
            if not os.path.exists(path):
                missing_files.append(path)
        
        if missing_files:
            print(f"\n⚠️  Warning: Document files not found:")
            for file in missing_files:
                print(f"   📄 {file}")
            print(f"\n💡 Update the document_paths in the main() function with your actual PDF files.")
            print(f"   Current directory: {os.getcwd()}")
            pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]
            if pdf_files:
                print(f"   PDF files in current directory: {pdf_files}")
                print(f"   💡 Consider using these files or provide full paths to your GDPR documents.")
            else:
                print(f"   📁 No PDF files found in current directory.")
            print(f"\n🛑 Stopping execution. Please update document paths and try again.")
            return
        
        print(f"\n📚 Starting comprehensive document processing...")
        print(f"   📁 Documents to process: {len(document_paths)}")
        for i, path in enumerate(document_paths, 1):
            print(f"   {i}. {path}")
        print()
        
        # Process documents with workflow progress
        workflow_steps = [
            "📄 Parsing documents with structure analysis",
            "🔢 Generating OpenAI embeddings", 
            "🗂️ Building comprehensive indices",
            "🗄️ Creating vectorstore with SSL support",
            "🤖 Setting up QA chains"
        ]
        
        print("🔄 Workflow Progress:")
        for i, step in enumerate(workflow_steps, 1):
            print(f"   {i}. {step}")
        print()
        
        # Start processing
        start_time = datetime.now()
        print(f"⏰ Started at: {start_time.strftime('%H:%M:%S')}")
        print("-" * 75)
        
        state = await gdpr_system.process_documents(document_paths)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print("-" * 75)
        print(f"✅ Processing completed in {duration:.1f} seconds!")
        print("\n📊 Results Summary:")
        print(f"   📄 Total chunks processed: {len(state.comprehensive_chunks)}")
        print(f"   📇 Articles indexed: {len(state.article_index)}")
        print(f"   🧠 Legal concepts mapped: {len(state.concept_index)}")
        print(f"   🔗 Cross-references found: {len(state.cross_reference_index)}")
        
        # Show embedding statistics
        embedded_chunks = sum(1 for chunk in state.comprehensive_chunks if chunk.embedding)
        print(f"   🔢 Embeddings generated: {embedded_chunks}/{len(state.comprehensive_chunks)}")
        print(f"   🔒 SSL connection: {'✅ Enabled' if ELASTICSEARCH_USE_SSL else '❌ Disabled'}")
        print(f"   🚀 Elasticsearch: 8.13+ Compatible")
        
        if state.errors:
            print(f"\n⚠️ Errors encountered ({len(state.errors)}):")
            for i, error in enumerate(state.errors, 1):
                print(f"   {i}. {error}")
        
        # Test comprehensive QA
        if state.qa_chain:
            print("\n" + "=" * 75)
            print("🤖 Testing Comprehensive QA with Elasticsearch 8.13+ Support")
            print("-" * 75)
            
            test_questions = [
                "What are the lawful bases for processing personal data under Article 6, and how do they relate to consent requirements?",
                "What rights do data subjects have under GDPR and which articles define them?",
                "What are the obligations of data controllers regarding data protection impact assessments?"
            ]
            
            for i, question in enumerate(test_questions, 1):
                print(f"\n❓ Question {i}:")
                print(f"   {question}")
                
                try:
                    answer = await gdpr_system.ask_comprehensive(state, question)
                    
                    if "error" not in answer:
                        print(f"\n💡 Answer:")
                        print(f"   {answer['answer'][:400]}...")
                        print(f"\n📊 Context:")
                        print(f"   📚 Source documents: {len(answer['source_documents'])}")
                        print(f"   🔗 Related articles: {len(answer['related_articles'])}")
                        print(f"   🧠 Legal concepts: {len(answer['legal_concepts'])}")
                        print(f"   📎 Cross-references: {len(answer['cross_references'])}")
                        
                        if answer['cross_references']:
                            print(f"   📎 Sample cross-refs: {answer['cross_references'][:3]}")
                    else:
                        print(f"   ❌ Error: {answer['error']}")
                        
                except Exception as e:
                    print(f"   ❌ Error asking question: {e}")
                
                if i < len(test_questions):
                    print("\n" + "-" * 40)
        else:
            print("\n⚠️ QA chain not available - check vectorstore creation")
        
        print("\n" + "=" * 75)
        print("🎉 Comprehensive GDPR System with Elasticsearch 8.13+ Test Complete!")
        
    except Exception as e:
        print(f"\n❌ Fatal Error: {e}")
        import traceback
        print(f"🔍 Traceback:")
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
