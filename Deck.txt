#!/usr/bin/env python3
"""
TTL-Aware Large File Merger
Proper syntax-aware chunking that preserves TTL structure
"""

import os
import glob
import sys
import time
import tempfile
import shutil
import re
from typing import List, Tuple, Optional, Dict
from rdflib import Graph
import gc

class TTLSyntaxAwareChunker:
    def __init__(self, show_progress: bool = True):
        self.show_progress = show_progress
        self.prefixes = []
        self.base_uri = None
        
    def extract_prefixes_and_base(self, filepath: str) -> Tuple[List[str], Optional[str]]:
        """Extract all @prefix and @base declarations from the file"""
        prefixes = []
        base_uri = None
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('@prefix') or line.startswith('PREFIX'):
                        prefixes.append(line)
                    elif line.startswith('@base') or line.startswith('BASE'):
                        base_uri = line
                    elif line and not line.startswith('#') and not line.startswith('@') and not line.startswith('PREFIX') and not line.startswith('BASE'):
                        # Stop when we hit actual triples
                        break
                        
        except Exception as e:
            if self.show_progress:
                print(f"   ⚠️  Warning: Could not extract prefixes: {e}")
        
        return prefixes, base_uri
    
    def is_complete_statement(self, lines: List[str]) -> bool:
        """Check if the current set of lines forms a complete TTL statement"""
        if not lines:
            return True
            
        # Join all lines and clean up
        combined = ' '.join(line.strip() for line in lines if line.strip())
        
        # Skip comments and directives
        if not combined or combined.startswith('#') or combined.startswith('@') or combined.startswith('PREFIX') or combined.startswith('BASE'):
            return True
        
        # A complete statement must end with a period
        # But we need to be careful about periods inside literals
        in_literal = False
        quote_char = None
        escape_next = False
        
        for i, char in enumerate(combined):
            if escape_next:
                escape_next = False
                continue
                
            if char == '\\':
                escape_next = True
                continue
                
            if char in ['"', "'"]:
                if not in_literal:
                    in_literal = True
                    quote_char = char
                elif char == quote_char:
                    in_literal = False
                    quote_char = None
            
            # Only count periods outside of literals
            if char == '.' and not in_literal:
                # Make sure it's not part of a decimal number or URI
                if i == len(combined) - 1 or combined[i + 1].isspace():
                    return True
        
        return False
    
    def split_file_by_statements(self, filepath: str, target_chunk_size: int) -> List[str]:
        """
        Split TTL file into chunks at statement boundaries
        target_chunk_size is in bytes
        """
        if self.show_progress:
            print(f"   📝 Analyzing TTL syntax for proper chunking...")
        
        # Extract prefixes first
        prefixes, base_uri = self.extract_prefixes_and_base(filepath)
        
        chunk_files = []
        current_chunk_lines = []
        current_chunk_size = 0
        chunk_number = 0
        statement_buffer = []
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                for line_num, line in enumerate(f, 1):
                    # Skip prefix and base declarations in the data (we'll add them to chunks)
                    if line.strip().startswith('@prefix') or line.strip().startswith('PREFIX') or line.strip().startswith('@base') or line.strip().startswith('BASE'):
                        continue
                    
                    # Add line to statement buffer
                    statement_buffer.append(line)
                    line_size = len(line.encode('utf-8'))
                    
                    # Check if we have a complete statement
                    if self.is_complete_statement(statement_buffer):
                        # We have a complete statement, decide whether to add to current chunk
                        statement_size = sum(len(l.encode('utf-8')) for l in statement_buffer)
                        
                        # If adding this statement would exceed chunk size and we have content, start new chunk
                        if (current_chunk_size + statement_size > target_chunk_size and 
                            current_chunk_lines):
                            
                            # Save current chunk
                            chunk_file = self._save_ttl_chunk(
                                current_chunk_lines, prefixes, base_uri, chunk_number
                            )
                            if chunk_file:
                                chunk_files.append(chunk_file)
                                if self.show_progress and len(chunk_files) % 10 == 0:
                                    print(f"      📦 Created {len(chunk_files)} chunks...")
                            
                            # Start new chunk
                            current_chunk_lines = []
                            current_chunk_size = 0
                            chunk_number += 1
                        
                        # Add complete statement to current chunk
                        current_chunk_lines.extend(statement_buffer)
                        current_chunk_size += statement_size
                        
                        # Clear statement buffer
                        statement_buffer = []
                    
                    # Progress indicator for very large files
                    if self.show_progress and line_num % 100000 == 0:
                        print(f"      📖 Processed {line_num:,} lines...")
                
                # Handle any remaining incomplete statement
                if statement_buffer:
                    if self.show_progress:
                        print(f"   ⚠️  Warning: File ends with incomplete statement, adding to last chunk")
                    current_chunk_lines.extend(statement_buffer)
                
                # Save final chunk
                if current_chunk_lines:
                    chunk_file = self._save_ttl_chunk(
                        current_chunk_lines, prefixes, base_uri, chunk_number
                    )
                    if chunk_file:
                        chunk_files.append(chunk_file)
        
        except Exception as e:
            if self.show_progress:
                print(f"   ❌ Error during syntax-aware splitting: {e}")
            return []
        
        if self.show_progress:
            print(f"   ✅ Created {len(chunk_files)} syntax-valid chunks")
        
        return chunk_files
    
    def _save_ttl_chunk(self, lines: List[str], prefixes: List[str], 
                       base_uri: Optional[str], chunk_number: int) -> Optional[str]:
        """Save a chunk with proper TTL structure"""
        try:
            if not hasattr(self, 'temp_dir') or not self.temp_dir:
                self.temp_dir = tempfile.mkdtemp(prefix="ttl_chunks_")
            
            chunk_file = os.path.join(self.temp_dir, f"chunk_{chunk_number:05d}.ttl")
            
            with open(chunk_file, 'w', encoding='utf-8') as f:
                # Write header comment
                f.write(f"# TTL Chunk {chunk_number + 1}\n")
                f.write(f"# Generated by TTL-Aware Chunker\n\n")
                
                # Write base URI if present
                if base_uri:
                    f.write(base_uri + '\n')
                
                # Write all prefixes
                for prefix in prefixes:
                    f.write(prefix + '\n')
                
                if prefixes:
                    f.write('\n')  # Blank line after prefixes
                
                # Write the actual data
                for line in lines:
                    f.write(line)
            
            return chunk_file
            
        except Exception as e:
            if self.show_progress:
                print(f"   ❌ Error saving chunk {chunk_number}: {e}")
            return None

class LargeFileTTLMerger:
    def __init__(self, show_progress: bool = True, chunk_size_mb: int = 100):
        self.show_progress = show_progress
        self.chunk_size_bytes = chunk_size_mb * 1024 * 1024
        self.merged_graph = Graph()
        self.chunker = TTLSyntaxAwareChunker(show_progress)
        self.temp_dirs = []
        self.stats = {
            'files_processed': 0,
            'files_failed': 0,
            'chunks_processed': 0,
            'chunks_failed': 0,
            'total_triples': 0,
            'processing_time': 0,
            'failed_files': [],
            'failed_chunks': []
        }
    
    def _is_large_file(self, filepath: str, threshold_mb: int = 500) -> bool:
        """Determine if a file should be processed as a large file"""
        try:
            size_mb = os.path.getsize(filepath) / (1024 * 1024)
            return size_mb > threshold_mb
        except:
            return False
    
    def _process_small_file(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """Process a regular-sized file directly"""
        try:
            initial_count = len(self.merged_graph)
            
            # Try different encodings
            encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']
            
            for encoding in encodings:
                try:
                    self.merged_graph.parse(filepath, format="turtle", encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    if "index out of range" in str(e).lower():
                        # File might be larger than we thought, try chunking
                        return self._process_large_file(filepath)
                    raise e
            else:
                # Try without specifying encoding
                self.merged_graph.parse(filepath, format="turtle")
            
            final_count = len(self.merged_graph)
            return True, final_count - initial_count, None
            
        except Exception as e:
            return False, 0, str(e)
    
    def _process_large_file(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """Process large file using syntax-aware chunking"""
        try:
            file_size = os.path.getsize(filepath)
            if self.show_progress:
                print(f"   📊 Large file: {self._format_size(file_size)}")
                print(f"   🔄 Using syntax-aware chunking...")
            
            # Create syntax-aware chunks
            chunk_files = self.chunker.split_file_by_statements(filepath, self.chunk_size_bytes)
            
            if not chunk_files:
                return False, 0, "Failed to create valid chunks"
            
            # Process each chunk
            total_loaded = 0
            successful_chunks = 0
            
            for i, chunk_file in enumerate(chunk_files):
                if self.show_progress:
                    print(f"   📦 Processing chunk {i+1}/{len(chunk_files)}")
                
                try:
                    # Validate chunk before processing
                    test_graph = Graph()
                    test_graph.parse(chunk_file, format="turtle")
                    chunk_triples = len(test_graph)
                    
                    if chunk_triples == 0:
                        if self.show_progress:
                            print(f"      ⚠️  Empty chunk, skipping")
                        continue
                    
                    # Merge into main graph
                    initial_count = len(self.merged_graph)
                    self.merged_graph += test_graph
                    final_count = len(self.merged_graph)
                    
                    added_triples = final_count - initial_count
                    total_loaded += added_triples
                    successful_chunks += 1
                    
                    self.stats['chunks_processed'] += 1
                    
                    if self.show_progress:
                        print(f"      ✅ {chunk_triples:,} loaded, {added_triples:,} new (total: {final_count:,})")
                    
                    # Clean up chunk graph
                    del test_graph
                    gc.collect()
                    
                except Exception as e:
                    self.stats['chunks_failed'] += 1
                    self.stats['failed_chunks'].append((chunk_file, str(e)))
                    if self.show_progress:
                        print(f"      ❌ Chunk failed: {e}")
                    continue
            
            if successful_chunks == 0:
                return False, 0, f"All {len(chunk_files)} chunks failed to process"
            
            if self.show_progress and self.stats['chunks_failed'] > 0:
                print(f"   ⚠️  {self.stats['chunks_failed']} chunks failed, {successful_chunks} succeeded")
            
            return True, total_loaded, None
            
        except Exception as e:
            return False, 0, f"Large file processing failed: {e}"
    
    def _cleanup_temp_files(self):
        """Clean up all temporary directories"""
        # Clean up chunker temp dir
        if hasattr(self.chunker, 'temp_dir') and self.chunker.temp_dir:
            try:
                shutil.rmtree(self.chunker.temp_dir)
            except:
                pass
        
        # Clean up any other temp dirs
        for temp_dir in self.temp_dirs:
            try:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
            except:
                pass
    
    def merge_files(self, input_pattern: str, output_file: str, 
                   output_format: str = "turtle") -> bool:
        """Merge TTL files with proper syntax handling"""
        start_time = time.time()
        
        try:
            if self.show_progress:
                print(f"🐢 TTL-Aware Large File Merger")
                print(f"📂 Input: {input_pattern}")
                print(f"📄 Output: {output_file}")
                print(f"📦 Chunk size: {self.chunk_size_bytes // (1024*1024)}MB")
                print()
            
            # Get accessible files
            ttl_files = glob.glob(input_pattern)
            if not ttl_files:
                print(f"❌ No files found: {input_pattern}")
                return False
            
            accessible_files = [f for f in ttl_files if os.path.exists(f) and os.access(f, os.R_OK)]
            if not accessible_files:
                print("❌ No accessible files found")
                return False
            
            # Show file info
            if self.show_progress:
                print(f"🔍 Processing {len(accessible_files)} files:")
                for filepath in accessible_files:
                    size = os.path.getsize(filepath)
                    large = "📊 LARGE" if self._is_large_file(filepath) else "📄"
                    print(f"   {large} {os.path.basename(filepath)} ({self._format_size(size)})")
                print()
            
            # Process each file
            for i, filepath in enumerate(accessible_files):
                if self.show_progress:
                    print(f"[{i+1}/{len(accessible_files)}] Processing: {os.path.basename(filepath)}")
                
                if self._is_large_file(filepath):
                    success, triples_loaded, error = self._process_large_file(filepath)
                else:
                    success, triples_loaded, error = self._process_small_file(filepath)
                
                if success:
                    self.stats['files_processed'] += 1
                    self.stats['total_triples'] += triples_loaded
                    if self.show_progress:
                        print(f"   ✅ Success: {triples_loaded:,} triples added")
                else:
                    self.stats['files_failed'] += 1
                    self.stats['failed_files'].append((filepath, error))
                    if self.show_progress:
                        print(f"   ❌ Failed: {error}")
                
                if self.show_progress:
                    print()
            
            # Final stats
            self.stats['processing_time'] = time.time() - start_time
            final_triples = len(self.merged_graph)
            
            if self.stats['files_processed'] == 0:
                print("❌ No files processed successfully")
                return False
            
            # Write output
            if self.show_progress:
                print(f"💾 Writing {final_triples:,} triples to {output_file}...")
            
            self.merged_graph.serialize(destination=output_file, format=output_format)
            
            # Verify output
            if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                print("❌ Output file creation failed")
                return False
            
            if self.show_progress:
                self._print_final_stats(output_file)
            
            return True
            
        except Exception as e:
            print(f"❌ Merge failed: {e}")
            return False
        finally:
            self._cleanup_temp_files()
    
    def _print_final_stats(self, output_file: str):
        """Print comprehensive final statistics"""
        print(f"\n🎉 MERGE COMPLETED!")
        print(f"=" * 40)
        print(f"📁 Output: {output_file}")
        print(f"📊 Files processed: {self.stats['files_processed']}/{self.stats['files_processed'] + self.stats['files_failed']}")
        
        if self.stats['chunks_processed'] > 0:
            print(f"📦 Chunks processed: {self.stats['chunks_processed']}")
            if self.stats['chunks_failed'] > 0:
                print(f"❌ Chunks failed: {self.stats['chunks_failed']}")
        
        print(f"🔗 Final triples: {len(self.merged_graph):,}")
        print(f"⏱️  Time: {self.stats['processing_time']:.1f}s")
        
        if self.stats['processing_time'] > 0:
            rate = len(self.merged_graph) / self.stats['processing_time']
            print(f"🚀 Rate: {rate:,.0f} triples/sec")
        
        output_size = os.path.getsize(output_file)
        print(f"📏 Output size: {self._format_size(output_size)}")
        
        # Show failures if any
        if self.stats['failed_files']:
            print(f"\n❌ Failed files:")
            for filepath, error in self.stats['failed_files'][:3]:
                print(f"   • {os.path.basename(filepath)}: {error}")
            if len(self.stats['failed_files']) > 3:
                print(f"   • ... and {len(self.stats['failed_files']) - 3} more")
    
    @staticmethod
    def _format_size(size_bytes: int) -> str:
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} PB"

def main():
    """Main function with proper TTL syntax handling"""
    if len(sys.argv) < 2 or "--help" in sys.argv:
        print("🐢 TTL-Aware Large File Merger")
        print("=" * 35)
        print("Syntax-aware chunking for multi-GB files")
        print()
        print("Usage:")
        print("  python merge_ttl.py [pattern] [output] [format] [options]")
        print()
        print("Options:")
        print("  --chunk-size N    Chunk size in MB (default: 100)")
        print("  --quiet          Minimal output")
        print("  --help           Show help")
        print()
        print("Examples:")
        print("  python merge_ttl.py 'huge.ttl' merged.ttl")
        print("  python merge_ttl.py '*.ttl' output.ttl --chunk-size 50")
        print()
        print("🛡️  Features:")
        print("   • Preserves TTL syntax across chunks")
        print("   • Never breaks statements mid-triple")
        print("   • Handles prefixes and base URIs correctly")
        print("   • Validates each chunk before processing")
        return
    
    input_pattern = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else "merged.ttl"
    output_format = sys.argv[3] if len(sys.argv) > 3 else "turtle"
    
    # Parse options
    chunk_size_mb = 100
    show_progress = "--quiet" not in sys.argv
    
    if "--chunk-size" in sys.argv:
        try:
            idx = sys.argv.index("--chunk-size")
            chunk_size_mb = int(sys.argv[idx + 1])
        except:
            print("⚠️  Invalid chunk size, using 100MB")
    
    # Run merger
    merger = LargeFileTTLMerger(show_progress=show_progress, chunk_size_mb=chunk_size_mb)
    success = merger.merge_files(input_pattern, output_file, output_format)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    try:
        import rdflib
        main()
    except ImportError:
        print("❌ rdflib required: pip install rdflib")
        sys.exit(1)
