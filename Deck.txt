#!/usr/bin/env python3
"""
Enhanced Agentic RAG + GraphRAG System with Concept Synonyms
Updated with latest library APIs and documentation (2025)

Features:
- Dynamic concept extraction and synonym generation
- Multi-embedding strategy (original + concept synonyms)
- Enhanced semantic coverage and vocabulary mismatch resolution
- Confidence-based ranking with concept hierarchies
- Query expansion with discovered synonyms
- Updated with latest Elasticsearch 9.x, PyMuPDF 1.26.x, FalkorDB 1.1.x APIs

REQUIRED ENVIRONMENT VARIABLES:
    OPENAI_API_KEY=your_openai_api_key

ELASTICSEARCH AUTHENTICATION:
    ELASTICSEARCH_HOST=https://your-cluster.com:9200  # Must include schema
    ELASTICSEARCH_USERNAME=your_username
    ELASTICSEARCH_PASSWORD=your_password
    ELASTICSEARCH_CA_CERT=/path/to/ca.crt (optional, for SSL verification)

OPTIONAL VARIABLES:
    ELASTICSEARCH_VERIFY_CERTS=true (default: true)
    FALKORDB_HOST=localhost (default: localhost)
    FALKORDB_PORT=6379 (default: 6379)
    FALKORDB_PASSWORD=your_redis_password (if authentication required)
    PDF_PATH=/path/to/your/document.pdf

EXAMPLE CONFIGURATIONS:

Local Development:
    export OPENAI_API_KEY="your-key"
    export ELASTICSEARCH_HOST="http://localhost:9200"
    export PDF_PATH="/path/to/document.pdf"

Production with SSL:
    export OPENAI_API_KEY="your-key"
    export ELASTICSEARCH_HOST="https://cluster.com:9200"
    export ELASTICSEARCH_USERNAME="elastic"
    export ELASTICSEARCH_PASSWORD="your-password"
    export ELASTICSEARCH_CA_CERT="/certs/ca.crt"
    export PDF_PATH="/docs/document.pdf"

Elastic Cloud:
    export ELASTICSEARCH_HOST="https://deployment.es.region.provider.cloud.es.io:9243"
    export ELASTICSEARCH_USERNAME="elastic"
    export ELASTICSEARCH_PASSWORD="your-cloud-password"

DEPENDENCIES:
    pip install elasticsearch>=9.0.0 falkordb>=1.1.0 openai>=1.0.0
    pip install langchain>=0.1.0 langgraph>=0.0.28 pymupdf>=1.26.0

RUN WITH CONFIGURATION HELP:
    python script.py --config-help
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated
from datetime import datetime
import re
from collections import defaultdict

# Core dependencies - updated imports
import pymupdf  # Updated from 'import fitz'
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedRAGState(TypedDict):
    """Enhanced state for concept-aware RAG system"""
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Dict[str, Any]]
    vector_results: List[Dict[str, Any]]
    graph_results: List[Dict[str, Any]]
    discovered_entities: List[Dict[str, Any]]
    discovered_concepts: List[Dict[str, Any]]
    concept_synonyms: List[Dict[str, Any]]
    query_analysis: Dict[str, Any]
    context: str
    reasoning_trace: List[str]
    search_strategy: str
    session_id: str

class DocumentProcessor:
    """Enhanced document processor with concept extraction using PyMuPDF 1.26.x"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", " "]
        )
        self.concept_cache = {}
    
    def extract_pdf_content(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract and chunk PDF content using updated PyMuPDF API"""
        logger.info(f"Processing PDF: {pdf_path}")
        
        # Updated PyMuPDF API
        doc = pymupdf.open(pdf_path)
        full_text = ""
        
        # Extract all text
        for page_num in range(len(doc)):
            page = doc[page_num]
            page_text = page.get_text()
            full_text += f"\n\n{page_text}"
        
        doc.close()
        
        # Create chunks
        text_chunks = self.text_splitter.split_text(full_text)
        
        chunks = []
        for i, chunk_text in enumerate(text_chunks):
            chunk = {
                "chunk_id": f"chunk_{i}",
                "text": chunk_text,
                "chunk_index": i,
                "source": pdf_path,
                "metadata": {
                    "word_count": len(chunk_text.split()),
                    "char_count": len(chunk_text)
                }
            }
            chunks.append(chunk)
        
        logger.info(f"Created {len(chunks)} chunks")
        return chunks

class EnhancedVectorEngine:
    """Enhanced vector engine with updated Elasticsearch 9.x API"""
    
    def __init__(self, 
                 host: str = "http://localhost:9200", 
                 index_name: str = "enhanced_rag",
                 username: str = None,
                 password: str = None,
                 ca_certs: str = None,
                 verify_certs: bool = True,
                 request_timeout: int = 30):
        
        self.index_name = index_name
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.concept_synonym_cache = {}
        
        # Configure Elasticsearch client with updated 9.x API
        self.client = self._create_authenticated_client(
            host, username, password, ca_certs, verify_certs, request_timeout
        )
        
        self._create_enhanced_index()
    
    def _create_authenticated_client(self, host, username, password, ca_certs, verify_certs, request_timeout):
        """Create Elasticsearch client with updated 9.x API"""
        
        # Validate URL format - must include schema
        if not host.startswith(('http://', 'https://')):
            raise ValueError(f"Elasticsearch host must include schema (http:// or https://). Got: {host}")
        
        # Base configuration for Elasticsearch 9.x
        client_config = {
            "hosts": [host],
            "request_timeout": request_timeout,  # Updated from 'timeout'
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # SSL/TLS Configuration (updated parameter names)
        if ca_certs or verify_certs is False:
            client_config["verify_certs"] = verify_certs
            if ca_certs:
                client_config["ca_certs"] = ca_certs
                logger.info(f"Using CA certificate: {ca_certs}")
        
        # Authentication Configuration (updated from http_auth to basic_auth)
        if username and password:
            client_config["basic_auth"] = (username, password)
            logger.info(f"Using basic authentication for user: {username}")
        elif username or password:
            logger.warning("Both username and password must be provided for authentication")
        
        try:
            client = Elasticsearch(**client_config)
            
            # Test connection
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                
                # Log cluster info (updated API call)
                info = client.info()
                logger.info(f"Elasticsearch version: {info.body['version']['number']}")
                logger.info(f"Cluster name: {info.body['cluster_name']}")
            else:
                raise ConnectionError("Failed to ping Elasticsearch cluster")
                
            return client
            
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            logger.error("Make sure your ELASTICSEARCH_HOST includes the schema (http:// or https://)")
            raise
    
    def _create_enhanced_index(self):
        """Create enhanced index with concept synonym support"""
        mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "concept_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer",
                                "keyword_repeat",
                                "remove_duplicates"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "concept_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 1536,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "embedding_type": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "is_primary": {"type": "boolean"},
                    
                    # Concept-specific fields
                    "primary_concept": {"type": "keyword"},
                    "synonym_term": {"type": "keyword"},
                    "aliases": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "concept_context": {"type": "text"},
                    "concept_category": {"type": "keyword"},
                    
                    # Agent extractions
                    "agent_extracted_entities": {"type": "text"},
                    "agent_extracted_concepts": {"type": "text"},
                    
                    "timestamp": {"type": "date"},
                    "metadata": {"type": "object"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                # Updated API call for Elasticsearch 9.x
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created enhanced index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise
    
    def index_enhanced_documents(self, chunks: List[Dict[str, Any]], agent_extractions: List[Dict[str, Any]]):
        """Index documents with concept synonyms and multiple embeddings"""
        logger.info("Creating enhanced document index with concept synonyms...")
        
        all_embeddings = []
        
        for i, chunk in enumerate(chunks):
            extraction = agent_extractions[i] if i < len(agent_extractions) else {}
            
            # Create primary embedding for original chunk
            primary_embedding = self._create_primary_embedding(chunk, extraction)
            all_embeddings.append(primary_embedding)
            
            # Create concept synonym embeddings
            concepts = extraction.get("concept_synonyms", [])
            for concept in concepts:
                synonym_embeddings = self._create_concept_synonym_embeddings(chunk, concept)
                all_embeddings.extend(synonym_embeddings)
            
            # Create contextual variant embeddings
            contextual_embeddings = self._create_contextual_embeddings(chunk, extraction)
            all_embeddings.extend(contextual_embeddings)
        
        # Index all embeddings using updated API
        for embedding_doc in all_embeddings:
            doc_id = f"{embedding_doc['chunk_id']}_{embedding_doc['embedding_type']}_{embedding_doc.get('variant_id', 'main')}"
            try:
                # Updated indexing API for Elasticsearch 9.x
                self.client.index(index=self.index_name, id=doc_id, document=embedding_doc)
            except Exception as e:
                logger.error(f"Failed to index document {doc_id}: {e}")
        
        # Refresh index
        self.client.indices.refresh(index=self.index_name)
        logger.info(f"Indexed {len(all_embeddings)} enhanced embeddings from {len(chunks)} chunks")
    
    def _create_primary_embedding(self, chunk: Dict, extraction: Dict) -> Dict:
        """Create primary embedding for original chunk"""
        embedding = self.embeddings.embed_query(chunk["text"])
        
        return {
            "text": chunk["text"],
            "embedding": embedding,
            "embedding_type": "original",
            "chunk_id": chunk["chunk_id"],
            "source": chunk["source"],
            "chunk_index": chunk["chunk_index"],
            "is_primary": True,
            "confidence_score": 1.0,
            "agent_extracted_entities": extraction.get("entities", ""),
            "agent_extracted_concepts": extraction.get("concepts", ""),
            "metadata": chunk["metadata"],
            "timestamp": datetime.now()
        }
    
    def _create_concept_synonym_embeddings(self, chunk: Dict, concept: Dict) -> List[Dict]:
        """Create embeddings for concept synonyms"""
        embeddings = []
        
        primary_term = concept.get("primary_term", "")
        synonyms = concept.get("synonyms", [])
        context = concept.get("context", "")
        confidence = concept.get("confidence", 0.8)
        
        for synonym in synonyms:
            # Create contextual synonym text
            synonym_text = f"{synonym}: {context} (also known as: {primary_term})"
            embedding = self.embeddings.embed_query(synonym_text)
            
            embedding_doc = {
                "text": synonym_text,
                "embedding": embedding,
                "embedding_type": "concept_synonym",
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "is_primary": False,
                "primary_concept": primary_term,
                "synonym_term": synonym,
                "aliases": [primary_term] + [s for s in synonyms if s != synonym],
                "confidence_score": confidence,
                "concept_context": context,
                "concept_category": concept.get("category", "general"),
                "timestamp": datetime.now()
            }
            embeddings.append(embedding_doc)
        
        return embeddings
    
    def _create_contextual_embeddings(self, chunk: Dict, extraction: Dict) -> List[Dict]:
        """Create contextual variant embeddings"""
        embeddings = []
        
        # Create embeddings for different contextual views
        entities = extraction.get("structured_entities", [])
        relationships = extraction.get("structured_relationships", [])
        
        if entities:
            # Entity-focused embedding
            entity_text = "Key entities: " + ", ".join([e.get("name", "") for e in entities if isinstance(e, dict)])
            if entity_text.strip() != "Key entities:":
                embedding = self.embeddings.embed_query(entity_text)
                
                embedding_doc = {
                    "text": entity_text,
                    "embedding": embedding,
                    "embedding_type": "contextual_variant",
                    "chunk_id": chunk["chunk_id"],
                    "source": chunk["source"],
                    "is_primary": False,
                    "confidence_score": 0.7,
                    "concept_context": "entity_focus",
                    "variant_id": "entities",
                    "timestamp": datetime.now()
                }
                embeddings.append(embedding_doc)
        
        if relationships:
            # Relationship-focused embedding
            rel_text = "Key relationships: " + "; ".join([
                f"{r.get('subject', '')} {r.get('predicate', '')} {r.get('object', '')}" 
                for r in relationships if isinstance(r, dict)
            ])
            if rel_text.strip() != "Key relationships:":
                embedding = self.embeddings.embed_query(rel_text)
                
                embedding_doc = {
                    "text": rel_text,
                    "embedding": embedding,
                    "embedding_type": "contextual_variant",
                    "chunk_id": chunk["chunk_id"],
                    "source": chunk["source"],
                    "is_primary": False,
                    "confidence_score": 0.7,
                    "concept_context": "relationship_focus",
                    "variant_id": "relationships",
                    "timestamp": datetime.now()
                }
                embeddings.append(embedding_doc)
        
        return embeddings
    
    def enhanced_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Multi-layer enhanced search with concept synonyms"""
        
        # Layer 1: Direct vector search on original embeddings
        direct_results = self._search_by_embedding_type(query, "original", top_k)
        
        # Layer 2: Concept synonym search
        synonym_results = self._search_by_embedding_type(query, "concept_synonym", top_k)
        
        # Layer 3: Contextual variant search
        contextual_results = self._search_by_embedding_type(query, "contextual_variant", top_k)
        
        # Layer 4: Hybrid search with query expansion
        expanded_query = self._expand_query_with_synonyms(query)
        hybrid_results = self._hybrid_search_expanded(expanded_query, top_k)
        
        # Merge and rank results
        all_results = self._merge_multi_layer_results([
            (direct_results, 1.0),
            (synonym_results, 0.9),
            (contextual_results, 0.8),
            (hybrid_results, 0.7)
        ])
        
        return all_results[:top_k]
    
    def _search_by_embedding_type(self, query: str, embedding_type: str, top_k: int) -> List[Dict[str, Any]]:
        """Search specific embedding type using updated Elasticsearch 9.x API"""
        query_embedding = self.embeddings.embed_query(query)
        
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"embedding_type": embedding_type}},
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                    "params": {"query_vector": query_embedding}
                                }
                            }
                        }
                    ]
                }
            },
            "size": top_k,
            "_source": [
                "text", "chunk_id", "source", "embedding_type", "confidence_score",
                "primary_concept", "synonym_term", "aliases", "concept_context"
            ]
        }
        
        try:
            # Updated search API for Elasticsearch 9.x
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_enhanced_results(response, embedding_type)
        except Exception as e:
            logger.error(f"Search failed for embedding type {embedding_type}: {e}")
            return []
    
    def _expand_query_with_synonyms(self, query: str) -> str:
        """Expand query with discovered concept synonyms"""
        # Extract potential concepts from query
        query_concepts = self._extract_query_concepts(query)
        
        expanded_terms = [query]
        
        for concept in query_concepts:
            # Find synonyms for this concept in our index
            synonyms = self._get_concept_synonyms_from_index(concept)
            expanded_terms.extend(synonyms[:3])  # Add top 3 synonyms
        
        return " ".join(expanded_terms)
    
    def _extract_query_concepts(self, query: str) -> List[str]:
        """Extract potential concepts from query"""
        # Simple approach - in production, use more sophisticated NLP
        words = re.findall(r'\b\w+\b', query.lower())
        
        # Filter for meaningful terms (length > 2, not common words)
        stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'how', 'what', 'when', 'where', 'why'}
        concepts = [word for word in words if len(word) > 2 and word not in stopwords]
        
        return concepts
    
    def _get_concept_synonyms_from_index(self, concept: str) -> List[str]:
        """Get synonyms for a concept from the index"""
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        {"term": {"primary_concept": concept}},
                        {"term": {"synonym_term": concept}},
                        {"terms": {"aliases": [concept]}}
                    ]
                }
            },
            "size": 10,
            "_source": ["aliases", "synonym_term", "primary_concept"]
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            synonyms = set()
            
            for hit in response.body["hits"]["hits"]:
                source = hit["_source"]
                if "aliases" in source:
                    synonyms.update(source["aliases"])
                if "synonym_term" in source:
                    synonyms.add(source["synonym_term"])
                if "primary_concept" in source:
                    synonyms.add(source["primary_concept"])
            
            return list(synonyms)
        except Exception as e:
            logger.warning(f"Could not retrieve synonyms for {concept}: {e}")
            return []
    
    def _hybrid_search_expanded(self, expanded_query: str, top_k: int) -> List[Dict[str, Any]]:
        """Hybrid search with expanded query"""
        query_embedding = self.embeddings.embed_query(expanded_query)
        
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                    "params": {"query_vector": query_embedding}
                                },
                                "boost": 1.0
                            }
                        },
                        {
                            "multi_match": {
                                "query": expanded_query,
                                "fields": [
                                    "text^1.0",
                                    "agent_extracted_entities^1.5",
                                    "agent_extracted_concepts^1.2",
                                    "primary_concept^2.0",
                                    "synonym_term^1.8",
                                    "aliases^1.5"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO",
                                "boost": 0.8
                            }
                        }
                    ]
                }
            },
            "size": top_k
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_enhanced_results(response, "hybrid_expanded")
        except Exception as e:
            logger.error(f"Hybrid search failed: {e}")
            return []
    
    def _merge_multi_layer_results(self, result_layers: List[tuple]) -> List[Dict[str, Any]]:
        """Merge results from multiple layers with confidence weighting"""
        merged_results = {}
        
        for results, layer_weight in result_layers:
            for result in results:
                chunk_id = result["chunk_id"]
                
                # Calculate enhanced relevance score
                enhanced_score = self._calculate_enhanced_relevance_score(result, layer_weight)
                
                if chunk_id not in merged_results or enhanced_score > merged_results[chunk_id]["final_score"]:
                    result["final_score"] = enhanced_score
                    result["layer_weight"] = layer_weight
                    merged_results[chunk_id] = result
        
        # Sort by final score
        sorted_results = sorted(merged_results.values(), key=lambda x: x["final_score"], reverse=True)
        return sorted_results
    
    def _calculate_enhanced_relevance_score(self, result: Dict, layer_weight: float) -> float:
        """Calculate enhanced relevance score with multiple factors"""
        base_score = result.get("score", 0.0)
        confidence = result.get("confidence_score", 1.0)
        
        # Boost based on embedding type
        type_boost = {
            "original": 1.0,
            "concept_synonym": 0.95,
            "contextual_variant": 0.85,
            "hybrid_expanded": 0.9
        }.get(result.get("embedding_type", "original"), 1.0)
        
        # Apply layer weight and confidence
        enhanced_score = base_score * type_boost * confidence * layer_weight
        
        return enhanced_score
    
    def _format_enhanced_results(self, response: Dict, search_type: str) -> List[Dict[str, Any]]:
        """Format enhanced search results"""
        results = []
        
        try:
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "text": source["text"],
                    "chunk_id": source["chunk_id"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "embedding_type": source.get("embedding_type", "unknown"),
                    "confidence_score": source.get("confidence_score", 1.0),
                    "search_type": search_type,
                    "primary_concept": source.get("primary_concept", ""),
                    "synonym_term": source.get("synonym_term", ""),
                    "aliases": source.get("aliases", []),
                    "concept_context": source.get("concept_context", "")
                }
                results.append(result)
        except Exception as e:
            logger.error(f"Error formatting search results: {e}")
        
        return results

class GraphRAGEngine:
    """Enhanced GraphRAG with concept relationships using updated FalkorDB 1.1.x API"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, password: str = None):
        # Configure FalkorDB connection with updated API
        try:
            if password:
                self.db = FalkorDB(host=host, port=port, password=password)
                logger.info(f"Connected to FalkorDB with authentication at {host}:{port}")
            else:
                self.db = FalkorDB(host=host, port=port)
                logger.info(f"Connected to FalkorDB without authentication at {host}:{port}")
                
            self.graph = self.db.select_graph("enhanced_knowledge_graph")
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def build_enhanced_graph(self, chunks: List[Dict[str, Any]], agent_extractions: List[Dict[str, Any]]):
        """Build enhanced graph with concept hierarchies"""
        logger.info("Building enhanced knowledge graph with concept relationships...")
        
        try:
            # Clear existing graph
            self.graph.query("MATCH (n) DETACH DELETE n")
            
            # Track concept hierarchies
            concept_hierarchies = defaultdict(set)
            
            for i, chunk in enumerate(chunks):
                extraction = agent_extractions[i] if i < len(agent_extractions) else {}
                
                # Create document node
                doc_query = f"""
                CREATE (d:Document {{
                    id: '{chunk['chunk_id']}',
                    text: $text,
                    source: '{chunk['source']}',
                    chunk_index: {chunk['chunk_index']}
                }})
                """
                self.graph.query(doc_query, {"text": chunk["text"]})
                
                # Create entity nodes
                entities = extraction.get("structured_entities", [])
                for entity in entities:
                    if isinstance(entity, dict):
                        entity_name = entity.get("name", "").replace("'", "\\'")
                        entity_type = entity.get("type", "unknown").replace("'", "\\'")
                        
                        if entity_name:
                            # Create entity node
                            entity_query = f"""
                            MERGE (e:Entity {{name: '{entity_name}', type: '{entity_type}'}})
                            """
                            self.graph.query(entity_query)
                            
                            # Link document to entity
                            link_query = f"""
                            MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                            MATCH (e:Entity {{name: '{entity_name}'}})
                            MERGE (d)-[:MENTIONS]->(e)
                            """
                            self.graph.query(link_query)
                
                # Create concept nodes with synonyms
                concepts = extraction.get("concept_synonyms", [])
                for concept in concepts:
                    if isinstance(concept, dict):
                        primary_term = concept.get("primary_term", "").replace("'", "\\'")
                        synonyms = concept.get("synonyms", [])
                        
                        if primary_term:
                            # Create primary concept node
                            concept_query = f"""
                            MERGE (c:Concept {{name: '{primary_term}', type: 'primary'}})
                            SET c.context = $context
                            """
                            self.graph.query(concept_query, {"context": concept.get("context", "")})
                            
                            # Create synonym nodes and relationships
                            for synonym in synonyms:
                                synonym = synonym.replace("'", "\\'")
                                synonym_query = f"""
                                MERGE (s:Concept {{name: '{synonym}', type: 'synonym'}})
                                MERGE (c:Concept {{name: '{primary_term}'}})
                                MERGE (s)-[:SYNONYM_OF]->(c)
                                """
                                self.graph.query(synonym_query)
                            
                            # Link document to concept
                            doc_concept_query = f"""
                            MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                            MATCH (c:Concept {{name: '{primary_term}'}})
                            MERGE (d)-[:DISCUSSES]->(c)
                            """
                            self.graph.query(doc_concept_query)
                
                # Create relationships
                relationships = extraction.get("structured_relationships", [])
                for rel in relationships:
                    if isinstance(rel, dict):
                        subj = rel.get("subject", "").replace("'", "\\'")
                        pred = rel.get("predicate", "").replace("'", "\\'")
                        obj = rel.get("object", "").replace("'", "\\'")
                        
                        if subj and obj and pred:
                            rel_query = f"""
                            MERGE (s:Entity {{name: '{subj}'}})
                            MERGE (o:Entity {{name: '{obj}'}})
                            MERGE (s)-[:RELATION {{type: '{pred}'}}]->(o)
                            """
                            self.graph.query(rel_query)
            
            logger.info("Enhanced knowledge graph built successfully")
        except Exception as e:
            logger.error(f"Failed to build knowledge graph: {e}")
            raise
    
    def enhanced_graph_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Enhanced graph search with concept synonym traversal"""
        results = []
        
        # Extract query terms
        query_terms = self._extract_query_terms(query)
        
        for term in query_terms[:3]:
            term = term.replace("'", "\\'")
            
            # Search through concept synonyms
            concept_query = f"""
            MATCH (c:Concept)-[:SYNONYM_OF*0..1]-(related:Concept)
            WHERE toLower(c.name) CONTAINS '{term}' OR toLower(related.name) CONTAINS '{term}'
            MATCH (d:Document)-[:DISCUSSES]->(c)
            RETURN DISTINCT d.id as doc_id, d.text as text, d.source as source,
                   collect(DISTINCT c.name) as concepts,
                   collect(DISTINCT related.name) as related_concepts
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(concept_query)
                
                for record in result.result_set:
                    doc_result = {
                        "doc_id": record[0],
                        "text": record[1],
                        "source": record[2],
                        "concepts": record[3],
                        "related_concepts": record[4],
                        "search_type": "enhanced_graph"
                    }
                    results.append(doc_result)
            except Exception as e:
                logger.warning(f"Enhanced graph query failed for '{term}': {e}")
        
        return results[:top_k]
    
    def _extract_query_terms(self, query: str) -> List[str]:
        """Extract meaningful terms from query"""
        words = re.findall(r'\b\w+\b', query.lower())
        stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'how', 'what', 'when', 'where', 'why'}
        return [word for word in words if len(word) > 2 and word not in stopwords]

# Enhanced Agent Tools
@tool
def enhanced_entity_extraction_agent(text: str) -> Dict[str, Any]:
    """Enhanced agent that extracts entities and concept synonyms"""
    llm = ChatOpenAI(model="o3-mini", temperature=0.1, reasoning_effort="high")
    
    prompt = f"""Analyze the following text and perform comprehensive extraction:

Text: {text}

Extract:
1. Named entities (people, organizations, systems, processes, etc.) with types
2. Key concepts and generate 3-7 synonyms for each concept
3. Relationships between entities
4. Context and definitions for each concept

Pay special attention to:
- Technical terminology and acronyms
- Domain-specific terms that users might search for differently
- Alternative phrasings and formal/informal variations
- Abbreviations and their full forms

Return JSON format:
{{
    "entities": ["entity1", "entity2", ...],
    "concepts": ["concept1", "concept2", ...],
    "structured_entities": [
        {{"name": "entity_name", "type": "discovered_type"}}, ...
    ],
    "structured_relationships": [
        {{"subject": "entity1", "predicate": "relationship_type", "object": "entity2"}}, ...
    ],
    "concept_synonyms": [
        {{
            "primary_term": "data governance",
            "synonyms": ["information governance", "data management", "data oversight", "DG", "data stewardship framework"],
            "context": "framework for managing organizational data assets and ensuring data quality",
            "confidence": 0.95,
            "category": "framework"
        }}, ...
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        # Extract JSON from response
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate and clean the result
            if "concept_synonyms" not in result:
                result["concept_synonyms"] = []
            
            return result
        else:
            return {
                "entities": [], "concepts": [], "structured_entities": [], 
                "structured_relationships": [], "concept_synonyms": []
            }
    
    except Exception as e:
        logger.warning(f"Enhanced entity extraction failed: {e}")
        return {
            "entities": [], "concepts": [], "structured_entities": [], 
            "structured_relationships": [], "concept_synonyms": []
        }

@tool
def enhanced_query_analysis_agent(query: str) -> Dict[str, Any]:
    """Enhanced query analysis with concept understanding"""
    llm = ChatOpenAI(model="o3-mini", temperature=0.1, reasoning_effort="high")
    
    prompt = f"""Analyze this query for optimal search strategy:

Query: {query}

Determine:
1. Query complexity and information need type
2. Whether concept synonyms and alternative terminology are important
3. If relationships between entities/concepts are crucial
4. Best search strategy considering synonym matching
5. Key concepts that might have alternative terms

Return analysis in JSON:
{{
    "complexity": "simple|moderate|complex",
    "information_need": "factual|conceptual|relational|procedural",
    "recommended_strategy": "vector_only|hybrid|graph_focused|enhanced_multi_layer",
    "key_concepts": ["concept1", "concept2", ...],
    "requires_synonyms": true|false,
    "requires_relationships": true|false,
    "synonym_importance": "low|medium|high",
    "reasoning": "detailed explanation"
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {
                "complexity": "moderate",
                "information_need": "conceptual",
                "recommended_strategy": "enhanced_multi_layer",
                "key_concepts": [],
                "requires_synonyms": True,
                "requires_relationships": False,
                "synonym_importance": "medium",
                "reasoning": "Default analysis due to parsing error"
            }
    
    except Exception as e:
        logger.warning(f"Enhanced query analysis failed: {e}")
        return {
            "complexity": "moderate",
            "information_need": "conceptual",
            "recommended_strategy": "enhanced_multi_layer",
            "key_concepts": [],
            "requires_synonyms": True,
            "requires_relationships": False,
            "synonym_importance": "medium",
            "reasoning": f"Error in analysis: {e}"
        }

@tool
def enhanced_vector_search_tool(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """Execute enhanced multi-layer vector search"""
    # Note: In production, these should be passed as parameters or configured globally
    engine = EnhancedVectorEngine(
        host=os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        username=os.getenv("ELASTICSEARCH_USERNAME"),
        password=os.getenv("ELASTICSEARCH_PASSWORD"),
        ca_certs=os.getenv("ELASTICSEARCH_CA_CERT"),
        verify_certs=os.getenv("ELASTICSEARCH_VERIFY_CERTS", "true").lower() == "true"
    )
    return engine.enhanced_search(query, top_k)

@tool
def enhanced_graph_search_tool(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """Execute enhanced graph search with concept synonyms"""
    # Note: In production, these should be passed as parameters or configured globally
    engine = GraphRAGEngine(
        host=os.getenv("FALKORDB_HOST", "localhost"),
        port=int(os.getenv("FALKORDB_PORT", 6379)),
        password=os.getenv("FALKORDB_PASSWORD")
    )
    return engine.enhanced_graph_search(query, top_k)

class EnhancedAgenticSupervisor:
    """Enhanced supervisor with concept-aware coordination"""
    
    def __init__(self, openai_api_key: str):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        self.llm = ChatOpenAI(
            model="o3-mini",
            temperature=0.1,
            reasoning_effort="high"
        )
        
        self.graph = self._build_enhanced_supervisor_graph()
        self.checkpointer = MemorySaver()
        self.app = self.graph.compile(checkpointer=self.checkpointer)
    
    def _build_enhanced_supervisor_graph(self) -> StateGraph:
        """Build enhanced supervisor graph with concept awareness"""
        
        def enhanced_analyze_query_node(state: EnhancedRAGState) -> EnhancedRAGState:
            """Enhanced query analysis"""
            query = state["messages"][-1].content
            
            analysis = enhanced_query_analysis_agent.invoke(query)
            state["query_analysis"] = analysis
            state["search_strategy"] = analysis["recommended_strategy"]
            state["reasoning_trace"].append(f"Enhanced analysis: {analysis['reasoning']}")
            
            return state
        
        def route_enhanced_strategy(state: EnhancedRAGState) -> str:
            """Route based on enhanced analysis"""
            strategy = state["search_strategy"]
            analysis = state["query_analysis"]
            
            if strategy == "enhanced_multi_layer" or analysis.get("requires_synonyms", False):
                return "enhanced_search"
            elif strategy == "graph_focused" or analysis.get("requires_relationships", False):
                return "enhanced_graph_search"
            else:
                return "basic_vector_search"
        
        def enhanced_search_node(state: EnhancedRAGState) -> EnhancedRAGState:
            """Execute enhanced multi-layer search"""
            query = state["messages"][-1].content
            results = enhanced_vector_search_tool.invoke(query)
            
            state["vector_results"] = results
            state["reasoning_trace"].append("Executed enhanced multi-layer search with concept synonyms")
            return state
        
        def enhanced_graph_search_node(state: EnhancedRAGState) -> EnhancedRAGState:
            """Execute enhanced graph search"""
            query = state["messages"][-1].content
            
            graph_results = enhanced_graph_search_tool.invoke(query, top_k=3)
            backup_results = enhanced_vector_search_tool.invoke(query, top_k=2)
            
            state["graph_results"] = graph_results
            state["vector_results"] = backup_results
            state["reasoning_trace"].append("Executed enhanced graph search with concept relationships")
            return state
        
        def basic_vector_search_node(state: EnhancedRAGState) -> EnhancedRAGState:
            """Execute basic vector search for simple queries"""
            query = state["messages"][-1].content
            results = enhanced_vector_search_tool.invoke(query)
            
            state["vector_results"] = results[:3]  # Limit for simple queries
            state["reasoning_trace"].append("Executed basic vector search")
            return state
        
        def enhanced_synthesize_node(state: EnhancedRAGState) -> EnhancedRAGState:
            """Enhanced synthesis with concept awareness"""
            query = state["messages"][-1].content
            vector_results = state.get("vector_results", [])
            graph_results = state.get("graph_results", [])
            analysis = state.get("query_analysis", {})
            
            # Build enhanced context
            contexts = []
            
            for result in vector_results[:3]:
                context_info = []
                context_info.append(f"Text: {result['text']}")
                
                if result.get("primary_concept"):
                    context_info.append(f"Primary Concept: {result['primary_concept']}")
                if result.get("aliases"):
                    context_info.append(f"Related Terms: {', '.join(result['aliases'])}")
                if result.get("concept_context"):
                    context_info.append(f"Context: {result['concept_context']}")
                
                contexts.append("\n".join(context_info))
            
            for result in graph_results[:2]:
                context_info = []
                context_info.append(f"Graph Text: {result['text']}")
                
                if result.get("concepts"):
                    context_info.append(f"Concepts: {', '.join(result['concepts'])}")
                if result.get("related_concepts"):
                    context_info.append(f"Related Concepts: {', '.join(result['related_concepts'])}")
                
                contexts.append("\n".join(context_info))
            
            combined_context = "\n\n---\n\n".join(contexts)
            state["context"] = combined_context
            
            # Enhanced response generation
            system_prompt = """You are an advanced AI assistant with enhanced semantic understanding capabilities.

You have access to:
1. Multi-layer semantic search results (original text + concept synonyms)
2. Knowledge graph relationships between concepts
3. Concept hierarchies and alternative terminology
4. Query analysis indicating information needs

Provide comprehensive, accurate answers that:
- Leverage concept synonyms and alternative terminology
- Connect related concepts and entities
- Address the specific information need identified
- Explain relationships when relevant
- Use the most appropriate terminology for the user"""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"""
Query: {query}

Query Analysis: {analysis}
Search Strategy: {state.get('search_strategy', 'unknown')}
Synonym Importance: {analysis.get('synonym_importance', 'medium')}

Enhanced Search Results:
{combined_context}

Reasoning Trace: {' -> '.join(state.get('reasoning_trace', []))}

Provide a comprehensive answer that takes advantage of the enhanced semantic understanding and concept relationships.
""")
            ]
            
            response = self.llm.invoke(messages)
            state["messages"].append(response)
            state["reasoning_trace"].append("Generated enhanced concept-aware response")
            
            return state
        
        # Build enhanced workflow
        workflow = StateGraph(EnhancedRAGState)
        
        workflow.add_node("analyze", enhanced_analyze_query_node)
        workflow.add_node("enhanced_search", enhanced_search_node)
        workflow.add_node("enhanced_graph_search", enhanced_graph_search_node)
        workflow.add_node("basic_vector_search", basic_vector_search_node)
        workflow.add_node("synthesize", enhanced_synthesize_node)
        
        workflow.add_edge(START, "analyze")
        workflow.add_conditional_edges(
            "analyze",
            route_enhanced_strategy,
            {
                "enhanced_search": "enhanced_search",
                "enhanced_graph_search": "enhanced_graph_search",
                "basic_vector_search": "basic_vector_search"
            }
        )
        
        workflow.add_edge("enhanced_search", "synthesize")
        workflow.add_edge("enhanced_graph_search", "synthesize")
        workflow.add_edge("basic_vector_search", "synthesize")
        workflow.add_edge("synthesize", END)
        
        return workflow
    
    def process_enhanced_query(self, query: str, session_id: str = "default") -> Dict[str, Any]:
        """Process query with enhanced concept understanding"""
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "documents": [],
            "vector_results": [],
            "graph_results": [],
            "discovered_entities": [],
            "discovered_concepts": [],
            "concept_synonyms": [],
            "query_analysis": {},
            "context": "",
            "reasoning_trace": [],
            "search_strategy": "",
            "session_id": session_id
        }
        
        config = {"configurable": {"thread_id": session_id}}
        final_state = self.app.invoke(initial_state, config=config)
        
        return {
            "answer": final_state["messages"][-1].content,
            "query_analysis": final_state.get("query_analysis", {}),
            "search_strategy": final_state.get("search_strategy", ""),
            "vector_results": final_state.get("vector_results", []),
            "graph_results": final_state.get("graph_results", []),
            "reasoning_trace": final_state.get("reasoning_trace", []),
            "context": final_state.get("context", "")
        }

class EnhancedRAGSystem:
    """Complete Enhanced Agentic RAG + GraphRAG System with Updated APIs"""
    
    def __init__(self, 
                 openai_api_key: str,
                 elasticsearch_host: str = "http://localhost:9200",
                 elasticsearch_username: str = None,
                 elasticsearch_password: str = None,
                 elasticsearch_ca_cert: str = None,
                 elasticsearch_verify_certs: bool = True,
                 falkordb_host: str = "localhost",
                 falkordb_port: int = 6379,
                 falkordb_password: str = None):
        
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        self.processor = DocumentProcessor()
        
        # Initialize vector engine with updated authentication
        self.vector_engine = EnhancedVectorEngine(
            host=elasticsearch_host,
            username=elasticsearch_username,
            password=elasticsearch_password,
            ca_certs=elasticsearch_ca_cert,
            verify_certs=elasticsearch_verify_certs
        )
        
        # Initialize graph engine with authentication
        self.graph_engine = GraphRAGEngine(
            host=falkordb_host, 
            port=falkordb_port,
            password=falkordb_password
        )
        
        self.supervisor = EnhancedAgenticSupervisor(openai_api_key)
        
        logger.info("Enhanced Agentic RAG + GraphRAG System with Updated APIs initialized")
    
    def ingest_document(self, pdf_path: str) -> Dict[str, Any]:
        """Ingest document with enhanced concept extraction"""
        logger.info(f"Ingesting document with enhanced concept analysis: {pdf_path}")
        
        # Process document
        chunks = self.processor.extract_pdf_content(pdf_path)
        
        # Enhanced entity and concept extraction
        logger.info("Running enhanced entity and concept synonym extraction...")
        agent_extractions = []
        
        for chunk in chunks:
            extraction = enhanced_entity_extraction_agent.invoke(chunk["text"])
            agent_extractions.append(extraction)
        
        # Create enhanced index with concept synonyms
        self.vector_engine.index_enhanced_documents(chunks, agent_extractions)
        
        # Build enhanced graph
        self.graph_engine.build_enhanced_graph(chunks, agent_extractions)
        
        # Calculate statistics
        total_entities = sum(len(ext.get("structured_entities", [])) for ext in agent_extractions)
        total_relationships = sum(len(ext.get("structured_relationships", [])) for ext in agent_extractions)
        total_concepts = sum(len(ext.get("concept_synonyms", [])) for ext in agent_extractions)
        total_synonyms = sum(
            len(concept.get("synonyms", [])) 
            for ext in agent_extractions 
            for concept in ext.get("concept_synonyms", [])
        )
        
        return {
            "status": "success",
            "chunks_processed": len(chunks),
            "entities_discovered": total_entities,
            "relationships_discovered": total_relationships,
            "concepts_discovered": total_concepts,
            "synonyms_generated": total_synonyms,
            "document": pdf_path
        }
    
    def query(self, question: str, session_id: str = "default") -> Dict[str, Any]:
        """Query the enhanced system"""
        logger.info(f"Processing enhanced query: {question}")
        return self.supervisor.process_enhanced_query(question, session_id)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Get enhanced system statistics"""
        try:
            # Elasticsearch stats
            es_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.index_name)
            doc_count = es_stats.body["indices"][self.vector_engine.index_name]["total"]["docs"]["count"]
            
            # Graph stats
            node_stats = self.graph_engine.graph.query("MATCH (n) RETURN count(n) as nodes")
            node_count = node_stats.result_set[0][0] if node_stats.result_set else 0
            
            rel_stats = self.graph_engine.graph.query("MATCH ()-[r]->() RETURN count(r) as rels")
            rel_count = rel_stats.result_set[0][0] if rel_stats.result_set else 0
            
            # Concept synonym stats
            concept_stats = self.graph_engine.graph.query("MATCH (c:Concept) RETURN count(c) as concepts")
            concept_count = concept_stats.result_set[0][0] if concept_stats.result_set else 0
            
            synonym_stats = self.graph_engine.graph.query("MATCH ()-[r:SYNONYM_OF]->() RETURN count(r) as synonyms")
            synonym_count = synonym_stats.result_set[0][0] if synonym_stats.result_set else 0
            
            return {
                "elasticsearch_documents": doc_count,
                "graph_nodes": node_count,
                "graph_relationships": rel_count,
                "concepts": concept_count,
                "synonym_relationships": synonym_count,
                "status": "operational"
            }
        except Exception as e:
            return {"error": str(e), "status": "error"}

def main():
    """Example usage of Enhanced System with Updated APIs"""
    
    # Load configuration from environment variables
    config = {
        # OpenAI Configuration
        "openai_api_key": os.getenv("OPENAI_API_KEY", "your-openai-api-key-here"),
        
        # Elasticsearch Configuration (updated parameter names)
        "elasticsearch_host": os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        "elasticsearch_username": os.getenv("ELASTICSEARCH_USERNAME"),
        "elasticsearch_password": os.getenv("ELASTICSEARCH_PASSWORD"), 
        "elasticsearch_ca_cert": os.getenv("ELASTICSEARCH_CA_CERT"),
        "elasticsearch_verify_certs": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "true").lower() == "true",
        
        # FalkorDB Configuration
        "falkordb_host": os.getenv("FALKORDB_HOST", "localhost"),
        "falkordb_port": int(os.getenv("FALKORDB_PORT", 6379)),
        "falkordb_password": os.getenv("FALKORDB_PASSWORD"),
        
        # Document Configuration
        "pdf_path": os.getenv("PDF_PATH", "your_138_page_document.pdf")
    }
    
    # Validate required configuration
    if not config["openai_api_key"] or config["openai_api_key"] == "your-openai-api-key-here":
        print("❌ Error: OPENAI_API_KEY environment variable is required")
        print("Set it with: export OPENAI_API_KEY='your-api-key'")
        return
    
    # Validate Elasticsearch URL format
    if not config["elasticsearch_host"].startswith(('http://', 'https://')):
        print("❌ Error: ELASTICSEARCH_HOST must include schema (http:// or https://)")
        print(f"Current value: {config['elasticsearch_host']}")
        print("Set it with: export ELASTICSEARCH_HOST='https://your-cluster.com:9200'")
        return
    
    print("🔧 Configuration loaded:")
    print(f"  Elasticsearch: {config['elasticsearch_host']}")
    print(f"  Authentication: {'Username/Password' if config['elasticsearch_username'] else 'None'}")
    print(f"  SSL/TLS: {'Enabled' if config['elasticsearch_verify_certs'] else 'Disabled'}")
    print(f"  CA Certificate: {'Yes' if config['elasticsearch_ca_cert'] else 'No'}")
    print(f"  FalkorDB: {config['falkordb_host']}:{config['falkordb_port']}")
    print(f"  FalkorDB Auth: {'Yes' if config['falkordb_password'] else 'No'}")
    
    try:
        # Initialize enhanced system with updated APIs
        system = EnhancedRAGSystem(
            openai_api_key=config["openai_api_key"],
            elasticsearch_host=config["elasticsearch_host"],
            elasticsearch_username=config["elasticsearch_username"],
            elasticsearch_password=config["elasticsearch_password"],
            elasticsearch_ca_cert=config["elasticsearch_ca_cert"],
            elasticsearch_verify_certs=config["elasticsearch_verify_certs"],
            falkordb_host=config["falkordb_host"],
            falkordb_port=config["falkordb_port"],
            falkordb_password=config["falkordb_password"]
        )
        
        print("✅ System initialized successfully")
        
    except Exception as e:
        print(f"❌ Failed to initialize system: {e}")
        print("\n🔧 Troubleshooting:")
        print("1. Ensure ELASTICSEARCH_HOST includes schema: https://cluster.com:9200")
        print("2. Check Elasticsearch connection and credentials")
        print("3. Verify SSL certificates if using HTTPS")
        print("4. Ensure FalkorDB is running and accessible")
        print("5. Validate username and password")
        return
    
    # Ingest document
    pdf_path = config["pdf_path"]
    
    if os.path.exists(pdf_path):
        try:
            print(f"\n📄 Processing document: {pdf_path}")
            ingestion_result = system.ingest_document(pdf_path)
            print(f"✅ Enhanced Ingestion Result: {ingestion_result}")
        except Exception as e:
            print(f"❌ Failed to ingest document: {e}")
            return
    else:
        print(f"⚠️  PDF not found: {pdf_path}")
        print("Set PDF_PATH environment variable with your document path")
        print("Example: export PDF_PATH='/path/to/your/document.pdf'")
    
    # Example queries that benefit from concept synonyms
    queries = [
        "What are data stewards responsible for?",  # May find "data custodian", "information manager"
        "How is data quality ensured?",  # May find "information quality", "data validation"
        "What governance policies exist?",  # May find "management frameworks", "oversight rules"
        "Who manages master data?",  # May find "reference data", "golden records"
        "What compliance requirements apply?",  # May find "regulatory obligations", "policy adherence"
        "How is data lineage tracked?",  # May find "data flow", "information traceability"
        "What is the DG framework?",  # Should expand "DG" to "Data Governance"
        "How are privacy policies enforced?"  # May find "data protection", "confidentiality rules"
    ]
    
    print("\n" + "="*80)
    print("ENHANCED AGENTIC RAG + GRAPHRAG WITH UPDATED APIS")
    print("="*80)
    
    for i, query in enumerate(queries, 1):
        print(f"\n[Query {i}] {query}")
        print("-" * 60)
        
        try:
            result = system.query(query)
            
            analysis = result['query_analysis']
            print(f"Concept Analysis: Synonyms={analysis.get('requires_synonyms', False)}, "
                  f"Importance={analysis.get('synonym_importance', 'unknown')}")
            print(f"Strategy: {result['search_strategy']}")
            print(f"Answer: {result['answer']}")
            
            # Show concept matches if available
            vector_results = result.get('vector_results', [])
            concept_matches = [r for r in vector_results if r.get('primary_concept')]
            if concept_matches:
                print(f"Concept Matches Found: {len(concept_matches)}")
                for match in concept_matches[:2]:
                    print(f"  - {match.get('primary_concept', '')} (via: {match.get('synonym_term', 'direct')})")
            
        except Exception as e:
            print(f"❌ Query failed: {e}")
        
        if i >= 3:  # Limit for demo
            print(f"\n... (showing first 3 queries)")
            break
    
    # Enhanced system stats
    try:
        stats = system.get_system_stats()
        print(f"\n📊 Enhanced System Statistics: {stats}")
    except Exception as e:
        print(f"⚠️  Could not retrieve system stats: {e}")

def print_configuration_examples():
    """Print configuration examples for different deployment scenarios"""
    
    print("""
🔧 CONFIGURATION EXAMPLES (UPDATED FOR LATEST APIS)

1. LOCAL DEVELOPMENT (No Authentication):
   export ELASTICSEARCH_HOST="http://localhost:9200"  # Must include http://
   export FALKORDB_HOST="localhost"
   export FALKORDB_PORT="6379"

2. PRODUCTION WITH USERNAME/PASSWORD:
   export ELASTICSEARCH_HOST="https://your-es-cluster.com:9200"  # Must include https://
   export ELASTICSEARCH_USERNAME="your_username"
   export ELASTICSEARCH_PASSWORD="your_password"
   export ELASTICSEARCH_CA_CERT="/path/to/ca.crt"
   export ELASTICSEARCH_VERIFY_CERTS="true"

3. ELASTIC CLOUD:
   export ELASTICSEARCH_HOST="https://deployment.es.us-central1.gcp.cloud.es.io:9243"
   export ELASTICSEARCH_USERNAME="elastic"
   export ELASTICSEARCH_PASSWORD="your_elastic_cloud_password"

4. WITH SSL DISABLED (NOT RECOMMENDED FOR PRODUCTION):
   export ELASTICSEARCH_HOST="http://your-es-cluster.com:9200"
   export ELASTICSEARCH_USERNAME="your_username"
   export ELASTICSEARCH_PASSWORD="your_password"
   export ELASTICSEARCH_VERIFY_CERTS="false"

5. FALKORDB WITH AUTHENTICATION:
   export FALKORDB_HOST="your-falkordb-host.com"
   export FALKORDB_PORT="6379"
   export FALKORDB_PASSWORD="your_redis_password"

REQUIRED:
   export OPENAI_API_KEY="your-openai-api-key"
   export PDF_PATH="/path/to/your/138_page_document.pdf"

IMPORTANT CHANGES IN LATEST APIS:
- Elasticsearch host MUST include schema: http:// or https://
- PyMuPDF: Use 'import pymupdf' instead of 'import fitz'
- Elasticsearch: 'timeout' replaced with 'request_timeout'
- Elasticsearch: 'http_auth' replaced with 'basic_auth'
- Elasticsearch: 'ca_cert_path' replaced with 'ca_certs'
""")

if __name__ == "__main__":
    if len(os.sys.argv) > 1 and os.sys.argv[1] == "--config-help":
        print_configuration_examples()
    else:
        main()
