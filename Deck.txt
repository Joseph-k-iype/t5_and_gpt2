import os
import logging
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from dataclasses import dataclass
import asyncio
import time
from enum import Enum

# Core imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool, tool
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field  # Using Pydantic v2
from langchain_community.graphs import FalkorDBGraph
from langchain_community.chains import FalkorDBQAChain

# LangGraph imports
from langgraph.prebuilt import create_react_agent
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt.tool_node import ToolNode

# FalkorDB imports
from falkordb import FalkorDB

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # Optional custom base URL
    FALKORDB_HOST = os.getenv("FALKORDB_HOST", "localhost")
    FALKORDB_PORT = int(os.getenv("FALKORDB_PORT", 6379))
    GRAPH_NAME = os.getenv("GRAPH_NAME", "knowledge_graph")

# State management for LangGraph
class GraphRAGState(TypedDict):
    messages: Annotated[List[Any], "Chat messages"]
    original_query: str
    schema_info: Dict[str, Any]
    decomposed_queries: List[str]
    query_plan: str
    search_results: List[Dict]
    reflection_count: int
    reflection_feedback: str
    is_complete: bool
    final_answer: str

# Schema information model
class GraphSchema(BaseModel):
    """Graph schema information."""
    node_labels: List[str] = Field(..., description="Available node labels")
    relationship_types: List[str] = Field(..., description="Available relationship types")
    sample_properties: Dict[str, List[str]] = Field(..., description="Sample properties for each label")
    node_count: int = Field(..., description="Total number of nodes")
    relationship_count: int = Field(..., description="Total number of relationships")

# Query plan model
class QueryPlan(BaseModel):
    """Query execution plan."""
    approach: str = Field(..., description="Search approach: schema_exploration, simple_query, relationship_analysis, or hybrid")
    sub_queries: List[str] = Field(..., description="List of sub-queries to execute")
    reasoning: str = Field(..., description="Reasoning for the chosen approach")

# FalkorDB tools (using only available algorithms)
@tool
async def falkordb_schema_exploration() -> str:
    """Explore the graph schema to understand available node labels, relationships, and structure.
    
    Returns:
        Schema information including labels, relationships, and sample data
    """
    try:
        db = FalkorDB(host=Config.FALKORDB_HOST, port=Config.FALKORDB_PORT)
        graph = db.select_graph(Config.GRAPH_NAME)
        
        schema_info = []
        
        # Get node labels
        result = graph.query("CALL db.labels()")
        labels = [row[0] for row in result.result_set] if result.result_set else []
        schema_info.append(f"Available node labels: {labels}")
        
        # Get relationship types
        result = graph.query("CALL db.relationshipTypes()")
        rel_types = [row[0] for row in result.result_set] if result.result_set else []
        schema_info.append(f"Available relationship types: {rel_types}")
        
        # Get node and relationship counts
        result = graph.query("MATCH (n) RETURN count(n) as node_count")
        node_count = result.result_set[0][0] if result.result_set else 0
        
        result = graph.query("MATCH ()-[r]->() RETURN count(r) as rel_count")
        rel_count = result.result_set[0][0] if result.result_set else 0
        
        schema_info.append(f"Graph statistics: {node_count} nodes, {rel_count} relationships")
        
        # Sample some nodes for each label
        for label in labels[:5]:  # Limit to first 5 labels
            result = graph.query(f"MATCH (n:{label}) RETURN properties(n) LIMIT 3")
            if result.result_set:
                sample_props = [str(row[0]) for row in result.result_set]
                schema_info.append(f"Sample {label} properties: {sample_props}")
        
        return "\n".join(schema_info)
        
    except Exception as e:
        return f"Error exploring schema: {str(e)}"

@tool
async def falkordb_cypher_query(query: str) -> str:
    """Execute a Cypher query against the FalkorDB graph database.
    
    Args:
        query: Cypher query string to execute
        
    Returns:
        Query results as a formatted string
    """
    try:
        db = FalkorDB(host=Config.FALKORDB_HOST, port=Config.FALKORDB_PORT)
        graph = db.select_graph(Config.GRAPH_NAME)
        result = graph.query(query)
        
        if result.result_set:
            formatted_results = []
            total_records = len(result.result_set)
            
            # Limit results to prevent overwhelming context
            limit = min(total_records, 50)  # Show max 50 results
            for i, record in enumerate(result.result_set[:limit]):
                formatted_results.append(str(record))
            
            if total_records > limit:
                return f"Query executed successfully. Showing {limit} of {total_records} records:\n" + "\n".join(formatted_results) + f"\n... ({total_records - limit} more records)"
            else:
                return f"Query executed successfully. Found {total_records} records:\n" + "\n".join(formatted_results)
        else:
            return "Query executed successfully but returned no results."
            
    except Exception as e:
        return f"Error executing Cypher query: {str(e)}"

@tool
async def falkordb_vector_search(query: str, limit: int = 10) -> str:
    """Perform vector similarity search on graph nodes using full-text search.
    
    Args:
        query: Search query for semantic similarity
        limit: Maximum number of results to return (default: 10)
        
    Returns:
        Search results as a formatted string
    """
    try:
        # Use FalkorDB's full-text search capabilities
        cypher_query = f"""
        CALL db.idx.fulltext.queryNodes('*', '{query}') 
        YIELD node, score 
        RETURN labels(node)[0] as label, properties(node) as props, score 
        ORDER BY score DESC
        LIMIT {limit}
        """
        return await falkordb_cypher_query.ainvoke({"query": cypher_query})
    except Exception as e:
        return f"Error in vector search: {str(e)}"

@tool
async def falkordb_bfs_traversal(start_node_property: str, start_node_value: str, max_depth: int = 3) -> str:
    """Perform breadth-first search traversal from a starting node using FalkorDB's BFS algorithm.
    
    Args:
        start_node_property: Property name to find the starting node (e.g., 'name')
        start_node_value: Property value to match (e.g., 'Apple')
        max_depth: Maximum depth for traversal (default: 3)
        
    Returns:
        BFS traversal results
    """
    try:
        # First find the starting node
        find_query = f"""
        MATCH (start) 
        WHERE start.{start_node_property} = '{start_node_value}'
        RETURN start
        LIMIT 1
        """
        
        result = await falkordb_cypher_query.ainvoke({"query": find_query})
        if "no results" in result:
            return f"No node found with {start_node_property} = '{start_node_value}'"
        
        # Use FalkorDB's BFS algorithm
        bfs_query = f"""
        MATCH (start) 
        WHERE start.{start_node_property} = '{start_node_value}'
        CALL algo.BFS(start, {max_depth}) 
        YIELD nodes, edges
        RETURN nodes, edges
        """
        
        return await falkordb_cypher_query.ainvoke({"query": bfs_query})
    except Exception as e:
        return f"Error in BFS traversal: {str(e)}"

@tool
async def falkordb_relationship_analysis(entity_property: str, entity_value: str, depth: int = 2) -> str:
    """Analyze relationships for a given entity up to specified depth.
    
    Args:
        entity_property: Property name to find the entity (e.g., 'name')
        entity_value: Property value to match (e.g., 'Apple')
        depth: Maximum relationship depth to explore (default: 2)
        
    Returns:
        Relationship analysis results
    """
    try:
        cypher_query = f"""
        MATCH (e)-[r*1..{depth}]-(connected)
        WHERE e.{entity_property} = '{entity_value}'
        RETURN 
            e.{entity_property} as source,
            type(r[0]) as relationship_type,
            connected.name as target,
            length(r) as depth,
            labels(connected)[0] as target_label
        ORDER BY depth, relationship_type
        LIMIT 30
        """
        return await falkordb_cypher_query.ainvoke({"query": cypher_query})
    except Exception as e:
        return f"Error in relationship analysis: {str(e)}"

@tool
async def falkordb_statistical_analysis() -> str:
    """Perform statistical analysis of the graph structure.
    
    Returns:
        Graph statistics and insights
    """
    try:
        stats = []
        
        # Node degree distribution
        degree_query = """
        MATCH (n)
        WITH n, size((n)--()) as degree
        RETURN 
            labels(n)[0] as label,
            avg(degree) as avg_degree,
            max(degree) as max_degree,
            min(degree) as min_degree,
            count(n) as node_count
        ORDER BY avg_degree DESC
        """
        
        result = await falkordb_cypher_query.ainvoke({"query": degree_query})
        stats.append("Node degree statistics:")
        stats.append(result)
        
        # Relationship type distribution
        rel_query = """
        MATCH ()-[r]->()
        RETURN type(r) as relationship_type, count(r) as count
        ORDER BY count DESC
        LIMIT 10
        """
        
        result = await falkordb_cypher_query.ainvoke({"query": rel_query})
        stats.append("\nTop relationship types:")
        stats.append(result)
        
        return "\n".join(stats)
    except Exception as e:
        return f"Error in statistical analysis: {str(e)}"

# Schema-aware query planner
class SchemaAwareQueryPlanner:
    def __init__(self, llm):
        self.llm = llm
        self.planning_prompt = PromptTemplate(
            template="""You are an expert graph query planner. Based on the schema information and user query, create an optimal query plan.

Schema Information:
{schema_info}

User Query: {query}

Your task:
1. Understand what the user is asking for
2. Determine the best approach based on available data
3. Break down the query into specific sub-queries
4. Choose the most appropriate tools

Available approaches:
- schema_exploration: When we need to understand the graph structure first
- simple_query: For direct queries that can be answered with basic Cypher
- relationship_analysis: For exploring connections between entities
- bfs_traversal: For finding paths or related entities using breadth-first search
- hybrid: Combination of multiple approaches

Available tools:
- falkordb_schema_exploration: Understand graph structure
- falkordb_cypher_query: Execute direct Cypher queries
- falkordb_vector_search: Semantic/full-text search
- falkordb_bfs_traversal: Breadth-first traversal from a node
- falkordb_relationship_analysis: Analyze entity relationships
- falkordb_statistical_analysis: Graph statistics

Create a detailed plan with reasoning:""",
            input_variables=["schema_info", "query"]
        )
    
    async def create_plan(self, query: str, schema_info: str) -> QueryPlan:
        """Create a query plan based on the query and schema."""
        try:
            prompt = self.planning_prompt.format(query=query, schema_info=schema_info)
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            # Parse the response to extract plan details
            content = response.content.lower()
            
            # Determine approach
            if "schema_exploration" in content:
                approach = "schema_exploration"
            elif "relationship_analysis" in content:
                approach = "relationship_analysis"
            elif "bfs_traversal" in content:
                approach = "bfs_traversal"
            elif "hybrid" in content:
                approach = "hybrid"
            else:
                approach = "simple_query"
            
            # Extract sub-queries (simplified)
            sub_queries = [query]  # Default to original query
            if "sub-queries:" in content:
                # Extract sub-queries from response
                lines = response.content.split('\n')
                in_subqueries = False
                extracted_queries = []
                for line in lines:
                    if "sub-queries:" in line.lower():
                        in_subqueries = True
                        continue
                    if in_subqueries and line.strip():
                        if line.strip().startswith(('1.', '2.', '3.', '-', '*')):
                            extracted_queries.append(line.strip()[2:].strip())
                        elif not line.strip().startswith(('reasoning:', 'approach:')):
                            break
                if extracted_queries:
                    sub_queries = extracted_queries
            
            return QueryPlan(
                approach=approach,
                sub_queries=sub_queries,
                reasoning=response.content
            )
        except Exception as e:
            logger.error(f"Error creating query plan: {e}")
            return QueryPlan(
                approach="simple_query",
                sub_queries=[query],
                reasoning=f"Default plan due to error: {str(e)}"
            )

# Reflection mechanism to avoid endless loops
class ReflectionAgent:
    def __init__(self, llm):
        self.llm = llm
        self.reflection_prompt = PromptTemplate(
            template="""You are a reflection agent evaluating the completeness and quality of a GraphRAG search.

Original Query: {original_query}
Query Plan: {query_plan}
Search Results: {search_results}
Current Answer: {current_answer}

Your task is to evaluate:
1. Is the current answer sufficient and accurate?
2. Are there any missing pieces of information?
3. Should we continue searching or is the answer complete?
4. What additional questions might the user be interested in?

Evaluation criteria:
- Completeness: Does the answer fully address the query?
- Accuracy: Is the information correct based on the search results?
- Relevance: Is the answer relevant to what was asked?
- Additional value: What else might be useful to the user?

Provide your evaluation as:
COMPLETE: yes/no
FEEDBACK: [specific feedback for improvement]
SUGGESTIONS: [additional topics the user might find interesting]

Remember: Avoid endless loops. If we have a reasonable answer, mark as complete.""",
            input_variables=["original_query", "query_plan", "search_results", "current_answer"]
        )
    
    async def reflect(self, state: GraphRAGState) -> Dict[str, Any]:
        """Reflect on the current state and provide feedback."""
        try:
            prompt = self.reflection_prompt.format(
                original_query=state["original_query"],
                query_plan=state.get("query_plan", ""),
                search_results=str(state.get("search_results", [])),
                current_answer=state.get("final_answer", "")
            )
            
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            content = response.content.lower()
            
            # Parse reflection response
            is_complete = "complete: yes" in content
            feedback = response.content
            
            return {
                "is_complete": is_complete,
                "feedback": feedback,
                "should_continue": not is_complete and state["reflection_count"] < 3
            }
        except Exception as e:
            logger.error(f"Error in reflection: {e}")
            return {
                "is_complete": True,  # Fail safe - avoid infinite loops
                "feedback": f"Reflection error: {str(e)}",
                "should_continue": False
            }

# Main Intelligent GraphRAG Search Engine
class IntelligentGraphRAGSearchEngine:
    def __init__(self, openai_base_url: Optional[str] = None):
        # Initialize o3-mini with reasoning capabilities
        openai_kwargs = {
            "model": "o3-mini",
            "temperature": 0.1,
            "api_key": Config.OPENAI_API_KEY,
            "model_kwargs": {
                "reasoning_effort": "medium",
            }
        }
        
        if openai_base_url or Config.OPENAI_BASE_URL:
            openai_kwargs["base_url"] = openai_base_url or Config.OPENAI_BASE_URL
        
        self.reasoning_llm = ChatOpenAI(**openai_kwargs)
        
        # Standard LLM for planning and reflection
        standard_kwargs = {
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "api_key": Config.OPENAI_API_KEY,
        }
        
        if openai_base_url or Config.OPENAI_BASE_URL:
            standard_kwargs["base_url"] = openai_base_url or Config.OPENAI_BASE_URL
            
        self.standard_llm = ChatOpenAI(**standard_kwargs)
        
        # Initialize components
        self.query_planner = SchemaAwareQueryPlanner(self.standard_llm)
        self.reflection_agent = ReflectionAgent(self.standard_llm)
        
        # Initialize FalkorDB connection
        self.falkordb_graph = FalkorDBGraph(
            host=Config.FALKORDB_HOST,
            port=Config.FALKORDB_PORT,
            database=Config.GRAPH_NAME
        )
        
        # Initialize tools
        self.tools = [
            falkordb_schema_exploration,
            falkordb_cypher_query,
            falkordb_vector_search,
            falkordb_bfs_traversal,
            falkordb_relationship_analysis,
            falkordb_statistical_analysis
        ]
        
        # Create React agent with reasoning model
        self.react_agent = create_react_agent(
            model=self.reasoning_llm,
            tools=self.tools,
            state_modifier="""You are an intelligent GraphRAG agent with deep understanding of graph databases and reasoning.

Key capabilities:
1. SCHEMA AWARENESS: Always understand the graph structure before querying
2. INTELLIGENT TOOL SELECTION: Choose the right tool for each query type
3. ADAPTIVE REASONING: Adjust your approach based on results
4. RESULT SYNTHESIS: Combine information from multiple sources

Available FalkorDB-specific tools:
- falkordb_schema_exploration: Understand graph structure and available data
- falkordb_cypher_query: Execute precise Cypher queries  
- falkordb_vector_search: Full-text/semantic search across nodes
- falkordb_bfs_traversal: Breadth-first search from specific nodes
- falkordb_relationship_analysis: Explore entity connections
- falkordb_statistical_analysis: Graph-wide statistics and insights

Guidelines:
1. Start with schema exploration for new or unclear queries
2. Use BFS for path-finding and relationship discovery
3. Use vector search for semantic/text-based queries
4. Combine multiple approaches for complex questions
5. Always provide clear, well-reasoned answers

Remember: Work efficiently and avoid redundant operations."""
        )
        
        # Initialize memory for stateful conversations
        self.memory = MemorySaver()
        
        # Build the main workflow graph
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the intelligent GraphRAG workflow."""
        
        workflow = StateGraph(GraphRAGState)
        
        # Add nodes
        workflow.add_node("schema_exploration", self._schema_exploration_node)
        workflow.add_node("query_planning", self._query_planning_node)
        workflow.add_node("execution", self._execution_node)
        workflow.add_node("reflection", self._reflection_node)
        workflow.add_node("synthesis", self._synthesis_node)
        
        # Define the flow
        workflow.add_edge(START, "schema_exploration")
        workflow.add_edge("schema_exploration", "query_planning")
        workflow.add_edge("query_planning", "execution")
        workflow.add_edge("execution", "reflection")
        
        # Conditional edge based on reflection
        workflow.add_conditional_edges(
            "reflection",
            self._should_continue,
            {
                "continue": "execution",  # Go back to execution for more searching
                "complete": "synthesis"   # Move to synthesis
            }
        )
        
        workflow.add_edge("synthesis", END)
        
        return workflow.compile(checkpointer=self.memory)
    
    async def _schema_exploration_node(self, state: GraphRAGState) -> GraphRAGState:
        """Explore the graph schema to understand structure."""
        try:
            schema_info = await falkordb_schema_exploration.ainvoke({})
            state["schema_info"] = {"raw": schema_info}
            logger.info("Schema exploration completed")
        except Exception as e:
            logger.error(f"Error in schema exploration: {e}")
            state["schema_info"] = {"raw": f"Error: {str(e)}"}
        
        return state
    
    async def _query_planning_node(self, state: GraphRAGState) -> GraphRAGState:
        """Create an intelligent query plan."""
        try:
            query = state["original_query"]
            schema_info = state["schema_info"]["raw"]
            
            plan = await self.query_planner.create_plan(query, schema_info)
            state["query_plan"] = plan.reasoning
            state["decomposed_queries"] = plan.sub_queries
            
            logger.info(f"Query plan created with approach: {plan.approach}")
        except Exception as e:
            logger.error(f"Error in query planning: {e}")
            state["query_plan"] = f"Error in planning: {str(e)}"
            state["decomposed_queries"] = [state["original_query"]]
        
        return state
    
    async def _execution_node(self, state: GraphRAGState) -> GraphRAGState:
        """Execute the search using the React agent."""
        try:
            # Prepare context for the agent
            context = f"""
            Original Query: {state['original_query']}
            Graph Schema: {state['schema_info']['raw']}
            Query Plan: {state['query_plan']}
            Sub-queries: {state['decomposed_queries']}
            
            Execute the search plan and provide comprehensive results.
            Use multiple tools as needed to gather complete information.
            """
            
            response = await self.react_agent.ainvoke(
                {"messages": [HumanMessage(content=context)]},
                config={"configurable": {"thread_id": "execution"}}
            )
            
            # Extract the final response
            final_message = response["messages"][-1]
            if hasattr(final_message, 'content'):
                content = final_message.content
            else:
                content = str(final_message)
            
            # Store search results
            if "search_results" not in state:
                state["search_results"] = []
            
            state["search_results"].append({
                "query": state["original_query"],
                "results": content,
                "timestamp": time.time()
            })
            
            state["final_answer"] = content
            
            logger.info("Execution completed")
        except Exception as e:
            logger.error(f"Error in execution: {e}")
            state["search_results"] = [{"error": str(e)}]
            state["final_answer"] = f"Error during execution: {str(e)}"
        
        return state
    
    async def _reflection_node(self, state: GraphRAGState) -> GraphRAGState:
        """Reflect on the results and determine if more searching is needed."""
        try:
            # Increment reflection count
            state["reflection_count"] = state.get("reflection_count", 0) + 1
            
            # Perform reflection
            reflection_result = await self.reflection_agent.reflect(state)
            
            state["reflection_feedback"] = reflection_result["feedback"]
            state["is_complete"] = reflection_result["is_complete"]
            
            logger.info(f"Reflection completed. Complete: {reflection_result['is_complete']}")
        except Exception as e:
            logger.error(f"Error in reflection: {e}")
            state["reflection_feedback"] = f"Reflection error: {str(e)}"
            state["is_complete"] = True  # Fail safe
        
        return state
    
    def _should_continue(self, state: GraphRAGState) -> str:
        """Determine if we should continue searching or complete."""
        # Stop if marked complete or too many reflections
        if state.get("is_complete", False) or state.get("reflection_count", 0) >= 3:
            return "complete"
        return "continue"
    
    async def _synthesis_node(self, state: GraphRAGState) -> GraphRAGState:
        """Synthesize the final answer using o3-mini reasoning."""
        try:
            synthesis_prompt = f"""
            You are synthesizing a comprehensive answer from GraphRAG search results.
            
            Original Query: {state['original_query']}
            Schema Information: {state['schema_info']['raw']}
            Search Results: {state.get('search_results', [])}
            Reflection Feedback: {state.get('reflection_feedback', '')}
            
            Your task:
            1. Synthesize all information into a coherent, comprehensive answer
            2. Highlight key insights and relationships discovered
            3. Mention what additional information might be interesting to the user
            4. Ensure the answer is accurate and well-structured
            
            Provide a thoughtful, complete response:
            """
            
            response = await self.reasoning_llm.ainvoke([HumanMessage(content=synthesis_prompt)])
            state["final_answer"] = response.content
            
            logger.info("Synthesis completed")
        except Exception as e:
            logger.error(f"Error in synthesis: {e}")
            # Keep the existing answer if synthesis fails
            if not state.get("final_answer"):
                state["final_answer"] = f"Error in synthesis: {str(e)}"
        
        return state
    
    async def search(self, query: str, thread_id: Optional[str] = None) -> Dict[str, Any]:
        """Execute an intelligent GraphRAG search."""
        if not thread_id:
            thread_id = f"search_{hash(query)}_{int(time.time())}"
        
        # Initialize state
        initial_state = GraphRAGState(
            messages=[HumanMessage(content=query)],
            original_query=query,
            schema_info={},
            decomposed_queries=[],
            query_plan="",
            search_results=[],
            reflection_count=0,
            reflection_feedback="",
            is_complete=False,
            final_answer=""
        )
        
        # Execute the workflow
        config = {"configurable": {"thread_id": thread_id}}
        final_state = await self.workflow.ainvoke(initial_state, config=config)
        
        return {
            "query": query,
            "schema_info": final_state["schema_info"],
            "query_plan": final_state["query_plan"],
            "decomposed_queries": final_state["decomposed_queries"],
            "search_results": final_state["search_results"],
            "reflection_feedback": final_state["reflection_feedback"],
            "reflection_count": final_state["reflection_count"],
            "answer": final_state["final_answer"]
        }

# Usage example and main function
async def main():
    """Example usage of the Intelligent GraphRAG Search Engine."""
    
    try:
        custom_base_url = os.getenv("OPENAI_BASE_URL")
        search_engine = IntelligentGraphRAGSearchEngine(openai_base_url=custom_base_url)
        logger.info("Intelligent GraphRAG Search Engine initialized successfully!")
    except Exception as e:
        logger.error(f"Failed to initialize search engine: {e}")
        return
    
    # Example queries that demonstrate schema-aware reasoning
    example_queries = [
        "What types of entities and relationships exist in this graph?",
        "Find the most connected entities and their relationships",
        "What are the main communities or clusters in the data?",
        "Show me statistical insights about the graph structure",
        "Find entities related to 'technology' and their connections"
    ]
    
    print("Intelligent GraphRAG Search Engine - Schema-Aware & Reflective")
    print("=" * 70)
    
    for i, query in enumerate(example_queries):
        print(f"\nQuery {i+1}: {query}")
        print("-" * 50)
        
        try:
            start_time = time.time()
            result = await search_engine.search(query, thread_id=f"demo_{i+1}")
            end_time = time.time()
            
            print(f"Execution time: {end_time - start_time:.2f} seconds")
            print(f"Reflections: {result['reflection_count']}")
            print(f"Schema discovered: {len(result['schema_info'])} elements")
            
            # Display answer with reasonable length
            answer = result['answer']
            if len(answer) > 800:
                print(f"Answer: {answer[:800]}... [truncated]")
            else:
                print(f"Answer: {answer}")
            
            print("-" * 50)
            
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            print(f"Error: {str(e)}")
            continue
    
    print("\nIntelligent GraphRAG demo completed!")

if __name__ == "__main__":
    # Set up environment variables
    os.environ.setdefault("OPENAI_API_KEY", "your-openai-api-key")
    os.environ.setdefault("FALKORDB_HOST", "localhost")
    os.environ.setdefault("FALKORDB_PORT", "6379")
    os.environ.setdefault("GRAPH_NAME", "knowledge_graph")
    
    # Run the example
    asyncio.run(main())
