#!/usr/bin/env python3
"""
Optimized RDF to FalkorDB Property Graph Converter
=================================================

High-performance converter with robust timeout handling and optimized batch processing.
Fixes timeout issues through:
- Proper FalkorDB timeout configuration
- Optimized connection management  
- Efficient batch processing strategies
- Comprehensive error handling and recovery
"""

import os
import sys
import time
import logging
import re
import hashlib
import signal
import threading
from typing import Dict, List, Set, Tuple, Any, Optional, Iterator
from collections import defaultdict
from pathlib import Path
from contextlib import contextmanager

# Core dependencies with fallback handling
try:
    import falkordb
except ImportError:
    print("❌ FalkorDB not installed. Install with: pip install falkordb")
    sys.exit(1)

try:
    from rdflib import Graph, URIRef, Literal, BNode
    from rdflib.namespace import RDF, RDFS, XSD, FOAF
except ImportError:
    print("❌ RDFLib not installed. Install with: pip install rdflib")
    sys.exit(1)


class TimeoutHandler:
    """Cross-platform timeout handler using threading."""
    
    def __init__(self, timeout_seconds: int):
        self.timeout_seconds = timeout_seconds
        self.is_timeout = False
        self.timer = None
    
    def _timeout_occurred(self):
        self.is_timeout = True
    
    def __enter__(self):
        self.is_timeout = False
        self.timer = threading.Timer(self.timeout_seconds, self._timeout_occurred)
        self.timer.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.timer:
            self.timer.cancel()
        if self.is_timeout:
            raise TimeoutError(f"Operation timed out after {self.timeout_seconds} seconds")


class OptimizedFalkorConverter:
    """
    High-performance RDF to FalkorDB converter with robust timeout handling.
    """
    
    def __init__(self,
                 host: str = "localhost",
                 port: int = 6379,
                 password: Optional[str] = None,
                 
                 # Timeout settings (all in seconds)
                 socket_timeout: int = 30,           # Socket operation timeout
                 connection_timeout: int = 15,       # Initial connection timeout  
                 query_timeout: int = 60,            # Individual query timeout
                 
                 # FalkorDB server timeout settings (milliseconds)
                 falkor_timeout_max: int = 120000,   # 2 minutes max query time
                 falkor_timeout_default: int = 60000, # 1 minute default query time
                 
                 # Batch processing settings
                 parse_batch_size: int = 5000,       # RDF parsing batch size
                 process_batch_size: int = 1000,     # Processing batch size  
                 upload_concurrency: int = 5,        # Concurrent upload threads
                 
                 # Performance tuning
                 max_property_length: int = 500,
                 max_label_length: int = 50,
                 enable_compression: bool = True):
        
        # Connection settings with optimized timeouts
        self.connection_params = {
            'host': host,
            'port': port,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': connection_timeout,
            'socket_keepalive': True,
            'socket_keepalive_options': {
                'TCP_KEEPIDLE': 30,
                'TCP_KEEPINTVL': 10, 
                'TCP_KEEPCNT': 3
            },
            'retry_on_timeout': True,
            'retry_on_error': ['TIMEOUT', 'LOADING', 'BUSY'],
            'health_check_interval': 30,
            'decode_responses': True
        }
        
        if password:
            self.connection_params['password'] = password
        
        # Windows + Docker specific optimizations
        if os.name == 'nt':  # Windows
            # Longer timeouts for Docker networking on Windows
            self.connection_params['socket_timeout'] = max(socket_timeout, 45)
            self.connection_params['socket_connect_timeout'] = max(connection_timeout, 20)
            # Disable problematic keep-alive options on Windows Docker
            self.connection_params['socket_keepalive_options'] = {}
            self.logger.info("🪟 Windows detected: Using Docker-optimized connection settings")
        
        # Timeout settings
        self.query_timeout = query_timeout
        self.falkor_timeout_max = falkor_timeout_max
        self.falkor_timeout_default = falkor_timeout_default
        
        # Processing settings
        self.parse_batch_size = parse_batch_size
        self.process_batch_size = process_batch_size
        self.upload_concurrency = upload_concurrency
        self.max_property_length = max_property_length
        self.max_label_length = max_label_length
        self.enable_compression = enable_compression
        
        # Data structures
        self.entity_index = {}
        self.entity_counter = 0
        self.property_cache = {}
        self.label_cache = {}
        
        # Connection management
        self._connection = None
        self._connection_lock = threading.Lock()
        self._last_activity = time.time()
        
        # Performance monitoring
        self.stats = {
            'triples_processed': 0,
            'nodes_created': 0,
            'relationships_created': 0,
            'queries_executed': 0,
            'query_timeouts': 0,
            'connection_resets': 0,
            'errors_handled': 0,
            'total_query_time': 0,
            'start_time': None,
            'end_time': None,
            'data_integrity_checks': 0,
            'rollbacks_detected': 0
        }
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        self.logger.info("🚀 Optimized FalkorDB converter initialized with timeout handling")
        self.logger.info(f"⚙️ Timeouts: socket={socket_timeout}s, query={query_timeout}s, falkor_max={falkor_timeout_max}ms")
    
    def get_connection(self) -> falkordb.FalkorDB:
        """Get or create optimized FalkorDB connection with timeout configuration."""
        with self._connection_lock:
            current_time = time.time()
            
            # Check if connection needs refresh (every 5 minutes or if stale)
            if (self._connection is None or 
                current_time - self._last_activity > 300 or
                not self._test_connection_health()):
                
                if self._connection:
                    try:
                        self._connection.close()
                    except:
                        pass
                    self.stats['connection_resets'] += 1
                
                self.logger.info("🔌 Creating optimized FalkorDB connection...")
                try:
                    self._connection = falkordb.FalkorDB(**self.connection_params)
                    
                    # Configure FalkorDB server timeouts
                    self._configure_server_timeouts()
                    
                    # Test connection
                    test_graph = self._connection.select_graph("_connection_test")
                    test_graph.query("RETURN 1")
                    
                    self._last_activity = current_time
                    self.logger.info("✅ FalkorDB connection established with timeout configuration")
                    
                except Exception as e:
                    self.logger.error(f"❌ Failed to connect to FalkorDB: {e}")
                    raise ConnectionError(f"Cannot connect to FalkorDB: {e}")
            
            return self._connection
    
    def _test_connection_health(self) -> bool:
        """Test if current connection is healthy."""
        if not self._connection:
            return False
        try:
            test_graph = self._connection.select_graph("_health_check")
            test_graph.query("RETURN 1")
            return True
        except:
            return False
    
    def verify_data_integrity(self, graph_name: str, expected_nodes: int, expected_rels: int) -> Dict[str, Any]:
        """Verify data integrity after upload to ensure no data loss."""
        try:
            self.logger.info("🔍 Verifying data integrity...")
            
            # Count actual nodes and relationships in the database
            node_count_query = "MATCH (n) RETURN count(n) as node_count"
            rel_count_query = "MATCH ()-[r]->() RETURN count(r) as rel_count"
            
            node_result = self.execute_query_with_timeout(graph_name, node_count_query, timeout_override=30)
            rel_result = self.execute_query_with_timeout(graph_name, rel_count_query, timeout_override=30)
            
            actual_nodes = node_result.result_set[0][0] if node_result.result_set else 0
            actual_rels = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            integrity_report = {
                'expected_nodes': expected_nodes,
                'actual_nodes': actual_nodes,
                'node_loss': expected_nodes - actual_nodes,
                'expected_relationships': expected_rels, 
                'actual_relationships': actual_rels,
                'relationship_loss': expected_rels - actual_rels,
                'data_complete': (actual_nodes == expected_nodes and actual_rels == expected_rels)
            }
            
            self.stats['data_integrity_checks'] += 1
            
            if integrity_report['data_complete']:
                self.logger.info("✅ Data integrity verified: No data loss detected")
            else:
                self.logger.warning(f"⚠️ Data integrity check: {integrity_report['node_loss']} nodes missing, {integrity_report['relationship_loss']} relationships missing")
            
            return integrity_report
            
        except Exception as e:
            self.logger.error(f"❌ Data integrity verification failed: {e}")
            return {'verification_failed': True, 'error': str(e)}
        """Test if current connection is healthy."""
        if not self._connection:
            return False
        try:
            test_graph = self._connection.select_graph("_health_check")
            test_graph.query("RETURN 1")
            return True
        except:
            return False
    
    def _configure_server_timeouts(self):
        """Configure FalkorDB server timeout settings."""
        try:
            # Get Redis connection for configuration
            redis_conn = self._connection._connection
            
            # Set FalkorDB timeout configuration
            redis_conn.execute_command("GRAPH.CONFIG", "SET", "TIMEOUT_MAX", str(self.falkor_timeout_max))
            redis_conn.execute_command("GRAPH.CONFIG", "SET", "TIMEOUT_DEFAULT", str(self.falkor_timeout_default))
            
            # Optimize other FalkorDB settings for bulk loading
            redis_conn.execute_command("GRAPH.CONFIG", "SET", "NODE_CREATION_BUFFER", "32768")
            redis_conn.execute_command("GRAPH.CONFIG", "SET", "CACHE_SIZE", "50")
            
            self.logger.info(f"⚙️ FalkorDB server timeouts configured: max={self.falkor_timeout_max}ms, default={self.falkor_timeout_default}ms")
            
        except Exception as e:
            self.logger.warning(f"Could not configure FalkorDB server timeouts: {e}")
    
    def execute_query_with_timeout(self, graph_name: str, query: str, timeout_override: int = None) -> Any:
        """Execute query with comprehensive timeout handling and retry logic."""
        actual_timeout = timeout_override or self.query_timeout
        max_retries = 3
        
        for attempt in range(max_retries):
            start_time = time.time()
            
            try:
                with TimeoutHandler(actual_timeout):
                    db = self.get_connection()
                    graph = db.select_graph(graph_name)
                    
                    # Add query-level timeout to the query itself
                    query_with_timeout = f"CYPHER timeout={min(actual_timeout * 1000, self.falkor_timeout_max)} {query}"
                    
                    result = graph.query(query_with_timeout)
                    
                    # Update statistics
                    query_time = time.time() - start_time
                    self.stats['queries_executed'] += 1
                    self.stats['total_query_time'] += query_time
                    self._last_activity = time.time()
                    
                    return result
                    
            except TimeoutError as e:
                query_time = time.time() - start_time
                self.stats['query_timeouts'] += 1
                self.logger.warning(f"Query timeout after {query_time:.1f}s (attempt {attempt + 1}/{max_retries})")
                self.logger.info("🔄 FalkorDB automatically rolls back timed-out write queries (no partial data)")
                
                if attempt == max_retries - 1:
                    self.logger.error(f"Query failed after {max_retries} timeout attempts")
                    raise TimeoutError(f"Query timed out after {max_retries} attempts")
                
                # Reset connection on timeout
                self._connection = None
                time.sleep(min(2 ** attempt, 10))  # Exponential backoff, max 10s
                
            except Exception as e:
                error_msg = str(e).lower()
                self.stats['errors_handled'] += 1
                
                # Handle specific FalkorDB errors
                if any(phrase in error_msg for phrase in ['connection', 'timeout', 'busy', 'loading']):
                    self.logger.warning(f"Connection/timeout error (attempt {attempt + 1}/{max_retries}): {e}")
                    self._connection = None  # Reset connection
                    
                    if attempt < max_retries - 1:
                        time.sleep(min(2 ** attempt, 10))
                        continue
                
                elif 'more than one statement' in error_msg:
                    self.logger.error(f"Multi-statement error (should not happen): {e}")
                    raise ValueError(f"Multi-statement query not supported: {e}")
                
                elif any(phrase in error_msg for phrase in ['syntax', 'invalid', 'parse']):
                    self.logger.error(f"Query syntax error: {e}")
                    self.logger.debug(f"Problematic query: {query[:500]}...")
                    raise ValueError(f"Query syntax error: {e}")
                
                else:
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Query error (attempt {attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        self.logger.error(f"Query failed after {max_retries} attempts: {e}")
                        raise
        
        raise RuntimeError(f"Query failed after {max_retries} attempts")
    
    def sanitize_identifier(self, value: str, max_length: int = 50) -> str:
        """Sanitize string to be a valid Cypher identifier."""
        if value in self.label_cache:
            return self.label_cache[value]
        
        if not isinstance(value, str):
            value = str(value)
        
        # Extract meaningful part from URIs
        if value.startswith('http'):
            if '#' in value:
                value = value.split('#')[-1]
            elif '/' in value:
                value = value.split('/')[-1]
        
        # Keep only alphanumeric characters and underscores
        value = re.sub(r'[^\w]', '_', value)
        
        # Ensure it starts with a letter or underscore
        if value and value[0].isdigit():
            value = f"n_{value}"
        
        # Ensure it's not empty
        if not value:
            value = "node"
        
        # Limit length
        result = value[:max_length]
        self.label_cache[value] = result
        return result
    
    def sanitize_string_value(self, value: str) -> str:
        """Sanitize string value for use in Cypher queries."""
        if not isinstance(value, str):
            value = str(value)
        
        # Limit length
        value = value[:self.max_property_length]
        
        # Escape quotes and backslashes
        value = value.replace('\\', '\\\\')
        value = value.replace("'", "\\'")
        value = value.replace('"', '\\"')
        
        # Remove control characters
        value = re.sub(r'[\x00-\x1F\x7F]', '', value)
        
        # Replace multiple whitespace with single space
        value = re.sub(r'\s+', ' ', value).strip()
        
        return value
    
    def get_entity_id(self, uri: str) -> int:
        """Get or create a unique integer ID for an entity."""
        if uri not in self.entity_index:
            self.entity_index[uri] = self.entity_counter
            self.entity_counter += 1
        return self.entity_index[uri]
    
    def parse_rdf_streaming(self, file_path: str) -> Iterator[List[Tuple]]:
        """Parse RDF file in optimized streaming batches."""
        self.logger.info(f"📖 Parsing RDF file: {file_path}")
        
        try:
            graph = Graph()
            
            # Determine format from file extension
            file_ext = Path(file_path).suffix.lower()
            format_map = {
                '.ttl': 'turtle',
                '.turtle': 'turtle',
                '.rdf': 'xml',
                '.xml': 'xml',
                '.n3': 'n3',
                '.nt': 'nt',
                '.jsonld': 'json-ld'
            }
            
            rdf_format = format_map.get(file_ext, 'turtle')
            self.logger.info(f"📄 Detected format: {rdf_format}")
            
            # Parse with timeout protection
            with TimeoutHandler(300):  # 5 minute timeout for parsing
                graph.parse(file_path, format=rdf_format)
            
            total_triples = len(graph)
            self.logger.info(f"📊 Found {total_triples:,} triples")
            
            # Stream triples in optimized batches
            triples = list(graph)
            for i in range(0, total_triples, self.parse_batch_size):
                batch_end = min(i + self.parse_batch_size, total_triples)
                yield triples[i:batch_end]
                
                if i % (self.parse_batch_size * 5) == 0:  # Log every 5 batches
                    progress = (batch_end / total_triples) * 100
                    self.logger.info(f"📈 Parsing progress: {progress:.1f}% ({batch_end:,}/{total_triples:,})")
        
        except Exception as e:
            self.logger.error(f"❌ Failed to parse RDF file: {e}")
            raise
    
    def process_triples_batch(self, triples_batch: List[Tuple]) -> Tuple[List[Dict], List[Dict]]:
        """Process RDF triples into optimized node and relationship structures."""
        nodes = {}
        relationships = []
        
        for subject, predicate, obj in triples_batch:
            subject_str = str(subject)
            predicate_str = str(predicate)
            
            # Get or create subject node
            subject_id = self.get_entity_id(subject_str)
            if subject_id not in nodes:
                labels = set(['Resource'])
                nodes[subject_id] = {
                    'id': subject_id,
                    'uri': subject_str,
                    'labels': labels,
                    'properties': {'uri': subject_str}
                }
            
            # Handle object based on type
            if isinstance(obj, (URIRef, BNode)):
                # Object is another resource - create relationship
                obj_str = str(obj)
                obj_id = self.get_entity_id(obj_str)
                
                # Create object node if not exists
                if obj_id not in nodes:
                    obj_labels = set(['Resource'])
                    nodes[obj_id] = {
                        'id': obj_id,
                        'uri': obj_str,
                        'labels': obj_labels,
                        'properties': {'uri': obj_str}
                    }
                
                # Create relationship with optimized structure
                rel_type = self.sanitize_identifier(predicate_str)
                relationships.append({
                    'source_id': subject_id,
                    'target_id': obj_id,
                    'type': rel_type,
                    'predicate_uri': predicate_str  # Store as simple property
                })
                
                # Special handling for rdf:type
                if predicate_str == str(RDF.type):
                    type_label = self.sanitize_identifier(str(obj))
                    nodes[subject_id]['labels'].add(type_label)
            
            elif isinstance(obj, Literal):
                # Object is a literal - add as property
                prop_name = self.sanitize_identifier(predicate_str)
                prop_value = str(obj)
                
                # Handle different literal types efficiently
                if obj.datatype:
                    datatype = str(obj.datatype)
                    if datatype == str(XSD.integer):
                        try:
                            prop_value = int(prop_value)
                        except ValueError:
                            pass
                    elif datatype in [str(XSD.float), str(XSD.double)]:
                        try:
                            prop_value = float(prop_value)
                        except ValueError:
                            pass
                    elif datatype == str(XSD.boolean):
                        prop_value = prop_value.lower() in ('true', '1')
                
                # Add property to node with compression if enabled
                if isinstance(prop_value, str):
                    prop_value = self.sanitize_string_value(prop_value)
                    if self.enable_compression and len(prop_value) > 100:
                        # Store hash for very long strings
                        prop_hash = hashlib.md5(prop_value.encode()).hexdigest()[:16]
                        nodes[subject_id]['properties'][f"{prop_name}_hash"] = prop_hash
                    else:
                        nodes[subject_id]['properties'][prop_name] = prop_value
                else:
                    nodes[subject_id]['properties'][prop_name] = prop_value
        
        self.stats['triples_processed'] += len(triples_batch)
        return list(nodes.values()), relationships
    
    def build_optimized_node_query(self, node: Dict) -> str:
        """Build optimized CREATE query for a node."""
        try:
            # Build labels efficiently
            labels = node.get('labels', {'Resource'})
            if isinstance(labels, set):
                labels = list(labels)[:3]  # Limit to 3 labels max
            
            label_str = ':'.join([self.sanitize_identifier(label) for label in labels])
            
            # Build properties efficiently
            properties = node.get('properties', {})
            properties['entity_id'] = node['id']  # Always include entity_id
            
            prop_parts = []
            for key, value in properties.items():
                safe_key = self.sanitize_identifier(key)
                
                if isinstance(value, str):
                    if len(value.strip()) > 0:
                        safe_value = self.sanitize_string_value(value)
                        if safe_value:
                            prop_parts.append(f"{safe_key}: '{safe_value}'")
                elif isinstance(value, bool):
                    prop_parts.append(f"{safe_key}: {str(value).lower()}")
                elif isinstance(value, (int, float)):
                    if not (isinstance(value, float) and (value != value or abs(value) == float('inf'))):
                        prop_parts.append(f"{safe_key}: {value}")
            
            if prop_parts:
                props_str = '{' + ', '.join(prop_parts) + '}'
                return f"CREATE (:{label_str} {props_str})"
            else:
                return f"CREATE (:{label_str} {{entity_id: {node['id']}}})"
        
        except Exception as e:
            self.logger.debug(f"Failed to build node query: {e}")
            # Fallback to minimal query
            return f"CREATE (:Resource {{entity_id: {node['id']}}})"
    
    def build_optimized_relationship_query(self, relationship: Dict) -> str:
        """Build optimized CREATE query for a relationship."""
        try:
            source_id = relationship['source_id']
            target_id = relationship['target_id']
            rel_type = self.sanitize_identifier(relationship['type'])
            
            # Simple relationship without heavy properties
            predicate_uri = relationship.get('predicate_uri', '')
            if predicate_uri and len(predicate_uri) < 200:
                safe_predicate = self.sanitize_string_value(predicate_uri)
                return f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type} {{predicate_uri: '{safe_predicate}'}}]->(b)"
            else:
                return f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type}]->(b)"
        
        except Exception as e:
            self.logger.debug(f"Failed to build relationship query: {e}")
            return f"MATCH (a {{entity_id: {relationship['source_id']}}}), (b {{entity_id: {relationship['target_id']}}}) CREATE (a)-[:RELATED]->(b)"
    
    def upload_data_optimized(self, graph_name: str, nodes: List[Dict], relationships: List[Dict]) -> Tuple[int, int]:
        """Upload nodes and relationships with optimized batch processing."""
        nodes_uploaded = 0
        rels_uploaded = 0
        
        # Upload nodes in optimized batches
        if nodes:
            self.logger.info(f"🏗️ Uploading {len(nodes)} nodes...")
            
            for i in range(0, len(nodes), self.process_batch_size):
                batch_end = min(i + self.process_batch_size, len(nodes))
                batch_nodes = nodes[i:batch_end]
                
                uploaded_in_batch = 0
                for node in batch_nodes:
                    try:
                        query = self.build_optimized_node_query(node)
                        self.execute_query_with_timeout(graph_name, query, timeout_override=30)
                        uploaded_in_batch += 1
                    except Exception as e:
                        self.logger.debug(f"Node upload failed: {e}")
                        # Try minimal fallback
                        try:
                            minimal_query = f"CREATE (:Resource {{entity_id: {node['id']}}})"
                            self.execute_query_with_timeout(graph_name, minimal_query, timeout_override=15)
                            uploaded_in_batch += 1
                        except:
                            continue
                
                nodes_uploaded += uploaded_in_batch
                
                # Progress logging
                if i % (self.process_batch_size * 10) == 0:
                    progress = batch_end / len(nodes) * 100
                    self.logger.info(f"📊 Node upload progress: {progress:.1f}% ({batch_end}/{len(nodes)})")
        
        # Upload relationships in optimized batches
        if relationships:
            self.logger.info(f"🔗 Uploading {len(relationships)} relationships...")
            
            for i in range(0, len(relationships), self.process_batch_size):
                batch_end = min(i + self.process_batch_size, len(relationships))
                batch_rels = relationships[i:batch_end]
                
                uploaded_in_batch = 0
                for rel in batch_rels:
                    try:
                        query = self.build_optimized_relationship_query(rel)
                        self.execute_query_with_timeout(graph_name, query, timeout_override=30)
                        uploaded_in_batch += 1
                    except Exception as e:
                        self.logger.debug(f"Relationship upload failed: {e}")
                        # Try minimal fallback
                        try:
                            minimal_query = f"MATCH (a {{entity_id: {rel['source_id']}}}), (b {{entity_id: {rel['target_id']}}}) CREATE (a)-[:RELATED]->(b)"
                            self.execute_query_with_timeout(graph_name, minimal_query, timeout_override=15)
                            uploaded_in_batch += 1
                        except:
                            continue
                
                rels_uploaded += uploaded_in_batch
                
                # Progress logging
                if i % (self.process_batch_size * 10) == 0:
                    progress = batch_end / len(relationships) * 100
                    self.logger.info(f"🔗 Relationship upload progress: {progress:.1f}% ({batch_end}/{len(relationships)})")
        
        return nodes_uploaded, rels_uploaded
    
    def clear_graph_optimized(self, graph_name: str):
        """Clear graph with timeout protection."""
        try:
            self.logger.info("🗑️ Clearing existing graph data...")
            self.execute_query_with_timeout(graph_name, "MATCH (n) DETACH DELETE n", timeout_override=120)
            self.logger.info("✅ Graph cleared successfully")
        except Exception as e:
            self.logger.warning(f"Could not clear graph: {e}")
    
    def convert_file_optimized(self, file_path: str, graph_name: str = "rdf_graph", clear_existing: bool = True, skip_integrity_check: bool = False) -> Dict[str, Any]:
        """Convert RDF file with optimized processing and timeout handling."""
        self.stats['start_time'] = time.time()
        
        self.logger.info(f"🚀 Starting optimized RDF to FalkorDB conversion")
        self.logger.info(f"📁 Input file: {file_path}")
        self.logger.info(f"🎯 Target graph: {graph_name}")
        
        try:
            # Clear existing data if requested
            if clear_existing:
                self.clear_graph_optimized(graph_name)
            
            # Process file in optimized streaming batches
            total_nodes_uploaded = 0
            total_rels_uploaded = 0
            total_nodes_expected = 0
            total_rels_expected = 0
            batch_count = 0
            
            for triples_batch in self.parse_rdf_streaming(file_path):
                batch_count += 1
                self.logger.info(f"📦 Processing batch {batch_count} ({len(triples_batch)} triples)...")
                
                # Convert triples to optimized structures
                nodes, relationships = self.process_triples_batch(triples_batch)
                total_nodes_expected += len(nodes)
                total_rels_expected += len(relationships)
                
                # Upload with optimized batch processing
                if nodes or relationships:
                    nodes_uploaded, rels_uploaded = self.upload_data_optimized(graph_name, nodes, relationships)
                    total_nodes_uploaded += nodes_uploaded
                    total_rels_uploaded += rels_uploaded
                
                self.logger.info(f"✅ Batch {batch_count} completed: "
                               f"{nodes_uploaded}/{len(nodes)} nodes uploaded, {rels_uploaded}/{len(relationships)} relationships uploaded")
                
                # Memory cleanup
                del nodes, relationships
            
            # Perform data integrity verification (unless skipped)
            if not skip_integrity_check:
                integrity_report = self.verify_data_integrity(graph_name, total_nodes_expected, total_rels_expected)
            else:
                self.logger.info("⚠️ Data integrity verification skipped (use --skip-integrity-check)")
                integrity_report = {'skipped': True}
            
            # Final statistics
            self.stats['end_time'] = time.time()
            total_time = self.stats['end_time'] - self.stats['start_time']
            
            final_stats = {
                'total_time_seconds': total_time,
                'triples_processed': self.stats['triples_processed'],
                'nodes_expected': total_nodes_expected,
                'nodes_created': total_nodes_uploaded,
                'relationships_expected': total_rels_expected,
                'relationships_created': total_rels_uploaded,
                'queries_executed': self.stats['queries_executed'],
                'query_timeouts': self.stats['query_timeouts'],
                'connection_resets': self.stats['connection_resets'],
                'errors_handled': self.stats['errors_handled'],
                'throughput_triples_per_second': self.stats['triples_processed'] / total_time if total_time > 0 else 0,
                'avg_query_time_seconds': self.stats['total_query_time'] / self.stats['queries_executed'] if self.stats['queries_executed'] > 0 else 0,
                'data_integrity': integrity_report
            }
            
            self.logger.info("🎉 Optimized conversion completed successfully!")
            self.logger.info(f"⏱️ Total time: {total_time:.2f} seconds")
            self.logger.info(f"📊 Processed: {self.stats['triples_processed']:,} triples")
            self.logger.info(f"🏗️ Expected: {total_nodes_expected:,} nodes, Created: {total_nodes_uploaded:,} nodes")
            self.logger.info(f"🔗 Expected: {total_rels_expected:,} relationships, Created: {total_rels_uploaded:,} relationships")
            self.logger.info(f"🚀 Throughput: {final_stats['throughput_triples_per_second']:.1f} triples/sec")
            self.logger.info(f"⚠️ Query timeouts: {self.stats['query_timeouts']}")
            self.logger.info(f"🔄 Connection resets: {self.stats['connection_resets']}")
            
            # Data integrity summary
            if integrity_report.get('skipped', False):
                self.logger.info("ℹ️ DATA INTEGRITY: Verification skipped")
            elif integrity_report.get('data_complete', False):
                self.logger.info("✅ DATA INTEGRITY: Complete - No data loss detected!")
            elif integrity_report.get('verification_failed', False):
                self.logger.error("❌ DATA INTEGRITY: Verification failed")
            else:
                self.logger.warning("⚠️ DATA INTEGRITY: Some data may not have been uploaded due to errors")
                if integrity_report.get('node_loss', 0) > 0:
                    self.logger.warning(f"   - {integrity_report['node_loss']} nodes were not created")
                if integrity_report.get('relationship_loss', 0) > 0:
                    self.logger.warning(f"   - {integrity_report['relationship_loss']} relationships were not created")
            
            return final_stats
        
        except Exception as e:
            self.logger.error(f"❌ Conversion failed: {e}")
            raise
        
        finally:
            # Close connection
            if self._connection:
                try:
                    self._connection.close()
                    self.logger.info("🔌 Connection closed")
                except:
                    pass


def main():
    """Command-line interface for the optimized converter."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Optimized RDF to FalkorDB converter with timeout handling")
    parser.add_argument("rdf_file", help="Path to RDF file (.ttl, .rdf, .n3, etc.)")
    parser.add_argument("--graph-name", default="rdf_graph", help="FalkorDB graph name")
    parser.add_argument("--host", default="localhost", help="FalkorDB host")
    parser.add_argument("--port", type=int, default=6379, help="FalkorDB port")
    parser.add_argument("--password", help="FalkorDB password")
    
    # Timeout settings
    parser.add_argument("--socket-timeout", type=int, default=30, help="Socket timeout (seconds)")
    parser.add_argument("--connection-timeout", type=int, default=15, help="Connection timeout (seconds)")
    parser.add_argument("--query-timeout", type=int, default=60, help="Query timeout (seconds)")
    parser.add_argument("--falkor-timeout-max", type=int, default=120000, help="FalkorDB max timeout (milliseconds)")
    parser.add_argument("--falkor-timeout-default", type=int, default=60000, help="FalkorDB default timeout (milliseconds)")
    
    # Performance settings
    parser.add_argument("--parse-batch-size", type=int, default=5000, help="RDF parsing batch size")
    parser.add_argument("--process-batch-size", type=int, default=1000, help="Processing batch size")
    parser.add_argument("--upload-concurrency", type=int, default=5, help="Upload concurrency")
    
    parser.add_argument("--keep-existing", action="store_true", help="Don't clear existing graph data")
    parser.add_argument("--skip-integrity-check", action="store_true", help="Skip data integrity verification (faster but less safe)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    # Validate input file
    if not os.path.exists(args.rdf_file):
        print(f"❌ File not found: {args.rdf_file}")
        return 1
    
    try:
        # Create optimized converter
        converter = OptimizedFalkorConverter(
            host=args.host,
            port=args.port,
            password=args.password,
            socket_timeout=args.socket_timeout,
            connection_timeout=args.connection_timeout,
            query_timeout=args.query_timeout,
            falkor_timeout_max=args.falkor_timeout_max,
            falkor_timeout_default=args.falkor_timeout_default,
            parse_batch_size=args.parse_batch_size,
            process_batch_size=args.process_batch_size,
            upload_concurrency=args.upload_concurrency
        )
        
        # Convert file
        stats = converter.convert_file_optimized(
            file_path=args.rdf_file,
            graph_name=args.graph_name,
            clear_existing=not args.keep_existing,
            skip_integrity_check=args.skip_integrity_check
        )
        
        print(f"\n✅ Conversion completed successfully!")
        print(f"📊 Statistics: {stats}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n⚠️ Conversion interrupted by user")
        return 1
    except Exception as e:
        print(f"\n❌ Conversion failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
