#!/usr/bin/env python3
"""
TTL to FalkorDB Property Graph Converter
Converts RDF triples from TTL files to property graph model and ingests into FalkorDB
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import redis
import falkordb
import hashlib
import json
import time
import logging
from typing import Dict, Set, List, Tuple, Any
from urllib.parse import urlparse
import re
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TTLToFalkorDB:
    def __init__(self, redis_host='localhost', redis_port=6379, db_name='knowledge_graph', password=None):
        """Initialize the converter with FalkorDB connection"""
        # Set longer timeouts for large operations
        socket_timeout = 300  # 5 minutes
        socket_connect_timeout = 30  # 30 seconds
        
        if password:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                password=password, 
                decode_responses=True,
                socket_timeout=socket_timeout,
                socket_connect_timeout=socket_connect_timeout,
                retry_on_timeout=True,
                health_check_interval=30
            )
            self.db = falkordb.FalkorDB(
                host=redis_host, 
                port=redis_port, 
                password=password
            ).select_graph(db_name)
        else:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                decode_responses=True,
                socket_timeout=socket_timeout,
                socket_connect_timeout=socket_connect_timeout,
                retry_on_timeout=True,
                health_check_interval=30
            )
            self.db = falkordb.FalkorDB(
                host=redis_host, 
                port=redis_port
            ).select_graph(db_name)
        self.batch_size = 1000
        self.node_cache = set()
        self.edge_cache = set()
        
    def clean_identifier(self, uri_or_literal: str) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            # Extract meaningful part from URI for label
            parsed = urlparse(str(uri_or_literal))
            if parsed.fragment:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.fragment)
            elif parsed.path:
                local_name = parsed.path.split('/')[-1]
                if local_name:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', local_name)
                else:
                    # Use domain name if no local name
                    return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.netloc)
            else:
                return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
        else:
            return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
    
    def create_node_id(self, resource) -> str:
        """Create node ID - use URI directly for URIRefs, special handling for blank nodes"""
        if isinstance(resource, BNode):
            return f"_:bnode_{str(resource)}"
        else:
            # Use the actual URI as the ID
            return str(resource)
    
    def create_node_label(self, resource) -> str:
        """Create node label from URI"""
        if isinstance(resource, BNode):
            return "BlankNode"
        else:
            # Extract meaningful label from URI
            uri_str = str(resource)
            parsed = urlparse(uri_str)
            
            # Try to get a meaningful label
            if parsed.fragment:
                return self.clean_identifier(URIRef(parsed.fragment))
            elif parsed.path:
                parts = parsed.path.strip('/').split('/')
                if parts and parts[-1]:
                    return self.clean_identifier(URIRef(parts[-1]))
                else:
                    # Use domain name
                    return self.clean_identifier(URIRef(parsed.netloc))
            else:
                # Fallback to cleaned full URI
                return self.clean_identifier(resource)
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, Any]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path:
            parts = parsed.path.strip('/').split('/')
            if parts and parts[-1]:
                properties['local_name'] = parts[-1]
                properties['namespace'] = uri_str.replace(parts[-1], '').rstrip('/')
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[Any, str]:
        """Process literal value and return (value, datatype)"""
        if literal.datatype:
            datatype = str(literal.datatype)
            # Handle common datatypes
            if 'integer' in datatype or 'int' in datatype:
                try:
                    return int(literal), 'integer'
                except:
                    return str(literal), 'string'
            elif 'decimal' in datatype or 'double' in datatype or 'float' in datatype:
                try:
                    return float(literal), 'float'
                except:
                    return str(literal), 'string'
            elif 'boolean' in datatype:
                return str(literal).lower() in ('true', '1'), 'boolean'
            elif 'date' in datatype:
                return str(literal), 'date'
            else:
                return str(literal), 'string'
        else:
            return str(literal), 'string'
    
    def parse_ttl_streaming(self, ttl_file_path: str):
        """Parse TTL file in streaming fashion"""
        logger.info(f"Starting to parse {ttl_file_path}")
        
        graph = Graph()
        
        # Parse the file
        try:
            graph.parse(ttl_file_path, format='turtle')
            logger.info(f"Successfully parsed TTL file. Found {len(graph)} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        return graph
    
    def convert_to_property_graph(self, graph: Graph):
        """Convert RDF graph to property graph model"""
        logger.info("Converting RDF triples to property graph model...")
        
        nodes = {}
        edges = []
        literal_properties = {}  # subject -> {predicate: value}
        
        total_triples = len(graph)
        processed = 0
        
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                processed += 1
                pbar.update(1)
                
                subject_id = self.create_node_id(subject)
                subject_label = self.create_node_label(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Create subject node if not exists
                if subject_id not in nodes:
                    if isinstance(subject, URIRef):
                        properties = self.extract_properties_from_uri(subject)
                        # Add the URI as the primary identifier
                        properties['uri'] = str(subject)
                    else:  # BNode
                        properties = {'uri': str(subject), 'blank_node_id': str(subject)}
                    
                    nodes[subject_id] = {
                        'id': subject_id,
                        'label': subject_label,
                        'properties': properties
                    }
                
                # Handle object
                if isinstance(obj, Literal):
                    # Object is a literal - add as property to subject node
                    if subject_id not in literal_properties:
                        literal_properties[subject_id] = {}
                    
                    value, datatype = self.process_literal_value(obj)
                    literal_properties[subject_id][predicate_clean] = {
                        'value': value,
                        'datatype': datatype
                    }
                    
                    # Also store language if present
                    if obj.language:
                        literal_properties[subject_id][f"{predicate_clean}_lang"] = obj.language
                
                else:
                    # Object is a resource - create edge
                    object_id = self.create_node_id(obj)
                    object_label = self.create_node_label(obj)
                    
                    # Create object node if not exists
                    if object_id not in nodes:
                        if isinstance(obj, URIRef):
                            properties = self.extract_properties_from_uri(obj)
                            properties['uri'] = str(obj)
                        else:  # BNode
                            properties = {'uri': str(obj), 'blank_node_id': str(obj)}
                        
                        nodes[object_id] = {
                            'id': object_id,
                            'label': object_label,
                            'properties': properties
                        }
                    
                    # Create edge
                    edge = {
                        'from': subject_id,
                        'to': object_id,
                        'from_label': subject_label,
                        'to_label': object_label,
                        'relationship': predicate_clean,
                        'properties': {
                            'predicate_uri': str(predicate)
                        }
                    }
                    edges.append(edge)
        
        # Merge literal properties into nodes
        for node_id, props in literal_properties.items():
            if node_id in nodes:
                for prop_name, prop_data in props.items():
                    nodes[node_id]['properties'][prop_name] = prop_data['value']
                    nodes[node_id]['properties'][f"{prop_name}_datatype"] = prop_data['datatype']
        
        logger.info(f"Conversion complete. Created {len(nodes)} nodes and {len(edges)} edges.")
        return list(nodes.values()), edges
    
    def escape_cypher_string(self, value: str) -> str:
        """Escape string for Cypher query"""
        if isinstance(value, str):
            # Escape single quotes, double quotes, backslashes, and newlines
            escaped = value.replace("\\", "\\\\")  # Escape backslashes first
            escaped = escaped.replace("'", "\\'")
            escaped = escaped.replace('"', '\\"')
            escaped = escaped.replace('\n', '\\n')
            escaped = escaped.replace('\r', '\\r')
            escaped = escaped.replace('\t', '\\t')
            return escaped
        return str(value)
    
    def escape_uri_for_cypher(self, uri: str) -> str:
        """Escape URI specifically for use in Cypher queries"""
        # For URIs used as node IDs, we need to be extra careful with escaping
        escaped = self.escape_cypher_string(uri)
        return escaped
    
    def create_cypher_properties(self, properties: Dict[str, Any]) -> str:
        """Convert properties dict to Cypher properties string"""
        prop_parts = []
        for key, value in properties.items():
            # Ensure key is valid Cypher identifier
            clean_key = re.sub(r'[^a-zA-Z0-9_]', '_', str(key))
            if clean_key != key:
                logger.debug(f"Property key changed from '{key}' to '{clean_key}'")
            
            if isinstance(value, str):
                escaped_value = self.escape_cypher_string(value)
                prop_parts.append(f"{clean_key}: '{escaped_value}'")
            elif isinstance(value, (int, float)):
                prop_parts.append(f"{clean_key}: {value}")
            elif isinstance(value, bool):
                prop_parts.append(f"{clean_key}: {str(value).lower()}")
            else:
                escaped_value = self.escape_cypher_string(str(value))
                prop_parts.append(f"{clean_key}: '{escaped_value}'")
        
        return "{" + ", ".join(prop_parts) + "}"
    
    def execute_query_with_retry(self, query, max_retries=3, delay=1):
        """Execute query with retry logic for timeout handling"""
        for attempt in range(max_retries):
            try:
                result = self.db.query(query)
                return result
            except (redis.exceptions.TimeoutError, redis.exceptions.ConnectionError) as e:
                logger.warning(f"Query timeout/connection error (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay * (attempt + 1))  # Exponential backoff
                    # Try to reconnect
                    try:
                        self.redis_client.ping()
                    except:
                        logger.warning("Reconnecting to FalkorDB...")
                        time.sleep(2)
                else:
                    logger.error(f"Query failed after {max_retries} attempts: {query[:100]}...")
                    raise
            except Exception as e:
                logger.error(f"Unexpected error in query: {e}")
                logger.error(f"Query: {query[:200]}...")
                raise
    
    def ingest_nodes_batch(self, nodes: List[Dict], batch_start: int, batch_end: int):
        """Ingest a batch of nodes into FalkorDB with timeout handling"""
        batch_nodes = nodes[batch_start:batch_end]
        
        if not batch_nodes:
            return
        
        # Use smaller sub-batches to prevent timeouts
        sub_batch_size = min(50, len(batch_nodes))  # Process max 50 nodes at once
        
        for i in range(0, len(batch_nodes), sub_batch_size):
            sub_batch = batch_nodes[i:i + sub_batch_size]
            
            # Create individual MERGE statements to avoid complex batch operations
            for node in sub_batch:
                try:
                    node_id = node['id']  # This is now the actual URI
                    node_label = node['label']  # This is the extracted label from URI
                    escaped_node_id = self.escape_uri_for_cypher(node_id)
                    
                    # Clean the label to ensure it's a valid Cypher label
                    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', node_label)
                    if not clean_label or clean_label[0].isdigit():
                        clean_label = f"Node_{clean_label}"
                    
                    properties_str = self.create_cypher_properties(node['properties'])
                    
                    # Create node with URI as ID and extracted label as the node label
                    # Use uri property for matching since IDs can be complex
                    cypher = f"MERGE (n:{clean_label} {{uri: '{escaped_node_id}'}}) SET n += {properties_str}"
                    
                    self.execute_query_with_retry(cypher)
                    
                except Exception as e:
                    logger.error(f"Error inserting node {node_id}: {e}")
                    # Continue with next node instead of failing completely
                    continue
            
            # Small delay between sub-batches
            time.sleep(0.05)
    
    def ingest_edges_batch(self, edges: List[Dict], batch_start: int, batch_end: int):
        """Ingest a batch of edges into FalkorDB with timeout handling"""
        batch_edges = edges[batch_start:batch_end]
        
        if not batch_edges:
            return
        
        # Use smaller sub-batches for edges too
        sub_batch_size = min(25, len(batch_edges))  # Even smaller for edges
        
        for i in range(0, len(batch_edges), sub_batch_size):
            sub_batch = batch_edges[i:i + sub_batch_size]
            
            for edge in sub_batch:
                try:
                    from_id = edge['from']  # URI of source node
                    to_id = edge['to']      # URI of target node
                    from_label = edge['from_label']
                    to_label = edge['to_label']
                    relationship = edge['relationship']
                    
                    # Clean labels for Cypher
                    clean_from_label = re.sub(r'[^a-zA-Z0-9_]', '_', from_label)
                    clean_to_label = re.sub(r'[^a-zA-Z0-9_]', '_', to_label)
                    clean_relationship = re.sub(r'[^a-zA-Z0-9_]', '_', relationship)
                    
                    if not clean_from_label or clean_from_label[0].isdigit():
                        clean_from_label = f"Node_{clean_from_label}"
                    if not clean_to_label or clean_to_label[0].isdigit():
                        clean_to_label = f"Node_{clean_to_label}"
                    if not clean_relationship or clean_relationship[0].isdigit():
                        clean_relationship = f"REL_{clean_relationship}"
                    
                    escaped_from_id = self.escape_uri_for_cypher(from_id)
                    escaped_to_id = self.escape_uri_for_cypher(to_id)
                    
                    properties_str = self.create_cypher_properties(edge['properties'])
                    
                    # Find nodes by URI and create edge
                    cypher = f"""
                    MATCH (from:{clean_from_label} {{uri: '{escaped_from_id}'}})
                    MATCH (to:{clean_to_label} {{uri: '{escaped_to_id}'}})
                    MERGE (from)-[r:{clean_relationship}]->(to)
                    SET r += {properties_str}
                    """
                    
                    self.execute_query_with_retry(cypher.strip())
                    
                except Exception as e:
                    logger.error(f"Error inserting edge {from_id}->{to_id}: {e}")
                    # Continue with next edge
                    continue
            
            # Small delay between sub-batches
            time.sleep(0.05)
    
    def ingest_to_falkordb(self, nodes: List[Dict], edges: List[Dict]):
        """Ingest nodes and edges into FalkorDB with batching"""
        logger.info("Starting ingestion into FalkorDB...")
        
        # Create indexes for better performance
        try:
            self.db.query("CREATE INDEX ON :Node(id)")
            self.db.query("CREATE INDEX ON :Resource(uri)")
        except Exception as e:
            logger.warning(f"Index creation warning (may already exist): {e}")
        
        # Ingest nodes in batches
        logger.info(f"Ingesting {len(nodes)} nodes in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(nodes), self.batch_size), desc="Ingesting nodes"):
            batch_end = min(i + self.batch_size, len(nodes))
            self.ingest_nodes_batch(nodes, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        # Ingest edges in batches
        logger.info(f"Ingesting {len(edges)} edges in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(edges), self.batch_size), desc="Ingesting edges"):
            batch_end = min(i + self.batch_size, len(edges))
            self.ingest_edges_batch(edges, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        logger.info("Ingestion completed successfully!")
    
    def convert_and_ingest(self, ttl_file_path: str):
        """Main method to convert TTL file and ingest into FalkorDB"""
        try:
            # Parse TTL file
            graph = self.parse_ttl_streaming(ttl_file_path)
            
            # Convert to property graph
            nodes, edges = self.convert_to_property_graph(graph)
            
            # Ingest into FalkorDB
            self.ingest_to_falkordb(nodes, edges)
            
            # Print summary
            result = self.db.query("MATCH (n) RETURN count(n) as node_count")
            node_count = result.result_set[0][0] if result.result_set else 0
            
            result = self.db.query("MATCH ()-[r]->() RETURN count(r) as edge_count")
            edge_count = result.result_set[0][0] if result.result_set else 0
            
            logger.info(f"Final database statistics:")
            logger.info(f"  Nodes: {node_count}")
            logger.info(f"  Edges: {edge_count}")
            
        except Exception as e:
            logger.error(f"Error in conversion and ingestion: {e}")
            raise

def main():
    """Main function with example usage"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description='Convert TTL file to FalkorDB property graph')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--host', default='localhost', help='Redis/FalkorDB host (default: localhost)')
    parser.add_argument('--port', type=int, default=6379, help='Redis/FalkorDB port (default: 6379)')
    parser.add_argument('--db_name', default='knowledge_graph', help='Database name (default: knowledge_graph)')
    parser.add_argument('--batch_size', type=int, default=0, help='Batch size for ingestion (0=auto-detect)')
    parser.add_argument('--password', help='Redis password if required')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        return
    
    # Auto-detect batch size based on file size
    batch_size = args.batch_size
    if batch_size == 0:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        if file_size_mb > 1000:  # >1GB
            batch_size = 50  # Much smaller for very large files
            logger.info(f"Large file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
        elif file_size_mb > 100:  # >100MB
            batch_size = 100  # Smaller for medium files
            logger.info(f"Medium file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
        else:
            batch_size = 200  # Conservative for small files
            logger.info(f"Small file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
    
    logger.info("=== TIMEOUT PREVENTION TIPS ===")
    logger.info("If you experience timeouts:")
    logger.info("1. Restart FalkorDB with: docker restart falkordb")
    logger.info("2. Use smaller batch size: --batch_size 25")
    logger.info("3. Increase FalkorDB memory: docker run -m 16g ...")
    logger.info("4. The script will auto-retry failed operations")
    logger.info("================================")
    
    # Test connection first
    try:
        if args.password:
            test_client = redis.Redis(host=args.host, port=args.port, password=args.password, decode_responses=True)
        else:
            test_client = redis.Redis(host=args.host, port=args.port, decode_responses=True)
        test_client.ping()
        logger.info(f"Successfully connected to FalkorDB at {args.host}:{args.port}")
    except redis.exceptions.AuthenticationError:
        logger.error(f"Authentication failed. If using password 'falkordb', try:")
        logger.error(f"  python3 ttl_converter.py {args.ttl_file} --password falkordb")
        logger.error(f"  OR with username: --password falkordb (username 'default' is used by default)")
        return
    except Exception as e:
        logger.error(f"Failed to connect to FalkorDB at {args.host}:{args.port}: {e}")
        logger.error("Please ensure FalkorDB is running and accessible")
        logger.error("If using authentication, add --password your_password")
        return
    
    # Create converter
    converter = TTLToFalkorDB(
        redis_host=args.host,
        redis_port=args.port,
        db_name=args.db_name,
        password=args.password
    )
    converter.batch_size = batch_size
    
    # Convert and ingest
    converter.convert_and_ingest(args.ttl_file)

if __name__ == "__main__":
    main()
