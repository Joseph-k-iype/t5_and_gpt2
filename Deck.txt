import requests
import time
from rdflib import Graph

class SPARQLQuerySplitter:
    def __init__(self, endpoint_url, headers=None):
        self.endpoint_url = endpoint_url
        self.headers = headers or {
            'Content-Type': 'application/sparql-query',
            'Accept': 'text/turtle'
        }
        
    def execute_construct_query(self, query, output_file):
        """Execute a CONSTRUCT query and save result to TTL file"""
        try:
            response = requests.post(
                self.endpoint_url,
                data=query,
                headers=self.headers,
                timeout=300  # 5 minutes
            )
            
            if response.status_code == 200:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"âœ“ Query saved to {output_file}")
                return True
            else:
                print(f"âœ— Query failed: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âœ— Error executing query: {e}")
            return False
    
    def execute_all_queries(self):
        """Execute all split queries"""
        
        queries = {
            "reports_basic.ttl": """
                PREFIX collibra: <http://www.hstbc.com/collibra/>
                
                CONSTRUCT {
                  ?s a collibra:Report .
                  ?s collibra:reportProducingProcess ?bp .
                  ?s collibra:associatedProcess ?associatedProcess .
                }
                WHERE {
                  ?s a collibra:Report .
                  ?s collibra:reportProducingProcess ?bp .
                  ?bp collibra:isAssociatedWithProcess ?associatedProcess .
                }
            """,
            
            "processes.ttl": """
                PREFIX collibra: <http://www.hstbc.com/collibra/>
                
                CONSTRUCT {
                  ?bp collibra:isAssociatedWithProcess ?associatedProcess .
                  ?bp collibra:consumes ?bde .
                  ?bp collibra:produces ?bde .
                  ?bp collibra:consumedStoredIn ?col .
                  ?bp collibra:producedStoredIn ?col .
                }
                WHERE {
                  ?s a collibra:Report .
                  ?s collibra:reportProducingProcess ?bp .
                  ?bp collibra:isAssociatedWithProcess ?associatedProcess .
                  ?bp collibra:consumes ?bde .
                  ?bp collibra:produces ?bde .
                  OPTIONAL { ?col collibra:hascolID ?agcId }
                  OPTIONAL { ?col collibra:consumedStoredIn ?col }
                  OPTIONAL { ?col collibra:producedStoredIn ?col }
                }
            """,
            
            "data_elements.ttl": """
                PREFIX collibra: <http://www.hstbc.com/collibra/>
                
                CONSTRUCT {
                  ?bde ?bdeProperty ?bdeObject .
                  ?col ?colProperty ?colObject .
                  ?col collibra:hascolID ?agcId .
                }
                WHERE {
                  ?s a collibra:Report .
                  ?s collibra:reportProducingProcess ?bp .
                  ?bp collibra:consumes ?bde .
                  ?bde ?bdeProperty ?bdeObject .
                  ?col ?colProperty ?colObject .
                  ?col collibra:hascolID ?agcId .
                }
            """,
            
            "applications.ttl": """
                PREFIX collibra: <http://www.hstbc.com/collibra/>
                
                CONSTRUCT {
                  ?app ?appProperty ?appObject .
                }
                WHERE {
                  ?s a collibra:Report .
                  ?s collibra:reportProducingProcess ?bp .
                  ?bp collibra:isAssociatedWithProcess ?associatedProcess .
                  ?bp collibra:consumes ?bde .
                  OPTIONAL { ?col collibra:consumedStoredIn ?app }
                  OPTIONAL { ?col collibra:producedStoredIn ?app }
                  ?app ?appProperty ?appObject .
                }
            """
        }
        
        successful_files = []
        
        for filename, query in queries.items():
            print(f"\nExecuting query for {filename}...")
            if self.execute_construct_query(query, filename):
                successful_files.append(filename)
            time.sleep(2)  # Brief pause between queries
            
        return successful_files
    
    def merge_ttl_files(self, ttl_files, output_file="merged_graph.ttl"):
        """Merge multiple TTL files into one"""
        try:
            merged_graph = Graph()
            
            # Add common prefixes
            merged_graph.bind("collibra", "http://www.hstbc.com/collibra/")
            
            for ttl_file in ttl_files:
                print(f"Merging {ttl_file}...")
                temp_graph = Graph()
                temp_graph.parse(ttl_file, format="turtle")
                
                # Add all triples to merged graph
                for triple in temp_graph:
                    merged_graph.add(triple)
                    
                print(f"  Added {len(temp_graph)} triples from {ttl_file}")
            
            # Serialize merged graph
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(merged_graph.serialize(format="turtle"))
                
            print(f"\nâœ“ Merged graph saved to {output_file}")
            print(f"  Total triples: {len(merged_graph)}")
            
            return output_file
            
        except Exception as e:
            print(f"âœ— Error merging files: {e}")
            return None

# Usage example
if __name__ == "__main__":
    # Configure your Anzo Graph endpoint
    ANZO_ENDPOINT = "http://your-anzo-server:port/sparql"
    
    # Optional: Add authentication headers
    headers = {
        'Content-Type': 'application/sparql-query',
        'Accept': 'text/turtle',
        # 'Authorization': 'Bearer your-token'  # if needed
    }
    
    splitter = SPARQLQuerySplitter(ANZO_ENDPOINT, headers)
    
    print("Starting split query execution...")
    successful_files = splitter.execute_all_queries()
    
    if successful_files:
        print(f"\nSuccessfully executed {len(successful_files)} queries")
        print("Merging TTL files...")
        merged_file = splitter.merge_ttl_files(successful_files, "complete_graph.ttl")
        
        if merged_file:
            print(f"\nðŸŽ‰ Complete! Your merged graph is in: {merged_file}")
    else:
        print("No queries executed successfully")
