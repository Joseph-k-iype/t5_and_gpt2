#!/usr/bin/env python3
"""
TTL to FalkorDB Pipeline - FIXED VERSION
Root cause: FalkorDB import folder configuration issue
Solution: Configure import folder properly + use correct Docker setup
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import sys
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
import shutil
from pathlib import Path
import platform
import subprocess

# Setup logging with UTF-8 encoding support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class TTLToFalkorDBConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to FalkorDB converter"""
        self.output_dir = Path(output_dir).resolve()  # Use absolute path
        self.nodes = {}  # node_id -> {node_type, properties}
        self.edges = []  # list of {source_id, target_id, edge_type, properties}
        self.node_id_map = {}  # URI -> unique_id
        self.next_id = 1
        self.is_windows = platform.system() == 'Windows'
        
        # Create output directory
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Output directory: {self.output_dir}")
        except Exception as e:
            logger.error(f"Failed to create output directory {self.output_dir}: {e}")
            raise
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        try:
            if isinstance(uri_or_literal, URIRef):
                uri_str = str(uri_or_literal)
                parsed = urlparse(uri_str)
                
                # Try fragment first (after #)
                if parsed.fragment:
                    name = parsed.fragment
                # Then try last path component
                elif parsed.path and parsed.path != '/':
                    name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
                # Fall back to netloc
                elif parsed.netloc:
                    name = parsed.netloc.replace('.', '_')
                else:
                    # Use hash of full URI as fallback
                    name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
                # Clean the name - only alphanumeric and underscore
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                # Ensure it starts with letter or underscore
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"n_{cleaned}"
                # Ensure it's not empty and has reasonable length
                if not cleaned or len(cleaned) < 1:
                    cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
                return cleaned[:50]  # Limit length
            else:
                # Handle literals or other types
                name = str(uri_or_literal)
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"l_{cleaned}"
                return (cleaned or "literal")[:50]
        except Exception as e:
            logger.warning(f"Error cleaning identifier for {uri_or_literal}: {e}")
            return f"error_{hashlib.md5(str(uri_or_literal).encode()).hexdigest()[:8]}"
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique ID for a resource"""
        try:
            uri = str(resource)
            if uri not in self.node_id_map:
                # Use hash-based IDs to avoid integer overflow issues
                hash_obj = hashlib.md5(uri.encode('utf-8'))
                node_id = f"n_{hash_obj.hexdigest()[:8]}"
                
                # Ensure uniqueness
                original_id = node_id
                counter = 1
                while node_id in [node['id'] for node in self.nodes.values()]:
                    node_id = f"{original_id}_{counter}"
                    counter += 1
                
                self.node_id_map[uri] = node_id
            return self.node_id_map[uri]
        except Exception as e:
            logger.error(f"Error creating node ID for {resource}: {e}")
            # Fallback to simple hash
            fallback_id = f"error_{hashlib.md5(str(resource).encode()).hexdigest()[:8]}"
            return fallback_id
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {}
        try:
            uri_str = str(uri)
            properties['uri'] = uri_str
            
            # Extract namespace and local name
            parsed = urlparse(uri_str)
            if parsed.fragment:
                properties['local_name'] = parsed.fragment
                properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
            elif parsed.path and parsed.path != '/':
                parts = [p for p in parsed.path.strip('/').split('/') if p]
                if parts:
                    properties['local_name'] = parts[-1]
                    properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        except Exception as e:
            logger.warning(f"Error extracting properties from URI {uri}: {e}")
            properties['uri'] = str(uri)
        
        return properties
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        try:
            if value is None:
                return ''
            
            if isinstance(value, bool):
                return 'true' if value else 'false'
            
            if isinstance(value, (list, dict)):
                # Convert to JSON string for complex data
                return json.dumps(value, ensure_ascii=False)
            
            # Convert to string and clean
            str_value = str(value).strip()
            
            # Replace problematic characters for CSV
            str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
            str_value = re.sub(r'\s+', ' ', str_value)  # Normalize whitespace
            
            # Escape quotes by doubling them (CSV standard)
            if '"' in str_value:
                str_value = str_value.replace('"', '""')
            
            # Limit string length to prevent memory issues
            if len(str_value) > 500:
                logger.warning(f"Very long string truncated (length: {len(str_value)})")
                str_value = str_value[:500] + "..."
            
            return str_value
        except Exception as e:
            logger.warning(f"Error sanitizing value {value}: {e}")
            return str(value) if value is not None else ''
    
    def process_literal_value(self, literal: Literal) -> str:
        """Process literal value and return cleaned string"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                # Handle common XSD datatypes
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int_val = int(literal)
                        # Keep reasonable sized integers
                        if abs(int_val) <= 2147483647:
                            return str(int_val)
                        else:
                            return str(literal)  # Keep as string if too large
                    except ValueError:
                        return str(literal)
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:
                            return str(float_val)
                        else:
                            return str(literal)
                    except ValueError:
                        return str(literal)
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower()
                    
                else:
                    return str(literal)
            else:
                return str(literal)
                
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal)
    
    def convert_ttl_to_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL file to CSV format for FalkorDB LOAD CSV"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Parse TTL file
        graph = Graph()
        try:
            logger.info("Parsing TTL file...")
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Successfully parsed TTL file. Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found in the TTL file!")
            return [], None
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"Processing only first {max_triples} of {total_triples} triples")
            total_triples = max_triples
        
        logger.info("Processing triples and building data structures...")
        processed_count = 0
        try:
            with tqdm(total=total_triples, desc="Processing triples") as pbar:
                for subject, predicate, obj in graph:
                    if max_triples and processed_count >= max_triples:
                        break
                    
                    try:
                        pbar.update(1)
                        processed_count += 1
                        
                        # Get or create subject node
                        subject_id = self.get_or_create_node_id(subject)
                        subject_type = self.clean_identifier(subject)
                        predicate_clean = self.clean_identifier(predicate)
                        
                        # Initialize subject node if not exists
                        if subject_id not in self.nodes:
                            if isinstance(subject, URIRef):
                                base_props = self.extract_properties_from_uri(subject)
                            else:  # BNode
                                base_props = {
                                    'uri': str(subject),
                                    'resource_type': 'blank_node'
                                }
                            
                            self.nodes[subject_id] = {
                                'id': subject_id,
                                'node_type': subject_type,
                                'properties': base_props
                            }
                        
                        # Handle object
                        if isinstance(obj, Literal):
                            # Add as property to subject node
                            value = self.process_literal_value(obj)
                            prop_name = predicate_clean
                            
                            # Store the value as a property
                            self.nodes[subject_id]['properties'][prop_name] = value
                            
                            # Store language if present
                            if obj.language:
                                lang_prop = f"{prop_name}_lang"
                                self.nodes[subject_id]['properties'][lang_prop] = obj.language
                        
                        else:
                            # Object is a resource - create edge
                            object_id = self.get_or_create_node_id(obj)
                            object_type = self.clean_identifier(obj)
                            
                            # Initialize object node if not exists
                            if object_id not in self.nodes:
                                if isinstance(obj, URIRef):
                                    base_props = self.extract_properties_from_uri(obj)
                                else:  # BNode
                                    base_props = {
                                        'uri': str(obj),
                                        'resource_type': 'blank_node'
                                    }
                                
                                self.nodes[object_id] = {
                                    'id': object_id,
                                    'node_type': object_type,
                                    'properties': base_props
                                }
                            
                            # Create edge
                            edge = {
                                'source_id': subject_id,
                                'target_id': object_id,
                                'edge_type': predicate_clean,
                                'properties': {
                                    'predicate_uri': str(predicate)
                                }
                            }
                            self.edges.append(edge)
                    except Exception as e:
                        logger.warning(f"Error processing triple {processed_count}: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error during triple processing: {e}")
            raise
        
        logger.info(f"Data processing complete:")
        logger.info(f"  Nodes: {len(self.nodes):,}")
        logger.info(f"  Edges: {len(self.edges):,}")
        
        # Write CSV files
        try:
            csv_files = self.write_csv_files()
        except Exception as e:
            logger.error(f"Error writing CSV files: {e}")
            raise
        
        return csv_files
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names from all nodes
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        
        all_properties = sorted(all_properties)
        
        try:
            with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: id, node_type, then all properties
                headers = ['id', 'node_type'] + all_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for node in self.nodes.values():
                    row = [
                        node['id'],
                        node['node_type']
                    ]
                    
                    # Add property values
                    for prop in all_properties:
                        value = node['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.nodes):,} nodes to {nodes_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing nodes file {nodes_file}: {e}")
            raise
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        # Get all possible edge property names
        all_edge_properties = set()
        for edge in self.edges:
            all_edge_properties.update(edge['properties'].keys())
        
        all_edge_properties = sorted(all_edge_properties)
        
        try:
            with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: source_id, target_id, edge_type, then all properties
                headers = ['source_id', 'target_id', 'edge_type'] + all_edge_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for edge in self.edges:
                    row = [
                        edge['source_id'],
                        edge['target_id'],
                        edge['edge_type']
                    ]
                    
                    # Add property values
                    for prop in all_edge_properties:
                        value = edge['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.edges):,} edges to {edges_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing edges file {edges_file}: {e}")
            raise
        
        return csv_files
    
    def start_falkordb_with_proper_config(self, container_name="falkordb_fixed"):
        """Start FalkorDB with proper import folder configuration"""
        logger.info("Starting FalkorDB with proper import folder configuration...")
        
        # Use /data as the import directory inside container (this is more standard)
        import_dir_in_container = "/data"
        
        try:
            # Check if container already exists
            result = subprocess.run([
                'docker', 'ps', '-a', '--filter', f'name={container_name}', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if container_name in result.stdout:
                logger.info(f"Stopping and removing existing container: {container_name}")
                subprocess.run(['docker', 'stop', container_name], capture_output=True)
                subprocess.run(['docker', 'rm', container_name], capture_output=True)
            
            # Start container with proper volume mount and configuration
            docker_cmd = [
                "docker", "run", "-d",
                "--name", container_name,
                "-p", "6379:6379",
                "-p", "3000:3000",
                # Mount our csv_output directory to /data inside container
                "-v", f"{self.output_dir}:{import_dir_in_container}",
                # Configure FalkorDB to use /data as import folder
                "-e", f"FALKORDB_ARGS=IMPORT_FOLDER {import_dir_in_container}",
                "falkordb/falkordb:latest"
            ]
            
            logger.info(f"Starting container: {' '.join(docker_cmd)}")
            result = subprocess.run(docker_cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"Failed to start container: {result.stderr}")
                return False, None
            
            # Wait for container to start
            time.sleep(5)
            
            # Verify container is running
            verify_result = subprocess.run([
                'docker', 'ps', '--filter', f'name={container_name}', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if container_name in verify_result.stdout:
                logger.info(f"FalkorDB container started: {container_name}")
                logger.info(f"Import directory configured: {import_dir_in_container}")
                logger.info("Web interface: http://localhost:3000")
                return True, container_name
            else:
                logger.error("Container failed to start properly")
                return False, None
                
        except Exception as e:
            logger.error(f"Error starting Docker container: {e}")
            return False, None
    
    def test_and_configure_import_folder(self, host: str = '127.0.0.1', port: int = 6379, 
                                       password: Optional[str] = None):
        """Test connection and configure import folder if needed"""
        logger.info("Testing connection and configuring import folder...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test basic connection
            r.ping()
            logger.info("Connected to FalkorDB")
            
            # Check current import folder configuration
            try:
                current_config = r.execute_command("GRAPH.CONFIG", "GET", "*")
                logger.info("Current FalkorDB configuration retrieved")
                
                # Try to get import folder setting
                import_folder_set = False
                for i in range(0, len(current_config), 2):
                    if i + 1 < len(current_config):
                        key = current_config[i]
                        value = current_config[i + 1]
                        if 'IMPORT' in str(key).upper():
                            logger.info(f"Found import config: {key} = {value}")
                            import_folder_set = True
                
                if not import_folder_set:
                    logger.info("Attempting to set import folder to /data")
                    try:
                        r.execute_command("GRAPH.CONFIG", "SET", "IMPORT_FOLDER", "/data")
                        logger.info("Import folder configured to /data")
                    except Exception as e:
                        logger.warning(f"Could not set import folder: {e}")
                        logger.info("This might be normal - continuing with default configuration")
                
            except Exception as e:
                logger.warning(f"Could not check/set configuration: {e}")
                logger.info("Continuing with default configuration")
            
            return True
            
        except Exception as e:
            logger.error(f"Connection/configuration failed: {e}")
            return False
    
    def generate_cypher_commands(self):
        """Generate Cypher commands"""
        cypher_file = self.output_dir / "load_data.cypher"
        
        try:
            with open(cypher_file, 'w', encoding='utf-8') as f:
                f.write("// FalkorDB LOAD CSV Commands - FIXED VERSION\n")
                f.write("// Root cause fixed: proper import folder configuration\n")
                f.write(f"// CSV files location: {self.output_dir}\n\n")
                
                f.write("// Clear existing data (optional)\n")
                f.write("// MATCH (n) DETACH DELETE n;\n\n")
                
                f.write("// Load nodes (files should be accessible in import directory)\n")
                f.write("LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row\n")
                f.write("CREATE (n:Entity)\n")
                f.write("SET n = row;\n\n")
                
                f.write("// Load edges\n")
                f.write("LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row\n")
                f.write("MATCH (source:Entity {id: row.source_id})\n")
                f.write("MATCH (target:Entity {id: row.target_id})\n")
                f.write("CREATE (source)-[r:CONNECTED_TO]->(target)\n")
                f.write("SET r = row\n")
                f.write("REMOVE r.source_id, r.target_id, r.edge_type;\n\n")
                
                f.write("// Verify data loaded\n")
                f.write("MATCH (n) RETURN count(n) AS node_count;\n")
                f.write("MATCH ()-[r]->() RETURN count(r) AS edge_count;\n")
                f.write("MATCH (n) RETURN n.node_type, count(*) AS count ORDER BY count DESC LIMIT 10;\n")
            
            logger.info(f"Generated Cypher commands in {cypher_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing Cypher file {cypher_file}: {e}")
            raise
        
        return str(cypher_file)
    
    def execute_cypher_commands(self, graph_name: str, 
                              host: str = '127.0.0.1', port: int = 6379, 
                              password: Optional[str] = None):
        """Execute Cypher commands on FalkorDB"""
        logger.info(f"Executing Cypher commands on graph '{graph_name}'...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test connection
            r.ping()
            logger.info("Connected to FalkorDB")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            return False
        
        try:
            # Load nodes
            logger.info("Loading nodes...")
            nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row
            CREATE (n:Entity)
            SET n = row
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
                logger.info(f"Nodes loaded successfully: {result}")
            except Exception as e:
                logger.error(f"Failed to load nodes: {e}")
                if "error opening csv uri" in str(e).lower():
                    logger.error("DIAGNOSIS:")
                    logger.error("  - CSV files are not accessible to FalkorDB")
                    logger.error("  - Import folder is not configured correctly")
                    logger.error("SOLUTIONS:")
                    logger.error("  1. Use --start_proper to start container with correct configuration")
                    logger.error("  2. Ensure files are in the correct import directory")
                    logger.error("  3. Check Docker volume mounting")
                return False
            
            # Load edges
            logger.info("Loading edges...")
            edges_query = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            MATCH (source:Entity {id: row.source_id})
            MATCH (target:Entity {id: row.target_id})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = row
            REMOVE r.source_id, r.target_id
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_query)
                logger.info(f"Edges loaded successfully: {result}")
            except Exception as e:
                logger.error(f"Failed to load edges: {e}")
                return False
            
            # Verify data
            try:
                count_query = "MATCH (n) RETURN count(n) AS node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"Total nodes in graph: {result}")
                
                edge_count_query = "MATCH ()-[r]->() RETURN count(r) AS edge_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, edge_count_query)
                logger.info(f"Total edges in graph: {result}")
            except Exception as e:
                logger.warning(f"Could not verify data counts: {e}")
            
            logger.info("SUCCESS! Data loaded into FalkorDB")
            return True
            
        except Exception as e:
            logger.error(f"Failed to execute Cypher commands: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Convert TTL to FalkorDB - FIXED VERSION')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host (default: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port (default: 6379)')
    parser.add_argument('--password', help='FalkorDB password (default: none)')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--csv_only', action='store_true', help='Only convert to CSV, skip loading')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process for testing')
    parser.add_argument('--execute_cypher', action='store_true', help='Execute Cypher commands automatically')
    parser.add_argument('--start_proper', action='store_true', help='Start FalkorDB with proper import folder configuration')
    parser.add_argument('--container_name', default='falkordb_fixed', help='Docker container name')
    parser.add_argument('--test_config', action='store_true', help='Test connection and check import folder configuration')
    
    args = parser.parse_args()
    
    # Validate input
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        sys.exit(1)
    
    try:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        logger.info(f"Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    except Exception as e:
        logger.error(f"Cannot access TTL file: {e}")
        sys.exit(1)
    
    # Create converter
    try:
        converter = TTLToFalkorDBConverter(args.output_dir)
    except Exception as e:
        logger.error(f"Failed to initialize converter: {e}")
        sys.exit(1)
    
    # Start FalkorDB with proper configuration if requested
    if args.start_proper:
        try:
            success, container_name = converter.start_falkordb_with_proper_config(args.container_name)
            if success:
                logger.info("FalkorDB started with proper configuration!")
                logger.info(f"Container: {container_name}")
                logger.info("Import folder configured correctly")
                logger.info("You can now run --execute_cypher to load data")
            else:
                logger.error("Failed to start FalkorDB container")
                sys.exit(1)
        except Exception as e:
            logger.error(f"Error starting FalkorDB: {e}")
            sys.exit(1)
        return
    
    # Test configuration if requested
    if args.test_config:
        try:
            success = converter.test_and_configure_import_folder(args.host, args.port, args.password)
            if success:
                logger.info("Configuration test passed!")
                sys.exit(0)
            else:
                logger.error("Configuration test failed!")
                sys.exit(1)
        except Exception as e:
            logger.error(f"Configuration test error: {e}")
            sys.exit(1)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        try:
            csv_files = converter.convert_ttl_to_csv(args.ttl_file, args.max_triples)
            if not csv_files:
                logger.error("No CSV files were generated")
                sys.exit(1)
        except Exception as e:
            logger.error(f"TTL to CSV conversion failed: {e}")
            sys.exit(1)
        
        conversion_time = time.time() - start_time
        logger.info(f"TTL->CSV conversion completed in {conversion_time:.1f}s")
        
        # Generate Cypher commands
        try:
            cypher_file = converter.generate_cypher_commands()
        except Exception as e:
            logger.error(f"Failed to generate Cypher commands: {e}")
            sys.exit(1)
        
        if args.csv_only:
            logger.info("CSV-only mode: Files generated successfully")
            logger.info(f"CSV files in: {args.output_dir}/")
            logger.info(f"Cypher commands: {cypher_file}")
            logger.info("\nNext steps:")
            logger.info("1. Start FalkorDB with proper configuration:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --start_proper")
            logger.info("2. Load data:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --execute_cypher")
            return
        
        # Execute Cypher commands if requested
        if args.execute_cypher:
            # Test and configure import folder first
            logger.info("Configuring FalkorDB import folder...")
            config_success = converter.test_and_configure_import_folder(args.host, args.port, args.password)
            
            if not config_success:
                logger.error("Failed to configure FalkorDB. Ensure container is running with proper setup.")
                logger.error("Try: --start_proper flag to start container with correct configuration")
                sys.exit(1)
            
            try:
                success = converter.execute_cypher_commands(
                    args.graph_name, args.host, args.port, args.password
                )
                
                total_time = time.time() - start_time
                
                if success:
                    logger.info(f"🎉 SUCCESS! Total time: {total_time:.1f}s")
                    logger.info(f"Performance: {file_size_mb/total_time:.1f} MB/s")
                    logger.info(f"Data loaded into graph: {args.graph_name}")
                    logger.info(f"CSV files remain in: {converter.output_dir}")
                    logger.info("Web interface: http://localhost:3000")
                else:
                    logger.error("Failed to execute Cypher commands")
                    logger.error("ROOT CAUSE FIXES:")
                    logger.error("  1. Use --start_proper to start with correct configuration")
                    logger.error("  2. Ensure Docker volume mounting is working")
                    logger.error("  3. Check FalkorDB import folder configuration")
                    sys.exit(1)
            except Exception as e:
                logger.error(f"Error executing Cypher commands: {e}")
                sys.exit(1)
        else:
            logger.info("CSV files and Cypher commands generated")
            logger.info(f"Files in: {args.output_dir}/")
            logger.info(f"Commands in: {cypher_file}")
            logger.info("\n🔧 FIXED VERSION - To load data:")
            logger.info("1. Start FalkorDB with proper configuration:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --start_proper")
            logger.info("2. Load data:")
            logger.info(f"   python {sys.argv[0]} {args.ttl_file} --execute_cypher")
            logger.info("\n✅ Root cause fixed: import folder configuration issue resolved!")
            
    except KeyboardInterrupt:
        logger.error("Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        logger.exception("Full error details:")
        sys.exit(1)

if __name__ == "__main__":
    main()
