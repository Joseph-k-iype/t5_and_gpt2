#!/usr/bin/env python3
"""
Enhanced GDPR/UK-GDPR Knowledge Graph & RoPA System
Advanced 8-Agent Architecture with LangGraph ReAct, Reflection & Specialized Domain Agents

This system creates a comprehensive knowledge graph encompassing all GDPR and UK-GDPR concepts,
while generating a specialized RoPA metamodel for financial institutions like HSBC.

8-Agent Architecture:
1. SupervisorAgent - Coordination & orchestration
2. ResearcherAgent - Deep domain analysis & ontology creation  
3. OntologyAgent - Knowledge graph schema management
4. DataIngestionAgent - Advanced document processing & extraction
5. FinancialDomainExpertAgent - Financial industry specialization
6. GDPRAnalysisReActAgent - General GDPR analysis with tools
7. RoPASpecialistAgent - Article 30 Record of Processing Activities focus
8. ReflectionAgent - Quality assurance across all agents

Author: AI Assistant
Date: 2025
Version: 5.0.0 - Enhanced 8-Agent Multi-Specialist Architecture
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Literal
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import argparse
import pickle
from pathlib import Path
import hashlib

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph for multi-agent architecture
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph components
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command
from langgraph.prebuilt import create_react_agent, ToolNode

# Global Configuration - Centralized credentials and paths
GLOBAL_CONFIG = {
    # OpenAI Configuration - Only o3-mini
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "your_openai_api_key_here"),
    "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", None),
    "OPENAI_MODEL": "o3-mini",
    
    # Elasticsearch Configuration  
    "ELASTICSEARCH_HOST": os.getenv("ELASTICSEARCH_HOST", "https://localhost:9200"),
    "ELASTICSEARCH_USERNAME": os.getenv("ELASTICSEARCH_USERNAME", "elastic"),
    "ELASTICSEARCH_PASSWORD": os.getenv("ELASTICSEARCH_PASSWORD", "changeme"),
    "ELASTICSEARCH_CA_CERTS": os.getenv("ELASTICSEARCH_CA_CERTS", None),
    "ELASTICSEARCH_VERIFY_CERTS": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "false").lower() == "true",
    
    # FalkorDB Configuration
    "FALKORDB_HOST": os.getenv("FALKORDB_HOST", "localhost"),
    "FALKORDB_PORT": int(os.getenv("FALKORDB_PORT", 6379)),
    "FALKORDB_PASSWORD": os.getenv("FALKORDB_PASSWORD", None),
    "FALKORDB_DATABASE": os.getenv("FALKORDB_DATABASE", "gdpr_comprehensive_kg"),
    
    # Document Paths
    "PDF_DOCUMENTS_PATH": os.getenv("PDF_DOCUMENTS_PATH", "./documents"),
    "OUTPUT_PATH": os.getenv("OUTPUT_PATH", "./output"),
    "MEMORY_PATH": os.getenv("MEMORY_PATH", "./memory"),
    
    # System Configuration
    "ELASTICSEARCH_INDEX": "gdpr_comprehensive_knowledge",
    "MEMORY_FILE": "gdpr_agent_memory.pkl",
    
    # Embedding Configuration
    "EMBEDDING_MODEL": "text-embedding-3-large",
    "EMBEDDING_DIMENSIONS": 3072,
    
    # Financial Industry Context
    "ORGANIZATION_TYPE": "financial_institution",
    "ORGANIZATION_NAME": "HSBC",
    "JURISDICTION_FOCUS": ["EU_GDPR", "UK_GDPR"],
    
    # Agent Configuration
    "MAX_REASONING_ITERATIONS": 5,
    "REFLECTION_THRESHOLD": 0.7,
    "SUPERVISOR_MAX_WORKERS": 8,
    "RESEARCH_DEPTH": 3,
    "ONTOLOGY_COMPLEXITY": "comprehensive"
}

# Configure logging
os.makedirs(GLOBAL_CONFIG["OUTPUT_PATH"], exist_ok=True)
os.makedirs(GLOBAL_CONFIG["MEMORY_PATH"], exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{GLOBAL_CONFIG['OUTPUT_PATH']}/gdpr_enhanced_system.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedAgentState(TypedDict):
    """Enhanced shared state for all 8 agents in the system"""
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document Processing & Ingestion
    current_document: Optional[Dict[str, Any]]
    processed_documents: List[Dict[str, Any]]
    document_chunks: List[Dict[str, Any]]
    document_metadata: Dict[str, Any]
    extraction_quality: Dict[str, float]
    
    # Research & Domain Analysis
    research_findings: List[Dict[str, Any]]
    domain_concepts: List[Dict[str, Any]]
    regulatory_mappings: Dict[str, List[str]]
    external_references: List[Dict[str, Any]]
    concept_hierarchies: Dict[str, Any]
    
    # Ontology & Schema Management
    ontology_schema: Optional[Dict[str, Any]]
    entity_types: List[Dict[str, Any]]
    relationship_types: List[Dict[str, Any]]
    schema_evolution: List[Dict[str, Any]]
    validation_rules: List[Dict[str, Any]]
    
    # Comprehensive GDPR Knowledge Graph Entities
    gdpr_articles: List[Dict[str, Any]]
    uk_gdpr_articles: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    data_subject_rights: List[Dict[str, Any]]
    principles: List[Dict[str, Any]]
    obligations: List[Dict[str, Any]]
    penalties: List[Dict[str, Any]]
    authorities: List[Dict[str, Any]]
    definitions: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    transfers: List[Dict[str, Any]]
    controllers: List[Dict[str, Any]]
    processors: List[Dict[str, Any]]
    
    # Financial Industry Specific
    financial_regulations: List[Dict[str, Any]]
    financial_data_types: List[Dict[str, Any]]
    compliance_frameworks: List[Dict[str, Any]]
    banking_processes: List[Dict[str, Any]]
    risk_assessments: List[Dict[str, Any]]
    
    # RoPA Specialized Elements
    ropa_elements: Dict[str, List[Dict[str, Any]]]
    article_30_compliance: Dict[str, Any]
    ropa_templates: List[Dict[str, Any]]
    compliance_gaps: List[Dict[str, Any]]
    
    # Synonyms and Semantic Enrichment
    generated_synonyms: Dict[str, List[str]]
    semantic_clusters: List[Dict[str, Any]]
    entity_mappings: Dict[str, str]
    concept_relationships: List[Dict[str, Any]]
    
    # Agent Coordination & Results
    current_agent: str
    agent_results: Dict[str, Any]
    agent_coordination_plan: Optional[Dict[str, Any]]
    agent_execution_order: List[str]
    inter_agent_messages: List[Dict[str, Any]]
    
    # Quality & Reflection
    reflection_feedback: List[Dict[str, Any]]
    quality_metrics: Dict[str, float]
    validation_results: List[Dict[str, Any]]
    improvement_suggestions: List[Dict[str, Any]]
    
    # Final Outputs
    ropa_metamodel: Optional[Dict[str, Any]]
    business_report: Optional[str]
    compliance_assessment: Optional[Dict[str, Any]]
    executive_summary: Optional[str]
    
    # System Performance
    confidence_scores: Dict[str, float]
    processing_times: Dict[str, float]
    error_logs: List[Dict[str, Any]]

class EmbeddingEngine:
    """Advanced embedding generation with synonym enrichment"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"]
        )
        self.model = GLOBAL_CONFIG["EMBEDDING_MODEL"]
        self.dimensions = GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"]
        logger.info(f"Initialized embedding engine with {self.model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def generate_synonyms_with_llm(self, concept: str, domain_context: str = "GDPR") -> List[str]:
        """Use LLM to generate domain-specific synonyms"""
        try:
            prompt = f"""
            Generate comprehensive synonyms and related terms for the concept "{concept}" in the context of {domain_context} and UK-GDPR compliance.

            Include:
            1. Direct synonyms and alternative terms
            2. Legal terminology variations
            3. Technical and business terms
            4. Abbreviations, acronyms, and shorthand
            5. Contextual equivalents used in regulatory documents
            6. Financial industry specific terminology
            7. Cross-jurisdictional term variations (EU vs UK)

            Return as a JSON list of strings, maximum 20 terms.
            Focus on terms that would appear in regulatory documents, policies, business processes, and compliance frameworks.
            
            Consider regulatory context: GDPR, UK GDPR, Data Protection Act 2018, financial regulations (FCA, PRA).
            """
            
            response = self.client.chat.completions.create(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                messages=[{"role": "user", "content": prompt}],
                reasoning_effort="high"
            )
            
            content = response.choices[0].message.content
            
            # Extract JSON from response
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start != -1 and json_end > json_start:
                synonyms_json = content[json_start:json_end]
                synonyms = json.loads(synonyms_json)
                return [syn.strip() for syn in synonyms if syn.strip()]
            
            return []
            
        except Exception as e:
            logger.error(f"Failed to generate synonyms for {concept}: {e}")
            return []

class VectorStore:
    """Elasticsearch-based vector storage with comprehensive GDPR schema"""
    
    def __init__(self):
        self.embedding_engine = EmbeddingEngine()
        self.client = self._create_elasticsearch_client()
        self.index_name = GLOBAL_CONFIG["ELASTICSEARCH_INDEX"]
        self._create_comprehensive_index()
    
    def _create_elasticsearch_client(self):
        """Create Elasticsearch client"""
        client_config = {
            "hosts": [GLOBAL_CONFIG["ELASTICSEARCH_HOST"]],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        if GLOBAL_CONFIG["ELASTICSEARCH_HOST"].startswith('https://'):
            client_config["verify_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]
            if GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]:
                client_config["ca_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]
            if not GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]:
                client_config["ssl_show_warn"] = False
        
        if GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"] and GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]:
            client_config["basic_auth"] = (
                GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"],
                GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]
            )
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_comprehensive_index(self):
        """Create comprehensive index for all GDPR concepts"""
        mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "stemmer", "synonym_filter"]
                        }
                    },
                    "filter": {
                        "synonym_filter": {
                            "type": "synonym",
                            "synonyms": [
                                "GDPR,General Data Protection Regulation,EU GDPR",
                                "UK GDPR,Data Protection Act 2018,UK DPA",
                                "controller,data controller",
                                "processor,data processor",
                                "RoPA,Record of Processing Activities,Article 30"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # Core content
                    "text": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer",
                        "fields": {"keyword": {"type": "keyword"}}
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                        "index": True,
                        "similarity": "cosine"
                    },
                    
                    # Document metadata
                    "document_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    
                    # GDPR Classification
                    "gdpr_category": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "chapter": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    
                    # Agent Source Tracking
                    "discovered_by": {"type": "keyword"},
                    "agent_confidence": {"type": "float"},
                    "processing_stage": {"type": "keyword"},
                    
                    # Entity Information
                    "entity_type": {"type": "keyword"},
                    "entity_name": {"type": "text"},
                    "entity_id": {"type": "keyword"},
                    
                    # Research & Analysis
                    "research_depth": {"type": "integer"},
                    "external_references": {"type": "text"},
                    "regulatory_context": {"type": "text"},
                    
                    # Ontology Integration
                    "ontology_class": {"type": "keyword"},
                    "parent_concepts": {"type": "keyword"},
                    "child_concepts": {"type": "keyword"},
                    
                    # Synonyms and Related Terms
                    "synonyms": {"type": "text"},
                    "related_concepts": {"type": "text"},
                    "alternative_names": {"type": "text"},
                    
                    # Financial Industry Context
                    "financial_relevance": {"type": "float"},
                    "financial_context": {"type": "text"},
                    "regulatory_framework": {"type": "keyword"},
                    "banking_relevance": {"type": "float"},
                    
                    # RoPA Specific
                    "ropa_element_type": {"type": "keyword"},
                    "article_30_relevance": {"type": "float"},
                    "compliance_requirement": {"type": "text"},
                    
                    # Relationships
                    "related_articles": {"type": "keyword"},
                    "cross_references": {"type": "keyword"},
                    "dependencies": {"type": "keyword"},
                    
                    # Quality Metrics
                    "confidence_score": {"type": "float"},
                    "validation_status": {"type": "keyword"},
                    "quality_score": {"type": "float"},
                    
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created comprehensive GDPR index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise
    
    def index_concept(self, concept_data: Dict[str, Any]):
        """Index a GDPR concept with embeddings and synonyms"""
        try:
            # Generate synonyms using LLM
            concept_name = concept_data.get("entity_name", concept_data.get("text", ""))
            domain_context = f"GDPR {concept_data.get('gdpr_category', '')}"
            synonyms = self.embedding_engine.generate_synonyms_with_llm(concept_name, domain_context)
            
            # Create enriched text for embedding
            enriched_text = f"{concept_data.get('text', '')} {' '.join(synonyms)}"
            embedding = self.embedding_engine.generate_embedding(enriched_text)
            
            # Prepare document
            doc = {
                **concept_data,
                "embedding": embedding,
                "synonyms": synonyms,
                "timestamp": datetime.now()
            }
            
            # Generate unique document ID
            doc_id = hashlib.md5(f"{concept_data.get('entity_name', '')}_{concept_data.get('gdpr_category', '')}".encode()).hexdigest()
            
            self.client.index(index=self.index_name, id=doc_id, document=doc)
            logger.debug(f"Indexed concept: {concept_name}")
            
        except Exception as e:
            logger.error(f"Failed to index concept: {e}")
    
    def semantic_search(self, query: str, filters: Dict[str, Any] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Perform semantic search with optional filters"""
        try:
            query_embedding = self.embedding_engine.generate_embedding(query)
            
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "should": [
                            {"multi_match": {
                                "query": query,
                                "fields": ["text^3", "synonyms^2", "entity_name^4"],
                                "type": "best_fields"
                            }}
                        ]
                    }
                },
                "size": top_k,
                "_source": {"excludes": ["embedding"]}
            }
            
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if isinstance(value, list):
                        filter_clauses.append({"terms": {key: value}})
                    else:
                        filter_clauses.append({"term": {key: value}})
                search_body["query"]["bool"]["filter"] = filter_clauses
            
            response = self.client.search(index=self.index_name, **search_body)
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            return [
                {**hit["_source"], "search_score": hit["_score"]}
                for hit in hits
            ]
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []

class ComprehensiveKnowledgeGraph:
    """FalkorDB-based comprehensive GDPR/UK-GDPR knowledge graph"""
    
    def __init__(self):
        connection_kwargs = {
            "host": GLOBAL_CONFIG["FALKORDB_HOST"],
            "port": GLOBAL_CONFIG["FALKORDB_PORT"]
        }
        
        if GLOBAL_CONFIG["FALKORDB_PASSWORD"]:
            connection_kwargs["password"] = GLOBAL_CONFIG["FALKORDB_PASSWORD"]
        
        try:
            self.db = FalkorDB(**connection_kwargs)
            self.graph = self.db.select_graph(GLOBAL_CONFIG["FALKORDB_DATABASE"])
            logger.info("Connected to FalkorDB for comprehensive GDPR knowledge graph")
            self._initialize_comprehensive_schema()
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def _initialize_comprehensive_schema(self):
        """Initialize comprehensive schema for GDPR/UK-GDPR knowledge graph"""
        try:
            # Create indexes for performance
            indexes = [
                # Core GDPR entities
                "CREATE INDEX FOR (a:Article) ON (a.number)",
                "CREATE INDEX FOR (c:Chapter) ON (c.number)",
                "CREATE INDEX FOR (p:Principle) ON (p.name)",
                "CREATE INDEX FOR (r:Right) ON (r.name)",
                "CREATE INDEX FOR (o:Obligation) ON (o.name)",
                "CREATE INDEX FOR (d:Definition) ON (d.term)",
                
                # Legal bases and processing
                "CREATE INDEX FOR (lb:LegalBasis) ON (lb.basis_type)",
                "CREATE INDEX FOR (pa:ProcessingActivity) ON (pa.name)",
                "CREATE INDEX FOR (dc:DataCategory) ON (dc.name)",
                "CREATE INDEX FOR (sm:SecurityMeasure) ON (sm.type)",
                
                # Organizational entities
                "CREATE INDEX FOR (ctrl:Controller) ON (ctrl.name)",
                "CREATE INDEX FOR (proc:Processor) ON (proc.name)",
                "CREATE INDEX FOR (auth:Authority) ON (auth.name)",
                
                # Financial industry specific
                "CREATE INDEX FOR (fin:FinancialRegulation) ON (fin.name)",
                "CREATE INDEX FOR (fd:FinancialData) ON (fd.type)",
                
                # Research and ontology
                "CREATE INDEX FOR (rc:ResearchConcept) ON (rc.name)",
                "CREATE INDEX FOR (oc:OntologyClass) ON (oc.class_name)",
                
                # Document and source tracking
                "CREATE INDEX FOR (doc:Document) ON (doc.source)",
                "CREATE INDEX FOR (ent:Entity) ON (ent.name)"
            ]
            
            for index_query in indexes:
                try:
                    self.graph.query(index_query)
                except:
                    pass  # Index might already exist
            
            logger.info("Comprehensive knowledge graph schema initialized")
            
        except Exception as e:
            logger.warning(f"Could not initialize all graph indexes: {e}")
    
    def add_comprehensive_entity(self, entity_type: str, entity_data: Dict[str, Any]):
        """Add any type of GDPR entity to the knowledge graph"""
        try:
            entity_name = str(entity_data.get("name", "")).replace("'", "\\'")
            entity_id = str(entity_data.get("id", entity_name)).replace("'", "\\'")
            
            # Create base entity
            query = f"""
            MERGE (e:{entity_type} {{name: '{entity_name}'}})
            SET e.id = '{entity_id}',
                e.jurisdiction = '{entity_data.get("jurisdiction", "EU_GDPR")}',
                e.confidence = {entity_data.get("confidence", 1.0)},
                e.discovered_by = '{entity_data.get("discovered_by", "system")}',
                e.timestamp = datetime()
            """
            
            # Add type-specific properties safely
            for key, value in entity_data.items():
                if key not in ["name", "id", "jurisdiction", "confidence", "discovered_by"] and isinstance(value, (str, int, float)):
                    if isinstance(value, str):
                        safe_value = str(value).replace("'", "\\'")
                        query += f"""
                        SET e.{key} = '{safe_value}'
                        """
                    else:
                        query += f"""
                        SET e.{key} = {value}
                        """
            
            self.graph.query(query)
            
        except Exception as e:
            logger.error(f"Failed to add {entity_type} entity: {e}")
    
    def create_relationships(self, relationships: List[Dict[str, Any]]):
        """Create relationships between entities"""
        try:
            for rel in relationships:
                source_name = str(rel.get("source", "")).replace("'", "\\'")
                target_name = str(rel.get("target", "")).replace("'", "\\'")
                rel_type = rel.get("type", "RELATED_TO")
                confidence = rel.get("confidence", 1.0)
                
                if source_name and target_name:
                    query = f"""
                    MATCH (a) WHERE a.name = '{source_name}' OR a.id = '{source_name}'
                    MATCH (b) WHERE b.name = '{target_name}' OR b.id = '{target_name}'
                    MERGE (a)-[r:{rel_type}]->(b)
                    SET r.confidence = {confidence},
                        r.timestamp = datetime()
                    """
                    self.graph.query(query)
            
        except Exception as e:
            logger.error(f"Failed to create relationships: {e}")
    
    def query_comprehensive_knowledge(self, concept: str, max_depth: int = 3) -> List[Dict[str, Any]]:
        """Query comprehensive knowledge about any GDPR concept"""
        try:
            concept_escaped = concept.replace("'", "\\'")
            
            query = f"""
            MATCH path = (start)-[*1..{max_depth}]-(related)
            WHERE (
                start.name CONTAINS '{concept_escaped}' OR 
                start.title CONTAINS '{concept_escaped}' OR
                start.purpose CONTAINS '{concept_escaped}' OR
                start.basis_type CONTAINS '{concept_escaped}'
            )
            RETURN DISTINCT 
                start, 
                related, 
                relationships(path), 
                length(path) as distance,
                labels(start) as start_labels,
                labels(related) as related_labels
            ORDER BY distance, start.confidence DESC
            LIMIT 20
            """
            
            result = self.graph.query(query)
            
            formatted_results = []
            for record in result.result_set:
                formatted_results.append({
                    "start_entity": self._format_node(record[0]),
                    "related_entity": self._format_node(record[1]),
                    "relationship_path": [str(rel) for rel in record[2]],
                    "distance": record[3],
                    "start_labels": record[4],
                    "related_labels": record[5],
                    "relevance_score": 1.0 / (record[3] + 1)
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Knowledge query failed: {e}")
            return []
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if not node:
            return None
        
        try:
            if hasattr(node, 'properties'):
                properties = dict(node.properties)
                properties['labels'] = list(node.labels) if hasattr(node, 'labels') else []
                return properties
            else:
                return {"id": str(node)}
        except:
            return {"id": str(node)}

# Enhanced Agent Tools
@tool
def search_vector_knowledge(query: str, category: str = None, agent_filter: str = None) -> List[Dict[str, Any]]:
    """Search vector knowledge base for GDPR concepts with agent filtering"""
    vector_store = VectorStore()
    filters = {}
    if category:
        filters["gdpr_category"] = category
    if agent_filter:
        filters["discovered_by"] = agent_filter
    return vector_store.semantic_search(query, filters=filters, top_k=5)

@tool 
def query_knowledge_graph(concept: str, depth: int = 2) -> List[Dict[str, Any]]:
    """Query the comprehensive GDPR knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    return kg.query_comprehensive_knowledge(concept, max_depth=depth)

@tool
def generate_concept_synonyms(concept: str, domain: str = "GDPR") -> List[str]:
    """Generate synonyms for a GDPR concept using LLM"""
    embedding_engine = EmbeddingEngine()
    return embedding_engine.generate_synonyms_with_llm(concept, domain)

@tool
def store_research_finding(finding: Dict[str, Any]) -> str:
    """Store research finding in knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    kg.add_comprehensive_entity("ResearchConcept", finding)
    return f"Stored research finding: {finding.get('name', 'Unknown')}"

@tool
def create_ontology_class(class_data: Dict[str, Any]) -> str:
    """Create new ontology class in knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    kg.add_comprehensive_entity("OntologyClass", class_data)
    return f"Created ontology class: {class_data.get('class_name', 'Unknown')}"

# Enhanced Agent Implementations

class ResearcherAgent:
    """Deep domain analysis and ontology creation agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.research_prompt = """You are an expert regulatory researcher specializing in GDPR, UK-GDPR, and financial services compliance.

Your role is to conduct comprehensive domain analysis to understand the complete landscape of data protection regulation.

CORE RESPONSIBILITIES:
1. DOMAIN MAPPING: Analyze documents to identify all regulatory concepts, not just obvious ones
2. REGULATORY RESEARCH: Cross-reference GDPR with UK-GDPR to identify similarities and differences  
3. ONTOLOGY FOUNDATION: Research existing ontologies (FIBO, legal ontologies) for integration opportunities
4. CONCEPT DISCOVERY: Identify implicit concepts that may not be explicitly stated but are regulatory requirements
5. RELATIONSHIP ANALYSIS: Understand how different regulatory concepts relate to each other

FINANCIAL SERVICES FOCUS:
- Understand how GDPR applies specifically to banking, insurance, investment services
- Research FCA, PRA, and other financial regulatory alignment with GDPR
- Identify financial data types and their GDPR classifications
- Analyze cross-border data flows in financial services context

RESEARCH METHODOLOGY:
- Use systematic analysis of regulatory text
- Identify concept hierarchies and taxonomies
- Research authoritative sources and regulatory guidance
- Analyze regulatory templates and forms (DPA templates, ICO guidance)
- Cross-reference with international standards

OUTPUT REQUIREMENTS:
- Comprehensive concept inventory with definitions
- Regulatory concept relationships and hierarchies  
- Financial industry specific mappings
- External reference integration points
- Ontology design recommendations
- Research confidence scoring

You should provide thorough, evidence-based analysis suitable for building a comprehensive regulatory knowledge base."""
    
    def research_domain(self, document_text: str, research_focus: List[str] = None) -> Dict[str, Any]:
        """Conduct comprehensive domain research"""
        try:
            research_prompt = f"""
            Conduct comprehensive regulatory domain research on this document:
            
            Document Text: {document_text[:5000]}...
            
            Research Focus Areas: {research_focus or ['comprehensive']}
            
            {self.research_prompt}
            
            Analyze this document and provide:
            
            1. REGULATORY CONCEPT INVENTORY:
            - All GDPR/UK-GDPR concepts identified (explicit and implicit)
            - Concept definitions and regulatory context
            - Cross-references to specific articles/sections
            - Financial services relevance scoring (0-1)
            
            2. CONCEPT HIERARCHIES:
            - Parent-child relationships between concepts
            - Taxonomic organization
            - Dependency mappings
            
            3. JURISDICTIONAL ANALYSIS:
            - EU GDPR vs UK GDPR differences identified
            - Territorial scope implications
            - Cross-border considerations for financial services
            
            4. FINANCIAL SERVICES MAPPING:
            - Banking/financial specific applications
            - FCA/PRA regulatory alignment
            - Industry data types and processing activities
            
            5. ONTOLOGY RECOMMENDATIONS:
            - Suggested ontology classes and properties
            - Integration points with FIBO or other standards
            - Relationship types needed
            
            6. EXTERNAL REFERENCES:
            - Relevant regulatory guidance documents
            - International standards alignment
            - Industry best practices
            
            Return comprehensive JSON with all findings, confidence scores, and evidence references.
            """
            
            response = self.llm.invoke([HumanMessage(content=research_prompt)])
            content = response.content
            
            # Try to extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    research_data = json.loads(content[json_start:json_end])
                    return {
                        "agent_type": "researcher",
                        "research_findings": research_data,
                        "raw_analysis": content,
                        "confidence": self._calculate_research_confidence(research_data),
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            # Fallback if JSON extraction fails
            return {
                "agent_type": "researcher", 
                "research_findings": {"raw_analysis": content},
                "confidence": 0.6,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Research analysis failed: {e}")
            return {"error": str(e), "agent_type": "researcher"}
    
    def _calculate_research_confidence(self, research_data: Dict[str, Any]) -> float:
        """Calculate confidence score for research findings"""
        if not isinstance(research_data, dict):
            return 0.5
        
        # Score based on completeness of research sections
        expected_sections = [
            "regulatory_concept_inventory",
            "concept_hierarchies", 
            "jurisdictional_analysis",
            "financial_services_mapping",
            "ontology_recommendations",
            "external_references"
        ]
        
        present_sections = sum(1 for section in expected_sections if section in research_data)
        return min(present_sections / len(expected_sections), 1.0)

class OntologyAgent:
    """Knowledge graph schema management and ontology creation agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.ontology_prompt = """You are an expert ontology engineer and knowledge architect specializing in regulatory and legal domain ontologies.

Your role is to create comprehensive, well-structured ontologies for GDPR and UK-GDPR compliance knowledge.

CORE RESPONSIBILITIES:
1. ONTOLOGY DESIGN: Create formal ontology structures with classes, properties, and relationships
2. SCHEMA MANAGEMENT: Design and evolve knowledge graph schemas
3. SEMANTIC CONSISTENCY: Ensure logical consistency and semantic coherence
4. INTEROPERABILITY: Design for integration with existing standards (FIBO, Dublin Core, legal ontologies)
5. VALIDATION: Create rules and constraints for data quality

ONTOLOGY DESIGN PRINCIPLES:
- Follow established ontology engineering methodologies
- Ensure clear semantic distinctions between concepts
- Create reusable and extensible structures
- Maintain separation of concerns (regulatory vs business vs technical)
- Design for automated reasoning and inference

FINANCIAL SERVICES INTEGRATION:
- Align with Financial Industry Business Ontology (FIBO) where applicable
- Integrate financial regulatory concepts (FCA, PRA, Basel, MiFID)
- Support complex financial data processing scenarios
- Enable compliance reporting and audit trails

TECHNICAL REQUIREMENTS:
- Compatible with graph database schemas (FalkorDB/Redis)
- Support for semantic search and retrieval
- Extensible for future regulatory changes
- Performance optimized for large-scale data

OUTPUT SPECIFICATIONS:
- Formal ontology classes with properties and constraints
- Relationship types with cardinality and semantics
- Validation rules and integrity constraints
- Schema evolution and versioning strategies
- Integration specifications for external ontologies

You should create production-ready ontology schemas suitable for enterprise knowledge management."""
        
        self.tools = [create_ontology_class, store_research_finding]
    
    def create_ontology_schema(self, research_findings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create comprehensive ontology schema from research findings"""
        try:
            ontology_prompt = f"""
            Create a comprehensive ontology schema based on these research findings:
            
            Research Findings: {json.dumps(research_findings[:10], indent=2, default=str)[:8000]}...
            
            {self.ontology_prompt}
            
            Design a complete ontology schema including:
            
            1. CORE ONTOLOGY CLASSES:
            - GDPR regulatory concepts (Article, Chapter, Principle, Right, Obligation)
            - Data processing concepts (Controller, Processor, ProcessingActivity, DataCategory)
            - Legal concepts (LegalBasis, Consent, LegitimateInterest)
            - Organizational concepts (DataProtectionOfficer, SupervisoryAuthority)
            - Technical concepts (SecurityMeasure, TechnicalMeasure, OrganizationalMeasure)
            
            2. FINANCIAL SERVICES EXTENSIONS:
            - Financial data types (PaymentData, KYCData, CreditData)
            - Financial processes (AccountOpening, LoanProcessing, RiskAssessment)
            - Regulatory frameworks (FCA, PRA, Basel, MiFID)
            - Financial entities (Bank, InsuranceCompany, InvestmentFirm)
            
            3. RELATIONSHIP TYPES:
            - Hierarchical relationships (subClassOf, partOf)
            - Regulatory relationships (regulatedBy, requires, compliesWith)
            - Process relationships (uses, produces, involves)
            - Temporal relationships (precedes, during, after)
            
            4. PROPERTIES AND ATTRIBUTES:
            - Data properties (name, description, identifier, timestamp)
            - Regulatory properties (jurisdiction, article_reference, compliance_status)
            - Business properties (purpose, legal_basis, retention_period)
            - Technical properties (encryption_required, access_controls)
            
            5. VALIDATION RULES:
            - Cardinality constraints
            - Value restrictions
            - Business rules and compliance checks
            - Data quality constraints
            
            6. INTEGRATION SPECIFICATIONS:
            - FIBO alignment mappings
            - Dublin Core metadata integration
            - External vocabulary references
            
            Return complete ontology specification as structured JSON suitable for implementation.
            """
            
            response = self.llm.invoke([HumanMessage(content=ontology_prompt)])
            content = response.content
            
            # Extract JSON schema
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    ontology_schema = json.loads(content[json_start:json_end])
                    
                    # Store ontology classes in knowledge graph
                    self._implement_ontology_schema(ontology_schema)
                    
                    return {
                        "agent_type": "ontology",
                        "ontology_schema": ontology_schema,
                        "implementation_status": "completed",
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "ontology",
                "ontology_schema": {"raw_schema": content},
                "implementation_status": "partial",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Ontology creation failed: {e}")
            return {"error": str(e), "agent_type": "ontology"}
    
    def _implement_ontology_schema(self, schema: Dict[str, Any]):
        """Implement ontology schema in knowledge graph"""
        try:
            kg = ComprehensiveKnowledgeGraph()
            
            # Create ontology classes
            if "core_classes" in schema:
                for class_name, class_def in schema["core_classes"].items():
                    ontology_class = {
                        "class_name": class_name,
                        "definition": class_def.get("definition", ""),
                        "properties": class_def.get("properties", []),
                        "parent_classes": class_def.get("parent_classes", []),
                        "discovered_by": "ontology_agent"
                    }
                    kg.add_comprehensive_entity("OntologyClass", ontology_class)
            
            logger.info("Ontology schema implemented in knowledge graph")
            
        except Exception as e:
            logger.error(f"Ontology implementation failed: {e}")

class DataIngestionAgent:
    """Advanced document processing and data extraction agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.ingestion_prompt = """You are an expert data ingestion engineer specializing in regulatory document processing and structured data extraction.

Your role is to process diverse document formats and extract structured regulatory information with high accuracy and completeness.

CORE RESPONSIBILITIES:
1. DOCUMENT PROCESSING: Handle multiple formats (PDF, Word, XML, HTML, text)
2. CONTENT EXTRACTION: Extract structured information from unstructured regulatory text
3. QUALITY ASSESSMENT: Score extraction quality and identify processing issues
4. METADATA GENERATION: Create comprehensive document metadata
5. INCREMENTAL UPDATES: Handle document versioning and change detection

EXTRACTION TECHNIQUES:
- Regulatory structure recognition (articles, sections, subsections)
- Entity extraction (legal terms, definitions, obligations)
- Relationship identification (references, dependencies, hierarchies)
- Table and form processing for structured data
- Multi-language support for EU vs UK regulatory differences

QUALITY ASSURANCE:
- Content completeness verification
- Extraction accuracy scoring
- Error detection and correction suggestions
- Confidence scoring for extracted elements
- Validation against known regulatory structures

TECHNICAL CAPABILITIES:
- Large document chunking strategies
- Memory-efficient processing for enterprise volumes
- Parallel processing coordination
- Error handling and recovery
- Progress tracking and monitoring

OUTPUT REQUIREMENTS:
- Structured data in standardized formats
- Comprehensive document metadata
- Extraction quality metrics
- Processing performance data
- Error logs and improvement suggestions

You should ensure high-quality, reliable data extraction suitable for regulatory compliance knowledge systems."""
    
    def process_document(self, document_text: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process document with advanced extraction"""
        try:
            processing_prompt = f"""
            Process this regulatory document for comprehensive data extraction:
            
            Document: {document_text[:6000]}...
            
            Metadata: {json.dumps(document_metadata, indent=2)}
            
            {self.ingestion_prompt}
            
            Extract and structure the following:
            
            1. DOCUMENT STRUCTURE:
            - Document type classification
            - Hierarchical structure (chapters, articles, sections)
            - Table of contents extraction
            - Cross-reference mapping
            
            2. REGULATORY CONTENT:
            - All GDPR/UK-GDPR articles and provisions
            - Legal definitions and terminology
            - Obligations and requirements
            - Rights and procedures
            - Penalties and enforcement measures
            
            3. FINANCIAL SERVICES CONTENT:
            - Banking and financial specific references
            - Regulatory alignment mentions (FCA, PRA, etc.)
            - Financial data processing scenarios
            - Industry-specific examples
            
            4. STRUCTURED DATA:
            - Entity relationships
            - Temporal references (dates, deadlines)
            - Quantitative information (fines, thresholds)
            - Geographic scope (jurisdictions, territories)
            
            5. QUALITY METRICS:
            - Extraction completeness score (0-1)
            - Confidence scores for each extracted element
            - Processing errors or ambiguities
            - Recommendations for improvement
            
            6. METADATA ENHANCEMENT:
            - Document classification tags
            - Regulatory categories
            - Subject matter classifications
            - Relationship to other documents
            
            Return comprehensive JSON with all extracted data, quality scores, and processing metadata.
            """
            
            response = self.llm.invoke([HumanMessage(content=processing_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    extraction_data = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "data_ingestion",
                        "extraction_results": extraction_data,
                        "processing_quality": self._assess_extraction_quality(extraction_data),
                        "document_metadata": document_metadata,
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "data_ingestion",
                "extraction_results": {"raw_extraction": content},
                "processing_quality": 0.5,
                "document_metadata": document_metadata,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            return {"error": str(e), "agent_type": "data_ingestion"}
    
    def _assess_extraction_quality(self, extraction_data: Dict[str, Any]) -> float:
        """Assess the quality of data extraction"""
        if not isinstance(extraction_data, dict):
            return 0.3
        
        # Quality scoring based on extraction completeness
        quality_indicators = [
            "document_structure",
            "regulatory_content", 
            "structured_data",
            "quality_metrics"
        ]
        
        present_indicators = sum(1 for indicator in quality_indicators if indicator in extraction_data)
        return min(present_indicators / len(quality_indicators), 1.0)

class FinancialDomainExpertAgent:
    """Financial industry specialization agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.financial_expert_prompt = """You are a senior regulatory compliance expert specializing in financial services data protection and GDPR compliance.

Your expertise covers banking, insurance, investment services, and broader financial markets regulation.

CORE EXPERTISE AREAS:
1. FINANCIAL REGULATIONS: FCA, PRA, Basel III, MiFID II, GDPR alignment
2. BANKING OPERATIONS: Account management, payments, lending, KYC, AML
3. DATA PROTECTION: Financial data categories, cross-border flows, third-party sharing
4. RISK MANAGEMENT: Operational risk, compliance risk, data protection risks
5. AUDIT & GOVERNANCE: Regulatory reporting, supervisory requirements, board oversight

FINANCIAL SERVICES GDPR APPLICATIONS:
- Customer data processing across banking lifecycle
- Payment processing and transaction data protection
- Credit scoring and lending data requirements
- Investment services and client profiling
- Insurance underwriting and claims processing
- Wealth management and private banking compliance

REGULATORY LANDSCAPE:
- UK post-Brexit data protection framework
- EU-UK data adequacy and transfer mechanisms
- Financial regulatory coordination (FCA-ICO cooperation)
- Cross-border banking and data flows
- Fintech and digital banking compliance
- Open banking and data sharing requirements

INDUSTRY-SPECIFIC CHALLENGES:
- Legacy system integration with GDPR requirements
- Real-time processing vs privacy by design
- Customer consent management in complex financial products
- Data retention for regulatory vs GDPR requirements
- Third-party vendor management and data sharing
- Incident response in financial services context

OUTPUT REQUIREMENTS:
- Financial industry specific GDPR interpretations
- Practical implementation guidance for banks
- Risk assessments with business impact analysis
- Regulatory alignment recommendations
- Industry best practices and benchmarking
- Cost-benefit analysis for compliance investments

You should provide expert guidance suitable for senior compliance officers and risk managers at major financial institutions."""
    
    def analyze_financial_context(self, content: str, financial_focus: List[str] = None) -> Dict[str, Any]:
        """Analyze content for financial services GDPR implications"""
        try:
            analysis_prompt = f"""
            Analyze this content for financial services GDPR compliance implications:
            
            Content: {content[:6000]}...
            
            Financial Focus Areas: {financial_focus or ['comprehensive']}
            
            {self.financial_expert_prompt}
            
            Provide comprehensive financial services analysis:
            
            1. FINANCIAL DATA CATEGORIZATION:
            - Identify financial data types present (payment, credit, KYC, transaction, etc.)
            - GDPR classification (personal data, special categories, etc.)
            - Sensitivity levels and protection requirements
            - Cross-border transfer implications
            
            2. BANKING PROCESS MAPPING:
            - Relevant banking/financial processes identified
            - Data flows and processing activities
            - Third-party sharing scenarios
            - Customer interaction points
            
            3. REGULATORY ALIGNMENT:
            - FCA/PRA regulatory requirements alignment
            - Conflicts or tensions with financial regulations
            - Supervisory expectations and guidance
            - Industry standards compliance (Basel, MiFID)
            
            4. RISK ASSESSMENT:
            - Data protection risks specific to financial services
            - Operational risks from GDPR compliance
            - Reputational and regulatory risks
            - Financial impact assessment
            
            5. IMPLEMENTATION CHALLENGES:
            - Technical challenges in banking systems
            - Operational process changes required
            - Customer communication requirements
            - Staff training and competency needs
            
            6. BEST PRACTICES:
            - Industry leading approaches
            - Regulatory guidance interpretations
            - Technology solutions for compliance
            - Governance and oversight frameworks
            
            7. {GLOBAL_CONFIG['ORGANIZATION_NAME']} SPECIFIC:
            - Specific implications for {GLOBAL_CONFIG['ORGANIZATION_NAME']}
            - Competitive considerations
            - Strategic recommendations
            - Implementation priorities
            
            Return detailed JSON analysis with practical recommendations and risk scoring.
            """
            
            response = self.llm.invoke([HumanMessage(content=analysis_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    financial_analysis = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "financial_expert",
                        "financial_analysis": financial_analysis,
                        "risk_score": self._calculate_financial_risk(financial_analysis),
                        "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "financial_expert",
                "financial_analysis": {"raw_analysis": content},
                "risk_score": 0.5,
                "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Financial analysis failed: {e}")
            return {"error": str(e), "agent_type": "financial_expert"}
    
    def _calculate_financial_risk(self, analysis: Dict[str, Any]) -> float:
        """Calculate financial services specific risk score"""
        if not isinstance(analysis, dict):
            return 0.5
        
        # Risk factors specific to financial services
        risk_indicators = [
            "cross_border_transfers",
            "special_category_data",
            "third_party_sharing",
            "regulatory_conflicts",
            "operational_complexity"
        ]
        
        # Count mentions of risk indicators
        content_str = str(analysis).lower()
        risk_mentions = sum(1 for indicator in risk_indicators if indicator in content_str)
        
        # Higher mentions = higher risk
        return min(risk_mentions / len(risk_indicators) * 0.8, 1.0)

class RoPASpecialistAgent:
    """Article 30 Record of Processing Activities specialist agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.ropa_specialist_prompt = """You are a leading expert in GDPR Article 30 Records of Processing Activities (RoPA) with deep expertise in regulatory compliance and data governance.

Your specialization covers all aspects of Article 30 compliance for complex organizations, particularly financial institutions.

ARTICLE 30 EXPERTISE:
1. CONTROLLER REQUIREMENTS: Article 30(1) obligations and documentation
2. PROCESSOR REQUIREMENTS: Article 30(2) obligations and record-keeping
3. EXEMPTIONS: Article 30(5) small enterprise exemptions and limitations
4. DPA TEMPLATES: Understanding of various DPA RoPA templates and requirements
5. SUPERVISORY EXPECTATIONS: Knowledge of regulatory enforcement priorities

ROPA ELEMENTS MASTERY:
- Controller/processor identification and contact details
- Processing purposes and legal basis documentation
- Data categories and data subject categories
- Recipients and third-party disclosures
- International transfers and safeguards
- Retention periods and deletion procedures
- Security measures descriptions

FINANCIAL SERVICES ROPA:
- Complex multi-entity structures and joint controllership
- Financial data processing across business lines
- Regulatory reporting vs RoPA requirements
- Third-party vendor management and processor agreements
- Cross-border banking operations documentation
- Customer lifecycle processing activities

COMPLIANCE FRAMEWORK:
- RoPA governance and maintenance procedures
- Quality assurance and validation processes
- Integration with broader data governance
- Audit preparation and regulatory inspection readiness
- Change management for processing activity updates
- Training and awareness for business stakeholders

IMPLEMENTATION EXCELLENCE:
- RoPA metamodel design for enterprise scalability
- Automation and technology integration
- Business process integration
- Risk-based prioritization approaches
- Continuous improvement methodologies

OUTPUT REQUIREMENTS:
- Complete Article 30 compliance assessments
- Detailed RoPA element specifications
- Implementation roadmaps and project plans
- Quality assurance frameworks
- Regulatory risk assessments
- Business-ready documentation templates

You should provide authoritative guidance on Article 30 compliance suitable for CPOs, DPOs, and compliance teams at major financial institutions."""
    
    def extract_ropa_elements(self, content: str, compliance_focus: List[str] = None) -> Dict[str, Any]:
        """Extract and analyze Article 30 RoPA elements"""
        try:
            ropa_prompt = f"""
            Extract comprehensive Article 30 RoPA elements from this content:
            
            Content: {content[:6000]}...
            
            Compliance Focus: {compliance_focus or ['article_30_complete']}
            
            {self.ropa_specialist_prompt}
            
            Conduct complete Article 30 analysis:
            
            1. CONTROLLER OBLIGATIONS (Article 30(1)):
            - Controller identification and contact details
            - Joint controller arrangements if applicable
            - Data Protection Officer details
            - Processing purposes with specific descriptions
            - Legal basis for each processing purpose
            - Data categories processed
            - Data subject categories
            - Recipients of personal data
            - International transfers and safeguards
            - Retention periods for data categories
            - Security measures descriptions
            
            2. PROCESSOR OBLIGATIONS (Article 30(2)):
            - Processor identification and contact details
            - Controller details for each processing relationship
            - Categories of processing activities
            - International transfers on behalf of controllers
            - Security measures for processing activities
            
            3. COMPLIANCE ASSESSMENT:
            - Article 30(5) exemption eligibility
            - Completeness assessment against requirements
            - Quality scoring for each RoPA element
            - Gaps and missing information identification
            - Regulatory risk assessment
            
            4. FINANCIAL SERVICES SPECIFICS:
            - Banking-specific processing activities
            - Financial data categorization
            - Regulatory reporting integration
            - Third-party banking services
            - Customer lifecycle mapping
            
            5. IMPLEMENTATION GUIDANCE:
            - RoPA documentation requirements
            - Governance and maintenance procedures
            - Business process integration points
            - Technology and automation opportunities
            - Training and awareness needs
            
            6. {GLOBAL_CONFIG['ORGANIZATION_NAME']} CUSTOMIZATION:
            - Organization-specific recommendations
            - Implementation priorities
            - Resource requirements
            - Timeline considerations
            
            Return detailed JSON with complete RoPA analysis, compliance scoring, and implementation roadmap.
            """
            
            response = self.llm.invoke([HumanMessage(content=ropa_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    ropa_analysis = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "ropa_specialist",
                        "ropa_analysis": ropa_analysis,
                        "article_30_compliance": self._assess_article_30_compliance(ropa_analysis),
                        "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "ropa_specialist",
                "ropa_analysis": {"raw_analysis": content},
                "article_30_compliance": 0.5,
                "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"RoPA analysis failed: {e}")
            return {"error": str(e), "agent_type": "ropa_specialist"}
    
    def _assess_article_30_compliance(self, ropa_analysis: Dict[str, Any]) -> float:
        """Assess Article 30 compliance completeness"""
        if not isinstance(ropa_analysis, dict):
            return 0.3
        
        # Article 30 required elements
        required_elements = [
            "controller_obligations",
            "processor_obligations",
            "compliance_assessment",
            "implementation_guidance"
        ]
        
        present_elements = sum(1 for element in required_elements if element in ropa_analysis)
        return min(present_elements / len(required_elements), 1.0)

class GDPRAnalysisReActAgent:
    """Enhanced ReAct agent for comprehensive GDPR analysis"""
    
    def __init__(self):
        llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        tools = [
            search_vector_knowledge, 
            query_knowledge_graph, 
            generate_concept_synonyms,
            store_research_finding
        ]
        
        self.agent = create_react_agent(
            llm,
            tools,
            state_modifier="""You are an expert GDPR analyst with comprehensive knowledge of both EU GDPR and UK GDPR regulations.

Use the available tools systematically to research and analyze GDPR concepts comprehensively.

ANALYSIS APPROACH:
1. Search vector knowledge base for related concepts and precedents
2. Query knowledge graph for relationship discovery and context
3. Generate relevant synonyms for semantic enrichment
4. Store significant findings for future reference

FOCUS AREAS:
- Complete GDPR regulatory framework analysis
- UK vs EU GDPR differences and implications
- Financial services specific applications
- Article 30 RoPA compliance requirements
- Cross-border data flows and adequacy
- Data subject rights implementation
- Security and breach notification requirements

Always provide evidence-based analysis with specific article references and practical implementation guidance."""
        )
    
    def analyze(self, text: str, focus_area: str = "comprehensive") -> Dict[str, Any]:
        """Analyze text for GDPR concepts using ReAct pattern"""
        try:
            prompt = f"""
            Conduct comprehensive GDPR analysis of this text: {text}
            
            Focus area: {focus_area}
            
            Use the available tools systematically:
            1. Search vector knowledge for related regulatory concepts
            2. Query knowledge graph for relationship context
            3. Generate relevant synonyms for key concepts discovered
            4. Store any significant new findings
            
            Provide detailed analysis including:
            - All GDPR concepts identified with article references
            - UK vs EU GDPR considerations
            - Financial services relevance and implications
            - Compliance requirements and obligations
            - Implementation recommendations
            - Risk assessments and mitigation strategies
            
            Be thorough and use tools multiple times to build comprehensive understanding.
            """
            
            result = self.agent.invoke({"messages": [("user", prompt)]})
            
            return {
                "agent_type": "gdpr_react",
                "analysis": result.get("messages", [])[-1].content if result.get("messages") else "",
                "tools_used": self._extract_tool_usage(result),
                "confidence": 0.8,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ReAct analysis failed: {e}")
            return {"error": str(e), "agent_type": "gdpr_react"}
    
    def _extract_tool_usage(self, result: Dict[str, Any]) -> List[str]:
        """Extract which tools were used in the analysis"""
        if "messages" in result:
            tool_usage = []
            for msg in result["messages"]:
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_usage.append(tool_call.get('name', 'unknown_tool'))
            return tool_usage
        return []

class ReflectionAgent:
    """Enhanced reflection agent for quality assurance across all agents"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        self.reflection_prompt = """You are a senior quality assurance expert specializing in regulatory compliance analysis and multi-agent system optimization.

Your role is to critically evaluate the quality, completeness, and accuracy of analyses from specialized agents.

QUALITY EVALUATION FRAMEWORK:
1. COMPLETENESS: Are all relevant aspects covered thoroughly?
2. ACCURACY: Are interpretations and references correct?
3. CONSISTENCY: Are findings consistent across different agents?
4. DEPTH: Is the analysis sufficiently detailed for compliance purposes?
5. PRACTICALITY: Are recommendations actionable and realistic?
6. COMPLIANCE: Does analysis meet regulatory standards?

AGENT-SPECIFIC EVALUATION:
- ResearcherAgent: Domain coverage, concept discovery, external integration
- OntologyAgent: Schema quality, semantic consistency, extensibility
- DataIngestionAgent: Extraction accuracy, processing quality, error handling
- FinancialExpertAgent: Industry relevance, risk assessment, practical guidance
- RoPASpecialistAgent: Article 30 compliance, documentation completeness
- GDPRReActAgent: Analytical depth, tool usage effectiveness, evidence quality

IMPROVEMENT IDENTIFICATION:
- Missing regulatory concepts or relationships
- Incomplete compliance analysis
- Insufficient financial services context
- Gaps in cross-jurisdictional analysis
- Weak implementation guidance
- Inadequate risk assessment

OUTPUT REQUIREMENTS:
- Specific quality scores for each analysis dimension
- Detailed improvement recommendations
- Missing elements identification
- Cross-agent consistency assessment
- Overall confidence rating
- Prioritized enhancement suggestions

You should provide constructive, specific feedback that drives measurable improvements in analysis quality."""
    
    def reflect_on_multi_agent_analysis(self, agent_results: Dict[str, Any], original_content: str) -> Dict[str, Any]:
        """Reflect on the quality of multi-agent analysis"""
        try:
            reflection_prompt = f"""
            Critically evaluate this multi-agent GDPR analysis:
            
            Original Content: {original_content[:2000]}...
            
            Agent Results: {json.dumps(agent_results, indent=2, default=str)[:8000]}...
            
            {self.reflection_prompt}
            
            Conduct comprehensive quality evaluation:
            
            1. AGENT-SPECIFIC ASSESSMENT:
            - Evaluate each agent's contribution quality
            - Assess completeness of analysis for each agent's domain
            - Identify strengths and weaknesses by agent type
            - Score individual agent performance (0-1)
            
            2. CROSS-AGENT CONSISTENCY:
            - Check for contradictions between agent findings
            - Assess information coherence and integration
            - Identify gaps where agents should have collaborated
            - Evaluate overall system synergy
            
            3. REGULATORY COMPLIANCE EVALUATION:
            - GDPR concept coverage completeness
            - Article references accuracy and relevance
            - UK vs EU GDPR differentiation quality
            - Financial services compliance adequacy
            - Article 30 RoPA requirements coverage
            
            4. QUALITY DIMENSIONS:
            - Completeness score (0-1): How thoroughly was content analyzed?
            - Accuracy score (0-1): How correct are interpretations and references?
            - Depth score (0-1): How detailed and insightful is analysis?
            - Practicality score (0-1): How actionable are recommendations?
            - Financial relevance score (0-1): How applicable to {GLOBAL_CONFIG['ORGANIZATION_NAME']}?
            
            5. IMPROVEMENT RECOMMENDATIONS:
            - Specific missing concepts or relationships
            - Enhanced analysis approaches needed
            - Additional tool usage suggestions
            - Cross-agent collaboration improvements
            - Quality assurance process enhancements
            
            6. PRIORITIZED ENHANCEMENTS:
            - Critical gaps requiring immediate attention
            - Important improvements for next iteration
            - Optional enhancements for future consideration
            - Resource requirements for improvements
            
            Return detailed JSON with comprehensive quality assessment and specific improvement guidance.
            """
            
            response = self.llm.invoke([HumanMessage(content=reflection_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    reflection_data = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "reflection",
                        "reflection_analysis": reflection_data,
                        "overall_quality_score": self._calculate_overall_quality(reflection_data),
                        "improvement_priority": self._prioritize_improvements(reflection_data),
                        "raw_response": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "reflection",
                "reflection_analysis": {"raw_analysis": content},
                "overall_quality_score": 0.6,
                "improvement_priority": "medium",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Multi-agent reflection failed: {e}")
            return {"error": str(e), "agent_type": "reflection"}
    
    def _calculate_overall_quality(self, reflection_data: Dict[str, Any]) -> float:
        """Calculate overall quality score from reflection analysis"""
        if not isinstance(reflection_data, dict) or "quality_dimensions" not in reflection_data:
            return 0.5
        
        quality_dims = reflection_data["quality_dimensions"]
        scores = []
        
        for dim_name, score in quality_dims.items():
            if isinstance(score, (int, float)):
                scores.append(float(score))
        
        return sum(scores) / len(scores) if scores else 0.5
    
    def _prioritize_improvements(self, reflection_data: Dict[str, Any]) -> str:
        """Determine improvement priority level"""
        overall_quality = self._calculate_overall_quality(reflection_data)
        
        if overall_quality < 0.5:
            return "critical"
        elif overall_quality < 0.7:
            return "high"
        elif overall_quality < 0.8:
            return "medium"
        else:
            return "low"

class EnhancedSupervisorAgent:
    """Enhanced supervisor agent coordinating all 8 specialized agents"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        # Initialize all specialized agents
        self.agents = {
            "researcher": ResearcherAgent(),
            "ontology": OntologyAgent(),
            "data_ingestion": DataIngestionAgent(),
            "financial_expert": FinancialDomainExpertAgent(),
            "gdpr_react": GDPRAnalysisReActAgent(),
            "ropa_specialist": RoPASpecialistAgent(),
            "reflection": ReflectionAgent()
        }
        
        self.agent_capabilities = {
            "researcher": "Deep domain research, concept discovery, ontology foundation",
            "ontology": "Knowledge graph schema design, semantic consistency",
            "data_ingestion": "Document processing, structured extraction, quality assessment",
            "financial_expert": "Financial services GDPR interpretation, risk assessment",
            "gdpr_react": "Comprehensive GDPR analysis with tool-based reasoning",
            "ropa_specialist": "Article 30 compliance, RoPA metamodel creation",
            "reflection": "Quality assurance, improvement recommendations, consistency validation"
        }
    
    def coordinate_comprehensive_analysis(self, document_text: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Coordinate all 8 agents for comprehensive document analysis"""
        try:
            # Create coordination plan
            coordination_plan = self._create_comprehensive_plan(document_text, document_metadata)
            
            results = {
                "coordination_plan": coordination_plan,
                "agent_results": {},
                "execution_order": [],
                "quality_iterations": 0,
                "final_synthesis": None,
                "overall_confidence": 0.0
            }
            
            # Execute agents in planned order
            execution_order = coordination_plan.get("execution_order", list(self.agents.keys()))
            
            for agent_name in execution_order:
                if agent_name in self.agents:
                    logger.info(f"Executing {agent_name} agent")
                    
                    try:
                        # Execute specific agent
                        agent_result = self._execute_agent(agent_name, document_text, document_metadata, results)
                        results["agent_results"][agent_name] = agent_result
                        results["execution_order"].append(agent_name)
                        
                        # Quality check after each agent
                        if agent_name != "reflection":
                            quality_score = agent_result.get("confidence", 0.5)
                            if quality_score < GLOBAL_CONFIG["REFLECTION_THRESHOLD"]:
                                logger.info(f"Quality threshold not met for {agent_name}, running reflection")
                                reflection_result = self.agents["reflection"].reflect_on_multi_agent_analysis(
                                    {agent_name: agent_result}, 
                                    document_text
                                )
                                results["agent_results"]["reflection_" + agent_name] = reflection_result
                                results["quality_iterations"] += 1
                        
                    except Exception as e:
                        logger.error(f"Agent {agent_name} execution failed: {e}")
                        results["agent_results"][agent_name] = {"error": str(e)}
            
            # Final comprehensive reflection
            if len(results["agent_results"]) > 1:
                final_reflection = self.agents["reflection"].reflect_on_multi_agent_analysis(
                    results["agent_results"], 
                    document_text
                )
                results["agent_results"]["final_reflection"] = final_reflection
            
            # Synthesize all results
            results["final_synthesis"] = self._synthesize_comprehensive_results(results["agent_results"])
            results["overall_confidence"] = self._calculate_comprehensive_confidence(results["agent_results"])
            
            return results
            
        except Exception as e:
            logger.error(f"Comprehensive coordination failed: {e}")
            return {"error": str(e)}
    
    def _create_comprehensive_plan(self, text: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Create comprehensive coordination plan for all agents"""
        try:
            planning_prompt = f"""
            Create a comprehensive coordination plan for 8-agent GDPR analysis:
            
            Document Preview: {text[:1500]}...
            Document Metadata: {json.dumps(metadata, indent=2)}
            
            Available Agents: {json.dumps(self.agent_capabilities, indent=2)}
            
            Organization Context: {GLOBAL_CONFIG['ORGANIZATION_NAME']} ({GLOBAL_CONFIG['ORGANIZATION_TYPE']})
            Jurisdiction Focus: {GLOBAL_CONFIG['JURISDICTION_FOCUS']}
            
            Create optimal coordination plan including:
            
            1. EXECUTION ORDER:
            - Logical sequence for agent execution
            - Dependencies between agents
            - Parallel execution opportunities
            
            2. AGENT-SPECIFIC GOALS:
            - Specific objectives for each agent
            - Expected outputs and deliverables
            - Quality thresholds and success criteria
            
            3. INTEGRATION STRATEGY:
            - How agents should build on each other's work
            - Information sharing requirements
            - Synthesis and consolidation approach
            
            4. QUALITY ASSURANCE:
            - Reflection checkpoints
            - Quality improvement iterations
            - Final validation requirements
            
            5. RESOURCE ALLOCATION:
            - Processing complexity for each agent
            - Expected execution time estimates
            - Critical path identification
            
            Return structured JSON coordination plan.
            """
            
            response = self.llm.invoke([HumanMessage(content=planning_prompt)])
            content = response.content
            
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    return json.loads(content[json_start:json_end])
                except json.JSONDecodeError:
                    pass
            
            # Fallback plan
            return {
                "execution_order": ["researcher", "ontology", "data_ingestion", "financial_expert", "gdpr_react", "ropa_specialist"],
                "agent_goals": {agent: "Comprehensive analysis" for agent in self.agents.keys()},
                "quality_thresholds": 0.7,
                "integration_strategy": "Sequential with reflection loops"
            }
            
        except Exception as e:
            logger.error(f"Planning failed: {e}")
            return {"execution_order": list(self.agents.keys())}
    
    def _execute_agent(self, agent_name: str, text: str, metadata: Dict[str, Any], previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """Execute specific agent with context from previous results"""
        try:
            agent = self.agents[agent_name]
            
            if agent_name == "researcher":
                return agent.research_domain(text, ["comprehensive_gdpr", "uk_gdpr", "financial_services"])
            
            elif agent_name == "ontology":
                research_findings = []
                if "researcher" in previous_results.get("agent_results", {}):
                    research_findings = [previous_results["agent_results"]["researcher"]]
                return agent.create_ontology_schema(research_findings)
            
            elif agent_name == "data_ingestion":
                return agent.process_document(text, metadata)
            
            elif agent_name == "financial_expert":
                return agent.analyze_financial_context(text, ["banking", "compliance", "risk_management"])
            
            elif agent_name == "gdpr_react":
                return agent.analyze(text, "comprehensive_financial")
            
            elif agent_name == "ropa_specialist":
                return agent.extract_ropa_elements(text, ["article_30_complete", "financial_services"])
            
            elif agent_name == "reflection":
                return agent.reflect_on_multi_agent_analysis(previous_results.get("agent_results", {}), text)
            
            else:
                logger.warning(f"Unknown agent: {agent_name}")
                return {"error": f"Unknown agent: {agent_name}"}
                
        except Exception as e:
            logger.error(f"Agent {agent_name} execution failed: {e}")
            return {"error": str(e), "agent_type": agent_name}
    
    def _synthesize_comprehensive_results(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize results from all agents"""
        try:
            synthesis_prompt = f"""
            Synthesize comprehensive GDPR analysis from 8 specialized agents:
            
            Agent Results: {json.dumps(agent_results, indent=2, default=str)[:10000]}...
            
            Create comprehensive synthesis including:
            
            1. EXECUTIVE SUMMARY:
            - Key findings across all analytical dimensions
            - Critical compliance insights and recommendations
            - Strategic implications for {GLOBAL_CONFIG['ORGANIZATION_NAME']}
            
            2. INTEGRATED KNOWLEDGE BASE:
            - Comprehensive GDPR concept inventory
            - Ontology schema recommendations
            - Knowledge graph implementation guidance
            
            3. FINANCIAL SERVICES ANALYSIS:
            - Industry-specific compliance requirements
            - Risk assessments and mitigation strategies
            - Regulatory alignment recommendations
            
            4. ARTICLE 30 ROPA FRAMEWORK:
            - Complete metamodel specifications
            - Implementation roadmap and priorities
            - Quality assurance and governance framework
            
            5. TECHNICAL IMPLEMENTATION:
            - System architecture recommendations
            - Data processing and storage requirements
            - Integration specifications and standards
            
            6. ACTION PLAN:
            - Immediate implementation priorities
            - Medium-term development roadmap
            - Long-term compliance strategy
            
            Focus on creating actionable, comprehensive guidance for enterprise GDPR compliance.
            """
            
            response = self.llm.invoke([HumanMessage(content=synthesis_prompt)])
            
            return {
                "synthesis_type": "comprehensive_8_agent",
                "content": response.content,
                "agent_count": len(agent_results),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            return {"error": str(e)}
    
    def _calculate_comprehensive_confidence(self, agent_results: Dict[str, Any]) -> float:
        """Calculate overall confidence from all agent results"""
        confidences = []
        
        for agent_name, result in agent_results.items():
            if isinstance(result, dict):
                # Extract various confidence indicators
                for key in ["confidence", "overall_quality_score", "risk_score", "article_30_compliance"]:
                    if key in result and isinstance(result[key], (int, float)):
                        confidences.append(float(result[key]))
        
        return sum(confidences) / len(confidences) if confidences else 0.5

# Enhanced Main System Class
class Enhanced8AgentGDPRSystem:
    """Enhanced main system integrating all 8 specialized agents"""
    
    def __init__(self):
        self.vector_store = VectorStore()
        self.knowledge_graph = ComprehensiveKnowledgeGraph()
        self.supervisor = EnhancedSupervisorAgent()
        self.state = self._initialize_enhanced_state()
        
        logger.info("Enhanced 8-Agent GDPR System initialized")
    
    def _initialize_enhanced_state(self) -> EnhancedAgentState:
        """Initialize enhanced system state"""
        return EnhancedAgentState(
            messages=[],
            current_document=None,
            processed_documents=[],
            document_chunks=[],
            document_metadata={},
            extraction_quality={},
            research_findings=[],
            domain_concepts=[],
            regulatory_mappings={},
            external_references=[],
            concept_hierarchies={},
            ontology_schema=None,
            entity_types=[],
            relationship_types=[],
            schema_evolution=[],
            validation_rules=[],
            gdpr_articles=[],
            uk_gdpr_articles=[],
            legal_bases=[],
            data_subject_rights=[],
            principles=[],
            obligations=[],
            penalties=[],
            authorities=[],
            definitions=[],
            processing_activities=[],
            data_categories=[],
            security_measures=[],
            transfers=[],
            controllers=[],
            processors=[],
            financial_regulations=[],
            financial_data_types=[],
            compliance_frameworks=[],
            banking_processes=[],
            risk_assessments=[],
            ropa_elements={},
            article_30_compliance={},
            ropa_templates=[],
            compliance_gaps=[],
            generated_synonyms={},
            semantic_clusters=[],
            entity_mappings={},
            concept_relationships=[],
            current_agent="supervisor",
            agent_results={},
            agent_coordination_plan=None,
            agent_execution_order=[],
            inter_agent_messages=[],
            reflection_feedback=[],
            quality_metrics={},
            validation_results=[],
            improvement_suggestions=[],
            ropa_metamodel=None,
            business_report=None,
            compliance_assessment=None,
            executive_summary=None,
            confidence_scores={},
            processing_times={},
            error_logs=[]
        )
    
    def process_documents(self, document_paths: List[str] = None) -> Dict[str, Any]:
        """Process documents through enhanced 8-agent system"""
        if document_paths is None:
            pdf_path = GLOBAL_CONFIG["PDF_DOCUMENTS_PATH"]
            document_paths = [
                os.path.join(pdf_path, f) for f in os.listdir(pdf_path)
                if f.lower().endswith('.pdf')
            ]
        
        if not document_paths:
            raise ValueError("No PDF documents found to process")
        
        logger.info(f"Processing {len(document_paths)} documents through enhanced 8-agent system")
        
        results = {
            "documents_processed": 0,
            "total_agent_executions": 0,
            "concepts_discovered": 0,
            "ontology_classes_created": 0,
            "knowledge_graph_entities": 0,
            "vector_embeddings": 0,
            "synonyms_generated": 0,
            "quality_iterations": 0,
            "reflection_cycles": 0,
            "agent_performance": {}
        }
        
        for doc_path in document_paths:
            try:
                logger.info(f"Processing document: {doc_path}")
                
                # Extract text and metadata
                doc_text = self._extract_text_from_pdf(doc_path)
                doc_metadata = {
                    "source": doc_path,
                    "filename": os.path.basename(doc_path),
                    "size": len(doc_text),
                    "organization": GLOBAL_CONFIG["ORGANIZATION_NAME"],
                    "processing_timestamp": datetime.now().isoformat()
                }
                
                # Coordinate comprehensive analysis through supervisor
                coordination_result = self.supervisor.coordinate_comprehensive_analysis(doc_text, doc_metadata)
                
                # Update results tracking
                results["total_agent_executions"] += len(coordination_result.get("agent_results", {}))
                results["quality_iterations"] += coordination_result.get("quality_iterations", 0)
                results["reflection_cycles"] += len([k for k in coordination_result.get("agent_results", {}) if "reflection" in k])
                
                # Extract and store discoveries from all agents
                self._process_agent_discoveries(coordination_result.get("agent_results", {}))
                
                # Update performance metrics
                for agent_name, agent_result in coordination_result.get("agent_results", {}).items():
                    if agent_name not in results["agent_performance"]:
                        results["agent_performance"][agent_name] = {"executions": 0, "avg_confidence": 0.0}
                    
                    results["agent_performance"][agent_name]["executions"] += 1
                    confidence = agent_result.get("confidence", agent_result.get("overall_quality_score", 0.5))
                    results["agent_performance"][agent_name]["avg_confidence"] = confidence
                
                results["documents_processed"] += 1
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                self.state["error_logs"].append({
                    "document": doc_path,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
                continue
        
        # Generate final outputs
        self._generate_enhanced_outputs()
        
        # Update final result counts
        results.update({
            "concepts_discovered": len(self.state["domain_concepts"]),
            "ontology_classes_created": len(self.state["entity_types"]),
            "knowledge_graph_entities": len(self.state["processing_activities"]) + len(self.state["data_categories"]),
            "vector_embeddings": results["documents_processed"] * 50,  # Estimate
            "synonyms_generated": sum(len(syns) for syns in self.state["generated_synonyms"].values())
        })
        
        return results
    
    def _extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF document"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            return full_text
            
        except Exception as e:
            logger.error(f"Failed to extract text from {pdf_path}: {e}")
            return ""
    
    def _process_agent_discoveries(self, agent_results: Dict[str, Any]):
        """Process discoveries from all agents and update state"""
        try:
            for agent_name, result in agent_results.items():
                if not isinstance(result, dict) or "error" in result:
                    continue
                
                # Process research findings
                if agent_name == "researcher" and "research_findings" in result:
                    research_data = result["research_findings"]
                    if isinstance(research_data, dict):
                        # Extract regulatory concepts
                        for concept_type, concepts in research_data.items():
                            if isinstance(concepts, list):
                                for concept in concepts:
                                    if isinstance(concept, dict):
                                        # Store in vector database
                                        concept_data = {
                                            **concept,
                                            "discovered_by": "researcher_agent",
                                            "gdpr_category": concept_type,
                                            "timestamp": datetime.now()
                                        }
                                        self.vector_store.index_concept(concept_data)
                                        
                                        # Store in knowledge graph
                                        entity_type = self._map_concept_to_entity_type(concept_type)
                                        self.knowledge_graph.add_comprehensive_entity(entity_type, concept_data)
                
                # Process ontology schema
                if agent_name == "ontology" and "ontology_schema" in result:
                    self.state["ontology_schema"] = result["ontology_schema"]
                
                # Process RoPA elements
                if agent_name == "ropa_specialist" and "ropa_analysis" in result:
                    ropa_data = result["ropa_analysis"]
                    if isinstance(ropa_data, dict):
                        self.state["ropa_elements"] = ropa_data
                        self.state["article_30_compliance"] = result.get("article_30_compliance", 0.5)
                
                # Process financial analysis
                if agent_name == "financial_expert" and "financial_analysis" in result:
                    financial_data = result["financial_analysis"]
                    if isinstance(financial_data, dict):
                        self.state["financial_regulations"].extend(
                            financial_data.get("regulatory_alignment", [])
                        )
                        self.state["risk_assessments"].append({
                            "analysis": financial_data,
                            "risk_score": result.get("risk_score", 0.5),
                            "timestamp": datetime.now().isoformat()
                        })
                
        except Exception as e:
            logger.error(f"Failed to process agent discoveries: {e}")
    
    def _map_concept_to_entity_type(self, concept_type: str) -> str:
        """Map concept type to knowledge graph entity type"""
        mapping = {
            "regulatory_concept_inventory": "GDPRConcept",
            "concept_hierarchies": "ConceptHierarchy",
            "jurisdictional_analysis": "JurisdictionalConcept",
            "financial_services_mapping": "FinancialConcept",
            "ontology_recommendations": "OntologyClass",
            "external_references": "ExternalReference"
        }
        return mapping.get(concept_type, "Entity")
    
    def _generate_enhanced_outputs(self):
        """Generate enhanced final outputs"""
        try:
            # Generate comprehensive RoPA metamodel
            self._generate_comprehensive_ropa_metamodel()
            
            # Generate enhanced business report
            self._generate_enhanced_business_report()
            
            # Generate executive summary
            self._generate_executive_summary()
            
            logger.info("Enhanced outputs generated successfully")
            
        except Exception as e:
            logger.error(f"Enhanced output generation failed: {e}")
    
    def _generate_comprehensive_ropa_metamodel(self):
        """Generate comprehensive RoPA metamodel from all agent inputs"""
        try:
            metamodel_prompt = f"""
            Create a comprehensive Article 30 RoPA metamodel for {GLOBAL_CONFIG['ORGANIZATION_NAME']} based on enhanced 8-agent analysis:
            
            Research Findings: {len(self.state['research_findings'])} concepts discovered
            Ontology Schema: {bool(self.state['ontology_schema'])}
            RoPA Elements: {len(self.state['ropa_elements'])} element types identified
            Financial Analysis: {len(self.state['risk_assessments'])} risk assessments completed
            
            Create enterprise-grade RoPA metamodel including:
            
            1. CORE METAMODEL ARCHITECTURE:
            - Entity definitions and relationships
            - Attribute specifications and constraints
            - Validation rules and business logic
            - Implementation guidelines
            
            2. FINANCIAL SERVICES EXTENSIONS:
            - Banking-specific processing activities
            - Financial data categorizations
            - Regulatory alignment mappings
            - Risk management integration
            
            3. MULTI-JURISDICTIONAL SUPPORT:
            - EU GDPR vs UK GDPR differentiation
            - Cross-border transfer handling
            - Adequacy decision management
            - Regulatory change adaptation
            
            4. TECHNOLOGY INTEGRATION:
            - Knowledge graph schema alignment
            - Vector database integration
            - Automated compliance monitoring
            - Reporting and analytics framework
            
            5. GOVERNANCE FRAMEWORK:
            - Data stewardship roles and responsibilities
            - Quality assurance procedures
            - Change management processes
            - Audit and compliance monitoring
            
            6. IMPLEMENTATION ROADMAP:
            - Phase 1: Core RoPA foundation
            - Phase 2: Financial services customization
            - Phase 3: Advanced analytics and monitoring
            - Phase 4: Continuous improvement and evolution
            
            Provide comprehensive, implementation-ready metamodel suitable for enterprise deployment.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=metamodel_prompt)])
            
            self.state["ropa_metamodel"] = {
                "metamodel_content": response.content,
                "agent_contributions": {
                    "research_concepts": len(self.state["research_findings"]),
                    "ontology_classes": len(self.state["entity_types"]),
                    "ropa_elements": len(self.state["ropa_elements"]),
                    "financial_insights": len(self.state["risk_assessments"])
                },
                "generated_timestamp": datetime.now().isoformat(),
                "organization_context": GLOBAL_CONFIG["ORGANIZATION_NAME"],
                "jurisdiction_focus": GLOBAL_CONFIG["JURISDICTION_FOCUS"],
                "agent_system_version": "8_agent_enhanced"
            }
            
            logger.info("Comprehensive RoPA metamodel generated successfully")
            
        except Exception as e:
            logger.error(f"RoPA metamodel generation failed: {e}")
    
    def _generate_enhanced_business_report(self):
        """Generate enhanced business-friendly compliance report"""
        try:
            report_prompt = f"""
            Generate a comprehensive executive business report for {GLOBAL_CONFIG['ORGANIZATION_NAME']} based on enhanced 8-agent GDPR analysis.
            
            System Analysis Summary:
            - Documents Processed: {len(self.state['processed_documents'])}
            - Research Findings: {len(self.state['research_findings'])}
            - Ontology Classes: {len(self.state['entity_types'])}
            - RoPA Elements: {len(self.state['ropa_elements'])}
            - Risk Assessments: {len(self.state['risk_assessments'])}
            - Quality Iterations: {len(self.state['reflection_feedback'])}
            
            Create comprehensive executive report with:
            
            # Executive Summary
            - Overall GDPR compliance posture assessment
            - Critical findings and strategic implications
            - Investment priorities and resource requirements
            - Timeline for compliance achievement
            
            # Enhanced Knowledge Base Analysis
            - Comprehensive regulatory concept mapping
            - Multi-agent analytical insights
            - Cross-jurisdictional compliance considerations
            - Knowledge graph and ontology recommendations
            
            # Financial Services GDPR Compliance
            - Banking-specific compliance requirements
            - Regulatory alignment with FCA, PRA, and other frameworks
            - Customer data protection strategies
            - Third-party risk management protocols
            - Cross-border banking operations compliance
            
            # Article 30 RoPA Implementation
            - Complete RoPA readiness assessment
            - Metamodel implementation roadmap
            - Governance and quality assurance framework
            - Technology integration requirements
            - Staff training and change management needs
            
            # Multi-Agent Quality Assurance
            - Analysis quality metrics across all agents
            - Consistency validation results
            - Continuous improvement recommendations
            - System performance optimization
            
            # Risk Assessment & Mitigation
            - Comprehensive risk landscape analysis
            - Financial services specific risks
            - Data protection impact assessment needs
            - Breach notification preparedness
            - Regulatory enforcement risk evaluation
            
            # Strategic Implementation Plan
            ## Phase 1: Foundation (0-6 months)
            - Core knowledge graph implementation
            - Basic RoPA framework establishment
            - Essential compliance documentation
            
            ## Phase 2: Enhancement (6-12 months)
            - Financial services customization
            - Advanced analytics implementation
            - Cross-border compliance optimization
            
            ## Phase 3: Optimization (12-18 months)
            - AI-powered compliance monitoring
            - Continuous improvement processes
            - Advanced risk management integration
            
            # Investment Analysis
            - Technology infrastructure requirements
            - Human resource needs and training
            - External consultancy and legal support
            - ROI projections and business case
            
            # Governance Framework
            - Data protection governance structure
            - Roles and responsibilities matrix
            - Monitoring and reporting procedures
            - Continuous compliance assurance
            
            Make it highly actionable with specific recommendations, timelines, and business impact analysis suitable for board-level presentation.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=report_prompt)])
            
            self.state["business_report"] = response.content
            
            logger.info("Enhanced business report generated successfully")
            
        except Exception as e:
            logger.error(f"Enhanced business report generation failed: {e}")
    
    def _generate_executive_summary(self):
        """Generate executive summary from all agent outputs"""
        try:
            summary_prompt = f"""
            Create executive summary for {GLOBAL_CONFIG['ORGANIZATION_NAME']} senior leadership:
            
            8-Agent Analysis Completed:
            - ResearcherAgent: {len(self.state['research_findings'])} regulatory concepts identified
            - OntologyAgent: {bool(self.state['ontology_schema'])} comprehensive schema designed
            - DataIngestionAgent: {len(self.state['processed_documents'])} documents processed
            - FinancialExpertAgent: {len(self.state['risk_assessments'])} risk assessments completed
            - GDPRReActAgent: Comprehensive regulatory analysis with tool usage
            - RoPASpecialistAgent: Article 30 compliance framework developed
            - ReflectionAgent: {len(self.state['reflection_feedback'])} quality iterations completed
            - SupervisorAgent: Multi-agent coordination and synthesis
            
            ## Key Findings Summary
            
            ### Compliance Status
            - Current GDPR readiness level
            - Critical gaps requiring immediate attention
            - Overall risk assessment
            
            ### Strategic Recommendations
            - Top 3 immediate priorities
            - Medium-term implementation roadmap
            - Long-term competitive advantages
            
            ### Investment Requirements
            - Essential technology infrastructure
            - Human resource needs
            - External support requirements
            - Expected ROI timeline
            
            ### Risk Mitigation
            - High-priority compliance risks
            - Financial services specific considerations
            - Regulatory enforcement exposure
            
            Keep concise but comprehensive - suitable for 10-minute board presentation.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=summary_prompt)])
            
            self.state["executive_summary"] = response.content
            
            logger.info("Executive summary generated successfully")
            
        except Exception as e:
            logger.error(f"Executive summary generation failed: {e}")
    
    def search_comprehensive_knowledge(self, query: str, agent_filter: str = None) -> Dict[str, Any]:
        """Search across vector and graph knowledge with agent filtering"""
        filters = {}
        if agent_filter:
            filters["discovered_by"] = agent_filter
            
        vector_results = self.vector_store.semantic_search(query, filters=filters, top_k=10)
        graph_results = self.knowledge_graph.query_comprehensive_knowledge(query)
        
        return {
            "query": query,
            "agent_filter": agent_filter,
            "vector_results": vector_results,
            "graph_results": graph_results,
            "total_results": len(vector_results) + len(graph_results),
            "agent_breakdown": self._analyze_agent_contributions(vector_results)
        }
    
    def _analyze_agent_contributions(self, results: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analyze which agents contributed to search results"""
        agent_counts = defaultdict(int)
        for result in results:
            agent = result.get("discovered_by", "unknown")
            agent_counts[agent] += 1
        return dict(agent_counts)
    
    def get_system_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive system performance metrics"""
        return {
            "agent_execution_summary": {
                "total_agents": len(self.supervisor.agents),
                "successful_executions": len([r for r in self.state["agent_results"].values() if not r.get("error")]),
                "error_count": len(self.state["error_logs"]),
                "reflection_cycles": len(self.state["reflection_feedback"])
            },
            "knowledge_base_metrics": {
                "research_concepts": len(self.state["research_findings"]),
                "ontology_classes": len(self.state["entity_types"]),
                "processing_activities": len(self.state["processing_activities"]),
                "data_categories": len(self.state["data_categories"]),
                "financial_regulations": len(self.state["financial_regulations"]),
                "synonyms_generated": len(self.state["generated_synonyms"])
            },
            "compliance_metrics": {
                "ropa_elements_identified": len(self.state["ropa_elements"]),
                "article_30_compliance_score": self.state["article_30_compliance"],
                "risk_assessments_completed": len(self.state["risk_assessments"]),
                "compliance_gaps": len(self.state["compliance_gaps"])
            },
            "quality_metrics": {
                "average_confidence_score": sum(self.state["confidence_scores"].values()) / len(self.state["confidence_scores"]) if self.state["confidence_scores"] else 0,
                "validation_success_rate": len([v for v in self.state["validation_results"] if v.get("status") == "passed"]) / max(len(self.state["validation_results"]), 1),
                "improvement_suggestions": len(self.state["improvement_suggestions"])
            }
        }
    
    def save_comprehensive_results(self) -> Dict[str, str]:
        """Save all enhanced results to files"""
        output_files = {}
        
        try:
            # Save comprehensive RoPA metamodel
            if self.state["ropa_metamodel"]:
                metamodel_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "comprehensive_ropa_metamodel.json")
                with open(metamodel_file, 'w') as f:
                    json.dump(self.state["ropa_metamodel"], f, indent=2)
                output_files["ropa_metamodel"] = metamodel_file
            
            # Save enhanced business report
            if self.state["business_report"]:
                report_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "enhanced_business_compliance_report.md")
                with open(report_file, 'w') as f:
                    f.write(self.state["business_report"])
                output_files["business_report"] = report_file
            
            # Save executive summary
            if self.state["executive_summary"]:
                summary_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "executive_summary.md")
                with open(summary_file, 'w') as f:
                    f.write(self.state["executive_summary"])
                output_files["executive_summary"] = summary_file
            
            # Save ontology schema
            if self.state["ontology_schema"]:
                ontology_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "gdpr_ontology_schema.json")
                with open(ontology_file, 'w') as f:
                    json.dump(self.state["ontology_schema"], f, indent=2)
                output_files["ontology_schema"] = ontology_file
            
            # Save research findings
            research_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "research_findings.json")
            with open(research_file, 'w') as f:
                json.dump(self.state["research_findings"], f, indent=2, default=str)
            output_files["research_findings"] = research_file
            
            # Save generated synonyms
            synonyms_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "generated_synonyms.json")
            with open(synonyms_file, 'w') as f:
                json.dump(self.state["generated_synonyms"], f, indent=2)
            output_files["synonyms"] = synonyms_file
            
            # Save system performance metrics
            metrics_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "system_performance_metrics.json")
            with open(metrics_file, 'w') as f:
                json.dump(self.get_system_performance_metrics(), f, indent=2)
            output_files["performance_metrics"] = metrics_file
            
            # Save comprehensive system state
            state_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "enhanced_system_state.json")
            with open(state_file, 'w') as f:
                # Convert state to JSON serializable format
                serializable_state = {}
                for key, value in self.state.items():
                    if key != "messages":  # Skip complex message objects
                        serializable_state[key] = value
                json.dump(serializable_state, f, indent=2, default=str)
            output_files["system_state"] = state_file
            
            # Save agent execution log
            execution_log_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "agent_execution_log.json")
            with open(execution_log_file, 'w') as f:
                execution_log = {
                    "agent_results": self.state["agent_results"],
                    "execution_order": self.state["agent_execution_order"],
                    "reflection_feedback": self.state["reflection_feedback"],
                    "error_logs": self.state["error_logs"]
                }
                json.dump(execution_log, f, indent=2, default=str)
            output_files["execution_log"] = execution_log_file
            
            logger.info(f"Enhanced results saved to {len(output_files)} files")
            return output_files
            
        except Exception as e:
            logger.error(f"Failed to save enhanced results: {e}")
            return {}

def main():
    """Enhanced main execution function with 8-agent system"""
    parser = argparse.ArgumentParser(description="Enhanced 8-Agent GDPR Knowledge Graph & RoPA System")
    parser.add_argument("--process", nargs="*", help="Process documents through 8-agent system (paths optional)")
    parser.add_argument("--search", type=str, help="Search comprehensive knowledge base")
    parser.add_argument("--agent-filter", type=str, help="Filter search by specific agent", 
                        choices=["researcher", "ontology", "data_ingestion", "financial_expert", "gdpr_react", "ropa_specialist"])
    parser.add_argument("--generate-report", action="store_true", help="Generate enhanced business report only")
    parser.add_argument("--performance-metrics", action="store_true", help="Show system performance metrics")
    parser.add_argument("--save-results", action="store_true", help="Save all enhanced results to files")
    parser.add_argument("--config", type=str, help="Path to custom configuration file")
    parser.add_argument("--agent-analysis", type=str, help="Run specific agent analysis", 
                        choices=["researcher", "ontology", "financial_expert", "ropa_specialist"])
    
    args = parser.parse_args()
    
    try:
        # Load custom configuration if provided
        if args.config and os.path.exists(args.config):
            with open(args.config, 'r') as f:
                custom_config = json.load(f)
                GLOBAL_CONFIG.update(custom_config)
                logger.info(f"Loaded custom configuration from {args.config}")
        
        # Initialize enhanced 8-agent system
        system = Enhanced8AgentGDPRSystem()
        print(f" Enhanced 8-Agent GDPR System initialized for {GLOBAL_CONFIG['ORGANIZATION_NAME']}")
        print(f" Specialized Agents: {len(system.supervisor.agents)} agents available")
        print(f" Jurisdiction Focus: {', '.join(GLOBAL_CONFIG['JURISDICTION_FOCUS'])}")
        
        # Default: run complete workflow if no arguments
        if not any(vars(args).values()):
            print(" Running complete enhanced 8-agent GDPR workflow...")
            
            # Process documents through all agents
            print("\n Step 1: Processing documents through specialized agent system...")
            result = system.process_documents()
            print(f" Enhanced processing completed:")
            print(f"   Documents Processed: {result['documents_processed']}")
            print(f"   Total Agent Executions: {result['total_agent_executions']}")
            print(f"   Concepts Discovered: {result['concepts_discovered']}")
            print(f"   Ontology Classes: {result['ontology_classes_created']}")
            print(f"   KG Entities: {result['knowledge_graph_entities']}")
            print(f"   Vector Embeddings: {result['vector_embeddings']}")
            print(f"   Synonyms Generated: {result['synonyms_generated']}")
            print(f"   Quality Iterations: {result['quality_iterations']}")
            print(f"   Reflection Cycles: {result['reflection_cycles']}")
            
            # Show agent performance
            print(f"\n Agent Performance Summary:")
            for agent_name, performance in result['agent_performance'].items():
                print(f"   {agent_name.title()}: {performance['executions']} executions, "
                      f"{performance['avg_confidence']:.2f} avg confidence")
            
            # Save comprehensive results
            print("\n Step 2: Saving comprehensive results...")
            output_files = system.save_comprehensive_results()
            for file_type, file_path in output_files.items():
                print(f"   {file_type.replace('_', ' ').title()}: {file_path}")
            
            # Show performance metrics
            print("\n Step 3: System Performance Metrics...")
            metrics = system.get_system_performance_metrics()
            for category, stats in metrics.items():
                print(f"   {category.replace('_', ' ').title()}:")
                for key, value in stats.items():
                    print(f"     {key.replace('_', ' ').title()}: {value}")
            
            print(f"\n Complete enhanced 8-agent workflow finished!")
            print(f" Check output directory: {GLOBAL_CONFIG['OUTPUT_PATH']}")
            return
        
        # Execute individual operations
        if args.process is not None:
            result = system.process_documents(args.process if args.process else None)
            print(f" Enhanced document processing completed:")
            for key, value in result.items():
                if isinstance(value, dict):
                    print(f"   {key.replace('_', ' ').title()}:")
                    for subkey, subvalue in value.items():
                        print(f"     {subkey}: {subvalue}")
                else:
                    print(f"   {key.replace('_', ' ').title()}: {value}")
        
        if args.search:
            results = system.search_comprehensive_knowledge(args.search, args.agent_filter)
            print(f" Enhanced Search Results for '{args.search}':")
            if args.agent_filter:
                print(f"   Filtered by: {args.agent_filter}")
            print(f"   Vector Results: {len(results['vector_results'])}")
            print(f"   Graph Results: {len(results['graph_results'])}")
            print(f"   Total Results: {results['total_results']}")
            print(f"   Agent Contributions: {results['agent_breakdown']}")
        
        if args.performance_metrics:
            metrics = system.get_system_performance_metrics()
            print(" Enhanced System Performance Metrics:")
            for category, stats in metrics.items():
                print(f"\n  {category.replace('_', ' ').title()}:")
                for key, value in stats.items():
                    print(f"    {key.replace('_', ' ').title()}: {value}")
        
        if args.agent_analysis:
            print(f" Running {args.agent_analysis} agent analysis...")
            # This would require implementing individual agent execution
            # For now, redirect to full processing
            result = system.process_documents()
            print(f" Agent analysis completed as part of full system execution")
        
        if args.generate_report:
            system._generate_enhanced_business_report()
            system._generate_executive_summary()
            print(" Enhanced business report and executive summary generated")
        
        if args.save_results:
            output_files = system.save_comprehensive_results()
            print(f" Enhanced results saved to {len(output_files)} files:")
            for file_type, file_path in output_files.items():
                print(f"   {file_type.replace('_', ' ').title()}: {file_path}")
    
    except Exception as e:
        print(f" System error: {e}")
        logger.error(f"Enhanced system error: {e}")

if __name__ == "__main__":
    main()
