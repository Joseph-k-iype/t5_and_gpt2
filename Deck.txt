#!/usr/bin/env python3
"""
Simple TTL to FalkorDB Loader
Just loads CSV files from csv_output/ folder directly to existing FalkorDB
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import sys
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TTLToFalkorDBLoader:
    def __init__(self, output_dir='csv_output'):
        """Initialize the loader"""
        self.output_dir = Path(output_dir).resolve()
        self.nodes = {}
        self.edges = []
        self.node_id_map = {}
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        try:
            if isinstance(uri_or_literal, URIRef):
                uri_str = str(uri_or_literal)
                parsed = urlparse(uri_str)
                
                if parsed.fragment:
                    name = parsed.fragment
                elif parsed.path and parsed.path != '/':
                    name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
                elif parsed.netloc:
                    name = parsed.netloc.replace('.', '_')
                else:
                    name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"n_{cleaned}"
                if not cleaned or len(cleaned) < 1:
                    cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
                return cleaned[:50]
            else:
                name = str(uri_or_literal)
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"l_{cleaned}"
                return (cleaned or "literal")[:50]
        except Exception as e:
            logger.warning(f"Error cleaning identifier: {e}")
            return f"error_{hashlib.md5(str(uri_or_literal).encode()).hexdigest()[:8]}"
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique ID for a resource"""
        uri = str(resource)
        if uri not in self.node_id_map:
            hash_obj = hashlib.md5(uri.encode('utf-8'))
            node_id = f"n_{hash_obj.hexdigest()[:8]}"
            
            original_id = node_id
            counter = 1
            while node_id in [node['id'] for node in self.nodes.values()]:
                node_id = f"{original_id}_{counter}"
                counter += 1
            
            self.node_id_map[uri] = node_id
        return self.node_id_map[uri]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {'uri': str(uri)}
        
        try:
            parsed = urlparse(str(uri))
            if parsed.fragment:
                properties['local_name'] = parsed.fragment
                properties['namespace'] = str(uri).replace('#' + parsed.fragment, '')
            elif parsed.path and parsed.path != '/':
                parts = [p for p in parsed.path.strip('/').split('/') if p]
                if parts:
                    properties['local_name'] = parts[-1]
                    properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        except Exception as e:
            logger.warning(f"Error extracting properties: {e}")
        
        return properties
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        if value is None:
            return ''
        
        if isinstance(value, bool):
            return 'true' if value else 'false'
        
        if isinstance(value, (list, dict)):
            return json.dumps(value, ensure_ascii=False)
        
        str_value = str(value).strip()
        str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
        str_value = re.sub(r'\s+', ' ', str_value)
        
        if '"' in str_value:
            str_value = str_value.replace('"', '""')
        
        if len(str_value) > 500:
            str_value = str_value[:500] + "..."
        
        return str_value
    
    def process_literal_value(self, literal: Literal) -> str:
        """Process literal value and return cleaned string"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int_val = int(literal)
                        if abs(int_val) <= 2147483647:
                            return str(int_val)
                    except ValueError:
                        pass
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:
                            return str(float_val)
                    except ValueError:
                        pass
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower()
            
            return str(literal)
        except Exception:
            return str(literal)
    
    def convert_ttl_to_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL file to CSV format"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Parse TTL file
        graph = Graph()
        try:
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found!")
            return []
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"Processing only first {max_triples} of {total_triples} triples")
            total_triples = max_triples
        
        # Process triples
        processed_count = 0
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                if max_triples and processed_count >= max_triples:
                    break
                
                pbar.update(1)
                processed_count += 1
                
                # Get or create subject node
                subject_id = self.get_or_create_node_id(subject)
                subject_type = self.clean_identifier(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Initialize subject node if not exists
                if subject_id not in self.nodes:
                    if isinstance(subject, URIRef):
                        base_props = self.extract_properties_from_uri(subject)
                    else:  # BNode
                        base_props = {
                            'uri': str(subject),
                            'resource_type': 'blank_node'
                        }
                    
                    self.nodes[subject_id] = {
                        'id': subject_id,
                        'node_type': subject_type,
                        'properties': base_props
                    }
                
                # Handle object
                if isinstance(obj, Literal):
                    # Add as property to subject node
                    value = self.process_literal_value(obj)
                    prop_name = predicate_clean
                    self.nodes[subject_id]['properties'][prop_name] = value
                    
                    if obj.language:
                        lang_prop = f"{prop_name}_lang"
                        self.nodes[subject_id]['properties'][lang_prop] = obj.language
                
                else:
                    # Object is a resource - create edge
                    object_id = self.get_or_create_node_id(obj)
                    object_type = self.clean_identifier(obj)
                    
                    # Initialize object node if not exists
                    if object_id not in self.nodes:
                        if isinstance(obj, URIRef):
                            base_props = self.extract_properties_from_uri(obj)
                        else:  # BNode
                            base_props = {
                                'uri': str(obj),
                                'resource_type': 'blank_node'
                            }
                        
                        self.nodes[object_id] = {
                            'id': object_id,
                            'node_type': object_type,
                            'properties': base_props
                        }
                    
                    # Create edge
                    edge = {
                        'source_id': subject_id,
                        'target_id': object_id,
                        'edge_type': predicate_clean,
                        'properties': {
                            'predicate_uri': str(predicate)
                        }
                    }
                    self.edges.append(edge)
        
        logger.info(f"Processing complete: {len(self.nodes):,} nodes, {len(self.edges):,} edges")
        
        # Write CSV files
        csv_files = self.write_csv_files()
        return csv_files
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        all_properties = sorted(all_properties)
        
        with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
            headers = ['id', 'node_type'] + all_properties
            writer = csv.writer(csvfile, **self.csv_params)
            writer.writerow(headers)
            
            for node in self.nodes.values():
                row = [node['id'], node['node_type']]
                for prop in all_properties:
                    value = node['properties'].get(prop, '')
                    row.append(self.sanitize_csv_value(value))
                writer.writerow(row)
        
        logger.info(f"Written {len(self.nodes):,} nodes to {nodes_file.name}")
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        # Get all possible edge property names
        all_edge_properties = set()
        for edge in self.edges:
            all_edge_properties.update(edge['properties'].keys())
        all_edge_properties = sorted(all_edge_properties)
        
        with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
            headers = ['source_id', 'target_id', 'edge_type'] + all_edge_properties
            writer = csv.writer(csvfile, **self.csv_params)
            writer.writerow(headers)
            
            for edge in self.edges:
                row = [edge['source_id'], edge['target_id'], edge['edge_type']]
                for prop in all_edge_properties:
                    value = edge['properties'].get(prop, '')
                    row.append(self.sanitize_csv_value(value))
                writer.writerow(row)
        
        logger.info(f"Written {len(self.edges):,} edges to {edges_file.name}")
        return csv_files
    
    def configure_import_folder_and_load(self, graph_name: str, host: str = '127.0.0.1', 
                                       port: int = 6379, password: Optional[str] = None,
                                       optimize_edges: bool = True, batch_size: int = 1000):
        """Configure FalkorDB import folder to point to csv_output and load data with optimizations"""
        logger.info(f"Connecting to FalkorDB at {host}:{port}...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Connect to FalkorDB
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            r.ping()
            logger.info("Connected to FalkorDB successfully!")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            logger.error("Make sure FalkorDB is running on port 6379")
            return False
        
        # Check if CSV files exist
        nodes_file = self.output_dir / "nodes.csv"
        edges_file = self.output_dir / "edges.csv"
        
        if not nodes_file.exists():
            logger.error(f"nodes.csv not found in {self.output_dir}")
            logger.error("Run with --convert first to generate CSV files")
            return False
            
        if not edges_file.exists():
            logger.error(f"edges.csv not found in {self.output_dir}")
            logger.error("Run with --convert first to generate CSV files")
            return False
        
        logger.info(f"CSV files found in: {self.output_dir}")
        
        # Try to configure import folder to point to our csv_output directory
        try:
            logger.info(f"Configuring FalkorDB import folder to: {self.output_dir}")
            r.execute_command("GRAPH.CONFIG", "SET", "IMPORT_FOLDER", str(self.output_dir))
            logger.info("Import folder configured successfully!")
        except Exception as e:
            logger.warning(f"Could not set import folder via GRAPH.CONFIG: {e}")
            logger.warning("This is normal for some FalkorDB versions")
            logger.warning("Will try to load with default configuration...")
        
        # Try loading with file:// URLs (should work if import folder is configured correctly)
        try:
            logger.info("Loading nodes...")
            nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row
            CREATE (n:Entity)
            SET n = row
            """
            
            result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
            logger.info(f"✅ Nodes loaded successfully: {result}")
            
        except Exception as e:
            logger.error(f"❌ Failed to load nodes: {e}")
            
            if "error opening csv uri" in str(e).lower():
                logger.error("\n🔧 SOLUTION NEEDED:")
                logger.error("FalkorDB cannot access your csv_output folder.")
                logger.error("You need to restart FalkorDB with csv_output mounted as import directory:")
                logger.error("")
                logger.error("Option 1 - Docker restart:")
                abs_path = str(self.output_dir)
                logger.error(f'docker stop <your_container>')
                logger.error(f'docker run -d -p 6379:6379 -v "{abs_path}:/var/lib/falkordb/import" falkordb/falkordb:latest')
                logger.error("")
                logger.error("Option 2 - Copy files to FalkorDB import directory:")
                logger.error("Find your FalkorDB import directory and copy nodes.csv and edges.csv there")
                logger.error("")
                logger.error("Option 3 - Use absolute file paths (if supported):")
                logger.error("Some FalkorDB versions support absolute paths in LOAD CSV")
            
            return False
        
        # Create index on Entity.id for faster edge loading
        try:
            logger.info("Creating index on Entity.id for faster edge loading...")
            index_query = "CREATE INDEX FOR (n:Entity) ON (n.id)"
            r.execute_command("GRAPH.QUERY", graph_name, index_query)
            logger.info("✅ Index created successfully")
        except Exception as e:
            logger.warning(f"Index creation failed (may already exist): {e}")
        
        # Load edges with multiple optimization strategies
        if optimize_edges:
            logger.info(f"Loading edges with optimizations (batch size: {batch_size})...")
            edge_success = self._load_edges_optimized(r, graph_name, batch_size)
        else:
            logger.info("Loading edges with basic query...")
            edge_success = self._load_edges_basic(r, graph_name)
        
        if not edge_success:
            return False
        
        # Verify data
        try:
            count_query = "MATCH (n) RETURN count(n) AS node_count"
            result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
            logger.info(f"📊 Total nodes in graph: {result}")
            
            edge_count_query = "MATCH ()-[r]->() RETURN count(r) AS edge_count"
            result = r.execute_command("GRAPH.QUERY", graph_name, edge_count_query)
            logger.info(f"📊 Total edges in graph: {result}")
            
        except Exception as e:
            logger.warning(f"Could not verify counts: {e}")
        
        logger.info("🎉 SUCCESS! Data loaded into FalkorDB!")
        logger.info(f"Graph name: {graph_name}")
        logger.info(f"CSV files remain in: {self.output_dir}")
        
        return True
    
    def _load_edges_optimized(self, redis_conn, graph_name: str, batch_size: int = 1000):
        """Load edges with multiple optimization strategies"""
        
        # Strategy 1: Single MATCH with comma-separated pattern (fastest)
        try:
            logger.info("🚀 Trying Strategy 1: Single MATCH with batching...")
            edges_query_strategy1 = f"""
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            CALL {{
                WITH row
                MATCH (source:Entity {{id: row.source_id}}), (target:Entity {{id: row.target_id}})
                CREATE (source)-[r:CONNECTED_TO]->(target)
                SET r = {{predicate_uri: row.predicate_uri}}
            }} IN TRANSACTIONS OF {batch_size} ROWS
            """
            
            result = redis_conn.execute_command("GRAPH.QUERY", graph_name, edges_query_strategy1)
            logger.info(f"✅ Strategy 1 SUCCESS: {result}")
            return True
            
        except Exception as e:
            logger.warning(f"⚠️ Strategy 1 failed: {e}")
        
        # Strategy 2: MERGE for duplicate-safe loading (slower but safer)
        try:
            logger.info("🚀 Trying Strategy 2: MERGE relationships...")
            edges_query_strategy2 = f"""
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            CALL {{
                WITH row
                MATCH (source:Entity {{id: row.source_id}})
                MATCH (target:Entity {{id: row.target_id}})
                MERGE (source)-[r:CONNECTED_TO]->(target)
                SET r = {{predicate_uri: row.predicate_uri}}
            }} IN TRANSACTIONS OF {batch_size} ROWS
            """
            
            result = redis_conn.execute_command("GRAPH.QUERY", graph_name, edges_query_strategy2)
            logger.info(f"✅ Strategy 2 SUCCESS: {result}")
            return True
            
        except Exception as e:
            logger.warning(f"⚠️ Strategy 2 failed: {e}")
        
        # Strategy 3: Basic optimization - avoid SET/REMOVE pattern
        try:
            logger.info("🚀 Trying Strategy 3: Optimized basic query...")
            edges_query_strategy3 = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            MATCH (source:Entity {id: row.source_id})
            MATCH (target:Entity {id: row.target_id})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = {predicate_uri: row.predicate_uri}
            """
            
            result = redis_conn.execute_command("GRAPH.QUERY", graph_name, edges_query_strategy3)
            logger.info(f"✅ Strategy 3 SUCCESS: {result}")
            return True
            
        except Exception as e:
            logger.warning(f"⚠️ Strategy 3 failed: {e}")
        
        # Fall back to basic loading
        logger.info("🔄 All optimization strategies failed, falling back to basic loading...")
        return self._load_edges_basic(redis_conn, graph_name)
    
    def _load_edges_basic(self, redis_conn, graph_name: str):
        """Load edges with basic query (fallback)"""
        try:
            logger.info("📝 Using basic edge loading query...")
            edges_query_basic = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            MATCH (source:Entity {id: row.source_id})
            MATCH (target:Entity {id: row.target_id})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = row
            REMOVE r.source_id, r.target_id, r.edge_type
            """
            
            result = redis_conn.execute_command("GRAPH.QUERY", graph_name, edges_query_basic)
            logger.info(f"✅ Basic edge loading SUCCESS: {result}")
            return True
            
        except Exception as e:
            logger.error(f"❌ Basic edge loading FAILED: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Load CSV files from csv_output/ to FalkorDB on port 6379')
    parser.add_argument('ttl_file', nargs='?', help='Path to TTL file (for conversion)')
    parser.add_argument('--convert', action='store_true', help='Convert TTL to CSV first')
    parser.add_argument('--load', action='store_true', help='Load existing CSV files to FalkorDB')
    parser.add_argument('--both', action='store_true', help='Convert TTL and load to FalkorDB')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--password', help='FalkorDB password')
    parser.add_argument('--output_dir', default='csv_output', help='CSV output directory')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process')
    parser.add_argument('--optimize_edges', action='store_true', default=True, help='Use optimized edge loading (default: True)')
    parser.add_argument('--basic_edges', action='store_true', help='Use basic edge loading (disable optimizations)')
    parser.add_argument('--batch_size', type=int, default=1000, help='Batch size for edge loading (default: 1000)')
    parser.add_argument('--large_dataset', action='store_true', help='Optimize for large datasets (batch_size=5000)')
    parser.add_argument('--small_dataset', action='store_true', help='Optimize for small datasets (batch_size=100)')
    
    args = parser.parse_args()
    
    # Create loader
    loader = TTLToFalkorDBLoader(args.output_dir)
    
    # If no action specified, show help
    if not any([args.convert, args.load, args.both]):
        if args.ttl_file:
            args.both = True  # Default action if TTL file provided
        else:
            parser.print_help()
            print("\nExamples:")
            print("  python script.py data.ttl --convert          # Convert TTL to CSV")
            print("  python script.py --load                      # Load existing CSV to FalkorDB")
            print("  python script.py data.ttl --both             # Convert and load")
            print("  python script.py data.ttl                    # Same as --both")
            print("\nOptimization options:")
            print("  python script.py --load --large_dataset      # Optimize for large datasets")
            print("  python script.py --load --small_dataset      # Optimize for small datasets")
            print("  python script.py --load --basic_edges        # Use basic edge loading")
            print("  python script.py --load --batch_size 2000    # Custom batch size")
            return
    
    start_time = time.time()
    
    # Convert TTL to CSV if requested
    if args.convert or args.both:
        if not args.ttl_file:
            logger.error("TTL file required for conversion")
            sys.exit(1)
        
        if not os.path.exists(args.ttl_file):
            logger.error(f"TTL file not found: {args.ttl_file}")
            sys.exit(1)
        
        try:
            file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
            logger.info(f"Converting TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
            
            csv_files = loader.convert_ttl_to_csv(args.ttl_file, args.max_triples)
            
            conversion_time = time.time() - start_time
            logger.info(f"✅ Conversion completed in {conversion_time:.1f}s")
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            sys.exit(1)
    
    # Load CSV to FalkorDB if requested
    if args.load or args.both:
        try:
            # Determine optimization settings
            optimize_edges = not args.basic_edges
            batch_size = args.batch_size
            
            if args.large_dataset:
                batch_size = 5000
                logger.info("🚀 Large dataset mode: batch_size=5000")
            elif args.small_dataset:
                batch_size = 100
                logger.info("🎯 Small dataset mode: batch_size=100")
            
            success = loader.configure_import_folder_and_load(
                args.graph_name, args.host, args.port, args.password,
                optimize_edges, batch_size
            )
            
            if success:
                total_time = time.time() - start_time
                logger.info(f"🎉 COMPLETE! Total time: {total_time:.1f}s")
                logger.info(f"📍 FalkorDB: {args.host}:{args.port}")
                logger.info(f"📊 Graph: {args.graph_name}")
                logger.info(f"📁 CSV files: {loader.output_dir}")
            else:
                logger.error("❌ Failed to load data to FalkorDB")
                sys.exit(1)
                
        except Exception as e:
            logger.error(f"❌ Loading failed: {e}")
            sys.exit(1)

if __name__ == "__main__":
    main()
