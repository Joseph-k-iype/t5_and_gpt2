#!/usr/bin/env python3
"""
Enhanced GDPR Record of Processing Activities (RoPA) Metamodel RAG System
Specialized for GDPR and UK GDPR Compliance in Financial Institutions

Features:
- GDPR and UK GDPR compliance analysis and requirements
- Financial sector specific RoPA requirements
- Cross-border data transfer between EU and UK
- Advanced regulatory concept extraction with o3-mini reasoning
- Iterative document understanding
- Comprehensive compliance reporting

REQUIRED ENVIRONMENT VARIABLES:
    OPENAI_API_KEY=your_openai_api_key
    OPENAI_BASE_URL=your_custom_openai_endpoint (optional)
    ELASTICSEARCH_HOST=https://your-elasticsearch-cluster.com:9200
    FALKORDB_HOST=localhost
    FALKORDB_PORT=6379

USAGE:
    python gdpr_ropa_system.py --ingest /path/to/gdpr/documents
    python gdpr_ropa_system.py --analyze
    python gdpr_ropa_system.py --generate-metamodel
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GDPR Territorial Scope and Applicability Framework
class GDPRTerritorialScope(Enum):
    GDPR_EU = "gdpr_eu"
    UK_GDPR = "uk_gdpr"

@dataclass
class GDPRApplicabilityFramework:
    name: str
    territorial_scope: List[str]
    extraterritorial_triggers: List[str]
    adequacy_decisions: List[str]
    cross_border_mechanisms: List[str]
    key_principles: List[str]
    penalties: str
    data_subject_rights: List[str]

# GDPR and UK GDPR Territorial Applicability Framework
GDPR_APPLICABILITY_FRAMEWORK = {
    GDPRTerritorialScope.GDPR_EU: GDPRApplicabilityFramework(
        name="EU General Data Protection Regulation",
        territorial_scope=[
            "All 27 EU Member States", "European Economic Area (EEA)", 
            "Organizations established in EU", "Processing in context of EU establishment"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to EU data subjects",
            "Monitoring behavior of EU data subjects",
            "Processing personal data of EU residents",
            "Targeting EU market regardless of location"
        ],
        adequacy_decisions=[
            "Andorra", "Argentina", "Canada (commercial)", "Faroe Islands", "Guernsey", 
            "Israel", "Isle of Man", "Japan", "Jersey", "New Zealand", "Republic of Korea",
            "Switzerland", "United Kingdom", "Uruguay"
        ],
        cross_border_mechanisms=[
            "Standard Contractual Clauses (SCCs)", "Binding Corporate Rules (BCRs)",
            "Certification mechanisms", "Codes of conduct", "Derogations Article 49"
        ],
        key_principles=["lawfulness", "fairness", "transparency", "purpose_limitation", "data_minimization", "accuracy", "storage_limitation", "integrity_confidentiality", "accountability"],
        penalties="Up to €20 million or 4% of annual global turnover",
        data_subject_rights=["access", "rectification", "erasure", "portability", "restriction", "objection", "automated_decision_making"]
    ),
    GDPRTerritorialScope.UK_GDPR: GDPRApplicabilityFramework(
        name="United Kingdom GDPR",
        territorial_scope=[
            "England", "Wales", "Scotland", "Northern Ireland",
            "Organizations established in UK", "Processing in context of UK establishment"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to UK data subjects",
            "Monitoring behavior of UK data subjects", 
            "Processing personal data of UK residents",
            "Targeting UK market regardless of location"
        ],
        adequacy_decisions=[
            "All countries with EU adequacy decisions", "Gibraltar",
            "Countries under transition arrangements", "EU Member States"
        ],
        cross_border_mechanisms=[
            "International Data Transfer Agreement (IDTA)", "Standard Contractual Clauses",
            "Binding Corporate Rules", "Certification mechanisms", "Derogations"
        ],
        key_principles=["lawfulness", "fairness", "transparency", "purpose_limitation", "data_minimization", "accuracy", "storage_limitation", "integrity_confidentiality", "accountability"],
        penalties="Up to £17.5 million or 4% of annual global turnover",
        data_subject_rights=["access", "rectification", "erasure", "portability", "restriction", "objection", "automated_decision_making"]
    )
}

# Global Jurisdictions that may be affected by GDPR/UK GDPR
POTENTIAL_AFFECTED_JURISDICTIONS = [
    # Asia-Pacific
    "India", "China", "Australia", "Singapore", "Japan", "South Korea", "Hong Kong",
    "Malaysia", "Thailand", "Philippines", "Indonesia", "Vietnam", "Taiwan",
    
    # Americas  
    "United States", "Canada", "Brazil", "Mexico", "Argentina", "Chile", "Colombia",
    
    # Europe (Non-EU)
    "Switzerland", "Norway", "Iceland", "Liechtenstein", "Serbia", "Montenegro", "Albania",
    "North Macedonia", "Bosnia and Herzegovina", "Moldova", "Ukraine", "Turkey", "Russia",
    
    # Middle East & Africa
    "United Arab Emirates", "Saudi Arabia", "Israel", "Egypt", "South Africa", "Nigeria",
    "Kenya", "Morocco", "Qatar", "Kuwait", "Bahrain",
    
    # EU Member States
    "Germany", "France", "Italy", "Spain", "Netherlands", "Belgium", "Austria", "Sweden",
    "Denmark", "Finland", "Poland", "Czech Republic", "Hungary", "Slovakia", "Slovenia",
    "Croatia", "Bulgaria", "Romania", "Greece", "Portugal", "Ireland", "Luxembourg",
    "Cyprus", "Malta", "Estonia", "Latvia", "Lithuania"
]

class RopaMetamodelState(TypedDict):
    """Enhanced state for RoPA metamodel generation with territorial scope discovery"""
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Dict[str, Any]]
    extracted_concepts: List[Dict[str, Any]]
    territorial_scope_analysis: Dict[str, Any]
    extraterritorial_applicability: List[Dict[str, Any]]
    adequacy_decisions_analysis: List[Dict[str, Any]]
    regulatory_entities: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    cross_border_transfers: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    retention_policies: List[Dict[str, Any]]
    metamodel_structure: Dict[str, Any]
    compliance_gaps: List[Dict[str, Any]]
    affected_jurisdictions: List[Dict[str, Any]]
    reasoning_trace: List[str]

class CustomOpenAIEmbeddings(Embeddings):
    """Custom OpenAI Embeddings for enhanced regulatory document processing"""
    
    def __init__(self, 
                 model: str = "text-embedding-3-large",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 dimensions: Optional[int] = 3072,
                 max_chunk_size: int = 8000):
        
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = OpenAI(**client_kwargs)
        self.model = model
        self.dimensions = dimensions
        self.max_chunk_size = max_chunk_size
        
        logger.info(f"Initialized CustomOpenAIEmbeddings with model: {model}")
    
    def _chunk_text_by_characters(self, text: str) -> List[str]:
        """Character-based chunking for regulatory documents"""
        if len(text) <= self.max_chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chunk_size
            
            if end < len(text):
                # Look for regulatory section boundaries
                chunk_end = text.rfind('Article ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('Section ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('Clause ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('.', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('\n', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind(' ', start, end)
                
                if chunk_end > start:
                    end = chunk_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed regulatory documents with enhanced processing"""
        all_embeddings = []
        
        for text in texts:
            chunks = self._chunk_text_by_characters(text)
            
            if len(chunks) == 1:
                embedding = self._get_single_embedding(chunks[0])
                all_embeddings.append(embedding)
            else:
                # For regulatory documents, average embeddings with weighting
                chunk_embeddings = []
                weights = []
                
                for i, chunk in enumerate(chunks):
                    chunk_embedding = self._get_single_embedding(chunk)
                    chunk_embeddings.append(chunk_embedding)
                    
                    # Weight chunks with regulatory keywords higher
                    weight = 1.0
                    regulatory_keywords = ['gdpr', 'article', 'section', 'data protection', 'privacy', 'controller', 'processor', 'consent', 'lawful basis']
                    for keyword in regulatory_keywords:
                        if keyword.lower() in chunk.lower():
                            weight += 0.2
                    weights.append(weight)
                
                # Weighted average
                total_weight = sum(weights)
                avg_embedding = [
                    sum(emb[i] * weights[j] for j, emb in enumerate(chunk_embeddings)) / total_weight
                    for i in range(len(chunk_embeddings[0]))
                ]
                all_embeddings.append(avg_embedding)
        
        return all_embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed regulatory queries"""
        chunks = self._chunk_text_by_characters(text)
        return self._get_single_embedding(chunks[0])
    
    def _get_single_embedding(self, text: str) -> List[float]:
        """Get embedding for single text chunk"""
        try:
            params = {"input": text, "model": self.model}
            if self.dimensions:
                params["dimensions"] = self.dimensions
            
            response = self.client.embeddings.create(**params)
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to get embedding: {e}")
            raise

class EnhancedRegulatoryProcessor:
    """Enhanced processor for regulatory and financial documents"""
    
    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 300):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "Article ", "Section ", "Clause ", ".", "!", "?", ";", ":", " "]
        )
        self.regulatory_patterns = {
            'article_references': r'Article\s+(\d+(?:\.\d+)*)',
            'legal_bases': r'(Article\s+6\s*\([a-f]\)|legitimate\s+interest|consent|contract|legal\s+obligation|vital\s+interest|public\s+task)',
            'data_categories': r'(personal\s+data|special\s+categor(?:y|ies)|biometric|health|financial|location|identifier)',
            'processing_purposes': r'(processing\s+(?:for|purpose)|purpose(?:s)?\s+of)',
            'retention_periods': r'(\d+\s+(?:days?|months?|years?)|retention\s+period|storage\s+limitation)',
            'transfers': r'(third\s+countr(?:y|ies)|international\s+transfer|adequacy\s+decision)',
            'rights': r'(right\s+to\s+(?:access|rectification|erasure|portability|restriction|object)|data\s+subject\s+rights)'
        }
    
    def extract_pdf_content(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract and enhance PDF content for regulatory analysis"""
        logger.info(f"Processing regulatory document: {pdf_path}")
        
        doc = pymupdf.open(pdf_path)
        full_text = ""
        metadata = {"pages": len(doc), "document_type": self._detect_document_type(pdf_path)}
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            page_text = page.get_text()
            full_text += f"\n\n{page_text}"
        
        doc.close()
        
        # Enhanced chunking with regulatory context
        text_chunks = self.text_splitter.split_text(full_text)
        
        chunks = []
        for i, chunk_text in enumerate(text_chunks):
            # Extract regulatory patterns
            regulatory_matches = self._extract_regulatory_patterns(chunk_text)
            
            chunk = {
                "chunk_id": f"chunk_{i}",
                "text": chunk_text,
                "chunk_index": i,
                "source": pdf_path,
                "document_type": metadata["document_type"],
                "regulatory_patterns": regulatory_matches,
                "metadata": {
                    "word_count": len(chunk_text.split()),
                    "char_count": len(chunk_text),
                    "contains_article_refs": len(regulatory_matches.get("article_references", [])) > 0,
                    "contains_legal_bases": len(regulatory_matches.get("legal_bases", [])) > 0,
                    "regulatory_density": self._calculate_regulatory_density(chunk_text)
                }
            }
            chunks.append(chunk)
        
        logger.info(f"Created {len(chunks)} enhanced regulatory chunks")
        return chunks
    
    def _detect_document_type(self, pdf_path: str) -> str:
        """Detect the type of regulatory document"""
        filename = os.path.basename(pdf_path).lower()
        
        if any(term in filename for term in ['gdpr', 'regulation', 'directive']):
            return "regulation"
        elif any(term in filename for term in ['policy', 'procedure', 'guideline']):
            return "policy"
        elif any(term in filename for term in ['compliance', 'audit', 'assessment']):
            return "compliance"
        elif any(term in filename for term in ['business', 'process', 'procedure']):
            return "business_process"
        else:
            return "general"
    
    def _extract_regulatory_patterns(self, text: str) -> Dict[str, List[str]]:
        """Extract regulatory patterns from text"""
        matches = {}
        
        for pattern_name, pattern in self.regulatory_patterns.items():
            matches[pattern_name] = re.findall(pattern, text, re.IGNORECASE)
        
        return matches
    
    def _calculate_regulatory_density(self, text: str) -> float:
        """Calculate density of regulatory terms in text"""
        regulatory_terms = [
            'gdpr', 'data protection', 'privacy', 'controller', 'processor', 'consent',
            'lawful basis', 'legitimate interest', 'personal data', 'processing',
            'data subject', 'supervisory authority', 'compliance', 'breach',
            'retention', 'security', 'accountability', 'transparency'
        ]
        
        text_lower = text.lower()
        term_count = sum(1 for term in regulatory_terms if term in text_lower)
        word_count = len(text.split())
        
        return term_count / word_count if word_count > 0 else 0.0

@tool
def llm_synonym_generation_agent(text: str, domain_context: str = "regulatory_compliance") -> Dict[str, Any]:
    """LLM agent to generate contextual synonyms for regulatory terms"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Generate comprehensive synonyms and alternative terms for regulatory and compliance concepts found in this text.

Text: {text}

Domain Context: {domain_context}

Your task is to identify key regulatory terms, concepts, and phrases, then generate contextually appropriate synonyms and alternative expressions that users might search for.

Focus on:

1. REGULATORY TERMINOLOGY:
   - Official legal terms and their common alternatives
   - Formal vs informal expressions
   - Technical vs business language variants
   - Acronyms and their full forms

2. TERRITORIAL AND JURISDICTIONAL TERMS:
   - Different ways to refer to the same jurisdiction
   - Formal vs common country/region names
   - Legal entity variations

3. COMPLIANCE CONCEPTS:
   - Different expressions for the same compliance requirement
   - Procedural terminology variants
   - Risk and assessment language alternatives

4. FINANCIAL SECTOR TERMINOLOGY:
   - Banking vs investment vs insurance terminology
   - Regulatory framework alternatives
   - Business process variations

5. CROSS-LANGUAGE CONSIDERATIONS:
   - Common translations or transliterations
   - International variations of terms

Generate synonyms that are:
- Contextually accurate and legally precise
- Commonly used in practice
- Varied in formality levels
- Inclusive of industry jargon and standard terms

Return comprehensive synonym mappings in JSON format:

{{
    "synonym_mappings": [
        {{
            "primary_term": "data controller",
            "category": "regulatory_entity",
            "synonyms": ["controller", "data controlling entity", "controlling organization", "data determiner"],
            "context": "Entity that determines purposes and means of processing personal data",
            "confidence": 0.95,
            "usage_level": "formal|informal|technical|business",
            "jurisdictional_variants": ["EU: controller", "UK: data controller", "US: business"],
            "related_concepts": ["data processor", "joint controller", "data steward"]
        }},
        {{
            "primary_term": "territorial scope",
            "category": "compliance_concept", 
            "synonyms": ["territorial application", "geographical scope", "jurisdictional reach", "extraterritorial application", "cross-border applicability"],
            "context": "Geographic extent of regulatory application",
            "confidence": 0.90,
            "usage_level": "formal",
            "jurisdictional_variants": ["EU: territorial scope", "UK: territorial application"],
            "related_concepts": ["adequacy decision", "transfer mechanism", "establishment"]
        }},
        {{
            "primary_term": "adequacy decision",
            "category": "transfer_mechanism",
            "synonyms": ["adequacy determination", "adequacy finding", "third country adequacy", "data protection adequacy"],
            "context": "European Commission decision that third country provides adequate protection",
            "confidence": 0.95,
            "usage_level": "formal",
            "jurisdictional_variants": ["EU: adequacy decision", "UK: adequacy regulations"],
            "related_concepts": ["transfer mechanism", "third country", "data protection level"]
        }}
    ],
    "extracted_terms": [
        {{
            "term": "term_found_in_text",
            "frequency": 3,
            "context_snippet": "surrounding text for context",
            "importance": "high|medium|low"
        }}
    ],
    "concept_relationships": [
        {{
            "concept1": "data controller",
            "concept2": "data processor", 
            "relationship_type": "complementary|hierarchical|sequential|conflicting",
            "description": "Controllers determine purposes, processors act on behalf of controllers"
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Ensure required fields exist
            if "synonym_mappings" not in result:
                result["synonym_mappings"] = []
            if "extracted_terms" not in result:
                result["extracted_terms"] = []
            if "concept_relationships" not in result:
                result["concept_relationships"] = []
            
            return result
        else:
            return {
                "synonym_mappings": [],
                "extracted_terms": [],
                "concept_relationships": []
            }
    
    except Exception as e:
        logger.warning(f"LLM synonym generation failed: {e}")
        return {
            "synonym_mappings": [],
            "extracted_terms": [],
            "concept_relationships": []
        }
    """Enhanced agent that extracts GDPR/UK GDPR territorial scope and applicability"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze this regulatory/financial document text to discover GDPR and UK GDPR territorial scope and applicability:

Text: {text}

Perform comprehensive territorial scope analysis and extract:

1. TERRITORIAL SCOPE ANALYSIS:
   - Which EU member states are mentioned or affected
   - UK-specific territorial provisions
   - References to EEA or other European territories
   - Cross-border processing contexts

2. EXTRATERRITORIAL APPLICABILITY:
   - Organizations outside EU/UK that must comply
   - Conditions triggering GDPR/UK GDPR for foreign entities
   - "Offering goods/services" provisions
   - "Monitoring behavior" provisions
   - Targeting of EU/UK markets

3. ADEQUACY DECISIONS AND THIRD COUNTRIES:
   - Countries with adequacy decisions mentioned
   - Third country transfer mechanisms
   - Specific country transfer arrangements
   - References to Switzerland, Japan, Canada, etc.

4. AFFECTED JURISDICTIONS DISCOVERY:
   - Countries/regions mentioned in processing context
   - Cross-border data flows identified
   - International subsidiaries or operations
   - Global financial institution operations

5. PROCESSING ACTIVITIES WITH TERRITORIAL CONTEXT:
   - Where processing occurs geographically
   - Data subject locations
   - Controller/processor establishments
   - Service provision territories

6. FINANCIAL SECTOR TERRITORIAL SPECIFICS:
   - Cross-border banking operations
   - International payment processing
   - Global customer bases
   - Regulatory reporting across jurisdictions

7. TRANSFER MECHANISMS AND SAFEGUARDS:
   - Standard Contractual Clauses (SCCs)
   - Binding Corporate Rules (BCRs)
   - International Data Transfer Agreements (IDTAs)
   - Certification and code mechanisms

Return comprehensive JSON focusing on territorial discovery:

{{
    "territorial_scope": {{
        "eu_member_states_affected": ["countries explicitly mentioned"],
        "uk_territorial_provisions": ["UK-specific territorial aspects"],
        "eea_references": ["EEA territorial scope mentions"],
        "processing_locations": ["where processing occurs"]
    }},
    "extraterritorial_applicability": [
        {{
            "trigger_condition": "offering_goods_services|monitoring_behavior|targeting_market",
            "affected_jurisdiction": "country/region name",
            "compliance_requirements": ["specific requirements"],
            "context": "detailed explanation"
        }}
    ],
    "adequacy_decisions_mentioned": [
        {{
            "country": "country name",
            "adequacy_status": "adequate|inadequate|under_review",
            "transfer_context": "context of mention",
            "specific_provisions": ["any specific provisions mentioned"]
        }}
    ],
    "affected_jurisdictions_discovered": [
        {{
            "jurisdiction": "country/region name",
            "gdpr_applicability": "direct|extraterritorial|transfer_only|not_applicable",
            "uk_gdpr_applicability": "direct|extraterritorial|transfer_only|not_applicable",
            "context": "why this jurisdiction is relevant",
            "data_flows": ["types of data flows identified"],
            "business_operations": ["business activities mentioned"]
        }}
    ],
    "processing_activities": [
        {{
            "activity_name": "name of processing activity",
            "territorial_context": "where activity occurs",
            "cross_border_elements": ["international aspects"],
            "affected_data_subjects": ["location of data subjects"],
            "legal_basis_territorial": "legal basis considering territory"
        }}
    ],
    "cross_border_transfers": [
        {{
            "origin_territory": "where data originates",
            "destination_territory": "where data is transferred",
            "transfer_mechanism": "SCC|BCR|adequacy|derogation|IDTA",
            "safeguards_required": ["additional safeguards needed"],
            "transfer_context": "business reason for transfer"
        }}
    ],
    "regulatory_entities": [
        {{
            "name": "entity name",
            "role": "controller|processor|supervisory_authority|dpo",
            "territorial_establishment": "where entity is established",
            "territorial_scope": "scope of territorial authority"
        }}
    ],
    "data_categories": [
        {{
            "category": "data category name",
            "territorial_restrictions": ["any territorial restrictions"],
            "cross_border_sensitivity": "sensitivity for transfers",
            "financial_sector_context": "relevance to financial services"
        }}
    ],
    "concept_synonyms": [
        {{
            "primary_term": "territorial scope|extraterritorial|adequacy decision|cross-border transfer",
            "synonyms": ["alternative terms found in text"],
            "context": "regulatory context and definition",
            "territorial_relevance": "why territorially relevant",
            "confidence": 0.95
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Ensure all required fields exist
            required_fields = [
                "territorial_scope", "extraterritorial_applicability", "adequacy_decisions_mentioned",
                "affected_jurisdictions_discovered", "processing_activities", "cross_border_transfers",
                "regulatory_entities", "data_categories", "concept_synonyms"
            ]
            
            for field in required_fields:
                if field not in result:
                    result[field] = [] if field != "territorial_scope" else {}
            
            return result
        else:
            return {field: [] if field != "territorial_scope" else {} for field in [
                "territorial_scope", "extraterritorial_applicability", "adequacy_decisions_mentioned",
                "affected_jurisdictions_discovered", "processing_activities", "cross_border_transfers",
                "regulatory_entities", "data_categories", "concept_synonyms"
            ]}
    
    except Exception as e:
        logger.warning(f"Enhanced territorial scope extraction failed: {e}")
        return {field: [] if field != "territorial_scope" else {} for field in [
            "territorial_scope", "extraterritorial_applicability", "adequacy_decisions_mentioned",
            "affected_jurisdictions_discovered", "processing_activities", "cross_border_transfers",
            "regulatory_entities", "data_categories", "concept_synonyms"
        ]}

@tool
def metamodel_analysis_agent(extracted_data: str) -> Dict[str, Any]:
    """Agent for analyzing extracted data and building metamodel structure"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze the extracted regulatory data to build a comprehensive GDPR RoPA metamodel structure for global financial institutions:

Extracted Data: {extracted_data}

Create a comprehensive metamodel that includes:

1. CORE METAMODEL ENTITIES:
   - Abstract base classes
   - Concrete implementation classes
   - Relationship mappings
   - Constraint definitions

2. JURISDICTIONAL EXTENSIONS:
   - Jurisdiction-specific requirements
   - Variation points for different laws
   - Inheritance hierarchies
   - Compatibility matrices

3. FINANCIAL SECTOR SPECIALIZATIONS:
   - Banking-specific processing
   - Insurance data handling
   - Investment management
   - Payment processing
   - Credit scoring
   - Risk management

4. COMPLIANCE FRAMEWORKS:
   - Audit trail requirements
   - Reporting structures
   - Documentation standards
   - Certification processes

5. OPERATIONAL MAPPINGS:
   - Business process integration
   - System architecture alignment
   - Data flow modeling
   - Control frameworks

Return structured metamodel design:

{{
    "metamodel_structure": {{
        "core_entities": [
            {{"entity": "entity_name", "type": "abstract|concrete", "attributes": ["attributes"], "relationships": ["relationships"], "constraints": ["constraints"]}}
        ],
        "jurisdictional_variants": [
            {{"jurisdiction": "jurisdiction_name", "extensions": ["specific_extensions"], "overrides": ["overridden_elements"], "additional_requirements": ["requirements"]}}
        ],
        "financial_specializations": [
            {{"domain": "banking|insurance|investment|payments", "specific_entities": ["entities"], "processing_types": ["types"], "data_categories": ["categories"]}}
        ],
        "relationship_matrix": [
            {{"entity1": "entity_name", "entity2": "entity_name", "relationship_type": "one-to-one|one-to-many|many-to-many", "constraints": ["constraints"]}}
        ]
    }},
    "implementation_guidance": {{
        "mandatory_elements": ["elements_required_in_all_jurisdictions"],
        "optional_elements": ["elements_for_specific_contexts"],
        "extensibility_points": ["areas_for_customization"],
        "validation_rules": ["rules_for_metamodel_consistency"]
    }},
    "compliance_mapping": {{
        "audit_requirements": ["requirements_per_jurisdiction"],
        "reporting_obligations": ["reporting_structures"],
        "documentation_standards": ["documentation_requirements"],
        "certification_processes": ["certification_frameworks"]
    }}
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {
                "metamodel_structure": {"core_entities": [], "jurisdictional_variants": [], "financial_specializations": [], "relationship_matrix": []},
                "implementation_guidance": {"mandatory_elements": [], "optional_elements": [], "extensibility_points": [], "validation_rules": []},
                "compliance_mapping": {"audit_requirements": [], "reporting_obligations": [], "documentation_standards": [], "certification_processes": []}
            }
    
    except Exception as e:
        logger.warning(f"Metamodel analysis failed: {e}")
        return {
            "metamodel_structure": {"core_entities": [], "jurisdictional_variants": [], "financial_specializations": [], "relationship_matrix": []},
            "implementation_guidance": {"mandatory_elements": [], "optional_elements": [], "extensibility_points": [], "validation_rules": []},
            "compliance_mapping": {"audit_requirements": [], "reporting_obligations": [], "documentation_standards": [], "certification_processes": []}
        }

@tool
def territorial_compliance_analysis_agent(territorial_data: str, applicability_framework: str) -> Dict[str, Any]:
    """Agent for analyzing territorial compliance across discovered jurisdictions"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Perform comprehensive territorial compliance analysis for GDPR and UK GDPR across discovered jurisdictions:

Territorial Data: {territorial_data}

GDPR/UK GDPR Applicability Framework: {applicability_framework}

Analyze and identify:

1. TERRITORIAL COVERAGE ANALYSIS:
   - Gaps in territorial scope understanding
   - Missing adequacy decision considerations
   - Incomplete cross-border transfer mechanisms
   - Unaddressed extraterritorial triggers

2. JURISDICTION-SPECIFIC COMPLIANCE REQUIREMENTS:
   - How GDPR applies to operations in India, China, Australia, etc.
   - UK GDPR applicability to global operations
   - EU member state specific variations
   - Third country transfer requirements

3. EXTRATERRITORIAL COMPLIANCE CHALLENGES:
   - Organizations outside EU/UK that must comply
   - Targeting and monitoring provisions
   - Cross-border service provision
   - Global financial institution obligations

4. ADEQUACY AND TRANSFER MECHANISM GAPS:
   - Countries without adequacy decisions
   - Required transfer safeguards by jurisdiction
   - Alternative transfer mechanisms needed
   - Risk assessments for transfers

5. OPERATIONAL COMPLIANCE PRIORITIES:
   - High-risk jurisdictional combinations
   - Complex multi-jurisdictional scenarios
   - Conflicting regulatory requirements
   - Implementation challenges

Return comprehensive territorial compliance analysis:

{{
    "territorial_coverage_gaps": [
        {{
            "jurisdiction": "jurisdiction_name",
            "gap_type": "territorial_scope|adequacy|transfer_mechanism|extraterritorial",
            "description": "detailed gap description",
            "gdpr_impact": "how GDPR compliance is affected",
            "uk_gdpr_impact": "how UK GDPR compliance is affected",
            "remediation": "recommended actions"
        }}
    ],
    "jurisdiction_specific_requirements": [
        {{
            "jurisdiction": "country/region name",
            "gdpr_applicability_analysis": "detailed analysis of how GDPR applies",
            "uk_gdpr_applicability_analysis": "detailed analysis of how UK GDPR applies",
            "compliance_obligations": ["specific obligations for this jurisdiction"],
            "transfer_requirements": ["transfer mechanism requirements"],
            "local_law_considerations": ["local law conflicts or considerations"]
        }}
    ],
    "extraterritorial_scenarios": [
        {{
            "scenario": "scenario description",
            "trigger_conditions": ["what triggers extraterritorial application"],
            "affected_entities": ["types of entities affected"],
            "compliance_requirements": ["specific compliance requirements"],
            "complexity_level": "low|medium|high|very_high"
        }}
    ],
    "adequacy_transfer_analysis": [
        {{
            "origin_jurisdiction": "where data originates",
            "destination_jurisdiction": "where data goes",
            "adequacy_status": "adequate|inadequate|partial|under_review",
            "required_mechanisms": ["transfer mechanisms needed"],
            "additional_safeguards": ["extra safeguards required"],
            "risk_level": "low|medium|high|very_high"
        }}
    ],
    "compliance_priorities": [
        {{
            "priority_area": "area requiring attention",
            "jurisdictions_affected": ["list of affected jurisdictions"],
            "urgency": "immediate|high|medium|low",
            "complexity": "simple|moderate|complex|very_complex",
            "business_impact": "impact on business operations",
            "recommended_approach": "recommended implementation approach"
        }}
    ],
    "cross_jurisdictional_conflicts": [
        {{
            "conflict_description": "description of regulatory conflict",
            "conflicting_jurisdictions": ["jurisdictions with conflicting requirements"],
            "conflict_type": "legal_basis|transfer|rights|enforcement",
            "resolution_strategy": "recommended resolution approach",
            "legal_precedents": ["relevant legal precedents if any"]
        }}
    ],
    "implementation_roadmap": [
        {{
            "phase": "implementation phase name",
            "jurisdictions_focus": ["jurisdictions to focus on in this phase"],
            "timeline": "recommended timeline",
            "key_activities": ["main activities for this phase"],
            "success_metrics": ["how to measure success"],
            "risk_factors": ["potential risks and mitigation"]
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {
                "territorial_coverage_gaps": [], "jurisdiction_specific_requirements": [],
                "extraterritorial_scenarios": [], "adequacy_transfer_analysis": [],
                "compliance_priorities": [], "cross_jurisdictional_conflicts": [],
                "implementation_roadmap": []
            }
    
    except Exception as e:
        logger.warning(f"Territorial compliance analysis failed: {e}")
        return {
            "territorial_coverage_gaps": [], "jurisdiction_specific_requirements": [],
            "extraterritorial_scenarios": [], "adequacy_transfer_analysis": [],
            "compliance_priorities": [], "cross_jurisdictional_conflicts": [],
            "implementation_roadmap": []
        }

class EnhancedRegulatoryVectorEngine:
    """Enhanced vector engine for regulatory document processing"""
    
    def __init__(self, 
                 host: str = "http://localhost:9200", 
                 index_name: str = "gdpr_ropa_metamodel",
                 username: str = None,
                 password: str = None,
                 ca_certs: str = None,
                 verify_certs: bool = True,
                 openai_api_key: str = None,
                 openai_base_url: str = None):
        
        self.index_name = index_name
        
        # Initialize enhanced embeddings for regulatory processing
        self.embeddings = CustomOpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=openai_api_key,
            base_url=openai_base_url,
            dimensions=3072
        )
        
        # Configure Elasticsearch client
        self.client = self._create_elasticsearch_client(
            host, username, password, ca_certs, verify_certs
        )
        
        self._create_regulatory_index()
    
    def _create_elasticsearch_client(self, host, username, password, ca_certs, verify_certs):
        """Create Elasticsearch client for regulatory data with proper CA certificate handling"""
        if not host.startswith(('http://', 'https://')):
            raise ValueError(f"Elasticsearch host must include schema. Got: {host}")
        
        client_config = {
            "hosts": [host],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # Configure SSL/TLS settings
        if host.startswith('https://'):
            client_config["use_ssl"] = True
            client_config["verify_certs"] = verify_certs
            
            if ca_certs:
                client_config["ca_certs"] = ca_certs
                logger.info(f"Using CA certificate: {ca_certs}")
            
            if not verify_certs:
                client_config["ssl_show_warn"] = False
                logger.warning("SSL certificate verification disabled")
        
        # Configure authentication
        if username and password:
            client_config["basic_auth"] = (username, password)
            logger.info(f"Using basic authentication for user: {username}")
        elif username or password:
            logger.warning("Both username and password must be provided for authentication")
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch for regulatory processing")
                info = client.info()
                logger.info(f"Elasticsearch version: {info.body['version']['number']}")
                logger.info(f"Cluster name: {info.body['cluster_name']}")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch cluster")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_regulatory_index(self):
        """Create enhanced index for regulatory documents"""
        mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "regulatory_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer",
                                "keyword_repeat",
                                "remove_duplicates",
                                "regulatory_synonyms"
                            ]
                        }
                    },
                    "filter": {
                        "regulatory_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "gdpr,general data protection regulation",
                                "controller,data controller",
                                "processor,data processor",
                                "dpo,data protection officer",
                                "ropa,record of processing activities",
                                "ccpa,california consumer privacy act",
                                "pipeda,personal information protection and electronic documents act",
                                "lgpd,lei geral de proteção de dados"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "regulatory_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "document_type": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "regulatory_patterns": {"type": "object"},
                    "regulatory_density": {"type": "float"},
                    
                    # RoPA specific fields
                    "processing_activities": {"type": "nested"},
                    "data_categories": {"type": "nested"},
                    "legal_bases": {"type": "nested"},
                    "regulatory_entities": {"type": "nested"},
                    "security_measures": {"type": "nested"},
                    "jurisdictional_mappings": {"type": "nested"},
                    
                    # Metamodel fields
                    "metamodel_entity": {"type": "keyword"},
                    "compliance_relevance": {"type": "float"},
                    "implementation_complexity": {"type": "keyword"},
                    
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "timestamp": {"type": "date"},
                    "metadata": {"type": "object"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created regulatory index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create regulatory index: {e}")
            raise
    
    def index_regulatory_documents(self, chunks: List[Dict[str, Any]], extractions: List[Dict[str, Any]]):
        """Index regulatory documents with enhanced metadata"""
        logger.info("Indexing regulatory documents with enhanced RoPA metadata...")
        
        for i, chunk in enumerate(chunks):
            extraction = extractions[i] if i < len(extractions) else {}
            
            # Create primary document embedding
            embedding = self.embeddings.embed_query(chunk["text"])
            
            doc = {
                "text": chunk["text"],
                "embedding": embedding,
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "document_type": chunk.get("document_type", "general"),
                "regulatory_patterns": chunk.get("regulatory_patterns", {}),
                "regulatory_density": chunk.get("metadata", {}).get("regulatory_density", 0.0),
                
                # Enhanced RoPA extractions with territorial context
                "processing_activities": extraction.get("processing_activities", []),
                "data_categories": extraction.get("data_categories", []),
                "regulatory_entities": extraction.get("regulatory_entities", []),
                "cross_border_transfers": extraction.get("cross_border_transfers", []),
                "affected_jurisdictions_discovered": extraction.get("affected_jurisdictions_discovered", []),
                "territorial_scope": extraction.get("territorial_scope", {}),
                "extraterritorial_applicability": extraction.get("extraterritorial_applicability", []),
                "adequacy_decisions_mentioned": extraction.get("adequacy_decisions_mentioned", []),
                
                # Metamodel relevance
                "metamodel_entity": self._determine_metamodel_relevance(chunk, extraction),
                "compliance_relevance": self._calculate_compliance_relevance(chunk, extraction),
                "implementation_complexity": self._assess_implementation_complexity(extraction),
                
                "metadata": chunk["metadata"],
                "timestamp": datetime.now()
            }
            
            try:
                self.client.index(index=self.index_name, id=chunk["chunk_id"], document=doc)
            except Exception as e:
                logger.error(f"Failed to index chunk {chunk['chunk_id']}: {e}")
        
        self.client.indices.refresh(index=self.index_name)
        logger.info(f"Successfully indexed {len(chunks)} regulatory chunks")
    
    def _determine_metamodel_relevance(self, chunk: Dict, extraction: Dict) -> str:
        """Determine the primary metamodel entity this chunk relates to"""
        if extraction.get("processing_activities"):
            return "processing_activity"
        elif extraction.get("data_categories"):
            return "data_category"
        elif extraction.get("affected_jurisdictions_discovered"):
            return "territorial_scope"
        elif extraction.get("cross_border_transfers"):
            return "cross_border_transfer"
        elif extraction.get("regulatory_entities"):
            return "regulatory_entity"
        elif extraction.get("extraterritorial_applicability"):
            return "extraterritorial_compliance"
        else:
            return "general_compliance"
    
    def _calculate_compliance_relevance(self, chunk: Dict, extraction: Dict) -> float:
        """Calculate compliance relevance score"""
        score = 0.0
        
        # Base regulatory density
        score += chunk.get("metadata", {}).get("regulatory_density", 0.0) * 0.3
        
        # Extraction richness
        extraction_count = sum(len(v) if isinstance(v, list) else 1 for v in extraction.values())
        score += min(extraction_count / 10.0, 0.4)
        
        # Regulatory pattern matches
        pattern_count = sum(len(v) if isinstance(v, list) else 1 for v in chunk.get("regulatory_patterns", {}).values())
        score += min(pattern_count / 5.0, 0.3)
        
        return min(score, 1.0)
    
    def _assess_implementation_complexity(self, extraction: Dict) -> str:
        """Assess implementation complexity based on territorial extractions"""
        complexity_indicators = 0
        
        # Multiple affected jurisdictions
        affected_jurisdictions = extraction.get("affected_jurisdictions_discovered", [])
        complexity_indicators += len(affected_jurisdictions)
        
        # Cross-border transfers
        transfers = extraction.get("cross_border_transfers", [])
        complexity_indicators += len(transfers) * 2  # Transfers are more complex
        
        # Extraterritorial applicability scenarios
        extraterritorial = extraction.get("extraterritorial_applicability", [])
        complexity_indicators += len(extraterritorial) * 1.5
        
        # Processing activities
        complexity_indicators += len(extraction.get("processing_activities", []))
        
        # Adequacy decision considerations
        adequacy_decisions = extraction.get("adequacy_decisions_mentioned", [])
        complexity_indicators += len(adequacy_decisions)
        
        if complexity_indicators >= 15:
            return "very_high"
        elif complexity_indicators >= 10:
            return "high"
        elif complexity_indicators >= 5:
            return "medium"
        else:
            return "low"
    
    def enhanced_regulatory_search(self, query: str, jurisdiction: str = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Enhanced search for territorial and regulatory concepts"""
        query_embedding = self.embeddings.embed_query(query)
        
        # Build complex search query with territorial awareness
        must_clauses = [
            {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            }
        ]
        
        should_clauses = [
            {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "text^1.0",
                        "processing_activities.activity_name^2.0",
                        "data_categories.category^1.8",
                        "regulatory_entities.name^1.5",
                        "affected_jurisdictions_discovered.jurisdiction^2.0",
                        "cross_border_transfers.origin_territory^1.5",
                        "cross_border_transfers.destination_territory^1.5",
                        "territorial_scope.eu_member_states_affected^1.8",
                        "extraterritorial_applicability.affected_jurisdiction^2.0"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            }
        ]
        
        # Add territorial scope filter if specified
        filter_clauses = []
        if jurisdiction:
            filter_clauses.append({
                "bool": {
                    "should": [
                        {
                            "nested": {
                                "path": "affected_jurisdictions_discovered",
                                "query": {"term": {"affected_jurisdictions_discovered.jurisdiction": jurisdiction}}
                            }
                        },
                        {
                            "nested": {
                                "path": "territorial_scope",
                                "query": {"terms": {"territorial_scope.eu_member_states_affected": [jurisdiction]}}
                            }
                        }
                    ]
                }
            })
        
        search_body = {
            "query": {
                "bool": {
                    "must": must_clauses,
                    "should": should_clauses,
                    "filter": filter_clauses
                }
            },
            "size": top_k,
            "_source": {
                "excludes": ["embedding"]
            }
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_regulatory_results(response)
        except Exception as e:
            logger.error(f"Enhanced territorial regulatory search failed: {e}")
            return []
    
    def _format_regulatory_results(self, response: Dict) -> List[Dict[str, Any]]:
        """Format enhanced territorial regulatory search results"""
        results = []
        
        try:
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "text": source["text"],
                    "chunk_id": source["chunk_id"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "document_type": source.get("document_type", "general"),
                    "metamodel_entity": source.get("metamodel_entity", "general"),
                    "compliance_relevance": source.get("compliance_relevance", 0.0),
                    "implementation_complexity": source.get("implementation_complexity", "unknown"),
                    "processing_activities": source.get("processing_activities", []),
                    "data_categories": source.get("data_categories", []),
                    "regulatory_entities": source.get("regulatory_entities", []),
                    "cross_border_transfers": source.get("cross_border_transfers", []),
                    "affected_jurisdictions_discovered": source.get("affected_jurisdictions_discovered", []),
                    "territorial_scope": source.get("territorial_scope", {}),
                    "extraterritorial_applicability": source.get("extraterritorial_applicability", []),
                    "adequacy_decisions_mentioned": source.get("adequacy_decisions_mentioned", [])
                }
                results.append(result)
        except Exception as e:
            logger.error(f"Error formatting territorial regulatory search results: {e}")
        
        return results

class RegulatoryGraphEngine:
    """Enhanced graph engine for regulatory knowledge representation"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, password: str = None):
        try:
            if password:
                self.db = FalkorDB(host=host, port=port, password=password)
            else:
                self.db = FalkorDB(host=host, port=port)
            
            self.graph = self.db.select_graph("gdpr_ropa_metamodel_graph")
            logger.info(f"Connected to FalkorDB for regulatory graph at {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def build_regulatory_knowledge_graph(self, chunks: List[Dict[str, Any]], extractions: List[Dict[str, Any]]):
        """Build comprehensive regulatory knowledge graph"""
        logger.info("Building comprehensive regulatory knowledge graph...")
        
        try:
            # Clear existing graph
            self.graph.query("MATCH (n) DETACH DELETE n")
            
            # Create jurisdiction nodes
            for jurisdiction, requirements in JURISDICTION_REQUIREMENTS.items():
                jurisdiction_query = f"""
                CREATE (j:Jurisdiction {{
                    id: '{jurisdiction.value}',
                    name: '{requirements.name}',
                    penalties: '{requirements.penalties}',
                    principles: {json.dumps(requirements.key_principles)},
                    ropa_requirements: {json.dumps(requirements.ropa_requirements)},
                    financial_specifics: {json.dumps(requirements.financial_specifics)},
                    data_subject_rights: {json.dumps(requirements.data_subject_rights)}
                }})
                """
                self.graph.query(jurisdiction_query)
            
            # Process each document chunk
            for i, chunk in enumerate(chunks):
                extraction = extractions[i] if i < len(extractions) else {}
                
                # Create document node
                doc_query = f"""
                CREATE (d:Document {{
                    id: '{chunk['chunk_id']}',
                    source: '{chunk['source']}',
                    document_type: '{chunk.get('document_type', 'general')}',
                    regulatory_density: {chunk.get('metadata', {}).get('regulatory_density', 0.0)}
                }})
                """
                self.graph.query(doc_query)
                
                # Create processing activity nodes
                for activity in extraction.get("processing_activities", []):
                    activity_name = activity.get("activity_name", "").replace("'", "\\'")
                    purpose = activity.get("purpose", "").replace("'", "\\'")
                    legal_basis = activity.get("legal_basis", "").replace("'", "\\'")
                    
                    if activity_name:
                        activity_query = f"""
                        MERGE (pa:ProcessingActivity {{
                            name: '{activity_name}',
                            purpose: '{purpose}',
                            legal_basis: '{legal_basis}',
                            data_categories: {json.dumps(activity.get('data_categories', []))},
                            data_subjects: {json.dumps(activity.get('data_subjects', []))},
                            retention: '{activity.get('retention', '')}',
                            jurisdiction_applicability: {json.dumps(activity.get('jurisdiction_applicability', []))}
                        }})
                        """
                        self.graph.query(activity_query)
                        
                        # Link to document
                        link_query = f"""
                        MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                        MATCH (pa:ProcessingActivity {{name: '{activity_name}'}})
                        MERGE (d)-[:DESCRIBES]->(pa)
                        """
                        self.graph.query(link_query)
                        
                        # Link to applicable jurisdictions
                        for jurisdiction in activity.get('jurisdiction_applicability', []):
                            jurisdiction = jurisdiction.replace("'", "\\'")
                            jurisdiction_link = f"""
                            MATCH (pa:ProcessingActivity {{name: '{activity_name}'}})
                            MATCH (j:Jurisdiction {{id: '{jurisdiction}'}})
                            MERGE (pa)-[:SUBJECT_TO]->(j)
                            """
                            self.graph.query(jurisdiction_link)
                
                # Create data category nodes
                for category in extraction.get("data_categories", []):
                    category_name = category.get("category", "").replace("'", "\\'")
                    sensitivity = category.get("sensitivity", "normal")
                    financial_specific = category.get("financial_specific", False)
                    
                    if category_name:
                        category_query = f"""
                        MERGE (dc:DataCategory {{
                            name: '{category_name}',
                            sensitivity: '{sensitivity}',
                            financial_specific: {str(financial_specific).lower()},
                            examples: {json.dumps(category.get('examples', []))},
                            protection_requirements: {json.dumps(category.get('protection_requirements', []))}
                        }})
                        """
                        self.graph.query(category_query)
                        
                        # Link to document
                        link_query = f"""
                        MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                        MATCH (dc:DataCategory {{name: '{category_name}'}})
                        MERGE (d)-[:DEFINES]->(dc)
                        """
                        self.graph.query(link_query)
                
                # Create legal basis nodes
                for basis in extraction.get("legal_bases", []):
                    basis_name = basis.get("basis", "").replace("'", "\\'")
                    description = basis.get("description", "").replace("'", "\\'")
                    jurisdiction = basis.get("jurisdiction", "").replace("'", "\\'")
                    
                    if basis_name:
                        basis_query = f"""
                        MERGE (lb:LegalBasis {{
                            name: '{basis_name}',
                            description: '{description}',
                            jurisdiction: '{jurisdiction}',
                            requirements: {json.dumps(basis.get('requirements', []))}
                        }})
                        """
                        self.graph.query(basis_query)
                        
                        # Link to jurisdiction
                        if jurisdiction:
                            jurisdiction_link = f"""
                            MATCH (lb:LegalBasis {{name: '{basis_name}'}})
                            MATCH (j:Jurisdiction {{id: '{jurisdiction}'}})
                            MERGE (lb)-[:APPLIES_IN]->(j)
                            """
                            self.graph.query(jurisdiction_link)
                
                # Create regulatory entity nodes
                for entity in extraction.get("regulatory_entities", []):
                    entity_name = entity.get("name", "").replace("'", "\\'")
                    entity_type = entity.get("type", "").replace("'", "\\'")
                    role = entity.get("role", "").replace("'", "\\'")
                    jurisdiction = entity.get("jurisdiction", "").replace("'", "\\'")
                    
                    if entity_name:
                        entity_query = f"""
                        MERGE (re:RegulatoryEntity {{
                            name: '{entity_name}',
                            type: '{entity_type}',
                            role: '{role}',
                            jurisdiction: '{jurisdiction}'
                        }})
                        """
                        self.graph.query(entity_query)
                
                # Create security measure nodes
                for measure in extraction.get("security_measures", []):
                    measure_name = measure.get("measure", "").replace("'", "\\'")
                    measure_type = measure.get("type", "").replace("'", "\\'")
                    description = measure.get("description", "").replace("'", "\\'")
                    
                    if measure_name:
                        measure_query = f"""
                        MERGE (sm:SecurityMeasure {{
                            name: '{measure_name}',
                            type: '{measure_type}',
                            description: '{description}',
                            mandatory_jurisdictions: {json.dumps(measure.get('mandatory_jurisdictions', []))}
                        }})
                        """
                        self.graph.query(measure_query)
            
            # Create cross-references and relationships
            self._create_cross_references()
            
            logger.info("Regulatory knowledge graph built successfully")
        
        except Exception as e:
            logger.error(f"Failed to build regulatory knowledge graph: {e}")
            raise
    
    def _create_cross_references(self):
        """Create complex cross-references between regulatory concepts"""
        
        # Link processing activities to data categories
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (dc:DataCategory)
        WHERE ANY(category IN pa.data_categories WHERE category = dc.name)
        MERGE (pa)-[:PROCESSES]->(dc)
        """)
        
        # Link processing activities to legal bases
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (lb:LegalBasis)
        WHERE pa.legal_basis = lb.name
        MERGE (pa)-[:BASED_ON]->(lb)
        """)
        
        # Create compliance relationships
        self.graph.query("""
        MATCH (j:Jurisdiction), (pa:ProcessingActivity)
        WHERE ANY(jurisdiction IN pa.jurisdiction_applicability WHERE jurisdiction = j.id)
        MERGE (pa)-[:MUST_COMPLY_WITH]->(j)
        """)
    
    def enhanced_regulatory_graph_search(self, query: str, jurisdiction: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Enhanced graph search for regulatory concepts"""
        results = []
        
        query_terms = self._extract_regulatory_terms(query)
        
        for term in query_terms[:3]:
            term = term.replace("'", "\\'")
            
            # Multi-level graph traversal
            graph_query = f"""
            MATCH path = (start)-[*1..3]-(related)
            WHERE (
                (start:ProcessingActivity AND toLower(start.name) CONTAINS '{term}') OR
                (start:DataCategory AND toLower(start.name) CONTAINS '{term}') OR
                (start:LegalBasis AND toLower(start.name) CONTAINS '{term}') OR
                (start:RegulatoryEntity AND toLower(start.name) CONTAINS '{term}') OR
                (start:SecurityMeasure AND toLower(start.name) CONTAINS '{term}')
            )
            RETURN DISTINCT start, related, relationships(path), length(path) as distance
            ORDER BY distance
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(graph_query)
                
                for record in result.result_set:
                    graph_result = {
                        "start_node": self._format_node(record[0]),
                        "related_node": self._format_node(record[1]),
                        "relationships": [str(rel) for rel in record[2]],
                        "distance": record[3],
                        "search_term": term,
                        "search_type": "enhanced_regulatory_graph"
                    }
                    results.append(graph_result)
            
            except Exception as e:
                logger.warning(f"Enhanced graph query failed for '{term}': {e}")
        
        return results[:top_k]
    
    def _extract_regulatory_terms(self, query: str) -> List[str]:
        """Extract regulatory terms from query"""
        words = re.findall(r'\b\w+\b', query.lower())
        regulatory_stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return [word for word in words if len(word) > 2 and word not in regulatory_stopwords]
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if hasattr(node, 'properties'):
            return dict(node.properties)
        return {"id": str(node)}

class GlobalRopaMetamodelSystem:
    """Complete Global GDPR RoPA Metamodel System with Territorial Scope Discovery"""
    
    def __init__(self,
                 openai_api_key: str,
                 openai_base_url: str = None,
                 elasticsearch_host: str = "http://localhost:9200",
                 elasticsearch_username: str = None,
                 elasticsearch_password: str = None,
                 falkordb_host: str = "localhost",
                 falkordb_port: int = 6379,
                 falkordb_password: str = None):
        
        os.environ["OPENAI_API_KEY"] = openai_api_key
        if openai_base_url:
            os.environ["OPENAI_BASE_URL"] = openai_base_url
        
        # Initialize components
        self.processor = EnhancedRegulatoryProcessor()
        
        self.vector_engine = EnhancedRegulatoryVectorEngine(
            host=elasticsearch_host,
            username=elasticsearch_username,
            password=elasticsearch_password,
            openai_api_key=openai_api_key,
            openai_base_url=openai_base_url
        )
        
        self.graph_engine = RegulatoryGraphEngine(
            host=falkordb_host,
            port=falkordb_port,
            password=falkordb_password
        )
        
        # State for iterative analysis
        self.analysis_state = {
            "processed_documents": [],
            "extracted_concepts": [],
            "metamodel_iterations": [],
            "compliance_gaps": [],
            "final_metamodel": None
        }
        
        logger.info("Global RoPA Metamodel System initialized for territorial scope discovery")
        if openai_base_url:
            logger.info(f"Using custom OpenAI endpoint: {openai_base_url}")
    
    def ingest_regulatory_documents(self, document_paths: List[str]) -> Dict[str, Any]:
        """Ingest and process regulatory documents"""
        logger.info(f"Ingesting {len(document_paths)} regulatory documents")
        
        all_chunks = []
        all_extractions = []
        
        for doc_path in document_paths:
            try:
                # Process document
                chunks = self.processor.extract_pdf_content(doc_path)
                self.analysis_state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "timestamp": datetime.now()
                })
                
                # Extract regulatory concepts
                extractions = []
                for chunk in chunks:
                    extraction = enhanced_regulatory_extraction_agent.invoke(chunk["text"])
                    extractions.append(extraction)
                    self.analysis_state["extracted_concepts"].extend(extraction.get("concept_synonyms", []))
                
                all_chunks.extend(chunks)
                all_extractions.extend(extractions)
                
                logger.info(f"Processed {doc_path}: {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Index in vector store
        self.vector_engine.index_regulatory_documents(all_chunks, all_extractions)
        
        # Build knowledge graph
        self.graph_engine.build_regulatory_knowledge_graph(all_chunks, all_extractions)
        
        # Calculate statistics
        total_processing_activities = sum(len(ext.get("processing_activities", [])) for ext in all_extractions)
        total_data_categories = sum(len(ext.get("data_categories", [])) for ext in all_extractions)
        total_affected_jurisdictions = len(set(
            jurisdiction.get("jurisdiction", "") 
            for ext in all_extractions 
            for jurisdiction in ext.get("affected_jurisdictions_discovered", [])
        ))
        total_transfer_mechanisms = len(set(
            transfer.get("transfer_mechanism", "")
            for ext in all_extractions
            for transfer in ext.get("cross_border_transfers", [])
        ))
        
        return {
            "status": "success",
            "documents_processed": len(document_paths),
            "total_chunks": len(all_chunks),
            "processing_activities_discovered": total_processing_activities,
            "data_categories_discovered": total_data_categories,
            "affected_jurisdictions_discovered": total_affected_jurisdictions,
            "transfer_mechanisms_identified": total_transfer_mechanisms,
            "territorial_scope_analysis": "GDPR and UK GDPR with global applicability discovery",
            "regulatory_concepts": len(self.analysis_state["extracted_concepts"]),
            "timestamp": datetime.now()
        }
    
    def iterative_metamodel_analysis(self, iterations: int = 3) -> Dict[str, Any]:
        """Perform iterative analysis to refine the metamodel"""
        logger.info(f"Starting iterative metamodel analysis with {iterations} iterations")
        
        for iteration in range(iterations):
            logger.info(f"Metamodel iteration {iteration + 1}/{iterations}")
            
            # Query the system for comprehensive understanding
            analysis_queries = [
                "What is the territorial scope of GDPR and how does it apply to organizations worldwide?",
                "How does GDPR apply extraterritorially to organizations in India, China, Australia, and other countries?",
                "What are the adequacy decisions under GDPR and how do they affect different countries?",
                "How does UK GDPR territorial scope differ from EU GDPR post-Brexit?",
                "What cross-border transfer mechanisms apply between EU, UK, and third countries?",
                "How do financial institutions with global operations ensure GDPR compliance across all territories?",
                "What are the specific compliance requirements for EU member states under GDPR?",
                "How does GDPR targeting provision apply to organizations offering services globally?",
                "What territorial considerations apply to processing activities in different jurisdictions?",
                "How do supervisory authorities across different territories coordinate GDPR enforcement?"
            ]
            
            iteration_insights = []
            
            for query in analysis_queries:
                # Search vector store
                vector_results = self.vector_engine.enhanced_regulatory_search(query, top_k=5)
                
                # Search knowledge graph
                graph_results = self.graph_engine.enhanced_regulatory_graph_search(query, top_k=3)
                
                # Analyze results
                combined_data = {
                    "query": query,
                    "vector_results": vector_results,
                    "graph_results": graph_results
                }
                
                metamodel_analysis = metamodel_analysis_agent.invoke(json.dumps(combined_data))
                iteration_insights.append(metamodel_analysis)
            
            # Store iteration results
            self.analysis_state["metamodel_iterations"].append({
                "iteration": iteration + 1,
                "insights": iteration_insights,
                "timestamp": datetime.now()
            })
        
        # Perform territorial compliance gap analysis
        all_territorial_data = {
            "iterations": self.analysis_state["metamodel_iterations"],
            "extracted_concepts": self.analysis_state["extracted_concepts"],
            "potential_jurisdictions": POTENTIAL_AFFECTED_JURISDICTIONS
        }
        
        gdpr_applicability_framework = {
            scope.value: framework.__dict__ 
            for scope, framework in GDPR_APPLICABILITY_FRAMEWORK.items()
        }
        
        territorial_analysis = territorial_compliance_analysis_agent.invoke(
            json.dumps(all_territorial_data),
            json.dumps(gdpr_applicability_framework)
        )
        
        self.analysis_state["compliance_gaps"] = territorial_analysis
        
        return {
            "iterations_completed": iterations,
            "insights_generated": len(iteration_insights) * iterations,
            "territorial_gaps_identified": len(territorial_analysis.get("territorial_coverage_gaps", [])),
            "jurisdictions_analyzed": len(territorial_analysis.get("jurisdiction_specific_requirements", [])),
            "extraterritorial_scenarios": len(territorial_analysis.get("extraterritorial_scenarios", [])),
            "compliance_priorities": len(territorial_analysis.get("compliance_priorities", [])),
            "status": "territorial_analysis_complete"
        }
    
    def generate_final_metamodel(self) -> Dict[str, Any]:
        """Generate the final comprehensive metamodel"""
        logger.info("Generating final comprehensive RoPA metamodel")
        
        # Consolidate all analysis
        consolidated_analysis = {
            "processed_documents": self.analysis_state["processed_documents"],
            "extracted_concepts": self.analysis_state["extracted_concepts"],
            "metamodel_iterations": self.analysis_state["metamodel_iterations"],
            "territorial_compliance_gaps": self.analysis_state["compliance_gaps"],
            "gdpr_applicability_framework": {
                scope.value: framework.__dict__ 
                for scope, framework in GDPR_APPLICABILITY_FRAMEWORK.items()
            },
            "potential_affected_jurisdictions": POTENTIAL_AFFECTED_JURISDICTIONS
        }
        
        # Generate final metamodel structure
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        prompt = f"""Based on comprehensive territorial scope analysis and iterative refinement, create the final GDPR and UK GDPR Record of Processing Activities (RoPA) metamodel with global territorial applicability discovery.

Consolidated Analysis: {json.dumps(consolidated_analysis, default=str)}

Create a comprehensive metamodel that includes:

1. CORE METAMODEL STRUCTURE:
   - Abstract base classes for territorial scope
   - Concrete implementation classes for jurisdictional applicability
   - Relationship definitions across territories
   - Constraint specifications for cross-border compliance
   - Validation rules for territorial compliance

2. TERRITORIAL SCOPE FRAMEWORK:
   - EU GDPR territorial provisions (Article 3)
   - UK GDPR territorial provisions
   - Extraterritorial application triggers
   - Cross-border processing contexts
   - Adequacy decision integration

3. JURISDICTION DISCOVERY MECHANISM:
   - Dynamic discovery of affected jurisdictions
   - Applicability assessment for any jurisdiction
   - Transfer mechanism determination
   - Compliance requirement mapping
   - Risk assessment by territory

4. FINANCIAL SECTOR TERRITORIAL SPECIALIZATIONS:
   - Global banking operations territorial compliance
   - Cross-border financial services
   - International payment processing
   - Multi-jurisdictional regulatory reporting
   - Global customer data management

5. EXTRATERRITORIAL COMPLIANCE FRAMEWORK:
   - Targeting provisions for global organizations
   - Monitoring behavior across territories
   - Service provision territorial scope
   - Controller/processor establishment rules
   - Third country compliance obligations

Return the complete territorial-aware metamodel specification:

{{
    "territorial_metamodel_specification": {{
        "version": "1.0.0",
        "scope": "GDPR and UK GDPR Global Territorial Applicability",
        "territorial_core_entities": [
            {{
                "entity": "TerritorialProcessingActivity",
                "type": "core",
                "attributes": ["id", "name", "territorial_scope", "extraterritorial_triggers", "affected_jurisdictions", "transfer_mechanisms", "compliance_requirements"],
                "territorial_relationships": [
                    {{"target": "Jurisdiction", "type": "applies_in", "cardinality": "many-to-many"}},
                    {{"target": "TransferMechanism", "type": "uses", "cardinality": "many-to-many"}},
                    {{"target": "ComplianceRequirement", "type": "must_meet", "cardinality": "many-to-many"}}
                ],
                "territorial_constraints": ["territorial_scope_validation", "extraterritorial_trigger_assessment", "transfer_mechanism_requirement"],
                "compliance_validation": ["gdpr_applicability_check", "uk_gdpr_applicability_check", "adequacy_validation"]
            }}
        ],
        "jurisdiction_discovery_framework": [
            {{
                "discovery_type": "gdpr_extraterritorial_applicability",
                "trigger_conditions": ["offering_goods_services", "monitoring_behavior", "targeting_eu_market"],
                "assessment_criteria": ["data_subject_location", "service_targeting", "behavioral_monitoring"],
                "compliance_implications": ["full_gdpr_compliance", "representative_requirement", "supervisory_authority_cooperation"]
            }},
            {{
                "discovery_type": "uk_gdpr_extraterritorial_applicability", 
                "trigger_conditions": ["offering_goods_services_uk", "monitoring_behavior_uk", "targeting_uk_market"],
                "assessment_criteria": ["uk_data_subject_location", "uk_service_targeting", "uk_behavioral_monitoring"],
                "compliance_implications": ["full_uk_gdpr_compliance", "uk_representative_requirement", "ico_cooperation"]
            }}
        ],
        "adequacy_decision_framework": [
            {{
                "framework_type": "eu_adequacy_decisions",
                "current_adequate_countries": ["Andorra", "Argentina", "Canada_commercial", "Faroe_Islands", "Guernsey", "Israel", "Isle_of_Man", "Japan", "Jersey", "New_Zealand", "Republic_of_Korea", "Switzerland", "United_Kingdom", "Uruguay"],
                "transfer_implications": ["no_additional_safeguards_required", "standard_transfer_permitted"],
                "ongoing_monitoring": ["adequacy_decision_review", "suspension_risk_assessment"]
            }},
            {{
                "framework_type": "uk_adequacy_recognitions",
                "recognized_territories": ["eu_member_states", "eea_countries", "gibraltar", "countries_with_eu_adequacy"],
                "transfer_implications": ["simplified_transfer_process", "bridging_arrangements"],
                "post_brexit_considerations": ["transition_arrangements", "future_adequacy_assessments"]
            }}
        ],
        "transfer_mechanism_matrix": [
            {{
                "origin_territory": "eu_member_state",
                "destination_categories": ["adequate_third_country", "inadequate_third_country", "uk_post_brexit"],
                "available_mechanisms": ["adequacy_decision", "standard_contractual_clauses", "binding_corporate_rules", "certification", "derogations"],
                "additional_requirements": ["transfer_impact_assessment", "supplementary_measures", "ongoing_monitoring"]
            }}
        ],
        "financial_sector_territorial_specializations": [
            {{
                "sector": "global_banking",
                "territorial_challenges": ["multi_jurisdictional_operations", "cross_border_payments", "correspondent_banking"],
                "compliance_requirements": ["local_representative_requirements", "supervisory_cooperation", "breach_notification_coordination"],
                "data_categories": ["customer_data_cross_border", "transaction_data_international", "regulatory_reporting_multi_jurisdiction"]
            }}
        ]
    }},
    "territorial_implementation_guidance": {{
        "jurisdiction_assessment_process": ["identify_data_subjects", "assess_targeting", "evaluate_monitoring", "determine_establishment", "map_compliance_requirements"],
        "transfer_mechanism_selection": ["assess_destination_adequacy", "evaluate_transfer_frequency", "determine_safeguards", "implement_additional_measures"],
        "compliance_monitoring": ["territorial_scope_review", "adequacy_decision_tracking", "transfer_mechanism_validation", "cross_border_incident_response"],
        "risk_assessment_framework": ["territorial_compliance_risk", "transfer_mechanism_risk", "supervisory_coordination_risk", "enforcement_risk"]
    }},
    "dynamic_compliance_validation": {{
        "territorial_scope_checks": ["gdpr_article_3_assessment", "uk_gdpr_territorial_provision_check", "extraterritorial_trigger_validation"],
        "transfer_compliance_validation": ["adequacy_status_verification", "transfer_mechanism_compliance", "supplementary_measures_assessment"],
        "ongoing_monitoring_requirements": ["adequacy_decision_changes", "supervisory_guidance_updates", "enforcement_action_tracking"]
    }}
}}
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            content = response.content
            
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = content[json_start:json_end]
                final_metamodel = json.loads(json_str)
                
                self.analysis_state["final_metamodel"] = final_metamodel
                
                return final_metamodel
            
        except Exception as e:
            logger.error(f"Failed to generate final metamodel: {e}")
            return {"error": "Failed to generate metamodel"}
    
    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive analysis and metamodel report"""
        logger.info("Generating comprehensive RoPA metamodel report")
        
        if not self.analysis_state["final_metamodel"]:
            self.generate_final_metamodel()
        
        # Generate detailed report using all collected data
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        report_data = {
            "analysis_state": self.analysis_state,
            "gdpr_applicability_framework": {
                scope.value: framework.__dict__ 
                for scope, framework in GDPR_APPLICABILITY_FRAMEWORK.items()
            },
            "potential_affected_jurisdictions": POTENTIAL_AFFECTED_JURISDICTIONS,
            "timestamp": datetime.now().isoformat()
        }
        
        prompt = f"""Generate a comprehensive executive report on the Global GDPR Record of Processing Activities (RoPA) Metamodel for Financial Institutions.

Analysis Data: {json.dumps(report_data, default=str)}

Create a detailed report with the following structure:

# Executive Summary
- Project overview and objectives
- Key findings and recommendations
- Critical compliance gaps identified
- Implementation timeline and priorities

# Methodology
- Document analysis approach
- AI-enhanced extraction techniques
- Iterative refinement process
- Validation procedures

# Global Regulatory Landscape Analysis
- GDPR (EU) requirements and implications
- CCPA (California) specific considerations
- PIPEDA (Canada) alignment requirements
- LGPD (Brazil) compliance factors
- Other jurisdictional requirements
- Cross-border data transfer implications

# Financial Sector Specific Requirements
- Banking industry processing activities
- Insurance data handling requirements
- Investment management compliance
- Payment processing obligations
- Credit scoring and risk assessment
- Anti-money laundering (AML) considerations
- Know Your Customer (KYC) requirements

# Metamodel Architecture
- Core entity definitions
- Relationship mappings
- Jurisdictional extension patterns
- Financial sector specializations
- Implementation framework
- Validation and compliance mechanisms

# Implementation Roadmap
- Phase 1: Foundation setup
- Phase 2: Core implementation
- Phase 3: Jurisdictional extensions
- Phase 4: Financial specializations
- Phase 5: Advanced features and optimization

# Risk Assessment and Mitigation
- Compliance risks identified
- Implementation challenges
- Mitigation strategies
- Contingency planning

# Technology Architecture
- System integration requirements
- Data storage and processing
- Security and privacy safeguards
- Scalability considerations
- Performance optimization

# Governance and Maintenance
- Ongoing governance structure
- Regular review processes
- Update and maintenance procedures
- Training and change management

# Conclusion and Next Steps
- Key deliverables
- Success metrics
- Future enhancements
- Support and maintenance plan

Provide a comprehensive, professional report that can be used by executives, compliance officers, and technical teams to understand and implement the global RoPA metamodel.
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            return response.content
        
        except Exception as e:
            logger.error(f"Failed to generate comprehensive report: {e}")
            return f"Error generating report: {e}"
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        try:
            # Elasticsearch stats
            es_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.index_name)
            doc_count = es_stats.body["indices"][self.vector_engine.index_name]["total"]["docs"]["count"]
            
            # Graph stats
            node_stats = self.graph_engine.graph.query("MATCH (n) RETURN count(n) as nodes")
            node_count = node_stats.result_set[0][0] if node_stats.result_set else 0
            
            rel_stats = self.graph_engine.graph.query("MATCH ()-[r]->() RETURN count(r) as rels")
            rel_count = rel_stats.result_set[0][0] if rel_stats.result_set else 0
            
            # Processing activity stats
            pa_stats = self.graph_engine.graph.query("MATCH (pa:ProcessingActivity) RETURN count(pa) as activities")
            pa_count = pa_stats.result_set[0][0] if pa_stats.result_set else 0
            
            # Data category stats
            dc_stats = self.graph_engine.graph.query("MATCH (dc:DataCategory) RETURN count(dc) as categories")
            dc_count = dc_stats.result_set[0][0] if dc_stats.result_set else 0
            
            # Territorial scope stats
            territorial_stats = self.graph_engine.graph.query("MATCH (ts:TerritorialScope) RETURN count(ts) as territorial_scopes")
            territorial_count = territorial_stats.result_set[0][0] if territorial_stats.result_set else 0
            
            # Affected jurisdiction stats
            jurisdiction_stats = self.graph_engine.graph.query("MATCH (aj:AffectedJurisdiction) RETURN count(aj) as affected_jurisdictions")
            affected_jurisdiction_count = jurisdiction_stats.result_set[0][0] if jurisdiction_stats.result_set else 0
            
            # Cross-border transfer stats
            transfer_stats = self.graph_engine.graph.query("MATCH (t:CrossBorderTransfer) RETURN count(t) as transfers")
            transfer_count = transfer_stats.result_set[0][0] if transfer_stats.result_set else 0
            
            return {
                "elasticsearch_documents": doc_count,
                "graph_nodes": node_count,
                "graph_relationships": rel_count,
                "processing_activities": pa_count,
                "data_categories": dc_count,
                "territorial_scopes": territorial_count,
                "affected_jurisdictions": affected_jurisdiction_count,
                "cross_border_transfers": transfer_count,
                "processed_documents": len(self.analysis_state["processed_documents"]),
                "extracted_concepts": len(self.analysis_state["extracted_concepts"]),
                "metamodel_iterations": len(self.analysis_state["metamodel_iterations"]),
                "territorial_compliance_gaps": len(self.analysis_state.get("compliance_gaps", {}).get("territorial_coverage_gaps", [])),
                "has_final_metamodel": self.analysis_state["final_metamodel"] is not None,
                "system_status": "operational"
            }
        except Exception as e:
            return {"error": str(e), "system_status": "error"}

def main():
    """Main execution function for the Global RoPA Metamodel System"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Global GDPR RoPA Metamodel System for Financial Institutions")
    parser.add_argument("--ingest", nargs="+", help="Paths to regulatory documents to ingest")
    parser.add_argument("--analyze", action="store_true", help="Perform iterative metamodel analysis")
    parser.add_argument("--generate-metamodel", action="store_true", help="Generate final metamodel")
    parser.add_argument("--generate-report", action="store_true", help="Generate comprehensive report")
    parser.add_argument("--stats", action="store_true", help="Show system statistics")
    parser.add_argument("--iterations", type=int, default=3, help="Number of analysis iterations")
    
    args = parser.parse_args()
    
    # Load configuration
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "openai_base_url": os.getenv("OPENAI_BASE_URL"),
        "elasticsearch_host": os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        "elasticsearch_username": os.getenv("ELASTICSEARCH_USERNAME"),
        "elasticsearch_password": os.getenv("ELASTICSEARCH_PASSWORD"),
        "falkordb_host": os.getenv("FALKORDB_HOST", "localhost"),
        "falkordb_port": int(os.getenv("FALKORDB_PORT", 6379)),
        "falkordb_password": os.getenv("FALKORDB_PASSWORD")
    }
    
    # Validate configuration
    if not config["openai_api_key"]:
        print("❌ Error: OPENAI_API_KEY environment variable is required")
        return
    
    if not config["elasticsearch_host"].startswith(('http://', 'https://')):
        print("❌ Error: ELASTICSEARCH_HOST must include schema (http:// or https://)")
        return
    
    print("🔧 Global RoPA Metamodel System Configuration:")
    print(f"  OpenAI Endpoint: {config['openai_base_url'] or 'Standard OpenAI API'}")
    print(f"  Elasticsearch: {config['elasticsearch_host']}")
    print(f"  FalkorDB: {config['falkordb_host']}:{config['falkordb_port']}")
    print("  Enhanced for: Financial Institutions, Global Compliance")
    
    try:
        # Initialize system
        system = GlobalRopaMetamodelSystem(
            openai_api_key=config["openai_api_key"],
            openai_base_url=config["openai_base_url"],
            elasticsearch_host=config["elasticsearch_host"],
            elasticsearch_username=config["elasticsearch_username"],
            elasticsearch_password=config["elasticsearch_password"],
            falkordb_host=config["falkordb_host"],
            falkordb_port=config["falkordb_port"],
            falkordb_password=config["falkordb_password"]
        )
        
        print("✅ Global RoPA Metamodel System initialized successfully")
        
        # Execute requested operations
        if args.ingest:
            print(f"\n📄 Ingesting {len(args.ingest)} regulatory documents...")
            result = system.ingest_regulatory_documents(args.ingest)
            print(f"✅ Ingestion completed: {result}")
        
        if args.analyze:
            print(f"\n🔍 Performing iterative metamodel analysis ({args.iterations} iterations)...")
            result = system.iterat
