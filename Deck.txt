#!/usr/bin/env python3
"""
General-Purpose Legal Knowledge Graph Reasoning System
Domain-agnostic, learns from input TTL, preserves all original information
"""

import os
import json
import asyncio
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging
from pathlib import Path
import hashlib
import uuid
import re
from collections import defaultdict, Counter

# Graph analysis and RDF processing
import networkx as nx
from rdflib import Graph, Namespace, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, XSD
from rdflib.plugins.sparql import prepareQuery
import rdflib.plugins.sparql as sparql

# Pydantic v2 with enhanced validation
from pydantic import BaseModel, Field, ValidationError, field_validator
from typing_extensions import Annotated

# LangGraph with advanced patterns
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReasoningType(Enum):
    """Types of reasoning patterns available"""
    CHAIN_OF_THOUGHT = "cot"
    TREE_OF_THOUGHTS = "tot"
    REFLEXION = "reflexion"
    LATS = "lats"
    REACT = "react"
    TEMPORAL = "temporal"
    FORMAL_VERIFICATION = "formal_verification"
    PATTERN_DISCOVERY = "pattern_discovery"

class ConfidenceLevel(Enum):
    """Confidence levels for reasoning outputs"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    UNCERTAIN = "uncertain"

# ===============================
# 1. CONTENT-PRESERVING TTL ANALYZER
# ===============================

@dataclass
class TTLContent:
    """Complete preservation of original TTL content with enhancements"""
    original_ttl: str
    enhanced_ttl: str
    preservation_map: Dict[str, str]  # Maps original to enhanced elements
    lost_information: List[str]  # Should always be empty
    enhancement_log: List[str]
    graph: Graph
    metadata: Dict[str, Any]

class ContentPreservingAnalyzer:
    """Analyzes TTL content while preserving every piece of information"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.discovered_patterns: Dict[str, Any] = {}
        self.concept_taxonomy: Dict[str, Set[str]] = defaultdict(set)
        
    async def analyze_ttl_comprehensively(self, ttl_content: str) -> TTLContent:
        """Comprehensive analysis that preserves all original content"""
        
        # Parse original content
        original_graph = Graph()
        try:
            original_graph.parse(data=ttl_content, format='turtle')
        except Exception as e:
            logger.error(f"Failed to parse original TTL: {e}")
            # Still preserve the content even if unparseable
            return TTLContent(
                original_ttl=ttl_content,
                enhanced_ttl=ttl_content,
                preservation_map={},
                lost_information=[f"Parsing error: {e}"],
                enhancement_log=["Original content preserved despite parsing issues"],
                graph=Graph(),
                metadata={"parse_error": str(e)}
            )
        
        # Extract all structural elements
        structural_analysis = self._extract_all_structural_elements(original_graph)
        
        # Discover domain-specific patterns
        discovered_patterns = await self._discover_domain_patterns(structural_analysis, ttl_content)
        
        # Generate enhanced version while preserving everything
        enhanced_content = await self._generate_enhanced_preserving_ttl(
            ttl_content, structural_analysis, discovered_patterns
        )
        
        # Verify no information was lost
        preservation_verification = self._verify_information_preservation(
            ttl_content, enhanced_content["enhanced_ttl"]
        )
        
        return TTLContent(
            original_ttl=ttl_content,
            enhanced_ttl=enhanced_content["enhanced_ttl"],
            preservation_map=enhanced_content["preservation_map"],
            lost_information=preservation_verification["lost_information"],
            enhancement_log=enhanced_content["enhancement_log"],
            graph=enhanced_content["enhanced_graph"],
            metadata={
                "original_size": len(ttl_content),
                "enhanced_size": len(enhanced_content["enhanced_ttl"]),
                "original_triples": len(original_graph),
                "enhanced_triples": len(enhanced_content["enhanced_graph"]),
                "structural_analysis": structural_analysis,
                "discovered_patterns": discovered_patterns,
                "preservation_verification": preservation_verification
            }
        )
    
    def _extract_all_structural_elements(self, graph: Graph) -> Dict[str, Any]:
        """Extract every structural element from the graph"""
        
        analysis = {
            "namespaces": dict(graph.namespaces()),
            "classes": set(),
            "properties": {"object": set(), "data": set(), "annotation": set()},
            "individuals": set(),
            "literals": {"values": set(), "datatypes": set(), "languages": set()},
            "blank_nodes": set(),
            "predicates": set(),
            "subjects": set(),
            "objects": set(),
            "triple_patterns": [],
            "hierarchical_relationships": [],
            "domain_range_relationships": [],
            "cardinality_constraints": [],
            "value_restrictions": [],
            "temporal_elements": [],
            "custom_annotations": []
        }
        
        # Extract all triples and categorize elements
        for subj, pred, obj in graph:
            # Record the triple pattern
            analysis["triple_patterns"].append({
                "subject": str(subj),
                "predicate": str(pred),
                "object": str(obj),
                "subject_type": type(subj).__name__,
                "object_type": type(obj).__name__
            })
            
            # Categorize subjects, predicates, objects
            analysis["subjects"].add(str(subj))
            analysis["predicates"].add(str(pred))
            analysis["objects"].add(str(obj))
            
            # Identify classes
            if pred == RDF.type and obj == OWL.Class:
                analysis["classes"].add(str(subj))
            elif pred == RDF.type and obj == RDFS.Class:
                analysis["classes"].add(str(subj))
            
            # Identify properties
            if pred == RDF.type:
                if obj == OWL.ObjectProperty:
                    analysis["properties"]["object"].add(str(subj))
                elif obj == OWL.DatatypeProperty:
                    analysis["properties"]["data"].add(str(subj))
                elif obj == OWL.AnnotationProperty:
                    analysis["properties"]["annotation"].add(str(subj))
            
            # Identify individuals
            if pred == RDF.type and str(obj) not in [str(OWL.Class), str(RDFS.Class), 
                                                   str(OWL.ObjectProperty), str(OWL.DatatypeProperty)]:
                analysis["individuals"].add(str(subj))
            
            # Handle literals
            if isinstance(obj, Literal):
                analysis["literals"]["values"].add(str(obj))
                if obj.datatype:
                    analysis["literals"]["datatypes"].add(str(obj.datatype))
                if obj.language:
                    analysis["literals"]["languages"].add(obj.language)
            
            # Handle blank nodes
            if isinstance(subj, BNode):
                analysis["blank_nodes"].add(str(subj))
            if isinstance(obj, BNode):
                analysis["blank_nodes"].add(str(obj))
            
            # Identify hierarchical relationships
            if pred == RDFS.subClassOf:
                analysis["hierarchical_relationships"].append({
                    "child": str(subj),
                    "parent": str(obj),
                    "relationship_type": "subclass"
                })
            elif pred == RDFS.subPropertyOf:
                analysis["hierarchical_relationships"].append({
                    "child": str(subj),
                    "parent": str(obj),
                    "relationship_type": "subproperty"
                })
            
            # Identify domain/range relationships
            if pred == RDFS.domain:
                analysis["domain_range_relationships"].append({
                    "property": str(subj),
                    "domain": str(obj),
                    "type": "domain"
                })
            elif pred == RDFS.range:
                analysis["domain_range_relationships"].append({
                    "property": str(subj),
                    "range": str(obj),
                    "type": "range"
                })
            
            # Identify temporal elements (any predicate/object suggesting time)
            temporal_indicators = [
                "time", "date", "temporal", "valid", "effective", "expires",
                "created", "modified", "established", "enacted", "repealed"
            ]
            if any(indicator in str(pred).lower() or indicator in str(obj).lower() 
                   for indicator in temporal_indicators):
                analysis["temporal_elements"].append({
                    "subject": str(subj),
                    "predicate": str(pred),
                    "object": str(obj),
                    "temporal_type": "discovered"
                })
        
        # Convert sets to lists for JSON serialization
        for key, value in analysis.items():
            if isinstance(value, set):
                analysis[key] = list(value)
        
        return analysis
    
    async def _discover_domain_patterns(self, structural_analysis: Dict[str, Any], ttl_content: str) -> Dict[str, Any]:
        """Discover domain-specific patterns from the content itself"""
        
        discovery_prompt = f"""
        Analyze this RDF/TTL content to discover domain-specific patterns and concepts.
        DO NOT assume any specific legal domain (like GDPR, SOX, etc.).
        
        Structural Analysis Summary:
        - Classes found: {len(structural_analysis['classes'])}
        - Properties found: {len(structural_analysis['properties']['object']) + len(structural_analysis['properties']['data'])}
        - Individuals found: {len(structural_analysis['individuals'])}
        - Hierarchical relationships: {len(structural_analysis['hierarchical_relationships'])}
        - Temporal elements: {len(structural_analysis['temporal_elements'])}
        
        Sample classes: {structural_analysis['classes'][:10]}
        Sample properties: {list(structural_analysis['properties']['object'])[:5] + list(structural_analysis['properties']['data'])[:5]}
        Sample temporal elements: {structural_analysis['temporal_elements'][:5]}
        
        Discover and return:
        1. **Domain classification**: What domain does this appear to be (legal, medical, financial, etc.)?
        2. **Conceptual patterns**: What types of concepts are being modeled?
        3. **Relationship patterns**: What relationship types exist between concepts?
        4. **Constraint patterns**: What constraints or rules are implied?
        5. **Temporal patterns**: How is time/validity represented?
        6. **Hierarchical patterns**: What hierarchies exist?
        7. **Naming conventions**: What naming patterns are used?
        8. **Custom semantics**: Any domain-specific semantic patterns?
        
        Base your analysis ONLY on what's actually present in the data.
        Return as JSON:
        {{
            "domain_classification": {{
                "primary_domain": "detected domain",
                "confidence": 0.0-1.0,
                "evidence": ["evidence1", "evidence2"]
            }},
            "conceptual_patterns": [
                {{
                    "pattern_type": "type of concept pattern",
                    "examples": ["example1", "example2"],
                    "frequency": "number of occurrences"
                }}
            ],
            "relationship_patterns": [
                {{
                    "pattern_name": "relationship pattern name",
                    "pattern_description": "what this relationship represents",
                    "examples": ["subj pred obj examples"]
                }}
            ],
            "constraint_patterns": [
                {{
                    "constraint_type": "type of constraint discovered",
                    "description": "what constraint is implied",
                    "evidence": ["evidence from data"]
                }}
            ],
            "temporal_patterns": {{
                "has_temporal_data": boolean,
                "temporal_representations": ["how time is represented"],
                "temporal_concepts": ["time-related concepts found"]
            }},
            "hierarchical_patterns": [
                {{
                    "hierarchy_type": "type of hierarchy",
                    "root_concepts": ["top-level concepts"],
                    "depth": "estimated depth"
                }}
            ],
            "naming_conventions": {{
                "uri_patterns": ["URI naming patterns"],
                "prefix_usage": {{"prefix": "namespace"}},
                "identifier_patterns": ["identifier patterns"]
            }},
            "custom_semantics": [
                {{
                    "semantic_pattern": "custom semantic meaning",
                    "evidence": ["supporting evidence"],
                    "implications": ["what this means for reasoning"]
                }}
            ]
        }}
        
        RESPOND ONLY WITH VALID JSON. Base analysis on actual data content.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=discovery_prompt)])
            patterns = json.loads(response.content)
            
            # Store discovered patterns for future use
            self.discovered_patterns = patterns
            
            return patterns
            
        except Exception as e:
            logger.error(f"Error discovering domain patterns: {e}")
            return {
                "domain_classification": {"primary_domain": "unknown", "confidence": 0.0},
                "conceptual_patterns": [],
                "relationship_patterns": [],
                "constraint_patterns": [],
                "temporal_patterns": {"has_temporal_data": False},
                "hierarchical_patterns": [],
                "naming_conventions": {},
                "custom_semantics": []
            }
    
    async def _generate_enhanced_preserving_ttl(self, 
                                              original_ttl: str, 
                                              structural_analysis: Dict[str, Any],
                                              discovered_patterns: Dict[str, Any]) -> Dict[str, Any]:
        """Generate enhanced TTL while preserving every piece of original information"""
        
        enhancement_prompt = f"""
        Enhance this TTL content while PRESERVING EVERY SINGLE piece of original information.
        You must NOT lose any triples, namespaces, comments, or data.
        
        Original TTL Content:
        {original_ttl}
        
        Discovered Domain Patterns:
        {json.dumps(discovered_patterns, indent=2)}
        
        Enhancement Guidelines:
        1. **PRESERVE EVERYTHING**: All original triples, namespaces, prefixes, comments must remain
        2. **ADD COMPLEMENTARY CONTENT**: Only add content that enhances understanding
        3. **Semantic Enrichment**: Add inferred relationships based on discovered patterns
        4. **Documentation**: Add rdfs:label and rdfs:comment for better understanding
        5. **Type Inference**: Add missing rdf:type statements where clearly implied
        6. **Constraint Formalization**: Add OWL restrictions based on discovered constraints
        7. **Temporal Formalization**: Formalize temporal patterns discovered
        8. **Hierarchical Completion**: Complete implied hierarchical relationships
        
        Enhanced content should include:
        - ALL original content exactly as provided
        - Additional rdfs:label for human readability
        - Additional rdfs:comment for documentation
        - Inferred rdf:type statements where obvious
        - OWL property characteristics (functional, transitive, etc.) where applicable
        - SHACL shapes for validation (based on discovered patterns)
        - Additional ontological structure that makes implicit knowledge explicit
        
        Return the enhanced TTL content that:
        1. Contains ALL original information
        2. Adds semantic richness
        3. Improves machine processability
        4. Maintains parsability
        
        RESPOND ONLY WITH VALID TTL CONTENT.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=enhancement_prompt)])
            enhanced_ttl = response.content
            
            # Parse enhanced content to verify it's valid
            enhanced_graph = Graph()
            enhanced_graph.parse(data=enhanced_ttl, format='turtle')
            
            # Create preservation map
            preservation_map = self._create_preservation_map(original_ttl, enhanced_ttl)
            
            # Create enhancement log
            enhancement_log = [
                f"Enhanced TTL from {len(original_ttl)} to {len(enhanced_ttl)} characters",
                f"Expanded from {len(structural_analysis['triple_patterns'])} to {len(enhanced_graph)} triples",
                "Added semantic annotations and documentation",
                "Formalized discovered patterns into explicit ontological structures",
                "Enhanced machine readability while preserving human readability"
            ]
            
            return {
                "enhanced_ttl": enhanced_ttl,
                "enhanced_graph": enhanced_graph,
                "preservation_map": preservation_map,
                "enhancement_log": enhancement_log
            }
            
        except Exception as e:
            logger.error(f"Error generating enhanced TTL: {e}")
            # If enhancement fails, return original content
            original_graph = Graph()
            original_graph.parse(data=original_ttl, format='turtle')
            
            return {
                "enhanced_ttl": original_ttl,
                "enhanced_graph": original_graph,
                "preservation_map": {"preservation_status": "original_preserved_due_to_enhancement_error"},
                "enhancement_log": [f"Enhancement failed: {e}", "Original content preserved"]
            }
    
    def _create_preservation_map(self, original: str, enhanced: str) -> Dict[str, str]:
        """Create a mapping showing how original content is preserved in enhanced version"""
        
        # Split into lines for analysis
        original_lines = original.strip().split('\n')
        enhanced_lines = enhanced.strip().split('\n')
        
        preservation_map = {}
        
        # Map original lines to enhanced content
        for i, original_line in enumerate(original_lines):
            original_line = original_line.strip()
            if original_line and not original_line.startswith('#'):
                # Find this line or similar in enhanced content
                found_in_enhanced = []
                for j, enhanced_line in enumerate(enhanced_lines):
                    if original_line in enhanced_line or enhanced_line in original_line:
                        found_in_enhanced.append(j)
                
                preservation_map[f"original_line_{i}"] = {
                    "content": original_line,
                    "found_in_enhanced_lines": found_in_enhanced,
                    "preserved": len(found_in_enhanced) > 0
                }
        
        return preservation_map
    
    def _verify_information_preservation(self, original: str, enhanced: str) -> Dict[str, Any]:
        """Verify that no information was lost during enhancement"""
        
        try:
            # Parse both versions
            original_graph = Graph()
            enhanced_graph = Graph()
            
            original_graph.parse(data=original, format='turtle')
            enhanced_graph.parse(data=enhanced, format='turtle')
            
            # Check if all original triples are present in enhanced version
            original_triples = set((str(s), str(p), str(o)) for s, p, o in original_graph)
            enhanced_triples = set((str(s), str(p), str(o)) for s, p, o in enhanced_graph)
            
            missing_triples = original_triples - enhanced_triples
            added_triples = enhanced_triples - original_triples
            
            # Check namespaces
            original_namespaces = dict(original_graph.namespaces())
            enhanced_namespaces = dict(enhanced_graph.namespaces())
            
            missing_namespaces = set(original_namespaces.items()) - set(enhanced_namespaces.items())
            
            lost_information = []
            
            if missing_triples:
                lost_information.extend([f"Missing triple: {t}" for t in missing_triples])
            
            if missing_namespaces:
                lost_information.extend([f"Missing namespace: {ns}" for ns in missing_namespaces])
            
            return {
                "lost_information": lost_information,
                "preservation_successful": len(lost_information) == 0,
                "original_triple_count": len(original_triples),
                "enhanced_triple_count": len(enhanced_triples),
                "added_triple_count": len(added_triples),
                "missing_triple_count": len(missing_triples),
                "preservation_ratio": len(original_triples & enhanced_triples) / max(len(original_triples), 1)
            }
            
        except Exception as e:
            return {
                "lost_information": [f"Verification error: {e}"],
                "preservation_successful": False,
                "preservation_ratio": 0.0
            }

# ===============================
# 2. ADAPTIVE REASONING ENGINE
# ===============================

@dataclass
class ThoughtNode:
    """Domain-agnostic thought node for reasoning"""
    thought_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    content: str = ""
    parent_id: Optional[str] = None
    children_ids: List[str] = field(default_factory=list)
    confidence_score: float = 0.0
    reasoning_depth: int = 0
    discovered_concepts: List[str] = field(default_factory=list)
    temporal_context: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class AdaptiveTreeOfThoughts:
    """Tree of Thoughts that adapts to any domain based on input content"""
    
    def __init__(self, llm: ChatOpenAI, max_depth: int = 5, breadth: int = 3):
        self.llm = llm
        self.max_depth = max_depth
        self.breadth = breadth
        self.thoughts: Dict[str, ThoughtNode] = {}
        self.root_id: Optional[str] = None
        self.domain_context: Dict[str, Any] = {}
        
    async def reasoning_with_domain_adaptation(self, 
                                             problem: str,
                                             discovered_patterns: Dict[str, Any],
                                             ttl_content: TTLContent) -> Dict[str, Any]:
        """Perform reasoning adapted to the discovered domain patterns"""
        
        # Set domain context
        self.domain_context = discovered_patterns
        
        # Initialize root with domain-aware context
        root = ThoughtNode(
            content=f"Domain Analysis: {discovered_patterns.get('domain_classification', {}).get('primary_domain', 'unknown')} - Problem: {problem}",
            reasoning_depth=0,
            discovered_concepts=list(ttl_content.metadata.get('structural_analysis', {}).get('classes', []))[:10]
        )
        self.thoughts[root.thought_id] = root
        self.root_id = root.thought_id
        
        # Generate domain-adapted reasoning tree
        await self._generate_domain_adapted_thoughts(problem, ttl_content)
        
        # Evaluate thoughts with domain context
        evaluations = await self._evaluate_domain_thoughts()
        
        # Find best reasoning path
        best_path = self._find_optimal_path()
        
        return {
            "reasoning_type": "adaptive_tree_of_thoughts",
            "domain_adapted": True,
            "domain_context": self.domain_context,
            "total_thoughts_generated": len(self.thoughts),
            "reasoning_depth_achieved": max(node.reasoning_depth for node in self.thoughts.values()),
            "best_reasoning_path": [
                {
                    "thought_content": node.content,
                    "confidence_score": node.confidence_score,
                    "discovered_concepts": node.discovered_concepts,
                    "reasoning_depth": node.reasoning_depth,
                    "temporal_context": node.temporal_context
                }
                for node in best_path
            ],
            "domain_insights": self._extract_domain_insights(),
            "adaptive_confidence": self._calculate_adaptive_confidence(evaluations)
        }
    
    async def _generate_domain_adapted_thoughts(self, problem: str, ttl_content: TTLContent):
        """Generate thoughts adapted to the discovered domain"""
        
        for depth in range(self.max_depth):
            current_nodes = [node for node in self.thoughts.values() 
                           if node.reasoning_depth == depth]
            
            for node in current_nodes:
                await self._expand_node_with_domain_context(node, problem, ttl_content)
    
    async def _expand_node_with_domain_context(self, node: ThoughtNode, problem: str, ttl_content: TTLContent):
        """Expand a node considering domain-specific context"""
        
        domain_info = self.domain_context.get('domain_classification', {})
        conceptual_patterns = self.domain_context.get('conceptual_patterns', [])
        relationship_patterns = self.domain_context.get('relationship_patterns', [])
        
        expansion_prompt = f"""
        Continue reasoning about this problem using domain-specific knowledge discovered from the data.
        
        Current reasoning: {node.content}
        Problem: {problem}
        
        Domain Context:
        - Primary domain: {domain_info.get('primary_domain', 'unknown')}
        - Conceptual patterns found: {[p.get('pattern_type', '') for p in conceptual_patterns]}
        - Relationship patterns: {[p.get('pattern_name', '') for p in relationship_patterns]}
        - Available concepts: {node.discovered_concepts}
        
        Generate {self.breadth} different reasoning approaches that:
        1. Build on current reasoning
        2. Use domain-specific patterns discovered in the data
        3. Consider the conceptual relationships found
        4. Explore different interpretations within this domain
        5. Consider temporal aspects if present in the data
        
        Base reasoning ONLY on patterns and concepts found in the actual data.
        Do NOT assume knowledge not present in the input.
        
        Return as JSON array:
        [
            {{
                "thought": "detailed reasoning step using discovered patterns",
                "discovered_concepts": ["concepts from data used in this reasoning"],
                "confidence": 0.0-1.0,
                "domain_evidence": ["evidence from data supporting this reasoning"],
                "temporal_context": {{"temporal_info": "if applicable"}}
            }}
        ]
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=expansion_prompt)])
            thoughts_data = json.loads(response.content)
            
            for thought_data in thoughts_data:
                new_thought = ThoughtNode(
                    content=thought_data["thought"],
                    parent_id=node.thought_id,
                    confidence_score=thought_data.get("confidence", 0.5),
                    reasoning_depth=node.reasoning_depth + 1,
                    discovered_concepts=thought_data.get("discovered_concepts", []),
                    temporal_context=thought_data.get("temporal_context"),
                    metadata={
                        "domain_evidence": thought_data.get("domain_evidence", []),
                        "generated_at": datetime.now().isoformat()
                    }
                )
                
                self.thoughts[new_thought.thought_id] = new_thought
                node.children_ids.append(new_thought.thought_id)
                
        except Exception as e:
            logger.error(f"Error expanding node: {e}")
    
    async def _evaluate_domain_thoughts(self) -> Dict[str, float]:
        """Evaluate thoughts considering domain-specific criteria"""
        
        evaluations = {}
        
        for thought_id, thought in self.thoughts.items():
            eval_prompt = f"""
            Evaluate this reasoning step for quality within the discovered domain context.
            
            Reasoning: {thought.content}
            Domain: {self.domain_context.get('domain_classification', {}).get('primary_domain', 'unknown')}
            Concepts used: {thought.discovered_concepts}
            Domain evidence: {thought.metadata.get('domain_evidence', [])}
            
            Evaluate based on:
            1. Accuracy within the domain (based on discovered patterns)
            2. Logical consistency with domain constraints
            3. Effective use of discovered concepts
            4. Evidence support from actual data
            5. Reasoning completeness for the domain
            
            Rate each criterion 0.0-1.0 and provide overall score.
            
            Return JSON: {{"overall_score": 0.0-1.0, "criteria_scores": {{}}, "evaluation_rationale": ""}}
            
            RESPOND ONLY WITH VALID JSON.
            """
            
            try:
                response = await self.llm.ainvoke([HumanMessage(content=eval_prompt)])
                eval_data = json.loads(response.content)
                
                score = eval_data.get("overall_score", 0.5)
                evaluations[thought_id] = score
                thought.confidence_score = score
                thought.metadata["evaluation"] = eval_data
                
            except Exception as e:
                logger.error(f"Error evaluating thought {thought_id}: {e}")
                evaluations[thought_id] = 0.5
        
        return evaluations
    
    def _find_optimal_path(self) -> List[ThoughtNode]:
        """Find the optimal reasoning path through the tree"""
        if not self.root_id:
            return []
        
        def find_best_path(node_id: str, path: List[ThoughtNode]) -> List[ThoughtNode]:
            node = self.thoughts[node_id]
            current_path = path + [node]
            
            if not node.children_ids:
                return current_path
            
            # Find child with highest confidence considering domain relevance
            best_child_id = max(
                node.children_ids,
                key=lambda cid: self.thoughts[cid].confidence_score
            )
            
            return find_best_path(best_child_id, current_path)
        
        return find_best_path(self.root_id, [])
    
    def _extract_domain_insights(self) -> Dict[str, Any]:
        """Extract insights specific to the discovered domain"""
        
        all_concepts = []
        all_evidence = []
        confidence_scores = []
        
        for thought in self.thoughts.values():
            all_concepts.extend(thought.discovered_concepts)
            all_evidence.extend(thought.metadata.get("domain_evidence", []))
            confidence_scores.append(thought.confidence_score)
        
        return {
            "most_used_concepts": [item for item, count in Counter(all_concepts).most_common(10)],
            "strongest_evidence": [item for item, count in Counter(all_evidence).most_common(5)],
            "average_confidence": sum(confidence_scores) / max(len(confidence_scores), 1),
            "domain_coverage": len(set(all_concepts)) / max(len(all_concepts), 1)
        }
    
    def _calculate_adaptive_confidence(self, evaluations: Dict[str, float]) -> float:
        """Calculate confidence adapted to domain context"""
        
        if not evaluations:
            return 0.0
        
        base_confidence = sum(evaluations.values()) / len(evaluations)
        
        # Adjust based on domain discovery confidence
        domain_confidence = self.domain_context.get('domain_classification', {}).get('confidence', 0.5)
        
        # Weight the confidence by domain understanding
        adaptive_confidence = (base_confidence * 0.7) + (domain_confidence * 0.3)
        
        return adaptive_confidence

# ===============================
# 3. GENERAL-PURPOSE FORMAL VERIFICATION
# ===============================

class GeneralPurposeFormalVerifier:
    """Domain-agnostic formal verification system"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.discovered_constraints: List[str] = []
        self.logical_patterns: Dict[str, Any] = {}
        
    async def discover_and_verify_constraints(self, ttl_content: TTLContent) -> Dict[str, Any]:
        """Discover constraints from data and verify consistency"""
        
        # Discover constraints from the actual data
        discovered_constraints = await self._discover_constraints_from_data(ttl_content)
        
        # Generate formal logical expressions
        formal_constraints = await self._formalize_discovered_constraints(discovered_constraints)
        
        # Verify consistency
        verification_result = await self._verify_against_discovered_constraints(
            ttl_content.graph, formal_constraints
        )
        
        return {
            "verification_type": "general_purpose_formal",
            "discovered_constraints": discovered_constraints,
            "formal_constraints": formal_constraints,
            "verification_result": verification_result,
            "constraint_coverage": self._calculate_constraint_coverage(ttl_content, discovered_constraints)
        }
    
    async def _discover_constraints_from_data(self, ttl_content: TTLContent) -> List[Dict[str, Any]]:
        """Discover logical constraints present in the data"""
        
        structural_analysis = ttl_content.metadata.get('structural_analysis', {})
        
        discovery_prompt = f"""
        Discover logical constraints and rules from this RDF data structure.
        
        Data Analysis:
        - Hierarchical relationships: {structural_analysis.get('hierarchical_relationships', [])}
        - Domain/Range relationships: {structural_analysis.get('domain_range_relationships', [])}
        - Classes: {structural_analysis.get('classes', [])}
        - Properties: {structural_analysis.get('properties', {})}
        - Triple patterns: {len(structural_analysis.get('triple_patterns', []))} total
        
        Sample triple patterns: {structural_analysis.get('triple_patterns', [])[:20]}
        
        Discover constraints that are IMPLIED by the data structure:
        1. **Mutual exclusivity**: concepts that cannot coexist
        2. **Dependency relationships**: A requires B
        3. **Cardinality constraints**: how many relationships are allowed
        4. **Type constraints**: what types are valid in what contexts
        5. **Temporal constraints**: time-based rules if temporal data exists
        6. **Hierarchical constraints**: rules about class/property hierarchies
        7. **Domain-specific rules**: rules specific to this domain
        
        Base discoveries ONLY on patterns visible in the actual data.
        Do NOT assume external knowledge.
        
        Return as JSON:
        [
            {{
                "constraint_type": "type of constraint discovered",
                "description": "what the constraint states",
                "evidence": ["data patterns supporting this constraint"],
                "entities_involved": ["entities this constraint applies to"],
                "logical_form": "informal logical expression",
                "confidence": 0.0-1.0
            }}
        ]
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=discovery_prompt)])
            constraints = json.loads(response.content)
            return constraints
        except Exception as e:
            logger.error(f"Error discovering constraints: {e}")
            return []
    
    async def _formalize_discovered_constraints(self, discovered_constraints: List[Dict[str, Any]]) -> List[str]:
        """Convert discovered constraints into formal logical expressions"""
        
        formalization_prompt = f"""
        Convert these discovered constraints into formal logical expressions.
        
        Discovered Constraints:
        {json.dumps(discovered_constraints, indent=2)}
        
        Convert each constraint into:
        1. **First-order logic** format where possible
        2. **SHACL constraint** format for validation
        3. **OWL axiom** format where applicable
        
        Use standard logical notation:
        - ∀ (for all), ∃ (exists)
        - ∧ (and), ∨ (or), ¬ (not)
        - → (implies), ↔ (if and only if)
        
        Example formats:
        - FOL: ∀x (ClassA(x) → ¬ClassB(x))
        - SHACL: sh:property [ sh:path :hasProperty ; sh:maxCount 1 ]
        - OWL: ClassA owl:disjointWith ClassB
        
        Return as JSON:
        [
            {{
                "original_constraint_id": "reference to original",
                "formal_expressions": {{
                    "first_order_logic": "FOL expression",
                    "shacl": "SHACL constraint",
                    "owl": "OWL axiom"
                }},
                "applicability": "when this constraint applies"
            }}
        ]
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=formalization_prompt)])
            formal_constraints = json.loads(response.content)
            
            # Extract just the logical expressions
            expressions = []
            for constraint in formal_constraints:
                formal_exprs = constraint.get("formal_expressions", {})
                if formal_exprs.get("first_order_logic"):
                    expressions.append(formal_exprs["first_order_logic"])
            
            return expressions
            
        except Exception as e:
            logger.error(f"Error formalizing constraints: {e}")
            return []
    
    async def _verify_against_discovered_constraints(self, 
                                                   graph: Graph, 
                                                   formal_constraints: List[str]) -> Dict[str, Any]:
        """Verify the graph against discovered constraints"""
        
        # Convert graph to triple list for analysis
        triples = [(str(s), str(p), str(o)) for s, p, o in graph]
        
        verification_prompt = f"""
        Verify this RDF graph against the discovered logical constraints.
        
        Graph Data:
        - Total triples: {len(triples)}
        - Sample triples: {triples[:30]}
        
        Formal Constraints to Check:
        {json.dumps(formal_constraints, indent=2)}
        
        For each constraint, verify:
        1. Does the data satisfy the constraint?
        2. Are there any violations?
        3. What evidence supports compliance/violation?
        4. What is the confidence in this verification?
        
        Return verification results as JSON:
        {{
            "overall_consistency": boolean,
            "constraint_results": [
                {{
                    "constraint": "the constraint being checked",
                    "satisfied": boolean,
                    "violations": [
                        {{
                            "violation_type": "what kind of violation",
                            "description": "specific violation found",
                            "affected_triples": ["relevant triples"],
                            "severity": "high|medium|low"
                        }}
                    ],
                    "evidence": ["supporting evidence"],
                    "confidence": 0.0-1.0
                }}
            ],
            "consistency_score": 0.0-1.0,
            "verification_summary": "overall assessment"
        }}
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=verification_prompt)])
            return json.loads(response.content)
        except Exception as e:
            logger.error(f"Error in verification: {e}")
            return {
                "overall_consistency": False,
                "constraint_results": [],
                "consistency_score": 0.0,
                "verification_summary": f"Verification failed: {e}"
            }
    
    def _calculate_constraint_coverage(self, ttl_content: TTLContent, constraints: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate how well constraints cover the data"""
        
        total_entities = len(ttl_content.metadata.get('structural_analysis', {}).get('classes', []))
        entities_with_constraints = len(set(
            entity for constraint in constraints 
            for entity in constraint.get('entities_involved', [])
        ))
        
        coverage_ratio = entities_with_constraints / max(total_entities, 1)
        
        return {
            "constraint_count": len(constraints),
            "entities_covered": entities_with_constraints,
            "total_entities": total_entities,
            "coverage_ratio": coverage_ratio,
            "coverage_assessment": (
                "comprehensive" if coverage_ratio >= 0.8 else
                "good" if coverage_ratio >= 0.6 else
                "partial" if coverage_ratio >= 0.4 else
                "minimal"
            )
        }

# ===============================
# 4. ADAPTIVE TEMPORAL REASONING
# ===============================

class AdaptiveTemporalReasoner:
    """Discovers and reasons about temporal patterns from data"""
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        
    async def discover_temporal_semantics(self, ttl_content: TTLContent) -> Dict[str, Any]:
        """Discover how time is represented in this specific dataset"""
        
        temporal_elements = ttl_content.metadata.get('structural_analysis', {}).get('temporal_elements', [])
        
        if not temporal_elements:
            return {
                "has_temporal_data": False,
                "temporal_reasoning_applicable": False,
                "message": "No temporal indicators found in the data"
            }
        
        discovery_prompt = f"""
        Analyze how time and temporal relationships are represented in this data.
        
        Temporal Elements Found:
        {json.dumps(temporal_elements, indent=2)}
        
        Discover:
        1. **Temporal representation patterns**: How is time encoded?
        2. **Temporal relationships**: What temporal relationships exist?
        3. **Validity patterns**: How is validity/applicability over time represented?
        4. **Temporal constraints**: What time-based rules are implied?
        5. **Temporal hierarchy**: Are there temporal precedence rules?
        6. **Change patterns**: How are changes over time represented?
        
        Base analysis ONLY on the temporal patterns visible in the data.
        
        Return as JSON:
        {{
            "has_temporal_data": true,
            "temporal_representations": [
                {{
                    "pattern_type": "how time is represented",
                    "examples": ["examples from data"],
                    "semantic_meaning": "what this represents"
                }}
            ],
            "temporal_relationships": [
                {{
                    "relationship_type": "type of temporal relationship",
                    "description": "what this relationship means",
                    "examples": ["examples from data"]
                }}
            ],
            "temporal_constraints": [
                {{
                    "constraint_type": "temporal constraint discovered",
                    "description": "what the constraint implies",
                    "evidence": ["supporting evidence from data"]
                }}
            ],
            "temporal_reasoning_rules": [
                {{
                    "rule_type": "temporal reasoning rule",
                    "rule_description": "how to reason about time in this domain",
                    "applicability": "when this rule applies"
                }}
            ]
        }}
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=discovery_prompt)])
            temporal_semantics = json.loads(response.content)
            
            # Apply discovered temporal reasoning
            temporal_analysis = await self._apply_discovered_temporal_reasoning(
                ttl_content, temporal_semantics
            )
            
            return {
                **temporal_semantics,
                "temporal_analysis": temporal_analysis,
                "temporal_reasoning_applicable": True
            }
            
        except Exception as e:
            logger.error(f"Error discovering temporal semantics: {e}")
            return {
                "has_temporal_data": len(temporal_elements) > 0,
                "temporal_reasoning_applicable": False,
                "error": str(e)
            }
    
    async def _apply_discovered_temporal_reasoning(self, 
                                                 ttl_content: TTLContent,
                                                 temporal_semantics: Dict[str, Any]) -> Dict[str, Any]:
        """Apply temporal reasoning based on discovered patterns"""
        
        reasoning_prompt = f"""
        Apply temporal reasoning to this data using the discovered temporal patterns.
        
        Discovered Temporal Semantics:
        {json.dumps(temporal_semantics, indent=2)}
        
        Graph size: {len(ttl_content.graph)} triples
        
        Apply temporal reasoning to determine:
        1. **Current validity**: What is currently valid/active?
        2. **Temporal conflicts**: Any conflicting temporal assertions?
        3. **Succession patterns**: What supersedes what over time?
        4. **Temporal gaps**: Missing temporal information?
        5. **Temporal consistency**: Are temporal assertions consistent?
        6. **Future implications**: What temporal changes are implied?
        
        Use ONLY the temporal reasoning rules discovered from this specific data.
        
        Return analysis as JSON:
        {{
            "temporal_consistency": boolean,
            "current_validity_status": [
                {{
                    "entity": "entity identifier",
                    "status": "active|expired|superseded|pending",
                    "evidence": "temporal evidence from data"
                }}
            ],
            "temporal_conflicts": [
                {{
                    "conflict_type": "type of temporal conflict",
                    "entities_involved": ["conflicting entities"],
                    "description": "nature of the conflict",
                    "resolution_suggestion": "how to resolve based on discovered patterns"
                }}
            ],
            "succession_chains": [
                {{
                    "predecessor": "earlier entity",
                    "successor": "later entity", 
                    "transition_evidence": "evidence for succession"
                }}
            ],
            "temporal_reasoning_confidence": 0.0-1.0
        }}
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=reasoning_prompt)])
            return json.loads(response.content)
        except Exception as e:
            logger.error(f"Error in temporal reasoning: {e}")
            return {
                "temporal_consistency": False,
                "error": str(e),
                "temporal_reasoning_confidence": 0.0
            }

# ===============================
# 5. MASTER COORDINATION SYSTEM
# ===============================

class GeneralPurposeLegalReasoningSystem:
    """General-purpose reasoning system that adapts to any domain"""
    
    def __init__(self, base_url: str, api_key: str, model: str = "o3-mini"):
        self.llm = ChatOpenAI(base_url=base_url, api_key=api_key, model=model)
        
        # Initialize adaptive components
        self.content_analyzer = ContentPreservingAnalyzer(self.llm)
        self.adaptive_tot = AdaptiveTreeOfThoughts(self.llm)
        self.formal_verifier = GeneralPurposeFormalVerifier(self.llm)
        self.temporal_reasoner = AdaptiveTemporalReasoner(self.llm)
        
    async def comprehensive_analysis(self, ttl_content: str) -> Dict[str, Any]:
        """Perform comprehensive analysis preserving all information"""
        
        analysis_id = str(uuid.uuid4())
        start_time = datetime.now()
        
        logger.info(f"Starting comprehensive analysis {analysis_id}")
        
        # Step 1: Content-preserving analysis
        logger.info("Step 1: Analyzing content while preserving all information")
        preserved_content = await self.content_analyzer.analyze_ttl_comprehensively(ttl_content)
        
        # Verify no information was lost
        if preserved_content.lost_information:
            logger.warning(f"Information loss detected: {preserved_content.lost_information}")
        
        # Step 2: Domain-adaptive reasoning
        logger.info("Step 2: Performing domain-adaptive reasoning")
        reasoning_problem = f"Analyze the semantics, constraints, and relationships in this {preserved_content.metadata.get('discovered_patterns', {}).get('domain_classification', {}).get('primary_domain', 'unknown')} knowledge graph"
        
        reasoning_result = await self.adaptive_tot.reasoning_with_domain_adaptation(
            problem=reasoning_problem,
            discovered_patterns=preserved_content.metadata.get('discovered_patterns', {}),
            ttl_content=preserved_content
        )
        
        # Step 3: Formal verification
        logger.info("Step 3: Performing formal verification")
        verification_result = await self.formal_verifier.discover_and_verify_constraints(preserved_content)
        
        # Step 4: Temporal reasoning (if applicable)
        logger.info("Step 4: Analyzing temporal aspects")
        temporal_result = await self.temporal_reasoner.discover_temporal_semantics(preserved_content)
        
        # Step 5: Consolidate insights
        logger.info("Step 5: Consolidating insights")
        consolidated_insights = await self._consolidate_all_insights(
            preserved_content, reasoning_result, verification_result, temporal_result
        )
        
        end_time = datetime.now()
        processing_duration = (end_time - start_time).total_seconds()
        
        return {
            "analysis_id": analysis_id,
            "analysis_timestamp": start_time.isoformat(),
            "processing_duration_seconds": processing_duration,
            "input_preservation": {
                "original_size": len(ttl_content),
                "enhanced_size": len(preserved_content.enhanced_ttl),
                "information_preserved": len(preserved_content.lost_information) == 0,
                "lost_information": preserved_content.lost_information,
                "preservation_ratio": preserved_content.metadata.get("preservation_verification", {}).get("preservation_ratio", 0.0)
            },
            "domain_discovery": preserved_content.metadata.get('discovered_patterns', {}),
            "content_analysis": {
                "original_content": preserved_content.original_ttl,
                "enhanced_content": preserved_content.enhanced_ttl,
                "preservation_map": preserved_content.preservation_map,
                "enhancement_log": preserved_content.enhancement_log,
                "structural_analysis": preserved_content.metadata.get('structural_analysis', {})
            },
            "reasoning_analysis": reasoning_result,
            "formal_verification": verification_result,
            "temporal_analysis": temporal_result,
            "consolidated_insights": consolidated_insights,
            "system_metadata": {
                "reasoning_patterns_used": ["adaptive_tree_of_thoughts", "formal_verification", "temporal_reasoning"],
                "domain_agnostic": True,
                "information_preserving": True,
                "adaptive_to_input": True
            }
        }
    
    async def _consolidate_all_insights(self, 
                                      preserved_content: TTLContent,
                                      reasoning_result: Dict[str, Any],
                                      verification_result: Dict[str, Any],
                                      temporal_result: Dict[str, Any]) -> Dict[str, Any]:
        """Consolidate insights from all analysis phases"""
        
        consolidation_prompt = f"""
        Consolidate insights from comprehensive analysis of this knowledge graph.
        
        Domain Discovery:
        {json.dumps(preserved_content.metadata.get('discovered_patterns', {}), indent=2)}
        
        Reasoning Analysis:
        {json.dumps(reasoning_result, indent=2)[:2000]}...
        
        Formal Verification:
        {json.dumps(verification_result, indent=2)[:2000]}...
        
        Temporal Analysis:
        {json.dumps(temporal_result, indent=2)[:1000]}...
        
        Provide consolidated insights:
        1. **Overall Assessment**: Quality and consistency of the knowledge graph
        2. **Key Findings**: Most important discoveries about the domain and data
        3. **Strengths**: What is well-represented and consistent
        4. **Issues Identified**: Problems, inconsistencies, or gaps found
        5. **Recommendations**: Actionable improvements based on discovered patterns
        6. **Domain-Specific Insights**: Insights specific to the discovered domain
        7. **Confidence Assessment**: Overall confidence in the analysis
        
        Base recommendations on actual patterns found in the data.
        
        Return as JSON:
        {{
            "overall_assessment": {{
                "quality_score": 0.0-1.0,
                "consistency_score": 0.0-1.0,
                "completeness_score": 0.0-1.0,
                "summary": "executive summary"
            }},
            "key_findings": [
                {{
                    "finding": "important discovery",
                    "evidence": ["supporting evidence"],
                    "implications": ["what this means"]
                }}
            ],
            "strengths": ["identified strengths"],
            "issues_identified": [
                {{
                    "issue_type": "type of issue",
                    "description": "specific issue found",
                    "severity": "high|medium|low",
                    "recommendation": "how to address"
                }}
            ],
            "domain_specific_insights": [
                {{
                    "insight": "domain-specific insight",
                    "relevance": "why this matters for this domain"
                }}
            ],
            "actionable_recommendations": [
                {{
                    "recommendation": "what to do",
                    "rationale": "why this is recommended",
                    "priority": "high|medium|low"
                }}
            ],
            "confidence_assessment": {{
                "overall_confidence": 0.0-1.0,
                "confidence_factors": ["factors affecting confidence"],
                "uncertainty_areas": ["areas of uncertainty"]
            }}
        }}
        
        RESPOND ONLY WITH VALID JSON.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=consolidation_prompt)])
            return json.loads(response.content)
        except Exception as e:
            logger.error(f"Error consolidating insights: {e}")
            return {
                "overall_assessment": {"quality_score": 0.0, "summary": f"Consolidation failed: {e}"},
                "key_findings": [],
                "strengths": [],
                "issues_identified": [],
                "domain_specific_insights": [],
                "actionable_recommendations": [],
                "confidence_assessment": {"overall_confidence": 0.0, "uncertainty_areas": ["consolidation_error"]}
            }


# ===============================
# 6. DEMONSTRATION FUNCTION
# ===============================

async def demonstrate_general_purpose_reasoning():
    """Demonstrate the general-purpose reasoning system"""
    
    # Initialize system
    reasoning_system = GeneralPurposeLegalReasoningSystem(
        base_url="https://api.openai.com/v1",
        api_key="your-api-key",
        model="o3-mini"
    )
    
    # Example TTL content (could be from any domain)
    sample_ttl = """
    @prefix ex: <http://example.org/> .
    @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
    
    # This could be any domain - legal, medical, financial, etc.
    ex:Document_123 a ex:RegulatoryDocument ;
        ex:title "Data Processing Guidelines" ;
        ex:effectiveDate "2023-01-01"^^xsd:date ;
        ex:hasRequirement ex:Requirement_1 .
    
    ex:Requirement_1 a ex:ComplianceRequirement ;
        ex:description "Organizations must obtain consent" ;
        ex:appliesTo ex:PersonalDataProcessing ;
        ex:mandatoryFrom "2023-06-01"^^xsd:date .
    
    ex:PersonalDataProcessing a ex:ProcessingActivity ;
        ex:requiresLegalBasis ex:ConsentBasis ;
        ex:subjectTo ex:Requirement_1 .
    
    ex:ConsentBasis a ex:LegalBasis ;
        rdfs:label "Consent" ;
        ex:validityDuration "P2Y"^^xsd:duration .
    """
    
    # Perform comprehensive analysis
    print("Starting general-purpose analysis...")
    analysis = await reasoning_system.comprehensive_analysis(sample_ttl)
    
    print("\n=== ANALYSIS COMPLETE ===")
    print(f"Analysis ID: {analysis['analysis_id']}")
    print(f"Processing Duration: {analysis['processing_duration_seconds']:.2f} seconds")
    
    print(f"\n=== INFORMATION PRESERVATION ===")
    print(f"Information Preserved: {analysis['input_preservation']['information_preserved']}")
    print(f"Original Size: {analysis['input_preservation']['original_size']} chars")
    print(f"Enhanced Size: {analysis['input_preservation']['enhanced_size']} chars")
    print(f"Preservation Ratio: {analysis['input_preservation']['preservation_ratio']:.2%}")
    
    print(f"\n=== DOMAIN DISCOVERY ===")
    domain_info = analysis['domain_discovery'].get('domain_classification', {})
    print(f"Detected Domain: {domain_info.get('primary_domain', 'unknown')}")
    print(f"Confidence: {domain_info.get('confidence', 0.0):.2%}")
    
    print(f"\n=== OVERALL ASSESSMENT ===")
    overall = analysis['consolidated_insights'].get('overall_assessment', {})
    print(f"Quality Score: {overall.get('quality_score', 0.0):.2%}")
    print(f"Consistency Score: {overall.get('consistency_score', 0.0):.2%}")
    print(f"Summary: {overall.get('summary', 'N/A')}")
    
    print(f"\n=== KEY FINDINGS ===")
    for finding in analysis['consolidated_insights'].get('key_findings', [])[:3]:
        print(f"- {finding.get('finding', 'N/A')}")
    
    print(f"\n=== RECOMMENDATIONS ===")
    for rec in analysis['consolidated_insights'].get('actionable_recommendations', [])[:3]:
        print(f"- {rec.get('recommendation', 'N/A')} (Priority: {rec.get('priority', 'N/A')})")
    
    print("\nGeneral-purpose analysis completed successfully!")
    print("The system adapted to the input without hardcoded assumptions.")


if __name__ == "__main__":
    asyncio.run(demonstrate_general_purpose_reasoning())
