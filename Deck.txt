# GDPR Multi-Agent System - PRODUCTION READY with FIXED ERRORS and ENHANCED LINKING
# Requirements: pip install langchain langgraph langchain-elasticsearch langchain-openai pymupdf==1.26.1 pydantic scikit-learn tqdm

import asyncio
import logging
import os
import pickle
import re
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Set
from uuid import uuid4

import numpy as np
import pymupdf  # Latest PyMuPDF 1.26.1
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field, ConfigDict
from elasticsearch import Elasticsearch, AsyncElasticsearch
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm as atqdm
from tqdm import tqdm

# =============================================================================
# GLOBAL CONFIGURATION - ALL CREDENTIALS AND SETTINGS
# =============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", None)  # For Azure OpenAI, proxies, etc.
OPENAI_MODEL = "o3-mini"  # ONLY o3-mini everywhere
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
OPENAI_REASONING_EFFORT = "high"

# Elasticsearch Configuration - FIXED without SSL complications
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost")
ELASTICSEARCH_PORT = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_USE_SSL = False  # Disabled to avoid SSL issues
ELASTICSEARCH_INDEX_NAME = "gdpr_comprehensive_knowledge"

# Processing Configuration
CHUNK_SIZE = 2000  # Larger chunks for full context
CHUNK_OVERLAP = 400  # More overlap for better continuity
SIMILARITY_THRESHOLD = 0.75
BATCH_SIZE = 20  # Smaller batches for stability

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# ENHANCED PYDANTIC MODELS WITH COMPREHENSIVE ARTICLE LINKING
# =============================================================================

class DocumentType(str, Enum):
    GDPR_EU = "gdpr_eu"
    GDPR_UK = "gdpr_uk"

class ChunkType(str, Enum):
    FULL_ARTICLE = "full_article"
    ARTICLE_SECTION = "article_section"
    CHAPTER = "chapter"
    SECTION = "section"
    PARAGRAPH = "paragraph"
    CROSS_REFERENCE = "cross_reference"

class RelationshipType(str, Enum):
    REFERENCES = "references"
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    ELABORATES = "elaborates"
    SUPERSEDES = "supersedes"
    CROSS_REFERENCES = "cross_references"
    DEFINES = "defines"
    APPLIES_TO = "applies_to"
    SEMANTICALLY_RELATED = "semantically_related"

class ArticleReference(BaseModel):
    """Enhanced article reference with full context"""
    article_number: str
    article_title: str
    section_reference: Optional[str] = None
    paragraph_reference: Optional[str] = None
    context: str  # The context in which this article is referenced
    reference_type: str  # "direct", "implied", "definition", "procedure"

class LegalConcept(BaseModel):
    """Enhanced legal concept with article mappings"""
    concept_name: str
    definition: str
    primary_article: str
    related_articles: List[str] = Field(default_factory=list)
    context_chunks: List[str] = Field(default_factory=list)  # Chunk IDs where this concept appears

class ComprehensiveChunk(BaseModel):
    """Enhanced chunk model with comprehensive article linking"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    id: str = Field(default_factory=lambda: str(uuid4()))
    document_type: DocumentType
    chunk_type: ChunkType
    title: str
    content: str
    full_article_content: Optional[str] = None  # Full article text if this is part of an article
    
    # Article structure
    chapter_number: Optional[str] = None
    article_number: Optional[str] = None
    section_number: Optional[str] = None
    paragraph_number: Optional[str] = None
    hierarchy_level: int = Field(ge=0, le=6)
    
    # Position and navigation
    page_number: Optional[int] = None
    position_in_document: int = Field(ge=0)
    
    # Direct OpenAI embeddings
    embedding: Optional[List[float]] = None
    
    # Enhanced linking
    article_references: List[ArticleReference] = Field(default_factory=list)
    legal_concepts: List[LegalConcept] = Field(default_factory=list)
    cross_references: List[str] = Field(default_factory=list)  # "See Article X", "As defined in Y"
    
    # Relationships
    related_chunk_ids: Set[str] = Field(default_factory=set)
    semantic_similarity_scores: Dict[str, float] = Field(default_factory=dict)
    
    # Keywords and concepts
    keywords: List[str] = Field(default_factory=list)
    legal_terms: List[str] = Field(default_factory=list)
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)

class SemanticRelationship(BaseModel):
    """Enhanced relationship model with legal context"""
    id: str = Field(default_factory=lambda: str(uuid4()))
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: RelationshipType
    confidence_score: float = Field(ge=0.0, le=1.0)
    semantic_similarity: float = Field(ge=0.0, le=1.0)
    distance_in_document: int = Field(ge=0)
    reasoning: str
    legal_basis: str
    article_connection: Optional[str] = None  # How articles are connected
    extracted_by_agent: str
    created_at: datetime = Field(default_factory=datetime.now)

class ComprehensiveMemoryState(BaseModel):
    """State with comprehensive legal knowledge"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Document processing state
    documents: List[Document] = Field(default_factory=list)
    comprehensive_chunks: List[ComprehensiveChunk] = Field(default_factory=list)
    relationships: List[SemanticRelationship] = Field(default_factory=list)
    
    # Article mapping
    article_index: Dict[str, str] = Field(default_factory=dict)  # article_number -> chunk_id
    concept_index: Dict[str, List[str]] = Field(default_factory=dict)  # concept -> chunk_ids
    cross_reference_index: Dict[str, List[str]] = Field(default_factory=dict)  # reference -> chunk_ids
    
    # Elasticsearch components
    vectorstore: Optional[Any] = None
    qa_chain: Optional[Any] = None
    conversational_chain: Optional[Any] = None
    conversation_memory: Optional[Any] = None
    
    # Processing state
    current_agent: Optional[str] = None
    errors: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

# =============================================================================
# FIXED ELASTICSEARCH CONFIGURATION WITHOUT SSL
# =============================================================================

class SimpleElasticsearchConfig:
    """Simple Elasticsearch configuration without SSL complications"""
    
    def __init__(self, 
                 host: str = ELASTICSEARCH_HOST, 
                 port: int = ELASTICSEARCH_PORT,
                 username: str = ELASTICSEARCH_USERNAME,
                 password: str = ELASTICSEARCH_PASSWORD):
        
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        
        if not self.password or self.password == "your-elasticsearch-password":
            raise ValueError("Elasticsearch password must be provided")
    
    def get_elasticsearch_url(self) -> str:
        """Get HTTP Elasticsearch URL (no SSL)"""
        return f"http://{self.host}:{self.port}"
    
    def get_client_config(self) -> Dict[str, Any]:
        """Get simple Elasticsearch client configuration"""
        return {
            "hosts": [f"{self.host}:{self.port}"],
            "basic_auth": (self.username, self.password),
            "verify_certs": False,
            "ssl_show_warn": False
        }
    
    def create_client(self) -> Elasticsearch:
        """Create simple Elasticsearch client"""
        return Elasticsearch(**self.get_client_config())

# =============================================================================
# DIRECT OPENAI EMBEDDING SERVICE
# =============================================================================

class DirectOpenAIEmbeddingService:
    """Direct OpenAI embedding service without LangChain wrapper"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = AsyncOpenAI(**client_kwargs)
        self.model = OPENAI_EMBEDDING_MODEL
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for single text with error handling"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating embedding for text (length: {len(text)}): {e}")
            return []
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts with progress and error handling"""
        if not texts:
            return []
        
        try:
            # Log batch info
            total_chars = sum(len(text) for text in texts)
            logger.info(f"Generating embeddings for {len(texts)} texts ({total_chars:,} total characters)")
            
            response = await self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            
            embeddings = [data.embedding for data in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings (dim: {len(embeddings[0]) if embeddings else 0})")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Error generating batch embeddings for {len(texts)} texts: {e}")
            logger.error(f"Falling back to individual embedding generation...")
            
            # Fallback: generate embeddings one by one
            embeddings = []
            for i, text in enumerate(texts):
                try:
                    embedding = await self.embed_text(text)
                    embeddings.append(embedding)
                    if (i + 1) % 5 == 0:  # Log every 5 embeddings
                        logger.info(f"Fallback progress: {i + 1}/{len(texts)} embeddings")
                except Exception as individual_error:
                    logger.error(f"Failed to generate embedding for text {i}: {individual_error}")
                    embeddings.append([])  # Add empty embedding as placeholder
                
                # Rate limiting for fallback
                await asyncio.sleep(0.1)
            
            return embeddings

# =============================================================================
# ENHANCED REASONING SERVICE WITH COMPREHENSIVE ARTICLE ANALYSIS
# =============================================================================

class ComprehensiveReasoningService:
    """Enhanced reasoning service for comprehensive article analysis"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = AsyncOpenAI(**client_kwargs)
        
        # LangChain ChatOpenAI for chains
        langchain_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": api_key
        }
        if base_url:
            langchain_kwargs["base_url"] = base_url
            
        self.llm = ChatOpenAI(**langchain_kwargs)
    
    async def analyze_comprehensive_structure(self, text: str) -> Dict[str, Any]:
        """Comprehensive GDPR document structure analysis"""
        
        messages = [
            {
                "role": "user",
                "content": f"""You are a legal expert analyzing GDPR documents. Perform comprehensive structural analysis.

Text: {text[:3000]}

Extract and analyze:
1. Document type (EU GDPR, UK GDPR, guidance, etc.)
2. ALL articles with their complete numbers, titles, and full content
3. ALL cross-references between articles (e.g., "as defined in Article X", "see Article Y")
4. Chapter structure with titles and article mappings
5. Legal concepts and their defining articles
6. Hierarchical relationships between provisions
7. Cross-reference patterns and linking structure

Provide comprehensive JSON structure:
{{
    "document_type": "gdpr_eu|gdpr_uk|guidance",
    "articles": [
        {{
            "number": "string",
            "title": "string",
            "full_content": "string",
            "sections": ["list of sections"],
            "cross_references": ["Article X", "Article Y"],
            "legal_concepts": ["concept1", "concept2"],
            "hierarchy_level": number
        }}
    ],
    "chapters": [
        {{
            "number": "string",
            "title": "string", 
            "articles": ["list of article numbers"]
        }}
    ],
    "cross_reference_map": {{
        "Article X": ["referenced by articles"],
        "Article Y": ["referenced by articles"]
    }},
    "legal_concepts": {{
        "concept_name": {{
            "definition": "string",
            "primary_article": "string",
            "related_articles": ["list"]
        }}
    }}
}}"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort=OPENAI_REASONING_EFFORT
            )
            
            import json
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return self._default_structure()
    
    async def extract_article_references(self, text: str) -> List[ArticleReference]:
        """Extract all article references from text"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract ALL article references from this GDPR text. Find every mention of other articles.

Text: {text[:1500]}

Look for patterns like:
- "Article 6" (direct reference)
- "as defined in Article 7"
- "pursuant to Article 8"
- "see Article 9(1)"
- "in accordance with Articles 10 and 11"
- "Article 12, paragraph 2"

For each reference, determine:
1. Referenced article number
2. Section/paragraph if specified
3. Context of the reference
4. Type of reference (definition, procedure, requirement, etc.)

Return JSON array:
[
    {{
        "article_number": "6",
        "section_reference": null,
        "paragraph_reference": null,
        "context": "legal basis for processing",
        "reference_type": "definition"
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            references_data = json.loads(response.choices[0].message.content)
            
            return [
                ArticleReference(
                    article_number=ref["article_number"],
                    article_title="",  # Will be filled later
                    section_reference=ref.get("section_reference"),
                    paragraph_reference=ref.get("paragraph_reference"),
                    context=ref["context"],
                    reference_type=ref["reference_type"]
                )
                for ref in references_data
            ]
            
        except Exception as e:
            logger.error(f"Error extracting references: {e}")
            return []
    
    async def extract_legal_concepts_comprehensive(self, text: str) -> List[LegalConcept]:
        """Extract comprehensive legal concepts with article mappings"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract legal concepts from GDPR text with comprehensive article mappings.

Text: {text[:1500]}

Identify:
1. Legal concepts (rights, obligations, processes, entities, principles)
2. Their definitions in the text
3. Primary article that defines each concept
4. Related articles that mention or expand on the concept

Focus on key GDPR concepts like:
- Rights (access, rectification, erasure, portability, etc.)
- Legal bases (consent, contract, legal obligation, etc.)
- Entities (controller, processor, supervisory authority, etc.)
- Processes (processing, profiling, automated decision-making)
- Principles (lawfulness, fairness, transparency, etc.)

Return JSON array:
[
    {{
        "concept_name": "right of access",
        "definition": "extracted definition",
        "primary_article": "15",
        "related_articles": ["12", "13", "14"]
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            concepts_data = json.loads(response.choices[0].message.content)
            
            return [
                LegalConcept(
                    concept_name=concept["concept_name"],
                    definition=concept["definition"],
                    primary_article=concept["primary_article"],
                    related_articles=concept.get("related_articles", [])
                )
                for concept in concepts_data
            ]
            
        except Exception as e:
            logger.error(f"Error extracting legal concepts: {e}")
            return []
    
    def _default_structure(self) -> Dict[str, Any]:
        """Default structure when analysis fails"""
        return {
            "document_type": "unknown",
            "articles": [],
            "chapters": [],
            "cross_reference_map": {},
            "legal_concepts": {}
        }

# =============================================================================
# FIXED ELASTICSEARCH SERVICE
# =============================================================================

class FixedElasticsearchService:
    """Fixed Elasticsearch service with real OpenAI embeddings"""
    
    def __init__(self, es_config: SimpleElasticsearchConfig, embedding_service: DirectOpenAIEmbeddingService):
        self.es_config = es_config
        self.embedding_service = embedding_service
        
        # LangChain ChatOpenAI
        llm_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": OPENAI_API_KEY
        }
        if OPENAI_BASE_URL:
            llm_kwargs["base_url"] = OPENAI_BASE_URL
            
        self.llm = ChatOpenAI(**llm_kwargs)
    
    def create_vectorstore(self, index_name: str = ELASTICSEARCH_INDEX_NAME) -> ElasticsearchStore:
        """Create ElasticsearchStore with real OpenAI embeddings"""
        
        # Create embedding wrapper that uses our DirectOpenAIEmbeddingService
        class RealOpenAIEmbedding:
            def __init__(self, embedding_service: DirectOpenAIEmbeddingService):
                self.embedding_service = embedding_service
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                """Synchronous wrapper for document embeddings"""
                # This is a sync method but we have async embedding service
                # We'll need to handle this in the calling code
                import asyncio
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # If loop is already running, we can't use run()
                    # This will be handled by the async calling code
                    return [[0.0] * 1536 for _ in texts]  # Placeholder
                else:
                    return loop.run_until_complete(self.embedding_service.embed_texts(texts))
            
            def embed_query(self, text: str) -> List[float]:
                """Synchronous wrapper for query embeddings"""
                import asyncio
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # This will be handled by the async calling code
                    return [0.0] * 1536  # Placeholder
                else:
                    return loop.run_until_complete(self.embedding_service.embed_text(text))
        
        real_embedding = RealOpenAIEmbedding(self.embedding_service)
        
        # Get Elasticsearch client
        es_client = self.es_config.create_client()
        
        # Create ElasticsearchStore with correct parameters - FIXED
        try:
            # Method 1: Use es_connection parameter
            vectorstore = ElasticsearchStore(
                index_name=index_name,
                embedding=real_embedding,
                es_connection=es_client
            )
            logger.info(f"âœ… Created ElasticsearchStore with real embeddings (Method 1)")
            return vectorstore
            
        except Exception as e1:
            logger.warning(f"Method 1 failed: {e1}, trying method 2...")
            try:
                # Method 2: Use individual parameters
                vectorstore = ElasticsearchStore(
                    index_name=index_name,
                    embedding=real_embedding,
                    es_url=f"http://{self.es_config.host}:{self.es_config.port}",
                    es_user=self.es_config.username,
                    es_password=self.es_config.password
                )
                logger.info(f"âœ… Created ElasticsearchStore with real embeddings (Method 2)")
                return vectorstore
                
            except Exception as e2:
                logger.warning(f"Method 2 failed: {e2}, trying method 3...")
                try:
                    # Method 3: Direct URL format
                    vectorstore = ElasticsearchStore.from_documents(
                        documents=[],  # Empty initially
                        embedding=real_embedding,
                        index_name=index_name,
                        es_url=f"http://{self.es_config.host}:{self.es_config.port}",
                        es_username=self.es_config.username,
                        es_password=self.es_config.password
                    )
                    logger.info(f"âœ… Created ElasticsearchStore with real embeddings (Method 3)")
                    return vectorstore
                    
                except Exception as e3:
                    logger.error(f"All methods failed. Last error: {e3}")
                    raise Exception(f"Cannot create ElasticsearchStore. Errors: {e1}, {e2}, {e3}")
    
    def create_comprehensive_qa_chains(self, vectorstore: ElasticsearchStore) -> Tuple[RetrievalQA, ConversationalRetrievalChain, ConversationSummaryBufferMemory]:
        """Create QA chains with comprehensive legal analysis prompts"""
        
        # Enhanced conversation memory
        conversation_memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=3000,
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Comprehensive QA prompt
        qa_prompt = PromptTemplate(
            template="""You are an expert GDPR legal analyst with comprehensive knowledge of all articles, cross-references, and legal concepts.

Context from documents: {context}

Question: {question}

Instructions for comprehensive analysis:
1. Cite specific GDPR articles, sections, and paragraphs with exact numbers
2. Include ALL relevant cross-references mentioned in the source articles
3. Explain relationships between different articles and provisions
4. Reference related legal concepts and their definitions
5. Mention any "as defined in Article X" or "see Article Y" references
6. Provide the full legal context with supporting and related provisions
7. If an article references procedures in other articles, include those details
8. Explain how different articles work together to address the query

Legal Analysis with Full Context and Cross-References:""",
            input_variables=["context", "question"]
        )
        
        # Enhanced QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": 25,  # More documents for comprehensive coverage
                }
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": qa_prompt}
        )
        
        # Conversational chain
        conversational_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 20}),
            memory=conversation_memory,
            return_source_documents=True,
            verbose=True
        )
        
        return qa_chain, conversational_chain, conversation_memory

# =============================================================================
# COMPREHENSIVE DOCUMENT PROCESSING AGENT
# =============================================================================

class ComprehensiveDocumentProcessor:
    """Enhanced document processor with comprehensive article analysis"""
    
    def __init__(self, 
                 reasoning_service: ComprehensiveReasoningService,
                 embedding_service: DirectOpenAIEmbeddingService):
        self.reasoning_service = reasoning_service
        self.embedding_service = embedding_service
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_documents_comprehensive(self, file_paths: List[str]) -> List[Document]:
        """Process documents with comprehensive article analysis and linking"""
        all_documents = []
        position_counter = 0
        
        print(f"ðŸ“š Processing {len(file_paths)} documents...")
        
        for file_path in tqdm(file_paths, desc="Processing files", unit="file"):
            try:
                print(f"\nðŸ“„ Processing: {file_path}")
                
                # Extract text
                text, page_info = self._extract_text_pymupdf(file_path)
                print(f"   ðŸ“– Extracted {len(text)} characters from {len(page_info)} pages")
                
                # Comprehensive structure analysis
                print(f"   ðŸ§  Analyzing document structure...")
                structure = await self.reasoning_service.analyze_comprehensive_structure(text)
                
                # Determine document type
                doc_type = self._determine_document_type(file_path, text, structure)
                print(f"   ðŸ“‹ Document type: {doc_type.value}")
                
                # Create comprehensive chunks
                print(f"   âœ‚ï¸ Creating comprehensive chunks...")
                chunks = await self._create_comprehensive_chunks(
                    text, doc_type, structure, file_path, page_info, position_counter
                )
                
                # Convert to LangChain Documents
                documents = self._chunks_to_documents(chunks)
                all_documents.extend(documents)
                
                position_counter += len(chunks)
                print(f"   âœ… Created {len(chunks)} comprehensive chunks")
                
            except Exception as e:
                print(f"   âŒ Error processing {file_path}: {e}")
                logger.error(f"Error processing {file_path}: {e}")
        
        print(f"\nâœ… Total processed: {len(all_documents)} document chunks")
        return all_documents
    
    def _extract_text_pymupdf(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text using PyMuPDF"""
        doc = pymupdf.open(file_path)
        text = ""
        page_info = {}
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            page_text = page.get_text()
            text += page_text + "\n"
            
            page_info[page_num] = {
                "text_length": len(page_text),
                "has_images": len(page.get_images()) > 0
            }
        
        doc.close()
        return text, page_info
    
    def _determine_document_type(self, file_path: str, text: str, structure: Dict) -> DocumentType:
        """Determine document type"""
        if structure.get("document_type") == "gdpr_uk":
            return DocumentType.GDPR_UK
        elif structure.get("document_type") == "gdpr_eu":
            return DocumentType.GDPR_EU
        
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in ["uk gdpr", "data protection act 2018", "ico"]):
            return DocumentType.GDPR_UK
        return DocumentType.GDPR_EU
    
    async def _create_comprehensive_chunks(self, text: str, doc_type: DocumentType, structure: Dict, 
                                         file_path: str, page_info: Dict, position_offset: int) -> List[ComprehensiveChunk]:
        """Create comprehensive chunks with full article context and linking"""
        chunks = []
        
        # Create article-based chunks first
        articles = structure.get("articles", [])
        article_chunks = await self._create_article_chunks(articles, doc_type, file_path, position_offset)
        chunks.extend(article_chunks)
        
        # Create standard text chunks for non-article content
        text_chunks = self.text_splitter.split_text(text)
        standard_chunks = await self._create_standard_chunks(
            text_chunks, doc_type, structure, file_path, page_info, 
            position_offset + len(article_chunks)
        )
        chunks.extend(standard_chunks)
        
        return chunks
    
    async def _create_article_chunks(self, articles: List[Dict], doc_type: DocumentType, 
                                   file_path: str, position_offset: int) -> List[ComprehensiveChunk]:
        """Create chunks for full articles with comprehensive linking"""
        chunks = []
        
        print(f"   ðŸ“„ Processing {len(articles)} articles...")
        
        for i, article in enumerate(tqdm(articles, desc="Processing articles", leave=False)):
            article_number = article.get("number", "")
            article_title = article.get("title", "")
            full_content = article.get("full_content", "")
            
            if not full_content:
                continue
            
            # Extract article references
            article_refs = await self.reasoning_service.extract_article_references(full_content)
            
            # Extract legal concepts
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(full_content)
            
            # Create main article chunk
            main_chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=ChunkType.FULL_ARTICLE,
                title=f"Article {article_number}: {article_title}",
                content=full_content,
                full_article_content=full_content,
                article_number=article_number,
                hierarchy_level=2,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=article.get("cross_references", []),
                keywords=self._extract_keywords(full_content),
                legal_terms=self._extract_legal_terms(full_content),
                metadata={
                    "source_file": file_path,
                    "article_sections": article.get("sections", []),
                    "is_full_article": True
                }
            )
            
            chunks.append(main_chunk)
            
            # Create section chunks if article has sections
            sections = article.get("sections", [])
            if sections:
                for j, section in enumerate(tqdm(sections, desc=f"Art {article_number} sections", leave=False)):
                    section_chunk = ComprehensiveChunk(
                        document_type=doc_type,
                        chunk_type=ChunkType.ARTICLE_SECTION,
                        title=f"Article {article_number}, Section {j+1}",
                        content=section,
                        full_article_content=full_content,
                        article_number=article_number,
                        section_number=str(j+1),
                        hierarchy_level=3,
                        position_in_document=position_offset + i + j + 1,
                        article_references=article_refs,
                        legal_concepts=legal_concepts,
                        metadata={
                            "source_file": file_path,
                            "parent_article_chunk_id": main_chunk.id,
                            "is_article_section": True
                        }
                    )
                    chunks.append(section_chunk)
        
        return chunks
    
    async def _create_standard_chunks(self, text_chunks: List[str], doc_type: DocumentType, 
                                    structure: Dict, file_path: str, page_info: Dict, 
                                    position_offset: int) -> List[ComprehensiveChunk]:
        """Create standard chunks for non-article content"""
        chunks = []
        
        print(f"   ðŸ“ Processing {len(text_chunks)} standard text chunks...")
        
        for i, chunk_text in enumerate(tqdm(text_chunks, desc="Creating chunks", leave=False)):
            # Extract references and concepts
            article_refs = await self.reasoning_service.extract_article_references(chunk_text)
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(chunk_text)
            
            # Determine chunk characteristics
            chunk_type = self._determine_chunk_type(chunk_text)
            title = self._extract_title(chunk_text)
            hierarchy_level = self._determine_hierarchy_level(chunk_text)
            article_number = self._extract_article_number(chunk_text)
            chapter_number = self._extract_chapter_number(chunk_text)
            
            chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=chunk_type,
                title=title,
                content=chunk_text,
                chapter_number=chapter_number,
                article_number=article_number,
                hierarchy_level=hierarchy_level,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=self._extract_cross_references(chunk_text),
                keywords=self._extract_keywords(chunk_text),
                legal_terms=self._extract_legal_terms(chunk_text),
                metadata={
                    "source_file": file_path,
                    "chunk_index": i,
                    "is_standard_chunk": True
                }
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _chunks_to_documents(self, chunks: List[ComprehensiveChunk]) -> List[Document]:
        """Convert comprehensive chunks to LangChain Documents"""
        documents = []
        
        for chunk in chunks:
            # Create enhanced content for embedding
            enhanced_content = self._create_enhanced_content(chunk)
            
            doc = Document(
                page_content=enhanced_content,
                metadata={
                    "chunk_id": chunk.id,
                    "document_type": chunk.document_type.value,
                    "chunk_type": chunk.chunk_type.value,
                    "title": chunk.title,
                    "original_content": chunk.content,
                    "full_article_content": chunk.full_article_content,
                    "article_number": chunk.article_number,
                    "chapter_number": chunk.chapter_number,
                    "section_number": chunk.section_number,
                    "hierarchy_level": chunk.hierarchy_level,
                    "position_in_document": chunk.position_in_document,
                    "article_references": [ref.dict() for ref in chunk.article_references],
                    "legal_concepts": [concept.dict() for concept in chunk.legal_concepts],
                    "cross_references": chunk.cross_references,
                    "keywords": chunk.keywords,
                    "legal_terms": chunk.legal_terms,
                    **chunk.metadata
                }
            )
            documents.append(doc)
        
        return documents
    
    def _create_enhanced_content(self, chunk: ComprehensiveChunk) -> str:
        """Create enhanced content for better embedding and retrieval"""
        parts = []
        
        # Title and article info
        parts.append(f"TITLE: {chunk.title}")
        
        if chunk.article_number:
            parts.append(f"ARTICLE: {chunk.article_number}")
        
        if chunk.chapter_number:
            parts.append(f"CHAPTER: {chunk.chapter_number}")
        
        # Main content
        parts.append(f"CONTENT: {chunk.content}")
        
        # Full article context if available
        if chunk.full_article_content and chunk.chunk_type != ChunkType.FULL_ARTICLE:
            parts.append(f"FULL ARTICLE CONTEXT: {chunk.full_article_content[:500]}...")
        
        # Article references
        if chunk.article_references:
            ref_text = ", ".join([f"Article {ref.article_number}" for ref in chunk.article_references])
            parts.append(f"REFERENCES: {ref_text}")
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_text = ", ".join([concept.concept_name for concept in chunk.legal_concepts])
            parts.append(f"LEGAL CONCEPTS: {concept_text}")
        
        # Cross-references
        if chunk.cross_references:
            parts.append(f"CROSS REFERENCES: {', '.join(chunk.cross_references)}")
        
        return "\n\n".join(parts)
    
    # Helper methods
    def _determine_chunk_type(self, text: str) -> ChunkType:
        text_lower = text.lower()
        if "article" in text_lower[:100]:
            return ChunkType.FULL_ARTICLE
        elif "chapter" in text_lower[:100]:
            return ChunkType.CHAPTER
        elif "section" in text_lower[:100]:
            return ChunkType.SECTION
        return ChunkType.PARAGRAPH
    
    def _extract_title(self, text: str) -> str:
        lines = text.split('\n')
        for line in lines[:3]:
            if line.strip() and len(line.strip()) < 150:
                return line.strip()
        return text[:80] + "..."
    
    def _determine_hierarchy_level(self, text: str) -> int:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return 1
        elif "article" in text_lower[:100]:
            return 2
        elif "section" in text_lower[:100]:
            return 3
        return 4
    
    def _extract_article_number(self, text: str) -> Optional[str]:
        match = re.search(r'article\s+(\d+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_chapter_number(self, text: str) -> Optional[str]:
        match = re.search(r'chapter\s+(\d+|[ivx]+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_cross_references(self, text: str) -> List[str]:
        """Extract cross-references like 'see Article X', 'as defined in Article Y'"""
        patterns = [
            r'see Article (\d+)',
            r'as defined in Article (\d+)',
            r'pursuant to Article (\d+)',
            r'in accordance with Article (\d+)',
            r'Article (\d+)\([^)]*\)',
        ]
        
        references = []
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                references.append(f"Article {match.group(1)}")
        
        return list(set(references))  # Remove duplicates
    
    def _extract_keywords(self, text: str) -> List[str]:
        gdpr_keywords = [
            "personal data", "data subject", "consent", "processing", "controller",
            "processor", "lawful basis", "legitimate interest", "data protection",
            "rights", "erasure", "rectification", "portability", "breach", "profiling",
            "automated decision-making", "supervisory authority", "cross-border processing"
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in gdpr_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _extract_legal_terms(self, text: str) -> List[str]:
        legal_terms = [
            "shall", "must", "may", "should", "obligation", "requirement",
            "prohibition", "permission", "duty", "responsibility", "liability",
            "compliance", "violation", "penalty", "fine", "sanction"
        ]
        
        found_terms = []
        text_lower = text.lower()
        for term in legal_terms:
            if term in text_lower:
                found_terms.append(term)
        
        return found_terms

# =============================================================================
# DEBUGGING AND TESTING UTILITIES
# =============================================================================

async def test_elasticsearch_connection():
    """Test Elasticsearch connection independently"""
    try:
        print("ðŸ” Testing Elasticsearch connection...")
        
        config = SimpleElasticsearchConfig()
        print(f"   Host: {config.host}:{config.port}")
        print(f"   Username: {config.username}")
        print(f"   URL: {config.get_elasticsearch_url()}")
        
        client = config.create_client()
        info = client.info()
        
        print(f"âœ… Connection successful!")
        print(f"   Version: {info.get('version', {}).get('number', 'unknown')}")
        print(f"   Cluster: {info.get('cluster_name', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Connection failed: {e}")
        print(f"   Error type: {type(e)}")
        return False

async def test_openai_connection():
    """Test OpenAI connection independently"""
    try:
        print("ðŸ¤– Testing OpenAI connection...")
        
        embedding_service = DirectOpenAIEmbeddingService()
        
        print("   ðŸ“¡ Testing embedding generation...")
        test_embedding = await embedding_service.embed_text("test text for GDPR processing")
        
        print(f"âœ… OpenAI connection successful!")
        print(f"   ðŸ“Š Embedding dimensions: {len(test_embedding)}")
        print(f"   ðŸ”¬ Embedding model: {OPENAI_EMBEDDING_MODEL}")
        
        return True
        
    except Exception as e:
        print(f"âŒ OpenAI connection failed: {e}")
        print(f"   ðŸ’¡ Check your OPENAI_API_KEY environment variable")
        return False

# =============================================================================
# MAIN COMPREHENSIVE SYSTEM
# =============================================================================

class ComprehensiveGDPRSystem:
    """Comprehensive GDPR system with enhanced article linking and direct embeddings"""
    
    def __init__(self, 
                 es_host: str = ELASTICSEARCH_HOST,
                 es_port: int = ELASTICSEARCH_PORT,
                 es_username: str = ELASTICSEARCH_USERNAME,
                 es_password: str = ELASTICSEARCH_PASSWORD):
        
        # Initialize services
        self.es_config = SimpleElasticsearchConfig(es_host, es_port, es_username, es_password)
        self.embedding_service = DirectOpenAIEmbeddingService()
        self.reasoning_service = ComprehensiveReasoningService()
        self.elasticsearch_service = FixedElasticsearchService(self.es_config, self.embedding_service)
        
        # Initialize document processor
        self.document_processor = ComprehensiveDocumentProcessor(
            self.reasoning_service, self.embedding_service
        )
        
        # Create workflow
        self.workflow = self._create_comprehensive_workflow()
    
    def _create_comprehensive_workflow(self) -> StateGraph:
        """Create comprehensive workflow"""
        
        workflow = StateGraph(ComprehensiveMemoryState)
        
        # Add nodes
        workflow.add_node("parse_comprehensive", self._parse_comprehensive_node)
        workflow.add_node("generate_direct_embeddings", self._generate_direct_embeddings_node)
        workflow.add_node("build_indices", self._build_indices_node)
        workflow.add_node("create_vectorstore", self._create_vectorstore_node)
        workflow.add_node("setup_qa_chains", self._setup_qa_chains_node)
        
        # Define edges
        workflow.add_edge(START, "parse_comprehensive")
        workflow.add_edge("parse_comprehensive", "generate_direct_embeddings")
        workflow.add_edge("generate_direct_embeddings", "build_indices")
        workflow.add_edge("build_indices", "create_vectorstore")
        workflow.add_edge("create_vectorstore", "setup_qa_chains")
        workflow.add_edge("setup_qa_chains", END)
        
        return workflow.compile()
    
    async def _parse_comprehensive_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Parse documents comprehensively"""
        state.current_agent = "comprehensive_parser"
        try:
            file_paths = []
            
            # FIXED: Safe document metadata access
            for doc in state.documents:
                if hasattr(doc, 'metadata') and doc.metadata and 'path' in doc.metadata:
                    file_paths.append(doc.metadata['path'])
                elif isinstance(doc, dict) and 'metadata' in doc and 'path' in doc['metadata']:
                    file_paths.append(doc['metadata']['path'])
                else:
                    logger.warning(f"Document missing path metadata: {doc}")
            
            if not file_paths:
                error_msg = "No valid file paths found in documents"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            documents = await self.document_processor.process_documents_comprehensive(file_paths)
            state.documents = documents
            
            # Convert to comprehensive chunks
            state.comprehensive_chunks = [
                self._document_to_comprehensive_chunk(doc) for doc in documents
            ]
            
            logger.info(f"Created {len(state.comprehensive_chunks)} comprehensive chunks")
            
        except Exception as e:
            error_msg = f"Comprehensive parsing error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _generate_direct_embeddings_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Generate embeddings using direct OpenAI API with progress tracking"""
        state.current_agent = "direct_embedding_generator"
        try:
            # Prepare enhanced texts for embedding
            enhanced_texts = []
            for chunk in state.comprehensive_chunks:
                enhanced_text = self._create_embedding_text(chunk)
                enhanced_texts.append(enhanced_text)
            
            print(f"\nðŸ”¢ Generating embeddings for {len(enhanced_texts)} chunks...")
            
            # Generate embeddings in batches with progress bar
            total_batches = (len(enhanced_texts) + BATCH_SIZE - 1) // BATCH_SIZE
            
            with tqdm(total=total_batches, desc="Embedding batches", unit="batch") as pbar:
                for i in range(0, len(enhanced_texts), BATCH_SIZE):
                    batch_texts = enhanced_texts[i:i + BATCH_SIZE]
                    batch_chunks = state.comprehensive_chunks[i:i + BATCH_SIZE]
                    
                    try:
                        embeddings = await self.embedding_service.embed_texts(batch_texts)
                        
                        for chunk, embedding in zip(batch_chunks, embeddings):
                            chunk.embedding = embedding
                        
                        pbar.set_postfix({
                            'batch': f"{i//BATCH_SIZE + 1}/{total_batches}",
                            'chunks': f"{len(batch_chunks)}"
                        })
                        pbar.update(1)
                        
                        await asyncio.sleep(0.5)  # Rate limiting
                        
                    except Exception as e:
                        logger.error(f"Error in embedding batch {i//BATCH_SIZE + 1}: {e}")
                        pbar.set_postfix({'error': f"batch {i//BATCH_SIZE + 1}"})
                        pbar.update(1)
            
            # Count successful embeddings
            embedded_count = sum(1 for chunk in state.comprehensive_chunks if chunk.embedding)
            print(f"âœ… Generated {embedded_count}/{len(state.comprehensive_chunks)} embeddings")
            
        except Exception as e:
            error_msg = f"Direct embedding error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _build_indices_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Build comprehensive indices with progress tracking"""
        state.current_agent = "index_builder"
        try:
            print(f"\nðŸ—‚ï¸ Building comprehensive indices...")
            
            # Build article index
            print(f"   ðŸ“„ Building article index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Article index", leave=False):
                if chunk.article_number:
                    state.article_index[chunk.article_number] = chunk.id
            
            # Build concept index
            print(f"   ðŸ§  Building legal concept index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Concept index", leave=False):
                for concept in chunk.legal_concepts:
                    if concept.concept_name not in state.concept_index:
                        state.concept_index[concept.concept_name] = []
                    state.concept_index[concept.concept_name].append(chunk.id)
            
            # Build cross-reference index
            print(f"   ðŸ”— Building cross-reference index...")
            for chunk in tqdm(state.comprehensive_chunks, desc="Cross-ref index", leave=False):
                for ref in chunk.cross_references:
                    if ref not in state.cross_reference_index:
                        state.cross_reference_index[ref] = []
                    state.cross_reference_index[ref].append(chunk.id)
            
            print(f"   âœ… Indices built:")
            print(f"      ðŸ“„ Articles: {len(state.article_index)}")
            print(f"      ðŸ§  Legal concepts: {len(state.concept_index)}")
            print(f"      ðŸ”— Cross-references: {len(state.cross_reference_index)}")
            
        except Exception as e:
            error_msg = f"Index building error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _create_vectorstore_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Create vector store with real embeddings"""
        state.current_agent = "vectorstore_creator"
        try:
            print(f"\nðŸ—„ï¸ Creating vectorstore...")
            logger.info(f"Elasticsearch config: {self.es_config.host}:{self.es_config.port}")
            
            # Test Elasticsearch connection first
            try:
                es_client = self.es_config.create_client()
                es_info = es_client.info()
                print(f"   âœ… Elasticsearch connected: v{es_info.get('version', {}).get('number', 'unknown')}")
            except Exception as es_error:
                error_msg = f"Elasticsearch connection failed: {str(es_error)}"
                logger.error(error_msg)
                state.errors.append(error_msg)
                return state
            
            vectorstore = self.elasticsearch_service.create_vectorstore()
            
            # Prepare documents with embeddings
            documents_with_embeddings = []
            print(f"   ðŸ“‹ Preparing documents for vectorstore...")
            
            for doc, chunk in tqdm(zip(state.documents, state.comprehensive_chunks), 
                                 desc="Preparing docs", total=len(state.documents)):
                if chunk.embedding and len(chunk.embedding) > 0:
                    # Add real embedding to document metadata
                    if hasattr(doc, 'metadata'):
                        doc.metadata["embedding"] = chunk.embedding
                        doc.metadata["embedding_model"] = OPENAI_EMBEDDING_MODEL
                        doc.metadata["embedding_dimensions"] = len(chunk.embedding)
                        documents_with_embeddings.append(doc)
                    else:
                        logger.warning(f"Document missing metadata attribute, skipping: {type(doc)}")
                        continue
                else:
                    logger.warning(f"Chunk missing embedding, skipping: {chunk.id}")
            
            print(f"   ðŸ“Š Adding {len(documents_with_embeddings)} documents to vectorstore...")
            
            # Add to vectorstore in batches with progress
            total_batches = (len(documents_with_embeddings) + BATCH_SIZE - 1) // BATCH_SIZE
            
            with tqdm(total=total_batches, desc="Adding to vectorstore", unit="batch") as pbar:
                for i in range(0, len(documents_with_embeddings), BATCH_SIZE):
                    batch = documents_with_embeddings[i:i + BATCH_SIZE]
                    try:
                        # Create documents with pre-computed embeddings
                        batch_docs_with_embeddings = []
                        for doc in batch:
                            # Create a new document that includes the embedding
                            doc_with_embedding = Document(
                                page_content=doc.page_content,
                                metadata={**doc.metadata, "vector": doc.metadata["embedding"]}
                            )
                            batch_docs_with_embeddings.append(doc_with_embedding)
                        
                        # Try async method first
                        if hasattr(vectorstore, 'aadd_documents'):
                            await vectorstore.aadd_documents(batch_docs_with_embeddings)
                        else:
                            # Fallback to sync method
                            vectorstore.add_documents(batch_docs_with_embeddings)
                        
                        pbar.set_postfix({
                            'batch': f"{i//BATCH_SIZE + 1}/{total_batches}",
                            'docs': f"{len(batch)}"
                        })
                        pbar.update(1)
                        
                    except Exception as batch_error:
                        logger.error(f"Error adding batch {i//BATCH_SIZE + 1}: {batch_error}")
                        pbar.set_postfix({'error': f"batch {i//BATCH_SIZE + 1}"})
                        pbar.update(1)
            
            state.vectorstore = vectorstore
            print(f"   âœ… Vectorstore created successfully with real OpenAI embeddings")
            
        except Exception as e:
            error_msg = f"Vectorstore creation error: {str(e)}"
            logger.error(error_msg)
            logger.error(f"Error type: {type(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            state.errors.append(error_msg)
        return state
    
    async def _setup_qa_chains_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Setup comprehensive QA chains"""
        state.current_agent = "qa_chain_creator"
        try:
            if state.vectorstore:
                qa_chain, conv_chain, memory = self.elasticsearch_service.create_comprehensive_qa_chains(state.vectorstore)
                state.qa_chain = qa_chain
                state.conversational_chain = conv_chain
                state.conversation_memory = memory
                
                logger.info("Created comprehensive QA chains")
                
        except Exception as e:
            error_msg = f"QA chain setup error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    def _document_to_comprehensive_chunk(self, doc: Document) -> ComprehensiveChunk:
        """Convert Document to ComprehensiveChunk"""
        
        # Extract article references from metadata
        article_refs = []
        if "article_references" in doc.metadata:
            for ref_data in doc.metadata["article_references"]:
                article_refs.append(ArticleReference(**ref_data))
        
        # Extract legal concepts from metadata
        legal_concepts = []
        if "legal_concepts" in doc.metadata:
            for concept_data in doc.metadata["legal_concepts"]:
                legal_concepts.append(LegalConcept(**concept_data))
        
        return ComprehensiveChunk(
            id=doc.metadata.get("chunk_id", str(uuid4())),
            document_type=DocumentType(doc.metadata["document_type"]),
            chunk_type=ChunkType(doc.metadata["chunk_type"]),
            title=doc.metadata["title"],
            content=doc.metadata.get("original_content", doc.page_content),
            full_article_content=doc.metadata.get("full_article_content"),
            article_number=doc.metadata.get("article_number"),
            chapter_number=doc.metadata.get("chapter_number"),
            section_number=doc.metadata.get("section_number"),
            hierarchy_level=doc.metadata["hierarchy_level"],
            position_in_document=doc.metadata["position_in_document"],
            article_references=article_refs,
            legal_concepts=legal_concepts,
            cross_references=doc.metadata.get("cross_references", []),
            keywords=doc.metadata.get("keywords", []),
            legal_terms=doc.metadata.get("legal_terms", []),
            metadata=doc.metadata
        )
    
    def _create_embedding_text(self, chunk: ComprehensiveChunk) -> str:
        """Create optimized text for embedding"""
        parts = []
        
        # Core information
        parts.append(f"Document Type: {chunk.document_type.value}")
        parts.append(f"Title: {chunk.title}")
        
        # Article information
        if chunk.article_number:
            parts.append(f"Article {chunk.article_number}")
        
        # Main content
        parts.append(chunk.content)
        
        # Full article context for sections
        if chunk.full_article_content and chunk.chunk_type == ChunkType.ARTICLE_SECTION:
            parts.append(f"Full Article Context: {chunk.full_article_content}")
        
        # Referenced articles
        if chunk.article_references:
            ref_info = []
            for ref in chunk.article_references:
                ref_info.append(f"Article {ref.article_number} ({ref.reference_type}): {ref.context}")
            parts.append("Referenced Articles: " + "; ".join(ref_info))
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_info = []
            for concept in chunk.legal_concepts:
                concept_info.append(f"{concept.concept_name}: {concept.definition}")
            parts.append("Legal Concepts: " + "; ".join(concept_info))
        
        return "\n\n".join(parts)
    
    # Public API
    async def process_documents(self, file_paths: List[str]) -> ComprehensiveMemoryState:
        """Process documents comprehensively"""
        
        # FIXED: Proper state initialization with metadata
        initial_state = ComprehensiveMemoryState(
            documents=[
                Document(page_content="", metadata={"path": path}) 
                for path in file_paths
            ]
        )
        
        # Initialize metadata separately to avoid AddableValuesDict issues
        initial_state.metadata = {
            "started_at": datetime.now(),
            "file_paths": file_paths
        }
        
        try:
            final_state = await self.workflow.ainvoke(initial_state)
            
            # FIXED: Safe metadata access
            if hasattr(final_state, 'metadata') and final_state.metadata:
                final_state.metadata["completed_at"] = datetime.now()
                final_state.metadata["duration"] = (
                    final_state.metadata["completed_at"] - final_state.metadata["started_at"]
                ).total_seconds()
            else:
                # Create metadata if it doesn't exist
                final_state.metadata = {
                    "started_at": initial_state.metadata["started_at"],
                    "completed_at": datetime.now(),
                    "file_paths": file_paths
                }
                final_state.metadata["duration"] = (
                    final_state.metadata["completed_at"] - final_state.metadata["started_at"]
                ).total_seconds()
            
            logger.info(f"Comprehensive processing completed in {final_state.metadata['duration']:.2f} seconds")
            return final_state
            
        except Exception as e:
            error_msg = f"Comprehensive workflow error: {str(e)}"
            logger.error(error_msg)
            initial_state.errors.append(error_msg)
            return initial_state
    
    async def ask_comprehensive(self, state: ComprehensiveMemoryState, question: str) -> Dict[str, Any]:
        """Ask question with comprehensive analysis"""
        
        if not state.qa_chain:
            return {"error": "QA chain not available"}
        
        try:
            result = await state.qa_chain.ainvoke({"query": question})
            
            # Enhanced response with related information
            response = {
                "answer": result["result"],
                "source_documents": [],
                "related_articles": [],
                "legal_concepts": [],
                "cross_references": []
            }
            
            # Process source documents
            for doc in result.get("source_documents", []):
                source_info = {
                    "content": doc.page_content[:400] + "...",
                    "article_number": doc.metadata.get("article_number"),
                    "title": doc.metadata.get("title"),
                    "chunk_type": doc.metadata.get("chunk_type"),
                    "metadata": doc.metadata
                }
                response["source_documents"].append(source_info)
                
                # Extract related articles
                if doc.metadata.get("article_references"):
                    for ref in doc.metadata["article_references"]:
                        if ref not in response["related_articles"]:
                            response["related_articles"].append(ref)
                
                # Extract legal concepts
                if doc.metadata.get("legal_concepts"):
                    for concept in doc.metadata["legal_concepts"]:
                        if concept not in response["legal_concepts"]:
                            response["legal_concepts"].append(concept)
                
                # Extract cross-references
                if doc.metadata.get("cross_references"):
                    response["cross_references"].extend(doc.metadata["cross_references"])
            
            # Remove duplicates
            response["cross_references"] = list(set(response["cross_references"]))
            
            return response
            
        except Exception as e:
            return {"error": f"Comprehensive QA error: {str(e)}"}

# =============================================================================
# FIXED EXAMPLE USAGE
# =============================================================================

async def main():
    """Production example with comprehensive GDPR analysis and progress tracking"""
    
    try:
        print("ðŸ§  Comprehensive GDPR System with Real OpenAI Embeddings")
        print("=" * 60)
        print(f"ðŸ“Š Configuration:")
        print(f"   ðŸ¤– OpenAI Model: {OPENAI_MODEL}")
        print(f"   ðŸ“„ Embedding Model: {OPENAI_EMBEDDING_MODEL}")
        print(f"   ðŸ” Elasticsearch: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
        print(f"   ðŸ‘¤ Username: {ELASTICSEARCH_USERNAME}")
        print(f"   ðŸ”’ SSL: {ELASTICSEARCH_USE_SSL}")
        print(f"   ðŸ“¦ Batch Size: {BATCH_SIZE}")
        print()
        
        # Test connections first
        print("ðŸ”§ Testing connections...")
        print("-" * 30)
        
        connection_tests = [
            ("Elasticsearch", test_elasticsearch_connection()),
            ("OpenAI API", test_openai_connection())
        ]
        
        for test_name, test_coro in connection_tests:
            success = await test_coro
            if not success:
                print(f"âŒ {test_name} connection failed. Cannot proceed.")
                return
        
        print("\nâœ… All connections successful!")
        print("=" * 60)
        
        # Initialize system
        print("ðŸš€ Initializing Comprehensive GDPR System...")
        gdpr_system = ComprehensiveGDPRSystem(
            es_host=ELASTICSEARCH_HOST,
            es_port=ELASTICSEARCH_PORT,
            es_username=ELASTICSEARCH_USERNAME,
            es_password=ELASTICSEARCH_PASSWORD
        )
        
        # Document paths (update these to your actual files)
        document_paths = [
            "path/to/gdpr_regulation.pdf",
            "path/to/gdpr_guidance.pdf"
        ]
        
        print(f"\nðŸ“š Starting comprehensive document processing...")
        print(f"   ðŸ“ Documents to process: {len(document_paths)}")
        for i, path in enumerate(document_paths, 1):
            print(f"   {i}. {path}")
        print()
        
        # Process documents with workflow progress
        workflow_steps = [
            "ðŸ“„ Parsing documents with structure analysis",
            "ðŸ”¢ Generating OpenAI embeddings", 
            "ðŸ—‚ï¸ Building comprehensive indices",
            "ðŸ—„ï¸ Creating vectorstore with real embeddings",
            "ðŸ¤– Setting up QA chains"
        ]
        
        print("ðŸ”„ Workflow Progress:")
        for i, step in enumerate(workflow_steps, 1):
            print(f"   {i}. {step}")
        print()
        
        # Start processing
        start_time = datetime.now()
        print(f"â° Started at: {start_time.strftime('%H:%M:%S')}")
        print("-" * 60)
        
        state = await gdpr_system.process_documents(document_paths)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print("-" * 60)
        print(f"âœ… Processing completed in {duration:.1f} seconds!")
        print("\nðŸ“Š Results Summary:")
        print(f"   ðŸ“„ Total chunks processed: {len(state.comprehensive_chunks)}")
        print(f"   ðŸ“‡ Articles indexed: {len(state.article_index)}")
        print(f"   ðŸ§  Legal concepts mapped: {len(state.concept_index)}")
        print(f"   ðŸ”— Cross-references found: {len(state.cross_reference_index)}")
        
        # Show embedding statistics
        embedded_chunks = sum(1 for chunk in state.comprehensive_chunks if chunk.embedding)
        print(f"   ðŸ”¢ Embeddings generated: {embedded_chunks}/{len(state.comprehensive_chunks)}")
        
        if state.errors:
            print(f"\nâš ï¸ Errors encountered ({len(state.errors)}):")
            for i, error in enumerate(state.errors, 1):
                print(f"   {i}. {error}")
        
        # Test comprehensive QA
        if state.qa_chain:
            print("\n" + "=" * 60)
            print("ðŸ¤– Testing Comprehensive QA with Real Embeddings")
            print("-" * 60)
            
            test_questions = [
                "What are the lawful bases for processing personal data under Article 6, and how do they relate to consent requirements?",
                "What rights do data subjects have under GDPR and which articles define them?",
                "What are the obligations of data controllers regarding data protection impact assessments?"
            ]
            
            for i, question in enumerate(test_questions, 1):
                print(f"\nâ“ Question {i}:")
                print(f"   {question}")
                
                try:
                    answer = await gdpr_system.ask_comprehensive(state, question)
                    
                    if "error" not in answer:
                        print(f"\nðŸ’¡ Answer:")
                        print(f"   {answer['answer'][:400]}...")
                        print(f"\nðŸ“Š Context:")
                        print(f"   ðŸ“š Source documents: {len(answer['source_documents'])}")
                        print(f"   ðŸ”— Related articles: {len(answer['related_articles'])}")
                        print(f"   ðŸ§  Legal concepts: {len(answer['legal_concepts'])}")
                        print(f"   ðŸ“Ž Cross-references: {len(answer['cross_references'])}")
                        
                        if answer['cross_references']:
                            print(f"   ðŸ“Ž Sample cross-refs: {answer['cross_references'][:3]}")
                    else:
                        print(f"   âŒ Error: {answer['error']}")
                        
                except Exception as e:
                    print(f"   âŒ Error asking question: {e}")
                
                if i < len(test_questions):
                    print("\n" + "-" * 40)
        else:
            print("\nâš ï¸ QA chain not available - check vectorstore creation")
        
        print("\n" + "=" * 60)
        print("ðŸŽ‰ Comprehensive GDPR System Test Complete!")
        
    except Exception as e:
        print(f"\nâŒ Fatal Error: {e}")
        import traceback
        print(f"ðŸ” Traceback:")
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
