"""
Multi-Agent GDPR Document Processing System with Advanced HNSW and Elasticsearch

This system processes GDPR and UK GDPR PDF documents, creates vector embeddings,
builds graph structures, and maintains long-term memory for cross-document linking.

Features:
- Modern PyMuPDF 1.26+ text extraction from PDF files
- Automatic document type detection (GDPR/UK_GDPR)
- Batch processing of multiple PDF files from data directory
- Advanced HNSW (Hierarchical Navigable Small World) vector indexing
- Scalar quantization (int8_hnsw) for 75% memory reduction
- Dual embedding strategy (full articles + chunks)
- Graph-based cross-document relationships
- Long-term memory with LangGraph persistence
- Hybrid search (text + vector) with rescoring
- Performance optimization and monitoring

HNSW Optimizations:
- int8_hnsw index type with automatic scalar quantization
- Configurable M and ef_construction parameters
- Query parallelization and segment optimization
- Filesystem cache preloading for vector files
- Automatic index optimization and merge policies

Performance Benefits:
- 75% memory reduction with int8 quantization
- Sub-millisecond vector similarity computations
- Logarithmic search complexity with HNSW
- Multi-threaded graph building and search

Requirements:
- Python 3.9+
- PyMuPDF 1.26+ (pip install pymupdf)
- OpenAI API access
- Elasticsearch 8.13+
- Create a 'data' directory in the project root
- Add PDF files (GDPR documents) to the data directory
- Files will be automatically detected and processed
- Document types are auto-determined based on filename and content

Installation:
pip install pymupdf openai elasticsearch pydantic langchain langgraph
"""

import asyncio
import json
import logging
import os
import ssl
import uuid
import glob
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Tuple
from urllib.parse import quote_plus

import openai
import pymupdf  # Modern PyMuPDF import (not fitz)
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from pydantic import BaseModel, Field, ConfigDict
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import ToolNode


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
    ES_PASSWORD = os.getenv("ES_PASSWORD")
    ES_HOST = os.getenv("ES_HOST", "localhost")
    ES_PORT = int(os.getenv("ES_PORT", "9200"))
    ES_CACERT_PATH = os.getenv("ES_CACERT_PATH", "cacert.crt")
    
    # Model configurations
    O3_MINI_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS = 3072
    REASONING_EFFORT = "high"
    
    # Token management to prevent data loss
    MAX_TOKENS_PER_REQUEST = 50000  # Conservative limit for o3-mini
    OVERLAP_TOKENS = 500  # Overlap between chunks to maintain context
    
    # HNSW & Quantization configurations
    # Using int8_hnsw for 75% memory reduction with minimal accuracy loss
    VECTOR_INDEX_TYPE = "int8_hnsw"  # Auto scalar quantization
    HNSW_M = 16  # Number of bidirectional links for each node (balance between speed/accuracy)
    HNSW_EF_CONSTRUCTION = 200  # Size of dynamic candidate list (higher = better accuracy, slower indexing)
    CONFIDENCE_INTERVAL = 0.95  # For int8 quantization confidence
    
    # Performance optimizations
    ENABLE_PRELOAD = True  # Filesystem cache preloading
    MAX_SEGMENTS = 10  # Merge policy for optimal HNSW performance


# Pydantic Models for Document Structure
class ChapterReference(BaseModel):
    """Reference to a chapter in a document"""
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None, description="Article identifier")
    title: str = Field(description="Chapter/Article title")
    relevance_score: float = Field(description="Relevance score 0-1")
    relationship_type: str = Field(description="Type of relationship (supports, contradicts, references, etc.)")


class FullArticle(BaseModel):
    """Complete article with full content embedding"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    article_id: str = Field(description="Unique article identifier")
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: str = Field(description="Article identifier")
    title: str = Field(description="Full article title")
    full_content: str = Field(description="Complete article text")
    full_article_embedding: List[float] = Field(description="Embedding of entire article")
    chunk_ids: List[str] = Field(description="IDs of chunks belonging to this article")
    key_concepts: List[str] = Field(default_factory=list, description="Key legal concepts")
    created_at: datetime = Field(default_factory=datetime.now)


class DocumentChunk(BaseModel):
    """Individual document chunk with metadata"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    chunk_id: str = Field(description="Unique chunk identifier")
    parent_article_id: Optional[str] = Field(default=None, description="Parent article ID")
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None)
    title: str = Field(description="Section title")
    content: str = Field(description="Text content")
    chunk_embedding: Optional[List[float]] = Field(default=None, description="Chunk-level vector embedding")
    supporting_references: List[ChapterReference] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.now)
    processed_by_agent: Optional[str] = Field(default=None)


class CrossDocumentLink(BaseModel):
    """Cross-document relationship"""
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: str
    confidence_score: float
    created_at: datetime = Field(default_factory=datetime.now)


class RawDocument(BaseModel):
    """Raw extracted document for data loss prevention"""
    raw_id: str = Field(description="Unique raw document identifier")
    file_path: str = Field(description="Original file path")
    document_type: str = Field(description="GDPR or UK_GDPR")
    raw_text: str = Field(description="Complete extracted text")
    page_count: int = Field(description="Number of pages in PDF")
    character_count: int = Field(description="Total characters extracted")
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)


class ProcessingValidation(BaseModel):
    """Validation results for processing completeness"""
    total_input_chars: int
    total_processed_chars: int
    coverage_percentage: float
    missing_sections: List[str] = Field(default_factory=list)
    validation_passed: bool
    notes: str = ""
    """Long-term memory structure for agents"""
    memory_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    agent_name: str
    memory_type: str  # semantic, episodic, procedural
    content: Dict[str, Any]
    namespace: List[str]
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)


# State Management
class ProcessingState(TypedDict):
    """State for the processing workflow"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    raw_documents: List[RawDocument]
    full_articles: List[FullArticle]
    documents: List[DocumentChunk]
    current_chunk: Optional[DocumentChunk]
    cross_links: List[CrossDocumentLink]
    processing_stage: str
    agent_memories: List[AgentMemory]
    validation_results: List[ProcessingValidation]
    elasticsearch_client: Optional[Any]


# Elasticsearch Manager
class ElasticsearchManager:
    """Manages Elasticsearch operations with SSL authentication and HNSW optimization"""
    
    def __init__(self):
        self.client = self._create_client()
        self._setup_indices()
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client with SSL configuration"""
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        if os.path.exists(Config.ES_CACERT_PATH):
            ssl_context.load_verify_locations(Config.ES_CACERT_PATH)
        
        return Elasticsearch(
            [{"host": Config.ES_HOST, "port": Config.ES_PORT, "scheme": "https"}],
            basic_auth=(Config.ES_USERNAME, Config.ES_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True,
            request_timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
    
    def _setup_indices(self):
        """Setup Elasticsearch indices with advanced HNSW and quantization configurations"""
        
        # Advanced HNSW vector field configuration
        def create_vector_field_config(field_name: str) -> Dict:
            """Create optimized vector field configuration with HNSW and quantization"""
            return {
                "type": "dense_vector",
                "dims": Config.EMBEDDING_DIMENSIONS,
                "index": True,
                "similarity": "cosine",
                "index_options": {
                    "type": Config.VECTOR_INDEX_TYPE,  # int8_hnsw for scalar quantization
                    "m": Config.HNSW_M,
                    "ef_construction": Config.HNSW_EF_CONSTRUCTION,
                    "confidence_interval": Config.CONFIDENCE_INTERVAL
                }
            }
        
        # Full articles index with advanced HNSW
        articles_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    # Merge policy optimization for HNSW performance
                    "merge.policy.max_merged_segment": "5gb",
                    "merge.policy.segments_per_tier": 4,
                    # Preload vector files for faster search
                    "store.preload": ["vec", "vem", "nvd", "nvm"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "full_content": {"type": "text", "analyzer": "standard"},
                    "full_article_embedding": create_vector_field_config("full_article_embedding"),
                    "chunk_ids": {"type": "keyword"},
                    "key_concepts": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Document chunks index with advanced HNSW
        chunks_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    # Optimized for frequent updates during cross-referencing
                    "refresh_interval": "5s",
                    "merge.policy.max_merged_segment": "2gb",
                    "merge.policy.segments_per_tier": 4,
                    # Preload quantized vector files for optimal performance
                    "store.preload": ["veq", "vemq"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "chunk_id": {"type": "keyword"},
                    "parent_article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "chunk_embedding": create_vector_field_config("chunk_embedding"),
                    "supporting_references": {
                        "type": "nested",
                        "properties": {
                            "document_type": {"type": "keyword"},
                            "chapter_number": {"type": "keyword"},
                            "article_number": {"type": "keyword"},
                            "title": {"type": "text"},
                            "relevance_score": {"type": "float"},
                            "relationship_type": {"type": "keyword"}
                        }
                    },
                    "created_at": {"type": "date"},
                    "processed_by_agent": {"type": "keyword"}
                }
            }
        }
        
        # Cross-document links index (optimized for graph traversal)
        links_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "1s",  # Fast updates for real-time graph building
                }
            },
            "mappings": {
                "properties": {
                    "source_chunk_id": {"type": "keyword"},
                    "target_chunk_id": {"type": "keyword"},
                    "relationship_type": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Raw documents index (for data loss prevention)
        raw_docs_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                }
            },
            "mappings": {
                "properties": {
                    "raw_id": {"type": "keyword"},
                    "file_path": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "raw_text": {"type": "text", "analyzer": "standard"},
                    "page_count": {"type": "integer"},
                    "character_count": {"type": "integer"},
                    "extraction_metadata": {"type": "object"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Agent memories index with semantic search capabilities
        memories_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                }
            },
            "mappings": {
                "properties": {
                    "memory_id": {"type": "keyword"},
                    "agent_name": {"type": "keyword"},
                    "memory_type": {"type": "keyword"},
                    "content": {"type": "object"},
                    "namespace": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
        
        # Create indices with advanced configurations
        indices = {
            "gdpr_raw_docs": raw_docs_mapping,
            "gdpr_articles": articles_mapping,
            "gdpr_chunks": chunks_mapping,
            "gdpr_links": links_mapping,
            "agent_memories": memories_mapping
        }
        
        for index_name, mapping in indices.items():
            if not self.client.indices.exists(index=index_name):
                self.client.indices.create(index=index_name, body=mapping)
                logger.info(f"Created index with HNSW optimization: {index_name}")
                
                # Verify HNSW configuration
                index_info = self.client.indices.get_mapping(index=index_name)
                logger.info(f"HNSW config verified for {index_name}: {Config.VECTOR_INDEX_TYPE}")
            else:
                logger.info(f"Index already exists: {index_name}")
    
    def index_raw_document(self, raw_doc: RawDocument) -> bool:
        """Index raw extracted document for data loss prevention"""
        try:
            doc = raw_doc.model_dump()
            doc["created_at"] = raw_doc.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_raw_docs",
                id=raw_doc.raw_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing raw document {raw_doc.raw_id}: {e}")
            raise
    
    def index_article(self, article: FullArticle) -> bool:
        """Index a full article"""
        try:
            doc = article.model_dump()
            doc["created_at"] = article.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_articles",
                id=article.article_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing article {article.article_id}: {e}")
            raise
    
    def index_chunk(self, chunk: DocumentChunk) -> bool:
        """Index a document chunk"""
        try:
            doc = chunk.model_dump()
            doc["created_at"] = chunk.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_chunks",
                id=chunk.chunk_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing chunk {chunk.chunk_id}: {e}")
            raise
    
    def index_link(self, link: CrossDocumentLink) -> bool:
        """Index a cross-document link"""
        try:
            doc = link.model_dump()
            doc["created_at"] = link.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_links",
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing link: {e}")
            raise
    
    def index_memory(self, memory: AgentMemory) -> bool:
        """Index agent memory"""
        try:
            doc = memory.model_dump()
            doc["created_at"] = memory.created_at.isoformat()
            doc["updated_at"] = memory.updated_at.isoformat()
            
            response = self.client.index(
                index="agent_memories",
                id=memory.memory_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing memory {memory.memory_id}: {e}")
            raise
    
    def hybrid_search(self, query: str, embedding: List[float], filters: Dict = None, 
                     search_level: str = "both", rescore: bool = True) -> Dict[str, List[Dict]]:
        """
        Perform optimized hybrid search with advanced HNSW and quantization features
        
        Args:
            query: Text query
            embedding: Query embedding vector
            filters: Search filters
            search_level: "articles", "chunks", or "both"
            rescore: Enable rescoring for quantized vectors (improves accuracy)
        
        Returns:
            Dict with "articles" and/or "chunks" keys containing results
        """
        results = {}
        
        try:
            # Search articles
            if search_level in ["articles", "both"]:
                article_search = {
                    "knn": {
                        "field": "full_article_embedding",
                        "query_vector": embedding,
                        "k": 10,
                        "num_candidates": 50,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^3", "full_content^2", "key_concepts^2"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 20,
                }
                
                # Add rescoring for quantized vectors
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    article_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'full_article_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                article_response = self.client.search(index="gdpr_articles", body=article_search)
                results["articles"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in article_response["hits"]["hits"]
                ]
            
            # Search chunks
            if search_level in ["chunks", "both"]:
                chunk_search = {
                    "knn": {
                        "field": "chunk_embedding",
                        "query_vector": embedding,
                        "k": 10,
                        "num_candidates": 50,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^2", "content^1.5"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 20,
                }
                
                # Add rescoring for quantized vectors
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    chunk_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'chunk_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                chunk_response = self.client.search(index="gdpr_chunks", body=chunk_search)
                results["chunks"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in chunk_response["hits"]["hits"]
                ]
            
            return results
            
        except Exception as e:
            logger.error(f"Error in optimized hybrid search: {e}")
            raise
    
    def get_related_chunks(self, chunk_id: str) -> List[Dict]:
        """Get chunks related through graph links"""
        try:
            # Find outgoing links
            outgoing_query = {
                "query": {"term": {"source_chunk_id": chunk_id}},
                "size": 50
            }
            
            # Find incoming links
            incoming_query = {
                "query": {"term": {"target_chunk_id": chunk_id}},
                "size": 50
            }
            
            outgoing_response = self.client.search(index="gdpr_links", body=outgoing_query)
            incoming_response = self.client.search(index="gdpr_links", body=incoming_query)
            
            related_chunk_ids = set()
            for hit in outgoing_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["target_chunk_id"])
            for hit in incoming_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["source_chunk_id"])
            
            if not related_chunk_ids:
                return []
            
            # Get the actual chunks
            chunks_query = {
                "query": {"terms": {"chunk_id": list(related_chunk_ids)}},
                "size": len(related_chunk_ids)
            }
            
            chunks_response = self.client.search(index="gdpr_chunks", body=chunks_query)
            return [hit["_source"] for hit in chunks_response["hits"]["hits"]]
            
        except Exception as e:
            logger.error(f"Error getting related chunks for {chunk_id}: {e}")
            raise
    
    def validate_processing_completeness(self, raw_doc: RawDocument, 
                                        articles: List[FullArticle], 
                                        chunks: List[DocumentChunk]) -> ProcessingValidation:
        """Validate that processing captured all content without data loss"""
        try:
            total_input_chars = len(raw_doc.raw_text)
            
            # Calculate total processed characters
            article_chars = sum(len(article.full_content) for article in articles)
            chunk_chars = sum(len(chunk.content) for chunk in chunks)
            
            # Use the higher of article or chunk content (avoid double counting)
            total_processed_chars = max(article_chars, chunk_chars)
            
            coverage_percentage = (total_processed_chars / total_input_chars) * 100 if total_input_chars > 0 else 0
            
            # Define acceptable coverage threshold
            ACCEPTABLE_COVERAGE = 85.0  # 85% coverage is acceptable due to formatting, headers, footers
            validation_passed = coverage_percentage >= ACCEPTABLE_COVERAGE
            
            missing_sections = []
            notes = f"Processed {total_processed_chars:,} chars out of {total_input_chars:,} chars"
            
            if not validation_passed:
                notes += f" - Coverage below threshold ({ACCEPTABLE_COVERAGE}%)"
                # Simple heuristic to identify potential missing sections
                if len(articles) == 0:
                    missing_sections.append("No articles identified")
                if len(chunks) < 5:
                    missing_sections.append("Very few chunks created")
            
            return ProcessingValidation(
                total_input_chars=total_input_chars,
                total_processed_chars=total_processed_chars,
                coverage_percentage=coverage_percentage,
                missing_sections=missing_sections,
                validation_passed=validation_passed,
                notes=notes
            )
            
        except Exception as e:
            logger.error(f"Error validating processing completeness: {e}")
            return ProcessingValidation(
                total_input_chars=0,
                total_processed_chars=0,
                coverage_percentage=0.0,
                missing_sections=["Validation failed"],
                validation_passed=False,
                notes=f"Validation error: {e}"
            )
    
    def get_index_stats(self) -> Dict[str, Any]:
        """Get comprehensive index statistics including HNSW performance metrics"""
        try:
            stats = {}
            
            for index_name in ["gdpr_raw_docs", "gdpr_articles", "gdpr_chunks", "gdpr_links", "agent_memories"]:
                if self.client.indices.exists(index=index_name):
                    # Basic index stats
                    index_stats = self.client.indices.stats(index=index_name)
                    
                    stats[index_name] = {
                        "documents": index_stats["indices"][index_name]["total"]["docs"]["count"],
                        "size_bytes": index_stats["indices"][index_name]["total"]["store"]["size_in_bytes"],
                        "segments": index_stats["indices"][index_name]["total"]["segments"]["count"],
                    }
                    
                    # Add vector memory estimation for indices with embeddings
                    if index_name in ["gdpr_articles", "gdpr_chunks"]:
                        stats[index_name]["vector_size_estimate"] = self._estimate_vector_memory_usage(index_name)
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting index stats: {e}")
            raise
    
    def _estimate_vector_memory_usage(self, index_name: str) -> Dict[str, str]:
        """Estimate memory usage for vectors with quantization"""
        try:
            doc_count_response = self.client.count(index=index_name)
            doc_count = doc_count_response["count"]
            
            if doc_count == 0:
                return {"estimated_memory": "0 MB", "quantization_savings": "N/A"}
            
            # Memory calculation based on quantization type
            if Config.VECTOR_INDEX_TYPE == "int8_hnsw":
                # int8 quantization: ~75% memory reduction
                vector_memory = doc_count * (Config.EMBEDDING_DIMENSIONS + 4) * 1  # 1 byte per dimension + metadata
                original_memory = doc_count * Config.EMBEDDING_DIMENSIONS * 4  # float32
                savings = ((original_memory - vector_memory) / original_memory) * 100
                
            elif Config.VECTOR_INDEX_TYPE == "int4_hnsw":
                # int4 quantization: ~87.5% memory reduction
                vector_memory = doc_count * (Config.EMBEDDING_DIMENSIONS / 2 + 4)
                original_memory = doc_count * Config.EMBEDDING_DIMENSIONS * 4
                savings = ((original_memory - vector_memory) / original_memory) * 100
                
            else:
                # No quantization
                vector_memory = doc_count * Config.EMBEDDING_DIMENSIONS * 4
                savings = 0
            
            # Add HNSW graph overhead
            hnsw_overhead = doc_count * 4 * Config.HNSW_M
            total_memory = vector_memory + hnsw_overhead
            
            return {
                "estimated_memory": f"{total_memory / (1024**2):.1f} MB",
                "quantization_savings": f"{savings:.1f}%",
                "hnsw_overhead": f"{hnsw_overhead / (1024**2):.1f} MB"
            }
            
        except Exception as e:
            logger.error(f"Error estimating memory usage: {e}")
            raise
    
    def optimize_indices(self):
        """Optimize indices for better HNSW performance"""
        try:
            for index_name in ["gdpr_articles", "gdpr_chunks"]:
                if self.client.indices.exists(index=index_name):
                    # Force merge to reduce segment count for better HNSW performance
                    logger.info(f"Optimizing HNSW performance for {index_name}...")
                    
                    self.client.indices.forcemerge(
                        index=index_name,
                        max_num_segments=Config.MAX_SEGMENTS,
                        wait_for_completion=False
                    )
                    
                    # Update settings for optimal search performance
                    optimization_settings = {
                        "index": {
                            "refresh_interval": "30s",  # Reduce refresh frequency for better performance
                            "merge.policy.max_merged_segment": "5gb",
                        }
                    }
                    
                    self.client.indices.put_settings(
                        index=index_name,
                        body={"settings": optimization_settings}
                    )
                    
                    logger.info(f"Applied HNSW optimization settings to {index_name}")
                    
        except Exception as e:
            logger.error(f"Error optimizing indices: {e}")
            raise
    
    async def benchmark_search_performance(self, sample_queries: List[str]) -> Dict[str, Any]:
        """Benchmark search performance with HNSW optimizations"""
        import time
        
        results = {
            "total_queries": len(sample_queries),
            "avg_latency_ms": 0,
            "quantization_type": Config.VECTOR_INDEX_TYPE,
            "hnsw_config": {
                "m": Config.HNSW_M,
                "ef_construction": Config.HNSW_EF_CONSTRUCTION
            }
        }
        
        total_time = 0
        successful_queries = 0
        
        try:
            openai_manager = OpenAIManager()
            
            for query in sample_queries:
                start_time = time.time()
                
                # Create embedding
                embedding = await openai_manager.create_embedding(query)
                
                # Perform search
                search_results = self.hybrid_search(
                    query=query,
                    embedding=embedding,
                    search_level="both"
                )
                
                end_time = time.time()
                
                query_time = (end_time - start_time) * 1000  # Convert to milliseconds
                total_time += query_time
                successful_queries += 1
                
                logger.info(f"Query '{query[:30]}...' took {query_time:.2f}ms")
                
            if successful_queries > 0:
                results["avg_latency_ms"] = total_time / successful_queries
                results["successful_queries"] = successful_queries
                
            return results
            
        except Exception as e:
            logger.error(f"Error in performance benchmark: {e}")
            raise


# OpenAI API Manager
class OpenAIManager:
    """Manages OpenAI API calls with direct API usage"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using OpenAI API directly"""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    async def reasoning_completion(self, messages: List[Dict], system_prompt: str = None) -> str:
        """Create completion using o3-mini with high reasoning effort"""
        try:
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({"role": "developer", "content": system_prompt})
            
            formatted_messages.extend(messages)
            
            response = self.client.chat.completions.create(
                model=Config.O3_MINI_MODEL,
                messages=formatted_messages,
                reasoning_effort=Config.REASONING_EFFORT
            )
            
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in reasoning completion: {e}")
            raise


# Document Processing Agent
class DocumentProcessingAgent:
    """Agent for processing and chunking documents"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.name = "DocumentProcessor"
    
    def _estimate_token_count(self, text: str) -> int:
        """Estimate token count for text (rough approximation)"""
        # Rough estimation: 1 token ≈ 4 characters for English text
        return len(text) // 4
    
    def _split_large_text(self, text: str, max_tokens: int) -> List[str]:
        """Split large text into smaller chunks to handle token limits"""
        estimated_tokens = self._estimate_token_count(text)
        
        if estimated_tokens <= max_tokens:
            return [text]
        
        # Calculate approximate character limit per chunk
        chars_per_token = 4
        max_chars = max_tokens * chars_per_token
        overlap_chars = Config.OVERLAP_TOKENS * chars_per_token
        
        chunks = []
        start = 0
        
        while start < len(text):
            # Find a good breaking point (end of sentence/paragraph)
            end = start + max_chars
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # Look for sentence ending within the last 1000 chars
            search_start = max(start + max_chars - 1000, start)
            
            # Find the last sentence ending
            for delimiter in ['\n\n', '. ', '.\n', '! ', '?\n']:
                last_delimiter = text.rfind(delimiter, search_start, end)
                if last_delimiter != -1:
                    end = last_delimiter + len(delimiter)
                    break
            
            chunks.append(text[start:end])
            start = max(start + 1, end - overlap_chars)  # Add overlap
        
        return chunks
    
    def _extract_text_from_pdf(self, file_path: str) -> Tuple[str, Dict[str, Any]]:
    def _extract_text_from_pdf(self, file_path: str) -> Tuple[str, Dict[str, Any]]:
        """Extract text from PDF file using modern PyMuPDF 1.26+ and return metadata"""
        try:
            # Modern PyMuPDF syntax - more robust than legacy fitz
            doc = pymupdf.open(file_path)
            
            # Check if document is encrypted
            if doc.needs_pass:
                doc.close()
                raise ValueError(f"PDF {file_path} is password protected. Please provide an unencrypted version.")
            
            # Extract metadata
            page_count = doc.page_count
            metadata = {
                "page_count": page_count,
                "title": doc.metadata.get("title", ""),
                "author": doc.metadata.get("author", ""),
                "subject": doc.metadata.get("subject", ""),
                "creator": doc.metadata.get("creator", ""),
                "producer": doc.metadata.get("producer", ""),
                "creation_date": doc.metadata.get("creationDate", ""),
                "modification_date": doc.metadata.get("modDate", ""),
            }
            
            if page_count == 0:
                doc.close()
                raise ValueError(f"PDF {file_path} contains no pages")
            
            text = ""
            extracted_pages = 0
            page_texts = []  # Store individual page texts for validation
            
            # Modern iteration over pages
            for page in doc:
                try:
                    page_text = page.get_text()  # UTF-8 encoded text
                    page_texts.append(page_text)
                    
                    if page_text.strip():  # Only add non-empty pages
                        text += page_text
                        text += "\n\n"  # Add page separation
                        extracted_pages += 1
                except Exception as page_error:
                    logger.warning(f"Error extracting text from page {page.number + 1} in {file_path}: {page_error}")
                    page_texts.append("")  # Placeholder for failed page
                    continue
            
            doc.close()
            
            if extracted_pages == 0:
                raise ValueError(f"No text could be extracted from PDF {file_path}")
            
            # Add extraction statistics to metadata
            metadata.update({
                "extracted_pages": extracted_pages,
                "failed_pages": page_count - extracted_pages,
                "character_count": len(text),
                "estimated_tokens": self._estimate_token_count(text),
                "page_texts_count": len([p for p in page_texts if p.strip()])
            })
            
            logger.info(f"Successfully extracted text from {extracted_pages}/{page_count} pages in {file_path}")
            logger.info(f"Extracted {len(text):,} characters (~{self._estimate_token_count(text):,} tokens)")
            
            return text.strip(), metadata
            
        except Exception as e:
            logger.error(f"Error extracting text from PDF {file_path}: {e}")
            raise
    
    def _determine_document_type(self, file_path: str, content: str) -> str:
        """Determine document type based on filename and content"""
        filename = os.path.basename(file_path).lower()
        content_lower = content.lower()
        
        # Check filename patterns
        if "uk" in filename and "gdpr" in filename:
            return "UK_GDPR"
        elif "gdpr" in filename:
            return "GDPR"
        elif "uk" in filename and ("data" in filename or "protection" in filename):
            return "UK_GDPR"
        
        # Check content patterns
        if "united kingdom" in content_lower or "uk data" in content_lower:
            return "UK_GDPR"
        elif "general data protection regulation" in content_lower or "regulation (eu)" in content_lower:
            return "GDPR"
        
        # Default based on filename
        return "GDPR" if "gdpr" in filename else "UK_GDPR"
    
    async def process_document(self, file_path: str, document_type: str = None) -> Tuple[RawDocument, List[FullArticle], List[DocumentChunk], ProcessingValidation]:
        """Process a PDF document with complete data loss prevention"""
        try:
            # Check if file exists
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                raise FileNotFoundError(f"Document file not found: {file_path}")
            
            # Extract text from PDF with metadata
            logger.info(f"Extracting text from PDF: {file_path}")
            content, extraction_metadata = self._extract_text_from_pdf(file_path)
            
            if not content.strip():
                logger.error(f"No text extracted from PDF: {file_path}")
                raise ValueError(f"No text content found in PDF: {file_path}")
            
            # Determine document type if not provided
            if not document_type:
                document_type = self._determine_document_type(file_path, content)
                logger.info(f"Determined document type: {document_type} for {file_path}")
            
            # Store raw document for data loss prevention
            raw_doc = RawDocument(
                raw_id=str(uuid.uuid4()),
                file_path=file_path,
                document_type=document_type,
                raw_text=content,
                page_count=extraction_metadata["page_count"],
                character_count=len(content),
                extraction_metadata=extraction_metadata
            )
            
            # Index raw document immediately
            self.es_manager.index_raw_document(raw_doc)
            logger.info(f"Stored raw document: {len(content):,} characters")
            
            # Handle large documents by splitting if necessary
            estimated_tokens = self._estimate_token_count(content)
            if estimated_tokens > Config.MAX_TOKENS_PER_REQUEST:
                logger.warning(f"Large document detected ({estimated_tokens:,} tokens). Processing in chunks.")
                return await self._process_large_document(raw_doc, content, document_type)
            
            logger.info(f"Processing {document_type} document: {file_path} ({len(content)} characters)")
            
            # Process in single pass for smaller documents
            articles, chunks = await self._process_document_content(content, document_type)
            
            # Validate processing completeness
            validation = self.es_manager.validate_processing_completeness(raw_doc, articles, chunks)
            
            if not validation.validation_passed:
                logger.warning(f"Processing validation failed: {validation.notes}")
                logger.warning(f"Missing sections: {validation.missing_sections}")
            else:
                logger.info(f"Processing validation passed: {validation.coverage_percentage:.1f}% coverage")
            
            return raw_doc, articles, chunks, validation
            
        except Exception as e:
            logger.error(f"Error processing document {file_path}: {e}")
            raise
    
    async def _process_large_document(self, raw_doc: RawDocument, content: str, document_type: str) -> Tuple[RawDocument, List[FullArticle], List[DocumentChunk], ProcessingValidation]:
        """Process large documents by splitting into manageable chunks"""
        text_chunks = self._split_large_text(content, Config.MAX_TOKENS_PER_REQUEST)
        logger.info(f"Split large document into {len(text_chunks)} chunks for processing")
        
        all_articles = []
        all_chunks = []
        
        for i, text_chunk in enumerate(text_chunks):
            logger.info(f"Processing chunk {i+1}/{len(text_chunks)} ({len(text_chunk):,} chars)")
            
            try:
                chunk_articles, chunk_chunks = await self._process_document_content(text_chunk, document_type)
                all_articles.extend(chunk_articles)
                all_chunks.extend(chunk_chunks)
            except Exception as e:
                logger.error(f"Error processing chunk {i+1}: {e}")
                # Continue with other chunks even if one fails
                continue
        
        # Validate overall processing
        validation = self.es_manager.validate_processing_completeness(raw_doc, all_articles, all_chunks)
        
        logger.info(f"Large document processing complete: {len(all_articles)} articles, {len(all_chunks)} chunks")
        
        return raw_doc, all_articles, all_chunks, validation
    
    async def _process_document_content(self, content: str, document_type: str) -> Tuple[List[FullArticle], List[DocumentChunk]]:
        """Process a PDF document and create both full articles and chunks"""
    async def _process_document_content(self, content: str, document_type: str) -> Tuple[List[FullArticle], List[DocumentChunk]]:
        """Process document content with AI and create articles and chunks"""
        # First pass: Use o3-mini to identify full articles
        article_system_prompt = """
        You are an expert in GDPR documentation. Identify complete articles in the document.
        Each article should contain the full text including all sub-sections, clauses, and paragraphs.
        
        IMPORTANT: Process ALL content provided. Do not skip any sections.
        
        Return a JSON array with the following structure:
        {
            "articles": [
                {
                    "chapter_number": "string",
                    "article_number": "string",
                    "title": "string",
                    "full_content": "complete article text including all subsections",
                    "key_concepts": ["concept1", "concept2", ...]
                }
            ]
        }
        """
        
        article_messages = [
            {"role": "user", "content": f"Document type: {document_type}\n\nDocument content:\n{content}"}
        ]
        
        article_response = await self.openai_manager.reasoning_completion(article_messages, article_system_prompt)
        
        # Second pass: Use o3-mini to create detailed chunks within each article
        chunk_system_prompt = """
        You are an expert in GDPR documentation. Break down each article into meaningful chunks 
        (paragraphs, clauses, sub-articles) while preserving the legal structure.
        
        IMPORTANT: Process ALL content provided. Ensure complete coverage of the input text.
        
        For each chunk, identify:
        - Parent article information
        - Specific section/clause details
        - Main content
        - Key legal concepts
        
        Return a JSON array of chunks with the following structure:
        {
            "chunks": [
                {
                    "parent_article_number": "string",
                    "chapter_number": "string",
                    "article_number": "string or null",
                    "title": "string",
                    "content": "string"
                }
            ]
        }
        """
        
        chunk_messages = [
            {"role": "user", "content": f"Document type: {document_type}\n\nDocument content:\n{content}"}
        ]
        
        chunk_response = await self.openai_manager.reasoning_completion(chunk_messages, chunk_system_prompt)
        
        # Parse responses
        try:
            articles_data = json.loads(article_response)["articles"]
            chunks_data = json.loads(chunk_response)["chunks"]
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Failed to parse AI responses: {e}")
            logger.error(f"Article response: {article_response[:500]}...")
            logger.error(f"Chunk response: {chunk_response[:500]}...")
            raise ValueError(f"AI response parsing failed for {document_type}")
        
        # Create FullArticle objects
        full_articles = []
        for article_data in articles_data:
            article_id = str(uuid.uuid4())
            
            # Create full article embedding
            full_text = f"{article_data.get('title', '')} {article_data.get('full_content', '')}"
            full_embedding = await self.openai_manager.create_embedding(full_text)
            
            article = FullArticle(
                article_id=article_id,
                document_type=document_type,
                chapter_number=article_data.get("chapter_number", ""),
                article_number=article_data.get("article_number", ""),
                title=article_data.get("title", ""),
                full_content=article_data.get("full_content", ""),
                full_article_embedding=full_embedding,
                chunk_ids=[],  # Will be populated below
                key_concepts=article_data.get("key_concepts", [])
            )
            
            full_articles.append(article)
            # Index in Elasticsearch
            self.es_manager.index_article(article)
        
        # Create DocumentChunk objects and link to articles
        chunks = []
        article_chunk_map = {}  # Maps article_number to chunk_ids
        
        for chunk_data in chunks_data:
            chunk_id = str(uuid.uuid4())
            
            # Find parent article
            parent_article = None
            parent_article_number = chunk_data.get("parent_article_number") or chunk_data.get("article_number")
            
            for article in full_articles:
                if article.article_number == parent_article_number:
                    parent_article = article
                    break
            
            # Create chunk embedding
            embedding_text = f"{chunk_data.get('title', '')} {chunk_data.get('content', '')}"
            chunk_embedding = await self.openai_manager.create_embedding(embedding_text)
            
            chunk = DocumentChunk(
                chunk_id=chunk_id,
                parent_article_id=parent_article.article_id if parent_article else None,
                document_type=document_type,
                chapter_number=chunk_data.get("chapter_number", ""),
                article_number=chunk_data.get("article_number"),
                title=chunk_data.get("title", ""),
                content=chunk_data.get("content", ""),
                chunk_embedding=chunk_embedding,
                processed_by_agent=self.name
            )
            
            chunks.append(chunk)
            
            # Index in Elasticsearch
            self.es_manager.index_chunk(chunk)
            
            # Update parent article's chunk_ids
            if parent_article:
                if parent_article.article_number not in article_chunk_map:
                    article_chunk_map[parent_article.article_number] = []
                article_chunk_map[parent_article.article_number].append(chunk_id)
        
        # Update articles with their chunk IDs
        for article in full_articles:
            if article.article_number in article_chunk_map:
                article.chunk_ids = article_chunk_map[article.article_number]
                # Re-index with updated chunk_ids
                self.es_manager.index_article(article)
        
        logger.info(f"Created {len(full_articles)} articles and {len(chunks)} chunks")
        return full_articles, chunks


# Cross-Reference Agent
class CrossReferenceAgent:
    """Agent for finding cross-references between documents"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager, 
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "CrossReferenceAgent"
    
    async def find_cross_references(self, chunk: DocumentChunk, all_chunks: List[DocumentChunk]) -> List[CrossDocumentLink]:
        """Find cross-references for a given chunk"""
        links = []
        
        # Get embedding for the current chunk
        chunk_embedding = chunk.chunk_embedding
        if not chunk_embedding:
            chunk_embedding = await self.openai_manager.create_embedding(
                f"{chunk.title} {chunk.content}"
            )
        
        # Perform hybrid search to find similar chunks
        search_results = self.es_manager.hybrid_search(
            query=f"{chunk.title} {chunk.content[:500]}",
            embedding=chunk_embedding,
            filters={"document_type": ["GDPR", "UK_GDPR"]},
            search_level="chunks"
        )
        
        # Filter out the same chunk and same document type
        candidate_chunks = [
            result for result in search_results.get("chunks", [])
            if result["chunk_id"] != chunk.chunk_id and 
               result["document_type"] != chunk.document_type
        ]
        
        # Use o3-mini to analyze relationships
        for candidate in candidate_chunks[:5]:  # Limit to top 5 candidates
            relationship = await self._analyze_relationship(chunk, candidate)
            
            if relationship and relationship["confidence_score"] > 0.6:
                link = CrossDocumentLink(
                    source_chunk_id=chunk.chunk_id,
                    target_chunk_id=candidate["chunk_id"],
                    relationship_type=relationship["relationship_type"],
                    confidence_score=relationship["confidence_score"]
                )
                
                links.append(link)
                self.es_manager.index_link(link)
        
        # Store findings in long-term memory
        await self._store_cross_reference_memory(chunk, links)
        
        return links
    
    async def _analyze_relationship(self, source_chunk: DocumentChunk, target_chunk: Dict) -> Optional[Dict]:
        """Analyze relationship between two chunks using o3-mini"""
        system_prompt = """
        You are an expert legal analyst specializing in GDPR and data protection laws. 
        Analyze the relationship between two legal text chunks and determine:
        
        1. The type of relationship (supports, contradicts, references, complements, specifies, generalizes)
        2. The confidence score (0.0 to 1.0)
        3. A brief explanation of the relationship
        
        Return JSON format:
        {
            "relationship_type": "string",
            "confidence_score": float,
            "explanation": "string"
        }
        
        Return null if no meaningful relationship exists (confidence < 0.6).
        """
        
        messages = [
            {
                "role": "user", 
                "content": f"""
                Source chunk ({source_chunk.document_type}):
                Title: {source_chunk.title}
                Content: {source_chunk.content}
                
                Target chunk ({target_chunk['document_type']}):
                Title: {target_chunk['title']}
                Content: {target_chunk['content']}
                
                Analyze the relationship between these chunks.
                """
            }
        ]
        
        try:
            response = await self.openai_manager.reasoning_completion(messages, system_prompt)
            result = json.loads(response)
            
            if result and result.get("confidence_score", 0) >= 0.6:
                return result
            return None
            
        except Exception as e:
            logger.error(f"Error analyzing relationship: {e}")
            raise
    
    async def _store_cross_reference_memory(self, chunk: DocumentChunk, links: List[CrossDocumentLink]):
        """Store cross-reference findings in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "chapter_number": chunk.chapter_number,
            "found_links": len(links),
            "link_types": ",".join([link.relationship_type for link in links]) if links else "",  # Convert list to string
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="episodic",
            content=memory_content,
            namespace=["cross_reference", chunk.document_type]
        )
        
        self.es_manager.index_memory(memory)
        
        # Store in LangGraph memory store with serializable content
        try:
            await self.memory_store.aput(
                namespace=tuple(["cross_reference", self.name]),  # Convert to tuple
                key=f"analysis_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing memory in LangGraph store: {e}")
            # Continue without failing the whole process


# Linking Agent
class LinkingAgent:
    """Agent for maintaining and updating document links"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager,
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "LinkingAgent"
    
    async def update_chunk_references(self, chunk: DocumentChunk, related_links: List[CrossDocumentLink]) -> DocumentChunk:
        """Update chunk with supporting references"""
        
        # Get related chunks from Elasticsearch
        related_chunks = self.es_manager.get_related_chunks(chunk.chunk_id)
        
        # Create ChapterReference objects
        references = []
        for related_chunk in related_chunks:
            # Find the corresponding link for confidence score
            link = next(
                (l for l in related_links 
                 if l.source_chunk_id == chunk.chunk_id and l.target_chunk_id == related_chunk["chunk_id"] or
                    l.target_chunk_id == chunk.chunk_id and l.source_chunk_id == related_chunk["chunk_id"]),
                None
            )
            
            if link:
                reference = ChapterReference(
                    document_type=related_chunk["document_type"],
                    chapter_number=related_chunk["chapter_number"],
                    article_number=related_chunk.get("article_number"),
                    title=related_chunk["title"],
                    relevance_score=link.confidence_score,
                    relationship_type=link.relationship_type
                )
                references.append(reference)
        
        # Update chunk with references
        chunk.supporting_references = references
        
        # Re-index with updated references
        self.es_manager.index_chunk(chunk)
        
        # Store linking activity in memory
        await self._store_linking_memory(chunk, references)
        
        return chunk
    
    async def _store_linking_memory(self, chunk: DocumentChunk, references: List[ChapterReference]):
        """Store linking activity in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "references_added": len(references),
            "reference_types": ",".join([ref.relationship_type for ref in references]) if references else "",  # Convert list to string
            "timestamp": datetime.now().isoformat()
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="procedural",
            content=memory_content,
            namespace=["linking", chunk.document_type]
        )
        
        self.es_manager.index_memory(memory)
        
        # Store in LangGraph memory with serializable content
        try:
            await self.memory_store.aput(
                namespace=tuple(["linking", self.name]),  # Convert to tuple
                key=f"linking_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing linking memory: {e}")
            # Continue without failing


# Tools for agents
@tool
async def search_similar_chunks(query: str, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for similar chunks using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    # Create embedding for query
    embedding = await openai_manager.create_embedding(query)
    
    # Prepare filters
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    # Perform hybrid search for chunks only
    results = es_manager.hybrid_search(query, embedding, filters, search_level="chunks")
    return results.get("chunks", [])


@tool
async def search_full_articles(query: str, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for full articles using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    # Create embedding for query
    embedding = await openai_manager.create_embedding(query)
    
    # Prepare filters
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    # Perform hybrid search for articles only
    results = es_manager.hybrid_search(query, embedding, filters, search_level="articles")
    return results.get("articles", [])


@tool
async def search_both_levels(query: str, document_type: str = None, config: RunnableConfig = None) -> Dict[str, List[Dict]]:
    """Search both full articles and chunks using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    # Create embedding for query
    embedding = await openai_manager.create_embedding(query)
    
    # Prepare filters
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    # Perform hybrid search at both levels
    results = es_manager.hybrid_search(query, embedding, filters, search_level="both")
    return results


@tool
async def get_chunk_relationships(chunk_id: str, config: RunnableConfig = None) -> List[Dict]:
    """Get all relationships for a specific chunk"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    related_chunks = es_manager.get_related_chunks(chunk_id)
    return related_chunks


@tool
async def store_agent_memory(agent_name: str, memory_type: str, content: Dict, 
                           namespace: List[str], config: RunnableConfig = None) -> bool:
    """Store information in agent's long-term memory"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    memory_store = config["configurable"]["memory_store"]
    
    # Ensure content is serializable by converting any lists to strings
    serializable_content = {}
    for key, value in content.items():
        if isinstance(value, list):
            if all(isinstance(item, str) for item in value):
                serializable_content[key] = ",".join(value) if value else ""
            else:
                serializable_content[key] = str(value)
        else:
            serializable_content[key] = value
    
    memory = AgentMemory(
        agent_name=agent_name,
        memory_type=memory_type,
        content=serializable_content,
        namespace=namespace
    )
    
    # Store in Elasticsearch
    es_success = es_manager.index_memory(memory)
    
    # Store in LangGraph memory with proper serialization
    try:
        await memory_store.aput(
            namespace=tuple(namespace + [agent_name]),  # Convert to tuple for hashability
            key=memory.memory_id,
            value=serializable_content
        )
        return es_success
    except Exception as e:
        logger.error(f"Error storing in LangGraph memory: {e}")
        return es_success  # Return ES result even if LangGraph storage fails


# Main workflow nodes
async def document_processing_node(state: ProcessingState) -> ProcessingState:
    """Process all PDF documents from the data directory with data loss prevention"""
    logger.info("Starting document processing...")
    
    openai_manager = OpenAIManager()
    es_manager = ElasticsearchManager()
    
    processor = DocumentProcessingAgent(openai_manager, es_manager)
    
    all_raw_docs = []
    all_articles = []
    all_chunks = []
    all_validations = []
    
    # Check if data directory exists
    data_dir = "data"
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"Data directory not found: {data_dir}. Please create the directory and add PDF files.")
    
    # Find all PDF files in the data directory
    pdf_pattern = os.path.join(data_dir, "*.pdf")
    pdf_files = glob.glob(pdf_pattern)
    
    if not pdf_files:
        raise FileNotFoundError(f"No PDF files found in {data_dir} directory. Please add PDF files to process.")
    
    logger.info(f"Found {len(pdf_files)} PDF files in {data_dir} directory:")
    for pdf_file in pdf_files:
        logger.info(f"  - {os.path.basename(pdf_file)}")
    
    # Process each PDF file
    for pdf_file in pdf_files:
        try:
            logger.info(f"Processing PDF: {pdf_file}")
            
            # Process document with full data loss prevention
            raw_doc, articles, chunks, validation = await processor.process_document(pdf_file)
            
            all_raw_docs.append(raw_doc)
            all_articles.extend(articles)
            all_chunks.extend(chunks)
            all_validations.append(validation)
            
            # Log processing results
            logger.info(f"Successfully processed {pdf_file}:")
            logger.info(f"  - Raw document: {raw_doc.character_count:,} chars")
            logger.info(f"  - Articles: {len(articles)}")
            logger.info(f"  - Chunks: {len(chunks)}")
            logger.info(f"  - Validation: {validation.coverage_percentage:.1f}% coverage ({'PASSED' if validation.validation_passed else 'FAILED'})")
            
            if not validation.validation_passed:
                logger.warning(f"  - Issues: {validation.notes}")
                if validation.missing_sections:
                    logger.warning(f"  - Missing: {', '.join(validation.missing_sections)}")
            
        except Exception as e:
            logger.error(f"Failed to process {pdf_file}: {e}")
            # Continue processing other files even if one fails
            continue
    
    if not all_articles and not all_chunks:
        raise ValueError("No documents were successfully processed. Please check your PDF files and try again.")
    
    # Calculate overall statistics
    total_chars = sum(raw_doc.character_count for raw_doc in all_raw_docs)
    passed_validations = sum(1 for v in all_validations if v.validation_passed)
    avg_coverage = sum(v.coverage_percentage for v in all_validations) / len(all_validations) if all_validations else 0
    
    state["raw_documents"] = all_raw_docs
    state["full_articles"] = all_articles
    state["documents"] = all_chunks
    state["validation_results"] = all_validations
    state["processing_stage"] = "cross_referencing"
    
    logger.info(f"Document processing completed:")
    logger.info(f"  - {len(all_raw_docs)} raw documents ({total_chars:,} total chars)")
    logger.info(f"  - {len(all_articles)} total articles")
    logger.info(f"  - {len(all_chunks)} total chunks")
    logger.info(f"  - {passed_validations}/{len(all_validations)} validations passed")
    logger.info(f"  - {avg_coverage:.1f}% average coverage")
    
    return state


async def cross_reference_node(state: ProcessingState) -> ProcessingState:
    """Find cross-references between documents"""
    logger.info("Finding cross-references...")
    
    openai_manager = OpenAIManager()
    es_manager = ElasticsearchManager()
    memory_store = InMemoryStore()
    
    cross_ref_agent = CrossReferenceAgent(openai_manager, es_manager, memory_store)
    
    all_links = []
    
    for chunk in state["documents"]:
        links = await cross_ref_agent.find_cross_references(chunk, state["documents"])
        all_links.extend(links)
    
    state["cross_links"] = all_links
    state["processing_stage"] = "linking"
    
    return state


async def linking_node(state: ProcessingState) -> ProcessingState:
    """Update chunks with supporting references"""
    logger.info("Updating chunk references...")
    
    openai_manager = OpenAIManager()
    es_manager = ElasticsearchManager()
    memory_store = InMemoryStore()
    
    linking_agent = LinkingAgent(openai_manager, es_manager, memory_store)
    
    updated_chunks = []
    for chunk in state["documents"]:
        # Get links for this chunk
        chunk_links = [
            link for link in state["cross_links"] 
            if link.source_chunk_id == chunk.chunk_id or link.target_chunk_id == chunk.chunk_id
        ]
        
        updated_chunk = await linking_agent.update_chunk_references(chunk, chunk_links)
        updated_chunks.append(updated_chunk)
    
    state["documents"] = updated_chunks
    state["processing_stage"] = "completed"
    
    return state


# Create the multi-agent workflow
def create_gdpr_processing_workflow():
    """Create the LangGraph workflow for GDPR document processing"""
    
    # Initialize components
    checkpointer = MemorySaver()
    memory_store = InMemoryStore()
    
    # Create tools
    tools = [search_similar_chunks, search_full_articles, search_both_levels, 
             get_chunk_relationships, store_agent_memory]
    tool_node = ToolNode(tools)
    
    # Create the graph
    workflow = StateGraph(ProcessingState)
    
    # Add nodes
    workflow.add_node("document_processing", document_processing_node)
    workflow.add_node("cross_reference", cross_reference_node)
    workflow.add_node("linking", linking_node)
    workflow.add_node("tools", tool_node)
    
    # Add edges
    workflow.add_edge(START, "document_processing")
    workflow.add_edge("document_processing", "cross_reference")
    workflow.add_edge("cross_reference", "linking")
    workflow.add_edge("linking", END)
    
    # Compile the graph
    app = workflow.compile(
        checkpointer=checkpointer,
        store=memory_store
    )
    
    return app


async def run_final_analysis(final_state: ProcessingState, es_manager: ElasticsearchManager) -> None:
    """Run final analysis and demonstrations"""
    try:
        # Performance benchmark with sample queries
        sample_queries = [
            "data processing lawful basis",
            "consent withdrawal GDPR",
            "data protection officer requirements",
            "cross-border data transfer",
            "data subject rights"
        ]
        
        print(f"\n=== Performance Benchmark ===")
        benchmark_results = await es_manager.benchmark_search_performance(sample_queries)
        print(f"Average Query Latency: {benchmark_results.get('avg_latency_ms', 0):.2f}ms")
        print(f"Successful Queries: {benchmark_results.get('successful_queries', 0)}/{benchmark_results.get('total_queries', 0)}")
        
        # Demonstrate hybrid search capabilities
        if final_state['documents']:
            print(f"\n=== Hybrid Search Demo ===")
            sample_query = "data processing consent"
            
            openai_manager = OpenAIManager()
            sample_embedding = await openai_manager.create_embedding(sample_query)
            
            search_results = es_manager.hybrid_search(
                query=sample_query,
                embedding=sample_embedding,
                search_level="both"
            )
            
            print(f"Search query: '{sample_query}'")
            print(f"Articles found: {len(search_results.get('articles', []))}")
            print(f"Chunks found: {len(search_results.get('chunks', []))}")
            
            if search_results.get('articles'):
                top_article = search_results['articles'][0]
                print(f"Top article: {top_article.get('title', 'N/A')} (Score: {top_article.get('_score', 0):.3f})")
                
    except Exception as e:
        logger.error(f"Error in final analysis: {e}")
        raise


# Main execution function
async def main():
    """Main execution function"""
    
    # Verify configuration
    if not Config.OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable is required")
    
    if not Config.ES_PASSWORD:
        raise ValueError("ES_PASSWORD environment variable is required")
    
    logger.info(f"Using OpenAI base URL: {Config.OPENAI_BASE_URL}")
    logger.info(f"Using Elasticsearch: {Config.ES_HOST}:{Config.ES_PORT}")
    
    # Check if data directory exists
    data_dir = "data"
    if not os.path.exists(data_dir):
        logger.error(f"Data directory '{data_dir}' not found")
        raise FileNotFoundError(f"Please create a '{data_dir}' directory and add your PDF files")
    
    # Check for PDF files
    pdf_files = glob.glob(os.path.join(data_dir, "*.pdf"))
    if not pdf_files:
        logger.error(f"No PDF files found in '{data_dir}' directory")
        raise FileNotFoundError(f"Please add PDF files to the '{data_dir}' directory")
    
    logger.info(f"Found {len(pdf_files)} PDF files to process")
    
    # Create the workflow
    app = create_gdpr_processing_workflow()
    
    # Initial state
    initial_state = ProcessingState(
        messages=[HumanMessage(content="Process GDPR and UK GDPR documents")],
        full_articles=[],
        documents=[],
        current_chunk=None,
        cross_links=[],
        processing_stage="initializing",
        agent_memories=[],
        elasticsearch_client=None
    )
    
    # Configuration for the run
    config = {
        "configurable": {
            "thread_id": "gdpr_processing_001",
            "elasticsearch_manager": ElasticsearchManager(),
            "openai_manager": OpenAIManager(),
            "memory_store": InMemoryStore()
        }
    }
    
    # Run the workflow
    logger.info("Starting GDPR document processing workflow...")
    
    try:
        final_state = await app.ainvoke(initial_state, config)
        
        logger.info(f"Processing completed. Processed {len(final_state['full_articles'])} full articles and {len(final_state['documents'])} chunks")
        logger.info(f"Created {len(final_state['cross_links'])} cross-document links")
        
        # Get Elasticsearch manager for performance analysis
        es_manager = config["configurable"]["elasticsearch_manager"]
        
        # Display comprehensive performance metrics
        print("\n=== GDPR Processing Summary ===")
        print(f"PDF files processed: {len(glob.glob('data/*.pdf'))}")
        print(f"Full articles processed: {len(final_state['full_articles'])}")
        print(f"Total chunks processed: {len(final_state['documents'])}")
        print(f"Cross-document links created: {len(final_state['cross_links'])}")
        print(f"Processing stage: {final_state['processing_stage']}")
        
        # Show document types found
        doc_types = {}
        for article in final_state['full_articles']:
            doc_type = article.document_type
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        print(f"\nDocument Types Processed:")
        for doc_type, count in doc_types.items():
            print(f"  {doc_type}: {count} articles")
        
        # HNSW and Quantization Performance Metrics
        print(f"\n=== HNSW & Quantization Optimizations ===")
        print(f"Vector Index Type: {Config.VECTOR_INDEX_TYPE}")
        print(f"HNSW Parameters: M={Config.HNSW_M}, EF_Construction={Config.HNSW_EF_CONSTRUCTION}")
        print(f"Embedding Dimensions: {Config.EMBEDDING_DIMENSIONS}")
        print(f"Preload Cache: {'Enabled' if Config.ENABLE_PRELOAD else 'Disabled'}")
        
        # Get index statistics
        index_stats = es_manager.get_index_stats()
        for index_name, stats in index_stats.items():
            print(f"\n{index_name.upper()} Index:")
            print(f"  Documents: {stats.get('documents', 0):,}")
            print(f"  Size: {stats.get('size_bytes', 0) / (1024**2):.1f} MB")
            print(f"  Segments: {stats.get('segments', 0)}")
            
            vector_stats = stats.get('vector_size_estimate', {})
            if vector_stats.get('estimated_memory') != 'Unknown':
                print(f"  Vector Memory: {vector_stats.get('estimated_memory')}")
                print(f"  Quantization Savings: {vector_stats.get('quantization_savings')}")
                print(f"  HNSW Overhead: {vector_stats.get('hnsw_overhead')}")
        
        # Optimize indices for production use
        print(f"\n=== Optimizing Indices for Production ===")
        es_manager.optimize_indices()
        
        # Show examples
        if final_state['full_articles']:
            article = final_state['full_articles'][0]
            print(f"\nExample full article: {article.title}")
            print(f"Article chunks: {len(article.chunk_ids)}")
            print(f"Key concepts: {len(article.key_concepts)}")
        
        if final_state['documents']:
            chunk = final_state['documents'][0]
            print(f"\nExample chunk: {chunk.title}")
            print(f"Parent article: {chunk.parent_article_id}")
            print(f"Supporting references: {len(chunk.supporting_references)}")
        
        # Run async final analysis
        await run_final_analysis(final_state, es_manager)
                
        return final_state
        
    except Exception as e:
        logger.error(f"Error in workflow execution: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
