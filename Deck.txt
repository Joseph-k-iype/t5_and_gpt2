# GDPR Multi-Agent System with Long-Term Memory & Latest PyMuPDF
# Requirements: pip install langchain langgraph langchain-elasticsearch langchain-openai pymupdf==1.26.1 pydantic

import asyncio
import logging
import os
import pickle
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Set
from uuid import uuid4

import numpy as np
import pymupdf  # Latest PyMuPDF 1.26.1 - direct import
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field, ConfigDict
from sklearn.metrics.pairwise import cosine_similarity

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# ENHANCED PYDANTIC MODELS WITH LONG-TERM MEMORY
# =============================================================================

class DocumentType(str, Enum):
    GDPR_EU = "gdpr_eu"
    GDPR_UK = "gdpr_uk"

class ChunkType(str, Enum):
    CHAPTER = "chapter"
    ARTICLE = "article"
    SECTION = "section"
    PARAGRAPH = "paragraph"

class RelationshipType(str, Enum):
    REFERENCES = "references"
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    ELABORATES = "elaborates"
    SUPERSEDES = "supersedes"
    SEMANTICALLY_RELATED = "semantically_related"  # For distant semantic connections

class MemoryChunk(BaseModel):
    """Enhanced chunk model with long-term memory capabilities"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    id: str = Field(default_factory=lambda: str(uuid4()))
    document_type: DocumentType
    chunk_type: ChunkType
    title: str
    content: str
    chapter_number: Optional[str] = None
    article_number: Optional[str] = None
    hierarchy_level: int = Field(ge=0, le=5)
    page_number: Optional[int] = None
    position_in_document: int = Field(ge=0)  # Position for context
    embedding: Optional[List[float]] = None
    keywords: List[str] = Field(default_factory=list)
    legal_concepts: List[str] = Field(default_factory=list)  # Extracted legal concepts
    related_chunk_ids: Set[str] = Field(default_factory=set)  # All related chunks (distant included)
    semantic_similarity_scores: Dict[str, float] = Field(default_factory=dict)  # chunk_id -> similarity
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)

class SemanticRelationship(BaseModel):
    """Enhanced relationship model for long-term memory"""
    id: str = Field(default_factory=lambda: str(uuid4()))
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: RelationshipType
    confidence_score: float = Field(ge=0.0, le=1.0)
    semantic_similarity: float = Field(ge=0.0, le=1.0)
    distance_in_document: int = Field(ge=0)  # How far apart the chunks are
    reasoning: str
    legal_basis: str  # Legal justification for the relationship
    extracted_by_agent: str
    created_at: datetime = Field(default_factory=datetime.now)

class LongTermMemoryState(BaseModel):
    """State with comprehensive long-term memory"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Document processing state
    documents: List[Document] = Field(default_factory=list)
    memory_chunks: List[MemoryChunk] = Field(default_factory=list)
    relationships: List[SemanticRelationship] = Field(default_factory=list)
    
    # Long-term memory components
    global_embedding_index: Dict[str, List[float]] = Field(default_factory=dict)  # chunk_id -> embedding
    concept_memory: Dict[str, Set[str]] = Field(default_factory=dict)  # concept -> chunk_ids
    temporal_memory: List[str] = Field(default_factory=list)  # chronological order of chunks
    semantic_clusters: Dict[str, List[str]] = Field(default_factory=dict)  # cluster_id -> chunk_ids
    
    # LangChain components
    vectorstore: Optional[Any] = None
    qa_chain: Optional[Any] = None
    conversational_chain: Optional[Any] = None
    conversation_memory: Optional[Any] = None
    
    # Processing state
    current_agent: Optional[str] = None
    errors: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

# =============================================================================
# LONG-TERM MEMORY MANAGER
# =============================================================================

class LongTermMemoryManager:
    """Manages long-term memory across document chunks"""
    
    def __init__(self, similarity_threshold: float = 0.7):
        self.similarity_threshold = similarity_threshold
        self.concept_embeddings: Dict[str, List[float]] = {}
        self.temporal_context_window = 10  # Consider 10 chunks before/after for context
        
    def add_chunk_to_memory(self, chunk: MemoryChunk, state: LongTermMemoryState):
        """Add chunk to long-term memory with comprehensive indexing"""
        
        # Add to global embedding index
        if chunk.embedding:
            state.global_embedding_index[chunk.id] = chunk.embedding
        
        # Add to concept memory
        for concept in chunk.legal_concepts:
            if concept not in state.concept_memory:
                state.concept_memory[concept] = set()
            state.concept_memory[concept].add(chunk.id)
        
        # Add to temporal memory
        state.temporal_memory.append(chunk.id)
        
        # Find semantic relationships with ALL existing chunks
        self._find_global_semantic_relationships(chunk, state)
        
        # Update semantic clusters
        self._update_semantic_clusters(chunk, state)
    
    def _find_global_semantic_relationships(self, new_chunk: MemoryChunk, state: LongTermMemoryState):
        """Find relationships with ALL chunks in memory, not just adjacent ones"""
        
        if not new_chunk.embedding:
            return
        
        new_embedding = np.array(new_chunk.embedding).reshape(1, -1)
        
        for existing_chunk in state.memory_chunks:
            if existing_chunk.id == new_chunk.id or not existing_chunk.embedding:
                continue
                
            existing_embedding = np.array(existing_chunk.embedding).reshape(1, -1)
            similarity = cosine_similarity(new_embedding, existing_embedding)[0][0]
            
            # Calculate distance in document
            new_pos = new_chunk.position_in_document
            existing_pos = existing_chunk.position_in_document
            distance = abs(new_pos - existing_pos)
            
            # Store similarity score
            new_chunk.semantic_similarity_scores[existing_chunk.id] = similarity
            existing_chunk.semantic_similarity_scores[new_chunk.id] = similarity
            
            # Create relationship if similarity is high enough
            if similarity > self.similarity_threshold:
                relationship_type = self._determine_relationship_type(
                    new_chunk, existing_chunk, similarity, distance
                )
                
                relationship = SemanticRelationship(
                    source_chunk_id=new_chunk.id,
                    target_chunk_id=existing_chunk.id,
                    relationship_type=relationship_type,
                    confidence_score=similarity,
                    semantic_similarity=similarity,
                    distance_in_document=distance,
                    reasoning=f"High semantic similarity ({similarity:.3f}) between chunks {distance} positions apart",
                    legal_basis=self._extract_legal_basis(new_chunk, existing_chunk),
                    extracted_by_agent="long_term_memory_manager"
                )
                
                state.relationships.append(relationship)
                new_chunk.related_chunk_ids.add(existing_chunk.id)
                existing_chunk.related_chunk_ids.add(new_chunk.id)
    
    def _determine_relationship_type(self, chunk1: MemoryChunk, chunk2: MemoryChunk, 
                                   similarity: float, distance: int) -> RelationshipType:
        """Determine relationship type based on content and distance"""
        
        # If chunks are close in document, likely structural relationship
        if distance < 5:
            if chunk1.hierarchy_level < chunk2.hierarchy_level:
                return RelationshipType.ELABORATES
            else:
                return RelationshipType.REFERENCES
        
        # If chunks are far apart but highly similar, semantic relationship
        if distance > 20 and similarity > 0.85:
            return RelationshipType.SEMANTICALLY_RELATED
            
        # Check for legal concept overlap
        common_concepts = set(chunk1.legal_concepts) & set(chunk2.legal_concepts)
        if common_concepts:
            return RelationshipType.SUPPORTS
            
        return RelationshipType.SEMANTICALLY_RELATED
    
    def _extract_legal_basis(self, chunk1: MemoryChunk, chunk2: MemoryChunk) -> str:
        """Extract legal justification for relationship"""
        common_concepts = set(chunk1.legal_concepts) & set(chunk2.legal_concepts)
        if common_concepts:
            return f"Shared legal concepts: {', '.join(common_concepts)}"
        
        if chunk1.article_number and chunk2.article_number:
            return f"Cross-reference between Article {chunk1.article_number} and Article {chunk2.article_number}"
            
        return "Semantic similarity in legal context"
    
    def _update_semantic_clusters(self, chunk: MemoryChunk, state: LongTermMemoryState):
        """Update semantic clusters based on similarity"""
        
        # Find best matching cluster
        best_cluster = None
        best_similarity = 0
        
        for cluster_id, chunk_ids in state.semantic_clusters.items():
            cluster_similarities = []
            for chunk_id in chunk_ids:
                if chunk_id in chunk.semantic_similarity_scores:
                    cluster_similarities.append(chunk.semantic_similarity_scores[chunk_id])
            
            if cluster_similarities:
                avg_similarity = np.mean(cluster_similarities)
                if avg_similarity > best_similarity and avg_similarity > self.similarity_threshold:
                    best_similarity = avg_similarity
                    best_cluster = cluster_id
        
        # Add to best cluster or create new one
        if best_cluster:
            state.semantic_clusters[best_cluster].append(chunk.id)
        else:
            new_cluster_id = f"cluster_{len(state.semantic_clusters)}"
            state.semantic_clusters[new_cluster_id] = [chunk.id]
    
    def get_related_chunks_by_concept(self, concept: str, state: LongTermMemoryState) -> List[MemoryChunk]:
        """Retrieve all chunks related to a legal concept"""
        if concept not in state.concept_memory:
            return []
        
        chunk_ids = state.concept_memory[concept]
        return [chunk for chunk in state.memory_chunks if chunk.id in chunk_ids]
    
    def get_temporally_related_chunks(self, chunk_id: str, state: LongTermMemoryState, 
                                    window_size: int = 5) -> List[MemoryChunk]:
        """Get chunks that appear before/after in document"""
        try:
            chunk_index = state.temporal_memory.index(chunk_id)
            start_idx = max(0, chunk_index - window_size)
            end_idx = min(len(state.temporal_memory), chunk_index + window_size + 1)
            
            related_ids = state.temporal_memory[start_idx:end_idx]
            return [chunk for chunk in state.memory_chunks if chunk.id in related_ids]
        except ValueError:
            return []
    
    def find_semantic_path(self, start_chunk_id: str, end_chunk_id: str, 
                          state: LongTermMemoryState, max_depth: int = 3) -> List[str]:
        """Find semantic path between two chunks through relationships"""
        
        visited = set()
        queue = [(start_chunk_id, [start_chunk_id])]
        
        while queue:
            current_id, path = queue.pop(0)
            
            if current_id == end_chunk_id:
                return path
            
            if current_id in visited or len(path) > max_depth:
                continue
                
            visited.add(current_id)
            
            # Find related chunks
            current_chunk = next((c for c in state.memory_chunks if c.id == current_id), None)
            if current_chunk:
                for related_id in current_chunk.related_chunk_ids:
                    if related_id not in visited:
                        queue.append((related_id, path + [related_id]))
        
        return []  # No path found

# =============================================================================
# ENHANCED SERVICES WITH LATEST PYMUPDF
# =============================================================================

class EnhancedLangChainElasticsearchService:
    def __init__(self, elasticsearch_url: str = "http://localhost:9200"):
        self.elasticsearch_url = elasticsearch_url
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.llm = ChatOpenAI(
            model="o3-mini",
            api_key=os.getenv("OPENAI_API_KEY"),
            model_kwargs={"reasoning_effort": "high"},
            temperature=0.1
        )
        
    def create_vectorstore(self, index_name: str = "gdpr_longterm_memory") -> ElasticsearchStore:
        """Create enhanced vector store for long-term memory"""
        return ElasticsearchStore(
            elasticsearch_url=self.elasticsearch_url,
            index_name=index_name,
            embedding=self.embeddings,
            strategy="dense_vector",  # Use dense vectors for better semantic search
        )
    
    def create_memory_aware_qa_chains(self, vectorstore: ElasticsearchStore) -> Tuple[RetrievalQA, ConversationalRetrievalChain, ConversationSummaryBufferMemory]:
        """Create QA chains with long-term conversational memory"""
        
        # Create persistent conversation memory
        conversation_memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=2000,  # Larger buffer for complex legal discussions
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Enhanced QA chain with memory awareness
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "score_threshold": 0.6,  # Lower threshold for broader context
                    "k": 15,  # More results for comprehensive context
                    "filter": None
                }
            ),
            return_source_documents=True,
            chain_type_kwargs={
                "prompt": PromptTemplate(
                    template="""You are an expert GDPR legal analyst with perfect memory of all previous discussions. 
                    
                    Consider both the immediate context and any related concepts from our previous conversations.
                    
                    Context from documents: {context}
                    
                    Question: {question}
                    
                    Instructions:
                    1. Provide comprehensive answer citing specific GDPR articles/sections
                    2. Consider relationships between different GDPR provisions  
                    3. Reference any related concepts from distant parts of the regulation
                    4. Explain how this connects to broader GDPR principles
                    
                    Legal Analysis:""",
                    input_variables=["context", "question"]
                )
            }
        )
        
        # Conversational chain with enhanced memory
        conversational_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(
                search_kwargs={
                    "k": 12,
                    "score_threshold": 0.6
                }
            ),
            memory=conversation_memory,
            return_source_documents=True,
            verbose=True,
            condense_question_prompt=PromptTemplate(
                template="""Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question that captures the full legal context.

                Chat History:
                {chat_history}
                
                Follow Up Input: {question}
                
                Standalone question with full legal context:""",
                input_variables=["chat_history", "question"]
            )
        )
        
        return qa_chain, conversational_chain, conversation_memory

class EnhancedDocumentParsingAgent:
    def __init__(self, reasoning_service, memory_manager: LongTermMemoryManager):
        self.reasoning_service = reasoning_service
        self.memory_manager = memory_manager
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # Larger chunks for better context
            chunk_overlap=300,  # More overlap for continuity
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
    async def process_documents_with_memory(self, file_paths: List[str]) -> List[Document]:
        """Process documents using latest PyMuPDF with enhanced memory"""
        all_documents = []
        position_counter = 0
        
        for file_path in file_paths:
            try:
                # Extract text with latest PyMuPDF
                text, page_info = self._extract_text_pymupdf_latest(file_path)
                
                # Analyze document structure with o3-mini
                structure = await self.reasoning_service.analyze_document_structure(text)
                
                # Determine document type
                doc_type = self._determine_document_type(file_path, text, structure)
                
                # Create memory-aware chunks
                chunks = await self._create_memory_chunks(
                    text, doc_type, structure, file_path, page_info, position_counter
                )
                
                # Convert to LangChain Documents
                documents = self._chunks_to_documents(chunks)
                all_documents.extend(documents)
                
                position_counter += len(chunks)
                logger.info(f"Processed {file_path}: {len(chunks)} memory-aware chunks")
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
        
        return all_documents
    
    def _extract_text_pymupdf_latest(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text using latest PyMuPDF 1.26.1 with page information"""
        doc = pymupdf.open(file_path)  # Latest import method
        text = ""
        page_info = {}
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            page_text = page.get_text()
            text += page_text + "\n"
            
            # Store page information for context
            page_info[page_num] = {
                "text_length": len(page_text),
                "has_images": len(page.get_images()) > 0,
                "has_tables": len(page.find_tables()) > 0 if hasattr(page, 'find_tables') else False
            }
        
        doc.close()
        return text, page_info
    
    def _determine_document_type(self, file_path: str, text: str, structure: Dict) -> DocumentType:
        """Enhanced document type detection"""
        text_lower = text.lower()
        
        uk_indicators = ["uk gdpr", "data protection act 2018", "ico", "information commissioner"]
        eu_indicators = ["regulation (eu)", "european union", "gdpr", "general data protection"]
        
        uk_score = sum(1 for indicator in uk_indicators if indicator in text_lower)
        eu_score = sum(1 for indicator in eu_indicators if indicator in text_lower)
        
        if uk_score > eu_score:
            return DocumentType.GDPR_UK
        return DocumentType.GDPR_EU
    
    async def _create_memory_chunks(self, text: str, doc_type: DocumentType, structure: Dict, 
                                  file_path: str, page_info: Dict, position_offset: int) -> List[MemoryChunk]:
        """Create enhanced memory chunks with legal concept extraction"""
        chunks = []
        text_chunks = self.text_splitter.split_text(text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Extract legal concepts using o3-mini
            legal_concepts = await self._extract_legal_concepts(chunk_text)
            
            # Determine chunk characteristics
            chunk_type = self._determine_chunk_type(chunk_text)
            title = self._extract_title(chunk_text)
            hierarchy_level = self._determine_hierarchy_level(chunk_text)
            article_number = self._extract_article_number(chunk_text)
            chapter_number = self._extract_chapter_number(chunk_text)
            page_number = self._estimate_page_number(i, len(text_chunks), len(page_info))
            
            # Create memory chunk
            memory_chunk = MemoryChunk(
                document_type=doc_type,
                chunk_type=chunk_type,
                title=title,
                content=chunk_text,
                chapter_number=chapter_number,
                article_number=article_number,
                hierarchy_level=hierarchy_level,
                page_number=page_number,
                position_in_document=position_offset + i,
                keywords=self._extract_keywords(chunk_text),
                legal_concepts=legal_concepts,
                metadata={
                    "source_file": file_path,
                    "chunk_index": i,
                    "total_chunks": len(text_chunks),
                    "structure_info": structure
                }
            )
            
            chunks.append(memory_chunk)
        
        return chunks
    
    async def _extract_legal_concepts(self, text: str) -> List[str]:
        """Extract legal concepts using o3-mini reasoning"""
        concept_prompt = ChatPromptTemplate.from_template("""
        Extract key legal concepts from this GDPR text. Focus on:
        - Rights (e.g., right to erasure, right of access)
        - Legal bases (e.g., consent, legitimate interest)
        - Processes (e.g., data processing, profiling)
        - Entities (e.g., data controller, data processor)
        - Principles (e.g., data minimization, purpose limitation)
        
        Text: {text}
        
        Return as JSON array: ["concept1", "concept2", ...]
        """)
        
        try:
            parser = JsonOutputParser()
            chain = concept_prompt | self.reasoning_service.llm | parser
            
            result = await chain.ainvoke({"text": text[:1000]})
            return result if isinstance(result, list) else []
        except Exception as e:
            logger.error(f"Error extracting legal concepts: {e}")
            return []
    
    def _chunks_to_documents(self, chunks: List[MemoryChunk]) -> List[Document]:
        """Convert memory chunks to LangChain Documents"""
        documents = []
        
        for chunk in chunks:
            doc = Document(
                page_content=chunk.content,
                metadata={
                    "chunk_id": chunk.id,
                    "document_type": chunk.document_type.value,
                    "chunk_type": chunk.chunk_type.value,
                    "title": chunk.title,
                    "hierarchy_level": chunk.hierarchy_level,
                    "article_number": chunk.article_number,
                    "chapter_number": chunk.chapter_number,
                    "page_number": chunk.page_number,
                    "position_in_document": chunk.position_in_document,
                    "keywords": chunk.keywords,
                    "legal_concepts": chunk.legal_concepts,
                    **chunk.metadata
                }
            )
            documents.append(doc)
        
        return documents
    
    # Helper methods (shortened for space)
    def _determine_chunk_type(self, text: str) -> ChunkType:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return ChunkType.CHAPTER
        elif "article" in text_lower[:100]:
            return ChunkType.ARTICLE
        elif any(marker in text_lower[:50] for marker in ["section", "§"]):
            return ChunkType.SECTION
        return ChunkType.PARAGRAPH
    
    def _extract_title(self, text: str) -> str:
        lines = text.split('\n')
        for line in lines[:3]:
            if line.strip() and len(line.strip()) < 120:
                return line.strip()
        return text[:60] + "..."
    
    def _determine_hierarchy_level(self, text: str) -> int:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return 1
        elif "article" in text_lower[:100]:
            return 2
        elif "section" in text_lower[:100]:
            return 3
        return 4
    
    def _extract_article_number(self, text: str) -> Optional[str]:
        import re
        match = re.search(r'article\s+(\d+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_chapter_number(self, text: str) -> Optional[str]:
        import re
        match = re.search(r'chapter\s+(\d+|[ivx]+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_keywords(self, text: str) -> List[str]:
        legal_keywords = [
            "personal data", "data subject", "consent", "processing", "controller",
            "processor", "lawful basis", "legitimate interest", "data protection",
            "rights", "erasure", "rectification", "portability", "breach", "profiling"
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in legal_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _estimate_page_number(self, chunk_index: int, total_chunks: int, total_pages: int) -> int:
        return min(int((chunk_index / total_chunks) * total_pages) + 1, total_pages)

# =============================================================================
# ENHANCED MULTI-AGENT SYSTEM WITH LONG-TERM MEMORY
# =============================================================================

class GDPRLongTermMemorySystem:
    def __init__(self, elasticsearch_url: str = "http://localhost:9200"):
        # Initialize services
        self.elasticsearch_service = EnhancedLangChainElasticsearchService(elasticsearch_url)
        self.reasoning_service = OpenAIReasoningService()
        self.memory_manager = LongTermMemoryManager()
        
        # Initialize enhanced agents
        self.document_parser = EnhancedDocumentParsingAgent(
            self.reasoning_service, self.memory_manager
        )
        
        # Create workflow
        self.workflow = self._create_memory_workflow()
    
    def _create_memory_workflow(self) -> StateGraph:
        """Create enhanced workflow with long-term memory"""
        
        workflow = StateGraph(LongTermMemoryState)
        
        # Add nodes
        workflow.add_node("parse_with_memory", self._parse_documents_with_memory_node)
        workflow.add_node("generate_embeddings", self._generate_embeddings_node)
        workflow.add_node("build_memory_index", self._build_memory_index_node)
        workflow.add_node("create_vectorstore", self._create_vectorstore_node)
        workflow.add_node("setup_memory_chains", self._setup_memory_chains_node)
        
        # Define edges
        workflow.add_edge(START, "parse_with_memory")
        workflow.add_edge("parse_with_memory", "generate_embeddings")
        workflow.add_edge("generate_embeddings", "build_memory_index")
        workflow.add_edge("build_memory_index", "create_vectorstore")
        workflow.add_edge("create_vectorstore", "setup_memory_chains")
        workflow.add_edge("setup_memory_chains", END)
        
        return workflow.compile()
    
    async def _parse_documents_with_memory_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Parse documents and create memory chunks"""
        state.current_agent = "enhanced_document_parser"
        try:
            file_paths = [doc.metadata.get("path", "") for doc in state.documents if "path" in doc.metadata]
            documents = await self.document_parser.process_documents_with_memory(file_paths)
            state.documents = documents
            
            # Create memory chunks from documents
            state.memory_chunks = [
                self._document_to_memory_chunk(doc) for doc in documents
            ]
            
            logger.info(f"Created {len(state.memory_chunks)} memory-aware chunks")
        except Exception as e:
            state.errors.append(f"Memory parsing error: {str(e)}")
        return state
    
    async def _generate_embeddings_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Generate embeddings for all chunks"""
        state.current_agent = "embedding_generator"
        try:
            # Prepare texts for embedding
            texts = [f"{chunk.title}\n\n{chunk.content}" for chunk in state.memory_chunks]
            
            # Generate embeddings in batches
            batch_size = 100
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_chunks = state.memory_chunks[i:i + batch_size]
                
                embeddings_response = await self.elasticsearch_service.embeddings.aembed_documents(batch_texts)
                
                for chunk, embedding in zip(batch_chunks, embeddings_response):
                    chunk.embedding = embedding
                
                logger.info(f"Generated embeddings for batch {i//batch_size + 1}")
            
        except Exception as e:
            state.errors.append(f"Embedding generation error: {str(e)}")
        return state
    
    async def _build_memory_index_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Build comprehensive long-term memory index"""
        state.current_agent = "memory_index_builder"
        try:
            # Add each chunk to long-term memory
            for chunk in state.memory_chunks:
                self.memory_manager.add_chunk_to_memory(chunk, state)
            
            logger.info(f"Built memory index with {len(state.memory_chunks)} chunks")
            logger.info(f"Found {len(state.relationships)} semantic relationships")
            logger.info(f"Created {len(state.semantic_clusters)} semantic clusters")
            
        except Exception as e:
            state.errors.append(f"Memory indexing error: {str(e)}")
        return state
    
    async def _create_vectorstore_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Create vector store from memory chunks"""
        state.current_agent = "vectorstore_creator"
        try:
            vectorstore = self.elasticsearch_service.create_vectorstore()
            
            # Add documents to vector store
            await vectorstore.aadd_documents(state.documents)
            state.vectorstore = vectorstore
            
            logger.info("Created enhanced vector store")
        except Exception as e:
            state.errors.append(f"Vector store error: {str(e)}")
        return state
    
    async def _setup_memory_chains_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Setup QA chains with long-term memory"""
        state.current_agent = "memory_chain_creator"
        try:
            if state.vectorstore:
                qa_chain, conv_chain, memory = self.elasticsearch_service.create_memory_aware_qa_chains(state.vectorstore)
                state.qa_chain = qa_chain
                state.conversational_chain = conv_chain
                state.conversation_memory = memory
                
                logger.info("Created memory-aware QA chains")
        except Exception as e:
            state.errors.append(f"QA chain setup error: {str(e)}")
        return state
    
    def _document_to_memory_chunk(self, doc: Document) -> MemoryChunk:
        """Convert LangChain Document to MemoryChunk"""
        return MemoryChunk(
            id=doc.metadata.get("chunk_id", str(uuid4())),
            document_type=DocumentType(doc.metadata["document_type"]),
            chunk_type=ChunkType(doc.metadata["chunk_type"]),
            title=doc.metadata["title"],
            content=doc.page_content,
            chapter_number=doc.metadata.get("chapter_number"),
            article_number=doc.metadata.get("article_number"),
            hierarchy_level=doc.metadata["hierarchy_level"],
            page_number=doc.metadata.get("page_number"),
            position_in_document=doc.metadata["position_in_document"],
            keywords=doc.metadata.get("keywords", []),
            legal_concepts=doc.metadata.get("legal_concepts", []),
            metadata=doc.metadata
        )
    
    # =============================================================================
    # PUBLIC API WITH LONG-TERM MEMORY
    # =============================================================================
    
    async def process_documents(self, file_paths: List[str]) -> LongTermMemoryState:
        """Process documents with comprehensive long-term memory"""
        
        initial_state = LongTermMemoryState(
            documents=[Document(page_content="", metadata={"path": path}) for path in file_paths],
            metadata={
                "started_at": datetime.now(),
                "file_paths": file_paths
            }
        )
        
        try:
            final_state = await self.workflow.ainvoke(initial_state)
            final_state.metadata["completed_at"] = datetime.now()
            final_state.metadata["duration"] = (
                final_state.metadata["completed_at"] - final_state.metadata["started_at"]
            ).total_seconds()
            
            logger.info(f"Processing completed in {final_state.metadata['duration']:.2f} seconds")
            return final_state
            
        except Exception as e:
            logger.error(f"Workflow error: {e}")
            initial_state.errors.append(f"Workflow error: {str(e)}")
            return initial_state
    
    async def ask_with_memory(self, state: LongTermMemoryState, question: str, 
                            use_long_term_context: bool = True) -> Dict[str, Any]:
        """Ask question with long-term memory awareness"""
        
        if not state.qa_chain:
            return {"error": "QA chain not available"}
        
        try:
            # If using long-term context, enhance the question with relevant memories
            enhanced_question = question
            related_context = ""
            
            if use_long_term_context:
                # Find relevant concepts and chunks
                related_chunks = self._find_related_memories(question, state)
                if related_chunks:
                    related_context = "\n\nAdditional relevant context from document memory:\n"
                    for chunk in related_chunks[:3]:  # Top 3 most relevant
                        related_context += f"- {chunk.title}: {chunk.content[:200]}...\n"
                    
                    enhanced_question = f"{question}\n{related_context}"
            
            result = await state.qa_chain.ainvoke({"query": enhanced_question})
            
            return {
                "answer": result["result"],
                "source_documents": [
                    {
                        "content": doc.page_content[:300] + "...",
                        "metadata": doc.metadata
                    }
                    for doc in result.get("source_documents", [])
                ],
                "long_term_context_used": bool(related_context),
                "related_memories": len(related_chunks) if use_long_term_context else 0
            }
            
        except Exception as e:
            return {"error": f"Memory-aware QA error: {str(e)}"}
    
    async def conversational_qa_with_memory(self, state: LongTermMemoryState, question: str) -> Dict[str, Any]:
        """Conversational QA with enhanced memory"""
        
        if not state.conversational_chain:
            return {"error": "Conversational chain not available"}
        
        try:
            result = await state.conversational_chain.ainvoke({"question": question})
            
            return {
                "answer": result["answer"],
                "source_documents": [
                    {
                        "content": doc.page_content[:300] + "...",
                        "metadata": doc.metadata
                    }
                    for doc in result.get("source_documents", [])
                ],
                "conversation_summary": state.conversation_memory.moving_summary_buffer if state.conversation_memory else None
            }
        except Exception as e:
            return {"error": f"Conversational memory QA error: {str(e)}"}
    
    def _find_related_memories(self, question: str, state: LongTermMemoryState, top_k: int = 5) -> List[MemoryChunk]:
        """Find chunks related to the question using long-term memory"""
        
        # Extract concepts from question
        question_lower = question.lower()
        relevant_chunks = []
        
        # Find chunks by concept matching
        for concept, chunk_ids in state.concept_memory.items():
            if concept.lower() in question_lower:
                for chunk_id in chunk_ids:
                    chunk = next((c for c in state.memory_chunks if c.id == chunk_id), None)
                    if chunk:
                        relevant_chunks.append(chunk)
        
        # Score by similarity and relevance
        scored_chunks = []
        for chunk in relevant_chunks:
            relevance_score = 0
            
            # Concept overlap score
            concept_overlap = len(set(chunk.legal_concepts) & set(question_lower.split()))
            relevance_score += concept_overlap * 0.3
            
            # Keyword overlap score
            keyword_overlap = len(set(chunk.keywords) & set(question_lower.split()))
            relevance_score += keyword_overlap * 0.2
            
            # Article/chapter relevance
            if chunk.article_number and "article" in question_lower:
                relevance_score += 0.3
            
            scored_chunks.append((chunk, relevance_score))
        
        # Sort by relevance and return top k
        scored_chunks.sort(key=lambda x: x[1], reverse=True)
        return [chunk for chunk, score in scored_chunks[:top_k]]
    
    def get_memory_summary(self, state: LongTermMemoryState) -> Dict[str, Any]:
        """Get comprehensive memory summary"""
        return {
            "total_chunks": len(state.memory_chunks),
            "total_relationships": len(state.relationships),
            "semantic_clusters": len(state.semantic_clusters),
            "concept_coverage": len(state.concept_memory),
            "document_types": list(set(chunk.document_type.value for chunk in state.memory_chunks)),
            "hierarchy_distribution": {
                level: len([c for c in state.memory_chunks if c.hierarchy_level == level])
                for level in range(1, 6)
            },
            "top_concepts": sorted(
                [(concept, len(chunk_ids)) for concept, chunk_ids in state.concept_memory.items()],
                key=lambda x: x[1], reverse=True
            )[:10],
            "relationship_types": {
                rel_type.value: len([r for r in state.relationships if r.relationship_type == rel_type])
                for rel_type in RelationshipType
            }
        }

# =============================================================================
# ENHANCED USAGE EXAMPLE
# =============================================================================

async def main():
    """Enhanced example with long-term memory capabilities"""
    
    os.environ["OPENAI_API_KEY"] = "your-openai-api-key-here"
    
    # Initialize enhanced system
    gdpr_system = GDPRLongTermMemorySystem()
    
    try:
        print("🧠 Processing GDPR documents with long-term memory...")
        
        document_paths = [
            "path/to/gdpr_eu_regulation.pdf",
            "path/to/uk_gdpr_guidance.pdf"
        ]
        
        state = await gdpr_system.process_documents(document_paths)
        
        # Display memory summary
        memory_summary = gdpr_system.get_memory_summary(state)
        print(f"📊 Memory Summary:")
        print(f"   📄 Total chunks: {memory_summary['total_chunks']}")
        print(f"   🔗 Relationships: {memory_summary['total_relationships']}")
        print(f"   🧩 Semantic clusters: {memory_summary['semantic_clusters']}")
        print(f"   💡 Legal concepts: {memory_summary['concept_coverage']}")
        
        if state.errors:
            print(f"⚠️ Errors: {state.errors}")
        
        # Memory-aware QA examples
        if state.qa_chain:
            print("\n🤖 Memory-Aware QA Examples:")
            
            questions = [
                "What are the lawful bases for processing personal data and how do they relate to consent requirements?",
                "How does the right to erasure connect to data minimization principles?",
                "What are the key differences between controllers and processors in data protection?",
                "How do breach notification requirements relate to data subject rights?"
            ]
            
            for question in questions:
                print(f"\n❓ Q: {question}")
                answer = await gdpr_system.ask_with_memory(state, question, use_long_term_context=True)
                
                if "error" not in answer:
                    print(f"💡 A: {answer['answer'][:400]}...")
                    print(f"📚 Sources: {len(answer['source_documents'])}")
                    print(f"🧠 Long-term memories used: {answer['related_memories']}")
        
        # Conversational memory example
        if state.conversational_chain:
            print("\n💬 Long-term Conversational Memory:")
            
            # First question
            conv1 = await gdpr_system.conversational_qa_with_memory(
                state, "Tell me about data subject rights under GDPR"
            )
            if "error" not in conv1:
                print(f"💡 Initial: {conv1['answer'][:200]}...")
            
            # Follow-up that references distant concepts
            conv2 = await gdpr_system.conversational_qa_with_memory(
                state, "How do those rights relate to the lawfulness provisions we discussed? Are there any conflicts?"
            )
            if "error" not in conv2:
                print(f"🔄 Follow-up: {conv2['answer'][:200]}...")
                if conv2.get('conversation_summary'):
                    print(f"📝 Conversation summary: {conv2['conversation_summary'][:150]}...")
        
        print(f"\n✅ Enhanced processing completed in {state.metadata.get('duration', 0):.2f} seconds")
        
    except Exception as e:
        print(f"❌ Error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
