#!/usr/bin/env python3
"""
FalkorDB CSV to Knowledge Graph Converter with OpenAI Embeddings
A streamlined tool for creating knowledge graphs from CSV data with OpenAI vector embeddings.

Features:
- OpenAI embeddings (text-embedding-3-large, text-embedding-3-small)
- Data quality validation and analysis
- User-defined relationships only
- FalkorDB vector indexing
- Performance optimizations

Usage:
    python enhanced_graph_converter.py <graph_name> <csv_directory_path> <config_file_path>
"""

import os
import sys
import json
import time
import logging
import argparse
import asyncio
import aiohttp
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

import pandas as pd
import numpy as np
from openai import OpenAI

try:
    from falkordb import FalkorDB
    import redis
except ImportError:
    print("Error: FalkorDB and redis packages not found. Install with: pip install falkordb redis")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('graph_converter.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DataQualityConfig:
    """Configuration for data quality validation."""
    enabled: bool = True
    max_null_percentage: float = 0.5
    duplicate_detection: bool = True
    outlier_detection: bool = True
    data_profiling: bool = True

class DataQualityAnalyzer:
    """Analyzes and validates data quality."""
    
    def __init__(self, config: DataQualityConfig):
        self.config = config
    
    def analyze_dataframe(self, df: pd.DataFrame) -> Dict:
        """Analyze data quality of a DataFrame."""
        report = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'null_percentages': {},
            'duplicate_rows': 0,
            'data_types': {},
            'outliers': {},
            'quality_score': 0.0
        }
        
        if not self.config.enabled:
            return report
        
        # Calculate null percentages
        for col in df.columns:
            null_pct = df[col].isnull().sum() / len(df) if len(df) > 0 else 0
            report['null_percentages'][col] = null_pct
        
        # Check for duplicates
        if self.config.duplicate_detection:
            report['duplicate_rows'] = df.duplicated().sum()
        
        # Data type analysis
        for col in df.columns:
            report['data_types'][col] = str(df[col].dtype)
        
        # Outlier detection for numeric columns
        if self.config.outlier_detection:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if len(df[col].dropna()) > 0:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    if IQR > 0:
                        outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]
                        report['outliers'][col] = len(outliers)
                    else:
                        report['outliers'][col] = 0
        
        # Calculate overall quality score
        avg_null_pct = np.mean(list(report['null_percentages'].values())) if report['null_percentages'] else 0
        duplicate_pct = report['duplicate_rows'] / len(df) if len(df) > 0 else 0
        
        quality_score = max(0, 1.0 - avg_null_pct - duplicate_pct)
        report['quality_score'] = quality_score
        
        return report

class OpenAIEmbeddingProvider:
    """OpenAI embedding provider supporting text-embedding-3-large and text-embedding-3-small."""
    
    def __init__(self, model_name: str, api_key: str, dimensions: Optional[int] = None, batch_size: int = 100):
        self.model_name = model_name
        self.batch_size = batch_size
        self.client = OpenAI(api_key=api_key)
        
        # Set default dimensions based on model
        if dimensions:
            self.dimensions = dimensions
        elif 'text-embedding-3-large' in model_name:
            self.dimensions = 3072
        elif 'text-embedding-3-small' in model_name:
            self.dimensions = 1536
        else:
            self.dimensions = 1536
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts using OpenAI API."""
        embeddings = []
        
        # Process in batches
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            
            try:
                response = self.client.embeddings.create(
                    input=batch,
                    model=self.model_name,
                    dimensions=self.dimensions
                )
                
                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                
                logger.debug(f"Generated embeddings for batch {i//self.batch_size + 1}")
                
            except Exception as e:
                logger.error(f"Error generating OpenAI embeddings for batch {i//self.batch_size + 1}: {e}")
                # Return zero vectors for failed batches
                embeddings.extend([[0.0] * self.dimensions] * len(batch))
        
        return embeddings
    
    def embed_text(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        try:
            response = self.client.embeddings.create(
                input=[text],
                model=self.model_name,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating OpenAI embedding: {e}")
            return [0.0] * self.dimensions

class GraphConverter:
    """Main class for converting CSV data to FalkorDB graph database with OpenAI embeddings."""
    
    def __init__(self, graph_name: str, csv_path: str, config_path: str):
        """Initialize the GraphConverter with configuration."""
        self.graph_name = graph_name
        self.csv_path = Path(csv_path)
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # Initialize components
        self.db = None
        self.graph = None
        self.redis_client = None
        self.embedding_provider = None
        self.data_quality_analyzer = DataQualityAnalyzer(
            DataQualityConfig(**self.config.get('data_quality', {}))
        )
        
        # Statistics tracking
        self.stats = {
            'nodes_created': 0,
            'relationships_created': 0,
            'embeddings_created': 0,
            'vector_indexes_created': 0,
            'indexes_created': 0,
            'constraints_created': 0,
            'processing_time': 0,
            'files_processed': 0,
            'data_quality_reports': {},
            'errors': []
        }
        
        self._setup_database()
        self._setup_embedding_provider()
    
    def _load_config(self) -> Dict:
        """Load and validate configuration."""
        try:
            with open(self.config_path, 'r') as f:
                config = json.load(f)
            
            # Add default configurations if not present
            if 'embedding' not in config:
                config['embedding'] = {
                    'enabled': False,
                    'model_name': 'text-embedding-3-small'
                }
            
            if 'data_quality' not in config:
                config['data_quality'] = {
                    'enabled': True,
                    'max_null_percentage': 0.5,
                    'duplicate_detection': True,
                    'outlier_detection': True,
                    'data_profiling': True
                }
            
            logger.info(f"Configuration loaded from {self.config_path}")
            return config
            
        except Exception as e:
            logger.error(f"Error loading config: {e}")
            raise
    
    def _setup_database(self):
        """Initialize FalkorDB connection."""
        try:
            db_config = self.config.get('database', {})
            
            connection_params = {
                'host': db_config.get('host', 'localhost'),
                'port': db_config.get('port', 6379)
            }
            
            if db_config.get('password'):
                connection_params['password'] = db_config.get('password')
            
            self.db = FalkorDB(**connection_params)
            self.graph = self.db.select_graph(self.graph_name)
            self.redis_client = redis.Redis(**connection_params, decode_responses=True)
            
            logger.info(f"Connected to FalkorDB: {self.graph_name}")
            
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            raise
    
    def _setup_embedding_provider(self):
        """Initialize OpenAI embedding provider."""
        embedding_config = self.config.get('embedding', {})
        
        if not embedding_config.get('enabled', False):
            logger.info("Embedding support disabled")
            return
        
        try:
            api_key = embedding_config.get('api_key')
            if not api_key:
                api_key = os.getenv('OPENAI_API_KEY')
            
            if not api_key:
                raise ValueError("OpenAI API key not provided in config or environment")
            
            self.embedding_provider = OpenAIEmbeddingProvider(
                model_name=embedding_config.get('model_name', 'text-embedding-3-small'),
                api_key=api_key,
                dimensions=embedding_config.get('dimensions'),
                batch_size=embedding_config.get('batch_size', 100)
            )
            
            logger.info(f"Initialized OpenAI embedding provider with model: {embedding_config.get('model_name')}")
            
        except Exception as e:
            logger.error(f"Error setting up OpenAI embedding provider: {e}")
            self.embedding_provider = None
    
    def _create_indexes_and_constraints(self):
        """Create indexes, constraints, and vector indexes."""
        try:
            # Create traditional indexes
            indexes = self.config.get('indexes', [])
            for index_config in indexes:
                label = index_config['label']
                properties = index_config['properties']
                
                for prop in properties:
                    try:
                        query = f"CREATE INDEX FOR (n:{label}) ON (n.{prop})"
                        self.graph.query(query)
                        self.stats['indexes_created'] += 1
                        logger.info(f"Created index on {label}.{prop}")
                    except Exception as e:
                        error_msg = str(e).lower()
                        if "already indexed" in error_msg or "already exists" in error_msg:
                            logger.debug(f"Index already exists for {label}.{prop}")
                        else:
                            logger.warning(f"Index creation failed for {label}.{prop}: {e}")
            
            # Create constraints
            constraints = self.config.get('constraints', [])
            for constraint_config in constraints:
                label = constraint_config['label']
                property_name = constraint_config['property']
                constraint_type = constraint_config.get('type', 'UNIQUE')
                
                try:
                    if constraint_type == 'UNIQUE':
                        result = self.redis_client.execute_command(
                            'GRAPH.CONSTRAINT', 'CREATE', 
                            self.graph_name, 
                            'UNIQUE', 'NODE', label, 
                            'PROPERTIES', '1', property_name
                        )
                        self.stats['constraints_created'] += 1
                        logger.info(f"Created unique constraint on {label}.{property_name}")
                        
                except Exception as e:
                    error_msg = str(e).lower()
                    if "already exists" in error_msg or "pending" in error_msg:
                        logger.debug(f"Constraint already exists for {label}.{property_name}")
                    else:
                        logger.warning(f"Constraint creation failed for {label}.{property_name}: {e}")
            
            # Create vector indexes if embedding provider is available
            if self.embedding_provider:
                self._create_vector_indexes()
                        
        except Exception as e:
            logger.error(f"Error creating indexes/constraints: {e}")
    
    def _create_vector_indexes(self):
        """Create vector indexes for embeddings."""
        vector_fields = self.config.get('embedding', {}).get('vector_fields', [])
        
        for field_config in vector_fields:
            try:
                entity_pattern = field_config['entity_pattern']
                attribute = field_config['attribute']
                
                options = {
                    'dimension': self.embedding_provider.dimensions,
                    'similarityFunction': field_config.get('similarityFunction', 'cosine'),
                    'M': field_config.get('M', 16),
                    'efConstruction': field_config.get('efConstruction', 200),
                    'efRuntime': field_config.get('efRuntime', 10)
                }
                
                # Build options string properly
                options_str = ", ".join([f"{k}: {v}" if isinstance(v, (int, float)) else f"{k}: '{v}'" for k, v in options.items()])
                
                query = f"CREATE VECTOR INDEX FOR {entity_pattern} ON ({attribute}) OPTIONS {{{options_str}}}"
                
                self.graph.query(query)
                self.stats['vector_indexes_created'] += 1
                logger.info(f"Created vector index: {entity_pattern} ON {attribute}")
                
            except Exception as e:
                error_msg = str(e).lower()
                if "already exists" in error_msg:
                    logger.debug(f"Vector index already exists: {entity_pattern}")
                else:
                    logger.error(f"Error creating vector index: {e}")
    
    def _generate_embeddings_for_text_fields(self, df: pd.DataFrame, text_fields: List[str]) -> Dict[str, List[List[float]]]:
        """Generate embeddings for specified text fields."""
        if not self.embedding_provider:
            return {}
        
        embeddings = {}
        
        for field in text_fields:
            if field not in df.columns:
                logger.warning(f"Field '{field}' not found in DataFrame columns")
                continue
            
            texts = df[field].fillna('').astype(str).tolist()
            
            try:
                field_embeddings = self.embedding_provider.embed_texts(texts)
                embeddings[field] = field_embeddings
                self.stats['embeddings_created'] += len(field_embeddings)
                logger.info(f"Generated {len(field_embeddings)} embeddings for field: {field}")
                
            except Exception as e:
                logger.error(f"Error generating embeddings for field {field}: {e}")
                embeddings[field] = [[0.0] * self.embedding_provider.dimensions] * len(texts)
        
        return embeddings
    
    def _process_nodes(self, file_config: Dict) -> int:
        """Process nodes with embeddings."""
        csv_file = self.csv_path / file_config['file']
        
        if not csv_file.exists():
            logger.error(f"CSV file not found: {csv_file}")
            return 0
        
        try:
            df = pd.read_csv(csv_file)
            logger.info(f"Processing {len(df)} rows from {csv_file}")
            
            # Data quality analysis
            quality_report = self.data_quality_analyzer.analyze_dataframe(df)
            self.stats['data_quality_reports'][file_config['file']] = quality_report
            logger.info(f"Data quality score for {csv_file.name}: {quality_report['quality_score']:.2f}")
            
            # Generate embeddings if configured
            embeddings = {}
            embedding_fields = file_config.get('embedding_fields', [])
            if embedding_fields and self.embedding_provider:
                embeddings = self._generate_embeddings_for_text_fields(df, embedding_fields)
            
            node_label = file_config['node_label']
            field_mappings = file_config['field_mappings']
            batch_size = file_config.get('batch_size', 1000)
            
            nodes_created = 0
            
            # Process in batches
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i + batch_size]
                
                create_statements = []
                for idx, (_, row) in enumerate(batch.iterrows()):
                    actual_idx = i + idx
                    
                    # Build basic properties
                    properties = self._build_cypher_properties(row, field_mappings)
                    
                    # Add embeddings as vector properties
                    for field, field_embeddings in embeddings.items():
                        if actual_idx < len(field_embeddings):
                            embedding = field_embeddings[actual_idx]
                            vector_field_name = f"{field}_embedding"
                            vector_str = f"vecf32({json.dumps(embedding)})"
                            # Remove closing brace and add vector property
                            if properties.endswith('}'):
                                if properties == '{}':
                                    properties = f"{{{vector_field_name}: {vector_str}}}"
                                else:
                                    properties = properties[:-1] + f", {vector_field_name}: {vector_str}" + "}"
                    
                    create_statements.append(f"CREATE (n{nodes_created}:{node_label} {properties})")
                    nodes_created += 1
                
                # Execute batch
                if create_statements:
                    query = " ".join(create_statements)
                    try:
                        self.graph.query(query)
                        logger.debug(f"Created batch of {len(create_statements)} {node_label} nodes")
                    except Exception as e:
                        logger.error(f"Error creating batch of {node_label} nodes: {e}")
                        self.stats['errors'].append(f"Node creation error in {csv_file}: {e}")
            
            logger.info(f"Created {nodes_created} {node_label} nodes from {csv_file.name}")
            return nodes_created
            
        except Exception as e:
            logger.error(f"Error processing nodes from {csv_file}: {e}")
            self.stats['errors'].append(f"Node processing error {csv_file}: {e}")
            return 0
    
    def _build_cypher_properties(self, row: pd.Series, field_mappings: Dict) -> str:
        """Build Cypher property string from pandas row."""
        properties = []
        
        for csv_field, graph_field in field_mappings.items():
            if csv_field in row.index:
                value = self._sanitize_value(row[csv_field])
                if value is not None:
                    if isinstance(value, str):
                        properties.append(f"{graph_field}: '{value}'")
                    else:
                        properties.append(f"{graph_field}: {value}")
        
        return "{" + ", ".join(properties) + "}"
    
    def _sanitize_value(self, value: Any) -> Any:
        """Sanitize and convert values for Cypher queries."""
        if pd.isna(value) or value is None:
            return None
        
        if isinstance(value, str):
            # Escape single quotes and handle special characters
            return value.replace("'", "\\'").replace('"', '\\"').replace('\n', '\\n').replace('\r', '\\r')
        
        if isinstance(value, (int, float)):
            return value
        
        if isinstance(value, bool):
            return str(value).lower()
        
        # Convert other types to string
        return str(value).replace("'", "\\'").replace('"', '\\"')
    
    def _process_relationships(self, file_config: Dict) -> int:
        """Process relationships from CSV file."""
        csv_file = self.csv_path / file_config['file']
        
        if not csv_file.exists():
            logger.error(f"CSV file not found: {csv_file}")
            return 0
        
        try:
            df = pd.read_csv(csv_file)
            logger.info(f"Processing {len(df)} relationships from {csv_file}")
            
            relationships_created = 0
            batch_size = file_config.get('batch_size', 1000)
            
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i + batch_size]
                
                for idx, row in batch.iterrows():
                    rel_config = file_config['relationship']
                    
                    # Check if foreign key values are not null before creating relationship
                    source_key = row[rel_config['source']['csv_field']]
                    target_key = row[rel_config['target']['csv_field']]
                    
                    if pd.isna(source_key) or pd.isna(target_key) or source_key is None or target_key is None:
                        logger.debug(f"Skipping relationship due to null key: source={source_key}, target={target_key}")
                        continue
                    
                    # Build MATCH clauses for source and target nodes
                    source_match = self._build_match_clause(row, rel_config['source'], "source")
                    target_match = self._build_match_clause(row, rel_config['target'], "target")
                    
                    # Build relationship properties
                    rel_properties = ""
                    if 'properties' in rel_config:
                        props = self._build_cypher_properties(row, rel_config['properties'])
                        if props != "{}":
                            rel_properties = f" {props}"
                    
                    # Create relationship query
                    rel_type = rel_config['type']
                    query = f"""
                    MATCH {source_match}
                    MATCH {target_match}
                    MERGE (source)-[r:{rel_type}{rel_properties}]->(target)
                    """
                    
                    try:
                        result = self.graph.query(query)
                        relationships_created += 1
                    except Exception as e:
                        logger.warning(f"Error creating relationship: {e}")
                        self.stats['errors'].append(f"Relationship creation error: {e}")
            
            logger.info(f"Created {relationships_created} relationships from {csv_file.name}")
            return relationships_created
            
        except Exception as e:
            logger.error(f"Error processing relationships from {csv_file}: {e}")
            self.stats['errors'].append(f"Relationship processing error {csv_file}: {e}")
            return 0
    
    def _build_match_clause(self, row: pd.Series, node_config: Dict, var_name: str = "n") -> str:
        """Build MATCH clause for finding nodes in relationships."""
        label = node_config['label']
        key_field = node_config['key_field']
        csv_field = node_config['csv_field']
        
        key_value = self._sanitize_value(row[csv_field])
        
        if isinstance(key_value, str):
            return f"({var_name}:{label} {{{key_field}: '{key_value}'}})"
        else:
            return f"({var_name}:{label} {{{key_field}: {key_value}}})"
    
    def _generate_profile_report(self) -> Dict:
        """Generate comprehensive profiling report."""
        try:
            # Get graph statistics
            node_count_query = "MATCH (n) RETURN count(n) as node_count"
            relationship_count_query = "MATCH ()-[r]->() RETURN count(r) as rel_count"
            
            node_result = self.graph.query(node_count_query)
            rel_result = self.graph.query(relationship_count_query)
            
            total_nodes = node_result.result_set[0][0] if node_result.result_set else 0
            total_relationships = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            # Get node type distribution
            node_types_query = "MATCH (n) RETURN labels(n) as label, count(n) as count ORDER BY count DESC"
            node_types_result = self.graph.query(node_types_query)
            
            node_distribution = {}
            for row in node_types_result.result_set:
                label = row[0][0] if row[0] else 'Unknown'
                count = row[1]
                node_distribution[label] = count
            
            # Get relationship type distribution
            rel_types_query = "MATCH ()-[r]->() RETURN type(r) as rel_type, count(r) as count ORDER BY count DESC"
            rel_types_result = self.graph.query(rel_types_query)
            
            rel_distribution = {}
            for row in rel_types_result.result_set:
                rel_type = row[0]
                count = row[1]
                rel_distribution[rel_type] = count
            
            # Embedding analytics
            embedding_analytics = {}
            if self.embedding_provider:
                embedding_analytics = {
                    'total_embeddings': self.stats['embeddings_created'],
                    'vector_dimensions': self.embedding_provider.dimensions,
                    'model_name': self.embedding_provider.model_name,
                    'vector_indexes': self.stats['vector_indexes_created']
                }
            
            profile_report = {
                'summary': {
                    'total_nodes': total_nodes,
                    'total_relationships': total_relationships,
                    'graph_density': total_relationships / (total_nodes * (total_nodes - 1)) if total_nodes > 1 else 0,
                    'processing_time_seconds': self.stats['processing_time'],
                    'files_processed': self.stats['files_processed']
                },
                'distributions': {
                    'nodes': node_distribution,
                    'relationships': rel_distribution
                },
                'embeddings': embedding_analytics,
                'data_quality': self.stats['data_quality_reports'],
                'performance_stats': self.stats,
                'timestamp': datetime.now().isoformat()
            }
            
            return profile_report
            
        except Exception as e:
            logger.error(f"Error generating profile report: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    def convert(self):
        """Main conversion process."""
        start_time = time.time()
        
        try:
            logger.info("Starting CSV to FalkorDB conversion with OpenAI embeddings...")
            
            # Create indexes, constraints, and vector indexes
            self._create_indexes_and_constraints()
            
            # Process node files
            node_files = self.config.get('node_files', [])
            for file_config in node_files:
                nodes_created = self._process_nodes(file_config)
                self.stats['nodes_created'] += nodes_created
                self.stats['files_processed'] += 1
            
            # Process relationship files
            relationship_files = self.config.get('relationship_files', [])
            for file_config in relationship_files:
                relationships_created = self._process_relationships(file_config)
                self.stats['relationships_created'] += relationships_created
                self.stats['files_processed'] += 1
            
            # Calculate processing time
            self.stats['processing_time'] = time.time() - start_time
            
            # Generate profile report
            profile_report = self._generate_profile_report()
            
            # Save profile report
            profile_path = f"{self.graph_name}_profile_report.json"
            with open(profile_path, 'w') as f:
                json.dump(profile_report, f, indent=2)
            
            logger.info("Conversion completed successfully!")
            logger.info(f"Profile report saved to: {profile_path}")
            logger.info(f"Total nodes created: {self.stats['nodes_created']}")
            logger.info(f"Total relationships created: {self.stats['relationships_created']}")
            logger.info(f"Total embeddings created: {self.stats['embeddings_created']}")
            logger.info(f"Vector indexes created: {self.stats['vector_indexes_created']}")
            logger.info(f"Processing time: {self.stats['processing_time']:.2f} seconds")
            
            if self.stats['errors']:
                logger.warning(f"Encountered {len(self.stats['errors'])} errors during processing")
                for error in self.stats['errors']:
                    logger.warning(f"Error: {error}")
            
        except Exception as e:
            logger.error(f"Conversion failed: {e}")
            raise
        finally:
            if self.db:
                self.db.close()
            if self.redis_client:
                self.redis_client.close()


def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description="Convert CSV files to FalkorDB graph database with OpenAI embeddings",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python graph_converter.py ecommerce_graph ./csv_files ./config.json
    python graph_converter.py social_network /path/to/csvs /path/to/config.json
        """
    )
    
    parser.add_argument('graph_name', help='Name of the graph to create in FalkorDB')
    parser.add_argument('csv_path', help='Path to directory containing CSV files')
    parser.add_argument('config_path', help='Path to configuration JSON file')
    
    args = parser.parse_args()
    
    # Validate arguments
    if not os.path.exists(args.csv_path):
        print(f"Error: CSV directory not found: {args.csv_path}")
        sys.exit(1)
    
    if not os.path.exists(args.config_path):
        print(f"Error: Configuration file not found: {args.config_path}")
        sys.exit(1)
    
    try:
        converter = GraphConverter(args.graph_name, args.csv_path, args.config_path)
        converter.convert()
        print(f"Successfully converted CSV data to FalkorDB graph: {args.graph_name}")
        
    except Exception as e:
        print(f"Conversion failed: {e}")
        sys.exit(1)
