#!/usr/bin/env python3
"""
Robust RDF to FalkorDB Property Graph Converter
=============================================

Converts RDF/Turtle files to FalkorDB property graphs with:
- Robust error handling and query sanitization
- Efficient batch processing
- Cross-platform compatibility
- Proper connection management
- Memory-efficient streaming for large files
"""

import os
import sys
import time
import logging
import re
import hashlib
from typing import Dict, List, Set, Tuple, Any, Optional, Iterator
from collections import defaultdict
from pathlib import Path

# Core dependencies with fallback handling
try:
    import falkordb
except ImportError:
    print("‚ùå FalkorDB not installed. Install with: pip install falkordb")
    sys.exit(1)

try:
    from rdflib import Graph, URIRef, Literal, BNode
    from rdflib.namespace import RDF, RDFS, XSD, FOAF
except ImportError:
    print("‚ùå RDFLib not installed. Install with: pip install rdflib")
    sys.exit(1)


class RDFToFalkorConverter:
    """
    Converts RDF files to FalkorDB property graphs with robust error handling.
    """
    
    def __init__(self,
                 host: str = "localhost",
                 port: int = 6379,
                 password: Optional[str] = None,
                 socket_timeout: int = 60,
                 connection_timeout: int = 30,
                 batch_size: int = 1000,
                 node_batch_size: int = 100,
                 relationship_batch_size: int = 50,
                 max_property_length: int = 1000,
                 max_label_length: int = 50):
        
        # Connection settings
        self.connection_params = {
            'host': host,
            'port': port,
            'socket_timeout': socket_timeout,
            'socket_connect_timeout': connection_timeout,
            'socket_keepalive': True,
            'retry_on_timeout': True,
            'health_check_interval': 30
        }
        
        if password:
            self.connection_params['password'] = password
        
        # Processing settings
        self.batch_size = batch_size
        self.node_batch_size = node_batch_size
        self.relationship_batch_size = relationship_batch_size
        self.max_property_length = max_property_length
        self.max_label_length = max_label_length
        
        # Data structures
        self.entity_index = {}
        self.entity_counter = 0
        self.property_cache = {}
        self.label_cache = {}
        
        # Statistics
        self.stats = {
            'triples_processed': 0,
            'nodes_created': 0,
            'relationships_created': 0,
            'queries_executed': 0,
            'errors_handled': 0,
            'start_time': None,
            'end_time': None
        }
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        # Connection (initialized on first use)
        self._connection = None
        
        self.logger.info("üöÄ RDF to FalkorDB converter initialized")
    
    def get_connection(self) -> falkordb.FalkorDB:
        """Get or create FalkorDB connection with proper error handling."""
        if self._connection is None:
            try:
                self.logger.info("üîå Connecting to FalkorDB...")
                self._connection = falkordb.FalkorDB(**self.connection_params)
                
                # Test connection
                test_graph = self._connection.select_graph("_test_connection")
                test_graph.query("RETURN 1")
                
                self.logger.info("‚úÖ Connected to FalkorDB successfully")
            except Exception as e:
                self.logger.error(f"‚ùå Failed to connect to FalkorDB: {e}")
                raise ConnectionError(f"Cannot connect to FalkorDB: {e}")
        
        return self._connection
    
    def execute_query(self, graph_name: str, query: str, max_retries: int = 3) -> Any:
        """Execute query with automatic retry and error handling."""
        for attempt in range(max_retries):
            try:
                db = self.get_connection()
                graph = db.select_graph(graph_name)
                result = graph.query(query)
                self.stats['queries_executed'] += 1
                return result
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # Handle connection errors
                if 'connection' in error_msg or 'timeout' in error_msg:
                    self._connection = None  # Reset connection
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Connection error, retrying... ({attempt + 1}/{max_retries})")
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                
                # Handle specific Cypher syntax errors that suggest batching issues
                elif any(phrase in error_msg for phrase in ['with clause', 'updating clause', 'syntax', 'invalid']):
                    self.logger.error(f"Cypher syntax error (likely batching issue): {e}")
                    self.logger.debug(f"Problematic query: {query[:500]}...")
                    raise ValueError(f"Cypher syntax error: {e}")
                
                # Other errors
                else:
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Query failed, retrying... ({attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        self.logger.error(f"Query failed after {max_retries} attempts: {e}")
                        self.stats['errors_handled'] += 1
                        raise
        
        raise RuntimeError(f"Query failed after {max_retries} attempts")
    
    def execute_query_batch(self, graph_name: str, queries: List[str], max_retries: int = 3) -> int:
        """Execute a batch of queries with proper separation and error handling."""
        if not queries:
            return 0
        
        # Clean up queries first
        clean_queries = []
        for query in queries:
            clean_query = query.strip()
            if clean_query:
                clean_queries.append(clean_query)
        
        if not clean_queries:
            return 0
        
        # Try different batching approaches
        approaches = [
            # Approach 1: Semicolon separated (most compatible)
            lambda qs: '; '.join(q.rstrip(';') for q in qs) + ';',
            # Approach 2: Space separated (fallback)
            lambda qs: ' '.join(q.rstrip(';') + ';' for q in qs),
            # Approach 3: Individual execution (always works)
            lambda qs: None  # Special marker for individual execution
        ]
        
        for approach_idx, approach in enumerate(approaches):
            try:
                if approach(clean_queries) is None:
                    # Individual execution approach
                    executed = 0
                    for query in clean_queries:
                        try:
                            self.execute_query(graph_name, query)
                            executed += 1
                        except Exception as e:
                            self.logger.debug(f"Individual query failed: {e}")
                            self.stats['errors_handled'] += 1
                    return executed
                else:
                    # Batch execution approach
                    combined_query = approach(clean_queries)
                    self.execute_query(graph_name, combined_query)
                    return len(clean_queries)
                    
            except Exception as e:
                if approach_idx == len(approaches) - 1:
                    # Last approach failed, log and return partial success
                    self.logger.warning(f"All batch approaches failed: {e}")
                    return 0
                else:
                    self.logger.debug(f"Batch approach {approach_idx + 1} failed, trying next: {e}")
                    continue
        
        return 0
        """Execute query with automatic retry and error handling."""
        for attempt in range(max_retries):
            try:
                db = self.get_connection()
                graph = db.select_graph(graph_name)
                result = graph.query(query)
                self.stats['queries_executed'] += 1
                return result
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # Handle connection errors
                if 'connection' in error_msg or 'timeout' in error_msg:
                    self._connection = None  # Reset connection
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Connection error, retrying... ({attempt + 1}/{max_retries})")
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                
                # Handle query syntax errors
                elif 'syntax' in error_msg or 'invalid' in error_msg:
                    self.logger.error(f"Query syntax error: {e}")
                    self.logger.debug(f"Problematic query: {query[:200]}...")
                    raise ValueError(f"Query syntax error: {e}")
                
                # Other errors
                else:
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Query failed, retrying... ({attempt + 1}/{max_retries}): {e}")
                        time.sleep(1)
                        continue
                    else:
                        self.logger.error(f"Query failed after {max_retries} attempts: {e}")
                        self.stats['errors_handled'] += 1
                        raise
        
        raise RuntimeError(f"Query failed after {max_retries} attempts")
    
    def sanitize_identifier(self, value: str, max_length: int = 50) -> str:
        """Sanitize string to be a valid Cypher identifier."""
        if not isinstance(value, str):
            value = str(value)
        
        # Extract meaningful part from URIs
        if value.startswith('http'):
            if '#' in value:
                value = value.split('#')[-1]
            elif '/' in value:
                value = value.split('/')[-1]
        
        # Keep only alphanumeric characters and underscores
        value = re.sub(r'[^\w]', '_', value)
        
        # Ensure it starts with a letter or underscore
        if value and value[0].isdigit():
            value = f"n_{value}"
        
        # Ensure it's not empty
        if not value:
            value = "node"
        
        # Limit length
        return value[:max_length]
    
    def sanitize_string_value(self, value: str) -> str:
        """Sanitize string value for use in Cypher queries."""
        if not isinstance(value, str):
            value = str(value)
        
        # Limit length
        value = value[:self.max_property_length]
        
        # Escape quotes and backslashes
        value = value.replace('\\', '\\\\')
        value = value.replace("'", "\\'")
        value = value.replace('"', '\\"')
        
        # Remove control characters
        value = re.sub(r'[\x00-\x1F\x7F]', '', value)
        
        # Replace multiple whitespace with single space
        value = re.sub(r'\s+', ' ', value).strip()
        
        return value
    
    def get_entity_id(self, uri: str) -> int:
        """Get or create a unique integer ID for an entity."""
        if uri not in self.entity_index:
            self.entity_index[uri] = self.entity_counter
            self.entity_counter += 1
        return self.entity_index[uri]
    
    def parse_rdf_streaming(self, file_path: str) -> Iterator[Tuple]:
        """Parse RDF file in streaming fashion to handle large files."""
        self.logger.info(f"üìñ Parsing RDF file: {file_path}")
        
        try:
            graph = Graph()
            
            # Determine format from file extension
            file_ext = Path(file_path).suffix.lower()
            format_map = {
                '.ttl': 'turtle',
                '.turtle': 'turtle',
                '.rdf': 'xml',
                '.xml': 'xml',
                '.n3': 'n3',
                '.nt': 'nt',
                '.jsonld': 'json-ld'
            }
            
            rdf_format = format_map.get(file_ext, 'turtle')
            self.logger.info(f"üìÑ Detected format: {rdf_format}")
            
            graph.parse(file_path, format=rdf_format)
            
            # Stream triples in batches
            triples = list(graph)
            total_triples = len(triples)
            self.logger.info(f"üìä Found {total_triples:,} triples")
            
            for i in range(0, total_triples, self.batch_size):
                batch_end = min(i + self.batch_size, total_triples)
                yield triples[i:batch_end]
                
                if i % (self.batch_size * 10) == 0:  # Log every 10 batches
                    progress = (batch_end / total_triples) * 100
                    self.logger.info(f"üìà Parsing progress: {progress:.1f}% ({batch_end:,}/{total_triples:,})")
        
        except Exception as e:
            self.logger.error(f"‚ùå Failed to parse RDF file: {e}")
            raise
    
    def process_triples_batch(self, triples_batch: List[Tuple]) -> Tuple[List[Dict], List[Dict]]:
        """Process a batch of RDF triples into nodes and relationships."""
        nodes = {}
        relationships = []
        
        for subject, predicate, obj in triples_batch:
            subject_str = str(subject)
            predicate_str = str(predicate)
            
            # Get or create subject node
            subject_id = self.get_entity_id(subject_str)
            if subject_id not in nodes:
                # Determine node labels
                labels = set(['Resource'])
                
                # Add specific labels based on RDF types
                if predicate_str == str(RDF.type):
                    if isinstance(obj, URIRef):
                        type_label = self.sanitize_identifier(str(obj))
                        labels.add(type_label)
                
                nodes[subject_id] = {
                    'id': subject_id,
                    'uri': subject_str,
                    'labels': labels,
                    'properties': {'uri': subject_str}
                }
            
            # Handle object based on type
            if isinstance(obj, (URIRef, BNode)):
                # Object is another resource - create relationship
                obj_str = str(obj)
                obj_id = self.get_entity_id(obj_str)
                
                # Create object node if not exists
                if obj_id not in nodes:
                    obj_labels = set(['Resource'])
                    nodes[obj_id] = {
                        'id': obj_id,
                        'uri': obj_str,
                        'labels': obj_labels,
                        'properties': {'uri': obj_str}
                    }
                
                # Create relationship
                rel_type = self.sanitize_identifier(predicate_str)
                relationships.append({
                    'source_id': subject_id,
                    'target_id': obj_id,
                    'type': rel_type,
                    'properties': {'predicate_uri': predicate_str}
                })
                
                # Special handling for rdf:type
                if predicate_str == str(RDF.type):
                    type_label = self.sanitize_identifier(str(obj))
                    nodes[subject_id]['labels'].add(type_label)
            
            elif isinstance(obj, Literal):
                # Object is a literal - add as property
                prop_name = self.sanitize_identifier(predicate_str)
                prop_value = str(obj)
                
                # Handle different literal types
                if obj.datatype:
                    datatype = str(obj.datatype)
                    if datatype == str(XSD.integer):
                        try:
                            prop_value = int(prop_value)
                        except ValueError:
                            pass
                    elif datatype == str(XSD.float) or datatype == str(XSD.double):
                        try:
                            prop_value = float(prop_value)
                        except ValueError:
                            pass
                    elif datatype == str(XSD.boolean):
                        prop_value = prop_value.lower() in ('true', '1')
                
                # Add property to node
                if isinstance(prop_value, str):
                    prop_value = self.sanitize_string_value(prop_value)
                
                nodes[subject_id]['properties'][prop_name] = prop_value
        
        self.stats['triples_processed'] += len(triples_batch)
        return list(nodes.values()), relationships
    
    def build_node_query(self, node: Dict) -> str:
        """Build CREATE query for a node."""
        try:
            # Build labels
            labels = node.get('labels', {'Resource'})
            if isinstance(labels, set):
                labels = list(labels)
            label_str = ':'.join([self.sanitize_identifier(label) for label in labels[:3]])
            
            # Build properties
            properties = node.get('properties', {})
            properties['entity_id'] = node['id']  # Always include entity_id
            
            prop_parts = []
            for key, value in properties.items():
                safe_key = self.sanitize_identifier(key)
                
                if isinstance(value, str):
                    safe_value = self.sanitize_string_value(value)
                    if safe_value:  # Only add non-empty strings
                        prop_parts.append(f"{safe_key}: '{safe_value}'")
                elif isinstance(value, bool):
                    prop_parts.append(f"{safe_key}: {str(value).lower()}")
                elif isinstance(value, (int, float)):
                    if not (isinstance(value, float) and (value != value or abs(value) == float('inf'))):
                        prop_parts.append(f"{safe_key}: {value}")
                else:
                    # Convert other types to string
                    str_value = self.sanitize_string_value(str(value))
                    if str_value:
                        prop_parts.append(f"{safe_key}: '{str_value}'")
            
            if prop_parts:
                props_str = '{' + ', '.join(prop_parts) + '}'
                return f"CREATE (:{label_str} {props_str})"
            else:
                return f"CREATE (:{label_str} {{entity_id: {node['id']}}})"
        
        except Exception as e:
            self.logger.debug(f"Failed to build node query: {e}")
            # Fallback to minimal query
            return f"CREATE (:Resource {{entity_id: {node['id']}}})"
    
    def build_relationship_batch_query(self, relationships: List[Dict]) -> str:
        """Build a single UNWIND query for a batch of relationships."""
        try:
            if not relationships:
                return ""
            
            # Build relationship data for UNWIND
            rel_data = []
            for rel in relationships:
                source_id = rel['source_id']
                target_id = rel['target_id']
                rel_type = self.sanitize_identifier(rel['type'])
                
                # Build properties
                properties = rel.get('properties', {})
                prop_dict = {}
                for key, value in properties.items():
                    safe_key = self.sanitize_identifier(key)
                    if isinstance(value, str):
                        safe_value = self.sanitize_string_value(value)
                        prop_dict[safe_key] = safe_value
                    else:
                        prop_dict[safe_key] = str(value)
                
                rel_data.append({
                    'source_id': source_id,
                    'target_id': target_id,
                    'type': rel_type,
                    'properties': prop_dict
                })
            
            # Create UNWIND query
            query = "UNWIND ["
            rel_strings = []
            for rel in rel_data:
                if rel['properties']:
                    prop_str = ', '.join([f"{k}: '{v}'" for k, v in rel['properties'].items()])
                    rel_strings.append(f"{{source_id: {rel['source_id']}, target_id: {rel['target_id']}, type: '{rel['type']}', properties: {{{prop_str}}}}}")
                else:
                    rel_strings.append(f"{{source_id: {rel['source_id']}, target_id: {rel['target_id']}, type: '{rel['type']}', properties: {{}}}}")
            
            query += ', '.join(rel_strings) + "] AS rel "
            query += """
            MATCH (source {entity_id: rel.source_id})
            MATCH (target {entity_id: rel.target_id})
            CALL {
                WITH source, target, rel
                CALL apoc.create.relationship(source, rel.type, rel.properties, target) YIELD rel as r
                RETURN r
            }
            RETURN count(*)
            """
            
            return query
            
        except Exception as e:
            self.logger.debug(f"Failed to build batch relationship query: {e}")
            # Fallback to simple approach
            return self.build_simple_relationship_batch(relationships)
    
    def build_simple_relationship_batch(self, relationships: List[Dict]) -> str:
        """Build simple relationship queries separated by semicolons."""
        queries = []
        for rel in relationships:
            try:
                source_id = rel['source_id']
                target_id = rel['target_id']
                rel_type = self.sanitize_identifier(rel['type'])
                
                # Build properties
                properties = rel.get('properties', {})
                if properties:
                    prop_parts = []
                    for key, value in properties.items():
                        safe_key = self.sanitize_identifier(key)
                        if isinstance(value, str):
                            safe_value = self.sanitize_string_value(value)
                            prop_parts.append(f"{safe_key}: '{safe_value}'")
                        else:
                            prop_parts.append(f"{safe_key}: '{str(value)}'")
                    
                    if prop_parts:
                        props_str = '{' + ', '.join(prop_parts) + '}'
                        query = f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type} {props_str}]->(b);"
                    else:
                        query = f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type}]->(b);"
                else:
                    query = f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type}]->(b);"
                
                queries.append(query)
            except Exception as e:
                self.logger.debug(f"Failed to build individual relationship query: {e}")
                # Skip this relationship
                continue
        
        return ' '.join(queries)
    
    def upload_nodes(self, graph_name: str, nodes: List[Dict]) -> int:
        """Upload nodes to FalkorDB in batches with proper Cypher syntax."""
        uploaded = 0
        total_nodes = len(nodes)
        
        self.logger.info(f"üèóÔ∏è Uploading {total_nodes} nodes...")
        
        for i in range(0, total_nodes, self.node_batch_size):
            batch_end = min(i + self.node_batch_size, total_nodes)
            batch_nodes = nodes[i:batch_end]
            
            try:
                # Build queries for batch
                queries = []
                for node in batch_nodes:
                    query = self.build_node_query(node)
                    if query:
                        queries.append(query.strip())
                
                if queries:
                    # Execute batch using the robust batch executor
                    executed = self.execute_query_batch(graph_name, queries)
                    uploaded += executed
                
                # Progress update
                if i % (self.node_batch_size * 10) == 0:
                    progress = batch_end / total_nodes * 100
                    self.logger.info(f"üìä Node upload progress: {progress:.1f}% ({batch_end}/{total_nodes})")
            
            except Exception as e:
                self.logger.warning(f"Node batch processing failed: {e}")
                self.stats['errors_handled'] += 1
        
        self.stats['nodes_created'] = uploaded
        self.logger.info(f"‚úÖ Uploaded {uploaded}/{total_nodes} nodes")
        return uploaded
    
    def upload_relationships(self, graph_name: str, relationships: List[Dict]) -> int:
        """Upload relationships to FalkorDB in batches with proper Cypher syntax."""
        uploaded = 0
        total_rels = len(relationships)
        
        self.logger.info(f"üîó Uploading {total_rels} relationships...")
        
        for i in range(0, total_rels, self.relationship_batch_size):
            batch_end = min(i + self.relationship_batch_size, total_rels)
            batch_rels = relationships[i:batch_end]
            
            try:
                # Build individual queries for each relationship
                queries = []
                for rel in batch_rels:
                    query = self.build_single_relationship_query(rel)
                    if query:
                        queries.append(query.strip())
                
                if queries:
                    # Execute batch using the robust batch executor
                    executed = self.execute_query_batch(graph_name, queries)
                    uploaded += executed
                
                # Progress update
                if i % (self.relationship_batch_size * 10) == 0:
                    progress = batch_end / total_rels * 100
                    self.logger.info(f"üîó Relationship upload progress: {progress:.1f}% ({batch_end}/{total_rels})")
            
            except Exception as e:
                self.logger.warning(f"Relationship batch processing failed: {e}")
                self.stats['errors_handled'] += 1
        
        self.stats['relationships_created'] = uploaded
        self.logger.info(f"‚úÖ Uploaded {uploaded}/{total_rels} relationships")
        return uploaded
    
    def build_single_relationship_query(self, relationship: Dict) -> str:
        """Build a single relationship CREATE query."""
        try:
            source_id = relationship['source_id']
            target_id = relationship['target_id']
            rel_type = self.sanitize_identifier(relationship['type'])
            
            # Build properties
            properties = relationship.get('properties', {})
            if properties:
                prop_parts = []
                for key, value in properties.items():
                    safe_key = self.sanitize_identifier(key)
                    if isinstance(value, str):
                        safe_value = self.sanitize_string_value(value)
                        prop_parts.append(f"{safe_key}: '{safe_value}'")
                    else:
                        prop_parts.append(f"{safe_key}: '{str(value)}'")
                
                if prop_parts:
                    props_str = '{' + ', '.join(prop_parts) + '}'
                    return f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type} {props_str}]->(b)"
            
            return f"MATCH (a {{entity_id: {source_id}}}), (b {{entity_id: {target_id}}}) CREATE (a)-[:{rel_type}]->(b)"
        
        except Exception as e:
            self.logger.debug(f"Failed to build relationship query: {e}")
            # Fallback to minimal query
            return f"MATCH (a {{entity_id: {relationship['source_id']}}}), (b {{entity_id: {relationship['target_id']}}}) CREATE (a)-[:RELATED]->(b)"
    
    def clear_graph(self, graph_name: str):
        """Clear all data from the graph."""
        try:
            self.logger.info("üóëÔ∏è Clearing existing graph data...")
            self.execute_query(graph_name, "MATCH (n) DETACH DELETE n")
            self.logger.info("‚úÖ Graph cleared successfully")
        except Exception as e:
            self.logger.warning(f"Could not clear graph (might not exist): {e}")
    
    def convert_file(self, file_path: str, graph_name: str = "rdf_graph", clear_existing: bool = True) -> Dict[str, Any]:
        """Convert RDF file to FalkorDB property graph."""
        self.stats['start_time'] = time.time()
        
        self.logger.info(f"üöÄ Starting RDF to FalkorDB conversion")
        self.logger.info(f"üìÅ Input file: {file_path}")
        self.logger.info(f"üéØ Target graph: {graph_name}")
        
        try:
            # Clear existing data if requested
            if clear_existing:
                self.clear_graph(graph_name)
            
            # Process file in streaming batches
            total_nodes_uploaded = 0
            total_rels_uploaded = 0
            batch_count = 0
            
            for triples_batch in self.parse_rdf_streaming(file_path):
                batch_count += 1
                self.logger.info(f"üì¶ Processing batch {batch_count} ({len(triples_batch)} triples)...")
                
                # Convert triples to nodes and relationships
                nodes, relationships = self.process_triples_batch(triples_batch)
                
                # Upload to FalkorDB
                if nodes:
                    nodes_uploaded = self.upload_nodes(graph_name, nodes)
                    total_nodes_uploaded += nodes_uploaded
                
                if relationships:
                    rels_uploaded = self.upload_relationships(graph_name, relationships)
                    total_rels_uploaded += rels_uploaded
                
                self.logger.info(f"‚úÖ Batch {batch_count} completed: "
                               f"{len(nodes)} nodes, {len(relationships)} relationships")
            
            # Final statistics
            self.stats['end_time'] = time.time()
            total_time = self.stats['end_time'] - self.stats['start_time']
            
            final_stats = {
                'total_time_seconds': total_time,
                'triples_processed': self.stats['triples_processed'],
                'nodes_created': total_nodes_uploaded,
                'relationships_created': total_rels_uploaded,
                'queries_executed': self.stats['queries_executed'],
                'errors_handled': self.stats['errors_handled'],
                'throughput_triples_per_second': self.stats['triples_processed'] / total_time if total_time > 0 else 0
            }
            
            self.logger.info("üéâ Conversion completed successfully!")
            self.logger.info(f"‚è±Ô∏è Total time: {total_time:.2f} seconds")
            self.logger.info(f"üìä Processed: {self.stats['triples_processed']:,} triples")
            self.logger.info(f"üèóÔ∏è Created: {total_nodes_uploaded:,} nodes")
            self.logger.info(f"üîó Created: {total_rels_uploaded:,} relationships")
            self.logger.info(f"üöÄ Throughput: {final_stats['throughput_triples_per_second']:.1f} triples/sec")
            
            return final_stats
        
        except Exception as e:
            self.logger.error(f"‚ùå Conversion failed: {e}")
            raise
        
        finally:
            # Close connection
            if self._connection:
                try:
                    self._connection.close()
                    self._connection = None
                    self.logger.info("üîå Connection closed")
                except:
                    pass


def main():
    """Command-line interface for the converter."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Convert RDF files to FalkorDB property graphs")
    parser.add_argument("rdf_file", help="Path to RDF file (.ttl, .rdf, .n3, etc.)")
    parser.add_argument("--graph-name", default="rdf_graph", help="FalkorDB graph name")
    parser.add_argument("--host", default="localhost", help="FalkorDB host")
    parser.add_argument("--port", type=int, default=6379, help="FalkorDB port")
    parser.add_argument("--password", help="FalkorDB password")
    parser.add_argument("--batch-size", type=int, default=1000, help="RDF parsing batch size")
    parser.add_argument("--node-batch-size", type=int, default=100, help="Node upload batch size")
    parser.add_argument("--rel-batch-size", type=int, default=50, help="Relationship upload batch size")
    parser.add_argument("--keep-existing", action="store_true", help="Don't clear existing graph data")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    # Validate input file
    if not os.path.exists(args.rdf_file):
        print(f"‚ùå File not found: {args.rdf_file}")
        return 1
    
    try:
        # Create converter
        converter = RDFToFalkorConverter(
            host=args.host,
            port=args.port,
            password=args.password,
            batch_size=args.batch_size,
            node_batch_size=args.node_batch_size,
            relationship_batch_size=args.rel_batch_size
        )
        
        # Convert file
        stats = converter.convert_file(
            file_path=args.rdf_file,
            graph_name=args.graph_name,
            clear_existing=not args.keep_existing
        )
        
        print(f"\n‚úÖ Conversion completed successfully!")
        print(f"üìä Statistics: {stats}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Conversion interrupted by user")
        return 1
    except Exception as e:
        print(f"\n‚ùå Conversion failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
