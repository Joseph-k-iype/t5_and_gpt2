#!/usr/bin/env python3
"""
Large File TTL Merger
Handles multi-gigabyte TTL files with streaming and chunking
"""

import os
import glob
import sys
import time
import tempfile
import shutil
from typing import List, Tuple, Optional, Iterator
from rdflib import Graph
import gc
import psutil

class LargeFileTTLMerger:
    def __init__(self, show_progress: bool = True, chunk_size_mb: int = 100):
        self.show_progress = show_progress
        self.chunk_size_bytes = chunk_size_mb * 1024 * 1024  # Convert MB to bytes
        self.merged_graph = Graph()
        self.temp_dir = None
        self.stats = {
            'files_processed': 0,
            'files_failed': 0,
            'total_triples_loaded': 0,
            'final_unique_triples': 0,
            'chunks_processed': 0,
            'processing_time': 0,
            'failed_files': [],
            'memory_peak_mb': 0
        }
    
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        try:
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0
    
    def _split_large_file(self, filepath: str, max_chunk_size: int) -> List[str]:
        """
        Split a large TTL file into smaller chunks while preserving triple integrity
        Returns list of chunk file paths
        """
        if self.show_progress:
            print(f"   📦 Splitting large file into chunks...")
        
        chunk_files = []
        current_chunk = []
        current_size = 0
        chunk_number = 0
        
        # Collect prefixes first
        prefixes = []
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                # First pass: collect prefixes
                for line in f:
                    line = line.strip()
                    if line.startswith('@prefix') or line.startswith('PREFIX'):
                        prefixes.append(line)
                    elif line and not line.startswith('#') and not line.startswith('@') and not line.startswith('PREFIX'):
                        break  # Stop when we hit actual triples
                
                # Second pass: split into chunks
                f.seek(0)
                for line in f:
                    line_size = len(line.encode('utf-8'))
                    
                    # Skip prefix declarations in data (we'll add them to each chunk)
                    if line.strip().startswith('@prefix') or line.strip().startswith('PREFIX'):
                        continue
                    
                    # Check if we need a new chunk
                    if current_size + line_size > max_chunk_size and current_chunk:
                        # Save current chunk
                        chunk_file = self._save_chunk(current_chunk, prefixes, chunk_number)
                        if chunk_file:
                            chunk_files.append(chunk_file)
                        
                        # Start new chunk
                        current_chunk = []
                        current_size = 0
                        chunk_number += 1
                    
                    current_chunk.append(line)
                    current_size += line_size
                
                # Save final chunk
                if current_chunk:
                    chunk_file = self._save_chunk(current_chunk, prefixes, chunk_number)
                    if chunk_file:
                        chunk_files.append(chunk_file)
        
        except Exception as e:
            if self.show_progress:
                print(f"   ❌ Error splitting file: {e}")
            return []
        
        if self.show_progress:
            print(f"   ✅ Created {len(chunk_files)} chunks")
        
        return chunk_files
    
    def _save_chunk(self, lines: List[str], prefixes: List[str], chunk_number: int) -> Optional[str]:
        """Save a chunk of lines to a temporary file"""
        try:
            if not self.temp_dir:
                self.temp_dir = tempfile.mkdtemp(prefix="ttl_merger_")
            
            chunk_file = os.path.join(self.temp_dir, f"chunk_{chunk_number:04d}.ttl")
            
            with open(chunk_file, 'w', encoding='utf-8') as f:
                # Write prefixes first
                for prefix in prefixes:
                    f.write(prefix + '\n')
                f.write('\n')
                
                # Write chunk data
                for line in lines:
                    f.write(line)
            
            return chunk_file
        except Exception as e:
            if self.show_progress:
                print(f"   ❌ Error saving chunk {chunk_number}: {e}")
            return None
    
    def _process_file_direct(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """Process a regular-sized file directly"""
        try:
            initial_count = len(self.merged_graph)
            
            # Try multiple parsing strategies for large files
            try:
                # Standard parsing
                self.merged_graph.parse(filepath, format="turtle")
            except Exception as e1:
                if "index out of range" in str(e1).lower() or "memory" in str(e1).lower():
                    # File might be too large, try chunking
                    return self._process_large_file(filepath)
                else:
                    # Try with different encoding
                    encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']
                    for encoding in encodings:
                        try:
                            temp_graph = Graph()
                            temp_graph.parse(filepath, format="turtle", encoding=encoding)
                            self.merged_graph += temp_graph
                            del temp_graph
                            gc.collect()
                            break
                        except:
                            continue
                    else:
                        return False, 0, str(e1)
            
            final_count = len(self.merged_graph)
            triples_loaded = final_count - initial_count
            
            return True, triples_loaded, None
            
        except Exception as e:
            return False, 0, str(e)
    
    def _process_large_file(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """Process a very large file using chunking strategy"""
        try:
            file_size = os.path.getsize(filepath)
            
            if self.show_progress:
                print(f"   📊 Large file detected: {self._format_size(file_size)}")
                print(f"   🔄 Using chunking strategy...")
            
            # Split the file into manageable chunks
            chunk_files = self._split_large_file(filepath, self.chunk_size_bytes)
            
            if not chunk_files:
                return False, 0, "Failed to split large file into chunks"
            
            total_loaded = 0
            
            # Process each chunk
            for i, chunk_file in enumerate(chunk_files):
                if self.show_progress:
                    print(f"   📦 Processing chunk {i+1}/{len(chunk_files)}")
                
                try:
                    initial_count = len(self.merged_graph)
                    self.merged_graph.parse(chunk_file, format="turtle")
                    final_count = len(self.merged_graph)
                    chunk_loaded = final_count - initial_count
                    total_loaded += chunk_loaded
                    
                    self.stats['chunks_processed'] += 1
                    
                    if self.show_progress:
                        mem_mb = self._get_memory_usage()
                        self.stats['memory_peak_mb'] = max(self.stats['memory_peak_mb'], mem_mb)
                        print(f"      ✅ {chunk_loaded:,} triples, Memory: {mem_mb:.1f}MB")
                    
                    # Force garbage collection after each chunk
                    gc.collect()
                    
                except Exception as e:
                    if self.show_progress:
                        print(f"      ❌ Chunk {i+1} failed: {e}")
                    continue
            
            return True, total_loaded, None
            
        except Exception as e:
            return False, 0, f"Large file processing failed: {e}"
    
    def _cleanup_temp_files(self):
        """Clean up temporary files"""
        if self.temp_dir and os.path.exists(self.temp_dir):
            try:
                shutil.rmtree(self.temp_dir)
                if self.show_progress:
                    print(f"   🧹 Cleaned up temporary files")
            except Exception as e:
                if self.show_progress:
                    print(f"   ⚠️  Warning: Could not clean up temp files: {e}")
    
    def merge_files(self, input_pattern: str, output_file: str, 
                   output_format: str = "turtle") -> bool:
        """
        Merge TTL files with large file support
        """
        start_time = time.time()
        
        try:
            if self.show_progress:
                print(f"🗂️  Large File TTL Merger")
                print(f"📂 Input pattern: {input_pattern}")
                print(f"📄 Output file: {output_file}")
                print(f"📦 Chunk size: {self.chunk_size_bytes // (1024*1024)}MB")
                print()
            
            # Get file list
            ttl_files = glob.glob(input_pattern)
            if not ttl_files:
                print(f"❌ No TTL files found matching: {input_pattern}")
                return False
            
            # Filter accessible files and categorize by size
            accessible_files = []
            large_files = []
            total_size = 0
            
            for filepath in ttl_files:
                if os.path.exists(filepath) and os.access(filepath, os.R_OK):
                    size = os.path.getsize(filepath)
                    accessible_files.append(filepath)
                    total_size += size
                    
                    # Files over 500MB are considered "large"
                    if size > 500 * 1024 * 1024:
                        large_files.append(filepath)
                else:
                    if self.show_progress:
                        print(f"⚠️  Skipping inaccessible: {filepath}")
            
            if not accessible_files:
                print("❌ No accessible files found")
                return False
            
            if self.show_progress:
                print(f"🔍 Found {len(accessible_files)} accessible files")
                print(f"📊 Total size: {self._format_size(total_size)}")
                if large_files:
                    print(f"🔍 Large files (>500MB): {len(large_files)}")
                print()
            
            # Process files
            for i, filepath in enumerate(accessible_files):
                if self.show_progress:
                    file_size = os.path.getsize(filepath)
                    print(f"[{i+1}/{len(accessible_files)}] 📖 {os.path.basename(filepath)} ({self._format_size(file_size)})")
                
                # Determine processing strategy based on file size
                if filepath in large_files:
                    success, triples_loaded, error = self._process_large_file(filepath)
                else:
                    success, triples_loaded, error = self._process_file_direct(filepath)
                
                if success:
                    self.stats['files_processed'] += 1
                    self.stats['total_triples_loaded'] += triples_loaded
                    
                    if self.show_progress:
                        current_total = len(self.merged_graph)
                        mem_mb = self._get_memory_usage()
                        self.stats['memory_peak_mb'] = max(self.stats['memory_peak_mb'], mem_mb)
                        print(f"   ✅ Added {triples_loaded:,} triples (total: {current_total:,}) Memory: {mem_mb:.1f}MB")
                else:
                    self.stats['files_failed'] += 1
                    self.stats['failed_files'].append((filepath, error))
                    if self.show_progress:
                        print(f"   ❌ Failed: {error}")
                
                # Force garbage collection periodically
                if i % 5 == 0:
                    gc.collect()
            
            # Final statistics
            self.stats['final_unique_triples'] = len(self.merged_graph)
            self.stats['processing_time'] = time.time() - start_time
            
            if self.stats['files_processed'] == 0:
                print("❌ No files were successfully processed")
                return False
            
            # Write output with memory management
            if self.show_progress:
                print(f"\n💾 Writing {len(self.merged_graph):,} triples to {output_file}...")
            
            try:
                self.merged_graph.serialize(destination=output_file, format=output_format)
                
                # Verify output
                if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                    print(f"❌ Output file creation failed")
                    return False
                
                if self.show_progress:
                    self._print_final_stats(output_file)
                
                return True
                
            except MemoryError:
                print(f"❌ Not enough memory to write output file")
                return False
            except Exception as e:
                print(f"❌ Error writing output: {e}")
                return False
        
        finally:
            # Always clean up
            self._cleanup_temp_files()
    
    def _print_final_stats(self, output_file: str):
        """Print final statistics"""
        print(f"\n🎉 MERGE COMPLETED!")
        print(f"=" * 40)
        print(f"📁 Output: {output_file}")
        print(f"📊 Files processed: {self.stats['files_processed']}")
        
        if self.stats['files_failed'] > 0:
            print(f"❌ Files failed: {self.stats['files_failed']}")
        
        if self.stats['chunks_processed'] > 0:
            print(f"📦 Chunks processed: {self.stats['chunks_processed']}")
        
        print(f"🔗 Total triples: {self.stats['final_unique_triples']:,}")
        print(f"⏱️  Processing time: {self.stats['processing_time']:.1f}s")
        print(f"🧠 Peak memory usage: {self.stats['memory_peak_mb']:.1f}MB")
        
        if self.stats['processing_time'] > 0:
            throughput = self.stats['final_unique_triples'] / self.stats['processing_time']
            print(f"🚀 Throughput: {throughput:,.0f} triples/second")
        
        # Output file info
        output_size = os.path.getsize(output_file)
        print(f"📏 Output size: {self._format_size(output_size)}")
        
        # Compression ratio
        if hasattr(self, 'total_input_size'):
            ratio = (output_size / self.total_input_size) * 100
            print(f"📉 Size ratio: {ratio:.1f}% of input")
    
    @staticmethod
    def _format_size(size_bytes: int) -> str:
        """Format file size"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} PB"

def merge_large_ttl_files(input_pattern: str, output_file: str, 
                         output_format: str = "turtle", **kwargs) -> bool:
    """Large file TTL merger with memory management"""
    merger = LargeFileTTLMerger(
        show_progress=kwargs.get('show_progress', True),
        chunk_size_mb=kwargs.get('chunk_size_mb', 100)
    )
    
    return merger.merge_files(input_pattern, output_file, output_format)

def main():
    """Main function optimized for large files"""
    print("🗂️  LARGE FILE TTL MERGER")
    print("=" * 35)
    print("Handles multi-gigabyte files with streaming")
    print()
    
    if len(sys.argv) < 2 or "--help" in sys.argv:
        print("💡 Usage:")
        print("  python merge_ttl.py [pattern] [output] [format] [options]")
        print("\nOptions:")
        print("  --chunk-size N    Chunk size in MB (default: 100)")
        print("  --quiet          Minimal output")
        print("  --help           Show this help")
        print("\nExamples:")
        print("  python merge_ttl.py 'huge_file.ttl' merged.ttl")
        print("  python merge_ttl.py '*.ttl' merged.ttl turtle --chunk-size 50")
        print("  python merge_ttl.py 'data/*.ttl' output.rdf xml")
        print("\n🗂️  Optimized for:")
        print("     • Files over 2GB")
        print("     • Memory-constrained systems")
        print("     • Chunked processing with cleanup")
        print("     • Progress monitoring")
        return
    
    input_pattern = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else "merged.ttl"
    output_format = sys.argv[3] if len(sys.argv) > 3 else "turtle"
    
    # Parse options
    chunk_size_mb = 100
    show_progress = "--quiet" not in sys.argv
    
    if "--chunk-size" in sys.argv:
        try:
            idx = sys.argv.index("--chunk-size")
            chunk_size_mb = int(sys.argv[idx + 1])
        except (IndexError, ValueError):
            print("⚠️  Invalid --chunk-size, using default: 100MB")
    
    # Validate format
    valid_formats = ["turtle", "xml", "n3", "nt", "json-ld", "trig"]
    if output_format not in valid_formats:
        print(f"❌ Invalid format: {output_format}")
        print(f"Valid formats: {', '.join(valid_formats)}")
        return
    
    # Check available memory
    try:
        available_gb = psutil.virtual_memory().available / (1024**3)
        if show_progress:
            print(f"🧠 Available memory: {available_gb:.1f}GB")
            if available_gb < 2:
                print(f"⚠️  Low memory detected - using smaller chunks")
                chunk_size_mb = min(chunk_size_mb, 50)
    except:
        pass
    
    # Check if files exist
    if not glob.glob(input_pattern):
        print(f"❌ No files found matching: {input_pattern}")
        return
    
    # Run the merger
    success = merge_large_ttl_files(
        input_pattern=input_pattern,
        output_file=output_file,
        output_format=output_format,
        chunk_size_mb=chunk_size_mb,
        show_progress=show_progress
    )
    
    if success:
        print(f"\n🎉 SUCCESS: Large file merge completed!")
    else:
        print(f"\n❌ FAILED: Could not complete merge")
        sys.exit(1)

if __name__ == "__main__":
    try:
        import rdflib
        try:
            import psutil
        except ImportError:
            print("⚠️  psutil not found (pip install psutil) - continuing without memory monitoring")
        main()
    except ImportError:
        print("❌ rdflib required: pip install rdflib")
        sys.exit(1)
