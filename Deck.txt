#!/usr/bin/env python3
"""
TTL to FalkorDB Pipeline using LOAD CSV
Converts TTL to CSV format then uses FalkorDB's LOAD CSV Cypher commands
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
import sys
import shutil
from pathlib import Path

# Setup logging with UTF-8 encoding support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class TTLToFalkorDBConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to FalkorDB converter"""
        self.output_dir = Path(output_dir)
        self.nodes = {}  # node_id -> {node_type, properties}
        self.edges = []  # list of {source_id, target_id, edge_type, properties}
        self.node_id_map = {}  # URI -> unique_id
        self.next_id = 1
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            uri_str = str(uri_or_literal)
            parsed = urlparse(uri_str)
            
            # Try fragment first (after #)
            if parsed.fragment:
                name = parsed.fragment
            # Then try last path component
            elif parsed.path and parsed.path != '/':
                name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
            # Fall back to netloc
            elif parsed.netloc:
                name = parsed.netloc.replace('.', '_')
            else:
                # Use hash of full URI as fallback
                name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
            
            # Clean the name - only alphanumeric and underscore
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            # Ensure it starts with letter or underscore
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"n_{cleaned}"
            # Ensure it's not empty and has reasonable length
            if not cleaned or len(cleaned) < 1:
                cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
            return cleaned[:50]  # Limit length
        else:
            # Handle literals or other types
            name = str(uri_or_literal)
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"l_{cleaned}"
            return (cleaned or "literal")[:50]
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique ID for a resource"""
        uri = str(resource)
        if uri not in self.node_id_map:
            # Use hash-based IDs to avoid integer overflow issues
            hash_obj = hashlib.md5(uri.encode('utf-8'))
            node_id = f"n_{hash_obj.hexdigest()[:8]}"
            
            # Ensure uniqueness
            original_id = node_id
            counter = 1
            while node_id in [node['id'] for node in self.nodes.values()]:
                node_id = f"{original_id}_{counter}"
                counter += 1
            
            self.node_id_map[uri] = node_id
        return self.node_id_map[uri]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path and parsed.path != '/':
            parts = [p for p in parsed.path.strip('/').split('/') if p]
            if parts:
                properties['local_name'] = parts[-1]
                properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        
        return properties
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        if value is None:
            return ''
        
        if isinstance(value, bool):
            return 'true' if value else 'false'
        
        if isinstance(value, (list, dict)):
            # Convert to JSON string for complex data
            return json.dumps(value, ensure_ascii=False)
        
        # Convert to string and clean
        str_value = str(value).strip()
        
        # Replace problematic characters for CSV
        str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
        str_value = re.sub(r'\s+', ' ', str_value)  # Normalize whitespace
        
        # Escape quotes by doubling them (CSV standard)
        if '"' in str_value:
            str_value = str_value.replace('"', '""')
        
        # Limit string length to prevent memory issues
        if len(str_value) > 500:
            logger.warning(f"[WARN] Very long string truncated (length: {len(str_value)})")
            str_value = str_value[:500] + "..."
        
        return str_value
    
    def process_literal_value(self, literal: Literal) -> str:
        """Process literal value and return cleaned string"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                # Handle common XSD datatypes
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int_val = int(literal)
                        # Keep reasonable sized integers
                        if abs(int_val) <= 2147483647:
                            return str(int_val)
                        else:
                            return str(literal)  # Keep as string if too large
                    except ValueError:
                        return str(literal)
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:
                            return str(float_val)
                        else:
                            return str(literal)
                    except ValueError:
                        return str(literal)
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower()
                    
                else:
                    return str(literal)
            else:
                return str(literal)
                
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal)
    
    def convert_ttl_to_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL file to CSV format for FalkorDB LOAD CSV"""
        logger.info(f"Converting {ttl_file_path} to CSV format for LOAD CSV...")
        
        # Parse TTL file
        graph = Graph()
        try:
            logger.info("Parsing TTL file...")
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Successfully parsed TTL file. Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found in the TTL file!")
            return []
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"[LIMIT] Processing only first {max_triples} of {total_triples} triples")
            total_triples = max_triples
        
        logger.info("Processing triples and building data structures...")
        processed_count = 0
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                if max_triples and processed_count >= max_triples:
                    break
                
                pbar.update(1)
                processed_count += 1
                
                # Get or create subject node
                subject_id = self.get_or_create_node_id(subject)
                subject_type = self.clean_identifier(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Initialize subject node if not exists
                if subject_id not in self.nodes:
                    if isinstance(subject, URIRef):
                        base_props = self.extract_properties_from_uri(subject)
                    else:  # BNode
                        base_props = {
                            'uri': str(subject),
                            'resource_type': 'blank_node'
                        }
                    
                    self.nodes[subject_id] = {
                        'id': subject_id,
                        'node_type': subject_type,
                        'properties': base_props
                    }
                
                # Handle object
                if isinstance(obj, Literal):
                    # Add as property to subject node
                    value = self.process_literal_value(obj)
                    prop_name = predicate_clean
                    
                    # Store the value as a property
                    self.nodes[subject_id]['properties'][prop_name] = value
                    
                    # Store language if present
                    if obj.language:
                        lang_prop = f"{prop_name}_lang"
                        self.nodes[subject_id]['properties'][lang_prop] = obj.language
                
                else:
                    # Object is a resource - create edge
                    object_id = self.get_or_create_node_id(obj)
                    object_type = self.clean_identifier(obj)
                    
                    # Initialize object node if not exists
                    if object_id not in self.nodes:
                        if isinstance(obj, URIRef):
                            base_props = self.extract_properties_from_uri(obj)
                        else:  # BNode
                            base_props = {
                                'uri': str(obj),
                                'resource_type': 'blank_node'
                            }
                        
                        self.nodes[object_id] = {
                            'id': object_id,
                            'node_type': object_type,
                            'properties': base_props
                        }
                    
                    # Create edge
                    edge = {
                        'source_id': subject_id,
                        'target_id': object_id,
                        'edge_type': predicate_clean,
                        'properties': {
                            'predicate_uri': str(predicate)
                        }
                    }
                    self.edges.append(edge)
        
        logger.info(f"Data processing complete:")
        logger.info(f"  Nodes: {len(self.nodes):,}")
        logger.info(f"  Edges: {len(self.edges):,}")
        
        # Write CSV files
        csv_files = self.write_csv_files()
        
        # Generate Cypher commands
        cypher_file = self.generate_cypher_commands(use_absolute_paths=False)
        
        return csv_files, cypher_file
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names from all nodes
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        
        all_properties = sorted(all_properties)
        
        try:
            with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: id, node_type, then all properties
                headers = ['id', 'node_type'] + all_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for node in self.nodes.values():
                    row = [
                        node['id'],
                        node['node_type']
                    ]
                    
                    # Add property values
                    for prop in all_properties:
                        value = node['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"[OK] Written {len(self.nodes):,} nodes to {nodes_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing nodes file {nodes_file}: {e}")
            raise
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        # Get all possible edge property names
        all_edge_properties = set()
        for edge in self.edges:
            all_edge_properties.update(edge['properties'].keys())
        
        all_edge_properties = sorted(all_edge_properties)
        
        try:
            with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: source_id, target_id, edge_type, then all properties
                headers = ['source_id', 'target_id', 'edge_type'] + all_edge_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for edge in self.edges:
                    row = [
                        edge['source_id'],
                        edge['target_id'],
                        edge['edge_type']
                    ]
                    
                    # Add property values
                    for prop in all_edge_properties:
                        value = edge['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"[OK] Written {len(self.edges):,} edges to {edges_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing edges file {edges_file}: {e}")
            raise
        
        return csv_files
    
    def generate_cypher_commands(self, use_absolute_paths=False):
        """Generate Cypher LOAD CSV commands"""
        cypher_file = self.output_dir / "load_data.cypher"
        
        # Determine file paths
        if use_absolute_paths:
            nodes_path = str((self.output_dir / "nodes.csv").resolve())
            edges_path = str((self.output_dir / "edges.csv").resolve())
            file_prefix = "file://"
        else:
            nodes_path = "nodes.csv"
            edges_path = "edges.csv"
            file_prefix = "file://"
        
        try:
            with open(cypher_file, 'w', encoding='utf-8') as f:
                f.write("// FalkorDB LOAD CSV Commands\n")
                f.write("// Generated automatically from TTL data\n\n")
                
                f.write("// IMPORTANT: CSV files must be in FalkorDB's import directory\n")
                f.write("// Default location: /var/lib/falkordb/import/ (in Docker)\n")
                f.write("// Or copy files to FalkorDB's configured import folder\n\n")
                
                f.write("// Clear existing data (optional)\n")
                f.write("// MATCH (n) DETACH DELETE n;\n\n")
                
                f.write("// METHOD 1: Basic loading (most compatible)\n")
                f.write("// Load all nodes with generic Entity label\n")
                f.write(f"LOAD CSV WITH HEADERS FROM '{file_prefix}{nodes_path}' AS row\n")
                f.write("CREATE (n:Entity)\n")
                f.write("SET n = row;\n\n")
                
                f.write("// Load all edges with generic CONNECTED_TO relationship\n")
                f.write(f"LOAD CSV WITH HEADERS FROM '{file_prefix}{edges_path}' AS row\n")
                f.write("MATCH (source:Entity {id: row.source_id})\n")
                f.write("MATCH (target:Entity {id: row.target_id})\n")
                f.write("CREATE (source)-[r:CONNECTED_TO]->(target)\n")
                f.write("SET r = row\n")
                f.write("REMOVE r.source_id, r.target_id, r.edge_type;\n\n")
                
                f.write("// METHOD 2: With batching (for large datasets)\n")
                f.write(f"// LOAD CSV WITH HEADERS FROM '{file_prefix}{nodes_path}' AS row\n")
                f.write("// CALL {\n")
                f.write("//   WITH row\n")
                f.write("//   CREATE (n:Entity)\n")
                f.write("//   SET n = row\n")
                f.write("//   RETURN n\n")
                f.write("// } IN TRANSACTIONS OF 1000 ROWS;\n\n")
                
                f.write("// METHOD 3: Using absolute file paths (if relative paths don't work)\n")
                f.write("// Replace with your actual file paths:\n")
                abs_nodes = str((self.output_dir / "nodes.csv").resolve())
                abs_edges = str((self.output_dir / "edges.csv").resolve())
                f.write(f"// LOAD CSV WITH HEADERS FROM 'file://{abs_nodes}' AS row\n")
                f.write("// CREATE (n:Entity) SET n = row;\n\n")
                
                f.write(f"// LOAD CSV WITH HEADERS FROM 'file://{abs_edges}' AS row\n")
                f.write("// MATCH (source:Entity {id: row.source_id})\n")
                f.write("// MATCH (target:Entity {id: row.target_id})\n")
                f.write("// CREATE (source)-[r:CONNECTED_TO]->(target)\n")
                f.write("// SET r = row REMOVE r.source_id, r.target_id, r.edge_type;\n\n")
                
                f.write("// METHOD 4: Copy files to import directory first\n")
                f.write("// Docker: docker cp nodes.csv CONTAINER:/var/lib/falkordb/import/\n")
                f.write("// Docker: docker cp edges.csv CONTAINER:/var/lib/falkordb/import/\n")
                f.write("// Then use: LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row...\n\n")
                
                f.write("// Verify data loaded\n")
                f.write("MATCH (n) RETURN count(n) AS node_count;\n")
                f.write("MATCH ()-[r]->() RETURN count(r) AS edge_count;\n")
                f.write("MATCH (n) RETURN n.node_type, count(*) AS count ORDER BY count DESC LIMIT 10;\n")
            
            logger.info(f"[OK] Generated Cypher commands in {cypher_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing Cypher file {cypher_file}: {e}")
            raise
        
        return str(cypher_file)
    
    def test_falkordb_connection(self, host: str, port: int, password: Optional[str] = None):
        """Test connection to FalkorDB"""
        logger.info(f"[TEST] Testing connection to FalkorDB at {host}:{port}...")
        
        try:
            import redis
            
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test basic connection
            response = r.ping()
            if response:
                logger.info("[OK] Redis connection successful")
            else:
                logger.error("[ERROR] Redis ping failed")
                return False
            
            # Test FalkorDB module
            try:
                modules = r.module_list()
                falkor_loaded = any('graph' in str(module).lower() for module in modules)
                if falkor_loaded:
                    logger.info("[OK] FalkorDB module is loaded")
                else:
                    logger.warning("[WARN] FalkorDB module not detected in module list")
                    logger.info("Available modules:", modules)
            except Exception as e:
                logger.warning(f"[WARN] Could not check modules: {e}")
            
    def find_falkordb_container(self):
        """Find running FalkorDB container"""
        try:
            import subprocess
            result = subprocess.run([
                'docker', 'ps', '--filter', 'ancestor=falkordb/falkordb', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                containers = result.stdout.strip().split('\n')
                logger.info(f"[FOUND] FalkorDB containers: {containers}")
                return containers[0]  # Return first one
            else:
                logger.info("[INFO] No FalkorDB containers found")
                return None
                
        except Exception as e:
            logger.warning(f"[WARN] Could not check for containers: {e}")
            return None
            
        except ImportError:
            logger.warning("[WARN] redis-py not installed, skipping connection test")
            logger.info("Install with: pip install redis")
            return True  # Don't fail if redis-py not available
            
        except Exception as e:
            logger.error(f"[ERROR] Connection test failed: {e}")
            return False
    
    def copy_files_to_falkordb_import(self, container_name: Optional[str] = None):
        """Copy CSV files to FalkorDB import directory"""
        logger.info("[COPY] Copying CSV files to FalkorDB import directory...")
        
        nodes_file = self.output_dir / "nodes.csv"
        edges_file = self.output_dir / "edges.csv"
        
        if container_name:
            # Copy to Docker container
            try:
                import subprocess
                
                # Copy nodes file
                result = subprocess.run([
                    'docker', 'cp', str(nodes_file), 
                    f"{container_name}:/var/lib/falkordb/import/"
                ], capture_output=True, text=True)
                
                if result.returncode != 0:
                    logger.error(f"[ERROR] Failed to copy nodes.csv: {result.stderr}")
                    return False
                
                # Copy edges file
                result = subprocess.run([
                    'docker', 'cp', str(edges_file), 
                    f"{container_name}:/var/lib/falkordb/import/"
                ], capture_output=True, text=True)
                
                if result.returncode != 0:
                    logger.error(f"[ERROR] Failed to copy edges.csv: {result.stderr}")
                    return False
                
                logger.info(f"[OK] Files copied to container {container_name}")
                return True
                
            except Exception as e:
                logger.error(f"[ERROR] Failed to copy files to container: {e}")
                return False
        else:
            logger.info("[INFO] Manual copy required:")
            logger.info("For Docker containers:")
            logger.info(f"  docker cp {nodes_file} YOUR_CONTAINER:/var/lib/falkordb/import/")
            logger.info(f"  docker cp {edges_file} YOUR_CONTAINER:/var/lib/falkordb/import/")
            logger.info("For local FalkorDB:")
            logger.info("  Copy files to your FalkorDB import directory")
            return True

    def find_falkordb_container(self):
        """Find running FalkorDB container"""
        try:
            import subprocess
            result = subprocess.run([
                'docker', 'ps', '--filter', 'ancestor=falkordb/falkordb', 
                '--format', '{{.Names}}'
            ], capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                containers = result.stdout.strip().split('\n')
                logger.info(f"[FOUND] FalkorDB containers: {containers}")
                return containers[0]  # Return first one
            else:
                logger.info("[INFO] No FalkorDB containers found")
                return None
                
        except Exception as e:
            logger.warning(f"[WARN] Could not check for containers: {e}")
            return None
    def execute_cypher_commands(self, cypher_file: str, graph_name: str, 
                              host: str = '127.0.0.1', port: int = 6379, 
                              password: Optional[str] = None, use_absolute_paths: bool = False):
        logger.info(f"[EXEC] Executing Cypher commands on graph '{graph_name}'...")
        
        try:
            import redis
            
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test connection
            r.ping()
            
            # Read and execute basic Cypher commands
            with open(cypher_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract the basic loading commands (most compatible approach)
            logger.info("[EXEC] Loading nodes...")
            nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://nodes.csv' AS row
            CREATE (n:Entity)
            SET n = row
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
                logger.info(f"[OK] Nodes loaded: {result}")
            except Exception as e:
                logger.error(f"[ERROR] Failed to load nodes: {e}")
                return False
            
            logger.info("[EXEC] Loading edges...")
            edges_query = """
            LOAD CSV WITH HEADERS FROM 'file://edges.csv' AS row
            MATCH (source:Entity {id: row.source_id})
            MATCH (target:Entity {id: row.target_id})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = row
            REMOVE r.source_id, r.target_id
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_query)
                logger.info(f"[OK] Edges loaded: {result}")
            except Exception as e:
                logger.error(f"[ERROR] Failed to load edges: {e}")
                return False
            
            # Verify data
            count_query = "MATCH (n) RETURN count(n) AS node_count"
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"[VERIFY] Total nodes in graph: {result}")
            except Exception as e:
                logger.warning(f"[WARN] Could not verify node count: {e}")
            
            return True
            
        except ImportError:
            logger.error("[ERROR] redis-py not installed. Install with: pip install redis")
            return False
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to execute Cypher commands: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Convert TTL to FalkorDB using LOAD CSV')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host (default: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port (default: 6379)')
    parser.add_argument('--password', help='FalkorDB password (default: none)')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--csv_only', action='store_true', help='Only convert to CSV, skip loading')
    parser.add_argument('--test_connection', action='store_true', help='Test FalkorDB connection only')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process for testing')
    parser.add_argument('--execute_cypher', action='store_true', help='Execute Cypher commands automatically')
    parser.add_argument('--use_absolute_paths', action='store_true', help='Use absolute file paths in Cypher commands')
    parser.add_argument('--copy_to_container', help='Copy CSV files to specified Docker container import directory')
    parser.add_argument('--auto_copy', action='store_true', help='Auto-detect and copy to first FalkorDB container found')
    parser.add_argument('--import_dir', help='Copy CSV files to specified FalkorDB import directory')
    
    args = parser.parse_args()
    
    # Validate input
    if not os.path.exists(args.ttl_file):
        logger.error(f"[ERROR] TTL file not found: {args.ttl_file}")
        sys.exit(1)
    
    file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
    logger.info(f"[FILE] Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    
    # Create converter
    converter = TTLToFalkorDBConverter(args.output_dir)
    
    # Test connection only if requested
    if args.test_connection:
        logger.info("[TEST] Connection test mode")
        success = converter.test_falkordb_connection(args.host, args.port, args.password)
        if success:
            logger.info("[OK] Connection test passed!")
            sys.exit(0)
        else:
            logger.error("[ERROR] Connection test failed!")
            sys.exit(1)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        csv_files, cypher_file = converter.convert_ttl_to_csv(args.ttl_file, args.max_triples)
        conversion_time = time.time() - start_time
        
        logger.info(f"[OK] TTL->CSV conversion: {conversion_time:.1f}s ({file_size_mb/conversion_time:.1f} MB/s)")
        
        if args.csv_only:
            logger.info("[TARGET] CSV-only mode: Files generated successfully")
            logger.info(f"[FOLDER] CSV files: {args.output_dir}/")
            logger.info(f"[CYPHER] Commands: {cypher_file}")
            logger.info("\n[NEXT STEPS]:")
            logger.info("1. Copy CSV files to FalkorDB import directory:")
            if args.copy_to_container:
                converter.copy_files_to_falkordb_import(args.copy_to_container)
            else:
                # Try to find container automatically
                container = converter.find_falkordb_container()
                if container:
                    logger.info(f"   Auto-detected container: {container}")
                    logger.info(f"   Run with: --copy_to_container {container}")
                logger.info("   For Docker: docker cp csv_output/nodes.csv YOUR_CONTAINER:/var/lib/falkordb/import/")
                logger.info("   For Docker: docker cp csv_output/edges.csv YOUR_CONTAINER:/var/lib/falkordb/import/")
                logger.info("   For local: copy to your FalkorDB import directory")
            logger.info("2. Connect to FalkorDB and run the Cypher commands")
            logger.info("3. Or use --execute_cypher flag to run automatically")
            return
        
        # Copy files if requested
        if args.copy_to_container:
            success = converter.copy_files_to_falkordb_import(args.copy_to_container)
            if not success:
                logger.error("[ERROR] Failed to copy files to container")
                sys.exit(1)
        elif args.auto_copy:
            container = converter.find_falkordb_container()
            if container:
                success = converter.copy_files_to_falkordb_import(container)
                if not success:
                    logger.error("[ERROR] Failed to auto-copy files to container")
                    sys.exit(1)
            else:
                logger.error("[ERROR] No FalkorDB containers found for auto-copy")
                sys.exit(1)
        
        if args.import_dir:
            try:
                import shutil
                nodes_file = converter.output_dir / "nodes.csv"
                edges_file = converter.output_dir / "edges.csv"
                shutil.copy2(nodes_file, args.import_dir)
                shutil.copy2(edges_file, args.import_dir)
                logger.info(f"[OK] Files copied to {args.import_dir}")
            except Exception as e:
                logger.error(f"[ERROR] Failed to copy files to {args.import_dir}: {e}")
                sys.exit(1)
        
        # Execute Cypher commands if requested
        if args.execute_cypher:
            success = converter.execute_cypher_commands(
                cypher_file, args.graph_name, args.host, args.port, args.password, args.use_absolute_paths
            )
            
            total_time = time.time() - start_time
            
            if success:
                logger.info(f"[OK] PIPELINE SUCCESS! Total time: {total_time:.1f}s")
                logger.info(f"   Performance: {file_size_mb/total_time:.1f} MB/s")
                logger.info(f"[GRAPH] Data loaded into graph: {args.graph_name}")
            else:
                logger.error("[ERROR] Failed to execute Cypher commands")
                sys.exit(1)
        else:
            logger.info("[MANUAL] CSV files and Cypher commands generated")
            logger.info(f"[FOLDER] Files in: {args.output_dir}/")
            logger.info(f"[CYPHER] Commands in: {cypher_file}")
            logger.info("\n[MANUAL STEPS]:")
            logger.info("1. Copy CSV files to FalkorDB import directory:")
            logger.info("   For Docker containers:")
            logger.info(f"     docker cp {args.output_dir}/nodes.csv YOUR_CONTAINER:/var/lib/falkordb/import/")
            logger.info(f"     docker cp {args.output_dir}/edges.csv YOUR_CONTAINER:/var/lib/falkordb/import/")
            logger.info("   For local FalkorDB:")
            logger.info("     Copy files to your configured import directory")
            logger.info("2. Connect to FalkorDB:")
            logger.info(f"   redis-cli -h {args.host} -p {args.port}" + (f" -a {args.password}" if args.password else ""))
            logger.info("3. Run the Cypher commands from the .cypher file")
            logger.info("4. Alternative options:")
            logger.info("   --copy_to_container CONTAINER_NAME  (auto-copy to Docker)")
            logger.info("   --use_absolute_paths                (use full file paths)")
            logger.info("   --execute_cypher                    (automatic execution)")
            
    except KeyboardInterrupt:
        logger.error("[ERROR] Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"[ERROR] Pipeline failed: {e}")
        logger.exception("Full error details:")
        sys.exit(1)

if __name__ == "__main__":
    main()
