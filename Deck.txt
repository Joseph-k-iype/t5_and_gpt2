#!/usr/bin/env python3
"""
Optimized TTL Files Merger using rdflib
High-performance version with reduced time complexity
"""

import os
import glob
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional
from rdflib import Graph

class OptimizedTTLMerger:
    def __init__(self, validate: bool = False, show_progress: bool = True, max_workers: int = 4):
        self.validate = validate
        self.show_progress = show_progress
        self.max_workers = max_workers
        self.merged_graph = Graph()
        self.stats = {
            'files_processed': 0,
            'files_failed': 0,
            'total_triples': 0,
            'processing_time': 0
        }
    
    def _parse_file_optimized(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """
        Parse a single TTL file directly into main graph (thread-safe)
        Returns: (success, triple_count, error_message)
        """
        try:
            # Create temporary graph for this thread
            temp_graph = Graph()
            temp_graph.parse(filepath, format="turtle")
            
            # Get triple count before merging
            triple_count = len(temp_graph)
            
            # Thread-safe merge into main graph
            self.merged_graph += temp_graph
            
            return True, triple_count, None
            
        except Exception as e:
            return False, 0, str(e)
    
    def _parse_file_direct(self, filepath: str) -> Tuple[bool, int, Optional[str]]:
        """
        Parse file directly into main graph (single-threaded, faster for small files)
        Returns: (success, triple_count, error_message)
        """
        try:
            # Get initial count
            initial_count = len(self.merged_graph)
            
            # Parse directly into merged graph (most efficient)
            self.merged_graph.parse(filepath, format="turtle")
            
            # Calculate triples added
            triple_count = len(self.merged_graph) - initial_count
            
            return True, triple_count, None
            
        except Exception as e:
            return False, 0, str(e)
    
    def _should_use_parallel(self, file_sizes: List[int]) -> bool:
        """Determine if parallel processing would be beneficial"""
        total_size = sum(file_sizes)
        avg_size = total_size / len(file_sizes) if file_sizes else 0
        
        # Use parallel for: many files OR large average file size
        return len(file_sizes) > 3 and (len(file_sizes) > 10 or avg_size > 1024 * 1024)  # 1MB
    
    def merge_files(self, input_pattern: str, output_file: str, 
                   output_format: str = "turtle", use_parallel: bool = None) -> bool:
        """
        Optimized merge with automatic parallel/serial decision
        Time complexity: O(n + m) where n=files, m=total triples
        """
        start_time = time.time()
        
        # Get file list
        ttl_files = glob.glob(input_pattern)
        if not ttl_files:
            print(f"‚ùå No TTL files found matching: {input_pattern}")
            return False
        
        if self.show_progress:
            print(f"üîç Found {len(ttl_files)} files to merge")
        
        # Get file sizes for optimization decision
        file_sizes = []
        valid_files = []
        
        for filepath in ttl_files:
            try:
                size = os.path.getsize(filepath)
                file_sizes.append(size)
                valid_files.append(filepath)
            except OSError:
                if self.show_progress:
                    print(f"‚ö†Ô∏è  Skipping inaccessible file: {filepath}")
        
        if not valid_files:
            print("‚ùå No accessible files found")
            return False
        
        # Decide processing strategy
        if use_parallel is None:
            use_parallel = self._should_use_parallel(file_sizes)
        
        if self.show_progress:
            strategy = "parallel" if use_parallel else "serial"
            total_size = sum(file_sizes)
            size_str = self._format_size(total_size)
            print(f"üìä Total data: {size_str} using {strategy} processing")
        
        # Process files
        if use_parallel and len(valid_files) > 1:
            success = self._process_parallel(valid_files)
        else:
            success = self._process_serial(valid_files)
        
        if not success:
            return False
        
        # Write output
        try:
            if self.show_progress:
                print(f"üíæ Writing {len(self.merged_graph):,} triples to {output_file}...")
            
            self.merged_graph.serialize(destination=output_file, format=output_format)
            
            # Update stats
            self.stats['processing_time'] = time.time() - start_time
            
            if self.show_progress:
                self._print_stats(output_file)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error writing output: {e}")
            return False
    
    def _process_serial(self, files: List[str]) -> bool:
        """Optimized serial processing - single pass, direct parsing"""
        for i, filepath in enumerate(files):
            if self.show_progress and len(files) > 5:
                print(f"üìñ Processing {i+1}/{len(files)}: {os.path.basename(filepath)}")
            
            success, triple_count, error = self._parse_file_direct(filepath)
            
            if success:
                self.stats['files_processed'] += 1
                self.stats['total_triples'] += triple_count
                if self.show_progress and len(files) <= 5:
                    print(f"   ‚úÖ {os.path.basename(filepath)}: {triple_count:,} triples")
            else:
                self.stats['files_failed'] += 1
                if self.show_progress:
                    print(f"   ‚ùå {os.path.basename(filepath)}: {error}")
        
        return self.stats['files_processed'] > 0
    
    def _process_parallel(self, files: List[str]) -> bool:
        """Parallel processing for large datasets"""
        if self.show_progress:
            print(f"üöÄ Processing {len(files)} files in parallel...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_file = {
                executor.submit(self._parse_file_optimized, filepath): filepath 
                for filepath in files
            }
            
            # Process results as they complete
            for future in as_completed(future_to_file):
                filepath = future_to_file[future]
                success, triple_count, error = future.result()
                
                if success:
                    self.stats['files_processed'] += 1
                    self.stats['total_triples'] += triple_count
                    if self.show_progress:
                        print(f"   ‚úÖ {os.path.basename(filepath)}: {triple_count:,} triples")
                else:
                    self.stats['files_failed'] += 1
                    if self.show_progress:
                        print(f"   ‚ùå {os.path.basename(filepath)}: {error}")
        
        return self.stats['files_processed'] > 0
    
    def _print_stats(self, output_file: str):
        """Print processing statistics"""
        print(f"\n‚úÖ Merge completed!")
        print(f"   üìÅ Output: {output_file}")
        print(f"   üìä Files processed: {self.stats['files_processed']}")
        if self.stats['files_failed'] > 0:
            print(f"   ‚ö†Ô∏è  Files failed: {self.stats['files_failed']}")
        print(f"   üîó Unique triples: {len(self.merged_graph):,}")
        print(f"   ‚è±Ô∏è  Processing time: {self.stats['processing_time']:.2f}s")
        
        # Calculate throughput
        if self.stats['processing_time'] > 0:
            throughput = len(self.merged_graph) / self.stats['processing_time']
            print(f"   üöÄ Throughput: {throughput:,.0f} triples/second")
        
        # Show file size
        if os.path.exists(output_file):
            size = os.path.getsize(output_file)
            print(f"   üìè Output size: {self._format_size(size)}")
    
    @staticmethod
    def _format_size(size_bytes: int) -> str:
        """Format file size in human readable format"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"

def merge_ttl_files_fast(input_pattern: str, output_file: str, 
                        output_format: str = "turtle", **kwargs) -> bool:
    """
    Fast TTL merger function - optimized for performance
    
    Args:
        input_pattern: File pattern (e.g., "*.ttl")
        output_file: Output filename
        output_format: Output format ("turtle", "xml", "n3", "nt", "json-ld")
        **kwargs: Additional options (validate, show_progress, max_workers, use_parallel)
    """
    merger = OptimizedTTLMerger(
        validate=kwargs.get('validate', False),
        show_progress=kwargs.get('show_progress', True),
        max_workers=kwargs.get('max_workers', 4)
    )
    
    return merger.merge_files(
        input_pattern, 
        output_file, 
        output_format,
        use_parallel=kwargs.get('use_parallel')
    )

def main():
    """Enhanced main with performance options"""
    print("üöÄ Optimized TTL Merger")
    print("=" * 30)
    
    # Parse arguments
    input_pattern = sys.argv[1] if len(sys.argv) > 1 else "*.ttl"
    output_file = sys.argv[2] if len(sys.argv) > 2 else "merged.ttl"
    output_format = sys.argv[3] if len(sys.argv) > 3 else "turtle"
    
    # Performance options
    use_parallel = None  # Auto-detect
    max_workers = 4
    
    # Check for performance flags
    if "--serial" in sys.argv:
        use_parallel = False
    elif "--parallel" in sys.argv:
        use_parallel = True
    
    if "--workers" in sys.argv:
        try:
            idx = sys.argv.index("--workers")
            max_workers = int(sys.argv[idx + 1])
        except (IndexError, ValueError):
            print("‚ö†Ô∏è  Invalid --workers value, using default: 4")
    
    # Validate format
    valid_formats = ["turtle", "xml", "n3", "nt", "json-ld", "trig"]
    if output_format not in valid_formats:
        print(f"‚ùå Invalid format: {output_format}")
        print(f"Valid formats: {', '.join(valid_formats)}")
        return
    
    # Show usage if no files found
    if not glob.glob(input_pattern):
        print(f"‚ùå No files found: {input_pattern}")
        print("\nüí° Usage:")
        print("  python merge_ttl.py [pattern] [output] [format] [options]")
        print("\nOptions:")
        print("  --parallel    Force parallel processing")
        print("  --serial      Force serial processing")
        print("  --workers N   Set thread count (default: 4)")
        print("\nExamples:")
        print("  python merge_ttl.py '*.ttl' merged.ttl")
        print("  python merge_ttl.py 'data/*.ttl' output.rdf xml --parallel")
        print("  python merge_ttl.py '*.ttl' out.ttl turtle --workers 8")
        return
    
    # Run optimized merger
    success = merge_ttl_files_fast(
        input_pattern=input_pattern,
        output_file=output_file, 
        output_format=output_format,
        use_parallel=use_parallel,
        max_workers=max_workers
    )
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    try:
        import rdflib
        main()
    except ImportError:
        print("‚ùå rdflib required: pip install rdflib")
        sys.exit(1)
