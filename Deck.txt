"""
Enhanced Multi-Agent GDPR Document Processing System with Recursive Text Splitting

This system processes GDPR and UK GDPR PDF documents using recursive text splitting,
creates vector embeddings, builds graph structures, and maintains long-term memory 
for comprehensive cross-document linking.

Enhanced Features:
- RecursiveCharacterTextSplitter for comprehensive content capture
- Modern PyMuPDF 1.26+ text extraction from PDF files
- Automatic document type detection (GDPR/UK_GDPR)
- Batch processing of multiple PDF files from data directory
- Advanced HNSW (Hierarchical Navigable Small World) vector indexing
- Scalar quantization (int8_hnsw) for 75% memory reduction
- Dual embedding strategy (full articles + comprehensive chunks)
- Graph-based cross-document relationships
- Long-term memory with LangGraph persistence
- Hybrid search (text + vector) with rescoring
- Performance optimization and monitoring

Requirements:
- Python 3.9+
- PyMuPDF 1.26+ (pip install pymupdf)
- OpenAI API access
- Elasticsearch 8.13+
- Create a 'data' directory in the project root
- Add PDF files (GDPR documents) to the data directory

Installation:
pip install pymupdf openai elasticsearch pydantic langchain langgraph langchain-text-splitters langchain-openai langchain-core
"""

import asyncio
import json
import logging
import os
import ssl
import uuid
import glob
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Sequence, Tuple, Union

import openai
import pymupdf  # Modern PyMuPDF import (not fitz)
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from pydantic import BaseModel, Field, ConfigDict
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import ToolNode, create_react_agent


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Utility Functions
def safe_json_parse(json_string: str, fallback_value: Any = None) -> Any:
    """Safely parse JSON with fallback handling"""
    if not json_string or not isinstance(json_string, str):
        logger.warning(f"Invalid JSON input: {type(json_string)}")
        return fallback_value
    
    # Clean the JSON string
    json_string = json_string.strip()
    
    # Try to extract JSON from markdown code blocks
    if "```json" in json_string:
        try:
            start_idx = json_string.find("```json") + 7
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    elif "```" in json_string:
        try:
            start_idx = json_string.find("```") + 3
            end_idx = json_string.find("```", start_idx)
            if end_idx > start_idx:
                json_string = json_string[start_idx:end_idx].strip()
        except Exception:
            pass
    
    # Multiple parsing attempts with different strategies
    parse_attempts = [
        lambda: json.loads(json_string),
        lambda: json.loads(json_string.replace("'", '"')),  # Fix single quotes
        lambda: json.loads(re.sub(r'(\w+):', r'"\1":', json_string)),  # Fix unquoted keys
        lambda: eval(json_string) if json_string.startswith('{') and json_string.endswith('}') else None,
    ]
    
    for attempt in parse_attempts:
        try:
            result = attempt()
            if result is not None:
                return result
        except Exception as e:
            continue
    
    logger.error(f"Failed to parse JSON: {json_string[:200]}...")
    return fallback_value


def create_fallback_articles(content: str, document_type: str) -> List[Dict]:
    """Create fallback article structure when AI parsing fails"""
    try:
        # Simple regex-based article extraction as fallback
        article_pattern = r'(Article\s+\d+[^:]*:?[^\n]*)\n([^A]*?)(?=Article\s+\d+|Chapter\s+\d+|$)'
        articles = re.findall(article_pattern, content, re.IGNORECASE | re.MULTILINE)
        
        fallback_articles = []
        for i, (title, article_content) in enumerate(articles[:10]):  # Limit to 10 articles
            fallback_articles.append({
                "chapter_number": f"Auto-{i+1}",
                "article_number": f"Art-{i+1}",
                "title": title.strip(),
                "full_content": article_content.strip() if article_content else title,
                "key_concepts": []
            })
        
        # If no articles found, create sections based on content length
        if not fallback_articles:
            content_length = len(content)
            section_size = min(2000, content_length // 5)  # Create up to 5 sections
            
            for i in range(0, min(content_length, 10000), section_size):
                section_content = content[i:i+section_size]
                if section_content.strip():
                    fallback_articles.append({
                        "chapter_number": f"Section-{len(fallback_articles)+1}",
                        "article_number": f"S{len(fallback_articles)+1}",
                        "title": f"{document_type} Section {len(fallback_articles)+1}",
                        "full_content": section_content.strip(),
                        "key_concepts": []
                    })
        
        logger.info(f"Created {len(fallback_articles)} fallback articles for {document_type}")
        return fallback_articles
        
    except Exception as e:
        logger.error(f"Error creating fallback articles: {e}")
        return [{
            "chapter_number": "1",
            "article_number": "1", 
            "title": f"{document_type} Document",
            "full_content": content[:2000] if content else "No content available",
            "key_concepts": []
        }]


def create_fallback_chunk_analysis(text_chunks: List[str]) -> List[Dict]:
    """Create fallback chunk analysis when AI analysis fails"""
    try:
        fallback_analysis = []
        for i, chunk in enumerate(text_chunks):
            # Extract potential title from first line
            lines = chunk.strip().split('\n')
            potential_title = lines[0] if lines else f"Section {i+1}"
            
            # Clean title
            if len(potential_title) > 100:
                potential_title = f"Section {i+1}"
            
            fallback_analysis.append({
                "chunk_index": i,
                "parent_article_number": None,
                "chapter_number": f"Ch-{(i//10)+1}",
                "article_number": None,
                "title": potential_title.strip(),
                "key_concepts": []
            })
        
        logger.info(f"Created fallback analysis for {len(text_chunks)} chunks")
        return fallback_analysis
        
    except Exception as e:
        logger.error(f"Error creating fallback chunk analysis: {e}")
        return []

# Configuration
class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    ES_USERNAME = os.getenv("ES_USERNAME", "elastic")
    ES_PASSWORD = os.getenv("ES_PASSWORD")
    ES_HOST = os.getenv("ES_HOST", "localhost")
    ES_PORT = int(os.getenv("ES_PORT", "9200"))
    ES_CACERT_PATH = os.getenv("ES_CACERT_PATH", "cacert.crt")
    
    # Model configurations
    O3_MINI_MODEL = "o3-mini-2025-01-31"
    EMBEDDING_MODEL = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS = 3072
    REASONING_EFFORT = "high"
    
    # Text splitting configurations
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    SEPARATORS = ["\n\n", "\n", ". ", " ", ""]
    
    # HNSW & Quantization configurations
    VECTOR_INDEX_TYPE = "int8_hnsw"  # Auto scalar quantization
    HNSW_M = 16
    HNSW_EF_CONSTRUCTION = 200
    CONFIDENCE_INTERVAL = 0.95
    
    # Performance optimizations
    ENABLE_PRELOAD = True
    MAX_SEGMENTS = 10


# Pydantic Models for Document Structure
class ChapterReference(BaseModel):
    """Reference to a chapter in a document"""
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None, description="Article identifier")
    title: str = Field(description="Chapter/Article title")
    relevance_score: float = Field(description="Relevance score 0-1")
    relationship_type: str = Field(description="Type of relationship (supports, contradicts, references, etc.)")


class FullArticle(BaseModel):
    """Complete article with full content embedding"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    article_id: str = Field(description="Unique article identifier")
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: str = Field(description="Article identifier")
    title: str = Field(description="Full article title")
    full_content: str = Field(description="Complete article text")
    full_article_embedding: List[float] = Field(description="Embedding of entire article")
    chunk_ids: List[str] = Field(description="IDs of chunks belonging to this article")
    key_concepts: List[str] = Field(default_factory=list, description="Key legal concepts")
    created_at: datetime = Field(default_factory=datetime.now)


class DocumentChunk(BaseModel):
    """Individual document chunk with metadata"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    chunk_id: str = Field(description="Unique chunk identifier")
    parent_article_id: Optional[str] = Field(default=None, description="Parent article ID")
    document_type: str = Field(description="GDPR or UK_GDPR")
    chapter_number: str = Field(description="Chapter identifier")
    article_number: Optional[str] = Field(default=None)
    title: str = Field(description="Section title")
    content: str = Field(description="Text content")
    chunk_embedding: Optional[List[float]] = Field(default=None, description="Chunk-level vector embedding")
    supporting_references: List[ChapterReference] = Field(default_factory=list)
    page_number: Optional[int] = Field(default=None, description="Source page number")
    chunk_index: int = Field(description="Sequential chunk index")
    created_at: datetime = Field(default_factory=datetime.now)
    processed_by_agent: Optional[str] = Field(default=None)


class CrossDocumentLink(BaseModel):
    """Cross-document relationship"""
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: str
    confidence_score: float
    created_at: datetime = Field(default_factory=datetime.now)


class AgentMemory(BaseModel):
    """Long-term memory structure for agents"""
    memory_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    agent_name: str
    memory_type: str  # semantic, episodic, procedural
    content: Dict[str, Any]
    namespace: List[str]
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)


# State Management
class ProcessingState(TypedDict):
    """State for the processing workflow"""
    messages: Annotated[Sequence[BaseMessage], "The conversation messages"]
    full_articles: List[FullArticle]
    documents: List[DocumentChunk]
    current_chunk: Optional[DocumentChunk]
    cross_links: List[CrossDocumentLink]
    processing_stage: str
    agent_memories: List[AgentMemory]
    elasticsearch_client: Optional[Any]


# Elasticsearch Manager
class ElasticsearchManager:
    """Manages Elasticsearch operations with SSL authentication and HNSW optimization"""
    
    def __init__(self):
        self.client = self._create_client()
        self._setup_indices()
    
    def _create_client(self) -> Elasticsearch:
        """Create Elasticsearch client with SSL configuration"""
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        if os.path.exists(Config.ES_CACERT_PATH):
            ssl_context.load_verify_locations(Config.ES_CACERT_PATH)
        
        return Elasticsearch(
            [{"host": Config.ES_HOST, "port": Config.ES_PORT, "scheme": "https"}],
            basic_auth=(Config.ES_USERNAME, Config.ES_PASSWORD),
            ssl_context=ssl_context,
            verify_certs=True,
            request_timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
    
    def _setup_indices(self):
        """Setup Elasticsearch indices with advanced HNSW and quantization configurations"""
        
        def create_vector_field_config(field_name: str) -> Dict:
            """Create optimized vector field configuration with HNSW and quantization"""
            return {
                "type": "dense_vector",
                "dims": Config.EMBEDDING_DIMENSIONS,
                "index": True,
                "similarity": "cosine",
                "index_options": {
                    "type": Config.VECTOR_INDEX_TYPE,
                    "m": Config.HNSW_M,
                    "ef_construction": Config.HNSW_EF_CONSTRUCTION,
                    "confidence_interval": Config.CONFIDENCE_INTERVAL
                }
            }
        
        # Full articles index with advanced HNSW
        articles_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "merge.policy.max_merged_segment": "5gb",
                    "merge.policy.segments_per_tier": 4,
                    "store.preload": ["vec", "vem", "nvd", "nvm"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "full_content": {"type": "text", "analyzer": "standard"},
                    "full_article_embedding": create_vector_field_config("full_article_embedding"),
                    "chunk_ids": {"type": "keyword"},
                    "key_concepts": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Document chunks index with advanced HNSW
        chunks_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "5s",
                    "merge.policy.max_merged_segment": "2gb",
                    "merge.policy.segments_per_tier": 4,
                    "store.preload": ["veq", "vemq"] if Config.ENABLE_PRELOAD else []
                }
            },
            "mappings": {
                "properties": {
                    "chunk_id": {"type": "keyword"},
                    "parent_article_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "chapter_number": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "standard"},
                    "content": {"type": "text", "analyzer": "standard"},
                    "chunk_embedding": create_vector_field_config("chunk_embedding"),
                    "supporting_references": {
                        "type": "nested",
                        "properties": {
                            "document_type": {"type": "keyword"},
                            "chapter_number": {"type": "keyword"},
                            "article_number": {"type": "keyword"},
                            "title": {"type": "text"},
                            "relevance_score": {"type": "float"},
                            "relationship_type": {"type": "keyword"}
                        }
                    },
                    "page_number": {"type": "integer"},
                    "chunk_index": {"type": "integer"},
                    "created_at": {"type": "date"},
                    "processed_by_agent": {"type": "keyword"}
                }
            }
        }
        
        # Cross-document links index
        links_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "1s",
                }
            },
            "mappings": {
                "properties": {
                    "source_chunk_id": {"type": "keyword"},
                    "target_chunk_id": {"type": "keyword"},
                    "relationship_type": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # Agent memories index
        memories_mapping = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                }
            },
            "mappings": {
                "properties": {
                    "memory_id": {"type": "keyword"},
                    "agent_name": {"type": "keyword"},
                    "memory_type": {"type": "keyword"},
                    "content": {"type": "object"},
                    "namespace": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
        
        # Create indices
        indices = {
            "gdpr_articles": articles_mapping,
            "gdpr_chunks": chunks_mapping,
            "gdpr_links": links_mapping,
            "agent_memories": memories_mapping
        }
        
        for index_name, mapping in indices.items():
            if not self.client.indices.exists(index=index_name):
                self.client.indices.create(index=index_name, body=mapping)
                logger.info(f"Created index with HNSW optimization: {index_name}")
            else:
                logger.info(f"Index already exists: {index_name}")
    
    def index_article(self, article: FullArticle) -> bool:
        """Index a full article"""
        try:
            doc = article.model_dump()
            doc["created_at"] = article.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_articles",
                id=article.article_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing article {article.article_id}: {e}")
            raise
    
    def index_chunk(self, chunk: DocumentChunk) -> bool:
        """Index a document chunk"""
        try:
            doc = chunk.model_dump()
            doc["created_at"] = chunk.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_chunks",
                id=chunk.chunk_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing chunk {chunk.chunk_id}: {e}")
            raise
    
    def index_link(self, link: CrossDocumentLink) -> bool:
        """Index a cross-document link"""
        try:
            doc = link.model_dump()
            doc["created_at"] = link.created_at.isoformat()
            
            response = self.client.index(
                index="gdpr_links",
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing link: {e}")
            raise
    
    def index_memory(self, memory: AgentMemory) -> bool:
        """Index agent memory"""
        try:
            doc = memory.model_dump()
            doc["created_at"] = memory.created_at.isoformat()
            doc["updated_at"] = memory.updated_at.isoformat()
            
            response = self.client.index(
                index="agent_memories",
                id=memory.memory_id,
                body=doc
            )
            return response["result"] in ["created", "updated"]
        except Exception as e:
            logger.error(f"Error indexing memory {memory.memory_id}: {e}")
            raise
    
    def hybrid_search(self, query: str, embedding: List[float], filters: Dict = None, 
                     search_level: str = "both", rescore: bool = True) -> Dict[str, List[Dict]]:
        """Perform optimized hybrid search with advanced HNSW and quantization features"""
        results = {}
        
        try:
            # Search articles
            if search_level in ["articles", "both"]:
                article_search = {
                    "knn": {
                        "field": "full_article_embedding",
                        "query_vector": embedding,
                        "k": 10,
                        "num_candidates": 50,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^3", "full_content^2", "key_concepts^2"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 20,
                }
                
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    article_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'full_article_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                article_response = self.client.search(index="gdpr_articles", body=article_search)
                results["articles"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in article_response["hits"]["hits"]
                ]
            
            # Search chunks
            if search_level in ["chunks", "both"]:
                chunk_search = {
                    "knn": {
                        "field": "chunk_embedding",
                        "query_vector": embedding,
                        "k": 10,
                        "num_candidates": 50,
                    },
                    "query": {
                        "bool": {
                            "should": [
                                {
                                    "multi_match": {
                                        "query": query,
                                        "fields": ["title^2", "content^1.5"],
                                        "type": "best_fields",
                                        "boost": 0.3
                                    }
                                }
                            ],
                            "filter": [{"terms": {k: v}} for k, v in (filters or {}).items()]
                        }
                    },
                    "_source": True,
                    "size": 20,
                }
                
                if rescore and Config.VECTOR_INDEX_TYPE.startswith("int"):
                    chunk_search["rescore"] = {
                        "window_size": 50,
                        "query": {
                            "rescore_query": {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'chunk_embedding') + 1.0",
                                        "params": {"query_vector": embedding}
                                    }
                                }
                            },
                            "query_weight": 0.7,
                            "rescore_query_weight": 0.3
                        }
                    }
                
                chunk_response = self.client.search(index="gdpr_chunks", body=chunk_search)
                results["chunks"] = [
                    {**hit["_source"], "_score": hit["_score"]} 
                    for hit in chunk_response["hits"]["hits"]
                ]
            
            return results
            
        except Exception as e:
            logger.error(f"Error in optimized hybrid search: {e}")
            raise
    
    def get_related_chunks(self, chunk_id: str) -> List[Dict]:
        """Get chunks related through graph links"""
        try:
            outgoing_query = {
                "query": {"term": {"source_chunk_id": chunk_id}},
                "size": 50
            }
            
            incoming_query = {
                "query": {"term": {"target_chunk_id": chunk_id}},
                "size": 50
            }
            
            outgoing_response = self.client.search(index="gdpr_links", body=outgoing_query)
            incoming_response = self.client.search(index="gdpr_links", body=incoming_query)
            
            related_chunk_ids = set()
            for hit in outgoing_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["target_chunk_id"])
            for hit in incoming_response["hits"]["hits"]:
                related_chunk_ids.add(hit["_source"]["source_chunk_id"])
            
            if not related_chunk_ids:
                return []
            
            chunks_query = {
                "query": {"terms": {"chunk_id": list(related_chunk_ids)}},
                "size": len(related_chunk_ids)
            }
            
            chunks_response = self.client.search(index="gdpr_chunks", body=chunks_query)
            return [hit["_source"] for hit in chunks_response["hits"]["hits"]]
            
        except Exception as e:
            logger.error(f"Error getting related chunks for {chunk_id}: {e}")
            raise
    
    def get_index_stats(self) -> Dict[str, Any]:
        """Get comprehensive index statistics"""
        try:
            stats = {}
            
            for index_name in ["gdpr_articles", "gdpr_chunks", "gdpr_links", "agent_memories"]:
                if self.client.indices.exists(index=index_name):
                    index_stats = self.client.indices.stats(index=index_name)
                    
                    stats[index_name] = {
                        "documents": index_stats["indices"][index_name]["total"]["docs"]["count"],
                        "size_bytes": index_stats["indices"][index_name]["total"]["store"]["size_in_bytes"],
                        "segments": index_stats["indices"][index_name]["total"]["segments"]["count"],
                        "vector_size_estimate": self._estimate_vector_memory_usage(index_name),
                    }
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting index stats: {e}")
            raise
    
    def _estimate_vector_memory_usage(self, index_name: str) -> Dict[str, str]:
        """Estimate memory usage for vectors with quantization"""
        try:
            doc_count_response = self.client.count(index=index_name)
            doc_count = doc_count_response["count"]
            
            if doc_count == 0:
                return {"estimated_memory": "0 MB", "quantization_savings": "N/A"}
            
            if Config.VECTOR_INDEX_TYPE == "int8_hnsw":
                vector_memory = doc_count * (Config.EMBEDDING_DIMENSIONS + 4) * 1
                original_memory = doc_count * Config.EMBEDDING_DIMENSIONS * 4
                savings = ((original_memory - vector_memory) / original_memory) * 100
            else:
                vector_memory = doc_count * Config.EMBEDDING_DIMENSIONS * 4
                savings = 0
            
            hnsw_overhead = doc_count * 4 * Config.HNSW_M
            total_memory = vector_memory + hnsw_overhead
            
            return {
                "estimated_memory": f"{total_memory / (1024**2):.1f} MB",
                "quantization_savings": f"{savings:.1f}%",
                "hnsw_overhead": f"{hnsw_overhead / (1024**2):.1f} MB"
            }
            
        except Exception as e:
            logger.error(f"Error estimating memory usage: {e}")
            return {"estimated_memory": "Unknown", "quantization_savings": "Unknown"}
    
    def optimize_indices(self):
        """Optimize indices for better HNSW performance"""
        try:
            for index_name in ["gdpr_articles", "gdpr_chunks"]:
                if self.client.indices.exists(index=index_name):
                    logger.info(f"Optimizing HNSW performance for {index_name}...")
                    
                    self.client.indices.forcemerge(
                        index=index_name,
                        max_num_segments=Config.MAX_SEGMENTS,
                        wait_for_completion=False
                    )
                    
                    optimization_settings = {
                        "index": {
                            "refresh_interval": "30s",
                            "merge.policy.max_merged_segment": "5gb",
                        }
                    }
                    
                    self.client.indices.put_settings(
                        index=index_name,
                        body={"settings": optimization_settings}
                    )
                    
                    logger.info(f"Applied HNSW optimization settings to {index_name}")
                    
        except Exception as e:
            logger.error(f"Error optimizing indices: {e}")
            raise


# OpenAI API Manager
class OpenAIManager:
    """Manages OpenAI API calls with direct API usage"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
    
    async def create_embedding(self, text: str) -> List[float]:
        """Create embedding using OpenAI API directly"""
        try:
            response = self.client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error creating embedding: {e}")
            raise
    
    async def reasoning_completion(self, messages: List[Dict], system_prompt: str = None) -> str:
        """Create completion using o3-mini with high reasoning effort"""
        try:
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({"role": "developer", "content": system_prompt})
            
            formatted_messages.extend(messages)
            
            response = self.client.chat.completions.create(
                model=Config.O3_MINI_MODEL,
                messages=formatted_messages,
                reasoning_effort=Config.REASONING_EFFORT
            )
            
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error in reasoning completion: {e}")
            raise


# Enhanced Document Processing Agent with Recursive Text Splitting
class DocumentProcessingAgent:
    """Agent for processing and chunking documents with comprehensive text splitting"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.name = "DocumentProcessor"
        
        # Initialize the recursive text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            separators=Config.SEPARATORS,
            keep_separator=True,
            is_separator_regex=False,
        )
    
    def _extract_text_from_pdf(self, file_path: str) -> Tuple[str, Dict[int, str]]:
        """Extract text from PDF file using modern PyMuPDF 1.26+ with page tracking"""
        try:
            doc = pymupdf.open(file_path)
            
            if doc.needs_pass:
                doc.close()
                raise ValueError(f"PDF {file_path} is password protected. Please provide an unencrypted version.")
            
            page_count = doc.page_count
            if page_count == 0:
                doc.close()
                raise ValueError(f"PDF {file_path} contains no pages")
            
            full_text = ""
            page_texts = {}
            extracted_pages = 0
            
            for page in doc:
                try:
                    page_text = page.get_text()
                    if page_text.strip():
                        page_texts[page.number] = page_text
                        full_text += f"\n\n--- PAGE {page.number + 1} ---\n\n"
                        full_text += page_text
                        extracted_pages += 1
                except Exception as page_error:
                    logger.warning(f"Error extracting text from page {page.number + 1} in {file_path}: {page_error}")
                    continue
            
            doc.close()
            
            if extracted_pages == 0:
                raise ValueError(f"No text could be extracted from PDF {file_path}")
            
            logger.info(f"Successfully extracted text from {extracted_pages}/{page_count} pages in {file_path}")
            return full_text.strip(), page_texts
            
        except Exception as e:
            logger.error(f"Error extracting text from PDF {file_path}: {e}")
            raise
    
    def _determine_document_type(self, file_path: str, content: str) -> str:
        """Determine document type based on filename and content"""
        filename = os.path.basename(file_path).lower()
        content_lower = content.lower()
        
        if "uk" in filename and "gdpr" in filename:
            return "UK_GDPR"
        elif "gdpr" in filename:
            return "GDPR"
        elif "uk" in filename and ("data" in filename or "protection" in filename):
            return "UK_GDPR"
        
        if "united kingdom" in content_lower or "uk data" in content_lower:
            return "UK_GDPR"
        elif "general data protection regulation" in content_lower or "regulation (eu)" in content_lower:
            return "GDPR"
        
        return "GDPR" if "gdpr" in filename else "UK_GDPR"
    
    def _extract_page_number(self, chunk_text: str) -> Optional[int]:
        """Extract page number from chunk text containing page markers"""
        import re
        page_match = re.search(r'--- PAGE (\d+) ---', chunk_text)
        if page_match:
            return int(page_match.group(1))
        return None
    
    async def process_document(self, file_path: str, document_type: str = None) -> Tuple[List[FullArticle], List[DocumentChunk]]:
        """Process a PDF document with comprehensive recursive text splitting and robust error handling"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"File not found: {file_path}")
                raise FileNotFoundError(f"Document file not found: {file_path}")
            
            logger.info(f"Extracting text from PDF: {file_path}")
            content, page_texts = self._extract_text_from_pdf(file_path)
            
            if not content.strip():
                logger.error(f"No text extracted from PDF: {file_path}")
                raise ValueError(f"No text content found in PDF: {file_path}")
            
            if not document_type:
                document_type = self._determine_document_type(file_path, content)
                logger.info(f"Determined document type: {document_type} for {file_path}")
            
            logger.info(f"Processing {document_type} document: {file_path} ({len(content)} characters)")
            
            # Step 1: Use recursive text splitter to create comprehensive chunks
            logger.info("Creating comprehensive text chunks using RecursiveCharacterTextSplitter...")
            try:
                text_chunks = self.text_splitter.split_text(content)
                logger.info(f"Created {len(text_chunks)} text chunks")
            except Exception as e:
                logger.error(f"Error in text splitting: {e}")
                # Fallback: simple splitting
                chunk_size = Config.CHUNK_SIZE
                text_chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
                logger.info(f"Fallback splitting created {len(text_chunks)} chunks")
            
            # Step 2: Use o3-mini to identify and organize articles within the document
            article_system_prompt = """
            You are an expert in GDPR documentation. Analyze the entire document and identify 
            complete articles/sections with their proper structure. Each article should contain 
            the full text including all sub-sections, clauses, and paragraphs.
            
            Return ONLY a valid JSON object with the following structure:
            {
                "articles": [
                    {
                        "chapter_number": "string",
                        "article_number": "string", 
                        "title": "string",
                        "full_content": "complete article text including all subsections",
                        "key_concepts": ["concept1", "concept2"]
                    }
                ]
            }
            
            Make sure to return valid JSON only. Do not include explanations or markdown formatting.
            """
            
            article_messages = [
                {"role": "user", "content": f"Document type: {document_type}\n\nDocument content:\n{content[:8000]}"}  # Limit content size
            ]
            
            articles_data = []
            try:
                article_response = await self.openai_manager.reasoning_completion(article_messages, article_system_prompt)
                articles_json = safe_json_parse(article_response, {"articles": []})
                articles_data = articles_json.get("articles", []) if isinstance(articles_json, dict) else []
                
                if not articles_data:
                    logger.warning("No articles found in AI response, using fallback")
                    articles_data = create_fallback_articles(content, document_type)
                else:
                    logger.info(f"AI successfully identified {len(articles_data)} articles")
                    
            except Exception as e:
                logger.error(f"Error in article analysis: {e}, using fallback")
                articles_data = create_fallback_articles(content, document_type)
            
            # Step 3: Use o3-mini to intelligently organize and categorize the text chunks
            chunk_system_prompt = """
            You are an expert in GDPR documentation. For each provided text chunk, identify:
            - Which article/section it belongs to
            - Its specific role within that article
            - Key legal concepts it contains
            - Appropriate title/heading
            
            Return ONLY a valid JSON object:
            {
                "chunk_analysis": [
                    {
                        "chunk_index": 0,
                        "parent_article_number": "string or null",
                        "chapter_number": "string",
                        "article_number": "string or null",
                        "title": "descriptive title for this chunk",
                        "key_concepts": ["concept1", "concept2"]
                    }
                ]
            }
            
            Return valid JSON only. Analyze ALL provided chunks.
            """
            
            # Send chunks in batches for analysis to avoid token limits
            chunk_analyses = []
            batch_size = 5  # Reduced batch size for better processing
            
            for i in range(0, len(text_chunks), batch_size):
                try:
                    batch_chunks = text_chunks[i:i+batch_size]
                    batch_content = ""
                    
                    for idx, chunk in enumerate(batch_chunks):
                        batch_content += f"\n\n=== CHUNK {i + idx} ===\n{chunk[:1000]}"  # Limit chunk size
                    
                    chunk_messages = [
                        {"role": "user", "content": f"Document type: {document_type}\n\nText chunks to analyze:\n{batch_content}"}
                    ]
                    
                    try:
                        batch_response = await self.openai_manager.reasoning_completion(chunk_messages, chunk_system_prompt)
                        batch_json = safe_json_parse(batch_response, {"chunk_analysis": []})
                        batch_analysis = batch_json.get("chunk_analysis", []) if isinstance(batch_json, dict) else []
                        
                        if batch_analysis:
                            chunk_analyses.extend(batch_analysis)
                            logger.info(f"Successfully analyzed batch {i//batch_size + 1}")
                        else:
                            logger.warning(f"No analysis for batch {i//batch_size + 1}, using fallback")
                            fallback_batch = create_fallback_chunk_analysis(batch_chunks)
                            # Adjust indices for fallback
                            for fb_chunk in fallback_batch:
                                fb_chunk["chunk_index"] = i + fb_chunk["chunk_index"]
                            chunk_analyses.extend(fallback_batch)
                            
                    except Exception as batch_error:
                        logger.error(f"Error analyzing batch {i//batch_size + 1}: {batch_error}")
                        fallback_batch = create_fallback_chunk_analysis(batch_chunks)
                        # Adjust indices for fallback
                        for fb_chunk in fallback_batch:
                            fb_chunk["chunk_index"] = i + fb_chunk["chunk_index"]
                        chunk_analyses.extend(fallback_batch)
                    
                    # Small delay to avoid rate limiting
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"Error processing batch {i//batch_size + 1}: {e}")
                    continue
            
            # Ensure we have analysis for all chunks
            if len(chunk_analyses) < len(text_chunks):
                logger.warning(f"Missing analysis for some chunks, creating fallback for remaining")
                missing_chunks = text_chunks[len(chunk_analyses):]
                fallback_analyses = create_fallback_chunk_analysis(missing_chunks)
                for fb_chunk in fallback_analyses:
                    fb_chunk["chunk_index"] = len(chunk_analyses) + fb_chunk["chunk_index"]
                chunk_analyses.extend(fallback_analyses)
            
            # Step 4: Create FullArticle objects with error handling
            full_articles = []
            article_chunk_map = {}
            
            for article_data in articles_data:
                try:
                    article_id = str(uuid.uuid4())
                    
                    # Safely extract article data
                    title = str(article_data.get('title', 'Untitled Article'))
                    full_content = str(article_data.get('full_content', ''))
                    
                    # Create embedding with error handling
                    try:
                        full_text = f"{title} {full_content}"
                        full_embedding = await self.openai_manager.create_embedding(full_text[:3000])  # Limit embedding text
                    except Exception as e:
                        logger.error(f"Error creating embedding for article {article_id}: {e}")
                        # Create a dummy embedding of correct dimensions
                        full_embedding = [0.0] * Config.EMBEDDING_DIMENSIONS
                    
                    article = FullArticle(
                        article_id=article_id,
                        document_type=document_type,
                        chapter_number=str(article_data.get("chapter_number", "Unknown")),
                        article_number=str(article_data.get("article_number", f"A{len(full_articles)+1}")),
                        title=title,
                        full_content=full_content,
                        full_article_embedding=full_embedding,
                        chunk_ids=[],
                        key_concepts=article_data.get("key_concepts", []) if isinstance(article_data.get("key_concepts"), list) else []
                    )
                    
                    full_articles.append(article)
                    
                    # Try to index in Elasticsearch with error handling
                    try:
                        self.es_manager.index_article(article)
                    except Exception as e:
                        logger.error(f"Error indexing article {article_id}: {e}")
                    
                    # Initialize chunk mapping
                    article_chunk_map[article.article_number] = {
                        "article": article,
                        "chunk_ids": []
                    }
                    
                except Exception as e:
                    logger.error(f"Error creating article: {e}")
                    continue
            
            # Step 5: Create DocumentChunk objects from all text chunks with error handling
            chunks = []
            
            for i, chunk_text in enumerate(text_chunks):
                try:
                    chunk_id = str(uuid.uuid4())
                    
                    # Get analysis for this chunk
                    chunk_analysis = None
                    for analysis in chunk_analyses:
                        if analysis.get("chunk_index") == i:
                            chunk_analysis = analysis
                            break
                    
                    if not chunk_analysis:
                        chunk_analysis = {
                            "parent_article_number": None,
                            "chapter_number": f"Ch-{(i//10)+1}",
                            "article_number": None,
                            "title": f"Section {i + 1}",
                            "key_concepts": []
                        }
                    
                    # Find parent article
                    parent_article = None
                    parent_article_number = chunk_analysis.get("parent_article_number")
                    
                    if parent_article_number and parent_article_number in article_chunk_map:
                        parent_article = article_chunk_map[parent_article_number]["article"]
                    
                    # Extract page number
                    page_number = self._extract_page_number(chunk_text)
                    
                    # Create chunk embedding with error handling
                    try:
                        title = str(chunk_analysis.get('title', f'Section {i + 1}'))
                        embedding_text = f"{title} {chunk_text[:1000]}"  # Limit text length
                        chunk_embedding = await self.openai_manager.create_embedding(embedding_text)
                    except Exception as e:
                        logger.error(f"Error creating embedding for chunk {chunk_id}: {e}")
                        chunk_embedding = [0.0] * Config.EMBEDDING_DIMENSIONS
                    
                    chunk = DocumentChunk(
                        chunk_id=chunk_id,
                        parent_article_id=parent_article.article_id if parent_article else None,
                        document_type=document_type,
                        chapter_number=str(chunk_analysis.get("chapter_number", "Unknown")),
                        article_number=str(chunk_analysis.get("article_number")) if chunk_analysis.get("article_number") else None,
                        title=str(chunk_analysis.get("title", f"Section {i + 1}")),
                        content=chunk_text,
                        chunk_embedding=chunk_embedding,
                        page_number=page_number,
                        chunk_index=i,
                        processed_by_agent=self.name
                    )
                    
                    chunks.append(chunk)
                    
                    # Try to index in Elasticsearch
                    try:
                        self.es_manager.index_chunk(chunk)
                    except Exception as e:
                        logger.error(f"Error indexing chunk {chunk_id}: {e}")
                    
                    # Update parent article's chunk_ids
                    if parent_article_number and parent_article_number in article_chunk_map:
                        article_chunk_map[parent_article_number]["chunk_ids"].append(chunk_id)
                    
                    # Small delay to avoid overwhelming the system
                    if i % 10 == 0:
                        await asyncio.sleep(0.1)
                        
                except Exception as e:
                    logger.error(f"Error creating chunk {i}: {e}")
                    continue
            
            # Step 6: Update articles with their chunk IDs
            for article_number, article_info in article_chunk_map.items():
                try:
                    article = article_info["article"]
                    article.chunk_ids = article_info["chunk_ids"]
                    self.es_manager.index_article(article)
                except Exception as e:
                    logger.error(f"Error updating article {article_number}: {e}")
            
            logger.info(f"Successfully processed {len(full_articles)} articles and {len(chunks)} chunks for {document_type}")
            logger.info(f"Total text chunks created: {len(text_chunks)}")
            logger.info(f"Chunks with page numbers: {sum(1 for chunk in chunks if chunk.page_number is not None)}")
            
            return full_articles, chunks
            
        except Exception as e:
            logger.error(f"Error processing document {file_path}: {e}")
            raise


# Cross-Reference Agent
class CrossReferenceAgent:
    """Agent for finding cross-references between documents"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager, 
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "CrossReferenceAgent"
    
    async def find_cross_references(self, chunk: DocumentChunk, all_chunks: List[DocumentChunk]) -> List[CrossDocumentLink]:
        """Find cross-references for a given chunk using enhanced search"""
        links = []
        
        chunk_embedding = chunk.chunk_embedding
        if not chunk_embedding:
            chunk_embedding = await self.openai_manager.create_embedding(
                f"{chunk.title} {chunk.content}"
            )
        
        # Enhanced hybrid search with more sophisticated filtering
        search_results = self.es_manager.hybrid_search(
            query=f"{chunk.title} {chunk.content[:500]}",
            embedding=chunk_embedding,
            filters={"document_type": ["GDPR", "UK_GDPR"]},
            search_level="chunks"
        )
        
        # Filter out same chunk and same document type, with enhanced similarity scoring
        candidate_chunks = []
        for result in search_results.get("chunks", []):
            if (result["chunk_id"] != chunk.chunk_id and 
                result["document_type"] != chunk.document_type and
                result.get("_score", 0) > 0.5):  # Only consider high-similarity chunks
                candidate_chunks.append(result)
        
        # Analyze relationships with top candidates
        for candidate in candidate_chunks[:8]:  # Increased to top 8 candidates
            relationship = await self._analyze_relationship(chunk, candidate)
            
            if relationship and relationship["confidence_score"] > 0.6:
                link = CrossDocumentLink(
                    source_chunk_id=chunk.chunk_id,
                    target_chunk_id=candidate["chunk_id"],
                    relationship_type=relationship["relationship_type"],
                    confidence_score=relationship["confidence_score"]
                )
                
                links.append(link)
                self.es_manager.index_link(link)
        
        # Store findings in long-term memory
        await self._store_cross_reference_memory(chunk, links)
        
        return links
    
    async def _analyze_relationship(self, source_chunk: DocumentChunk, target_chunk: Dict) -> Optional[Dict]:
        """Analyze relationship between two chunks using o3-mini with enhanced prompting and error handling"""
        system_prompt = """
        You are an expert legal analyst specializing in GDPR and data protection laws. 
        Analyze the relationship between two legal text chunks from different documents
        (GDPR vs UK GDPR) and determine:
        
        1. The type of relationship:
           - "supports": Content supports or reinforces the other
           - "contradicts": Content conflicts or differs significantly  
           - "references": Content explicitly references the other
           - "complements": Content adds additional information
           - "specifies": Content provides specific implementation details
           - "generalizes": Content provides broader context
           - "equivalent": Content is substantially similar
        
        2. Confidence score (0.0 to 1.0):
           - 0.9-1.0: Strong, clear relationship
           - 0.7-0.8: Good relationship with minor uncertainties
           - 0.6-0.6: Moderate relationship, some ambiguity
           - Below 0.6: Weak or unclear relationship
        
        3. Brief explanation of the relationship
        
        Return ONLY valid JSON format:
        {
            "relationship_type": "string",
            "confidence_score": 0.0,
            "explanation": "string"
        }
        
        Return null if no meaningful relationship exists (confidence < 0.6).
        Focus on legal concepts, obligations, rights, and procedures.
        """
        
        messages = [
            {
                "role": "user", 
                "content": f"""
                Source chunk ({source_chunk.document_type}):
                Chapter: {source_chunk.chapter_number}
                Article: {source_chunk.article_number or 'N/A'}
                Title: {source_chunk.title}
                Content: {source_chunk.content[:800]}...
                
                Target chunk ({target_chunk['document_type']}):
                Chapter: {target_chunk['chapter_number']}
                Article: {target_chunk.get('article_number', 'N/A')}
                Title: {target_chunk['title']}
                Content: {target_chunk['content'][:800]}...
                
                Analyze the legal relationship between these chunks from different regulatory frameworks.
                """
            }
        ]
        
        try:
            response = await self.openai_manager.reasoning_completion(messages, system_prompt)
            
            # Use safe JSON parsing
            result = safe_json_parse(response, None)
            
            if result and isinstance(result, dict):
                # Validate required fields
                if all(key in result for key in ['relationship_type', 'confidence_score', 'explanation']):
                    confidence = float(result.get("confidence_score", 0))
                    if confidence >= 0.6:
                        return {
                            "relationship_type": str(result["relationship_type"]),
                            "confidence_score": confidence,
                            "explanation": str(result["explanation"])
                        }
            
            # If parsing fails or confidence too low, return None
            return None
            
        except Exception as e:
            logger.error(f"Error analyzing relationship: {e}")
            return None
    
    async def _store_cross_reference_memory(self, chunk: DocumentChunk, links: List[CrossDocumentLink]):
        """Store cross-reference findings in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "chapter_number": chunk.chapter_number,
            "chunk_index": chunk.chunk_index,
            "found_links": len(links),
            "link_types": ",".join([link.relationship_type for link in links]) if links else "",
            "analysis_timestamp": datetime.now().isoformat(),
            "page_number": chunk.page_number
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="episodic",
            content=memory_content,
            namespace=["cross_reference", chunk.document_type]
        )
        
        self.es_manager.index_memory(memory)
        
        try:
            await self.memory_store.aput(
                namespace=tuple(["cross_reference", self.name]),
                key=f"analysis_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing memory in LangGraph store: {e}")


# Linking Agent
class LinkingAgent:
    """Agent for maintaining and updating document links"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager,
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "LinkingAgent"
    
    async def update_chunk_references(self, chunk: DocumentChunk, related_links: List[CrossDocumentLink]) -> DocumentChunk:
        """Update chunk with supporting references"""
        
        related_chunks = self.es_manager.get_related_chunks(chunk.chunk_id)
        
        references = []
        for related_chunk in related_chunks:
            link = next(
                (l for l in related_links 
                 if l.source_chunk_id == chunk.chunk_id and l.target_chunk_id == related_chunk["chunk_id"] or
                    l.target_chunk_id == chunk.chunk_id and l.source_chunk_id == related_chunk["chunk_id"]),
                None
            )
            
            if link:
                reference = ChapterReference(
                    document_type=related_chunk["document_type"],
                    chapter_number=related_chunk["chapter_number"],
                    article_number=related_chunk.get("article_number"),
                    title=related_chunk["title"],
                    relevance_score=link.confidence_score,
                    relationship_type=link.relationship_type
                )
                references.append(reference)
        
        chunk.supporting_references = references
        self.es_manager.index_chunk(chunk)
        
        await self._store_linking_memory(chunk, references)
        
        return chunk
    
    async def _store_linking_memory(self, chunk: DocumentChunk, references: List[ChapterReference]):
        """Store linking activity in long-term memory"""
        memory_content = {
            "chunk_id": chunk.chunk_id,
            "document_type": chunk.document_type,
            "chunk_index": chunk.chunk_index,
            "references_added": len(references),
            "reference_types": ",".join([ref.relationship_type for ref in references]) if references else "",
            "timestamp": datetime.now().isoformat(),
            "page_number": chunk.page_number
        }
        
        memory = AgentMemory(
            agent_name=self.name,
            memory_type="procedural",
            content=memory_content,
            namespace=["linking", chunk.document_type]
        )
        
        self.es_manager.index_memory(memory)
        
        try:
            await self.memory_store.aput(
                namespace=tuple(["linking", self.name]),
                key=f"linking_{chunk.chunk_id}",
                value=memory_content
            )
        except Exception as e:
            logger.error(f"Error storing linking memory: {e}")


# ReAct Query Agent for Intelligent GDPR Analysis
class ReActQueryAgent:
    """ReAct-based agent for intelligent GDPR document querying and analysis"""
    
    def __init__(self, openai_manager: OpenAIManager, es_manager: ElasticsearchManager, 
                 memory_store: InMemoryStore):
        self.openai_manager = openai_manager
        self.es_manager = es_manager
        self.memory_store = memory_store
        self.name = "ReActQueryAgent"
        
        # Create the ReAct agent with o3-mini model
        self.model = openai.OpenAI(
            api_key=Config.OPENAI_API_KEY,
            base_url=Config.OPENAI_BASE_URL
        )
        
        # System prompt for the ReAct agent
        self.system_prompt = """
        You are an expert GDPR and data protection law analyst with access to comprehensive 
        GDPR and UK GDPR document databases. You can reason through complex legal queries 
        and use tools to find relevant information.
        
        Your capabilities include:
        - Searching through GDPR and UK GDPR articles and chunks
        - Finding cross-document relationships and references
        - Analyzing legal concepts and requirements
        - Providing detailed, accurate legal analysis
        - Comparing regulations between GDPR and UK GDPR
        
        When answering questions:
        1. Think step by step about what information you need
        2. Use appropriate tools to search for relevant content
        3. Analyze relationships between different sections
        4. Provide comprehensive, well-reasoned answers
        5. Cite specific articles and sections when possible
        
        Always be thorough in your analysis and use multiple searches if needed to 
        provide complete answers.
        """
        
        # Create tools list for the ReAct agent
        self.tools = [
            self._create_search_articles_tool(),
            self._create_search_chunks_tool(),
            self._create_search_both_tool(),
            self._create_get_relationships_tool(),
            self._create_compare_documents_tool(),
            self._create_analyze_concept_tool()
        ]
        
        # Create the ReAct agent
        self.react_agent = None
        
    def _create_search_articles_tool(self):
        """Create tool for searching full articles with robust error handling"""
        @tool
        async def search_articles_tool(query: str, document_type: str = None) -> str:
            """
            Search for full GDPR/UK GDPR articles based on a query.
            
            Args:
                query: The search query describing what you're looking for
                document_type: Optional filter for "GDPR" or "UK_GDPR"
            
            Returns:
                String containing relevant articles with titles, content, and scores
            """
            try:
                if not query or not isinstance(query, str):
                    return "Error: Invalid query provided"
                
                embedding = await self.openai_manager.create_embedding(query.strip())
                filters = {}
                if document_type and document_type in ["GDPR", "UK_GDPR"]:
                    filters["document_type"] = [document_type]
                
                results = self.es_manager.hybrid_search(query.strip(), embedding, filters, search_level="articles")
                
                if not results or not results.get("articles"):
                    return f"No articles found for query: {query}"
                
                formatted_results = []
                articles = results["articles"][:5]  # Top 5 results
                
                for i, article in enumerate(articles):
                    try:
                        score = article.get('_score', 0)
                        doc_type = article.get('document_type', 'N/A')
                        chapter = article.get('chapter_number', 'N/A')
                        article_num = article.get('article_number', 'N/A')
                        title = article.get('title', 'N/A')
                        content = article.get('full_content', '')
                        concepts = article.get('key_concepts', [])
                        
                        # Ensure concepts is a list
                        if not isinstance(concepts, list):
                            concepts = []
                        
                        formatted_results.append(
                            f"Article {i+1} (Score: {score:.3f}):\n"
                            f"Type: {doc_type}\n"
                            f"Chapter: {chapter}\n"
                            f"Article: {article_num}\n"
                            f"Title: {title}\n"
                            f"Content: {content[:500]}...\n"
                            f"Key Concepts: {', '.join(concepts[:5])}\n"
                        )
                    except Exception as e:
                        logger.error(f"Error formatting article result {i}: {e}")
                        continue
                
                if not formatted_results:
                    return f"Found {len(articles)} articles but could not format results for query: {query}"
                
                return "\n---\n".join(formatted_results)
                
            except Exception as e:
                logger.error(f"Error in search_articles_tool: {e}")
                return f"Error searching articles: {str(e)}"
        
        return search_articles_tool
    
    def _create_search_chunks_tool(self):
        """Create tool for searching document chunks"""
        @tool
        async def search_chunks_tool(query: str, document_type: str = None) -> str:
            """
            Search for specific GDPR/UK GDPR document chunks/sections based on a query.
            
            Args:
                query: The search query describing what you're looking for
                document_type: Optional filter for "GDPR" or "UK_GDPR"
            
            Returns:
                String containing relevant chunks with context and scores
            """
            try:
                embedding = await self.openai_manager.create_embedding(query)
                filters = {}
                if document_type:
                    filters["document_type"] = [document_type]
                
                results = self.es_manager.hybrid_search(query, embedding, filters, search_level="chunks")
                
                if not results.get("chunks"):
                    return f"No chunks found for query: {query}"
                
                formatted_results = []
                for i, chunk in enumerate(results["chunks"][:7]):  # Top 7 results
                    formatted_results.append(
                        f"Chunk {i+1} (Score: {chunk.get('_score', 0):.3f}):\n"
                        f"Type: {chunk.get('document_type', 'N/A')}\n"
                        f"Chapter: {chunk.get('chapter_number', 'N/A')}\n"
                        f"Article: {chunk.get('article_number', 'N/A')}\n"
                        f"Title: {chunk.get('title', 'N/A')}\n"
                        f"Page: {chunk.get('page_number', 'N/A')}\n"
                        f"Content: {chunk.get('content', '')[:400]}...\n"
                        f"References: {len(chunk.get('supporting_references', []))}\n"
                    )
                
                return "\n---\n".join(formatted_results)
                
            except Exception as e:
                return f"Error searching chunks: {str(e)}"
        
        return search_chunks_tool
    
    def _create_search_both_tool(self):
        """Create tool for comprehensive search across articles and chunks"""
        @tool
        async def search_both_tool(query: str, document_type: str = None) -> str:
            """
            Comprehensive search across both full articles and chunks for maximum coverage.
            
            Args:
                query: The search query describing what you're looking for
                document_type: Optional filter for "GDPR" or "UK_GDPR"
            
            Returns:
                String containing both article and chunk results with analysis
            """
            try:
                embedding = await self.openai_manager.create_embedding(query)
                filters = {}
                if document_type:
                    filters["document_type"] = [document_type]
                
                results = self.es_manager.hybrid_search(query, embedding, filters, search_level="both")
                
                formatted_output = []
                
                # Articles section
                if results.get("articles"):
                    formatted_output.append("=== FULL ARTICLES ===")
                    for i, article in enumerate(results["articles"][:3]):
                        formatted_output.append(
                            f"Article {i+1}: {article.get('document_type')} - "
                            f"{article.get('title', 'N/A')} (Score: {article.get('_score', 0):.3f})\n"
                            f"Chapter {article.get('chapter_number')}, Article {article.get('article_number')}\n"
                            f"Content: {article.get('full_content', '')[:300]}...\n"
                        )
                
                # Chunks section
                if results.get("chunks"):
                    formatted_output.append("=== SPECIFIC SECTIONS ===")
                    for i, chunk in enumerate(results["chunks"][:5]):
                        formatted_output.append(
                            f"Section {i+1}: {chunk.get('document_type')} - "
                            f"{chunk.get('title', 'N/A')} (Score: {chunk.get('_score', 0):.3f})\n"
                            f"Chapter {chunk.get('chapter_number')}, Page {chunk.get('page_number', 'N/A')}\n"
                            f"Content: {chunk.get('content', '')[:250]}...\n"
                        )
                
                if not formatted_output:
                    return f"No results found for query: {query}"
                
                return "\n\n".join(formatted_output)
                
            except Exception as e:
                return f"Error in comprehensive search: {str(e)}"
        
        return search_both_tool
    
    def _create_get_relationships_tool(self):
        """Create tool for finding relationships between chunks"""
        @tool
        async def get_relationships_tool(chunk_id: str) -> str:
            """
            Find all relationships and cross-references for a specific chunk.
            
            Args:
                chunk_id: The ID of the chunk to find relationships for
            
            Returns:
                String describing all related chunks and their relationships
            """
            try:
                related_chunks = self.es_manager.get_related_chunks(chunk_id)
                
                if not related_chunks:
                    return f"No relationships found for chunk ID: {chunk_id}"
                
                formatted_results = []
                for i, related in enumerate(related_chunks):
                    formatted_results.append(
                        f"Related {i+1}:\n"
                        f"Type: {related.get('document_type', 'N/A')}\n"
                        f"Title: {related.get('title', 'N/A')}\n"
                        f"Chapter: {related.get('chapter_number', 'N/A')}\n"
                        f"Content: {related.get('content', '')[:200]}...\n"
                    )
                
                return f"Found {len(related_chunks)} related chunks:\n\n" + "\n---\n".join(formatted_results)
                
            except Exception as e:
                return f"Error finding relationships: {str(e)}"
        
        return get_relationships_tool
    
    def _create_compare_documents_tool(self):
        """Create tool for comparing GDPR vs UK GDPR on specific topics"""
        @tool
        async def compare_documents_tool(topic: str) -> str:
            """
            Compare how a specific topic is handled in GDPR vs UK GDPR.
            
            Args:
                topic: The legal topic or concept to compare between documents
            
            Returns:
                String with comparative analysis of the topic across both regulations
            """
            try:
                embedding = await self.openai_manager.create_embedding(topic)
                
                # Search GDPR
                gdpr_results = self.es_manager.hybrid_search(
                    topic, embedding, {"document_type": ["GDPR"]}, search_level="both"
                )
                
                # Search UK GDPR
                uk_gdpr_results = self.es_manager.hybrid_search(
                    topic, embedding, {"document_type": ["UK_GDPR"]}, search_level="both"
                )
                
                comparison = []
                
                comparison.append(f"=== COMPARISON: {topic.upper()} ===\n")
                
                # GDPR section
                comparison.append("--- GDPR (EU) ---")
                if gdpr_results.get("articles"):
                    best_gdpr = gdpr_results["articles"][0]
                    comparison.append(
                        f"Primary Article: {best_gdpr.get('title', 'N/A')}\n"
                        f"Chapter {best_gdpr.get('chapter_number')}, Article {best_gdpr.get('article_number')}\n"
                        f"Content: {best_gdpr.get('full_content', '')[:400]}...\n"
                    )
                elif gdpr_results.get("chunks"):
                    best_gdpr = gdpr_results["chunks"][0]
                    comparison.append(
                        f"Primary Section: {best_gdpr.get('title', 'N/A')}\n"
                        f"Content: {best_gdpr.get('content', '')[:400]}...\n"
                    )
                else:
                    comparison.append("No relevant GDPR content found.\n")
                
                # UK GDPR section
                comparison.append("--- UK GDPR ---")
                if uk_gdpr_results.get("articles"):
                    best_uk = uk_gdpr_results["articles"][0]
                    comparison.append(
                        f"Primary Article: {best_uk.get('title', 'N/A')}\n"
                        f"Chapter {best_uk.get('chapter_number')}, Article {best_uk.get('article_number')}\n"
                        f"Content: {best_uk.get('full_content', '')[:400]}...\n"
                    )
                elif uk_gdpr_results.get("chunks"):
                    best_uk = uk_gdpr_results["chunks"][0]
                    comparison.append(
                        f"Primary Section: {best_uk.get('title', 'N/A')}\n"
                        f"Content: {best_uk.get('content', '')[:400]}...\n"
                    )
                else:
                    comparison.append("No relevant UK GDPR content found.\n")
                
                return "\n\n".join(comparison)
                
            except Exception as e:
                return f"Error comparing documents: {str(e)}"
        
        return compare_documents_tool
    
    def _create_analyze_concept_tool(self):
        """Create tool for deep analysis of legal concepts"""
        @tool
        async def analyze_concept_tool(concept: str) -> str:
            """
            Perform deep analysis of a legal concept across all available documents.
            
            Args:
                concept: The legal concept or term to analyze in depth
            
            Returns:
                String with comprehensive analysis of the concept
            """
            try:
                embedding = await self.openai_manager.create_embedding(concept)
                
                # Comprehensive search
                results = self.es_manager.hybrid_search(
                    concept, embedding, search_level="both"
                )
                
                analysis = []
                analysis.append(f"=== DEEP ANALYSIS: {concept.upper()} ===\n")
                
                # Collect all relevant content
                relevant_articles = results.get("articles", [])[:3]
                relevant_chunks = results.get("chunks", [])[:8]
                
                if relevant_articles:
                    analysis.append("--- PRIMARY ARTICLES ---")
                    for i, article in enumerate(relevant_articles):
                        analysis.append(
                            f"{i+1}. {article.get('document_type')} - {article.get('title', 'N/A')}\n"
                            f"   Key Concepts: {', '.join(article.get('key_concepts', []))}\n"
                            f"   Content: {article.get('full_content', '')[:300]}...\n"
                        )
                
                if relevant_chunks:
                    analysis.append("--- SPECIFIC PROVISIONS ---")
                    for i, chunk in enumerate(relevant_chunks):
                        analysis.append(
                            f"{i+1}. {chunk.get('document_type')} - {chunk.get('title', 'N/A')}\n"
                            f"   Chapter {chunk.get('chapter_number')}, Page {chunk.get('page_number', 'N/A')}\n"
                            f"   Content: {chunk.get('content', '')[:250]}...\n"
                        )
                
                # Cross-references
                if relevant_chunks:
                    total_refs = sum(len(chunk.get('supporting_references', [])) for chunk in relevant_chunks)
                    if total_refs > 0:
                        analysis.append(f"--- CROSS-REFERENCES ---")
                        analysis.append(f"Found {total_refs} cross-references across {len(relevant_chunks)} sections.")
                
                if not relevant_articles and not relevant_chunks:
                    return f"No content found for concept: {concept}"
                
                return "\n\n".join(analysis)
                
            except Exception as e:
                return f"Error analyzing concept: {str(e)}"
        
        return analyze_concept_tool
    
    def create_react_agent(self) -> Any:
        """Create and configure the ReAct agent with proper error handling"""
        try:
            # Validate configuration
            if not Config.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY is required but not set")
            
            # Create a chat model interface for the ReAct agent
            try:
                chat_model = ChatOpenAI(
                    model=Config.O3_MINI_MODEL,
                    api_key=Config.OPENAI_API_KEY,
                    base_url=Config.OPENAI_BASE_URL,
                    temperature=0.1,
                    streaming=False,
                    timeout=60,
                    max_retries=3
                )
                
                # Test the model with a simple call
                test_response = chat_model.invoke("Test")
                logger.info("Chat model successfully initialized and tested")
                
            except Exception as e:
                logger.error(f"Error initializing chat model: {e}")
                raise ValueError(f"Failed to initialize chat model: {e}")
            
            # Validate tools
            if not self.tools or len(self.tools) == 0:
                raise ValueError("No tools available for ReAct agent")
            
            logger.info(f"Creating ReAct agent with {len(self.tools)} tools")
            
            # Create the ReAct agent with tools and error handling
            try:
                self.react_agent = create_react_agent(
                    model=chat_model,
                    tools=self.tools,
                    prompt=self.system_prompt
                )
                
                logger.info("ReAct agent created successfully with GDPR analysis tools")
                return self.react_agent
                
            except Exception as e:
                logger.error(f"Error creating ReAct agent: {e}")
                raise ValueError(f"Failed to create ReAct agent: {e}")
            
        except Exception as e:
            logger.error(f"Error in create_react_agent: {e}")
            raise
    
    async def query(self, user_query: str, thread_id: str = None) -> Dict[str, Any]:
        """Process a user query using the ReAct agent with comprehensive error handling"""
        try:
            if not user_query or not isinstance(user_query, str):
                return {
                    "query": user_query,
                    "answer": "Error: Invalid query provided",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            # Clean and validate query
            user_query = user_query.strip()
            if len(user_query) < 3:
                return {
                    "query": user_query,
                    "answer": "Error: Query too short, please provide a more detailed question",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            # Create or validate ReAct agent
            if not self.react_agent:
                try:
                    self.create_react_agent()
                except Exception as e:
                    return {
                        "query": user_query,
                        "answer": f"Error: Could not initialize ReAct agent - {str(e)}",
                        "thread_id": thread_id,
                        "messages": []
                    }
            
            # Configure the agent run
            if not thread_id:
                thread_id = f"react_query_{uuid.uuid4().hex[:8]}"
            
            config = {
                "configurable": {
                    "thread_id": thread_id,
                }
            }
            
            # Invoke the ReAct agent with timeout protection
            try:
                logger.info(f"Processing ReAct query: {user_query[:100]}...")
                
                response = self.react_agent.invoke(
                    {"messages": [{"role": "user", "content": user_query}]},
                    config=config
                )
                
                if not response:
                    return {
                        "query": user_query,
                        "answer": "Error: No response from ReAct agent",
                        "thread_id": thread_id,
                        "messages": []
                    }
                
            except Exception as e:
                logger.error(f"Error invoking ReAct agent: {e}")
                return {
                    "query": user_query,
                    "answer": f"Error during ReAct processing: {str(e)}",
                    "thread_id": thread_id,
                    "messages": []
                }
            
            # Extract the final answer safely
            messages = response.get("messages", [])
            final_answer = "No answer generated"
            
            try:
                # Look for the final AI message with content
                for message in reversed(messages):
                    if hasattr(message, 'content') and message.content:
                        content = str(message.content).strip()
                        # Skip tool calls and system messages
                        if (content and 
                            not content.startswith('{"') and 
                            not content.startswith('Error:') and
                            len(content) > 10):
                            final_answer = content
                            break
                
                # If no good final answer found, try to extract from any message
                if final_answer == "No answer generated" and messages:
                    for message in messages:
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content).strip()
                            if content and len(content) > 20:
                                final_answer = content
                                break
                
            except Exception as e:
                logger.error(f"Error extracting answer from messages: {e}")
                final_answer = f"Error extracting answer: {str(e)}"
            
            # Store the query in memory
            try:
                await self._store_query_memory(user_query, final_answer, thread_id)
            except Exception as e:
                logger.error(f"Error storing query memory: {e}")
            
            return {
                "query": user_query,
                "answer": final_answer,
                "thread_id": thread_id,
                "messages": messages
            }
            
        except Exception as e:
            logger.error(f"Unexpected error processing ReAct query: {e}")
            return {
                "query": user_query or "Unknown query",
                "answer": f"Unexpected error processing query: {str(e)}",
                "thread_id": thread_id,
                "messages": []
            }
    
    async def _store_query_memory(self, query: str, answer: str, thread_id: str):
        """Store query and response in long-term memory"""
        try:
            memory_content = {
                "query": query,
                "answer_length": len(answer),
                "thread_id": thread_id or "unknown",
                "timestamp": datetime.now().isoformat(),
                "agent_type": "ReAct"
            }
            
            memory = AgentMemory(
                agent_name=self.name,
                memory_type="episodic",
                content=memory_content,
                namespace=["react_queries", "gdpr_analysis"]
            )
            
            self.es_manager.index_memory(memory)
            
            await self.memory_store.aput(
                namespace=tuple(["react_queries", self.name]),
                key=f"query_{uuid.uuid4()}",
                value=memory_content
            )
            
        except Exception as e:
            logger.error(f"Error storing query memory: {e}")


# Tools for agents
@tool
async def search_similar_chunks(query: str, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for similar chunks using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    embedding = await openai_manager.create_embedding(query)
    
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    results = es_manager.hybrid_search(query, embedding, filters, search_level="chunks")
    return results.get("chunks", [])


@tool
async def search_full_articles(query: str, document_type: str = None, config: RunnableConfig = None) -> List[Dict]:
    """Search for full articles using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    embedding = await openai_manager.create_embedding(query)
    
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    results = es_manager.hybrid_search(query, embedding, filters, search_level="articles")
    return results.get("articles", [])


@tool
async def search_both_levels(query: str, document_type: str = None, config: RunnableConfig = None) -> Dict[str, List[Dict]]:
    """Search both full articles and chunks using hybrid search"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    openai_manager = config["configurable"]["openai_manager"]
    
    embedding = await openai_manager.create_embedding(query)
    
    filters = {}
    if document_type:
        filters["document_type"] = [document_type]
    
    results = es_manager.hybrid_search(query, embedding, filters, search_level="both")
    return results


@tool
async def get_chunk_relationships(chunk_id: str, config: RunnableConfig = None) -> List[Dict]:
    """Get all relationships for a specific chunk"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    related_chunks = es_manager.get_related_chunks(chunk_id)
    return related_chunks


@tool
async def store_agent_memory(agent_name: str, memory_type: str, content: Dict, 
                           namespace: List[str], config: RunnableConfig = None) -> bool:
    """Store information in agent's long-term memory"""
    es_manager = config["configurable"]["elasticsearch_manager"]
    memory_store = config["configurable"]["memory_store"]
    
    serializable_content = {}
    for key, value in content.items():
        if isinstance(value, list):
            if all(isinstance(item, str) for item in value):
                serializable_content[key] = ",".join(value) if value else ""
            else:
                serializable_content[key] = str(value)
        else:
            serializable_content[key] = value
    
    memory = AgentMemory(
        agent_name=agent_name,
        memory_type=memory_type,
        content=serializable_content,
        namespace=namespace
    )
    
    es_success = es_manager.index_memory(memory)
    
    try:
        await memory_store.aput(
            namespace=tuple(namespace + [agent_name]),
            key=memory.memory_id,
            value=serializable_content
        )
        return es_success
    except Exception as e:
        logger.error(f"Error storing in LangGraph memory: {e}")
        return es_success


@tool
async def react_query_gdpr(query: str, config: RunnableConfig = None) -> str:
    """Use ReAct agent for intelligent GDPR analysis and querying"""
    try:
        es_manager = config["configurable"]["elasticsearch_manager"]
        openai_manager = config["configurable"]["openai_manager"]
        memory_store = config["configurable"]["memory_store"]
        
        # Create ReAct agent
        react_agent = ReActQueryAgent(openai_manager, es_manager, memory_store)
        
        # Process the query
        result = await react_agent.query(query)
        
        return result["answer"]
        
    except Exception as e:
        logger.error(f"Error in ReAct GDPR query: {e}")
        return f"Error processing query with ReAct agent: {str(e)}"


# Main workflow nodes
async def document_processing_node(state: ProcessingState) -> ProcessingState:
    """Process all PDF documents from the data directory with comprehensive chunking and error handling"""
    logger.info("Starting enhanced document processing with recursive text splitting...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        
        processor = DocumentProcessingAgent(openai_manager, es_manager)
        
        all_articles = []
        all_chunks = []
        
        data_dir = "data"
        if not os.path.exists(data_dir):
            error_msg = f"Data directory not found: {data_dir}. Please create the directory and add PDF files."
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        pdf_pattern = os.path.join(data_dir, "*.pdf")
        pdf_files = glob.glob(pdf_pattern)
        
        if not pdf_files:
            error_msg = f"No PDF files found in {data_dir} directory. Please add PDF files to process."
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        logger.info(f"Found {len(pdf_files)} PDF files in {data_dir} directory:")
        for pdf_file in pdf_files:
            logger.info(f"  - {os.path.basename(pdf_file)}")
        
        # Process each PDF file with error handling
        successful_files = 0
        for i, pdf_file in enumerate(pdf_files):
            try:
                logger.info(f"Processing PDF {i+1}/{len(pdf_files)} with comprehensive chunking: {pdf_file}")
                
                articles, chunks = await processor.process_document(pdf_file)
                
                if articles or chunks:  # Only count if we got some results
                    all_articles.extend(articles)
                    all_chunks.extend(chunks)
                    successful_files += 1
                    
                    logger.info(f"Successfully processed {pdf_file}: {len(articles)} articles, {len(chunks)} comprehensive chunks")
                else:
                    logger.warning(f"No content extracted from {pdf_file}")
                
                # Small delay between files to avoid overwhelming the system
                if i < len(pdf_files) - 1:  # Don't delay after the last file
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"Failed to process {pdf_file}: {e}")
                # Continue processing other files even if one fails
                continue
        
        if successful_files == 0:
            error_msg = "No documents were successfully processed. Please check your PDF files and try again."
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        if successful_files < len(pdf_files):
            logger.warning(f"Only {successful_files}/{len(pdf_files)} files processed successfully")
        
        state["full_articles"] = all_articles
        state["documents"] = all_chunks
        state["processing_stage"] = "cross_referencing"
        
        logger.info(f"Enhanced document processing completed: {len(all_articles)} total articles, {len(all_chunks)} total comprehensive chunks")
        
        # Add success message
        success_message = AIMessage(
            content=f"Document processing completed successfully. Processed {successful_files} PDF files, "
                   f"created {len(all_articles)} articles and {len(all_chunks)} chunks."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in document processing node: {e}")
        # Add error message to state
        error_message = AIMessage(content=f"Document processing failed: {str(e)}")
        state["messages"].append(error_message)
        state["processing_stage"] = "error"
        raise


async def cross_reference_node(state: ProcessingState) -> ProcessingState:
    """Find cross-references between documents with enhanced analysis and error handling"""
    logger.info("Finding cross-references with enhanced relationship analysis...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        memory_store = InMemoryStore()
        
        cross_ref_agent = CrossReferenceAgent(openai_manager, es_manager, memory_store)
        
        all_links = []
        documents = state.get("documents", [])
        
        if not documents:
            logger.warning("No documents found for cross-reference analysis")
            state["cross_links"] = []
            state["processing_stage"] = "linking"
            return state
        
        logger.info(f"Analyzing cross-references for {len(documents)} chunks...")
        
        successful_analyses = 0
        for i, chunk in enumerate(documents):
            try:
                logger.info(f"Analyzing cross-references for chunk {i+1}/{len(documents)}: {chunk.title[:50]}...")
                
                links = await cross_ref_agent.find_cross_references(chunk, documents)
                
                if links:
                    all_links.extend(links)
                    logger.info(f"Found {len(links)} cross-document links for chunk {chunk.chunk_index}")
                
                successful_analyses += 1
                
                # Progress logging every 10 chunks
                if (i + 1) % 10 == 0:
                    logger.info(f"Progress: {i+1}/{len(documents)} chunks analyzed, {len(all_links)} links found so far")
                
                # Small delay to avoid rate limiting
                if i < len(documents) - 1:  # Don't delay after the last chunk
                    await asyncio.sleep(0.2)
                    
            except Exception as e:
                logger.error(f"Error analyzing chunk {i+1} ({chunk.chunk_id}): {e}")
                # Continue with next chunk
                continue
        
        state["cross_links"] = all_links
        state["processing_stage"] = "linking"
        
        logger.info(f"Cross-reference analysis completed: {len(all_links)} total links found from {successful_analyses}/{len(documents)} successful analyses")
        
        # Add success message
        success_message = AIMessage(
            content=f"Cross-reference analysis completed. Found {len(all_links)} cross-document links "
                   f"from {successful_analyses} chunk analyses."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in cross-reference node: {e}")
        # Add error message to state
        error_message = AIMessage(content=f"Cross-reference analysis failed: {str(e)}")
        state["messages"].append(error_message)
        state["processing_stage"] = "error"
        # Don't raise, continue with empty links
        state["cross_links"] = []
        state["processing_stage"] = "linking"
        return state


async def linking_node(state: ProcessingState) -> ProcessingState:
    """Update chunks with supporting references and comprehensive error handling"""
    logger.info("Updating chunk references with comprehensive linking...")
    
    try:
        openai_manager = OpenAIManager()
        es_manager = ElasticsearchManager()
        memory_store = InMemoryStore()
        
        linking_agent = LinkingAgent(openai_manager, es_manager, memory_store)
        
        documents = state.get("documents", [])
        cross_links = state.get("cross_links", [])
        
        if not documents:
            logger.warning("No documents found for linking")
            state["processing_stage"] = "completed"
            return state
        
        logger.info(f"Updating references for {len(documents)} chunks using {len(cross_links)} cross-links...")
        
        updated_chunks = []
        successful_updates = 0
        
        for i, chunk in enumerate(documents):
            try:
                logger.info(f"Updating references for chunk {i+1}/{len(documents)}")
                
                # Get links for this chunk
                chunk_links = [
                    link for link in cross_links 
                    if link.source_chunk_id == chunk.chunk_id or link.target_chunk_id == chunk.chunk_id
                ]
                
                updated_chunk = await linking_agent.update_chunk_references(chunk, chunk_links)
                updated_chunks.append(updated_chunk)
                successful_updates += 1
                
                # Progress logging every 20 chunks
                if (i + 1) % 20 == 0:
                    logger.info(f"Progress: {i+1}/{len(documents)} chunks updated")
                
                # Small delay to avoid overwhelming the system
                if i < len(documents) - 1 and i % 50 == 0:  # Delay every 50 chunks
                    await asyncio.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"Error updating chunk {i+1} ({chunk.chunk_id}): {e}")
                # Use original chunk if update fails
                updated_chunks.append(chunk)
                continue
        
        state["documents"] = updated_chunks
        state["processing_stage"] = "completed"
        
        logger.info(f"Comprehensive linking completed: {successful_updates}/{len(documents)} chunks successfully updated")
        
        # Add success message
        success_message = AIMessage(
            content=f"Linking completed successfully. Updated {successful_updates} chunks with cross-references."
        )
        state["messages"].append(success_message)
        
        return state
        
    except Exception as e:
        logger.error(f"Error in linking node: {e}")
        # Add error message to state
        error_message = AIMessage(content=f"Linking process failed: {str(e)}")
        state["messages"].append(error_message)
        state["processing_stage"] = "completed_with_errors"
        return state


async def react_query_node(state: ProcessingState) -> ProcessingState:
    """ReAct agent node for intelligent querying and analysis"""
    logger.info("ReAct agent available for intelligent GDPR querying...")
    
    # The ReAct agent is now available and can be accessed through the react_query_gdpr tool
    # This node simply marks that ReAct capabilities are ready
    state["processing_stage"] = "react_ready"
    
    # Add a message indicating ReAct agent is ready
    react_message = AIMessage(content="ReAct agent initialized and ready for intelligent GDPR analysis queries.")
    state["messages"].append(react_message)
    
    return state


# Create the multi-agent workflow
def create_gdpr_processing_workflow():
    """Create the enhanced LangGraph workflow for comprehensive GDPR document processing"""
    
    checkpointer = MemorySaver()
    memory_store = InMemoryStore()
    
    tools = [search_similar_chunks, search_full_articles, search_both_levels, 
             get_chunk_relationships, store_agent_memory, react_query_gdpr]
    tool_node = ToolNode(tools)
    
    workflow = StateGraph(ProcessingState)
    
    workflow.add_node("document_processing", document_processing_node)
    workflow.add_node("cross_reference", cross_reference_node)
    workflow.add_node("linking", linking_node)
    workflow.add_node("tools", tool_node)
    
    workflow.add_edge(START, "document_processing")
    workflow.add_edge("document_processing", "cross_reference")
    workflow.add_edge("cross_reference", "linking")
    workflow.add_edge("linking", END)
    
    app = workflow.compile(
        checkpointer=checkpointer,
        store=memory_store
    )
    
    return app


async def run_comprehensive_analysis(final_state: ProcessingState, es_manager: ElasticsearchManager) -> None:
    """Run comprehensive analysis with enhanced metrics and ReAct agent demonstration"""
    try:
        sample_queries = [
            "data processing lawful basis",
            "consent withdrawal GDPR", 
            "data protection officer requirements",
            "cross-border data transfer",
            "data subject rights",
            "automated decision making",
            "privacy by design",
            "breach notification",
            "controller processor relationship",
            "data retention periods"
        ]
        
        print(f"\n=== Enhanced Performance Benchmark ===")
        
        openai_manager = OpenAIManager()
        total_time = 0
        successful_queries = 0
        
        for query in sample_queries[:5]:  # Test first 5 queries
            import time
            start_time = time.time()
            
            embedding = await openai_manager.create_embedding(query)
            search_results = es_manager.hybrid_search(
                query=query,
                embedding=embedding,
                search_level="both"
            )
            
            end_time = time.time()
            query_time = (end_time - start_time) * 1000
            total_time += query_time
            successful_queries += 1
            
            print(f"Query '{query}' took {query_time:.2f}ms")
        
        avg_latency = total_time / successful_queries if successful_queries > 0 else 0
        print(f"Average Query Latency: {avg_latency:.2f}ms")
        print(f"Successful Queries: {successful_queries}/{len(sample_queries[:5])}")
        
        # Enhanced search demonstration
        if final_state['documents']:
            print(f"\n=== Enhanced Hybrid Search Demo ===")
            sample_query = "data processing consent requirements"
            
            sample_embedding = await openai_manager.create_embedding(sample_query)
            
            search_results = es_manager.hybrid_search(
                query=sample_query,
                embedding=sample_embedding,
                search_level="both"
            )
            
            print(f"Search query: '{sample_query}'")
            print(f"Articles found: {len(search_results.get('articles', []))}")
            print(f"Chunks found: {len(search_results.get('chunks', []))}")
            
            if search_results.get('chunks'):
                top_chunk = search_results['chunks'][0]
                print(f"Top chunk: {top_chunk.get('title', 'N/A')} (Score: {top_chunk.get('_score', 0):.3f})")
                print(f"Document type: {top_chunk.get('document_type', 'N/A')}")
                print(f"Page: {top_chunk.get('page_number', 'N/A')}")
                print(f"Chunk index: {top_chunk.get('chunk_index', 'N/A')}")
        
        # ReAct Agent Demonstration
        print(f"\n=== ReAct Agent Demonstration ===")
        memory_store = InMemoryStore()
        react_agent = ReActQueryAgent(openai_manager, es_manager, memory_store)
        
        # Create the ReAct agent
        try:
            react_agent.create_react_agent()
            print("ReAct agent successfully created with o3-mini model")
            
            # Demo queries for ReAct agent
            demo_queries = [
                "What are the key differences between GDPR and UK GDPR regarding data transfer?",
                "Explain the requirements for obtaining valid consent under GDPR",
                "What are the obligations of a Data Protection Officer?"
            ]
            
            print(f"\nReAct Agent Tools Available:")
            print(f"  - search_articles_tool: Search full GDPR/UK GDPR articles")
            print(f"  - search_chunks_tool: Search specific document sections")
            print(f"  - search_both_tool: Comprehensive search across all content")
            print(f"  - get_relationships_tool: Find cross-document relationships")
            print(f"  - compare_documents_tool: Compare GDPR vs UK GDPR on topics")
            print(f"  - analyze_concept_tool: Deep analysis of legal concepts")
            
            print(f"\nExample ReAct queries you can now ask:")
            for i, query in enumerate(demo_queries, 1):
                print(f"  {i}. {query}")
            
            # Demonstrate one ReAct query if documents are loaded
            if final_state['documents']:
                print(f"\n--- ReAct Agent Demo Query ---")
                demo_query = "What does GDPR say about data processing lawful basis?"
                print(f"Query: {demo_query}")
                
                # Run a simplified version for demo (without full ReAct to avoid complexity in demo)
                print("ReAct agent would:")
                print("1. THINK: I need to search for information about lawful basis for data processing")
                print("2. ACT: Use search_articles_tool to find relevant GDPR articles")
                print("3. OBSERVE: Analyze the results and potentially search for more specific chunks")
                print("4. THINK: Compile comprehensive answer with specific article references")
                print("5. RESPOND: Provide detailed analysis with citations")
                
                print("\nReAct agent is ready for intelligent GDPR analysis!")
            
        except Exception as e:
            print(f"ReAct agent creation demo error: {e}")
            print("ReAct agent framework is implemented and ready for use")
                
    except Exception as e:
        logger.error(f"Error in comprehensive analysis: {e}")


# Main execution function
async def main():
    """Main execution function with comprehensive error handling"""
    
    # Validate configuration at startup
    try:
        logger.info("Validating configuration...")
        
        if not Config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        if not Config.ES_PASSWORD:
            raise ValueError("ES_PASSWORD environment variable is required")
        
        # Validate model availability
        try:
            test_client = openai.OpenAI(
                api_key=Config.OPENAI_API_KEY,
                base_url=Config.OPENAI_BASE_URL
            )
            # Test with a simple completion
            test_response = test_client.chat.completions.create(
                model=Config.O3_MINI_MODEL,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            logger.info(f"Successfully validated OpenAI API access with model: {Config.O3_MINI_MODEL}")
            
        except Exception as e:
            logger.error(f"OpenAI API validation failed: {e}")
            raise ValueError(f"Cannot access OpenAI API with model {Config.O3_MINI_MODEL}: {e}")
        
        logger.info(f"Using OpenAI base URL: {Config.OPENAI_BASE_URL}")
        logger.info(f"Using Elasticsearch: {Config.ES_HOST}:{Config.ES_PORT}")
        logger.info(f"Text splitting configuration: chunk_size={Config.CHUNK_SIZE}, overlap={Config.CHUNK_OVERLAP}")
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        raise
    
    # Validate data directory and files
    try:
        data_dir = "data"
        if not os.path.exists(data_dir):
            logger.error(f"Data directory '{data_dir}' not found")
            raise FileNotFoundError(f"Please create a '{data_dir}' directory and add your PDF files")
        
        pdf_files = glob.glob(os.path.join(data_dir, "*.pdf"))
        if not pdf_files:
            logger.error(f"No PDF files found in '{data_dir}' directory")
            raise FileNotFoundError(f"Please add PDF files to the '{data_dir}' directory")
        
        logger.info(f"Found {len(pdf_files)} PDF files to process with comprehensive chunking")
        
        # Validate PDF files are readable
        readable_files = []
        for pdf_file in pdf_files:
            try:
                with pymupdf.open(pdf_file) as doc:
                    if doc.page_count > 0:
                        readable_files.append(pdf_file)
                        logger.info(f"   {os.path.basename(pdf_file)} ({doc.page_count} pages)")
                    else:
                        logger.warning(f"   {os.path.basename(pdf_file)} (no pages)")
            except Exception as e:
                logger.warning(f"   {os.path.basename(pdf_file)} (error: {e})")
        
        if not readable_files:
            raise ValueError("No readable PDF files found")
        
        logger.info(f"Successfully validated {len(readable_files)} readable PDF files")
        
    except Exception as e:
        logger.error(f"Data validation failed: {e}")
        raise
    
    # Test Elasticsearch connection
    try:
        logger.info("Testing Elasticsearch connection...")
        test_es = ElasticsearchManager()
        test_info = test_es.client.info()
        logger.info(f"Successfully connected to Elasticsearch: {test_info.get('version', {}).get('number', 'unknown')}")
    except Exception as e:
        logger.error(f"Elasticsearch connection failed: {e}")
        raise ValueError(f"Cannot connect to Elasticsearch: {e}")
    
    # Create and run the workflow
    try:
        logger.info("Creating GDPR processing workflow...")
        app = create_gdpr_processing_workflow()
        
        initial_state = ProcessingState(
            messages=[HumanMessage(content="Process GDPR and UK GDPR documents with comprehensive chunking and ReAct agent")],
            full_articles=[],
            documents=[],
            current_chunk=None,
            cross_links=[],
            processing_stage="initializing",
            agent_memories=[],
            elasticsearch_client=None
        )
        
        config = {
            "configurable": {
                "thread_id": f"gdpr_comprehensive_react_processing_{int(time.time())}",
                "elasticsearch_manager": ElasticsearchManager(),
                "openai_manager": OpenAIManager(),
                "memory_store": InMemoryStore()
            }
        }
        
        logger.info("Starting comprehensive GDPR document processing workflow with ReAct agent...")
        
        final_state = await app.ainvoke(initial_state, config)
        
        # Validate results
        if not final_state:
            raise ValueError("Workflow returned empty state")
        
        articles_count = len(final_state.get('full_articles', []))
        chunks_count = len(final_state.get('documents', []))
        links_count = len(final_state.get('cross_links', []))
        
        if articles_count == 0 and chunks_count == 0:
            raise ValueError("No content was processed successfully")
        
        logger.info(f"Comprehensive processing with ReAct agent completed!")
        logger.info(f"Processed {articles_count} full articles")
        logger.info(f"Created {chunks_count} comprehensive chunks")
        logger.info(f"Established {links_count} cross-document links")
        logger.info(f"ReAct agent status: {final_state.get('processing_stage', 'unknown')}")
        
        es_manager = config["configurable"]["elasticsearch_manager"]
        
        print("\n" + "="*70)
        print("COMPREHENSIVE GDPR PROCESSING WITH REACT AGENT SUMMARY")
        print("="*70)
        print(f"PDF files processed: {len(readable_files)}")
        print(f"Full articles processed: {articles_count}")
        print(f"Total comprehensive chunks: {chunks_count}")
        print(f"Cross-document links created: {links_count}")
        print(f"Processing stage: {final_state.get('processing_stage', 'unknown')}")
        
        react_status = " Ready for intelligent querying" if final_state.get('processing_stage') == 'react_ready' else " Not ready"
        print(f"ReAct agent: {react_status}")
        
        # Enhanced document type analysis with error handling
        try:
            doc_types = {}
            chunk_counts = {}
            
            for article in final_state.get('full_articles', []):
                doc_type = getattr(article, 'document_type', 'Unknown')
                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                
            for chunk in final_state.get('documents', []):
                doc_type = getattr(chunk, 'document_type', 'Unknown')
                chunk_counts[doc_type] = chunk_counts.get(doc_type, 0) + 1
            
            if doc_types:
                print(f"\nDocument Types Processed:")
                for doc_type in doc_types:
                    print(f"  {doc_type}: {doc_types[doc_type]} articles, {chunk_counts.get(doc_type, 0)} chunks")
        except Exception as e:
            logger.error(f"Error in document type analysis: {e}")
        
        # Text splitting metrics with error handling
        try:
            page_covered_chunks = sum(1 for chunk in final_state.get('documents', []) 
                                    if hasattr(chunk, 'page_number') and chunk.page_number is not None)
            print(f"\nText Splitting Metrics:")
            print(f"  Chunk size: {Config.CHUNK_SIZE} characters")
            print(f"  Chunk overlap: {Config.CHUNK_OVERLAP} characters")
            print(f"  Chunks with page tracking: {page_covered_chunks}/{chunks_count}")
        except Exception as e:
            logger.error(f"Error in text splitting metrics: {e}")
        
        # HNSW and optimization details
        print(f"\n" + "="*50)
        print("ENHANCED HNSW & QUANTIZATION OPTIMIZATIONS")
        print("="*50)
        print(f"Vector Index Type: {Config.VECTOR_INDEX_TYPE}")
        print(f"HNSW Parameters: M={Config.HNSW_M}, EF_Construction={Config.HNSW_EF_CONSTRUCTION}")
        print(f"Embedding Dimensions: {Config.EMBEDDING_DIMENSIONS}")
        print(f"Preload Cache: {'Enabled' if Config.ENABLE_PRELOAD else 'Disabled'}")
        
        # Index statistics with error handling
        try:
            index_stats = es_manager.get_index_stats()
            for index_name, stats in index_stats.items():
                print(f"\n{index_name.upper()} Index:")
                print(f"  Documents: {stats.get('documents', 0):,}")
                print(f"  Size: {stats.get('size_bytes', 0) / (1024**2):.1f} MB")
                print(f"  Segments: {stats.get('segments', 0)}")
                
                vector_stats = stats.get('vector_size_estimate', {})
                if vector_stats.get('estimated_memory', 'Unknown') != 'Unknown':
                    print(f"  Vector Memory: {vector_stats.get('estimated_memory')}")
                    print(f"  Quantization Savings: {vector_stats.get('quantization_savings')}")
                    print(f"  HNSW Overhead: {vector_stats.get('hnsw_overhead')}")
        except Exception as e:
            logger.error(f"Error getting index statistics: {e}")
        
        # Index optimization
        try:
            print(f"\n" + "="*40)
            print("OPTIMIZING INDICES FOR PRODUCTION")
            print("="*40)
            es_manager.optimize_indices()
            print("Index optimization completed")
        except Exception as e:
            logger.error(f"Error optimizing indices: {e}")
        
        # Enhanced examples with error handling
        try:
            if final_state.get('full_articles'):
                article = final_state['full_articles'][0]
                print(f"\nExample full article: {getattr(article, 'title', 'N/A')[:100]}...")
                print(f"Article chunks: {len(getattr(article, 'chunk_ids', []))}")
                print(f"Key concepts: {len(getattr(article, 'key_concepts', []))}")
        except Exception as e:
            logger.error(f"Error displaying article example: {e}")
        
        try:
            if final_state.get('documents'):
                chunk = final_state['documents'][0]
                print(f"\nExample comprehensive chunk: {getattr(chunk, 'title', 'N/A')[:50]}...")
                print(f"Chunk index: {getattr(chunk, 'chunk_index', 'N/A')}")
                print(f"Page number: {getattr(chunk, 'page_number', 'N/A')}")
                print(f"Content length: {len(getattr(chunk, 'content', ''))}")
                print(f"Parent article: {getattr(chunk, 'parent_article_id', 'N/A')}")
                print(f"Supporting references: {len(getattr(chunk, 'supporting_references', []))}")
        except Exception as e:
            logger.error(f"Error displaying chunk example: {e}")
        
        # Run comprehensive analysis
        try:
            await run_comprehensive_analysis(final_state, es_manager)
        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
        
        print(f"\n" + "="*70)
        print("PROCESSING COMPLETED SUCCESSFULLY!")
        print("="*70)
        
        return final_state
        
    except Exception as e:
        logger.error(f"Error in comprehensive workflow execution: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
