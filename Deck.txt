#!/usr/bin/env python3
"""
Next-Generation GDPR Search Engine with Advanced RAG & Multi-Agent Architecture - PYDANTIC V2 FIXED
===============================================================================

A state-of-the-art command-line search engine featuring:
- Advanced RAG techniques (CRAG, Self-RAG, Adaptive RAG)
- Enhanced GraphRAG with FalkorDB knowledge graphs 
- Agentic Elasticsearch with hybrid search
- Multi-agent architecture with LangGraph coordination
- Real-time evaluation and correction mechanisms
- Custom OpenAI embeddings integration

Author: Enhanced with 2025 SOTA techniques
Date: June 2025
Framework: LangChain v0.3+ + LangGraph v0.4+ + FalkorDB + Elasticsearch
Python: 3.10+ (Cross-platform compatible)

FIXED: Pydantic v2 migration + JSON parsing + None iteration errors
"""

import asyncio
import json
import logging
import os
import sys
from pathlib import Path
import uuid
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Literal, Union, Sequence, Annotated
from dataclasses import dataclass, field, asdict
import argparse
import traceback
from enum import Enum
import pickle
import hashlib
from abc import ABC, abstractmethod
import concurrent.futures
import threading
import re

# Environment variable loading
from dotenv import load_dotenv
load_dotenv()

# Core libraries
import pandas as pd
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.markdown import Markdown
import click

# Enterprise web search (no API keys required)
import requests
from bs4 import BeautifulSoup
import feedparser
import wikipediaapi
from urllib.parse import urljoin, urlparse, quote
import ssl

# LangChain v0.3+ imports with Pydantic v2
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool, BaseTool
from pydantic import BaseModel, Field  # FIXED: Pydantic v2
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory

# Enhanced LangChain imports for advanced RAG
from langchain_community.chains.graph_qa.falkordb import FalkorDBQAChain
from langchain_community.graphs import FalkorDBGraph
from langchain_elasticsearch import ElasticsearchStore
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.ensemble import EnsembleRetriever

# LangGraph v0.4+ imports with latest API
from langgraph.graph import StateGraph, START, END, MessagesState, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import create_react_agent, ToolNode
from langgraph.types import Command
from typing_extensions import TypedDict

# Database clients
from elasticsearch import Elasticsearch
from falkordb import FalkorDB

# Async support
import aiofiles
import httpx

# OpenAI for direct API access
import openai

# ============================================================================
# ENHANCED JSON PARSING WITH ROBUST ERROR HANDLING - FIXED
# ============================================================================

def safe_json_parse(text: str, fallback: dict = None) -> dict:
    """Safely parse JSON with multiple fallback strategies"""
    if fallback is None:
        fallback = {}
    
    if not text or not isinstance(text, str):
        return fallback
    
    # Strategy 1: Try direct JSON parsing
    try:
        return json.loads(text)
    except:
        pass
    
    # Strategy 2: Extract JSON from markdown code blocks
    json_patterns = [
        r'```json\s*(\{.*?\})\s*```',
        r'```\s*(\{.*?\})\s*```',
        r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})',
    ]
    
    for pattern in json_patterns:
        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
        for match in matches:
            try:
                return json.loads(match.strip())
            except:
                continue
    
    # Strategy 3: Find the largest JSON-like structure
    try:
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        if start_idx >= 0 and end_idx > start_idx:
            json_text = text[start_idx:end_idx + 1]
            return json.loads(json_text)
    except:
        pass
    
    # Strategy 4: Try to extract key-value pairs manually
    try:
        result = {}
        # Look for common patterns like "key": "value"
        kv_pattern = r'"([^"]+)":\s*"([^"]+)"'
        matches = re.findall(kv_pattern, text)
        for key, value in matches:
            result[key] = value
        
        # Look for boolean/number patterns
        bool_pattern = r'"([^"]+)":\s*(true|false|null|\d+(?:\.\d+)?)'
        bool_matches = re.findall(bool_pattern, text, re.IGNORECASE)
        for key, value in bool_matches:
            if value.lower() == 'true':
                result[key] = True
            elif value.lower() == 'false':
                result[key] = False
            elif value.lower() == 'null':
                result[key] = None
            else:
                try:
                    result[key] = float(value) if '.' in value else int(value)
                except:
                    result[key] = value
        
        if result:
            return result
    except:
        pass
    
    logging.warning(f"Failed to parse JSON from: {text[:200]}...")
    return fallback

def extract_json_from_response(response_text: str, required_keys: List[str] = None) -> dict:
    """Extract JSON from LLM response with validation and fallbacks"""
    if required_keys is None:
        required_keys = []
    
    # Clean the response text
    cleaned_text = response_text.strip()
    
    # Parse JSON
    result = safe_json_parse(cleaned_text)
    
    # Validate required keys and provide defaults
    defaults = {
        "intent": "factual",
        "complexity": "moderate",
        "domain": "general",
        "recency": "static",
        "reasoning": "direct",
        "confidence": 0.7,
        "key_concepts": [],
        "reasoning_chain": "Analysis completed",
        "action": "correct",
        "needs_revision": False,
        "issues": [],
        "strengths": ["Response generated"],
        "suggestions": [],
        "decision": "complete",
        "overall_confidence": 0.7,
        "dimension_scores": {},
        "weaknesses": [],
        "improvement_suggestions": [],
        "query_type": "factual",
        "retrieval_strategy": "single_vector"
    }
    
    # Ensure all required keys exist with defaults
    for key in required_keys:
        if key not in result:
            result[key] = defaults.get(key, "unknown")
    
    # Ensure common keys exist
    for key, default_value in defaults.items():
        if key not in result:
            result[key] = default_value
    
    return result

def run_async_in_thread(coro):
    """Helper function to run async code in a thread without event loop conflicts"""
    def run_in_new_loop():
        # Create a new event loop for this thread
        new_loop = asyncio.new_event_loop()
        try:
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
    
    # Run in a separate thread to avoid event loop conflicts
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(run_in_new_loop)
        return future.result()

def safe_get(data: Any, key: str, default: Any = None) -> Any:
    """Safely get value from dict/object with default"""
    if data is None:
        return default
    if isinstance(data, dict):
        return data.get(key, default)
    return getattr(data, key, default)

def safe_iterate(data: Any, default: List = None) -> List:
    """Safely iterate over data with fallback to empty list"""
    if default is None:
        default = []
    
    if data is None:
        return default
    
    if isinstance(data, (list, tuple)):
        return list(data)
    
    if isinstance(data, dict):
        return list(data.values())
    
    return default

# ============================================================================
# PYDANTIC V2 MODELS
# ============================================================================

class QueryAnalysis(BaseModel):
    """Pydantic v2 model for query analysis"""
    intent: str = Field(default="factual", description="Query intent classification")
    complexity: str = Field(default="moderate", description="Query complexity level")
    domain: str = Field(default="general", description="GDPR domain")
    recency: str = Field(default="static", description="Information recency needs")
    reasoning: str = Field(default="direct", description="Reasoning requirements")
    confidence: float = Field(default=0.7, ge=0.0, le=1.0, description="Confidence score")
    key_concepts: List[str] = Field(default_factory=list, description="Key concepts identified")
    reasoning_chain: str = Field(default="Analysis completed", description="Reasoning process")

class RetrievalEvaluation(BaseModel):
    """Pydantic v2 model for retrieval evaluation"""
    confidence: float = Field(default=0.7, ge=0.0, le=1.0, description="Confidence in retrieval quality")
    action: str = Field(default="correct", description="Recommended action")
    reasoning: str = Field(default="Evaluation completed", description="Evaluation reasoning")

class ResponseCritique(BaseModel):
    """Pydantic v2 model for response critique"""
    needs_revision: bool = Field(default=False, description="Whether response needs revision")
    confidence: float = Field(default=0.7, ge=0.0, le=1.0, description="Confidence in response quality")
    issues: List[str] = Field(default_factory=list, description="Identified issues")
    strengths: List[str] = Field(default_factory=list, description="Response strengths")
    suggestions: List[str] = Field(default_factory=list, description="Improvement suggestions")

# ============================================================================
# CUSTOM EMBEDDINGS IMPLEMENTATION
# ============================================================================

class CustomOpenAIEmbeddings:
    """Custom OpenAI embeddings using direct API access"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-large", dimensions: int = 3072):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.dimensions = dimensions
    
    async def embed_query(self, text: str) -> List[float]:
        """Generate embedding for a single query"""
        try:
            response = await self.client.embeddings.create(
                input=text,
                model=self.model,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logging.error(f"❌ OpenAI embedding error: {e}")
            # Return zero vector as fallback
            return [0.0] * self.dimensions
    
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple documents"""
        try:
            response = await self.client.embeddings.create(
                input=texts,
                model=self.model,
                dimensions=self.dimensions
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logging.error(f"❌ OpenAI embedding error: {e}")
            # Return zero vectors as fallback
            return [[0.0] * self.dimensions for _ in texts]

# ============================================================================
# ENTERPRISE WEB SEARCH (NO API KEYS REQUIRED)
# ============================================================================

class EnterpriseWebSearch:
    """Enterprise web search without external API keys - Windows compatible"""
    
    def __init__(self, config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': getattr(config, 'USER_AGENT', 'GDPR-Search-Engine/1.0 (Enterprise)'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # Configure corporate proxy with authentication
        http_proxy = getattr(config, 'HTTP_PROXY', '')
        https_proxy = getattr(config, 'HTTPS_PROXY', '')
        
        if http_proxy or https_proxy:
            proxies = {}
            if http_proxy:
                proxies['http'] = http_proxy
            if https_proxy:
                proxies['https'] = https_proxy
            
            self.session.proxies.update(proxies)
            logging.info("Corporate proxy configured")
        
        # Initialize Wikipedia API
        enable_wikipedia = getattr(config, 'ENABLE_WIKIPEDIA_SEARCH', True)
        if enable_wikipedia:
            try:
                self.wiki = wikipediaapi.Wikipedia(
                    language='en',
                    user_agent=getattr(config, 'USER_AGENT', 'GDPR-Search-Engine/1.0 (Enterprise)')
                )
            except Exception as e:
                logging.warning(f"Wikipedia API initialization failed: {e}")
                self.wiki = None
        else:
            self.wiki = None
        
        # GDPR-specific RSS feeds (no API keys required)
        self.gdpr_rss_feeds = [
            "https://edpb.europa.eu/news/feed",  # European Data Protection Board
            "https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/feed/",  # UK ICO
            "https://www.cnil.fr/en/rss.xml",  # French CNIL
            "https://www.bfdi.bund.de/SiteGlobals/Functions/RSSFeed/RSSNewsletter_EN.xml",  # German DPA
        ]
        
        # GDPR authority websites for direct scraping
        self.gdpr_authorities = {
            "EDPB": "https://edpb.europa.eu",
            "UK_ICO": "https://ico.org.uk", 
            "EU_Commission": "https://commission.europa.eu",
            "CNIL": "https://www.cnil.fr/en",
            "BfDI": "https://www.bfdi.bund.de/EN"
        }
    
    async def search(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """Comprehensive web search using multiple sources"""
        all_results = []
        
        try:
            # 1. DuckDuckGo Instant Answer (no API key)
            web_search_enabled = getattr(self.config, 'WEB_SEARCH_ENABLED', True)
            if web_search_enabled:
                ddg_results = await self._duckduckgo_search(query, max_results=3)
                all_results.extend(ddg_results)
            
            # 2. Wikipedia search
            enable_wikipedia = getattr(self.config, 'ENABLE_WIKIPEDIA_SEARCH', True)
            if enable_wikipedia and self.wiki:
                wiki_results = await self._wikipedia_search(query, max_results=2)
                all_results.extend(wiki_results)
            
            # 3. RSS feeds from GDPR authorities
            enable_rss = getattr(self.config, 'ENABLE_RSS_FEEDS', True)
            if enable_rss:
                rss_results = await self._rss_search(query, max_results=3)
                all_results.extend(rss_results)
            
            # 4. Direct scraping of GDPR authority sites
            enable_scraping = getattr(self.config, 'ENABLE_WEB_SCRAPING', True)
            if enable_scraping:
                authority_results = await self._authority_search(query, max_results=2)
                all_results.extend(authority_results)
            
            # Sort by relevance score and return top results
            all_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            return all_results[:max_results]
            
        except Exception as e:
            logging.error(f"Enterprise web search error: {e}")
            return []
    
    async def _duckduckgo_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search using DuckDuckGo Instant Answer API (no key required)"""
        results = []
        
        try:
            # DuckDuckGo Instant Answer API
            ddg_url = "https://api.duckduckgo.com/"
            params = {
                'q': f"GDPR {query}",
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            timeout = getattr(self.config, 'REQUEST_TIMEOUT', 10)
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.session.get(ddg_url, params=params, timeout=timeout)
            )
            response.raise_for_status()
            
            data = response.json()
            
            # Process instant answer
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', 'DuckDuckGo Result'),
                    'content': data['AbstractText'],
                    'url': data.get('AbstractURL', ''),
                    'source': 'DuckDuckGo',
                    'relevance_score': 0.8,
                    'type': 'instant_answer'
                })
            
            # Process related topics
            for topic in safe_iterate(data.get('RelatedTopics', []))[:max_results-1]:
                if isinstance(topic, dict) and topic.get('Text'):
                    results.append({
                        'title': topic.get('Text', '')[:100] + '...',
                        'content': topic.get('Text', ''),
                        'url': topic.get('FirstURL', ''),
                        'source': 'DuckDuckGo',
                        'relevance_score': 0.7,
                        'type': 'related_topic'
                    })
            
        except Exception as e:
            logging.error(f"DuckDuckGo search error: {e}")
        
        return results
    
    async def _wikipedia_search(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """Search Wikipedia for GDPR-related information"""
        results = []
        
        if not self.wiki:
            return results
        
        try:
            # Search Wikipedia
            search_query = f"GDPR {query}"
            search_results = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.wiki.search(search_query, results=max_results * 2)
            )
            
            for title in safe_iterate(search_results)[:max_results]:
                try:
                    page = await asyncio.get_event_loop().run_in_executor(
                        None, lambda: self.wiki.page(title)
                    )
                    
                    if page.exists():
                        # Calculate relevance based on GDPR keyword density
                        content = page.summary[:1000]
                        gdpr_keywords = ['gdpr', 'data protection', 'privacy', 'personal data', 'controller', 'processor']
                        relevance_score = sum(content.lower().count(keyword) for keyword in gdpr_keywords) / 10
                        relevance_score = min(relevance_score, 1.0)
                        
                        if relevance_score > 0.1:  # Only include relevant results
                            results.append({
                                'title': page.title,
                                'content': content,
                                'url': page.fullurl,
                                'source': 'Wikipedia',
                                'relevance_score': relevance_score,
                                'type': 'encyclopedia'
                            })
                            
                except Exception as e:
                    logging.warning(f"Error processing Wikipedia page {title}: {e}")
                    continue
                    
        except Exception as e:
            logging.error(f"Wikipedia search error: {e}")
        
        return results
    
    async def _rss_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search RSS feeds from GDPR authorities"""
        results = []
        query_lower = query.lower()
        
        for feed_url in self.gdpr_rss_feeds:
            try:
                # Parse RSS feed
                feed_data = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: feedparser.parse(feed_url)
                )
                
                for entry in safe_iterate(feed_data.entries)[:10]:  # Check last 10 entries per feed
                    title = entry.get('title', '')
                    summary = entry.get('summary', entry.get('description', ''))
                    content = f"{title} {summary}".lower()
                    
                    # Calculate relevance
                    query_words = query_lower.split()
                    relevance_score = sum(1 for word in query_words if word in content) / len(query_words) if query_words else 0
                    
                    if relevance_score > 0.3:  # Only include relevant news
                        results.append({
                            'title': title,
                            'content': summary[:500],
                            'url': entry.get('link', ''),
                            'source': f"RSS: {urlparse(feed_url).netloc}",
                            'relevance_score': relevance_score,
                            'type': 'news',
                            'published': entry.get('published', '')
                        })
                        
                        if len(results) >= max_results:
                            break
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                logging.warning(f"Error parsing RSS feed {feed_url}: {e}")
                continue
        
        return results
    
    async def _authority_search(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """Search GDPR authority websites directly"""
        results = []
        
        for authority_name, base_url in self.gdpr_authorities.items():
            try:
                # Search specific authority site
                search_results = await self._scrape_authority_site(authority_name, base_url, query)
                results.extend(search_results)
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                logging.warning(f"Error searching {authority_name}: {e}")
                continue
        
        return results[:max_results]
    
    async def _scrape_authority_site(self, authority_name: str, base_url: str, query: str) -> List[Dict[str, Any]]:
        """Scrape specific GDPR authority website"""
        results = []
        
        try:
            # Common search patterns for different authorities
            search_patterns = {
                "EDPB": "/search?q={query}",
                "UK_ICO": "/search?q={query}",
                "EU_Commission": "/search/?queryText={query}",
                "CNIL": "/en/search?search_api_fulltext={query}",
                "BfDI": "/EN/search?query={query}"
            }
            
            search_path = search_patterns.get(authority_name, "/search?q={query}")
            search_url = urljoin(base_url, search_path.format(query=query.replace(' ', '+')))
            
            timeout = getattr(self.config, 'REQUEST_TIMEOUT', 10)
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.session.get(search_url, timeout=timeout)
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract search results (common patterns)
            result_selectors = [
                '.search-result',
                '.result-item', 
                '.search-item',
                'article',
                '.news-item'
            ]
            
            for selector in result_selectors:
                elements = soup.select(selector)[:3]  # Max 3 per authority
                
                for element in elements:
                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'a'])
                    content_elem = element.find(['p', '.summary', '.excerpt'])
                    link_elem = element.find('a', href=True)
                    
                    if title_elem and content_elem:
                        title = title_elem.get_text(strip=True)
                        content = content_elem.get_text(strip=True)[:300]
                        url = urljoin(base_url, link_elem['href']) if link_elem else base_url
                        
                        # Calculate relevance
                        query_words = query.lower().split()
                        text_content = f"{title} {content}".lower()
                        relevance_score = sum(1 for word in query_words if word in text_content) / len(query_words) if query_words else 0
                        
                        if relevance_score > 0.2:
                            results.append({
                                'title': title,
                                'content': content,
                                'url': url,
                                'source': f"{authority_name} Official",
                                'relevance_score': relevance_score + 0.2,  # Boost official sources
                                'type': 'official_document'
                            })
                
                if results:  # If we found results with this selector, break
                    break
                    
        except Exception as e:
            logging.warning(f"Error scraping {authority_name}: {e}")
        
        return results

# ============================================================================
# ENHANCED CONFIGURATION WITH 2025 STANDARDS
# ============================================================================

@dataclass
class AdvancedSearchEngineConfig:
    """Enhanced configuration with 2025 SOTA features"""
    
    # OpenAI Configuration - Enterprise Standard
    OPENAI_API_KEY: str = field(default_factory=lambda: os.getenv("OPENAI_API_KEY", ""))
    OPENAI_BASE_URL: str = field(default_factory=lambda: os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"))
    REASONING_MODEL: str = "gpt-4o"  # Stable model
    REASONING_EFFORT: str = field(default_factory=lambda: os.getenv("REASONING_EFFORT", "medium"))
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS: int = 3072
    
    # Advanced RAG Configuration
    ENABLE_CRAG: bool = field(default_factory=lambda: os.getenv("ENABLE_CRAG", "true").lower() == "true")
    ENABLE_SELF_RAG: bool = field(default_factory=lambda: os.getenv("ENABLE_SELF_RAG", "true").lower() == "true")
    ENABLE_ADAPTIVE_RAG: bool = field(default_factory=lambda: os.getenv("ENABLE_ADAPTIVE_RAG", "true").lower() == "true")
    ENABLE_MULTIMODAL: bool = field(default_factory=lambda: os.getenv("ENABLE_MULTIMODAL", "false").lower() == "true")
    
    # Retrieval Enhancement Settings
    RETRIEVAL_CONFIDENCE_THRESHOLD: float = 0.7
    MAX_RETRIEVAL_ITERATIONS: int = 3
    ENABLE_QUERY_EXPANSION: bool = True
    ENABLE_HIERARCHICAL_CHUNKING: bool = True
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    
    # FalkorDB Enhanced Configuration
    FALKORDB_HOST: str = field(default_factory=lambda: os.getenv("FALKOR_HOST", "localhost"))
    FALKORDB_PORT: int = field(default_factory=lambda: int(os.getenv("FALKOR_PORT", "6379")))
    FALKORDB_PASSWORD: str = field(default_factory=lambda: os.getenv("FALKOR_PASSWORD", ""))
    FALKORDB_GRAPH_NAME: str = "gdpr_metamodel_graph"
    FALKORDB_USE_TLS: bool = field(default_factory=lambda: os.getenv("FALKOR_TLS", "false").lower() == "true")
    
    # Elasticsearch Enhanced Configuration
    ELASTICSEARCH_HOST: str = field(default_factory=lambda: os.getenv("ES_HOST", "localhost"))
    ELASTICSEARCH_PORT: int = field(default_factory=lambda: int(os.getenv("ES_PORT", "9200")))
    ELASTICSEARCH_USERNAME: str = field(default_factory=lambda: os.getenv("ES_USERNAME", "elastic"))
    ELASTICSEARCH_PASSWORD: str = field(default_factory=lambda: os.getenv("ES_PASSWORD", ""))
    ELASTICSEARCH_INDEX: str = "gdpr_metamodel_kb"
    ELASTICSEARCH_CA_CERTS: str = field(default_factory=lambda: os.getenv("ES_CA_CERTS", ""))
    ELASTICSEARCH_VERIFY_CERTS: bool = field(default_factory=lambda: os.getenv("ES_VERIFY_CERTS", "false").lower() == "true")
    ENABLE_HYBRID_SEARCH: bool = True
    
    # Web Search Configuration
    WEB_SEARCH_ENABLED: bool = field(default_factory=lambda: os.getenv("WEB_SEARCH_ENABLED", "true").lower() == "true")
    ENABLE_WIKIPEDIA_SEARCH: bool = field(default_factory=lambda: os.getenv("ENABLE_WIKIPEDIA_SEARCH", "true").lower() == "true")
    ENABLE_RSS_FEEDS: bool = field(default_factory=lambda: os.getenv("ENABLE_RSS_FEEDS", "true").lower() == "true")
    ENABLE_WEB_SCRAPING: bool = field(default_factory=lambda: os.getenv("ENABLE_WEB_SCRAPING", "true").lower() == "true")
    USER_AGENT: str = "GDPR-Search-Engine/1.0 (Enterprise)"
    REQUEST_TIMEOUT: int = 10
    
    # Corporate Proxy Configuration (Windows format)
    HTTP_PROXY: str = field(default_factory=lambda: os.getenv("HTTP_PROXY", ""))
    HTTPS_PROXY: str = field(default_factory=lambda: os.getenv("HTTPS_PROXY", ""))
    NO_PROXY: str = field(default_factory=lambda: os.getenv("NO_PROXY", "localhost,127.0.0.1"))
    
    # Agent System Configuration
    MAX_AGENT_ITERATIONS: int = 5
    AGENT_REFLECTION_ENABLED: bool = True
    ENABLE_AGENT_MEMORY: bool = True
    ENABLE_COORDINATED_RETRIEVAL: bool = True
    
    # Evaluation and Feedback
    ENABLE_REAL_TIME_EVALUATION: bool = True
    CONFIDENCE_THRESHOLD: float = 0.8
    HALLUCINATION_DETECTION: bool = True
    FEEDBACK_LEARNING_RATE: float = 0.1
    
    # Performance and Scalability
    MAX_SEARCH_RESULTS: int = 15
    MAX_CONVERSATION_HISTORY: int = 30
    ENABLE_CACHING: bool = True
    CACHE_TTL: int = 3600  # 1 hour
    
    # Enterprise Features
    ENABLE_AUDIT_LOGGING: bool = True
    ENABLE_METRICS_COLLECTION: bool = True
    PRIVACY_MODE: bool = field(default_factory=lambda: os.getenv("PRIVACY_MODE", "false").lower() == "true")
    
    # Storage paths (cross-platform)
    SESSION_STORAGE_PATH: Path = field(default_factory=lambda: Path("./sessions"))
    FEEDBACK_STORAGE_PATH: Path = field(default_factory=lambda: Path("./feedback"))
    CACHE_STORAGE_PATH: Path = field(default_factory=lambda: Path("./cache"))
    LOGS_PATH: Path = field(default_factory=lambda: Path("./logs"))
    METRICS_PATH: Path = field(default_factory=lambda: Path("./metrics"))
    
    def validate(self):
        """Enhanced validation with comprehensive checks"""
        if not self.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY is required. Please set it in your .env file.")
        
        # Validate thresholds
        if not 0.0 <= self.RETRIEVAL_CONFIDENCE_THRESHOLD <= 1.0:
            raise ValueError("RETRIEVAL_CONFIDENCE_THRESHOLD must be between 0.0 and 1.0")
        
        if not 0.0 <= self.CONFIDENCE_THRESHOLD <= 1.0:
            raise ValueError("CONFIDENCE_THRESHOLD must be between 0.0 and 1.0")
        
        # Create directories (cross-platform)
        try:
            for path in [self.SESSION_STORAGE_PATH, self.FEEDBACK_STORAGE_PATH, 
                        self.CACHE_STORAGE_PATH, self.LOGS_PATH, self.METRICS_PATH]:
                path.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logging.error(f"Failed to create directories: {e}")
            raise

# ============================================================================
# ADVANCED RAG TECHNIQUES IMPLEMENTATION - FIXED
# ============================================================================

class RetrievalEvaluator:
    """Lightweight retrieval evaluator for CRAG implementation - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            api_key=config.OPENAI_API_KEY,
            temperature=0.1  # Lower temperature for more consistent JSON output
        )
        
    async def evaluate_retrieval_quality(self, query: str, documents: List[Document]) -> Dict[str, Any]:
        """Evaluate the quality and relevance of retrieved documents"""
        
        if not documents or len(documents) == 0:
            return {
                "confidence": 0.0,
                "action": "incorrect",
                "reasoning": "No documents retrieved"
            }
        
        # Create evaluation prompt with strict JSON formatting
        doc_contents = "\n\n".join([f"Document {i+1}: {doc.page_content[:300]}..." 
                                   for i, doc in enumerate(safe_iterate(documents)[:3])])
        
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a retrieval quality evaluator for GDPR compliance queries.

Evaluate the relevance and accuracy of retrieved documents for the given query.

IMPORTANT: Respond with ONLY valid JSON in this exact format:

{
  "confidence": 0.85,
  "action": "correct",
  "reasoning": "Documents are highly relevant to the query"
}

Classification options for action:
- "correct": Documents are highly relevant and accurate
- "ambiguous": Documents are partially relevant but need clarification  
- "incorrect": Documents are irrelevant or inaccurate

Confidence should be a number between 0.0 and 1.0."""),
            ("human", f"""Query: {query}

Retrieved Documents:
{doc_contents}

Evaluate the retrieval quality and respond with valid JSON only:""")
        ])
        
        try:
            response = await self.llm.ainvoke(evaluation_prompt.format_messages())
            
            # Parse with robust error handling
            required_keys = ["confidence", "action", "reasoning"]
            evaluation = extract_json_from_response(response.content.strip(), required_keys)
            
            # Validate and constrain values
            evaluation["confidence"] = max(0.0, min(1.0, float(evaluation.get("confidence", 0.7))))
            
            if evaluation.get("action") not in ["correct", "ambiguous", "incorrect"]:
                evaluation["action"] = "ambiguous"
            
            return evaluation
            
        except Exception as e:
            logging.error(f"Retrieval evaluation error: {e}")
            return {
                "confidence": 0.5,
                "action": "ambiguous",
                "reasoning": f"Evaluation failed: {str(e)}"
            }

class CorrectiveRAG:
    """Corrective RAG (CRAG) implementation with self-correction - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig, web_search_tool, vector_search_tool):
        self.config = config
        self.evaluator = RetrievalEvaluator(config)
        self.web_search_tool = web_search_tool
        self.vector_search_tool = vector_search_tool
        self.llm = ChatOpenAI(model=config.REASONING_MODEL, api_key=config.OPENAI_API_KEY, temperature=0.1)
        
    async def corrective_retrieve(self, query: str, initial_documents: List[Document]) -> Dict[str, Any]:
        """Perform corrective retrieval with quality evaluation"""
        
        # Ensure we have a list of documents
        documents = safe_iterate(initial_documents)
        
        # Step 1: Evaluate initial retrieval
        evaluation = await self.evaluator.evaluate_retrieval_quality(query, documents)
        
        corrected_documents = documents.copy()
        correction_steps = []
        
        # Step 2: Apply corrective actions based on evaluation
        action = safe_get(evaluation, "action", "correct")
        
        if action == "correct":
            # Documents are good, use as-is
            correction_steps.append("Initial retrieval was sufficient")
            
        elif action == "ambiguous":
            # Try to get additional context
            correction_steps.append("Retrieving additional context for ambiguous results")
            
            try:
                # Perform additional vector search with expanded query
                expanded_query = await self._expand_query(query)
                additional_results = self.vector_search_tool.invoke({"query": expanded_query})
                
                if safe_get(additional_results, "success") and safe_get(additional_results, "results"):
                    additional_docs = []
                    for result in safe_iterate(additional_results.get("results", []))[:3]:
                        content = str(safe_get(result, "data", ""))
                        if content and content.strip():
                            additional_docs.append(Document(page_content=content))
                    
                    corrected_documents.extend(additional_docs)
                    correction_steps.append(f"Added {len(additional_docs)} additional documents")
            except Exception as e:
                logging.warning(f"Additional vector search failed: {e}")
                correction_steps.append("Additional search failed, using original documents")
                
        elif action == "incorrect":
            # Try web search for current information
            correction_steps.append("Initial retrieval was poor, trying web search")
            
            try:
                web_results = self.web_search_tool.invoke({"query": query})
                
                if safe_get(web_results, "success") and safe_get(web_results, "results"):
                    web_docs = []
                    for result in safe_iterate(web_results.get("results", []))[:3]:
                        content = safe_get(result, "content", "")
                        if content and content.strip():
                            web_docs.append(Document(
                                page_content=content,
                                metadata={
                                    "source": safe_get(result, "url", ""), 
                                    "title": safe_get(result, "title", "")
                                }
                            ))
                    
                    if web_docs:
                        corrected_documents = web_docs  # Replace with web results
                        correction_steps.append(f"Replaced with {len(web_docs)} web search results")
                    else:
                        correction_steps.append("Web search returned no valid results")
                else:
                    correction_steps.append("Web search failed, using original documents")
            except Exception as e:
                logging.warning(f"Web search failed: {e}")
                correction_steps.append("Web search failed, using original documents")
        
        # Step 3: Re-evaluate if we made corrections
        if len(correction_steps) > 1:
            final_evaluation = await self.evaluator.evaluate_retrieval_quality(query, corrected_documents)
        else:
            final_evaluation = evaluation
        
        return {
            "documents": corrected_documents,
            "initial_evaluation": evaluation,
            "final_evaluation": final_evaluation,
            "correction_steps": correction_steps,
            "confidence": safe_get(final_evaluation, "confidence", 0.7)
        }
    
    async def _expand_query(self, query: str) -> str:
        """Expand query for better retrieval"""
        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a query expansion expert. Expand the given GDPR query with related terms and synonyms. Return only the expanded query without any additional formatting."),
            ("human", f"Original query: {query}\n\nExpanded query:")
        ])
        
        try:
            response = await self.llm.ainvoke(expansion_prompt.format_messages())
            expanded = response.content.strip()
            return expanded if expanded else query
        except Exception as e:
            logging.error(f"Query expansion error: {e}")
            return query

class SelfReflectiveRAG:
    """Self-RAG implementation with reflection tokens - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            api_key=config.OPENAI_API_KEY,
            temperature=0.1
        )
        
    async def generate_with_reflection(self, query: str, documents: List[Document], 
                                     conversation_context: str = "") -> Dict[str, Any]:
        """Generate response with self-reflection and critique"""
        
        # Ensure we have valid documents
        docs = safe_iterate(documents)
        
        # Step 1: Initial generation
        initial_response = await self._generate_initial_response(query, docs, conversation_context)
        
        # Step 2: Self-critique
        critique = await self._self_critique(query, initial_response, docs)
        
        # Step 3: Reflection and potential revision
        needs_revision = safe_get(critique, "needs_revision", False)
        
        if needs_revision:
            revised_response = await self._revise_response(query, initial_response, critique, docs)
            final_response = revised_response
            reflection_steps = ["Generated initial response", "Self-critique identified issues", "Revised response"]
        else:
            final_response = initial_response
            reflection_steps = ["Generated initial response", "Self-critique confirmed quality"]
        
        return {
            "response": final_response,
            "initial_response": initial_response,
            "critique": critique,
            "reflection_steps": reflection_steps,
            "confidence": safe_get(critique, "confidence", 0.7)
        }
    
    async def _generate_initial_response(self, query: str, documents: List[Document], 
                                       conversation_context: str) -> str:
        """Generate initial response from documents"""
        
        docs = safe_iterate(documents)
        doc_context = "\n\n".join([f"Document {i+1}: {safe_get(doc, 'page_content', '')}" 
                                  for i, doc in enumerate(docs[:5]) if safe_get(doc, 'page_content', '')])
        
        if not doc_context:
            doc_context = "No relevant documents available."
        
        generation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a GDPR compliance expert. Generate a comprehensive response based on the provided documents.

Key requirements:
- Be accurate and cite specific GDPR articles when relevant
- Provide practical guidance
- Acknowledge uncertainty when information is incomplete
- Use clear, professional language
- If no relevant documents are available, state this clearly"""),
            ("human", f"""Conversation Context:
{conversation_context}

Question: {query}

Available Documents:
{doc_context}

Response:""")
        ])
        
        try:
            response = await self.llm.ainvoke(generation_prompt.format_messages())
            return response.content.strip() if response.content else "Unable to generate response."
        except Exception as e:
            logging.error(f"Initial generation error: {e}")
            return "I apologize, but I encountered an error while generating the response."
    
    async def _self_critique(self, query: str, response: str, documents: List[Document]) -> Dict[str, Any]:
        """Perform self-critique on the generated response"""
        
        critique_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a response quality critic for GDPR compliance. Evaluate the response quality.

IMPORTANT: Respond with ONLY valid JSON in this exact format:

{
  "needs_revision": false,
  "confidence": 0.85,
  "issues": ["issue1", "issue2"],
  "strengths": ["strength1", "strength2"],
  "suggestions": ["suggestion1", "suggestion2"]
}

Evaluation criteria:
1. Accuracy: Is the information factually correct?
2. Completeness: Does it fully answer the question?
3. Relevance: Is it directly relevant to the query?
4. Clarity: Is it clear and well-structured?
5. Citation: Are GDPR articles properly referenced?"""),
            ("human", f"""Query: {query}

Response to evaluate:
{response}

Provide your critique as valid JSON only:""")
        ])
        
        try:
            critique_response = await self.llm.ainvoke(critique_prompt.format_messages())
            
            required_keys = ["needs_revision", "confidence", "issues", "strengths", "suggestions"]
            critique = extract_json_from_response(critique_response.content.strip(), required_keys)
            
            # Validate types and values
            critique["needs_revision"] = bool(critique.get("needs_revision", False))
            critique["confidence"] = max(0.0, min(1.0, float(critique.get("confidence", 0.7))))
            critique["issues"] = safe_iterate(critique.get("issues", []))
            critique["strengths"] = safe_iterate(critique.get("strengths", ["Response generated"]))
            critique["suggestions"] = safe_iterate(critique.get("suggestions", []))
            
            return critique
            
        except Exception as e:
            logging.error(f"Self-critique error: {e}")
            return {
                "needs_revision": False,
                "confidence": 0.7,
                "issues": [],
                "strengths": ["Response generated successfully"],
                "suggestions": []
            }
    
    async def _revise_response(self, query: str, initial_response: str, 
                             critique: Dict, documents: List[Document]) -> str:
        """Revise response based on critique"""
        
        docs = safe_iterate(documents)
        doc_context = "\n\n".join([f"Document {i+1}: {safe_get(doc, 'page_content', '')}" 
                                  for i, doc in enumerate(docs[:5]) if safe_get(doc, 'page_content', '')])
        
        issues = safe_iterate(critique.get('issues', []))
        suggestions = safe_iterate(critique.get('suggestions', []))
        
        revision_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are revising a GDPR compliance response based on critique feedback.

Address the identified issues while maintaining the strengths. Focus on:
- Fixing factual inaccuracies
- Adding missing information
- Improving clarity and structure
- Ensuring proper GDPR citations"""),
            ("human", f"""Original Query: {query}

Initial Response:
{initial_response}

Critique Issues: {issues}
Suggestions: {suggestions}

Available Documents:
{doc_context}

Revised Response:""")
        ])
        
        try:
            response = await self.llm.ainvoke(revision_prompt.format_messages())
            return response.content.strip() if response.content else initial_response
        except Exception as e:
            logging.error(f"Response revision error: {e}")
            return initial_response

class AdaptiveRAG:
    """Adaptive RAG with query complexity routing - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.llm = ChatOpenAI(model=config.REASONING_MODEL, api_key=config.OPENAI_API_KEY, temperature=0.1)
        
    async def analyze_query_complexity(self, query: str) -> Dict[str, Any]:
        """Analyze query complexity and determine retrieval strategy"""
        
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a query complexity analyzer for GDPR compliance queries.

IMPORTANT: Respond with ONLY valid JSON in this exact format:

{
  "complexity": "moderate",
  "query_type": "factual", 
  "retrieval_strategy": "single_vector",
  "reasoning": "explanation of the analysis"
}

Complexity levels: simple, moderate, complex
Query types: factual, procedural, analytical, hybrid
Retrieval strategies: single_vector, multi_vector, graph_traversal, web_enhanced, comprehensive"""),
            ("human", f"Analyze this GDPR query: {query}\n\nRespond with valid JSON only:")
        ])
        
        try:
            response = await self.llm.ainvoke(analysis_prompt.format_messages())
            
            required_keys = ["complexity", "query_type", "retrieval_strategy", "reasoning"]
            analysis = extract_json_from_response(response.content.strip(), required_keys)
            
            # Validate values
            valid_complexity = ["simple", "moderate", "complex"]
            if analysis.get("complexity") not in valid_complexity:
                analysis["complexity"] = "moderate"
            
            valid_types = ["factual", "procedural", "analytical", "hybrid"]
            if analysis.get("query_type") not in valid_types:
                analysis["query_type"] = "factual"
            
            valid_strategies = ["single_vector", "multi_vector", "graph_traversal", "web_enhanced", "comprehensive"]
            if analysis.get("retrieval_strategy") not in valid_strategies:
                analysis["retrieval_strategy"] = "single_vector"
            
            return analysis
            
        except Exception as e:
            logging.error(f"Query complexity analysis error: {e}")
            return {
                "complexity": "moderate",
                "query_type": "factual",
                "retrieval_strategy": "single_vector", 
                "reasoning": f"Analysis failed: {str(e)}"
            }

# ============================================================================
# ENHANCED DATABASE MANAGERS WITH ADVANCED FEATURES - FIXED
# ============================================================================

class AdvancedFalkorDBManager:
    """Enhanced FalkorDB manager with QA chains and openCypher queries - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.client = None
        self.graph = None
        self.qa_chain = None
        self._setup_client()
        self._setup_qa_chain()
    
    def _setup_client(self):
        """Setup FalkorDB client with enhanced configuration"""
        try:
            connection_params = {
                'host': self.config.FALKORDB_HOST,
                'port': self.config.FALKORDB_PORT,
                'decode_responses': True
            }
            
            if self.config.FALKORDB_PASSWORD:
                connection_params['password'] = self.config.FALKORDB_PASSWORD
            
            if self.config.FALKORDB_USE_TLS:
                connection_params['ssl'] = True
                connection_params['ssl_cert_reqs'] = 'required'
            
            self.client = FalkorDB(**connection_params)
            self.graph = self.client.select_graph(self.config.FALKORDB_GRAPH_NAME)
            
            # Initialize FalkorDB Graph for LangChain
            self.langchain_graph = FalkorDBGraph(
                database=self.config.FALKORDB_GRAPH_NAME,
                host=self.config.FALKORDB_HOST,
                port=self.config.FALKORDB_PORT,
                password=self.config.FALKORDB_PASSWORD,
                ssl=self.config.FALKORDB_USE_TLS
            )
            
            logging.info(f"✅ Connected to FalkorDB: {self.config.FALKORDB_GRAPH_NAME}")
            
        except Exception as e:
            logging.error(f"❌ FalkorDB connection error: {e}")
            raise
    
    def _setup_qa_chain(self):
        """Setup enhanced FalkorDB QA chain"""
        try:
            llm = ChatOpenAI(
                model=self.config.REASONING_MODEL,
                api_key=self.config.OPENAI_API_KEY
            )
            
            # Create FalkorDB QA Chain with custom prompts
            self.qa_chain = FalkorDBQAChain.from_llm(
                llm=llm,
                graph=self.langchain_graph,
                verbose=True,
                return_intermediate_steps=True
            )
            
            logging.info("✅ FalkorDB QA Chain initialized")
            
        except Exception as e:
            logging.error(f"❌ FalkorDB QA Chain setup error: {e}")
            self.qa_chain = None
    
    async def enhanced_graph_search(self, query: str, search_type: str = "adaptive") -> List[Dict[str, Any]]:
        """Enhanced graph search with multiple strategies"""
        try:
            if search_type == "qa_chain":
                return await self._qa_chain_search(query)
            elif search_type == "multi_hop":
                return await self._multi_hop_search(query)
            elif search_type == "pattern_matching":
                return await self._pattern_matching_search(query)
            elif search_type == "adaptive":
                return await self._adaptive_graph_search(query)
            else:
                return await self._semantic_graph_search(query)
                
        except Exception as e:
            logging.error(f"❌ Enhanced graph search error: {e}")
            return []
    
    async def _qa_chain_search(self, query: str) -> List[Dict[str, Any]]:
        """Use FalkorDB QA Chain for natural language queries"""
        if not self.qa_chain:
            return []
        
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.qa_chain.invoke({"query": query})
            )
            
            return [{
                "type": "qa_chain_result",
                "data": {
                    "query": query,
                    "result": safe_get(result, "result", ""),
                    "intermediate_steps": safe_iterate(result.get("intermediate_steps", []))
                },
                "score": 0.9,
                "source": "FalkorDB QA Chain"
            }]
            
        except Exception as e:
            logging.error(f"❌ QA Chain search error: {e}")
            return []
    
    async def _multi_hop_search(self, query: str) -> List[Dict[str, Any]]:
        """Multi-hop reasoning through graph relationships using openCypher"""
        try:
            # openCypher query for multi-hop traversal
            multi_hop_query = """
            MATCH path = (start)-[*1..3]-(end)
            WHERE start.label CONTAINS $query OR start.definition CONTAINS $query
            WITH path, start, end, length(path) as path_length
            ORDER BY path_length ASC
            RETURN start, end, path, path_length
            LIMIT 10
            """
            
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.graph.query(multi_hop_query, {'query': query.lower()})
            )
            
            results = []
            for record in safe_iterate(result.result_set):
                if len(record) >= 4:
                    results.append({
                        "type": "multi_hop_path",
                        "data": {
                            "start_node": record[0],
                            "end_node": record[1], 
                            "path": record[2],
                            "path_length": record[3]
                        },
                        "score": 0.85,
                        "reasoning_type": "multi_hop"
                    })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Multi-hop search error: {e}")
            return []
    
    async def _pattern_matching_search(self, query: str) -> List[Dict[str, Any]]:
        """Pattern matching search using openCypher patterns"""
        try:
            # Pattern matching for GDPR concepts
            pattern_queries = [
                # Find concepts and their broader/narrower relationships
                """
                MATCH (c:Concept)-[:skos_broader]->(broader)
                WHERE c.label CONTAINS $query OR c.definition CONTAINS $query
                RETURN c, broader, 'broader' as relationship_type
                LIMIT 5
                """,
                
                # Find articles referencing concepts
                """
                MATCH (a:Article)-[:references]->(c:Concept)
                WHERE c.label CONTAINS $query OR a.title CONTAINS $query
                RETURN a, c, 'references' as relationship_type
                LIMIT 5
                """,
                
                # Find related concepts through shared properties
                """
                MATCH (c1:Concept)-[:skos_related]-(c2:Concept)
                WHERE c1.label CONTAINS $query OR c1.definition CONTAINS $query
                RETURN c1, c2, 'related' as relationship_type
                LIMIT 5
                """
            ]
            
            results = []
            for i, cypher_query in enumerate(pattern_queries):
                try:
                    result = await asyncio.get_event_loop().run_in_executor(
                        None, lambda q=cypher_query: self.graph.query(q, {'query': query.lower()})
                    )
                    
                    for record in safe_iterate(result.result_set):
                        if len(record) >= 2:
                            results.append({
                                "type": f"pattern_match_{i}",
                                "data": {
                                    "node1": record[0],
                                    "node2": record[1] if len(record) > 1 else None,
                                    "relationship": record[2] if len(record) > 2 else "unknown"
                                },
                                "score": 0.8,
                                "source": "FalkorDB Pattern"
                            })
                        
                except Exception as pattern_error:
                    logging.warning(f"Pattern query {i} failed: {pattern_error}")
                    continue
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Pattern matching search error: {e}")
            return []
    
    async def _adaptive_graph_search(self, query: str) -> List[Dict[str, Any]]:
        """Adaptive search combining multiple approaches"""
        all_results = []
        
        # Try QA Chain first for natural language understanding
        try:
            qa_results = await self._qa_chain_search(query)
            all_results.extend(safe_iterate(qa_results))
        except Exception as e:
            logging.warning(f"QA chain search failed: {e}")
        
        # Add semantic search
        try:
            semantic_results = await self._semantic_graph_search(query)
            all_results.extend(safe_iterate(semantic_results))
        except Exception as e:
            logging.warning(f"Semantic search failed: {e}")
        
        # Add multi-hop if query seems complex
        if len(query.split()) > 3:
            try:
                multi_hop_results = await self._multi_hop_search(query)
                all_results.extend(safe_iterate(multi_hop_results))
            except Exception as e:
                logging.warning(f"Multi-hop search failed: {e}")
        
        # Add pattern matching for relationship queries
        if any(word in query.lower() for word in ['related', 'connected', 'similar', 'relationship']):
            try:
                pattern_results = await self._pattern_matching_search(query)
                all_results.extend(safe_iterate(pattern_results))
            except Exception as e:
                logging.warning(f"Pattern matching failed: {e}")
        
        # Sort by score and remove duplicates
        unique_results = []
        seen_content = set()
        
        for result in sorted(all_results, key=lambda x: x.get('score', 0), reverse=True):
            content_hash = hashlib.md5(str(result.get('data', '')).encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        return unique_results[:self.config.MAX_SEARCH_RESULTS]
    
    async def _semantic_graph_search(self, query: str) -> List[Dict[str, Any]]:
        """Enhanced semantic search with openCypher"""
        results = []
        try:
            # Enhanced semantic query with multiple node types
            semantic_query = """
            MATCH (n)
            WHERE n.label CONTAINS $query 
               OR n.definition CONTAINS $query
               OR n.description CONTAINS $query
               OR n.content CONTAINS $query
            OPTIONAL MATCH (n)-[r]-(related)
            WITH n, collect({type: type(r), related: related}) as relationships
            RETURN n, relationships, 
                   CASE 
                       WHEN n.label CONTAINS $query THEN 1.0
                       WHEN n.definition CONTAINS $query THEN 0.8
                       ELSE 0.6
                   END as relevance_score
            ORDER BY relevance_score DESC
            LIMIT 10
            """
            
            result = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.graph.query(semantic_query, {'query': query.lower()})
            )
            
            for record in safe_iterate(result.result_set):
                if len(record) >= 1:
                    node = record[0]
                    relationships = safe_iterate(record[1]) if len(record) > 1 else []
                    relevance_score = record[2] if len(record) > 2 else 0.5
                    
                    results.append({
                        'type': 'semantic_concept',
                        'data': node,
                        'relationships': relationships,
                        'score': relevance_score,
                        'source': 'FalkorDB Semantic'
                    })
                
        except Exception as e:
            logging.error(f"❌ Semantic graph search error: {e}")
        
        return results

# ============================================================================
# ELASTICSEARCH MANAGER - TRUNCATED DUE TO LENGTH
# ============================================================================

class AdvancedElasticsearchManager:
    """Enhanced Elasticsearch manager with agentic capabilities - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.client = None
        self.embeddings = None
        self._setup_client()
        self._setup_embeddings()
    
    def _setup_client(self):
        """Setup Elasticsearch client with enhanced configuration"""
        try:
            connection_params = {
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True,
                "verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
            }
            
            if self.config.ELASTICSEARCH_CA_CERTS:
                connection_params["ca_certs"] = self.config.ELASTICSEARCH_CA_CERTS
                protocol = "https"
            else:
                protocol = "https" if self.config.ELASTICSEARCH_VERIFY_CERTS else "http"
            
            host_url = f"{protocol}://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"
            connection_params["hosts"] = [host_url]
            
            if self.config.ELASTICSEARCH_PASSWORD:
                connection_params["basic_auth"] = (
                    self.config.ELASTICSEARCH_USERNAME, 
                    self.config.ELASTICSEARCH_PASSWORD
                )
            
            self.client = Elasticsearch(**connection_params)
            
            if self.client.ping():
                logging.info(f"✅ Connected to Elasticsearch at {host_url}")
            else:
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"❌ Elasticsearch connection error: {e}")
            raise
    
    def _setup_embeddings(self):
        """Setup custom OpenAI embeddings"""
        try:
            self.embeddings = CustomOpenAIEmbeddings(
                api_key=self.config.OPENAI_API_KEY,
                model=self.config.EMBEDDING_MODEL,
                dimensions=self.config.EMBEDDING_DIMENSIONS
            )
            logging.info("✅ Custom OpenAI embeddings initialized")
        except Exception as e:
            logging.error(f"❌ Embeddings setup error: {e}")
            raise
    
    async def agentic_vector_search(self, query: str, search_strategy: str = "hybrid") -> List[Dict[str, Any]]:
        """Agentic vector search with multiple strategies"""
        try:
            if search_strategy == "hybrid":
                return await self._hybrid_search(query)
            elif search_strategy == "multi_query":
                return await self._multi_query_search(query)
            elif search_strategy == "contextual_compression":
                return await self._contextual_compression_search(query)
            else:
                return await self._dense_vector_search(query)
                
        except Exception as e:
            logging.error(f"❌ Agentic vector search error: {e}")
            return []
    
    async def _hybrid_search(self, query: str) -> List[Dict[str, Any]]:
        """Hybrid search combining dense vector and sparse text search"""
        try:
            # Dense vector search
            vector_results = await self._dense_vector_search(query)
            
            # Sparse keyword search
            keyword_results = await self._keyword_search(query)
            
            # Combine and rerank results
            combined_results = []
            
            # Add vector results with higher weight for semantic relevance
            for i, result in enumerate(safe_iterate(vector_results)[:7]):
                result['hybrid_score'] = result.get('score', 0.7) * 0.7 + (1.0 - i * 0.1)
                result['search_type'] = 'vector'
                combined_results.append(result)
            
            # Add keyword results with weight for exact matches
            for i, result in enumerate(safe_iterate(keyword_results)[:5]):
                result['hybrid_score'] = result.get('score', 0.6) * 0.3 + (0.8 - i * 0.1)
                result['search_type'] = 'keyword'
                combined_results.append(result)
            
            # Sort by hybrid score and remove duplicates
            unique_results = []
            seen_ids = set()
            
            for result in sorted(combined_results, key=lambda x: x.get('hybrid_score', 0), reverse=True):
                result_id = result.get('id') or hash(str(result.get('data', '')))
                if result_id not in seen_ids:
                    seen_ids.add(result_id)
                    unique_results.append(result)
            
            return unique_results[:self.config.MAX_SEARCH_RESULTS]
            
        except Exception as e:
            logging.error(f"❌ Hybrid search error: {e}")
            return await self._dense_vector_search(query)
    
    async def _dense_vector_search(self, query: str) -> List[Dict[str, Any]]:
        """Dense vector search using embeddings"""
        try:
            # Generate query embedding
            query_embedding = await self.embeddings.embed_query(query)
            
            # Elasticsearch vector search
            search_body = {
                "size": self.config.MAX_SEARCH_RESULTS,
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embeddings') + 1.0",
                            "params": {"query_vector": query_embedding}
                        }
                    }
                },
                "_source": {
                    "excludes": ["embeddings"]
                }
            }
            
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.client.search(
                    index=self.config.ELASTICSEARCH_INDEX,
                    body=search_body
                )
            )
            
            results = []
            hits = safe_get(response, 'hits', {})
            for hit in safe_iterate(hits.get('hits', [])):
                results.append({
                    'type': 'dense_vector',
                    'data': safe_get(hit, '_source', {}),
                    'score': safe_get(hit, '_score', 0.0),
                    'id': safe_get(hit, '_id', ''),
                    'source': 'Elasticsearch Dense Vector'
                })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Dense vector search error: {e}")
            return []
    
    async def _keyword_search(self, query: str) -> List[Dict[str, Any]]:
        """Keyword-based search for exact matches"""
        try:
            search_body = {
                "size": self.config.MAX_SEARCH_RESULTS,
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["content^2", "title^1.5", "document_type", "source_file"],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                }
            }
            
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.client.search(
                    index=self.config.ELASTICSEARCH_INDEX,
                    body=search_body
                )
            )
            
            results = []
            hits = safe_get(response, 'hits', {})
            for hit in safe_iterate(hits.get('hits', [])):
                results.append({
                    'type': 'keyword_match',
                    'data': safe_get(hit, '_source', {}),
                    'score': safe_get(hit, '_score', 0.0) / 10.0,  # Normalize score
                    'id': safe_get(hit, '_id', ''),
                    'source': 'Elasticsearch Keyword'
                })
            
            return results
            
        except Exception as e:
            logging.error(f"❌ Keyword search error: {e}")
            return []
    
    # Additional methods would continue here...
    async def _multi_query_search(self, query: str) -> List[Dict[str, Any]]:
        """Placeholder for multi-query search"""
        return await self._dense_vector_search(query)
    
    async def _contextual_compression_search(self, query: str) -> List[Dict[str, Any]]:
        """Placeholder for contextual compression search"""
        return await self._dense_vector_search(query)

# ============================================================================
# ENHANCED MULTI-AGENT TOOLS WITH ADVANCED CAPABILITIES - FIXED
# ============================================================================

# Global instances for enhanced tool injection
falkor_manager: Optional[AdvancedFalkorDBManager] = None
es_manager: Optional[AdvancedElasticsearchManager] = None
web_search_instance = None
corrective_rag: Optional[CorrectiveRAG] = None
self_reflective_rag: Optional[SelfReflectiveRAG] = None
adaptive_rag: Optional[AdaptiveRAG] = None

@tool
def enhanced_graph_search_tool(query: str, search_type: str = "adaptive") -> Dict[str, Any]:
    """Enhanced graph search with QA chains and openCypher - FIXED"""
    try:
        if falkor_manager is None:
            return {
                "success": False,
                "error": "Graph database not available",
                "results": []
            }
        
        # Use the helper function to run async code
        try:
            results = run_async_in_thread(falkor_manager.enhanced_graph_search(query, search_type))
        except Exception as e:
            logging.error(f"Graph search execution error: {e}")
            results = []
        
        return {
            "success": True,
            "results": safe_iterate(results),
            "count": len(safe_iterate(results)),
            "search_type": search_type,
            "capabilities": ["qa_chain", "openCypher", "multi_hop", "semantic"]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def agentic_vector_search_tool(query: str, search_strategy: str = "hybrid") -> Dict[str, Any]:
    """Agentic vector search with hybrid capabilities - FIXED"""
    try:
        if es_manager is None:
            return {
                "success": False,
                "error": "Vector database not available",
                "results": []
            }
        
        # Use the helper function to run async code
        try:
            results = run_async_in_thread(es_manager.agentic_vector_search(query, search_strategy))
        except Exception as e:
            logging.error(f"Vector search execution error: {e}")
            results = []
        
        return {
            "success": True,
            "results": safe_iterate(results),
            "count": len(safe_iterate(results)),
            "search_strategy": search_strategy,
            "capabilities": ["hybrid", "multi_query", "contextual_compression"]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool 
def corrective_retrieval_tool(query: str) -> Dict[str, Any]:
    """Corrective RAG (CRAG) with quality evaluation and self-correction - FIXED"""
    try:
        if corrective_rag is None:
            return {
                "success": False,
                "error": "Corrective RAG not available",
                "results": []
            }
        
        # Get initial vector search results
        initial_search = agentic_vector_search_tool.invoke({"query": query, "search_strategy": "hybrid"})
        
        if not safe_get(initial_search, "success"):
            return initial_search
        
        # Convert to documents for evaluation
        initial_docs = []
        for result in safe_iterate(initial_search.get("results", [])):
            content = str(safe_get(result, "data", {}).get("content", ""))
            if content and content.strip():
                initial_docs.append(Document(page_content=content))
        
        # Run corrective retrieval with helper function
        try:
            correction_result = run_async_in_thread(corrective_rag.corrective_retrieve(query, initial_docs))
        except Exception as e:
            logging.error(f"Corrective RAG execution error: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }
        
        corrected_docs = safe_iterate(correction_result.get("documents", []))
        return {
            "success": True,
            "results": [{"type": "corrective_rag", "data": safe_get(doc, "page_content", ""), "metadata": safe_get(doc, "metadata", {})} 
                       for doc in corrected_docs],
            "initial_evaluation": safe_get(correction_result, "initial_evaluation", {}),
            "final_evaluation": safe_get(correction_result, "final_evaluation", {}),
            "correction_steps": safe_iterate(correction_result.get("correction_steps", [])),
            "confidence": safe_get(correction_result, "confidence", 0.7)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def adaptive_routing_tool(query: str) -> Dict[str, Any]:
    """Adaptive query routing based on complexity analysis - FIXED"""
    try:
        if adaptive_rag is None:
            return {
                "success": False,
                "error": "Adaptive RAG not available",
                "results": []
            }
        
        # Analyze query complexity with helper function
        try:
            analysis = run_async_in_thread(adaptive_rag.analyze_query_complexity(query))
        except Exception as e:
            logging.error(f"Adaptive routing execution error: {e}")
            analysis = {
                "complexity": "moderate",
                "query_type": "factual",
                "retrieval_strategy": "single_vector",
                "reasoning": f"Analysis failed: {str(e)}"
            }
        
        # Route to appropriate search strategy
        strategy_map = {
            "single_vector": "hybrid",
            "multi_vector": "multi_query", 
            "graph_traversal": "adaptive",
            "web_enhanced": "hybrid",
            "comprehensive": "adaptive"
        }
        
        retrieval_strategy = safe_get(analysis, "retrieval_strategy", "single_vector")
        search_strategy = strategy_map.get(retrieval_strategy, "hybrid")
        
        # Execute appropriate search
        if retrieval_strategy == "graph_traversal":
            search_result = enhanced_graph_search_tool.invoke({
                "query": query, 
                "search_type": "adaptive"
            })
        else:
            search_result = agentic_vector_search_tool.invoke({
                "query": query,
                "search_strategy": search_strategy
            })
        
        return {
            "success": True,
            "query_analysis": analysis,
            "selected_strategy": search_strategy,
            "results": safe_iterate(search_result.get("results", [])),
            "routing_reasoning": safe_get(analysis, "reasoning", "")
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def web_search_tool(query: str) -> Dict[str, Any]:
    """Enhanced web search for latest GDPR information - FIXED"""
    try:
        if web_search_instance is None:
            return {
                "success": False,
                "error": "Web search not available",
                "results": []
            }
        
        # Run web search with helper function
        try:
            results = run_async_in_thread(web_search_instance.search(f"GDPR {query}"))
        except Exception as e:
            logging.error(f"Web search execution error: {e}")
            results = []
        
        safe_results = safe_iterate(results)
        return {
            "success": True,
            "results": safe_results,
            "count": len(safe_results),
            "sources": list(set(safe_get(r, 'source', 'Unknown') for r in safe_results))
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

@tool
def comprehensive_search_tool(query: str) -> Dict[str, Any]:
    """Comprehensive search combining all available methods - FIXED"""
    try:
        # Start with adaptive routing
        routing_result = adaptive_routing_tool.invoke({"query": query})
        
        # Get graph results
        graph_result = enhanced_graph_search_tool.invoke({
            "query": query,
            "search_type": "adaptive"
        })
        
        # Get vector results with corrective RAG
        corrective_result = corrective_retrieval_tool.invoke({"query": query})
        
        # Get web results for current information
        web_result = web_search_tool.invoke({"query": query})
        
        return {
            "success": True,
            "adaptive_routing": routing_result,
            "graph_search": graph_result,
            "corrective_rag": corrective_result,
            "web_search": web_result,
            "total_sources": 4,
            "comprehensive": True
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }

# ============================================================================
# ENHANCED MULTI-AGENT ARCHITECTURE WITH ADVANCED COORDINATION - FIXED
# ============================================================================

class AdvancedSearchState(TypedDict):
    """Enhanced state for advanced multi-agent search system"""
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    query_analysis: Optional[Dict[str, Any]]
    routing_decision: Optional[str]
    
    # Search results from different methods
    graph_results: Optional[List[Dict]]
    vector_results: Optional[List[Dict]]
    web_results: Optional[List[Dict]]
    corrective_results: Optional[Dict[str, Any]]
    
    # Advanced RAG results
    self_rag_results: Optional[Dict[str, Any]]
    adaptive_routing: Optional[Dict[str, Any]]
    
    # Evaluation and reflection
    retrieval_evaluations: List[Dict[str, Any]]
    confidence_scores: Dict[str, float]
    reflection_steps: List[str]
    
    # Final synthesis
    synthesized_response: Optional[str]
    final_confidence: Optional[float]
    requires_feedback: bool
    
    # Agent coordination
    active_agent: Optional[str]
    agent_history: List[str]
    iteration_count: int
    
    # Performance metrics
    search_latency: Dict[str, float]
    total_documents: int

class EnhancedGDPRSearchAgents:
    """Advanced multi-agent system with 2025 SOTA techniques - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        
        # Enhanced LLM with reasoning capabilities
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            temperature=0.1  # Lower temperature for more consistent output
        )
        
        # Initialize advanced RAG components
        self.corrective_rag = None
        self.self_reflective_rag = None
        self.adaptive_rag = None
        
        # Enhanced tools with advanced capabilities
        self.tools = [
            enhanced_graph_search_tool,
            agentic_vector_search_tool,
            corrective_retrieval_tool,
            adaptive_routing_tool,
            web_search_tool,
            comprehensive_search_tool
        ]
        
        # Enhanced memory and state management
        self.memory = MemorySaver()
        self.store = InMemoryStore()
        
        # Build the enhanced agent graph
        self.search_graph = self._build_enhanced_search_graph()
    
    def initialize_advanced_rag(self, falkor_mgr, es_mgr, web_search):
        """Initialize advanced RAG components after database setup"""
        global corrective_rag, self_reflective_rag, adaptive_rag
        
        self.corrective_rag = CorrectiveRAG(self.config, web_search_tool, agentic_vector_search_tool)
        self.self_reflective_rag = SelfReflectiveRAG(self.config)
        self.adaptive_rag = AdaptiveRAG(self.config)
        
        # Set global instances
        corrective_rag = self.corrective_rag
        self_reflective_rag = self.self_reflective_rag
        adaptive_rag = self.adaptive_rag
    
    def _build_enhanced_search_graph(self) -> StateGraph:
        """Build enhanced multi-agent search graph with advanced coordination - FIXED"""
        
        workflow = StateGraph(AdvancedSearchState)
        
        # Enhanced agent nodes - ALL ARE NOW ASYNC
        workflow.add_node("query_analyzer", self._query_analyzer_agent)
        workflow.add_node("adaptive_router", self._adaptive_router_agent)
        workflow.add_node("graph_specialist", self._enhanced_graph_specialist)
        workflow.add_node("vector_specialist", self._enhanced_vector_specialist)
        workflow.add_node("web_researcher", self._enhanced_web_researcher)
        workflow.add_node("corrective_agent", self._corrective_rag_agent)
        workflow.add_node("reflection_agent", self._self_reflection_agent)
        workflow.add_node("synthesis_coordinator", self._synthesis_coordinator)
        workflow.add_node("quality_evaluator", self._quality_evaluator)
        workflow.add_node("response_optimizer", self._response_optimizer)
        
        # Enhanced routing with adaptive decision making
        workflow.add_edge(START, "query_analyzer")
        workflow.add_edge("query_analyzer", "adaptive_router")
        
        # Adaptive routing to specialists
        workflow.add_conditional_edges(
            "adaptive_router",
            self._route_to_specialists,
            {
                "graph_only": "graph_specialist",
                "vector_only": "vector_specialist",
                "web_only": "web_researcher", 
                "graph_vector": "graph_specialist",
                "comprehensive": "corrective_agent",
                "simple": "vector_specialist"
            }
        )
        
        # Specialist coordination paths
        workflow.add_edge("graph_specialist", "synthesis_coordinator")
        workflow.add_edge("vector_specialist", "corrective_agent")
        workflow.add_edge("web_researcher", "synthesis_coordinator")
        workflow.add_edge("corrective_agent", "reflection_agent")
        workflow.add_edge("reflection_agent", "synthesis_coordinator")
        
        # Enhanced evaluation and optimization
        workflow.add_edge("synthesis_coordinator", "quality_evaluator")
        
        workflow.add_conditional_edges(
            "quality_evaluator",
            self._evaluate_quality,
            {
                "optimize": "response_optimizer",
                "retry_graph": "graph_specialist",
                "retry_vector": "vector_specialist", 
                "retry_web": "web_researcher",
                "complete": END
            }
        )
        
        workflow.add_edge("response_optimizer", END)
        
        return workflow.compile(checkpointer=self.memory, store=self.store)
    
    async def _query_analyzer_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced query analysis with complexity assessment - ASYNC FIXED"""
        
        messages = safe_iterate(state.get("messages", []))
        if not messages:
            return state
        
        last_message = messages[-1] if messages else None
        query = safe_get(last_message, 'content', '') if hasattr(last_message, 'content') else str(last_message) if last_message else ""
        
        if not query:
            query = "What is GDPR?"  # Default fallback
        
        state["query"] = query
        
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced GDPR query analyzer.

IMPORTANT: Respond with ONLY valid JSON in this exact format:

{
  "intent": "factual",
  "complexity": "moderate",
  "domain": "general",
  "recency": "static",
  "reasoning": "direct",
  "confidence": 0.8,
  "key_concepts": ["gdpr", "compliance"],
  "reasoning_chain": "Query analysis completed successfully"
}

Intent options: factual, procedural, analytical, comparative, hybrid
Complexity options: simple, moderate, complex, expert
Domain options: general, articles, case_law, implementation, cross_domain
Recency options: static, recent, current, mixed
Reasoning options: direct, multi_hop, synthesis, analysis, creative"""),
            ("human", f"Analyze this GDPR query: {query}\n\nRespond with valid JSON only:")
        ])
        
        try:
            # Use ainvoke with better error handling
            response = await self.llm.ainvoke(analysis_prompt.format_messages())
            
            required_keys = ["intent", "complexity", "domain", "recency", "reasoning", "confidence", "key_concepts", "reasoning_chain"]
            analysis = extract_json_from_response(response.content.strip(), required_keys)
            
            # Validate confidence
            analysis["confidence"] = max(0.0, min(1.0, float(analysis.get("confidence", 0.7))))
            
            state["query_analysis"] = analysis
            state["active_agent"] = "query_analyzer"
            state["agent_history"] = ["query_analyzer"]
            state["iteration_count"] = 0
            
            # Add analysis message
            analysis_msg = AIMessage(
                content=f"Query analyzed: {analysis['complexity']} {analysis['intent']} query about {analysis['domain']} GDPR topics",
                name="query_analyzer"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [analysis_msg]
            
            logging.info(f"🔍 Query Analysis: {analysis['complexity']} {analysis['intent']} query (confidence: {analysis['confidence']:.2f})")
            
        except Exception as e:
            logging.error(f"❌ Query analysis error: {e}")
            # Provide robust fallback
            state["query_analysis"] = {
                "intent": "factual",
                "complexity": "moderate", 
                "domain": "general",
                "recency": "static",
                "reasoning": "direct",
                "confidence": 0.7,
                "key_concepts": ["gdpr"],
                "reasoning_chain": f"Analysis failed: {str(e)}"
            }
        
        return state
    
    async def _adaptive_router_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced adaptive routing based on query analysis - ASYNC FIXED"""
        
        analysis = safe_get(state, "query_analysis", {})
        query = safe_get(state, "query", "")
        
        # Advanced routing logic based on analysis
        complexity = safe_get(analysis, "complexity", "moderate")
        intent = safe_get(analysis, "intent", "factual")
        domain = safe_get(analysis, "domain", "general")
        reasoning = safe_get(analysis, "reasoning", "direct")
        recency = safe_get(analysis, "recency", "static")
        
        # Sophisticated routing decision
        if complexity == "expert" or reasoning == "analysis":
            routing_decision = "comprehensive"
        elif domain == "articles" or reasoning == "multi_hop":
            routing_decision = "graph_vector"
        elif recency in ["recent", "current"]:
            routing_decision = "web_only"
        elif intent == "factual" and complexity == "simple":
            routing_decision = "simple"
        elif domain == "case_law" or intent == "comparative":
            routing_decision = "graph_only"
        else:
            routing_decision = "vector_only"
        
        # Enhanced reasoning for routing
        routing_reasoning = self._generate_routing_reasoning(analysis, routing_decision)
        
        state["routing_decision"] = routing_decision
        state["adaptive_routing"] = {
            "decision": routing_decision,
            "reasoning": routing_reasoning,
            "analysis_basis": analysis
        }
        state["active_agent"] = "adaptive_router"
        
        agent_history = safe_iterate(state.get("agent_history", []))
        agent_history.append("adaptive_router")
        state["agent_history"] = agent_history
        
        # Add routing message
        routing_msg = AIMessage(
            content=f"Routing to {routing_decision} strategy: {routing_reasoning}",
            name="adaptive_router"
        )
        state["messages"] = safe_iterate(state.get("messages", [])) + [routing_msg]
        
        logging.info(f"🧭 Adaptive Routing: {routing_decision} ({routing_reasoning})")
        
        return state
    
    def _generate_routing_reasoning(self, analysis: Dict, decision: str) -> str:
        """Generate reasoning for routing decision"""
        complexity = safe_get(analysis, "complexity", "moderate")
        intent = safe_get(analysis, "intent", "factual") 
        domain = safe_get(analysis, "domain", "general")
        reasoning = safe_get(analysis, "reasoning", "direct")
        
        reasoning_map = {
            "comprehensive": f"Complex {intent} query requiring multi-source analysis",
            "graph_vector": f"Domain-specific query needing relationship exploration", 
            "web_only": f"Current information needed for {domain} topics",
            "simple": f"Simple {intent} query with direct answer available",
            "graph_only": f"Relationship-heavy query in {domain} domain",
            "vector_only": f"Standard {complexity} query suitable for document search"
        }
        
        return reasoning_map.get(decision, "Standard routing applied")
    
    async def _enhanced_graph_specialist(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced graph specialist with QA chains and openCypher - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        analysis = safe_get(state, "query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Determine graph search strategy based on analysis
            reasoning = safe_get(analysis, "reasoning", "direct")
            domain = safe_get(analysis, "domain", "general")
            
            if reasoning == "multi_hop":
                search_type = "multi_hop"
            elif domain == "articles":
                search_type = "qa_chain"
            elif "relationship" in query.lower() or "connected" in query.lower():
                search_type = "pattern_matching"
            else:
                search_type = "adaptive"
            
            # Execute enhanced graph search using the tool
            results = enhanced_graph_search_tool.invoke({
                "query": query,
                "search_type": search_type
            })
            
            state["graph_results"] = safe_iterate(results.get("results", []))
            state["active_agent"] = "graph_specialist"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("graph_specialist")
            state["agent_history"] = agent_history
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            search_latency = safe_get(state, "search_latency", {})
            search_latency["graph"] = end_time - start_time
            state["search_latency"] = search_latency
            
            # Add specialist message
            specialist_msg = AIMessage(
                content=f"Graph search completed: {len(state['graph_results'])} results using {search_type} strategy",
                name="graph_specialist"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [specialist_msg]
            
            logging.info(f"📊 Graph Specialist: {len(state['graph_results'])} results ({search_type})")
            
        except Exception as e:
            logging.error(f"❌ Graph specialist error: {e}")
            state["graph_results"] = []
        
        return state
    
    async def _enhanced_vector_specialist(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced vector specialist with agentic search - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        analysis = safe_get(state, "query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Determine search strategy based on analysis
            complexity = safe_get(analysis, "complexity", "moderate")
            intent = safe_get(analysis, "intent", "factual")
            
            if complexity == "expert":
                search_strategy = "multi_query"
            elif intent == "comparative":
                search_strategy = "contextual_compression"
            else:
                search_strategy = "hybrid"
            
            # Execute agentic vector search using the tool
            results = agentic_vector_search_tool.invoke({
                "query": query,
                "search_strategy": search_strategy
            })
            
            state["vector_results"] = safe_iterate(results.get("results", []))
            state["active_agent"] = "vector_specialist"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("vector_specialist")
            state["agent_history"] = agent_history
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            search_latency = safe_get(state, "search_latency", {})
            search_latency["vector"] = end_time - start_time
            state["search_latency"] = search_latency
            
            # Add specialist message
            specialist_msg = AIMessage(
                content=f"Vector search completed: {len(state['vector_results'])} results using {search_strategy} strategy",
                name="vector_specialist"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [specialist_msg]
            
            logging.info(f"🔍 Vector Specialist: {len(state['vector_results'])} results ({search_strategy})")
            
        except Exception as e:
            logging.error(f"❌ Vector specialist error: {e}")
            state["vector_results"] = []
        
        return state
    
    async def _enhanced_web_researcher(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced web researcher with authority focus - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        analysis = safe_get(state, "query_analysis", {})
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Execute web search using the tool
            results = web_search_tool.invoke({"query": query})
            
            state["web_results"] = safe_iterate(results.get("results", []))
            state["active_agent"] = "web_researcher"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("web_researcher")
            state["agent_history"] = agent_history
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            search_latency = safe_get(state, "search_latency", {})
            search_latency["web"] = end_time - start_time
            state["search_latency"] = search_latency
            
            # Add researcher message
            sources = safe_iterate(results.get('sources', []))
            researcher_msg = AIMessage(
                content=f"Web research completed: {len(state['web_results'])} results from {len(sources)} sources",
                name="web_researcher"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [researcher_msg]
            
            logging.info(f"🌐 Web Researcher: {len(state['web_results'])} results")
            
        except Exception as e:
            logging.error(f"❌ Web researcher error: {e}")
            state["web_results"] = []
        
        return state
    
    async def _corrective_rag_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Corrective RAG agent with quality evaluation - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Execute corrective retrieval using the tool
            results = corrective_retrieval_tool.invoke({"query": query})
            
            state["corrective_results"] = results
            state["active_agent"] = "corrective_agent"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("corrective_agent")
            state["agent_history"] = agent_history
            
            # Record evaluation
            retrieval_evaluations = safe_iterate(state.get("retrieval_evaluations", []))
            
            if safe_get(results, "success"):
                retrieval_evaluations.append({
                    "agent": "corrective_rag",
                    "initial_evaluation": safe_get(results, "initial_evaluation", {}),
                    "final_evaluation": safe_get(results, "final_evaluation", {}),
                    "correction_steps": safe_iterate(results.get("correction_steps", []))
                })
                state["retrieval_evaluations"] = retrieval_evaluations
            
            # Record performance metrics
            end_time = asyncio.get_event_loop().time()
            search_latency = safe_get(state, "search_latency", {})
            search_latency["corrective"] = end_time - start_time
            state["search_latency"] = search_latency
            
            # Add corrective message
            correction_steps = safe_iterate(results.get('correction_steps', []))
            corrective_msg = AIMessage(
                content=f"Corrective RAG completed: {len(correction_steps)} correction steps applied",
                name="corrective_agent"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [corrective_msg]
            
            logging.info(f"🔧 Corrective RAG: {len(correction_steps)} corrections")
            
        except Exception as e:
            logging.error(f"❌ Corrective RAG error: {e}")
            state["corrective_results"] = {"success": False, "error": str(e)}
        
        return state
    
    async def _self_reflection_agent(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Self-reflection agent with critique capabilities - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        
        # Collect all available documents for reflection - FIXED NULL ITERATION
        all_documents = []
        
        # From vector results
        vector_results = safe_iterate(state.get("vector_results", []))
        for result in vector_results:
            content = str(safe_get(result, "data", {}).get("content", ""))
            if content and content.strip():
                all_documents.append(Document(page_content=content))
        
        # From corrective results
        corrective_results = safe_get(state, "corrective_results", {})
        if safe_get(corrective_results, "success"):
            for result in safe_iterate(corrective_results.get("results", [])):
                content = safe_get(result, "data", "")
                if content and content.strip():
                    all_documents.append(Document(page_content=content))
        
        # From graph results
        graph_results = safe_iterate(state.get("graph_results", []))
        for result in graph_results:
            content = str(safe_get(result, "data", ""))
            if content and content.strip():
                all_documents.append(Document(page_content=content))
        
        try:
            if self_reflective_rag and all_documents:
                # Execute self-reflective generation
                conversation_context = self._build_conversation_context(state)
                
                # Run the self-reflection directly since we're already in async context
                reflection_result = await self_reflective_rag.generate_with_reflection(
                    query, all_documents, conversation_context
                )
                
                state["self_rag_results"] = reflection_result
                state["reflection_steps"] = safe_iterate(reflection_result.get("reflection_steps", []))
                
                # Record confidence
                confidence_scores = safe_get(state, "confidence_scores", {})
                confidence_scores["self_reflection"] = safe_get(reflection_result, "confidence", 0.7)
                state["confidence_scores"] = confidence_scores
                
            else:
                state["self_rag_results"] = {"response": "Self-reflection not available", "confidence": 0.5}
                state["reflection_steps"] = ["Self-reflection skipped - no documents available"]
            
            state["active_agent"] = "reflection_agent"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("reflection_agent")
            state["agent_history"] = agent_history
            
            # Add reflection message
            reflection_steps = safe_iterate(state.get("reflection_steps", []))
            reflection_msg = AIMessage(
                content=f"Self-reflection completed: {len(reflection_steps)} reflection steps",
                name="reflection_agent"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [reflection_msg]
            
            logging.info(f"🤔 Self-Reflection: {len(reflection_steps)} steps")
            
        except Exception as e:
            logging.error(f"❌ Self-reflection error: {e}")
            state["self_rag_results"] = {"response": f"Reflection failed: {str(e)}", "confidence": 0.3}
            state["reflection_steps"] = [f"Reflection error: {str(e)}"]
        
        return state
    
    async def _synthesis_coordinator(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced synthesis coordinator with multi-source integration - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        analysis = safe_get(state, "query_analysis", {})
        
        # Collect all available information
        synthesis_context = self._build_synthesis_context(state)
        
        synthesis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced GDPR synthesis coordinator.

Synthesize information from multiple sources to create a comprehensive, accurate response.

Requirements:
1. Integrate information from all available sources
2. Prioritize authoritative GDPR sources
3. Provide practical, actionable guidance
4. Cite specific GDPR articles when relevant
5. Acknowledge uncertainty and limitations
6. Use clear, professional language
7. Structure response logically"""),
            ("human", f"""Query: {query}

Query Analysis: {analysis}

Available Information:
{synthesis_context}

Provide a comprehensive synthesized response:""")
        ])
        
        try:
            # Use ainvoke instead of invoke - FIXED
            response = await self.llm.ainvoke(synthesis_prompt.format_messages())
            synthesized_response = response.content.strip() if response.content else "Unable to generate response."
            
            state["synthesized_response"] = synthesized_response
            state["active_agent"] = "synthesis_coordinator"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("synthesis_coordinator")
            state["agent_history"] = agent_history
            
            # Calculate document count
            total_docs = (
                len(safe_iterate(state.get("graph_results", []))) +
                len(safe_iterate(state.get("vector_results", []))) +
                len(safe_iterate(state.get("web_results", []))) +
                len(safe_iterate(state.get("corrective_results", {}).get("results", [])))
            )
            state["total_documents"] = total_docs
            
            # Add synthesis message
            synthesis_msg = AIMessage(
                content=f"Synthesis completed: Integrated information from {total_docs} sources",
                name="synthesis_coordinator"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [synthesis_msg]
            
            logging.info(f"🔄 Synthesis: Integrated {total_docs} sources")
            
        except Exception as e:
            logging.error(f"❌ Synthesis error: {e}")
            state["synthesized_response"] = "I apologize, but I encountered an error while synthesizing the response. Please try rephrasing your question."
        
        return state
    
    async def _quality_evaluator(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Enhanced quality evaluator with comprehensive assessment - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        response = safe_get(state, "synthesized_response", "")
        analysis = safe_get(state, "query_analysis", {})
        
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an advanced quality evaluator for GDPR responses.

IMPORTANT: Respond with ONLY valid JSON in this exact format:

{
  "decision": "complete",
  "overall_confidence": 0.85,
  "dimension_scores": {
    "accuracy": 0.9,
    "completeness": 0.8,
    "clarity": 0.9
  },
  "strengths": ["strength1", "strength2"],
  "weaknesses": ["weakness1"],
  "improvement_suggestions": ["suggestion1"],
  "reasoning_chain": "evaluation reasoning"
}

Decision options: complete, optimize, retry_graph, retry_vector, retry_web"""),
            ("human", f"""Query: {query}

Response to evaluate:
{response}

Sources used:
- Graph results: {len(safe_iterate(state.get('graph_results', [])))}
- Vector results: {len(safe_iterate(state.get('vector_results', [])))}
- Web results: {len(safe_iterate(state.get('web_results', [])))}

Provide comprehensive evaluation as valid JSON only:""")
        ])
        
        try:
            # Use ainvoke instead of invoke - FIXED
            response_eval = await self.llm.ainvoke(evaluation_prompt.format_messages())
            
            required_keys = ["decision", "overall_confidence", "dimension_scores", "strengths", "weaknesses", "improvement_suggestions", "reasoning_chain"]
            evaluation = extract_json_from_response(response_eval.content.strip(), required_keys)
            
            # Validate evaluation structure
            evaluation["overall_confidence"] = max(0.0, min(1.0, float(evaluation.get("overall_confidence", 0.7))))
            
            valid_decisions = ["complete", "optimize", "retry_graph", "retry_vector", "retry_web"]
            if evaluation.get("decision") not in valid_decisions:
                evaluation["decision"] = "complete"
            
            state["final_confidence"] = evaluation["overall_confidence"]
            
            # Record comprehensive evaluation
            retrieval_evaluations = safe_iterate(state.get("retrieval_evaluations", []))
            retrieval_evaluations.append({
                "agent": "quality_evaluator",
                "evaluation": evaluation,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })
            state["retrieval_evaluations"] = retrieval_evaluations
            
            # Determine if feedback is required
            if evaluation["overall_confidence"] < self.config.CONFIDENCE_THRESHOLD:
                state["requires_feedback"] = True
            else:
                state["requires_feedback"] = False
            
            state["active_agent"] = "quality_evaluator"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("quality_evaluator")
            state["agent_history"] = agent_history
            
            # Add evaluation message
            eval_msg = AIMessage(
                content=f"Quality evaluation: {evaluation['decision']} (confidence: {evaluation['overall_confidence']:.2f})",
                name="quality_evaluator"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [eval_msg]
            
            logging.info(f"⚖️ Quality Evaluation: {evaluation['decision']} (confidence: {evaluation['overall_confidence']:.2f})")
            
        except Exception as e:
            logging.error(f"❌ Quality evaluation error: {e}")
            evaluation = {
                "decision": "complete",
                "overall_confidence": 0.7,
                "reasoning_chain": f"Evaluation failed: {str(e)}"
            }
            state["final_confidence"] = 0.7
        
        return state
    
    async def _response_optimizer(self, state: AdvancedSearchState) -> AdvancedSearchState:
        """Response optimizer for final enhancement - ASYNC FIXED"""
        
        query = safe_get(state, "query", "")
        current_response = safe_get(state, "synthesized_response", "")
        
        evaluations = safe_iterate(state.get("retrieval_evaluations", []))
        evaluation = evaluations[-1] if evaluations else {}
        
        # Get improvement suggestions from evaluation
        eval_data = safe_get(evaluation, "evaluation", {})
        suggestions = safe_iterate(eval_data.get("improvement_suggestions", []))
        weaknesses = safe_iterate(eval_data.get("weaknesses", []))
        
        optimization_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a response optimizer for GDPR compliance responses.

Enhance the response based on evaluation feedback while maintaining accuracy.

Focus areas:
1. Address identified weaknesses
2. Implement improvement suggestions
3. Enhance clarity and structure
4. Strengthen authoritative citations
5. Improve practical applicability
6. Ensure GDPR compliance accuracy"""),
            ("human", f"""Query: {query}

Current Response:
{current_response}

Identified Weaknesses:
{weaknesses}

Improvement Suggestions:
{suggestions}

Optimized Response:""")
        ])
        
        try:
            # Use ainvoke instead of invoke - FIXED
            response = await self.llm.ainvoke(optimization_prompt.format_messages())
            optimized_response = response.content.strip() if response.content else current_response
            
            state["synthesized_response"] = optimized_response
            state["active_agent"] = "response_optimizer"
            
            agent_history = safe_iterate(state.get("agent_history", []))
            agent_history.append("response_optimizer")
            state["agent_history"] = agent_history
            
            # Boost confidence slightly for optimization
            final_confidence = safe_get(state, "final_confidence", 0.7)
            state["final_confidence"] = min(final_confidence + 0.1, 1.0)
            
            # Add optimizer message
            optimizer_msg = AIMessage(
                content=f"Response optimized: Enhanced based on {len(suggestions)} suggestions",
                name="response_optimizer"
            )
            state["messages"] = safe_iterate(state.get("messages", [])) + [optimizer_msg]
            
            logging.info(f"✨ Response Optimizer: Enhanced with {len(suggestions)} improvements")
            
        except Exception as e:
            logging.error(f"❌ Response optimization error: {e}")
            # Keep original response if optimization fails
            pass
        
        return state
    
    def _build_conversation_context(self, state: AdvancedSearchState) -> str:
        """Build conversation context for self-reflection"""
        messages = safe_iterate(state.get("messages", []))
        if len(messages) <= 1:
            return ""
        
        context_parts = []
        for msg in messages[-5:]:  # Last 5 messages
            if hasattr(msg, 'content') and hasattr(msg, 'name'):
                context_parts.append(f"{msg.name}: {msg.content}")
            elif hasattr(msg, 'content'):
                context_parts.append(f"User: {msg.content}")
        
        return "\n".join(context_parts)
    
    def _build_synthesis_context(self, state: AdvancedSearchState) -> str:
        """Build comprehensive synthesis context"""
        context_parts = []
        
        # Graph results
        graph_results = safe_iterate(state.get("graph_results", []))
        if graph_results:
            context_parts.append("**Graph Knowledge:**")
            for i, result in enumerate(graph_results[:3]):
                result_type = safe_get(result, 'type', 'concept')
                data = safe_get(result, 'data', {})
                context_parts.append(f"- {result_type}: {str(data)[:200]}...")
        
        # Vector results
        vector_results = safe_iterate(state.get("vector_results", []))
        if vector_results:
            context_parts.append("\n**Document Content:**")
            for i, result in enumerate(vector_results[:3]):
                content = safe_get(result, 'data', {}).get('content', '')
                if content:
                    context_parts.append(f"- {content[:200]}...")
        
        # Web results
        web_results = safe_iterate(state.get("web_results", []))
        if web_results:
            context_parts.append("\n**Current Information:**")
            for i, result in enumerate(web_results[:3]):
                title = safe_get(result, 'title', 'Web Result')
                content = safe_get(result, 'content', '')
                if content:
                    context_parts.append(f"- {title}: {content[:200]}...")
        
        # Corrective results
        corrective_results = safe_get(state, "corrective_results", {})
        if safe_get(corrective_results, "success"):
            context_parts.append("\n**Quality-Assessed Information:**")
            correction_steps = safe_iterate(corrective_results.get("correction_steps", []))
            context_parts.append(f"- Applied {len(correction_steps)} quality corrections")
            for result in safe_iterate(corrective_results.get("results", []))[:2]:
                content = safe_get(result, "data", "")
                if content:
                    context_parts.append(f"- {content[:200]}...")
        
        # Self-reflection results
        self_rag = safe_get(state, "self_rag_results", {})
        if safe_get(self_rag, "response"):
            context_parts.append("\n**Self-Reflection Analysis:**")
            response_text = safe_get(self_rag, "response", "")
            if response_text:
                context_parts.append(f"- {response_text[:300]}...")
            
            critique = safe_get(self_rag, "critique", {})
            if critique:
                strengths = safe_iterate(critique.get("strengths", []))
                issues = safe_iterate(critique.get("issues", []))
                if strengths:
                    context_parts.append(f"- Strengths: {', '.join(strengths[:3])}")
                if issues:
                    context_parts.append(f"- Issues addressed: {', '.join(issues[:3])}")
        
        return "\n".join(context_parts)
    
    def _route_to_specialists(self, state: AdvancedSearchState) -> str:
        """Enhanced routing logic based on adaptive analysis"""
        routing_decision = safe_get(state, "routing_decision", "vector_only")
        iteration_count = safe_get(state, "iteration_count", 0)
        
        # Prevent infinite loops
        if iteration_count >= self.config.MAX_AGENT_ITERATIONS:
            return "simple"
        
        return routing_decision
    
    def _evaluate_quality(self, state: AdvancedSearchState) -> str:
        """Enhanced quality evaluation routing"""
        evaluations = safe_iterate(state.get("retrieval_evaluations", []))
        if not evaluations:
            return "complete"
        
        latest_eval = evaluations[-1]
        evaluation = safe_get(latest_eval, "evaluation", {})
        decision = safe_get(evaluation, "decision", "complete")
        iteration_count = safe_get(state, "iteration_count", 0)
        
        # Prevent infinite loops
        if iteration_count >= self.config.MAX_AGENT_ITERATIONS:
            return "complete"
        
        # Increment iteration count for retry decisions
        if decision.startswith("retry_"):
            state["iteration_count"] = iteration_count + 1
        
        return decision

# ============================================================================
# PERFORMANCE MONITORING AND METRICS - FIXED
# ============================================================================

class PerformanceMetrics:
    """Advanced performance monitoring and metrics collection - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.metrics_storage = config.METRICS_PATH
        self.session_metrics = {}
    
    async def record_search_metrics(self, session_id: str, query: str, 
                                  state: AdvancedSearchState, response_time: float):
        """Record comprehensive search metrics"""
        try:
            metrics = {
                "session_id": session_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "query": query,
                "response_time": response_time,
                "total_documents": safe_get(state, "total_documents", 0),
                "final_confidence": safe_get(state, "final_confidence", 0.0),
                "agent_path": safe_iterate(state.get("agent_history", [])),
                "iteration_count": safe_get(state, "iteration_count", 0),
                "search_latencies": safe_get(state, "search_latency", {}),
                "retrieval_evaluations": len(safe_iterate(state.get("retrieval_evaluations", []))),
                "query_analysis": safe_get(state, "query_analysis", {}),
                "routing_decision": safe_get(state, "routing_decision", ""),
                "requires_feedback": safe_get(state, "requires_feedback", False)
            }
            
            # Store in session metrics
            if session_id not in self.session_metrics:
                self.session_metrics[session_id] = []
            self.session_metrics[session_id].append(metrics)
            
            # Save to file
            await self._save_metrics(metrics)
            
        except Exception as e:
            logging.error(f"❌ Metrics recording error: {e}")
    
    async def _save_metrics(self, metrics: Dict):
        """Save metrics to storage"""
        try:
            metrics_file = self.metrics_storage / f"metrics_{datetime.now().strftime('%Y%m%d')}.jsonl"
            
            async with aiofiles.open(metrics_file, 'a', encoding='utf-8') as f:
                await f.write(json.dumps(metrics) + '\n')
                
        except Exception as e:
            logging.error(f"❌ Metrics save error: {e}")
    
    async def get_performance_summary(self, session_id: str = None) -> Dict[str, Any]:
        """Get performance summary for analysis"""
        try:
            if session_id and session_id in self.session_metrics:
                metrics_list = self.session_metrics[session_id]
            else:
                # Load all metrics for today
                metrics_list = await self._load_daily_metrics()
            
            if not metrics_list:
                return {"message": "No metrics available"}
            
            # Calculate summary statistics
            response_times = [safe_get(m, "response_time", 0) for m in metrics_list]
            confidences = [safe_get(m, "final_confidence", 0) for m in metrics_list]
            document_counts = [safe_get(m, "total_documents", 0) for m in metrics_list]
            
            summary = {
                "total_queries": len(metrics_list),
                "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
                "avg_confidence": sum(confidences) / len(confidences) if confidences else 0,
                "avg_documents": sum(document_counts) / len(document_counts) if document_counts else 0,
                "feedback_rate": sum(1 for m in metrics_list if safe_get(m, "requires_feedback")) / len(metrics_list) * 100,
                "agent_usage": self._analyze_agent_usage(metrics_list),
                "routing_patterns": self._analyze_routing_patterns(metrics_list)
            }
            
            return summary
            
        except Exception as e:
            logging.error(f"❌ Performance summary error: {e}")
            return {"error": str(e)}
    
    async def _load_daily_metrics(self) -> List[Dict]:
        """Load metrics for current day"""
        try:
            metrics_file = self.metrics_storage / f"metrics_{datetime.now().strftime('%Y%m%d')}.jsonl"
            
            if not metrics_file.exists():
                return []
            
            metrics_list = []
            async with aiofiles.open(metrics_file, 'r', encoding='utf-8') as f:
                async for line in f:
                    if line.strip():
                        try:
                            metrics_list.append(json.loads(line.strip()))
                        except:
                            continue
            
            return metrics_list
            
        except Exception as e:
            logging.error(f"❌ Daily metrics load error: {e}")
            return []
    
    def _analyze_agent_usage(self, metrics_list: List[Dict]) -> Dict[str, int]:
        """Analyze agent usage patterns"""
        agent_counts = {}
        
        for metrics in metrics_list:
            for agent in safe_iterate(metrics.get("agent_path", [])):
                agent_counts[agent] = agent_counts.get(agent, 0) + 1
        
        return agent_counts
    
    def _analyze_routing_patterns(self, metrics_list: List[Dict]) -> Dict[str, int]:
        """Analyze routing decision patterns"""
        routing_counts = {}
        
        for metrics in metrics_list:
            routing = safe_get(metrics, "routing_decision", "unknown")
            routing_counts[routing] = routing_counts.get(routing, 0) + 1
        
        return routing_counts

# ============================================================================
# ENHANCED CONVERSATION MANAGEMENT WITH ADVANCED MEMORY - FIXED
# ============================================================================

class AdvancedConversationManager:
    """Enhanced conversation manager with advanced memory and context handling - FIXED"""
    
    def __init__(self, config: AdvancedSearchEngineConfig):
        self.config = config
        self.session_storage = config.SESSION_STORAGE_PATH
        self.current_session = None
        self.conversation_embeddings = None
        
        # Initialize conversation embeddings for semantic memory
        if config.ENABLE_AGENT_MEMORY:
            try:
                self.conversation_embeddings = CustomOpenAIEmbeddings(
                    api_key=config.OPENAI_API_KEY,
                    model=config.EMBEDDING_MODEL,
                    dimensions=config.EMBEDDING_DIMENSIONS
                )
            except Exception as e:
                logging.warning(f"Conversation embeddings initialization failed: {e}")
    
    async def start_enhanced_session(self, user_id: str = None, session_context: Dict = None) -> str:
        """Start enhanced conversation session with context"""
        session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        session_data = {
            "session_id": session_id,
            "user_id": user_id or "anonymous",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "session_context": session_context or {},
            "conversation_history": [],
            "search_history": [],
            "feedback_history": [],
            "user_preferences": {},
            "conversation_summary": "",
            "semantic_memory": [],
            "performance_metrics": {
                "total_queries": 0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "satisfaction_score": 0.0
            }
        }
        
        await self._save_session(session_data)
        self.current_session = session_data
        
        logging.info(f"🆕 Enhanced session started: {session_id}")
        return session_id
    
    async def load_session(self, session_id: str) -> bool:
        """Load existing conversation session"""
        try:
            session_file = self.session_storage / f"{session_id}.json"
            
            if not session_file.exists():
                return False
            
            async with aiofiles.open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.loads(await f.read())
            
            self.current_session = session_data
            logging.info(f"📂 Loaded session: {session_id}")
            return True
            
        except Exception as e:
            logging.error(f"❌ Error loading session {session_id}: {e}")
            return False
    
    async def list_sessions(self, console: Console, limit: int = 10):
        """List recent sessions with enhanced information"""
        try:
            session_files = sorted(
                self.session_storage.glob("session_*.json"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )[:limit]
            
            if not session_files:
                console.print("📂 [yellow]No sessions found.[/yellow]")
                return
            
            console.print(f"\n📂 [bold blue]Recent Sessions (last {limit}):[/bold blue]")
            
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("Session ID", style="cyan")
            table.add_column("Start Time", style="green")
            table.add_column("Queries", style="yellow")
            table.add_column("Avg Confidence", style="blue")
            table.add_column("User", style="white")
            
            for file in session_files:
                try:
                    async with aiofiles.open(file, 'r', encoding='utf-8') as f:
                        data = json.loads(await f.read())
                    
                    session_id = safe_get(data, "session_id", "Unknown")
                    start_time = safe_get(data, "start_time", "Unknown")
                    queries = safe_get(data, "performance_metrics", {}).get("total_queries", 0)
                    avg_confidence = safe_get(data, "performance_metrics", {}).get("avg_confidence", 0.0)
                    user_id = safe_get(data, "user_id", "anonymous")
                    
                    # Format timestamp
                    try:
                        dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                        formatted_time = dt.strftime("%Y-%m-%d %H:%M")
                    except:
                        formatted_time = start_time
                    
                    table.add_row(
                        session_id, 
                        formatted_time, 
                        str(queries), 
                        f"{avg_confidence:.2f}",
                        user_id
                    )
                    
                except Exception as e:
                    logging.error(f"❌ Error reading session file {file}: {e}")
            
            console.print(table)
            
        except Exception as e:
            console.print(f"❌ [red]Error listing sessions: {e}[/red]")
    
    async def add_enhanced_interaction(self, query: str, response: str, 
                                     search_state: AdvancedSearchState, 
                                     response_time: float):
        """Add enhanced interaction with semantic memory"""
        if not self.current_session:
            return
        
        interaction = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "query": query,
            "response": response,
            "response_time": response_time,
            "confidence": safe_get(search_state, "final_confidence", 0.0),
            "agent_path": safe_iterate(search_state.get("agent_history", [])),
            "total_documents": safe_get(search_state, "total_documents", 0),
            "query_analysis": safe_get(search_state, "query_analysis", {}),
            "retrieval_evaluations": safe_iterate(search_state.get("retrieval_evaluations", [])),
            "requires_feedback": safe_get(search_state, "requires_feedback", False)
        }
        
        # Add to conversation history
        history = safe_iterate(self.current_session.get("conversation_history", []))
        history.append(interaction)
        self.current_session["conversation_history"] = history
        
        # Update performance metrics
        await self._update_session_metrics(interaction)
        
        # Add to semantic memory if enabled
        if self.conversation_embeddings and self.config.ENABLE_AGENT_MEMORY:
            await self._add_to_semantic_memory(query, response, interaction)
        
        # Keep conversation history manageable
        if len(self.current_session["conversation_history"]) > self.config.MAX_CONVERSATION_HISTORY:
            await self._compress_conversation_history()
        
        await self._save_session(self.current_session)
    
    async def _update_session_metrics(self, interaction: Dict):
        """Update session performance metrics"""
        metrics = safe_get(self.current_session, "performance_metrics", {})
        current_count = safe_get(metrics, "total_queries", 0)
        
        # Update averages incrementally
        metrics["total_queries"] = current_count + 1
        metrics["avg_response_time"] = (
            (safe_get(metrics, "avg_response_time", 0.0) * current_count + safe_get(interaction, "response_time", 0.0)) / 
            metrics["total_queries"]
        )
        metrics["avg_confidence"] = (
            (safe_get(metrics, "avg_confidence", 0.0) * current_count + safe_get(interaction, "confidence", 0.0)) / 
            metrics["total_queries"]
        )
        
        self.current_session["performance_metrics"] = metrics
    
    async def _add_to_semantic_memory(self, query: str, response: str, interaction: Dict):
        """Add interaction to semantic memory for future retrieval"""
        try:
            # Create embedding for the interaction
            interaction_text = f"Query: {query}\nResponse: {response[:500]}"
            
            embedding = await self.conversation_embeddings.embed_query(interaction_text)
            
            memory_entry = {
                "timestamp": safe_get(interaction, "timestamp", ""),
                "query": query,
                "response_summary": response[:200] + "..." if len(response) > 200 else response,
                "embedding": embedding,
                "confidence": safe_get(interaction, "confidence", 0.7),
                "topics": safe_iterate(interaction.get("query_analysis", {}).get("key_concepts", []))
            }
            
            semantic_memory = safe_iterate(self.current_session.get("semantic_memory", []))
            semantic_memory.append(memory_entry)
            self.current_session["semantic_memory"] = semantic_memory
            
            # Keep semantic memory manageable (last 50 interactions)
            if len(self.current_session["semantic_memory"]) > 50:
                self.current_session["semantic_memory"] = self.current_session["semantic_memory"][-50:]
                
        except Exception as e:
            logging.warning(f"Semantic memory addition failed: {e}")
    
    async def get_relevant_conversation_context(self, current_query: str, max_context: int = 3) -> str:
        """Get relevant conversation context using semantic similarity"""
        if not self.current_session or not self.config.ENABLE_AGENT_MEMORY:
            return self._get_basic_conversation_context()
        
        try:
            semantic_memory = safe_iterate(self.current_session.get("semantic_memory", []))
            if not semantic_memory or not self.conversation_embeddings:
                return self._get_basic_conversation_context()
            
            # Get embedding for current query
            query_embedding = await self.conversation_embeddings.embed_query(current_query)
            
            # Calculate similarities
            similarities = []
            for i, memory in enumerate(semantic_memory):
                embedding = safe_get(memory, "embedding", [])
                if embedding:
                    try:
                        # Simple cosine similarity calculation
                        dot_product = sum(a * b for a, b in zip(query_embedding, embedding))
                        norm_a = sum(a * a for a in query_embedding) ** 0.5
                        norm_b = sum(b * b for b in embedding) ** 0.5
                        similarity = dot_product / (norm_a * norm_b) if norm_a * norm_b > 0 else 0.0
                        similarities.append((i, similarity, memory))
                    except:
                        continue
            
            # Sort by similarity and get top contexts
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            context_parts = []
            for i, (_, similarity, memory) in enumerate(similarities[:max_context]):
                if similarity > 0.3:  # Threshold for relevance
                    context_parts.append(
                        f"Previous Q: {safe_get(memory, 'query', '')}\n"
                        f"Previous A: {safe_get(memory, 'response_summary', '')}"
                    )
            
            return "\n\n".join(context_parts) if context_parts else self._get_basic_conversation_context()
            
        except Exception as e:
            logging.warning(f"Semantic context retrieval failed: {e}")
            return self._get_basic_conversation_context()
    
    def _get_basic_conversation_context(self) -> str:
        """Get basic conversation context from recent history"""
        if not self.current_session:
            return ""
        
        history = safe_iterate(self.current_session.get("conversation_history", []))
        if not history:
            return ""
        
        # Get last 3 interactions for basic context
        recent_history = history[-3:]
        context_parts = []
        
        for interaction in recent_history:
            query = safe_get(interaction, 'query', '')
            response = safe_get(interaction, 'response', '')
            context_parts.append(f"Q: {query}")
            context_parts.append(f"A: {response[:200]}...")
        
        return "\n".join(context_parts)
    
    async def _compress_conversation_history(self):
        """Compress old conversation history using summarization"""
        try:
            history = safe_iterate(self.current_session.get("conversation_history", []))
            if len(history) <= self.config.MAX_CONVERSATION_HISTORY:
                return
            
            # Keep recent interactions, summarize older ones
            recent_count = self.config.MAX_CONVERSATION_HISTORY // 2
            recent_history = history[-recent_count:]
            old_history = history[:-recent_count]
            
            # Create summary of old interactions
            old_interactions_text = "\n\n".join([
                f"Q: {safe_get(interaction, 'query', '')}\nA: {safe_get(interaction, 'response', '')[:300]}"
                for interaction in old_history[-10:]  # Summarize last 10 of the old ones
            ])
            
            summary_prompt = ChatPromptTemplate.from_messages([
                ("system", "Summarize the key topics, questions, and insights from this GDPR conversation history. Keep important details and context."),
                ("human", f"Conversation history to summarize:\n\n{old_interactions_text}")
            ])
            
            llm = ChatOpenAI(model=self.config.REASONING_MODEL, api_key=self.config.OPENAI_API_KEY)
            response = await llm.ainvoke(summary_prompt.format_messages())
            
            # Update session with compressed history
            self.current_session["conversation_summary"] = response.content.strip()
            self.current_session["conversation_history"] = recent_history
            
            logging.info(f"📝 Compressed conversation history: {len(old_history)} → summary + {len(recent_history)} recent")
            
        except Exception as e:
            logging.error(f"❌ Conversation compression error: {e}")
    
    async def _save_session(self, session_data: Dict):
        """Enhanced session saving with compression"""
        try:
            session_file = self.session_storage / f"{session_data['session_id']}.json"
            
            # Compress large sessions
            if self.config.ENABLE_CACHING:
                session_size = len(json.dumps(session_data))
                if session_size > 100000:  # 100KB threshold
                    await self._compress_session_data(session_data)
            
            async with aiofiles.open(session_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(session_data, indent=2, ensure_ascii=False, default=str))
                
        except Exception as e:
            logging.error(f"❌ Enhanced session save error: {e}")
    
    async def _compress_session_data(self, session_data: Dict):
        """Compress session data to reduce storage"""
        # Remove embeddings from semantic memory for storage
        semantic_memory = safe_iterate(session_data.get("semantic_memory", []))
        for memory in semantic_memory:
            if "embedding" in memory:
                del memory["embedding"]
        
        # Truncate very long responses in history
        conversation_history = safe_iterate(session_data.get("conversation_history", []))
        for interaction in conversation_history:
            response = safe_get(interaction, "response", "")
            if len(response) > 2000:
                interaction["response"] = response[:2000] + "... [truncated for storage]"


# ============================================================================
# MAIN ENHANCED SEARCH ENGINE APPLICATION - COMPLETE FIXED VERSION
# ============================================================================

class EnhancedGDPRSearchEngineCLI:
    """Next-generation GDPR search engine with advanced capabilities - COMPLETE FIXED"""
    
    def __init__(self):
        self.config = AdvancedSearchEngineConfig()
        self.console = Console()
        
        # Core components
        self.falkor_manager = None
        self.es_manager = None
        self.web_search_instance = None
        self.search_agents = None
        
        # Advanced components
        self.conversation_manager = None
        self.performance_metrics = None
        
        # Advanced RAG components
        self.corrective_rag = None
        self.self_reflective_rag = None
        self.adaptive_rag = None
    
    async def initialize(self):
        """Initialize all enhanced components"""
        try:
            self.console.print("🚀 [bold blue]Initializing Next-Generation GDPR Search Engine...[/bold blue]")
            
            # Validate configuration
            self.config.validate()
            self.console.print("✅ Enhanced configuration validated")
            
            # Initialize components with progress tracking
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:
                
                # Database connections (optional - graceful degradation)
                task1 = progress.add_task("🗄️ Connecting to Enhanced FalkorDB...", total=1)
                try:
                    self.falkor_manager = AdvancedFalkorDBManager(self.config)
                    progress.update(task1, completed=1)
                except Exception as e:
                    logging.warning(f"FalkorDB initialization failed: {e}")
                    self.falkor_manager = None
                    progress.update(task1, completed=1)
                
                task2 = progress.add_task("🔍 Connecting to Agentic Elasticsearch...", total=1)
                try:
                    self.es_manager = AdvancedElasticsearchManager(self.config)
                    progress.update(task2, completed=1)
                except Exception as e:
                    logging.warning(f"Elasticsearch initialization failed: {e}")
                    self.es_manager = None
                    progress.update(task2, completed=1)
                
                # Enhanced systems
                task3 = progress.add_task("🤖 Initializing Advanced Multi-Agent System...", total=1)
                self.search_agents = EnhancedGDPRSearchAgents(self.config)
                progress.update(task3, completed=1)
                
                task4 = progress.add_task("💬 Initializing Enhanced Conversation Manager...", total=1)
                self.conversation_manager = AdvancedConversationManager(self.config)
                progress.update(task4, completed=1)
                
                task5 = progress.add_task("📈 Setting up Performance Monitoring...", total=1)
                self.performance_metrics = PerformanceMetrics(self.config)
                progress.update(task5, completed=1)
                
                # Web search (if enabled)
                if self.config.WEB_SEARCH_ENABLED:
                    task6 = progress.add_task("🌐 Initializing Enterprise Web Search...", total=1)
                    self.web_search_instance = EnterpriseWebSearch(self.config)
                    progress.update(task6, completed=1)
            
            # Set global instances for tool injection
            global falkor_manager, es_manager, web_search_instance
            falkor_manager = self.falkor_manager
            es_manager = self.es_manager
            web_search_instance = self.web_search_instance
            
            # Initialize advanced RAG components
            self.search_agents.initialize_advanced_rag(
                self.falkor_manager, 
                self.es_manager, 
                self.web_search_instance
            )
            
            self.console.print("🎉 [bold green]Next-generation system initialization complete![/bold green]")
            self.console.print(f"🔧 [cyan]Enabled features: CRAG={self.config.ENABLE_CRAG}, Self-RAG={self.config.ENABLE_SELF_RAG}, Adaptive={self.config.ENABLE_ADAPTIVE_RAG}[/cyan]")
            
            return True
            
        except Exception as e:
            self.console.print(f"❌ [bold red]Initialization failed: {e}[/bold red]")
            logging.error(f"❌ Initialization error: {e}")
            traceback.print_exc()
            return False
    
    async def run_enhanced_interactive_mode(self):
        """Run enhanced interactive mode with advanced features"""
        
        self.console.print("\n" + "="*90)
        self.console.print("🔍 [bold blue]Next-Generation GDPR Search Engine - Enhanced Interactive Mode[/bold blue]")
        self.console.print("="*90)
        
        # Start enhanced session
        session_id = await self.conversation_manager.start_enhanced_session()
        self.console.print(f"📝 Enhanced session started: [cyan]{session_id}[/cyan]")
        
        # Display enhanced commands
        self._display_enhanced_commands()
        
        self.console.print("\n💬 [bold]Ask your GDPR questions or use advanced commands above.[/bold]")
        self.console.print("💡 [italic]Pro tip: Try complex queries - the system uses advanced reasoning and multiple sources![/italic]")
        
        while True:
            try:
                # Get user input with enhanced prompt
                query = Prompt.ask(
                    "\n🤔 [bold cyan]Your GDPR question[/bold cyan]",
                    default="",
                    show_default=False
                )
                
                if not query.strip():
                    continue
                
                # Handle enhanced commands
                if query.startswith("/"):
                    await self._handle_enhanced_command(query)
                    continue
                
                # Process enhanced search query
                await self._process_enhanced_search_query(query, session_id)
                
            except KeyboardInterrupt:
                self.console.print("\n👋 [yellow]Thank you for using the enhanced GDPR search engine![/yellow]")
                
                # Display session summary
                await self._display_session_summary(session_id)
                break
                
            except Exception as e:
                self.console.print(f"❌ [red]Error: {e}[/red]")
                logging.error(f"❌ Enhanced interactive mode error: {e}")
                traceback.print_exc()
    
    def _display_enhanced_commands(self):
        """Display enhanced command options"""
        commands_table = Table(show_header=True, header_style="bold magenta", title="Enhanced Commands")
        commands_table.add_column("Command", style="cyan", width=25)
        commands_table.add_column("Description", style="white", width=50)
        
        commands = [
            ("/help", "Show this enhanced help message"),
            ("/sessions", "List recent sessions with metrics"),
            ("/load <session_id>", "Load previous session with context"),
            ("/performance", "View system performance metrics"),
            ("/clear", "Start new session"),
            ("/debug", "Toggle debug mode"),
            ("/quit", "Exit the application")
        ]
        
        for cmd, desc in commands:
            commands_table.add_row(cmd, desc)
        
        self.console.print(commands_table)
    
    async def _handle_enhanced_command(self, command: str):
        """Handle enhanced CLI commands"""
        parts = command.split()
        cmd = parts[0].lower()
        
        if cmd == "/help":
            self._display_enhanced_commands()
            
        elif cmd == "/sessions":
            await self.conversation_manager.list_sessions(self.console)
            
        elif cmd == "/load" and len(parts) > 1:
            session_id = parts[1]
            if await self.conversation_manager.load_session(session_id):
                self.console.print(f"✅ [green]Loaded session: {session_id}[/green]")
            else:
                self.console.print(f"❌ [red]Session not found: {session_id}[/red]")
            
        elif cmd == "/performance":
            await self._show_performance_metrics()
            
        elif cmd == "/clear":
            session_id = await self.conversation_manager.start_enhanced_session()
            self.console.print(f"🆕 [green]New enhanced session started: {session_id}[/green]")
            
        elif cmd == "/debug":
            current_level = logging.getLogger().level
            new_level = logging.DEBUG if current_level != logging.DEBUG else logging.INFO
            logging.getLogger().setLevel(new_level)
            level_name = "DEBUG" if new_level == logging.DEBUG else "INFO"
            self.console.print(f"🐛 [yellow]Debug mode: {level_name}[/yellow]")
            
        elif cmd == "/quit":
            raise KeyboardInterrupt()
            
        else:
            self.console.print(f"❓ [yellow]Unknown command: {command}[/yellow]")
            self._display_enhanced_commands()
    
    async def _process_enhanced_search_query(self, query: str, session_id: str):
        """Process enhanced search query with advanced multi-agent system"""
        
        start_time = asyncio.get_event_loop().time()
        
        self.console.print(f"\n🔍 [bold]Processing with advanced AI: [cyan]{query}[/cyan][/bold]")
        
        # Get relevant conversation context
        conversation_context = await self.conversation_manager.get_relevant_conversation_context(query)
        
        # Create enhanced initial state
        initial_state = AdvancedSearchState(
            messages=[HumanMessage(content=query)],
            query=query,
            query_analysis=None,
            routing_decision=None,
            graph_results=None,
            vector_results=None,
            web_results=None,
            corrective_results=None,
            self_rag_results=None,
            adaptive_routing=None,
            retrieval_evaluations=[],
            confidence_scores={},
            reflection_steps=[],
            synthesized_response=None,
            final_confidence=None,
            requires_feedback=False,
            active_agent=None,
            agent_history=[],
            iteration_count=0,
            search_latency={},
            total_documents=0
        )
        
        try:
            # Process with enhanced multi-agent system
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=self.console
            ) as progress:
                
                search_task = progress.add_task("🤖 Advanced multi-agent processing...", total=1)
                
                # Run the enhanced agent graph
                config = {"configurable": {"thread_id": session_id}}
                result_state = await self.search_agents.search_graph.ainvoke(
                    initial_state,
                    config=config
                )
                
                progress.update(search_task, completed=1)
            
            # Calculate total response time
            end_time = asyncio.get_event_loop().time()
            total_response_time = end_time - start_time
            
            # Display enhanced results
            await self._display_enhanced_search_results(result_state, query, session_id, total_response_time)
            
            # Record metrics and add to conversation
            await self.performance_metrics.record_search_metrics(
                session_id, query, result_state, total_response_time
            )
            
            await self.conversation_manager.add_enhanced_interaction(
                query, 
                safe_get(result_state, "synthesized_response", "No response generated"),
                result_state,
                total_response_time
            )
            
        except Exception as e:
            self.console.print(f"❌ [red]Enhanced search processing error: {e}[/red]")
            logging.error(f"❌ Enhanced search processing error: {e}")
            traceback.print_exc()
    
    async def _display_enhanced_search_results(self, result_state: AdvancedSearchState, 
                                            query: str, session_id: str, response_time: float):
        """Display comprehensive enhanced search results"""
        
        self.console.print("\n" + "="*90)
        self.console.print("📋 [bold blue]Enhanced Search Results[/bold blue]")
        self.console.print("="*90)
        
        # Performance metrics
        confidence = safe_get(result_state, "final_confidence", 0.0)
        agent_path = safe_iterate(result_state.get("agent_history", []))
        total_docs = safe_get(result_state, "total_documents", 0)
        
        metrics_table = Table(title="Performance Metrics", show_header=True, header_style="bold cyan")
        metrics_table.add_column("Metric", style="cyan")
        metrics_table.add_column("Value", style="green")
        
        metrics_table.add_row("Response Time", f"{response_time:.2f}s")
        metrics_table.add_row("Confidence Score", f"{confidence:.2f}")
        metrics_table.add_row("Agent Path", " → ".join(agent_path))
        metrics_table.add_row("Total Sources", str(total_docs))
        metrics_table.add_row("Iterations", str(safe_get(result_state, "iteration_count", 0)))
        
        self.console.print(metrics_table)
        
        # Query analysis
        query_analysis = safe_get(result_state, "query_analysis")
        if query_analysis:
            self.console.print(f"\n🔍 [bold]Query Analysis:[/bold]")
            intent = safe_get(query_analysis, 'intent', 'unknown')
            complexity = safe_get(query_analysis, 'complexity', 'moderate')
            domain = safe_get(query_analysis, 'domain', 'general')
            reasoning = safe_get(query_analysis, 'reasoning', 'direct')
            key_concepts = safe_iterate(query_analysis.get('key_concepts', []))
            
            self.console.print(f"• Type: {intent} ({complexity} complexity)")
            self.console.print(f"• Domain: {domain}")
            self.console.print(f"• Reasoning: {reasoning}")
            if key_concepts:
                self.console.print(f"• Key concepts: {', '.join(key_concepts)}")
        
        # Main response
        synthesized_response = safe_get(result_state, "synthesized_response")
        if synthesized_response:
            self.console.print(f"\n💬 [bold blue]Response:[/bold blue]")
            response_panel = Panel(
                Markdown(synthesized_response),
                title="🤖 Enhanced GDPR Assistant Response",
                border_style="blue"
            )
            self.console.print(response_panel)
        
        # Smart feedback prompt based on confidence and complexity
        await self._smart_feedback_prompt(result_state, query, session_id)
    
    async def _smart_feedback_prompt(self, result_state: AdvancedSearchState, 
                                   query: str, session_id: str):
        """Smart feedback prompting based on result characteristics"""
        
        confidence = safe_get(result_state, "final_confidence", 0.7)
        requires_feedback = safe_get(result_state, "requires_feedback", False)
        agent_count = len(safe_iterate(result_state.get("agent_history", [])))
        
        # Determine feedback likelihood based on various factors
        should_request_feedback = (
            requires_feedback or 
            confidence < 0.7 or 
            agent_count > 4 or
            safe_get(result_state, "iteration_count", 0) > 2
        )
        
        if should_request_feedback:
            # Customize feedback request based on situation
            if confidence < 0.6:
                prompt_text = "🔄 This response had low confidence - your feedback would be very valuable!"
            elif agent_count > 5:
                prompt_text = "🤖 This query used advanced multi-agent coordination - how did we do?"
            elif safe_get(result_state, "corrective_results", {}).get("correction_steps"):
                prompt_text = "🔧 We applied quality corrections - was the final result better?"
            else:
                prompt_text = "📝 Would you like to provide feedback to help improve future responses?"
            
            request_feedback = Confirm.ask(f"\n{prompt_text}", default=False)
            
            if request_feedback:
                self.console.print("📝 [green]Thank you for your willingness to provide feedback![/green]")
                self.console.print("💡 [cyan]Feedback collection system would be implemented here.[/cyan]")
    
    async def _show_performance_metrics(self):
        """Show system performance metrics"""
        try:
            summary = await self.performance_metrics.get_performance_summary()
            
            self.console.print("\n" + "="*70)
            self.console.print("📈 [bold blue]System Performance Metrics[/bold blue]")
            self.console.print("="*70)
            
            if "error" in summary:
                self.console.print(f"❌ [red]Error: {summary['error']}[/red]")
                return
            
            if "message" in summary:
                self.console.print(f"📊 [yellow]{summary['message']}[/yellow]")
                return
            
            # Performance table
            perf_table = Table(show_header=True, header_style="bold magenta")
            perf_table.add_column("Metric", style="cyan")
            perf_table.add_column("Value", style="green")
            
            perf_table.add_row("Total Queries", str(safe_get(summary, "total_queries", 0)))
            perf_table.add_row("Avg Response Time", f"{safe_get(summary, 'avg_response_time', 0):.2f}s")
            perf_table.add_row("Avg Confidence", f"{safe_get(summary, 'avg_confidence', 0):.2f}")
            perf_table.add_row("Avg Documents", f"{safe_get(summary, 'avg_documents', 0):.1f}")
            perf_table.add_row("Feedback Rate", f"{safe_get(summary, 'feedback_rate', 0):.1f}%")
            
            self.console.print(perf_table)
            
            # Agent usage
            agent_usage = safe_get(summary, "agent_usage", {})
            if agent_usage:
                self.console.print(f"\n🤖 [bold cyan]Agent Usage[/bold cyan]")
                for agent, count in sorted(agent_usage.items(), key=lambda x: x[1], reverse=True):
                    self.console.print(f"• {agent}: {count} times")
            
            # Routing patterns
            routing_patterns = safe_get(summary, "routing_patterns", {})
            if routing_patterns:
                self.console.print(f"\n🧭 [bold cyan]Routing Patterns[/bold cyan]")
                for pattern, count in sorted(routing_patterns.items(), key=lambda x: x[1], reverse=True):
                    self.console.print(f"• {pattern}: {count} times")
                    
        except Exception as e:
            self.console.print(f"❌ [red]Error showing performance metrics: {e}[/red]")
    
    async def _display_session_summary(self, session_id: str):
        """Display session summary on exit"""
        try:
            summary = await self.performance_metrics.get_performance_summary(session_id)
            
            if summary and "total_queries" in summary:
                self.console.print("\n" + "="*60)
                self.console.print("📊 [bold blue]Session Summary[/bold blue]")
                self.console.print("="*60)
                
                self.console.print(f"• Queries processed: {summary['total_queries']}")
                self.console.print(f"• Average response time: {summary['avg_response_time']:.2f}s")
                self.console.print(f"• Average confidence: {summary['avg_confidence']:.2f}")
                self.console.print(f"• Feedback provided: {summary['feedback_rate']:.1f}%")
                
        except Exception as e:
            logging.error(f"Session summary error: {e}")
    
    async def run_simple_test(self):
        """Run a simple test query"""
        if not await self.initialize():
            return
        
        query = "What is GDPR?"
        self.console.print(f"\n🔍 [bold]Testing query: [cyan]{query}[/cyan][/bold]")
        
        # Create simple initial state
        initial_state = AdvancedSearchState(
            messages=[HumanMessage(content=query)],
            query=query,
            query_analysis=None,
            routing_decision=None,
            graph_results=None,
            vector_results=None,
            web_results=None,
            corrective_results=None,
            self_rag_results=None,
            adaptive_routing=None,
            retrieval_evaluations=[],
            confidence_scores={},
            reflection_steps=[],
            synthesized_response=None,
            final_confidence=None,
            requires_feedback=False,
            active_agent=None,
            agent_history=[],
            iteration_count=0,
            search_latency={},
            total_documents=0
        )
        
        try:
            # Run the agent graph
            config = {"configurable": {"thread_id": "test_session"}}
            result_state = await self.search_agents.search_graph.ainvoke(
                initial_state,
                config=config
            )
            
            # Display results
            response = safe_get(result_state, "synthesized_response", "No response generated")
            confidence = safe_get(result_state, "final_confidence", 0.0)
            
            self.console.print(f"\n💬 [bold blue]Response (Confidence: {confidence:.2f}):[/bold blue]")
            self.console.print(Panel(response, title="🤖 GDPR Assistant", border_style="blue"))
            
        except Exception as e:
            self.console.print(f"❌ [red]Test failed: {e}[/red]")
            logging.error(f"❌ Test error: {e}")
            traceback.print_exc()

# ============================================================================
# ENHANCED CLI COMMAND INTERFACE - COMPLETE FIXED
# ============================================================================

@click.group()
@click.option('--debug', is_flag=True, help='Enable debug logging')
@click.option('--config-file', help='Path to configuration file')
def cli(debug, config_file):
    """Next-Generation GDPR Search Engine with Advanced RAG & Multi-Agent Architecture - FIXED"""
    if debug:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    if config_file:
        # Load configuration from file if specified
        logging.info(f"Loading configuration from {config_file}")

@cli.command()
@click.option('--session-id', help='Load specific session')
@click.option('--user-id', help='User identifier for personalization')
@click.option('--enable-crag/--disable-crag', default=True, help='Enable/disable Corrective RAG')
@click.option('--enable-self-rag/--disable-self-rag', default=True, help='Enable/disable Self-Reflective RAG')
@click.option('--reasoning-effort', type=click.Choice(['low', 'medium', 'high']), default='medium', help='Reasoning effort level')
def interactive(session_id, user_id, enable_crag, enable_self_rag, reasoning_effort):
    """Start enhanced interactive search mode with advanced features"""
    async def run_enhanced_interactive():
        engine = EnhancedGDPRSearchEngineCLI()
        
        # Override config with CLI options
        engine.config.ENABLE_CRAG = enable_crag
        engine.config.ENABLE_SELF_RAG = enable_self_rag
        engine.config.REASONING_EFFORT = reasoning_effort
        
        if await engine.initialize():
            if session_id:
                success = await engine.conversation_manager.load_session(session_id)
                if success:
                    engine.console.print(f"✅ [green]Loaded session: {session_id}[/green]")
                else:
                    engine.console.print(f"❌ [red]Session not found: {session_id}[/red]")
            
            await engine.run_enhanced_interactive_mode()
        else:
            engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
    
    asyncio.run(run_enhanced_interactive())

@cli.command()
@click.argument('query')
@click.option('--reasoning-effort', type=click.Choice(['low', 'medium', 'high']), default='medium', help='Reasoning effort level')
@click.option('--enable-web/--disable-web', default=True, help='Enable/disable web search')
def search(query, reasoning_effort, enable_web):
    """Perform a single enhanced search query"""
    async def run_enhanced_search():
        engine = EnhancedGDPRSearchEngineCLI()
        
        # Override config
        engine.config.REASONING_EFFORT = reasoning_effort
        engine.config.WEB_SEARCH_ENABLED = enable_web
        
        if not await engine.initialize():
            engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
            return
        
        # Start session for single query
        session_id = await engine.conversation_manager.start_enhanced_session()
        
        # Process query
        await engine._process_enhanced_search_query(query, session_id)
    
    asyncio.run(run_enhanced_search())

@cli.command()
@click.option('--limit', default=10, help='Number of sessions to show')
@click.option('--show-metrics/--hide-metrics', default=True, help='Show performance metrics')
def sessions(limit, show_metrics):
    """List recent conversation sessions with enhanced information"""
    async def list_enhanced_sessions():
        config = AdvancedSearchEngineConfig()
        conversation_manager = AdvancedConversationManager(config)
        console = Console()
        
        await conversation_manager.list_sessions(console, limit)
        
        if show_metrics:
            performance_metrics = PerformanceMetrics(config)
            summary = await performance_metrics.get_performance_summary()
            
            if summary and "total_queries" in summary:
                console.print(f"\n📊 [bold cyan]Today's Performance[/bold cyan]")
                console.print(f"• Total queries: {summary['total_queries']}")
                console.print(f"• Average response time: {summary['avg_response_time']:.2f}s")
                console.print(f"• Average confidence: {summary['avg_confidence']:.2f}")
    
    asyncio.run(list_enhanced_sessions())

@cli.command()
@click.option('--session-id', help='Show metrics for specific session')
def metrics(session_id):
    """View system performance metrics and analytics"""
    async def show_enhanced_metrics():
        config = AdvancedSearchEngineConfig()
        performance_metrics = PerformanceMetrics(config)
        console = Console()
        
        summary = await performance_metrics.get_performance_summary(session_id)
        
        if "error" in summary:
            console.print(f"❌ [red]Error: {summary['error']}[/red]")
            return
        
        console.print("\n📈 [bold blue]Performance Analytics[/bold blue]")
        
        # Display comprehensive metrics
        if "total_queries" in summary:
            metrics_table = Table(show_header=True, header_style="bold magenta")
            metrics_table.add_column("Metric", style="cyan")
            metrics_table.add_column("Value", style="green")
            
            for key, value in summary.items():
                if isinstance(value, (int, float)):
                    if isinstance(value, float):
                        metrics_table.add_row(key.replace('_', ' ').title(), f"{value:.2f}")
                    else:
                        metrics_table.add_row(key.replace('_', ' ').title(), str(value))
            
            console.print(metrics_table)
    
    asyncio.run(show_enhanced_metrics())

@cli.command()
def test():
    """Run system test with sample query"""
    async def run_test():
        engine = EnhancedGDPRSearchEngineCLI()
        await engine.run_simple_test()
    
    asyncio.run(run_test())

@cli.command()
def benchmark():
    """Run system benchmark and performance tests"""
    async def run_benchmark():
        console = Console()
        console.print("🚀 [bold blue]Running Enhanced System Benchmark[/bold blue]")
        
        engine = EnhancedGDPRSearchEngineCLI()
        
        if not await engine.initialize():
            console.print("❌ [red]Failed to initialize for benchmark[/red]")
            return
        
        # Test queries of varying complexity
        test_queries = [
            "What is GDPR?",  # Simple
            "How to implement data subject rights in practice?",  # Moderate
            "Compare GDPR compliance requirements for controllers vs processors in cross-border data transfers",  # Complex
        ]
        
        session_id = await engine.conversation_manager.start_enhanced_session()
        
        benchmark_results = []
        
        for i, query in enumerate(test_queries, 1):
            console.print(f"\n🧪 Test {i}/3: {query[:50]}...")
            
            start_time = asyncio.get_event_loop().time()
            await engine._process_enhanced_search_query(query, session_id)
            end_time = asyncio.get_event_loop().time()
            
            response_time = end_time - start_time
            benchmark_results.append({
                "query": query,
                "response_time": response_time,
                "complexity": ["Simple", "Moderate", "Complex"][i-1]
            })
        
        # Display benchmark results
        console.print("\n📊 [bold blue]Benchmark Results[/bold blue]")
        
        benchmark_table = Table(show_header=True, header_style="bold magenta")
        benchmark_table.add_column("Complexity", style="cyan")
        benchmark_table.add_column("Response Time", style="green")
        benchmark_table.add_column("Query Preview", style="white")
        
        for result in benchmark_results:
            benchmark_table.add_row(
                result["complexity"],
                f"{result['response_time']:.2f}s",
                result["query"][:40] + "..."
            )
        
        console.print(benchmark_table)
        
        avg_time = sum(r["response_time"] for r in benchmark_results) / len(benchmark_results)
        console.print(f"\n⚡ Average response time: {avg_time:.2f}s")
    
    asyncio.run(run_benchmark())

if __name__ == "__main__":
    # Enhanced entry point with better error handling
    import sys
    
    try:
        if len(sys.argv) == 1:
            # No arguments, run enhanced interactive mode by default
            async def main():
                engine = EnhancedGDPRSearchEngineCLI()
                if await engine.initialize():
                    await engine.run_enhanced_interactive_mode()
                else:
                    engine.console.print("❌ [red]Failed to initialize enhanced search engine[/red]")
                    sys.exit(1)
            
            asyncio.run(main())
        else:
            # Run CLI commands
            cli()
    except KeyboardInterrupt:
        print("\n👋 Goodbye!")
    except Exception as e:
        console = Console()
        console.print(f"❌ [red]Fatal error: {e}[/red]")
        logging.error(f"Fatal error: {e}")
        traceback.print_exc()
        sys.exit(1)
