#!/usr/bin/env python3
"""
GDPR Dynamic Multi-Agent Knowledge System - Fixed Version
=========================================================

A corrected multi-agent system for creating GDPR knowledge graphs using:
- LangGraph multi-agent architecture with proper StateGraph
- Dynamic article discovery and analysis
- PyMuPDF (not fitz) for advanced PDF processing
- Elasticsearch 8.13 with latest client features
- FalkorDB for labeled property graphs with OpenCypher
- OpenAI with proper reasoning_effort parameter

Author: AI Assistant
Created: June 2025
Fixed: Based on latest documentation
"""

import os
import json
import logging
import asyncio
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, Literal
from dataclasses import dataclass, field
from pathlib import Path

# Core libraries
import pandas as pd
import numpy as np
from tqdm import tqdm

# Document processing with PyMuPDF (correct import)
import pymupdf  # Not fitz - this is the correct modern import

# LangChain and LangGraph - latest syntax
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph for multi-agent architecture - correct imports
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict, Annotated

# Elasticsearch 8.13 client
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# FalkorDB for labeled property graphs
from falkordb import FalkorDB

# Async support
import aiofiles

# ============================================================================
# GLOBAL CONFIGURATION
# ============================================================================

class Config:
    """Dynamic configuration with proper parameter names"""
    
    # OpenAI Configuration - Updated for latest API
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    REASONING_MODEL: str = "o3-mini"
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    REASONING_EFFORT: str = "high"  # low, medium, high
    
    # Elasticsearch 8.13 Configuration
    ELASTICSEARCH_HOST: str = os.getenv("ES_HOST", "localhost")
    ELASTICSEARCH_PORT: int = int(os.getenv("ES_PORT", "9200"))
    ELASTICSEARCH_USERNAME: str = os.getenv("ES_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD: str = os.getenv("ES_PASSWORD")
    ELASTICSEARCH_INDEX: str = "gdpr_knowledge_base"
    ELASTICSEARCH_CERT_PATH: str = os.getenv("ES_CERT_PATH", "")
    
    # FalkorDB Configuration
    FALKORDB_HOST: str = os.getenv("FALKOR_HOST", "localhost")
    FALKORDB_PORT: int = int(os.getenv("FALKOR_PORT", "6379"))
    FALKORDB_PASSWORD: str = os.getenv("FALKOR_PASSWORD", "")
    FALKORDB_GRAPH_NAME: str = "gdpr_knowledge_graph"
    
    # Document Processing
    CHUNK_SIZE: int = 1500
    CHUNK_OVERLAP: int = 300
    
    # File Paths
    DOCUMENTS_PATH: Path = Path(os.getenv("DOCS_PATH", "./documents"))
    OUTPUT_PATH: Path = Path(os.getenv("OUTPUT_PATH", "./output"))
    
    @classmethod
    def validate(cls):
        """Validate configuration"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")

# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class AgentState(TypedDict):
    """State shared between agents using proper TypedDict"""
    documents: List[Document]
    extracted_articles: Dict[str, Any]
    concepts: List[Dict[str, Any]]
    knowledge_graph_stats: Dict[str, Any]
    vector_index_stats: Dict[str, Any]
    current_agent: str
    iteration_count: int
    errors: List[str]
    metadata: Dict[str, Any]

@dataclass
class GDPRArticle:
    """Dynamic GDPR Article structure"""
    article_id: str
    number: Union[int, str]
    title: str
    content: str
    regulation_type: str  # "GDPR" or "UK_GDPR"
    key_concepts: List[str]
    obligations: List[str]
    rights: List[str]
    source_document: str
    confidence_score: float

@dataclass
class GDPRConcept:
    """GDPR Concept structure"""
    concept_id: str
    label: str
    definition: str
    category: str
    article_references: List[str]
    regulation_type: str
    related_concepts: List[str]
    source_document: str
    confidence_score: float

# ============================================================================
# ADVANCED PDF PROCESSOR WITH PYMUPDF
# ============================================================================

class AdvancedPDFProcessor:
    """Advanced PDF processing using PyMuPDF (not fitz)"""
    
    def __init__(self, config: Config):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_pdf(self, file_path: Path) -> Dict[str, Any]:
        """Process PDF with PyMuPDF (correct syntax)"""
        try:
            # Open document with pymupdf (not fitz)
            doc = pymupdf.open(str(file_path))
            
            doc_info = {
                "file_path": str(file_path),
                "metadata": doc.metadata,
                "page_count": doc.page_count,
                "pages": [],
                "full_text": "",
                "document_type": self._classify_document(file_path.name)
            }
            
            # Process each page
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                
                # Extract text with proper pymupdf syntax
                text = page.get_text()
                
                # Extract tables if available
                try:
                    tables = page.find_tables()
                    table_data = [table.extract() for table in tables]
                except:
                    table_data = []
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": text,
                    "tables": table_data,
                    "word_count": len(text.split())
                }
                
                doc_info["pages"].append(page_info)
                doc_info["full_text"] += text + "\n"
            
            # Create document chunks
            chunks = await self._create_chunks(doc_info)
            doc_info["chunks"] = chunks
            
            doc.close()
            return doc_info
            
        except Exception as e:
            logging.error(f"Error processing PDF {file_path}: {e}")
            return {"error": str(e), "file_path": str(file_path)}
    
    def _classify_document(self, filename: str) -> str:
        """Classify document type"""
        filename_lower = filename.lower()
        
        if "gdpr" in filename_lower and "uk" not in filename_lower:
            return "GDPR_REGULATION"
        elif "uk" in filename_lower and "gdpr" in filename_lower:
            return "UK_GDPR_REGULATION"
        elif any(term in filename_lower for term in ["business", "company", "internal"]):
            return "BUSINESS_DOCUMENT"
        else:
            return "UNKNOWN"
    
    async def _create_chunks(self, doc_info: Dict[str, Any]) -> List[Document]:
        """Create document chunks with metadata"""
        chunks = []
        full_text = doc_info["full_text"]
        
        text_chunks = self.text_splitter.split_text(full_text)
        
        for i, chunk_text in enumerate(text_chunks):
            metadata = {
                "source": doc_info["file_path"],
                "chunk_id": f"chunk_{i}",
                "chunk_index": i,
                "document_type": doc_info["document_type"],
                "total_chunks": len(text_chunks),
                "extracted_at": datetime.now().isoformat()
            }
            
            chunk_doc = Document(
                page_content=chunk_text,
                metadata=metadata
            )
            chunks.append(chunk_doc)
        
        return chunks

# ============================================================================
# ELASTICSEARCH 8.13 MANAGER
# ============================================================================

class ElasticsearchManager:
    """Elasticsearch 8.13 manager with correct client syntax"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup Elasticsearch 8.13 client"""
        try:
            # Correct Elasticsearch 8.13 connection
            self.client = Elasticsearch(
                hosts=[f"http://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"],
                basic_auth=(self.config.ELASTICSEARCH_USERNAME, self.config.ELASTICSEARCH_PASSWORD),
                verify_certs=False,
                request_timeout=30,
                max_retries=3,
                retry_on_timeout=True
            )
            
            # Test connection
            if self.client.ping():
                logging.info("‚úÖ Connected to Elasticsearch 8.13")
            else:
                logging.error("‚ùå Failed to connect to Elasticsearch")
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"‚ùå Elasticsearch connection error: {e}")
            raise
    
    async def create_index(self):
        """Create index with 8.13 mappings"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "content": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "embeddings": {
                        "type": "dense_vector",
                        "dims": 3072,  # text-embedding-3-large dimensions
                        "index": True,
                        "similarity": "cosine"
                    },
                    "document_type": {"type": "keyword"},
                    "regulation_type": {"type": "keyword"},
                    "article_reference": {"type": "keyword"},
                    "concepts": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "source_file": {"type": "keyword"},
                    "extracted_at": {"type": "date"}
                }
            }
        }
        
        try:
            if self.client.indices.exists(index=self.config.ELASTICSEARCH_INDEX):
                self.client.indices.delete(index=self.config.ELASTICSEARCH_INDEX)
            
            self.client.indices.create(
                index=self.config.ELASTICSEARCH_INDEX,
                body=mapping
            )
            logging.info(f"‚úÖ Created Elasticsearch index: {self.config.ELASTICSEARCH_INDEX}")
            
        except Exception as e:
            logging.error(f"‚ùå Error creating index: {e}")
            raise
    
    async def bulk_index_documents(self, documents: List[Dict[str, Any]]):
        """Bulk index documents"""
        try:
            actions = []
            for doc in documents:
                action = {
                    "_index": self.config.ELASTICSEARCH_INDEX,
                    "_id": doc.get("document_id", str(uuid.uuid4())),
                    "_source": doc
                }
                actions.append(action)
            
            success, failed = bulk(self.client, actions)
            logging.info(f"‚úÖ Indexed {success} documents")
            
            return {"success": success, "failed": len(failed)}
            
        except Exception as e:
            logging.error(f"‚ùå Error bulk indexing: {e}")
            raise
    
    async def vector_search(self, query_embedding: List[float], k: int = 10) -> List[Dict]:
        """Vector similarity search using Elasticsearch 8.13"""
        try:
            query = {
                "knn": {
                    "field": "embeddings",
                    "query_vector": query_embedding,
                    "k": k,
                    "num_candidates": k * 2
                },
                "_source": {
                    "excludes": ["embeddings"]
                }
            }
            
            response = self.client.search(
                index=self.config.ELASTICSEARCH_INDEX,
                body=query,
                size=k
            )
            
            return response["hits"]["hits"]
            
        except Exception as e:
            logging.error(f"‚ùå Error in vector search: {e}")
            return []

# ============================================================================
# FALKORDB MANAGER FOR LABELED PROPERTY GRAPHS
# ============================================================================

class FalkorDBManager:
    """FalkorDB manager for labeled property graphs with OpenCypher"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self.graph = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup FalkorDB client"""
        try:
            # Correct FalkorDB connection syntax
            connection_params = {
                "host": self.config.FALKORDB_HOST,
                "port": self.config.FALKORDB_PORT,
                "decode_responses": True
            }
            
            if self.config.FALKORDB_PASSWORD:
                connection_params["password"] = self.config.FALKORDB_PASSWORD
            
            self.client = FalkorDB(**connection_params)
            self.graph = self.client.select_graph(self.config.FALKORDB_GRAPH_NAME)
            
            # Test connection with simple OpenCypher query
            self.graph.query("RETURN 1 as test")
            logging.info("‚úÖ Connected to FalkorDB")
            
        except Exception as e:
            logging.error(f"‚ùå FalkorDB connection error: {e}")
            raise
    
    async def create_schema(self):
        """Create labeled property graph schema with OpenCypher"""
        schema_queries = [
            # Create indexes for better performance
            "CREATE INDEX ON :GDPRArticle(article_number)",
            "CREATE INDEX ON :UKGDPRArticle(article_number)",
            "CREATE INDEX ON :Concept(concept_id)",
            "CREATE INDEX ON :Document(document_id)",
            
            # Create constraints for uniqueness
            "CREATE CONSTRAINT ON (a:GDPRArticle) ASSERT a.article_id IS UNIQUE",
            "CREATE CONSTRAINT ON (c:Concept) ASSERT c.concept_id IS UNIQUE",
            "CREATE CONSTRAINT ON (d:Document) ASSERT d.document_id IS UNIQUE",
        ]
        
        for query in schema_queries:
            try:
                self.graph.query(query)
                logging.info(f"‚úÖ Executed schema query: {query[:50]}...")
            except Exception as e:
                # Indexes/constraints might already exist
                logging.debug(f"Schema query note: {e}")
    
    async def create_article_node(self, article: GDPRArticle) -> bool:
        """Create article node using OpenCypher"""
        try:
            # Determine node label based on regulation type
            label = "GDPRArticle" if article.regulation_type == "GDPR" else "UKGDPRArticle"
            
            # Escape strings for Cypher
            title_escaped = article.title.replace("'", "\\'")
            content_escaped = article.content[:1000].replace("'", "\\'")  # Limit content length
            
            query = f"""
            CREATE (a:{label} {{
                article_id: '{article.article_id}',
                article_number: '{article.number}',
                title: '{title_escaped}',
                content: '{content_escaped}',
                regulation_type: '{article.regulation_type}',
                key_concepts: {json.dumps(article.key_concepts)},
                obligations: {json.dumps(article.obligations)},
                rights: {json.dumps(article.rights)},
                source_document: '{article.source_document}',
                confidence_score: {article.confidence_score},
                created_at: datetime()
            }})
            RETURN a.article_id as id
            """
            
            result = self.graph.query(query)
            return len(result.result_set) > 0
            
        except Exception as e:
            logging.error(f"‚ùå Error creating article node: {e}")
            return False
    
    async def create_concept_node(self, concept: GDPRConcept) -> bool:
        """Create concept node using OpenCypher"""
        try:
            # Escape strings for Cypher
            label_escaped = concept.label.replace("'", "\\'")
            definition_escaped = concept.definition.replace("'", "\\'")
            
            query = f"""
            CREATE (c:Concept {{
                concept_id: '{concept.concept_id}',
                label: '{label_escaped}',
                definition: '{definition_escaped}',
                category: '{concept.category}',
                article_references: {json.dumps(concept.article_references)},
                regulation_type: '{concept.regulation_type}',
                related_concepts: {json.dumps(concept.related_concepts)},
                source_document: '{concept.source_document}',
                confidence_score: {concept.confidence_score},
                created_at: datetime()
            }})
            RETURN c.concept_id as id
            """
            
            result = self.graph.query(query)
            return len(result.result_set) > 0
            
        except Exception as e:
            logging.error(f"‚ùå Error creating concept node: {e}")
            return False
    
    async def create_relationship(self, source_id: str, target_id: str, relationship_type: str, properties: Dict[str, Any] = None) -> bool:
        """Create relationship using OpenCypher"""
        try:
            if properties is None:
                properties = {}
            
            # Add default properties
            properties.update({
                "created_at": "datetime()",
                "confidence": properties.get("confidence", 0.8)
            })
            
            # Format properties for Cypher
            props_str = ", ".join([
                f"{k}: {json.dumps(v) if isinstance(v, (dict, list)) else f\"'{v}'\" if isinstance(v, str) and k != 'created_at' else str(v)}"
                for k, v in properties.items()
            ])
            
            query = f"""
            MATCH (a {{concept_id: '{source_id}' OR article_id: '{source_id}'}}),
                  (b {{concept_id: '{target_id}' OR article_id: '{target_id}'}})
            CREATE (a)-[r:{relationship_type} {{ {props_str} }}]->(b)
            RETURN r
            """
            
            result = self.graph.query(query)
            return len(result.result_set) > 0
            
        except Exception as e:
            logging.error(f"‚ùå Error creating relationship: {e}")
            return False
    
    async def get_graph_stats(self) -> Dict[str, Any]:
        """Get knowledge graph statistics"""
        try:
            stats_queries = {
                "total_nodes": "MATCH (n) RETURN count(n) as count",
                "gdpr_articles": "MATCH (n:GDPRArticle) RETURN count(n) as count",
                "uk_gdpr_articles": "MATCH (n:UKGDPRArticle) RETURN count(n) as count",
                "concepts": "MATCH (n:Concept) RETURN count(n) as count",
                "total_relationships": "MATCH ()-[r]->() RETURN count(r) as count"
            }
            
            stats = {}
            for stat_name, query in stats_queries.items():
                result = self.graph.query(query)
                stats[stat_name] = result.result_set[0][0] if result.result_set else 0
            
            return stats
            
        except Exception as e:
            logging.error(f"‚ùå Error getting graph stats: {e}")
            return {}

# ============================================================================
# MULTI-AGENT COMPONENTS
# ============================================================================

class DocumentAnalyzerAgent:
    """Agent for analyzing PDF documents"""
    
    def __init__(self, config: Config):
        self.config = config
        self.pdf_processor = AdvancedPDFProcessor(config)
    
    async def analyze_documents(self, state: AgentState) -> AgentState:
        """Analyze all PDF documents"""
        logging.info("üîç DocumentAnalyzerAgent: Starting document analysis")
        
        try:
            documents = []
            pdf_files = list(self.config.DOCUMENTS_PATH.glob("*.pdf"))
            
            for pdf_file in pdf_files:
                logging.info(f"üìÑ Processing: {pdf_file.name}")
                
                doc_info = await self.pdf_processor.process_pdf(pdf_file)
                
                if "error" not in doc_info:
                    documents.extend(doc_info["chunks"])
                    state["metadata"][str(pdf_file)] = {
                        "processing_status": "completed",
                        "page_count": doc_info["page_count"],
                        "chunk_count": len(doc_info["chunks"])
                    }
                else:
                    state["errors"].append(f"Failed to process {pdf_file}: {doc_info['error']}")
            
            state["documents"] = documents
            state["current_agent"] = "concept_extractor"
            
            logging.info(f"‚úÖ Processed {len(documents)} document chunks from {len(pdf_files)} files")
            return state
            
        except Exception as e:
            error_msg = f"DocumentAnalyzerAgent error: {e}"
            logging.error(f"‚ùå {error_msg}")
            state["errors"].append(error_msg)
            return state

class ConceptExtractionAgent:
    """Agent for extracting GDPR concepts and articles"""
    
    def __init__(self, config: Config):
        self.config = config
        # Correct OpenAI client initialization with reasoning_effort
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL,
            reasoning_effort=config.REASONING_EFFORT  # Correct parameter name
        )
    
    async def extract_concepts(self, state: AgentState) -> AgentState:
        """Extract GDPR concepts and articles dynamically"""
        logging.info("üß† ConceptExtractionAgent: Starting concept extraction")
        
        try:
            # Extract articles dynamically
            articles = await self._extract_articles_dynamically(state["documents"])
            state["extracted_articles"] = {article.article_id: article for article in articles}
            
            # Extract concepts
            concepts = await self._extract_concepts_dynamically(state["documents"])
            state["concepts"] = [asdict(concept) for concept in concepts]
            
            state["current_agent"] = "knowledge_builder"
            
            logging.info(f"‚úÖ Extracted {len(articles)} articles and {len(concepts)} concepts")
            return state
            
        except Exception as e:
            error_msg = f"ConceptExtractionAgent error: {e}"
            logging.error(f"‚ùå {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _extract_articles_dynamically(self, documents: List[Document]) -> List[GDPRArticle]:
        """Dynamically extract all GDPR articles using o3-mini"""
        extraction_prompt = ChatPromptTemplate.from_template("""
        You are an expert legal analyst specializing in GDPR and UK GDPR regulations.
        
        Analyze the following regulatory text and extract ALL articles, sections, and provisions.
        
        For each article found, provide:
        1. Article number (e.g., "5", "4(1)", "12A")
        2. Complete title
        3. Full content/text
        4. Regulation type (GDPR or UK_GDPR)
        5. Key concepts mentioned
        6. Obligations established
        7. Rights established
        
        Text to analyze:
        {text}
        
        Return valid JSON:
        {{
            "articles": [
                {{
                    "article_number": "5",
                    "title": "Article title",
                    "content": "Full article text",
                    "regulation_type": "GDPR",
                    "key_concepts": ["concept1", "concept2"],
                    "obligations": ["obligation1", "obligation2"],
                    "rights": ["right1", "right2"]
                }}
            ]
        }}
        
        Extract every article you can find - be comprehensive.
        """)
        
        articles = []
        
        # Process regulation documents
        regulation_docs = [
            doc for doc in documents 
            if "GDPR" in doc.metadata.get("document_type", "")
        ]
        
        for doc in regulation_docs:
            try:
                messages = extraction_prompt.format_messages(text=doc.page_content[:6000])
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    for article_data in extracted_data.get("articles", []):
                        article = GDPRArticle(
                            article_id=f"{article_data['regulation_type'].lower()}_article_{article_data['article_number']}",
                            number=article_data["article_number"],
                            title=article_data["title"],
                            content=article_data["content"],
                            regulation_type=article_data["regulation_type"],
                            key_concepts=article_data.get("key_concepts", []),
                            obligations=article_data.get("obligations", []),
                            rights=article_data.get("rights", []),
                            source_document=doc.metadata.get("source", ""),
                            confidence_score=0.9
                        )
                        articles.append(article)
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"‚ö†Ô∏è Failed to parse article extraction: {e}")
                    
            except Exception as e:
                logging.error(f"‚ùå Error extracting articles: {e}")
        
        return articles
    
    async def _extract_concepts_dynamically(self, documents: List[Document]) -> List[GDPRConcept]:
        """Dynamically extract GDPR concepts"""
        concept_extraction_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR expert with deep knowledge of data protection law.
        
        Analyze the following text and extract ALL GDPR-related concepts including:
        - Data protection principles
        - Legal bases for processing
        - Data subject rights
        - Controller/processor obligations
        - Data categories
        - Processing activities
        - Security measures
        - Governance concepts
        
        Text to analyze:
        {text}
        
        Return valid JSON:
        {{
            "concepts": [
                {{
                    "concept_id": "unique_id",
                    "label": "Human readable name",
                    "definition": "Clear definition",
                    "category": "principle|legal_basis|right|obligation|data_category|processing_activity|security_measure|governance",
                    "article_references": ["article1", "article2"],
                    "regulation_type": "GDPR|UK_GDPR|BOTH",
                    "related_concepts": ["related1", "related2"]
                }}
            ]
        }}
        
        Extract every relevant concept you can identify.
        """)
        
        concepts = []
        concept_map = {}
        
        for doc in documents:
            try:
                messages = concept_extraction_prompt.format_messages(text=doc.page_content[:6000])
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    for concept_data in extracted_data.get("concepts", []):
                        concept_id = concept_data["concept_id"]
                        
                        if concept_id not in concept_map:
                            concept = GDPRConcept(
                                concept_id=concept_id,
                                label=concept_data["label"],
                                definition=concept_data["definition"],
                                category=concept_data["category"],
                                article_references=concept_data.get("article_references", []),
                                regulation_type=concept_data.get("regulation_type", "BOTH"),
                                related_concepts=concept_data.get("related_concepts", []),
                                source_document=doc.metadata.get("source", ""),
                                confidence_score=0.8
                            )
                            concept_map[concept_id] = concept
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"‚ö†Ô∏è Failed to parse concept extraction: {e}")
                    
            except Exception as e:
                logging.error(f"‚ùå Error extracting concepts: {e}")
        
        return list(concept_map.values())

class KnowledgeGraphBuilderAgent:
    """Agent for building the labeled property graph"""
    
    def __init__(self, config: Config):
        self.config = config
        self.falkordb_manager = FalkorDBManager(config)
    
    async def build_knowledge_graph(self, state: AgentState) -> AgentState:
        """Build labeled property graph with FalkorDB"""
        logging.info("üï∏Ô∏è KnowledgeGraphBuilderAgent: Building knowledge graph")
        
        try:
            # Create schema
            await self.falkordb_manager.create_schema()
            
            # Add articles to graph
            article_count = 0
            for article_data in state["extracted_articles"].values():
                if isinstance(article_data, dict):
                    # Convert dict back to GDPRArticle
                    article = GDPRArticle(**article_data)
                else:
                    article = article_data
                
                if await self.falkordb_manager.create_article_node(article):
                    article_count += 1
            
            # Add concepts to graph
            concept_count = 0
            for concept_data in state["concepts"]:
                concept = GDPRConcept(**concept_data)
                if await self.falkordb_manager.create_concept_node(concept):
                    concept_count += 1
            
            # Create relationships
            relationship_count = await self._create_relationships(state["concepts"])
            
            # Get final statistics
            stats = await self.falkordb_manager.get_graph_stats()
            state["knowledge_graph_stats"] = stats
            
            state["current_agent"] = "vector_indexer"
            
            logging.info(f"‚úÖ Knowledge graph built: {article_count} articles, {concept_count} concepts, {relationship_count} relationships")
            return state
            
        except Exception as e:
            error_msg = f"KnowledgeGraphBuilderAgent error: {e}"
            logging.error(f"‚ùå {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _create_relationships(self, concepts_data: List[Dict[str, Any]]) -> int:
        """Create relationships between concepts"""
        relationship_count = 0
        
        for concept_data in concepts_data:
            concept_id = concept_data["concept_id"]
            
            # Create relationships to related concepts
            for related_id in concept_data.get("related_concepts", []):
                if await self.falkordb_manager.create_relationship(
                    concept_id, 
                    related_id, 
                    "RELATED_TO",
                    {"relationship_type": "semantic_similarity"}
                ):
                    relationship_count += 1
            
            # Create relationships to articles
            for article_ref in concept_data.get("article_references", []):
                article_id = f"gdpr_article_{article_ref}"
                if await self.falkordb_manager.create_relationship(
                    concept_id,
                    article_id,
                    "REFERENCED_IN",
                    {"article_number": article_ref}
                ):
                    relationship_count += 1
        
        return relationship_count

class VectorIndexerAgent:
    """Agent for creating vector index in Elasticsearch"""
    
    def __init__(self, config: Config):
        self.config = config
        self.elasticsearch_manager = ElasticsearchManager(config)
        # Correct OpenAI embeddings initialization
        self.embeddings = OpenAIEmbeddings(
            model=config.EMBEDDING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
    
    async def create_vector_index(self, state: AgentState) -> AgentState:
        """Create vector index in Elasticsearch 8.13"""
        logging.info("üîç VectorIndexerAgent: Creating vector index")
        
        try:
            # Create Elasticsearch index
            await self.elasticsearch_manager.create_index()
            
            # Prepare documents for indexing
            indexed_docs = await self._prepare_documents_for_indexing(state["documents"])
            
            # Bulk index documents
            result = await self.elasticsearch_manager.bulk_index_documents(indexed_docs)
            
            state["vector_index_stats"] = {
                "total_documents": len(indexed_docs),
                "successfully_indexed": result["success"],
                "failed": result["failed"],
                "embedding_dimensions": 3072
            }
            
            state["current_agent"] = "completed"
            
            logging.info(f"‚úÖ Vector index created: {result['success']} documents indexed")
            return state
            
        except Exception as e:
            error_msg = f"VectorIndexerAgent error: {e}"
            logging.error(f"‚ùå {error_msg}")
            state["errors"].append(error_msg)
            return state
    
    async def _prepare_documents_for_indexing(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """Prepare documents for vector indexing"""
        indexed_docs = []
        
        # Generate embeddings for all documents
        texts = [doc.page_content for doc in documents]
        
        # Generate embeddings in batches
        embeddings = []
        batch_size = 50
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            try:
                batch_embeddings = await self.embeddings.aembed_documents(batch_texts)
                embeddings.extend(batch_embeddings)
            except Exception as e:
                logging.error(f"‚ùå Error generating embeddings for batch {i}: {e}")
                # Add empty embeddings as fallback
                embeddings.extend([[0.0] * 3072] * len(batch_texts))
        
        for doc, embedding in zip(documents, embeddings):
            indexed_doc = {
                "document_id": f"doc_{uuid.uuid4()}",
                "content": doc.page_content,
                "embeddings": embedding,
                "document_type": doc.metadata.get("document_type", "unknown"),
                "source_file": doc.metadata.get("source", ""),
                "chunk_index": doc.metadata.get("chunk_index", 0),
                "extracted_at": datetime.now().isoformat(),
                "regulation_type": self._determine_regulation_type(doc.page_content),
                "article_reference": self._extract_article_references(doc.page_content),
                "concepts": self._extract_concept_keywords(doc.page_content)
            }
            
            indexed_docs.append(indexed_doc)
        
        return indexed_docs
    
    def _determine_regulation_type(self, content: str) -> str:
        """Determine regulation type from content"""
        content_lower = content.lower()
        if "uk gdpr" in content_lower:
            return "UK_GDPR"
        elif "gdpr" in content_lower:
            return "GDPR"
        return "unknown"
    
    def _extract_article_references(self, content: str) -> List[str]:
        """Extract article references from content"""
        import re
        patterns = [
            r"[Aa]rticle (\d+)",
            r"[Aa]rt\. (\d+)",
            r"[Aa]rticle (\d+\.\d+)"
        ]
        
        references = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            references.extend(matches)
        
        return list(set(references))
    
    def _extract_concept_keywords(self, content: str) -> List[str]:
        """Extract concept keywords from content"""
        keywords = []
        content_lower = content.lower()
        
        gdpr_terms = [
            "consent", "legitimate interest", "contract", "legal obligation",
            "personal data", "processing", "controller", "processor",
            "data subject", "data protection officer", "privacy by design",
            "data minimisation", "accountability", "transparency"
        ]
        
        for term in gdpr_terms:
            if term in content_lower:
                keywords.append(term)
        
        return keywords

# ============================================================================
# MULTI-AGENT ORCHESTRATOR WITH LANGGRAPH
# ============================================================================

class GDPRMultiAgentOrchestrator:
    """Multi-agent orchestrator using LangGraph StateGraph"""
    
    def __init__(self, config: Config):
        self.config = config
        config.validate()
        
        # Initialize agents
        self.document_analyzer = DocumentAnalyzerAgent(config)
        self.concept_extractor = ConceptExtractionAgent(config)
        self.knowledge_builder = KnowledgeGraphBuilderAgent(config)
        self.vector_indexer = VectorIndexerAgent(config)
        
        # Setup logging
        self.setup_logging()
        
        # Create workflow graph with proper StateGraph syntax
        self.workflow = self._create_workflow()
    
    def setup_logging(self):
        """Setup logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.config.OUTPUT_PATH / 'gdpr_system.log'),
                logging.StreamHandler()
            ]
        )
    
    def _create_workflow(self) -> StateGraph:
        """Create LangGraph workflow with proper syntax"""
        # Create StateGraph with AgentState
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("document_analyzer", self._document_analyzer_node)
        workflow.add_node("concept_extractor", self._concept_extractor_node)
        workflow.add_node("knowledge_builder", self._knowledge_builder_node)
        workflow.add_node("vector_indexer", self._vector_indexer_node)
        
        # Define edges with proper LangGraph syntax
        workflow.add_edge(START, "document_analyzer")
        workflow.add_edge("document_analyzer", "concept_extractor")
        workflow.add_edge("concept_extractor", "knowledge_builder")
        workflow.add_edge("knowledge_builder", "vector_indexer")
        workflow.add_edge("vector_indexer", END)
        
        return workflow.compile()
    
    async def _document_analyzer_node(self, state: AgentState) -> AgentState:
        """Document analyzer node"""
        logging.info("üîÑ Executing DocumentAnalyzer node")
        state["current_agent"] = "document_analyzer"
        state["iteration_count"] += 1
        
        try:
            return await self.document_analyzer.analyze_documents(state)
        except Exception as e:
            logging.error(f"‚ùå DocumentAnalyzer node failed: {e}")
            state["errors"].append(f"DocumentAnalyzer: {str(e)}")
            return state
    
    async def _concept_extractor_node(self, state: AgentState) -> AgentState:
        """Concept extractor node"""
        logging.info("üîÑ Executing ConceptExtractor node")
        state["current_agent"] = "concept_extractor"
        state["iteration_count"] += 1
        
        try:
            return await self.concept_extractor.extract_concepts(state)
        except Exception as e:
            logging.error(f"‚ùå ConceptExtractor node failed: {e}")
            state["errors"].append(f"ConceptExtractor: {str(e)}")
            return state
    
    async def _knowledge_builder_node(self, state: AgentState) -> AgentState:
        """Knowledge builder node"""
        logging.info("üîÑ Executing KnowledgeBuilder node")
        state["current_agent"] = "knowledge_builder"
        state["iteration_count"] += 1
        
        try:
            return await self.knowledge_builder.build_knowledge_graph(state)
        except Exception as e:
            logging.error(f"‚ùå KnowledgeBuilder node failed: {e}")
            state["errors"].append(f"KnowledgeBuilder: {str(e)}")
            return state
    
    async def _vector_indexer_node(self, state: AgentState) -> AgentState:
        """Vector indexer node"""
        logging.info("üîÑ Executing VectorIndexer node")
        state["current_agent"] = "vector_indexer"
        state["iteration_count"] += 1
        
        try:
            return await self.vector_indexer.create_vector_index(state)
        except Exception as e:
            logging.error(f"‚ùå VectorIndexer node failed: {e}")
            state["errors"].append(f"VectorIndexer: {str(e)}")
            return state
    
    async def run_pipeline(self) -> Dict[str, Any]:
        """Run the complete multi-agent pipeline"""
        start_time = datetime.now()
        logging.info("üöÄ Starting GDPR Multi-Agent Knowledge System Pipeline")
        
        try:
            # Initialize state with proper TypedDict structure
            initial_state: AgentState = {
                "documents": [],
                "extracted_articles": {},
                "concepts": [],
                "knowledge_graph_stats": {},
                "vector_index_stats": {},
                "current_agent": "",
                "iteration_count": 0,
                "errors": [],
                "metadata": {
                    "pipeline_start": start_time.isoformat(),
                    "config": {
                        "reasoning_model": self.config.REASONING_MODEL,
                        "embedding_model": self.config.EMBEDDING_MODEL,
                        "reasoning_effort": self.config.REASONING_EFFORT
                    }
                }
            }
            
            # Execute workflow
            logging.info("üîÑ Executing multi-agent workflow...")
            final_state = await self.workflow.ainvoke(initial_state)
            
            # Calculate processing time
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            # Generate results
            results = {
                "status": "success" if not final_state["errors"] else "completed_with_errors",
                "processing_time": processing_time,
                "statistics": {
                    "documents_processed": len(final_state["documents"]),
                    "articles_extracted": len(final_state["extracted_articles"]),
                    "concepts_identified": len(final_state["concepts"]),
                    "agent_iterations": final_state["iteration_count"]
                },
                "knowledge_graph_stats": final_state["knowledge_graph_stats"],
                "vector_index_stats": final_state["vector_index_stats"],
                "errors": final_state["errors"]
            }
            
            # Log completion
            if final_state["errors"]:
                logging.warning(f"‚ö†Ô∏è Pipeline completed with {len(final_state['errors'])} errors")
                for error in final_state["errors"]:
                    logging.warning(f"  - {error}")
            else:
                logging.info("‚úÖ Pipeline completed successfully!")
            
            logging.info(f"üìä Results:")
            logging.info(f"  - Processing Time: {processing_time:.2f} seconds")
            logging.info(f"  - Documents: {results['statistics']['documents_processed']}")
            logging.info(f"  - Articles: {results['statistics']['articles_extracted']}")
            logging.info(f"  - Concepts: {results['statistics']['concepts_identified']}")
            
            return results
            
        except Exception as e:
            error_msg = f"Pipeline execution failed: {e}"
            logging.error(f"‚ùå {error_msg}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Main execution function"""
    try:
        # Load configuration
        config = Config()
        
        # Ensure directories exist
        config.DOCUMENTS_PATH.mkdir(exist_ok=True)
        config.OUTPUT_PATH.mkdir(exist_ok=True)
        
        print("üöÄ GDPR Dynamic Multi-Agent Knowledge System")
        print("=" * 50)
        print(f"üìÅ Documents Path: {config.DOCUMENTS_PATH}")
        print(f"üìÇ Output Path: {config.OUTPUT_PATH}")
        print(f"ü§ñ Reasoning Model: {config.REASONING_MODEL}")
        print(f"üß† Reasoning Effort: {config.REASONING_EFFORT}")
        print(f"üìä Embedding Model: {config.EMBEDDING_MODEL}")
        
        # Create orchestrator
        orchestrator = GDPRMultiAgentOrchestrator(config)
        
        # Run pipeline
        print("\nüîÑ Starting multi-agent pipeline...")
        results = await orchestrator.run_pipeline()
        
        # Display results
        print("\n" + "=" * 50)
        print("üìä RESULTS")
        print("=" * 50)
        print(f"Status: {results['status']}")
        print(f"Processing Time: {results['processing_time']:.2f} seconds")
        print(f"Documents Processed: {results['statistics']['documents_processed']}")
        print(f"Articles Extracted: {results['statistics']['articles_extracted']}")
        print(f"Concepts Identified: {results['statistics']['concepts_identified']}")
        
        if results.get('knowledge_graph_stats'):
            print(f"\nKnowledge Graph:")
            for key, value in results['knowledge_graph_stats'].items():
                print(f"  - {key}: {value}")
        
        if results.get('vector_index_stats'):
            print(f"\nVector Index:")
            for key, value in results['vector_index_stats'].items():
                print(f"  - {key}: {value}")
        
        if results.get('errors'):
            print(f"\n‚ö†Ô∏è Errors: {len(results['errors'])}")
            for error in results['errors']:
                print(f"  - {error}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå System failed: {e}")
        logging.error(f"System failure: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
