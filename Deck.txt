#!/usr/bin/env python3
"""
Unified RDF Triples to FalkorDB Converter
Single CSV containing all RDF triples with complete node/edge/property metadata
More efficient and faithful to RDF structure
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import sys
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class UnifiedRDFConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the unified RDF converter"""
        self.output_dir = Path(output_dir).resolve()
        
        # Single comprehensive triples list
        self.triples = []  # Complete RDF triples with all metadata
        self.uri_to_id = {}  # URI/BNode -> unique_id mapping
        self.node_info = {}  # node_id -> {uri, type, label} for reference
        
        # Statistics
        self.stats = {
            'total_triples': 0,
            'literal_triples': 0,  # Subject-Predicate-Literal
            'resource_triples': 0,  # Subject-Predicate-Object(URI)
            'unique_resources': 0,
            'unique_predicates': 0
        }
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_uri_for_label(self, uri_str: str) -> str:
        """Extract clean label from URI for display purposes"""
        try:
            parsed = urlparse(uri_str)
            
            # Try fragment first (after #)
            if parsed.fragment:
                name = parsed.fragment
            # Then try last path component
            elif parsed.path and parsed.path != '/':
                path_parts = [p for p in parsed.path.strip('/').split('/') if p]
                if path_parts:
                    name = path_parts[-1]
                else:
                    name = parsed.netloc or 'unknown'
            # Fall back to netloc
            elif parsed.netloc:
                name = parsed.netloc.replace('.', '_')
            else:
                name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
            
            # Clean for use as identifier
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"uri_{cleaned}"
            if not cleaned:
                cleaned = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
            return cleaned[:50]  # Limit length
            
        except Exception as e:
            logger.warning(f"Error cleaning URI {uri_str}: {e}")
            return f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"

    def get_or_create_resource_id(self, resource) -> tuple:
        """Get or create a unique resource ID and return (id, uri, type, label)"""
        uri_str = str(resource)
        
        if uri_str not in self.uri_to_id:
            # Create unique ID based on URI hash
            hash_obj = hashlib.md5(uri_str.encode('utf-8'))
            base_id = f"n_{hash_obj.hexdigest()[:12]}"
            
            # Ensure uniqueness
            resource_id = base_id
            counter = 1
            while resource_id in self.uri_to_id.values():
                resource_id = f"{base_id}_{counter}"
                counter += 1
            
            self.uri_to_id[uri_str] = resource_id
            
            # Store resource info
            resource_type = 'BlankNode' if isinstance(resource, BNode) else 'URI'
            resource_label = self.clean_uri_for_label(uri_str)
            
            self.node_info[resource_id] = {
                'uri': uri_str,
                'type': resource_type,
                'label': resource_label
            }
        
        info = self.node_info[self.uri_to_id[uri_str]]
        return self.uri_to_id[uri_str], info['uri'], info['type'], info['label']

    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        if value is None:
            return ''
        
        if isinstance(value, bool):
            return 'true' if value else 'false'
        
        if isinstance(value, (list, dict)):
            return json.dumps(value, ensure_ascii=False)
        
        str_value = str(value).strip()
        
        # Handle newlines and normalize whitespace
        str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
        str_value = re.sub(r'\s+', ' ', str_value)
        
        # Escape quotes for CSV
        if '"' in str_value:
            str_value = str_value.replace('"', '""')
        
        # Limit length to prevent issues
        if len(str_value) > 1000:
            logger.warning(f"Very long value truncated (length: {len(str_value)})")
            str_value = str_value[:1000] + "..."
        
        return str_value

    def process_literal_value(self, literal: Literal) -> tuple:
        """Process literal and return (value, datatype, language)"""
        try:
            value = str(literal)
            datatype = str(literal.datatype) if literal.datatype else None
            language = literal.language if literal.language else None
            
            # Try to convert typed literals to appropriate Python types for better storage
            if datatype:
                if 'integer' in datatype.lower() or 'int' in datatype.lower():
                    try:
                        int_val = int(literal)
                        if abs(int_val) <= 2147483647:  # SQL INT range
                            return str(int_val), datatype, language
                    except ValueError:
                        pass
                        
                elif 'decimal' in datatype.lower() or 'double' in datatype.lower() or 'float' in datatype.lower():
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:  # Reasonable range
                            return str(float_val), datatype, language
                    except ValueError:
                        pass
                        
                elif 'boolean' in datatype.lower():
                    bool_val = str(literal).lower() in ('true', '1')
                    return str(bool_val).lower(), datatype, language
            
            return value, datatype, language
            
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal), None, None

    def convert_ttl_to_unified_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL to unified CSV containing all RDF triple information"""
        logger.info(f"🔄 Converting {ttl_file_path} to unified RDF triples CSV...")
        logger.info("📋 Single CSV will contain: nodes + edges + properties + metadata")
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Parse RDF graph
        rdf_graph = Graph()
        try:
            logger.info("📖 Parsing RDF graph...")
            rdf_graph.parse(ttl_file_path, format='turtle')
            total_triples = len(rdf_graph)
            self.stats['total_triples'] = total_triples
            logger.info(f"✅ Parsed {total_triples:,} RDF triples")
        except Exception as e:
            logger.error(f"❌ Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("⚠️ No triples found in TTL file!")
            return None
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"🎯 Processing only first {max_triples:,} of {total_triples:,} triples")
            total_triples = max_triples
            self.stats['total_triples'] = max_triples
        
        # Process all RDF triples into unified format
        logger.info("🔄 Processing RDF triples into unified format...")
        processed_count = 0
        unique_predicates = set()
        
        with tqdm(total=total_triples, desc="Processing RDF triples") as pbar:
            for subject, predicate, obj in rdf_graph:
                if max_triples and processed_count >= max_triples:
                    break
                
                try:
                    pbar.update(1)
                    processed_count += 1
                    
                    # Get subject information
                    subject_id, subject_uri, subject_type, subject_label = self.get_or_create_resource_id(subject)
                    
                    # Get predicate information
                    predicate_uri = str(predicate)
                    predicate_label = self.clean_uri_for_label(predicate_uri)
                    unique_predicates.add(predicate_uri)
                    
                    # Create base triple record
                    triple_record = {
                        # Subject information
                        'subject_id': subject_id,
                        'subject_uri': subject_uri,
                        'subject_type': subject_type,
                        'subject_label': subject_label,
                        
                        # Predicate information
                        'predicate_uri': predicate_uri,
                        'predicate_label': predicate_label,
                        
                        # Object information (will be filled based on type)
                        'object_id': '',
                        'object_uri': '',
                        'object_type': '',
                        'object_label': '',
                        
                        # Literal value information
                        'object_value': '',
                        'object_datatype': '',
                        'object_language': '',
                        
                        # Triple type
                        'triple_type': '',
                        
                        # Triple sequence number
                        'triple_seq': processed_count
                    }
                    
                    # Process based on object type
                    if isinstance(obj, Literal):
                        # Object is literal -> Property triple
                        self.stats['literal_triples'] += 1
                        
                        value, datatype, language = self.process_literal_value(obj)
                        
                        triple_record.update({
                            'object_value': value,
                            'object_datatype': datatype or '',
                            'object_language': language or '',
                            'triple_type': 'literal_property'
                        })
                    
                    else:
                        # Object is URI/BNode -> Relationship triple
                        self.stats['resource_triples'] += 1
                        
                        object_id, object_uri, object_type, object_label = self.get_or_create_resource_id(obj)
                        
                        triple_record.update({
                            'object_id': object_id,
                            'object_uri': object_uri,
                            'object_type': object_type,
                            'object_label': object_label,
                            'triple_type': 'resource_relationship'
                        })
                    
                    self.triples.append(triple_record)
                
                except Exception as e:
                    logger.warning(f"⚠️ Error processing triple {processed_count}: {e}")
                    continue
        
        # Update statistics
        self.stats['unique_resources'] = len(self.node_info)
        self.stats['unique_predicates'] = len(unique_predicates)
        
        logger.info("✅ RDF processing complete!")
        logger.info(f"📊 Statistics:")
        logger.info(f"   Total triples processed: {processed_count:,}")
        logger.info(f"   Literal triples (properties): {self.stats['literal_triples']:,}")
        logger.info(f"   Resource triples (relationships): {self.stats['resource_triples']:,}")
        logger.info(f"   Unique resources (nodes): {self.stats['unique_resources']:,}")
        logger.info(f"   Unique predicates: {self.stats['unique_predicates']:,}")
        
        # Write unified CSV file
        csv_file = self.write_unified_csv()
        return csv_file

    def write_unified_csv(self):
        """Write all RDF triples to a single unified CSV file"""
        csv_file = self.output_dir / "rdf_triples.csv"
        
        logger.info(f"💾 Writing {len(self.triples):,} RDF triples to unified CSV...")
        
        # Define CSV headers for unified format
        headers = [
            # Subject information
            'subject_id', 'subject_uri', 'subject_type', 'subject_label',
            
            # Predicate information  
            'predicate_uri', 'predicate_label',
            
            # Object information (for resource relationships)
            'object_id', 'object_uri', 'object_type', 'object_label',
            
            # Literal value information (for properties)
            'object_value', 'object_datatype', 'object_language',
            
            # Triple metadata
            'triple_type', 'triple_seq'
        ]
        
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for triple in self.triples:
                    row = [self.sanitize_csv_value(triple.get(header, '')) for header in headers]
                    writer.writerow(row)
            
            logger.info(f"✅ Written unified RDF CSV: {csv_file.name}")
            logger.info(f"   📏 File size: {csv_file.stat().st_size / (1024*1024):.1f} MB")
            
            # Write statistics
            stats_file = self.output_dir / "conversion_stats.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2)
            
            logger.info(f"📊 Written statistics: {stats_file.name}")
            
            return str(csv_file)
            
        except Exception as e:
            logger.error(f"❌ Error writing unified CSV: {e}")
            raise

    def load_unified_csv_to_falkordb(self, graph_name: str, host: str = '127.0.0.1', 
                                   port: int = 6379, password: Optional[str] = None):
        """Load unified CSV to FalkorDB creating both nodes and edges"""
        logger.info(f"🚀 Loading unified RDF CSV to FalkorDB (graph: {graph_name})...")
        
        try:
            import redis
        except ImportError:
            logger.error("❌ redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Connect to FalkorDB
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            r.ping()
            logger.info(f"✅ Connected to FalkorDB at {host}:{port}")
            
        except Exception as e:
            logger.error(f"❌ Failed to connect to FalkorDB: {e}")
            return False
        
        # Check CSV file exists
        csv_file = self.output_dir / "rdf_triples.csv"
        if not csv_file.exists():
            logger.error(f"❌ Unified CSV not found: {csv_file}")
            logger.error("Run conversion first with --convert")
            return False
        
        # Configure import folder
        try:
            logger.info(f"🔧 Configuring import folder: {self.output_dir}")
            r.execute_command("GRAPH.CONFIG", "SET", "IMPORT_FOLDER", str(self.output_dir))
            logger.info("✅ Import folder configured")
        except Exception as e:
            logger.warning(f"⚠️ Could not set import folder: {e}")
        
        try:
            # Step 1a: Create subject nodes
            logger.info("📥 Step 1a: Creating subject nodes...")
            subject_nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://rdf_triples.csv' AS row
            WITH row, row.subject_id AS node_id, row.subject_uri AS node_uri, 
                 row.subject_type AS node_type, row.subject_label AS node_label
            WHERE node_id IS NOT NULL AND node_id <> ''
            MERGE (n:Resource {id: node_id})
            ON CREATE SET n.uri = node_uri, n.type = node_type, n.label = node_label
            RETURN count(n) AS subjects_processed
            """
            
            result = r.execute_command("GRAPH.QUERY", graph_name, subject_nodes_query)
            logger.info(f"✅ Subject nodes processed: {result}")
            
            # Step 1b: Create object nodes (only for resource relationships)
            logger.info("📥 Step 1b: Creating object nodes...")
            object_nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://rdf_triples.csv' AS row
            WITH row, row.object_id AS node_id, row.object_uri AS node_uri,
                 row.object_type AS node_type, row.object_label AS node_label
            WHERE row.triple_type = 'resource_relationship' AND node_id IS NOT NULL AND node_id <> ''
            MERGE (n:Resource {id: node_id})
            ON CREATE SET n.uri = node_uri, n.type = node_type, n.label = node_label
            RETURN count(n) AS objects_processed
            """
            
            result = r.execute_command("GRAPH.QUERY", graph_name, object_nodes_query)
            logger.info(f"✅ Object nodes processed: {result}")
            
            # Step 2: Add literal properties to nodes
            logger.info("📝 Step 2: Adding literal properties to nodes...")
            
            # Use a simple approach - create separate property nodes
            # This is more compatible with FalkorDB/RedisGraph limitations
            logger.info("🔄 Creating property nodes for literal values...")
            property_nodes_query = """
            LOAD CSV WITH HEADERS FROM 'file://rdf_triples.csv' AS row
            WHERE row.triple_type = 'literal_property'
            MATCH (n:Resource {id: row.subject_id})
            CREATE (p:Property {
                predicate_uri: row.predicate_uri,
                predicate_label: row.predicate_label,
                value: row.object_value,
                datatype: row.object_datatype,
                language: row.object_language
            })
            CREATE (n)-[:HAS_PROPERTY]->(p)
            RETURN count(p) AS property_nodes_created
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, property_nodes_query)
                logger.info(f"✅ Property nodes created: {result}")
            except Exception as e:
                logger.warning(f"⚠️ Property nodes creation failed: {e}")
                # Try even simpler approach - just mark nodes as having properties
                logger.info("🔄 Trying basic property marking...")
                simple_props_query = """
                LOAD CSV WITH HEADERS FROM 'file://rdf_triples.csv' AS row
                WHERE row.triple_type = 'literal_property'
                MATCH (n:Resource {id: row.subject_id})
                SET n.has_properties = true
                RETURN count(n) AS nodes_marked
                """
                result = r.execute_command("GRAPH.QUERY", graph_name, simple_props_query)
                logger.info(f"✅ Nodes marked with properties: {result}")
            
            # Step 3: Create relationships between nodes
            logger.info("🔗 Step 3: Creating relationships...")
            relationships_query = """
            LOAD CSV WITH HEADERS FROM 'file://rdf_triples.csv' AS row
            WHERE row.triple_type = 'resource_relationship'
            MATCH (source:Resource {id: row.subject_id})
            MATCH (target:Resource {id: row.object_id})
            CREATE (source)-[r:PREDICATE]->(target)
            SET r.predicate_uri = row.predicate_uri,
                r.predicate_label = row.predicate_label
            RETURN count(r) AS relationships_created
            """
            
            result = r.execute_command("GRAPH.QUERY", graph_name, relationships_query)
            logger.info(f"✅ Relationships created: {result}")
            
            # Step 4: Create index for better performance
            logger.info("🔍 Step 4: Creating performance index...")
            try:
                index_query = "CREATE INDEX FOR (n:Resource) ON (n.id)"
                r.execute_command("GRAPH.QUERY", graph_name, index_query)
                logger.info("✅ Index created")
            except Exception as e:
                logger.warning(f"⚠️ Index creation failed: {e}")
            
            # Verify loaded data
            try:
                # Count nodes
                count_query = "MATCH (n:Resource) RETURN count(n) AS node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"📊 Total nodes: {result}")
                
                # Count relationships
                rel_count_query = "MATCH ()-[r]->() RETURN count(r) AS rel_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, rel_count_query)
                logger.info(f"📊 Total relationships: {result}")
                
                # Sample predicates
                pred_sample_query = "MATCH ()-[r]->() RETURN DISTINCT r.predicate_label LIMIT 5"
                result = r.execute_command("GRAPH.QUERY", graph_name, pred_sample_query)
                logger.info(f"📊 Sample predicates: {result}")
                
                # Sample nodes with properties
                props_sample_query = "MATCH (n:Resource) WHERE size(keys(n)) > 3 RETURN n.label, keys(n) LIMIT 3"
                result = r.execute_command("GRAPH.QUERY", graph_name, props_sample_query)
                logger.info(f"📊 Sample node properties: {result}")
                
            except Exception as e:
                logger.warning(f"⚠️ Could not verify data: {e}")
            
            logger.info("🎉 SUCCESS! Unified RDF data loaded to FalkorDB!")
            logger.info(f"   🔗 Graph name: {graph_name}")
            logger.info(f"   📁 Source file: {csv_file}")
            logger.info(f"   🌐 FalkorDB: {host}:{port}")
            logger.info("   🔍 Query: MATCH (n:Resource)-[r:PREDICATE]->(m:Resource) RETURN n.label, r.predicate_label, m.label LIMIT 5")
            
            return True
            
        except Exception as e:
            logger.error(f"❌ Failed to load unified data: {e}")
            return False


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Unified RDF to Property Graph Converter - Single CSV with all data'
    )
    parser.add_argument('ttl_file', nargs='?', help='Path to TTL file')
    parser.add_argument('--convert', action='store_true', help='Convert TTL to unified CSV')
    parser.add_argument('--load', action='store_true', help='Load unified CSV to FalkorDB')
    parser.add_argument('--both', action='store_true', help='Convert and load')
    parser.add_argument('--graph_name', default='rdf_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--password', help='FalkorDB password')
    parser.add_argument('--output_dir', default='csv_output', help='CSV output directory')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process')

    args = parser.parse_args()

    # Default action
    if not any([args.convert, args.load, args.both]):
        if args.ttl_file:
            args.both = True
        else:
            parser.print_help()
            print("\n📋 Unified RDF to Property Graph Converter")
            print("Single CSV containing all RDF data:")
            print("  • All triples with complete subject/predicate/object info")
            print("  • Node metadata (URI, type, label)")
            print("  • Edge metadata (predicate URI, label)")
            print("  • Property metadata (datatype, language)")
            print("  • More efficient loading and management")
            print("\nExamples:")
            print("  python script.py data.ttl --convert    # Convert to unified CSV")
            print("  python script.py --load               # Load unified CSV to FalkorDB")
            print("  python script.py data.ttl             # Convert and load (default)")
            return

    # Create converter
    converter = UnifiedRDFConverter(args.output_dir)
    start_time = time.time()

    # Convert if requested
    if args.convert or args.both:
        if not args.ttl_file:
            logger.error("❌ TTL file required for conversion")
            sys.exit(1)
        
        if not os.path.exists(args.ttl_file):
            logger.error(f"❌ TTL file not found: {args.ttl_file}")
            sys.exit(1)
        
        try:
            file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
            logger.info(f"🎯 Converting TTL: {args.ttl_file} ({file_size_mb:.1f}MB)")
            
            csv_file = converter.convert_ttl_to_unified_csv(args.ttl_file, args.max_triples)
            
            if csv_file:
                conversion_time = time.time() - start_time
                logger.info(f"✅ Conversion completed in {conversion_time:.1f}s")
                logger.info(f"📋 Unified CSV: {csv_file}")
            else:
                logger.error("❌ Conversion failed")
                sys.exit(1)
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            sys.exit(1)

    # Load if requested
    if args.load or args.both:
        try:
            success = converter.load_unified_csv_to_falkordb(
                args.graph_name, args.host, args.port, args.password
            )
            
            if success:
                total_time = time.time() - start_time
                logger.info(f"🎉 COMPLETE! Total time: {total_time:.1f}s")
                logger.info(f"📋 All RDF data loaded from single unified CSV!")
            else:
                logger.error("❌ Failed to load to FalkorDB")
                sys.exit(1)
                
        except Exception as e:
            logger.error(f"❌ Loading failed: {e}")
            sys.exit(1)


if __name__ == "__main__":
    main()
