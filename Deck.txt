#!/usr/bin/env python3
"""
TTL to FalkorDB Property Graph Converter
Converts RDF triples from TTL files to property graph model and ingests into FalkorDB
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import redis
import falkordb
import hashlib
import json
import time
import logging
from typing import Dict, Set, List, Tuple, Any
from urllib.parse import urlparse
import re
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TTLToFalkorDB:
    def __init__(self, redis_host='localhost', redis_port=6379, db_name='knowledge_graph', password=None):
        """Initialize the converter with FalkorDB connection"""
        if password:
            self.redis_client = redis.Redis(host=redis_host, port=redis_port, password=password, decode_responses=True)
            self.db = falkordb.FalkorDB(host=redis_host, port=redis_port, password=password).select_graph(db_name)
        else:
            self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
            self.db = falkordb.FalkorDB(host=redis_host, port=redis_port).select_graph(db_name)
        self.batch_size = 1000
        self.node_cache = set()
        self.edge_cache = set()
        
    def clean_identifier(self, uri_or_literal: str) -> str:
        """Clean and create valid identifier for nodes/properties"""
        if isinstance(uri_or_literal, URIRef):
            # Extract meaningful part from URI
            parsed = urlparse(str(uri_or_literal))
            if parsed.fragment:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.fragment)
            elif parsed.path:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.path.split('/')[-1])
            else:
                return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
        else:
            return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
    
    def create_node_id(self, resource) -> str:
        """Create unique node ID from resource"""
        if isinstance(resource, BNode):
            return f"bnode_{str(resource)}"
        else:
            # Use hash for consistent IDs
            return hashlib.md5(str(resource).encode()).hexdigest()[:12]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, Any]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path:
            parts = parsed.path.strip('/').split('/')
            if parts and parts[-1]:
                properties['local_name'] = parts[-1]
                properties['namespace'] = uri_str.replace(parts[-1], '').rstrip('/')
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[Any, str]:
        """Process literal value and return (value, datatype)"""
        if literal.datatype:
            datatype = str(literal.datatype)
            # Handle common datatypes
            if 'integer' in datatype or 'int' in datatype:
                try:
                    return int(literal), 'integer'
                except:
                    return str(literal), 'string'
            elif 'decimal' in datatype or 'double' in datatype or 'float' in datatype:
                try:
                    return float(literal), 'float'
                except:
                    return str(literal), 'string'
            elif 'boolean' in datatype:
                return str(literal).lower() in ('true', '1'), 'boolean'
            elif 'date' in datatype:
                return str(literal), 'date'
            else:
                return str(literal), 'string'
        else:
            return str(literal), 'string'
    
    def parse_ttl_streaming(self, ttl_file_path: str):
        """Parse TTL file in streaming fashion"""
        logger.info(f"Starting to parse {ttl_file_path}")
        
        graph = Graph()
        
        # Parse the file
        try:
            graph.parse(ttl_file_path, format='turtle')
            logger.info(f"Successfully parsed TTL file. Found {len(graph)} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        return graph
    
    def convert_to_property_graph(self, graph: Graph):
        """Convert RDF graph to property graph model"""
        logger.info("Converting RDF triples to property graph model...")
        
        nodes = {}
        edges = []
        literal_properties = {}  # subject -> {predicate: value}
        
        total_triples = len(graph)
        processed = 0
        
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                processed += 1
                pbar.update(1)
                
                subject_id = self.create_node_id(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Create subject node if not exists
                if subject_id not in nodes:
                    if isinstance(subject, URIRef):
                        properties = self.extract_properties_from_uri(subject)
                        node_type = 'Resource'
                    else:  # BNode
                        properties = {'uri': str(subject)}
                        node_type = 'BlankNode'
                    
                    nodes[subject_id] = {
                        'id': subject_id,
                        'type': node_type,
                        'properties': properties
                    }
                
                # Handle object
                if isinstance(obj, Literal):
                    # Object is a literal - add as property to subject node
                    if subject_id not in literal_properties:
                        literal_properties[subject_id] = {}
                    
                    value, datatype = self.process_literal_value(obj)
                    literal_properties[subject_id][predicate_clean] = {
                        'value': value,
                        'datatype': datatype
                    }
                    
                    # Also store language if present
                    if obj.language:
                        literal_properties[subject_id][f"{predicate_clean}_lang"] = obj.language
                
                else:
                    # Object is a resource - create edge
                    object_id = self.create_node_id(obj)
                    
                    # Create object node if not exists
                    if object_id not in nodes:
                        if isinstance(obj, URIRef):
                            properties = self.extract_properties_from_uri(obj)
                            node_type = 'Resource'
                        else:  # BNode
                            properties = {'uri': str(obj)}
                            node_type = 'BlankNode'
                        
                        nodes[object_id] = {
                            'id': object_id,
                            'type': node_type,
                            'properties': properties
                        }
                    
                    # Create edge
                    edge = {
                        'from': subject_id,
                        'to': object_id,
                        'relationship': predicate_clean,
                        'properties': {
                            'predicate_uri': str(predicate)
                        }
                    }
                    edges.append(edge)
        
        # Merge literal properties into nodes
        for node_id, props in literal_properties.items():
            if node_id in nodes:
                for prop_name, prop_data in props.items():
                    nodes[node_id]['properties'][prop_name] = prop_data['value']
                    nodes[node_id]['properties'][f"{prop_name}_datatype"] = prop_data['datatype']
        
        logger.info(f"Conversion complete. Created {len(nodes)} nodes and {len(edges)} edges.")
        return list(nodes.values()), edges
    
    def escape_cypher_string(self, value: str) -> str:
        """Escape string for Cypher query"""
        if isinstance(value, str):
            return value.replace("'", "\\'").replace('"', '\\"').replace('\n', '\\n').replace('\r', '\\r')
        return str(value)
    
    def create_cypher_properties(self, properties: Dict[str, Any]) -> str:
        """Convert properties dict to Cypher properties string"""
        prop_parts = []
        for key, value in properties.items():
            if isinstance(value, str):
                escaped_value = self.escape_cypher_string(value)
                prop_parts.append(f"{key}: '{escaped_value}'")
            elif isinstance(value, (int, float)):
                prop_parts.append(f"{key}: {value}")
            elif isinstance(value, bool):
                prop_parts.append(f"{key}: {str(value).lower()}")
            else:
                escaped_value = self.escape_cypher_string(str(value))
                prop_parts.append(f"{key}: '{escaped_value}'")
        
        return "{" + ", ".join(prop_parts) + "}"
    
    def ingest_nodes_batch(self, nodes: List[Dict], batch_start: int, batch_end: int):
        """Ingest a batch of nodes into FalkorDB"""
        batch_nodes = nodes[batch_start:batch_end]
        
        if not batch_nodes:
            return
        
        # Create MERGE statements for nodes to avoid duplicates
        cypher_parts = []
        for node in batch_nodes:
            node_id = node['id']
            node_type = node['type']
            properties_str = self.create_cypher_properties(node['properties'])
            
            cypher_parts.append(f"MERGE (n{node_id}:Node:{node_type} {{id: '{node_id}'}}) SET n{node_id} += {properties_str}")
        
        # Execute batch
        try:
            for cypher in cypher_parts:
                self.db.query(cypher)
        except Exception as e:
            logger.error(f"Error in batch nodes ingestion: {e}")
            # Try individual inserts
            for i, cypher in enumerate(cypher_parts):
                try:
                    self.db.query(cypher)
                except Exception as inner_e:
                    logger.error(f"Error inserting node {batch_start + i}: {inner_e}")
    
    def ingest_edges_batch(self, edges: List[Dict], batch_start: int, batch_end: int):
        """Ingest a batch of edges into FalkorDB"""
        batch_edges = edges[batch_start:batch_end]
        
        if not batch_edges:
            return
        
        # Create MATCH and MERGE statements for edges
        cypher_parts = []
        for edge in batch_edges:
            from_id = edge['from']
            to_id = edge['to']
            relationship = edge['relationship']
            properties_str = self.create_cypher_properties(edge['properties'])
            
            cypher = f"""
            MATCH (from:Node {{id: '{from_id}'}})
            MATCH (to:Node {{id: '{to_id}'}})
            MERGE (from)-[r:{relationship}]->(to)
            SET r += {properties_str}
            """
            cypher_parts.append(cypher.strip())
        
        # Execute batch
        try:
            for cypher in cypher_parts:
                self.db.query(cypher)
        except Exception as e:
            logger.error(f"Error in batch edges ingestion: {e}")
            # Try individual inserts
            for i, cypher in enumerate(cypher_parts):
                try:
                    self.db.query(cypher)
                except Exception as inner_e:
                    logger.error(f"Error inserting edge {batch_start + i}: {inner_e}")
    
    def ingest_to_falkordb(self, nodes: List[Dict], edges: List[Dict]):
        """Ingest nodes and edges into FalkorDB with batching"""
        logger.info("Starting ingestion into FalkorDB...")
        
        # Create indexes for better performance
        try:
            self.db.query("CREATE INDEX ON :Node(id)")
            self.db.query("CREATE INDEX ON :Resource(uri)")
        except Exception as e:
            logger.warning(f"Index creation warning (may already exist): {e}")
        
        # Ingest nodes in batches
        logger.info(f"Ingesting {len(nodes)} nodes in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(nodes), self.batch_size), desc="Ingesting nodes"):
            batch_end = min(i + self.batch_size, len(nodes))
            self.ingest_nodes_batch(nodes, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        # Ingest edges in batches
        logger.info(f"Ingesting {len(edges)} edges in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(edges), self.batch_size), desc="Ingesting edges"):
            batch_end = min(i + self.batch_size, len(edges))
            self.ingest_edges_batch(edges, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        logger.info("Ingestion completed successfully!")
    
    def convert_and_ingest(self, ttl_file_path: str):
        """Main method to convert TTL file and ingest into FalkorDB"""
        try:
            # Parse TTL file
            graph = self.parse_ttl_streaming(ttl_file_path)
            
            # Convert to property graph
            nodes, edges = self.convert_to_property_graph(graph)
            
            # Ingest into FalkorDB
            self.ingest_to_falkordb(nodes, edges)
            
            # Print summary
            result = self.db.query("MATCH (n) RETURN count(n) as node_count")
            node_count = result.result_set[0][0] if result.result_set else 0
            
            result = self.db.query("MATCH ()-[r]->() RETURN count(r) as edge_count")
            edge_count = result.result_set[0][0] if result.result_set else 0
            
            logger.info(f"Final database statistics:")
            logger.info(f"  Nodes: {node_count}")
            logger.info(f"  Edges: {edge_count}")
            
        except Exception as e:
            logger.error(f"Error in conversion and ingestion: {e}")
            raise

def main():
    """Main function with example usage"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description='Convert TTL file to FalkorDB property graph')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--host', default='localhost', help='Redis/FalkorDB host (default: localhost)')
    parser.add_argument('--port', type=int, default=6379, help='Redis/FalkorDB port (default: 6379)')
    parser.add_argument('--db_name', default='knowledge_graph', help='Database name (default: knowledge_graph)')
    parser.add_argument('--batch_size', type=int, default=0, help='Batch size for ingestion (0=auto-detect)')
    parser.add_argument('--password', help='Redis password if required')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        return
    
    # Auto-detect batch size based on file size
    batch_size = args.batch_size
    if batch_size == 0:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        if file_size_mb > 1000:  # >1GB
            batch_size = 200
            logger.info(f"Large file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
        elif file_size_mb > 100:  # >100MB
            batch_size = 500
            logger.info(f"Medium file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
        else:
            batch_size = 1000
            logger.info(f"Small file detected ({file_size_mb:.1f}MB), using batch_size={batch_size}")
    
    # Test connection first
    try:
        if args.password:
            test_client = redis.Redis(host=args.host, port=args.port, password=args.password, decode_responses=True)
        else:
            test_client = redis.Redis(host=args.host, port=args.port, decode_responses=True)
        test_client.ping()
        logger.info(f"Successfully connected to FalkorDB at {args.host}:{args.port}")
    except Exception as e:
        logger.error(f"Failed to connect to FalkorDB at {args.host}:{args.port}: {e}")
        logger.error("Please ensure FalkorDB is running and accessible")
        return
    
    # Create converter
    converter = TTLToFalkorDB(
        redis_host=args.host,
        redis_port=args.port,
        db_name=args.db_name,
        password=args.password
    )
    converter.batch_size = batch_size
    
    # Convert and ingest
    converter.convert_and_ingest(args.ttl_file)

if __name__ == "__main__":
    main()
