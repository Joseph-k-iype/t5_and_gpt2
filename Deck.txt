#!/usr/bin/env python3
"""
Comprehensive GDPR/UK-GDPR Knowledge Graph & RoPA Metamodel System
Advanced Multi-Agent Architecture with LangGraph ReAct, Reflection & Supervisor Agents

This system creates a comprehensive knowledge graph encompassing all GDPR and UK-GDPR concepts,
while generating a specialized RoPA metamodel for financial institutions like HSBC.

Features:
- LangGraph ReAct agents for reasoning and action
- Reflection agents for quality assurance
- Supervisor agent for coordination
- Comprehensive GDPR/UK-GDPR knowledge graph in FalkorDB
- Vector embeddings in Elasticsearch with LLM-generated synonyms
- Financial industry-specific RoPA metamodel
- Business-friendly reporting

Author: AI Assistant
Date: 2025
Version: 4.0.0 - LangGraph Multi-Agent with Comprehensive GDPR Knowledge Graph
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Literal
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import argparse
import pickle
from pathlib import Path
import hashlib

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph for multi-agent architecture
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph components
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command
from langgraph.prebuilt import create_react_agent, ToolNode

# Global Configuration - Centralized credentials and paths
GLOBAL_CONFIG = {
    # OpenAI Configuration - Only o3-mini
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "your_openai_api_key_here"),
    "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", None),
    "OPENAI_MODEL": "o3-mini",
    
    # Elasticsearch Configuration  
    "ELASTICSEARCH_HOST": os.getenv("ELASTICSEARCH_HOST", "https://localhost:9200"),
    "ELASTICSEARCH_USERNAME": os.getenv("ELASTICSEARCH_USERNAME", "elastic"),
    "ELASTICSEARCH_PASSWORD": os.getenv("ELASTICSEARCH_PASSWORD", "changeme"),
    "ELASTICSEARCH_CA_CERTS": os.getenv("ELASTICSEARCH_CA_CERTS", None),
    "ELASTICSEARCH_VERIFY_CERTS": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "false").lower() == "true",
    
    # FalkorDB Configuration
    "FALKORDB_HOST": os.getenv("FALKORDB_HOST", "localhost"),
    "FALKORDB_PORT": int(os.getenv("FALKORDB_PORT", 6379)),
    "FALKORDB_PASSWORD": os.getenv("FALKORDB_PASSWORD", None),
    "FALKORDB_DATABASE": os.getenv("FALKORDB_DATABASE", "gdpr_comprehensive_kg"),
    
    # Document Paths
    "PDF_DOCUMENTS_PATH": os.getenv("PDF_DOCUMENTS_PATH", "./documents"),
    "OUTPUT_PATH": os.getenv("OUTPUT_PATH", "./output"),
    "MEMORY_PATH": os.getenv("MEMORY_PATH", "./memory"),
    
    # System Configuration
    "ELASTICSEARCH_INDEX": "gdpr_comprehensive_knowledge",
    "MEMORY_FILE": "gdpr_agent_memory.pkl",
    
    # Embedding Configuration
    "EMBEDDING_MODEL": "text-embedding-3-large",
    "EMBEDDING_DIMENSIONS": 3072,
    
    # Financial Industry Context
    "ORGANIZATION_TYPE": "financial_institution",
    "ORGANIZATION_NAME": "HSBC",
    "JURISDICTION_FOCUS": ["EU_GDPR", "UK_GDPR"],
    
    # Agent Configuration
    "MAX_REASONING_ITERATIONS": 5,
    "REFLECTION_THRESHOLD": 0.7,
    "SUPERVISOR_MAX_WORKERS": 6
}

# Configure logging
os.makedirs(GLOBAL_CONFIG["OUTPUT_PATH"], exist_ok=True)
os.makedirs(GLOBAL_CONFIG["MEMORY_PATH"], exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{GLOBAL_CONFIG['OUTPUT_PATH']}/gdpr_comprehensive_system.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AgentState(TypedDict):
    """Shared state for all agents in the system"""
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document Processing
    current_document: Optional[Dict[str, Any]]
    processed_documents: List[Dict[str, Any]]
    document_chunks: List[Dict[str, Any]]
    
    # Comprehensive GDPR Knowledge Graph Entities
    gdpr_articles: List[Dict[str, Any]]
    uk_gdpr_articles: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    data_subject_rights: List[Dict[str, Any]]
    principles: List[Dict[str, Any]]
    obligations: List[Dict[str, Any]]
    penalties: List[Dict[str, Any]]
    authorities: List[Dict[str, Any]]
    definitions: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    transfers: List[Dict[str, Any]]
    controllers: List[Dict[str, Any]]
    processors: List[Dict[str, Any]]
    
    # Financial Industry Specific
    financial_regulations: List[Dict[str, Any]]
    financial_data_types: List[Dict[str, Any]]
    compliance_frameworks: List[Dict[str, Any]]
    
    # Synonyms and Semantic Enrichment
    generated_synonyms: Dict[str, List[str]]
    semantic_clusters: List[Dict[str, Any]]
    entity_mappings: Dict[str, str]
    
    # Agent Coordination
    current_agent: str
    agent_results: Dict[str, Any]
    reflection_feedback: List[Dict[str, Any]]
    supervisor_decisions: List[Dict[str, Any]]
    
    # RoPA Metamodel (focused output)
    ropa_metamodel: Optional[Dict[str, Any]]
    business_report: Optional[str]
    compliance_assessment: Optional[Dict[str, Any]]
    
    # Quality and Performance
    confidence_scores: Dict[str, float]
    validation_results: List[Dict[str, Any]]

class EmbeddingEngine:
    """Advanced embedding generation with synonym enrichment"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"]
        )
        self.model = GLOBAL_CONFIG["EMBEDDING_MODEL"]
        self.dimensions = GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"]
        logger.info(f"Initialized embedding engine with {self.model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def generate_synonyms_with_llm(self, concept: str, domain_context: str = "GDPR") -> List[str]:
        """Use LLM to generate domain-specific synonyms"""
        try:
            prompt = f"""
            Generate comprehensive synonyms and related terms for the concept "{concept}" in the context of {domain_context} and UK-GDPR compliance.

            Include:
            1. Direct synonyms
            2. Related legal terms
            3. Technical variations
            4. Abbreviations and acronyms
            5. Contextual equivalents
            6. Financial industry specific terms if applicable

            Return as a JSON list of strings, maximum 15 terms.
            Focus on terms that would appear in regulatory documents, policies, and business contexts.
            """
            
            response = self.client.chat.completions.create(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                messages=[{"role": "user", "content": prompt}],
                reasoning_effort="high"
            )
            
            content = response.choices[0].message.content
            
            # Extract JSON from response
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start != -1 and json_end > json_start:
                synonyms_json = content[json_start:json_end]
                synonyms = json.loads(synonyms_json)
                return [syn.strip() for syn in synonyms if syn.strip()]
            
            return []
            
        except Exception as e:
            logger.error(f"Failed to generate synonyms for {concept}: {e}")
            return []

class VectorStore:
    """Elasticsearch-based vector storage with comprehensive GDPR schema"""
    
    def __init__(self):
        self.embedding_engine = EmbeddingEngine()
        self.client = self._create_elasticsearch_client()
        self.index_name = GLOBAL_CONFIG["ELASTICSEARCH_INDEX"]
        self._create_comprehensive_index()
    
    def _create_elasticsearch_client(self):
        """Create Elasticsearch client"""
        client_config = {
            "hosts": [GLOBAL_CONFIG["ELASTICSEARCH_HOST"]],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        if GLOBAL_CONFIG["ELASTICSEARCH_HOST"].startswith('https://'):
            client_config["verify_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]
            if GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]:
                client_config["ca_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]
            if not GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]:
                client_config["ssl_show_warn"] = False
        
        if GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"] and GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]:
            client_config["basic_auth"] = (
                GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"],
                GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]
            )
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_comprehensive_index(self):
        """Create comprehensive index for all GDPR concepts"""
        mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "stemmer", "synonym_filter"]
                        }
                    },
                    "filter": {
                        "synonym_filter": {
                            "type": "synonym",
                            "synonyms": [
                                "GDPR,General Data Protection Regulation,EU GDPR",
                                "UK GDPR,Data Protection Act 2018,UK DPA",
                                "controller,data controller",
                                "processor,data processor",
                                "RoPA,Record of Processing Activities,Article 30"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # Core content
                    "text": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer",
                        "fields": {"keyword": {"type": "keyword"}}
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                        "index": True,
                        "similarity": "cosine"
                    },
                    
                    # Document metadata
                    "document_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    
                    # GDPR Classification
                    "gdpr_category": {"type": "keyword"},  # article, principle, right, obligation, etc.
                    "article_number": {"type": "keyword"},
                    "chapter": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},  # EU_GDPR, UK_GDPR, both
                    
                    # Entity Information
                    "entity_type": {"type": "keyword"},
                    "entity_name": {"type": "text"},
                    "entity_id": {"type": "keyword"},
                    
                    # Synonyms and Related Terms
                    "synonyms": {"type": "text"},
                    "related_concepts": {"type": "text"},
                    "alternative_names": {"type": "text"},
                    
                    # Financial Industry Context
                    "financial_relevance": {"type": "float"},
                    "financial_context": {"type": "text"},
                    "regulatory_framework": {"type": "keyword"},
                    
                    # Relationships
                    "related_articles": {"type": "keyword"},
                    "cross_references": {"type": "keyword"},
                    "dependencies": {"type": "keyword"},
                    
                    # Quality Metrics
                    "confidence_score": {"type": "float"},
                    "validation_status": {"type": "keyword"},
                    "agent_source": {"type": "keyword"},
                    
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created comprehensive GDPR index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise
    
    def index_concept(self, concept_data: Dict[str, Any]):
        """Index a GDPR concept with embeddings and synonyms"""
        try:
            # Generate synonyms using LLM
            concept_name = concept_data.get("entity_name", concept_data.get("text", ""))
            domain_context = f"GDPR {concept_data.get('gdpr_category', '')}"
            synonyms = self.embedding_engine.generate_synonyms_with_llm(concept_name, domain_context)
            
            # Create enriched text for embedding
            enriched_text = f"{concept_data.get('text', '')} {' '.join(synonyms)}"
            embedding = self.embedding_engine.generate_embedding(enriched_text)
            
            # Prepare document
            doc = {
                **concept_data,
                "embedding": embedding,
                "synonyms": synonyms,
                "timestamp": datetime.now()
            }
            
            # Generate unique document ID
            doc_id = hashlib.md5(f"{concept_data.get('entity_name', '')}_{concept_data.get('gdpr_category', '')}".encode()).hexdigest()
            
            self.client.index(index=self.index_name, id=doc_id, document=doc)
            logger.debug(f"Indexed concept: {concept_name}")
            
        except Exception as e:
            logger.error(f"Failed to index concept: {e}")
    
    def semantic_search(self, query: str, filters: Dict[str, Any] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Perform semantic search with optional filters"""
        try:
            query_embedding = self.embedding_engine.generate_embedding(query)
            
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "should": [
                            {"multi_match": {
                                "query": query,
                                "fields": ["text^3", "synonyms^2", "entity_name^4"],
                                "type": "best_fields"
                            }}
                        ]
                    }
                },
                "size": top_k,
                "_source": {"excludes": ["embedding"]}
            }
            
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if isinstance(value, list):
                        filter_clauses.append({"terms": {key: value}})
                    else:
                        filter_clauses.append({"term": {key: value}})
                search_body["query"]["bool"]["filter"] = filter_clauses
            
            response = self.client.search(index=self.index_name, **search_body)
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            return [
                {**hit["_source"], "search_score": hit["_score"]}
                for hit in hits
            ]
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []

class ComprehensiveKnowledgeGraph:
    """FalkorDB-based comprehensive GDPR/UK-GDPR knowledge graph"""
    
    def __init__(self):
        connection_kwargs = {
            "host": GLOBAL_CONFIG["FALKORDB_HOST"],
            "port": GLOBAL_CONFIG["FALKORDB_PORT"]
        }
        
        if GLOBAL_CONFIG["FALKORDB_PASSWORD"]:
            connection_kwargs["password"] = GLOBAL_CONFIG["FALKORDB_PASSWORD"]
        
        try:
            self.db = FalkorDB(**connection_kwargs)
            self.graph = self.db.select_graph(GLOBAL_CONFIG["FALKORDB_DATABASE"])
            logger.info("Connected to FalkorDB for comprehensive GDPR knowledge graph")
            self._initialize_comprehensive_schema()
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def _initialize_comprehensive_schema(self):
        """Initialize comprehensive schema for GDPR/UK-GDPR knowledge graph"""
        try:
            # Create indexes for performance
            indexes = [
                # Core GDPR entities
                "CREATE INDEX FOR (a:Article) ON (a.number)",
                "CREATE INDEX FOR (c:Chapter) ON (c.number)",
                "CREATE INDEX FOR (p:Principle) ON (p.name)",
                "CREATE INDEX FOR (r:Right) ON (r.name)",
                "CREATE INDEX FOR (o:Obligation) ON (o.name)",
                "CREATE INDEX FOR (d:Definition) ON (d.term)",
                
                # Legal bases and processing
                "CREATE INDEX FOR (lb:LegalBasis) ON (lb.basis_type)",
                "CREATE INDEX FOR (pa:ProcessingActivity) ON (pa.name)",
                "CREATE INDEX FOR (dc:DataCategory) ON (dc.name)",
                "CREATE INDEX FOR (sm:SecurityMeasure) ON (sm.type)",
                
                # Organizational entities
                "CREATE INDEX FOR (ctrl:Controller) ON (ctrl.name)",
                "CREATE INDEX FOR (proc:Processor) ON (proc.name)",
                "CREATE INDEX FOR (auth:Authority) ON (auth.name)",
                
                # Financial industry specific
                "CREATE INDEX FOR (fin:FinancialRegulation) ON (fin.name)",
                "CREATE INDEX FOR (fd:FinancialData) ON (fd.type)",
                
                # Document and source tracking
                "CREATE INDEX FOR (doc:Document) ON (doc.source)",
                "CREATE INDEX FOR (ent:Entity) ON (ent.name)"
            ]
            
            for index_query in indexes:
                try:
                    self.graph.query(index_query)
                except:
                    pass  # Index might already exist
            
            logger.info("Comprehensive knowledge graph schema initialized")
            
        except Exception as e:
            logger.warning(f"Could not initialize all graph indexes: {e}")
    
    def add_gdpr_article(self, article_data: Dict[str, Any]):
        """Add GDPR article with comprehensive relationships"""
        try:
            article_num = str(article_data.get("article_number", "")).replace("'", "\\'")
            title = str(article_data.get("title", "")).replace("'", "\\'")
            content = str(article_data.get("content", "")).replace("'", "\\'")
            chapter = str(article_data.get("chapter", "")).replace("'", "\\'")
            jurisdiction = str(article_data.get("jurisdiction", "EU_GDPR")).replace("'", "\\'")
            
            query = f"""
            MERGE (a:Article {{number: '{article_num}', jurisdiction: '{jurisdiction}'}})
            SET a.title = '{title}',
                a.content = '{content}',
                a.chapter = '{chapter}',
                a.timestamp = datetime()
            
            MERGE (c:Chapter {{number: '{chapter}', jurisdiction: '{jurisdiction}'}})
            MERGE (a)-[:BELONGS_TO]->(c)
            """
            
            self.graph.query(query)
            logger.debug(f"Added article {article_num} to knowledge graph")
            
        except Exception as e:
            logger.error(f"Failed to add article: {e}")
    
    def add_comprehensive_entity(self, entity_type: str, entity_data: Dict[str, Any]):
        """Add any type of GDPR entity to the knowledge graph"""
        try:
            entity_name = str(entity_data.get("name", "")).replace("'", "\\'")
            entity_id = str(entity_data.get("id", entity_name)).replace("'", "\\'")
            
            # Create base entity
            query = f"""
            MERGE (e:{entity_type} {{name: '{entity_name}'}})
            SET e.id = '{entity_id}',
                e.jurisdiction = '{entity_data.get("jurisdiction", "EU_GDPR")}',
                e.confidence = {entity_data.get("confidence", 1.0)},
                e.timestamp = datetime()
            """
            
            # Add type-specific properties
            if entity_type == "LegalBasis":
                basis_type = str(entity_data.get("basis_type", "")).replace("'", "\\'")
                article_ref = str(entity_data.get("article_reference", "")).replace("'", "\\'")
                query += f"""
                SET e.basis_type = '{basis_type}',
                    e.article_reference = '{article_ref}'
                """
            
            elif entity_type == "ProcessingActivity":
                purpose = str(entity_data.get("purpose", "")).replace("'", "\\'")
                legal_basis = str(entity_data.get("legal_basis", "")).replace("'", "\\'")
                query += f"""
                SET e.purpose = '{purpose}',
                    e.legal_basis = '{legal_basis}'
                """
            
            elif entity_type == "DataCategory":
                sensitivity = str(entity_data.get("sensitivity", "normal")).replace("'", "\\'")
                special_category = str(entity_data.get("special_category", "false")).lower()
                query += f"""
                SET e.sensitivity = '{sensitivity}',
                    e.special_category = {special_category}
                """
            
            # Add other type-specific properties as needed
            for key, value in entity_data.items():
                if key not in ["name", "id", "jurisdiction", "confidence"] and isinstance(value, str):
                    safe_value = str(value).replace("'", "\\'")
                    query += f"""
                    SET e.{key} = '{safe_value}'
                    """
            
            self.graph.query(query)
            
        except Exception as e:
            logger.error(f"Failed to add {entity_type} entity: {e}")
    
    def create_relationships(self, relationships: List[Dict[str, Any]]):
        """Create relationships between entities"""
        try:
            for rel in relationships:
                source_name = str(rel.get("source", "")).replace("'", "\\'")
                target_name = str(rel.get("target", "")).replace("'", "\\'")
                rel_type = rel.get("type", "RELATED_TO")
                confidence = rel.get("confidence", 1.0)
                
                if source_name and target_name:
                    query = f"""
                    MATCH (a) WHERE a.name = '{source_name}' OR a.id = '{source_name}'
                    MATCH (b) WHERE b.name = '{target_name}' OR b.id = '{target_name}'
                    MERGE (a)-[r:{rel_type}]->(b)
                    SET r.confidence = {confidence},
                        r.timestamp = datetime()
                    """
                    self.graph.query(query)
            
        except Exception as e:
            logger.error(f"Failed to create relationships: {e}")
    
    def query_comprehensive_knowledge(self, concept: str, max_depth: int = 3) -> List[Dict[str, Any]]:
        """Query comprehensive knowledge about any GDPR concept"""
        try:
            concept_escaped = concept.replace("'", "\\'")
            
            query = f"""
            MATCH path = (start)-[*1..{max_depth}]-(related)
            WHERE (
                start.name CONTAINS '{concept_escaped}' OR 
                start.title CONTAINS '{concept_escaped}' OR
                start.purpose CONTAINS '{concept_escaped}' OR
                start.basis_type CONTAINS '{concept_escaped}'
            )
            RETURN DISTINCT 
                start, 
                related, 
                relationships(path), 
                length(path) as distance,
                labels(start) as start_labels,
                labels(related) as related_labels
            ORDER BY distance, start.confidence DESC
            LIMIT 20
            """
            
            result = self.graph.query(query)
            
            formatted_results = []
            for record in result.result_set:
                formatted_results.append({
                    "start_entity": self._format_node(record[0]),
                    "related_entity": self._format_node(record[1]),
                    "relationship_path": [str(rel) for rel in record[2]],
                    "distance": record[3],
                    "start_labels": record[4],
                    "related_labels": record[5],
                    "relevance_score": 1.0 / (record[3] + 1)
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Knowledge query failed: {e}")
            return []
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if not node:
            return None
        
        try:
            if hasattr(node, 'properties'):
                properties = dict(node.properties)
                properties['labels'] = list(node.labels) if hasattr(node, 'labels') else []
                return properties
            else:
                return {"id": str(node)}
        except:
            return {"id": str(node)}

# LangGraph Agent Tools
@tool
def search_vector_knowledge(query: str, category: str = None) -> List[Dict[str, Any]]:
    """Search vector knowledge base for GDPR concepts"""
    vector_store = VectorStore()
    filters = {"gdpr_category": category} if category else None
    return vector_store.semantic_search(query, filters=filters, top_k=5)

@tool 
def query_knowledge_graph(concept: str) -> List[Dict[str, Any]]:
    """Query the comprehensive GDPR knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    return kg.query_comprehensive_knowledge(concept, max_depth=2)

@tool
def generate_concept_synonyms(concept: str, domain: str = "GDPR") -> List[str]:
    """Generate synonyms for a GDPR concept using LLM"""
    embedding_engine = EmbeddingEngine()
    return embedding_engine.generate_synonyms_with_llm(concept, domain)

# ReAct Agent for GDPR Analysis
class GDPRAnalysisReActAgent:
    """ReAct agent for comprehensive GDPR analysis"""
    
    def __init__(self):
        llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        tools = [search_vector_knowledge, query_knowledge_graph, generate_concept_synonyms]
        
        self.agent = create_react_agent(
            llm,
            tools,
            state_modifier="You are an expert GDPR analyst. Use the available tools to research and analyze GDPR concepts comprehensively."
        )
    
    def analyze(self, text: str, focus_area: str = "general") -> Dict[str, Any]:
        """Analyze text for GDPR concepts using ReAct pattern"""
        try:
            prompt = f"""
            Analyze this text for GDPR and UK-GDPR concepts: {text}
            
            Focus area: {focus_area}
            
            Use the available tools to:
            1. Search for related concepts in the vector knowledge base
            2. Query the knowledge graph for relationships
            3. Generate relevant synonyms for key concepts
            
            Provide comprehensive analysis including:
            - Identified GDPR concepts
            - Article references
            - Compliance implications
            - Related concepts and synonyms
            """
            
            result = self.agent.invoke({"messages": [("user", prompt)]})
            
            return {
                "agent_type": "react",
                "analysis": result.get("messages", [])[-1].content if result.get("messages") else "",
                "tools_used": self._extract_tool_usage(result),
                "confidence": 0.8
            }
            
        except Exception as e:
            logger.error(f"ReAct analysis failed: {e}")
            return {"error": str(e)}
    
    def _extract_tool_usage(self, result: Dict[str, Any]) -> List[str]:
        """Extract which tools were used in the analysis"""
        if "messages" in result:
            tool_usage = []
            for msg in result["messages"]:
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_usage.append(tool_call.get('name', 'unknown_tool'))
            return tool_usage
        return []

# Reflection Agent for Quality Assurance
class ReflectionAgent:
    """Agent for reflecting on and improving analysis quality"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
    
    def reflect_on_analysis(self, analysis: Dict[str, Any], original_text: str) -> Dict[str, Any]:
        """Reflect on the quality of GDPR analysis"""
        try:
            reflection_prompt = f"""
            Original text: {original_text}
            
            Analysis to reflect on: {json.dumps(analysis, indent=2)}
            
            As a GDPR compliance expert, critically evaluate this analysis:
            
            1. Completeness: Are all relevant GDPR concepts identified?
            2. Accuracy: Are the article references and interpretations correct?
            3. Depth: Is the analysis sufficiently detailed for compliance purposes?
            4. Financial Industry Relevance: How relevant is this for a financial institution like HSBC?
            5. UK-GDPR Considerations: Are UK-specific aspects properly addressed?
            
            Provide:
            - Quality score (0-1)
            - Specific improvements needed
            - Missing concepts or relationships
            - Enhanced recommendations
            
            Return as JSON with fields: quality_score, improvements, missing_concepts, recommendations
            """
            
            response = self.llm.invoke([HumanMessage(content=reflection_prompt)])
            content = response.content
            
            # Try to extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    reflection_data = json.loads(content[json_start:json_end])
                    return {
                        "agent_type": "reflection",
                        "reflection": reflection_data,
                        "raw_response": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "reflection",
                "reflection": {"raw_analysis": content},
                "quality_score": 0.5,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Reflection failed: {e}")
            return {"error": str(e)}

# Supervisor Agent for Coordination
class SupervisorAgent:
    """Supervisor agent to coordinate multiple specialized agents"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
        )
        
        # Initialize specialized agents
        self.react_agent = GDPRAnalysisReActAgent()
        self.reflection_agent = ReflectionAgent()
        
        # Agent capabilities
        self.agents = {
            "gdpr_analysis": "Comprehensive GDPR and UK-GDPR analysis with tool usage",
            "reflection": "Quality assurance and improvement recommendations",
            "synthesis": "Combining multiple analyses into coherent insights"
        }
    
    def coordinate_analysis(self, document_text: str, analysis_goals: List[str]) -> Dict[str, Any]:
        """Coordinate multiple agents for comprehensive analysis"""
        try:
            coordination_plan = self._create_coordination_plan(document_text, analysis_goals)
            
            results = {
                "coordination_plan": coordination_plan,
                "agent_results": {},
                "synthesis": None,
                "overall_confidence": 0.0
            }
            
            # Execute ReAct analysis
            if "gdpr_analysis" in coordination_plan["required_agents"]:
                gdpr_result = self.react_agent.analyze(
                    document_text, 
                    focus_area=coordination_plan.get("focus_area", "general")
                )
                results["agent_results"]["gdpr_analysis"] = gdpr_result
            
            # Execute reflection if quality threshold not met
            if "reflection" in coordination_plan["required_agents"]:
                if "gdpr_analysis" in results["agent_results"]:
                    reflection_result = self.reflection_agent.reflect_on_analysis(
                        results["agent_results"]["gdpr_analysis"],
                        document_text
                    )
                    results["agent_results"]["reflection"] = reflection_result
                    
                    # Re-run analysis if improvements needed
                    quality_score = reflection_result.get("reflection", {}).get("quality_score", 0.5)
                    if quality_score < GLOBAL_CONFIG["REFLECTION_THRESHOLD"]:
                        logger.info("Re-running analysis based on reflection feedback")
                        improved_analysis = self.react_agent.analyze(
                            document_text + "\n\nImprovement guidance: " + 
                            str(reflection_result.get("reflection", {})),
                            focus_area="improvement"
                        )
                        results["agent_results"]["gdpr_analysis_improved"] = improved_analysis
            
            # Synthesize results
            results["synthesis"] = self._synthesize_results(results["agent_results"])
            results["overall_confidence"] = self._calculate_overall_confidence(results["agent_results"])
            
            return results
            
        except Exception as e:
            logger.error(f"Coordination failed: {e}")
            return {"error": str(e)}
    
    def _create_coordination_plan(self, text: str, goals: List[str]) -> Dict[str, Any]:
        """Create a plan for agent coordination"""
        try:
            planning_prompt = f"""
            Text to analyze: {text[:1000]}...
            Analysis goals: {goals}
            
            Create a coordination plan specifying:
            1. Which agents are required: gdpr_analysis, reflection, synthesis
            2. Focus area for analysis
            3. Priority concepts to identify
            4. Success criteria
            
            Available agents: {self.agents}
            
            Return as JSON with fields: required_agents, focus_area, priority_concepts, success_criteria
            """
            
            response = self.llm.invoke([HumanMessage(content=planning_prompt)])
            content = response.content
            
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    return json.loads(content[json_start:json_end])
                except json.JSONDecodeError:
                    pass
            
            # Fallback plan
            return {
                "required_agents": ["gdpr_analysis", "reflection"],
                "focus_area": "comprehensive",
                "priority_concepts": ["processing_activity", "legal_basis", "data_category"],
                "success_criteria": "Complete GDPR compliance analysis"
            }
            
        except Exception as e:
            logger.error(f"Planning failed: {e}")
            return {
                "required_agents": ["gdpr_analysis"],
                "focus_area": "basic",
                "priority_concepts": [],
                "success_criteria": "Basic analysis"
            }
    
    def _synthesize_results(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize results from multiple agents"""
        try:
            synthesis_prompt = f"""
            Synthesize these agent results into comprehensive GDPR insights:
            
            {json.dumps(agent_results, indent=2, default=str)[:3000]}...
            
            Create a synthesis including:
            1. Key GDPR concepts identified
            2. Compliance implications
            3. Financial industry specific considerations
            4. UK vs EU GDPR differences noted
            5. Recommended actions
            
            Focus on creating actionable insights for RoPA compliance.
            """
            
            response = self.llm.invoke([HumanMessage(content=synthesis_prompt)])
            
            return {
                "synthesis_type": "comprehensive",
                "content": response.content,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            return {"error": str(e)}
    
    def _calculate_overall_confidence(self, agent_results: Dict[str, Any]) -> float:
        """Calculate overall confidence from agent results"""
        confidences = []
        for result in agent_results.values():
            if isinstance(result, dict) and "confidence" in result:
                confidences.append(result["confidence"])
        
        return sum(confidences) / len(confidences) if confidences else 0.5

# Main System Class
class ComprehensiveGDPRSystem:
    """Main system integrating all components"""
    
    def __init__(self):
        self.vector_store = VectorStore()
        self.knowledge_graph = ComprehensiveKnowledgeGraph()
        self.supervisor = SupervisorAgent()
        self.state = self._initialize_state()
        
        logger.info("Comprehensive GDPR System initialized")
    
    def _initialize_state(self) -> AgentState:
        """Initialize system state"""
        return AgentState(
            messages=[],
            current_document=None,
            processed_documents=[],
            document_chunks=[],
            gdpr_articles=[],
            uk_gdpr_articles=[],
            legal_bases=[],
            data_subject_rights=[],
            principles=[],
            obligations=[],
            penalties=[],
            authorities=[],
            definitions=[],
            processing_activities=[],
            data_categories=[],
            security_measures=[],
            transfers=[],
            controllers=[],
            processors=[],
            financial_regulations=[],
            financial_data_types=[],
            compliance_frameworks=[],
            generated_synonyms={},
            semantic_clusters=[],
            entity_mappings={},
            current_agent="supervisor",
            agent_results={},
            reflection_feedback=[],
            supervisor_decisions=[],
            ropa_metamodel=None,
            business_report=None,
            compliance_assessment=None,
            confidence_scores={},
            validation_results=[]
        )
    
    def process_documents(self, document_paths: List[str] = None) -> Dict[str, Any]:
        """Process documents through the comprehensive system"""
        if document_paths is None:
            pdf_path = GLOBAL_CONFIG["PDF_DOCUMENTS_PATH"]
            document_paths = [
                os.path.join(pdf_path, f) for f in os.listdir(pdf_path)
                if f.lower().endswith('.pdf')
            ]
        
        if not document_paths:
            raise ValueError("No PDF documents found to process")
        
        logger.info(f"Processing {len(document_paths)} documents through comprehensive GDPR system")
        
        results = {
            "documents_processed": 0,
            "concepts_extracted": 0,
            "knowledge_graph_entities": 0,
            "vector_embeddings": 0,
            "synonyms_generated": 0
        }
        
        for doc_path in document_paths:
            try:
                logger.info(f"Processing document: {doc_path}")
                
                # Extract text
                doc_text = self._extract_text_from_pdf(doc_path)
                
                # Chunk document
                chunks = self._chunk_document(doc_text, doc_path)
                
                for chunk in chunks:
                    # Coordinate analysis through supervisor
                    analysis_goals = [
                        "comprehensive_gdpr_analysis",
                        "uk_gdpr_compliance",
                        "financial_industry_relevance",
                        "ropa_elements"
                    ]
                    
                    coordination_result = self.supervisor.coordinate_analysis(
                        chunk["text"], 
                        analysis_goals
                    )
                    
                    # Extract and store concepts
                    concepts = self._extract_concepts_from_analysis(coordination_result)
                    
                    for concept in concepts:
                        # Store in vector database
                        self.vector_store.index_concept(concept)
                        results["vector_embeddings"] += 1
                        
                        # Store in knowledge graph
                        self.knowledge_graph.add_comprehensive_entity(
                            concept.get("entity_type", "Entity"),
                            concept
                        )
                        results["knowledge_graph_entities"] += 1
                        
                        # Generate and store synonyms
                        if concept.get("entity_name"):
                            synonyms = self.vector_store.embedding_engine.generate_synonyms_with_llm(
                                concept["entity_name"],
                                concept.get("gdpr_category", "GDPR")
                            )
                            self.state["generated_synonyms"][concept["entity_name"]] = synonyms
                            results["synonyms_generated"] += len(synonyms)
                    
                    results["concepts_extracted"] += len(concepts)
                
                results["documents_processed"] += 1
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Generate RoPA metamodel and business report
        self._generate_ropa_metamodel()
        self._generate_business_report()
        
        return results
    
    def _extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF document"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            return full_text
            
        except Exception as e:
            logger.error(f"Failed to extract text from {pdf_path}: {e}")
            return ""
    
    def _chunk_document(self, text: str, source: str) -> List[Dict[str, Any]]:
        """Chunk document for processing"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=400,
            separators=["\n\n", "\n", "Article ", "Section ", ".", " "]
        )
        
        chunks = text_splitter.split_text(text)
        
        return [
            {
                "chunk_id": f"chunk_{i}_{os.path.basename(source)}",
                "text": chunk,
                "source": source,
                "chunk_index": i
            }
            for i, chunk in enumerate(chunks)
        ]
    
    def _extract_concepts_from_analysis(self, coordination_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract GDPR concepts from coordination result"""
        concepts = []
        
        try:
            synthesis = coordination_result.get("synthesis", {})
            content = synthesis.get("content", "")
            
            # Extract concepts using LLM
            extraction_prompt = f"""
            Extract GDPR concepts from this analysis: {content}
            
            For each concept found, return:
            - entity_name: Name of the concept
            - entity_type: Type (Article, Principle, Right, Obligation, etc.)
            - gdpr_category: Category classification
            - jurisdiction: EU_GDPR, UK_GDPR, or both
            - financial_relevance: Score 0-1 for financial industry relevance
            - confidence: Confidence score 0-1
            
            Return as JSON array of concept objects.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=extraction_prompt)])
            content = response.content
            
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    extracted_concepts = json.loads(content[json_start:json_end])
                    concepts.extend(extracted_concepts)
                except json.JSONDecodeError:
                    pass
            
        except Exception as e:
            logger.error(f"Concept extraction failed: {e}")
        
        return concepts
    
    def _generate_ropa_metamodel(self):
        """Generate RoPA metamodel focused on financial industry"""
        try:
            # Query comprehensive knowledge for RoPA-relevant concepts
            ropa_concepts = []
            
            ropa_queries = [
                "processing activity",
                "controller",
                "processor", 
                "legal basis",
                "data category",
                "retention period",
                "security measures",
                "international transfers"
            ]
            
            for query in ropa_queries:
                vector_results = self.vector_store.semantic_search(
                    query, 
                    filters={"financial_relevance": {"gte": 0.5}},
                    top_k=10
                )
                graph_results = self.knowledge_graph.query_comprehensive_knowledge(query)
                
                ropa_concepts.extend(vector_results)
                ropa_concepts.extend(graph_results)
            
            # Generate metamodel using LLM
            metamodel_prompt = f"""
            Create a comprehensive RoPA metamodel for financial institutions based on these GDPR concepts:
            
            {json.dumps(ropa_concepts[:50], indent=2, default=str)[:5000]}...
            
            The metamodel should include:
            1. Core RoPA entities and their attributes
            2. Relationships between entities
            3. Financial industry specific extensions
            4. UK GDPR vs EU GDPR considerations
            5. Validation rules and constraints
            6. Implementation guidance
            
            Structure as a comprehensive metamodel suitable for {GLOBAL_CONFIG['ORGANIZATION_NAME']}.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=metamodel_prompt)])
            
            self.state["ropa_metamodel"] = {
                "metamodel_content": response.content,
                "source_concepts": len(ropa_concepts),
                "generated_timestamp": datetime.now().isoformat(),
                "organization_context": GLOBAL_CONFIG["ORGANIZATION_NAME"],
                "jurisdiction_focus": GLOBAL_CONFIG["JURISDICTION_FOCUS"]
            }
            
            logger.info("RoPA metamodel generated successfully")
            
        except Exception as e:
            logger.error(f"Metamodel generation failed: {e}")
    
    def _generate_business_report(self):
        """Generate business-friendly compliance report"""
        try:
            report_prompt = f"""
            Generate a comprehensive business report for {GLOBAL_CONFIG['ORGANIZATION_NAME']} on GDPR compliance status.
            
            Based on the analysis of {len(self.state['processed_documents'])} documents and 
            {len(self.state['generated_synonyms'])} concept mappings.
            
            Include:
            # Executive Summary
            - Overall compliance status
            - Key findings and risks
            - Strategic recommendations
            
            # GDPR Knowledge Base Analysis
            - Comprehensive concepts identified
            - UK vs EU GDPR considerations
            - Financial industry specific implications
            
            # Record of Processing Activities (RoPA) Readiness
            - Article 30 compliance assessment
            - Required RoPA elements status
            - Implementation recommendations
            
            # Risk Assessment
            - High-risk processing activities
            - Data protection impact assessment needs
            - Breach notification preparedness
            
            # Action Plan
            - Immediate priorities
            - Medium-term implementation steps
            - Long-term compliance maintenance
            
            # Financial Industry Considerations
            - Regulatory alignment (FCA, PRA, etc.)
            - Customer data protection
            - Third-party risk management
            
            Make it executive-friendly with clear action items and business impact analysis.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"]
            )
            
            response = llm.invoke([HumanMessage(content=report_prompt)])
            
            self.state["business_report"] = response.content
            
            logger.info("Business report generated successfully")
            
        except Exception as e:
            logger.error(f"Business report generation failed: {e}")
    
    def search_comprehensive_knowledge(self, query: str) -> Dict[str, Any]:
        """Search across both vector and graph knowledge"""
        vector_results = self.vector_store.semantic_search(query, top_k=10)
        graph_results = self.knowledge_graph.query_comprehensive_knowledge(query)
        
        return {
            "query": query,
            "vector_results": vector_results,
            "graph_results": graph_results,
            "total_results": len(vector_results) + len(graph_results)
        }
    
    def save_results(self) -> Dict[str, str]:
        """Save all results to files"""
        output_files = {}
        
        try:
            # Save RoPA metamodel
            if self.state["ropa_metamodel"]:
                metamodel_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "ropa_metamodel.json")
                with open(metamodel_file, 'w') as f:
                    json.dump(self.state["ropa_metamodel"], f, indent=2)
                output_files["metamodel"] = metamodel_file
            
            # Save business report
            if self.state["business_report"]:
                report_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "business_compliance_report.md")
                with open(report_file, 'w') as f:
                    f.write(self.state["business_report"])
                output_files["report"] = report_file
            
            # Save synonyms
            synonyms_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "generated_synonyms.json")
            with open(synonyms_file, 'w') as f:
                json.dump(self.state["generated_synonyms"], f, indent=2)
            output_files["synonyms"] = synonyms_file
            
            # Save comprehensive state
            state_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "system_state.json")
            with open(state_file, 'w') as f:
                # Convert state to JSON serializable format
                serializable_state = {}
                for key, value in self.state.items():
                    if key != "messages":  # Skip complex message objects
                        serializable_state[key] = value
                json.dump(serializable_state, f, indent=2, default=str)
            output_files["state"] = state_file
            
            logger.info(f"Results saved to {len(output_files)} files")
            return output_files
            
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return {}

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description="Comprehensive GDPR Knowledge Graph & RoPA System")
    parser.add_argument("--process", nargs="*", help="Process documents (paths optional)")
    parser.add_argument("--search", type=str, help="Search comprehensive knowledge base")
    parser.add_argument("--generate-report", action="store_true", help="Generate business report only")
    parser.add_argument("--save-results", action="store_true", help="Save all results to files")
    parser.add_argument("--config", type=str, help="Path to custom configuration file")
    
    args = parser.parse_args()
    
    try:
        # Load custom configuration if provided
        if args.config and os.path.exists(args.config):
            with open(args.config, 'r') as f:
                custom_config = json.load(f)
                GLOBAL_CONFIG.update(custom_config)
                logger.info(f"Loaded custom configuration from {args.config}")
        
        # Initialize comprehensive system
        system = ComprehensiveGDPRSystem()
        print(f" Comprehensive GDPR System initialized for {GLOBAL_CONFIG['ORGANIZATION_NAME']}")
        
        # Default: run complete workflow if no arguments
        if not any(vars(args).values()):
            print(" Running complete GDPR knowledge graph & RoPA workflow...")
            
            # Process documents
            print("\n Step 1: Processing documents through multi-agent system...")
            result = system.process_documents()
            print(f" Processing completed:")
            print(f"   Documents: {result['documents_processed']}")
            print(f"   Concepts: {result['concepts_extracted']}")
            print(f"   KG Entities: {result['knowledge_graph_entities']}")
            print(f"   Vector Embeddings: {result['vector_embeddings']}")
            print(f"   Synonyms Generated: {result['synonyms_generated']}")
            
            # Save results
            print("\n Step 2: Saving results...")
            output_files = system.save_results()
            for file_type, file_path in output_files.items():
                print(f"   {file_type.title()}: {file_path}")
            
            print(f"\n Complete workflow finished!")
            print(f" Check output directory: {GLOBAL_CONFIG['OUTPUT_PATH']}")
            return
        
        # Execute individual operations
        if args.process is not None:
            result = system.process_documents(args.process if args.process else None)
            print(f" Document processing completed:")
            for key, value in result.items():
                print(f"   {key.replace('_', ' ').title()}: {value}")
        
        if args.search:
            results = system.search_comprehensive_knowledge(args.search)
            print(f" Search Results for '{args.search}':")
            print(f"   Vector results: {len(results['vector_results'])}")
            print(f"   Graph results: {len(results['graph_results'])}")
            print(f"   Total: {results['total_results']}")
        
        if args.generate_report:
            system._generate_business_report()
            print(" Business report generated")
        
        if args.save_results:
            output_files = system.save_results()
            print(f" Results saved to {len(output_files)} files:")
            for file_type, file_path in output_files.items():
                print(f"   {file_type.title()}: {file_path}")
    
    except Exception as e:
        print(f" System error: {e}")
        logger.error(f"System error: {e}")

if __name__ == "__main__":
    main()
