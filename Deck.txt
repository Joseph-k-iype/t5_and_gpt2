import os
import logging
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from dataclasses import dataclass
import asyncio
import time
from enum import Enum

# Core imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool, tool
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field  # Using Pydantic v2
from langchain_community.graphs import FalkorDBGraph
from langchain_community.chains import FalkorDBQAChain

# LangGraph imports
from langgraph.prebuilt import create_react_agent
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt.tool_node import ToolNode

# FalkorDB imports
from falkordb import FalkorDB

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # Optional custom base URL
    FALKORDB_HOST = os.getenv("FALKORDB_HOST", "localhost")
    FALKORDB_PORT = int(os.getenv("FALKORDB_PORT", 6379))
    GRAPH_NAME = os.getenv("GRAPH_NAME", "knowledge_graph")

# State management for LangGraph
class GraphRAGState(TypedDict):
    messages: Annotated[List[Any], "Chat messages"]
    original_query: str
    decomposed_queries: List[str]
    query_route: str  # "vector" or "graph" or "hybrid"
    vector_results: List[Dict]
    graph_results: List[Dict]
    reasoning_context: str
    final_answer: str

# Query routing model
class QueryRoute(BaseModel):
    """Route a user query to the most relevant approach."""
    route: str = Field(..., description="Route: 'vector', 'graph', or 'hybrid'")
    reasoning: str = Field(..., description="Reasoning for the routing decision")

# Decomposed query model
class DecomposedQuery(BaseModel):
    """Decomposed sub-queries from a complex query."""
    sub_queries: List[str] = Field(..., description="List of sub-queries")
    reasoning: str = Field(..., description="Reasoning for decomposition")

# Multi-query generator
class MultiQueryGenerator:
    def __init__(self, llm):
        self.llm = llm
        self.decomposition_prompt = PromptTemplate(
            template="""You are an expert at breaking down complex queries into simpler sub-queries.
            Your task is to decompose the following query into multiple focused sub-queries that can be answered independently.
            
            Original Query: {query}
            
            Guidelines:
            1. If the query is simple, keep it as is
            2. For complex queries, break them into 2-5 sub-queries
            3. Each sub-query should be self-contained
            4. Focus on entities, relationships, and key concepts
            
            Examples:
            Query: "What are the connections between Company A and Company B through their executives?"
            Sub-queries: [
                "Who are the executives of Company A?",
                "Who are the executives of Company B?", 
                "What connections exist between these executive groups?"
            ]
            
            Query: "Find research papers about AI safety published after 2020"
            Sub-queries: ["Find research papers about AI safety published after 2020"]
            
            Decompose the query:""",
            input_variables=["query"]
        )
    
    async def decompose_query(self, query: str) -> DecomposedQuery:
        """Decompose a complex query into sub-queries."""
        try:
            prompt = self.decomposition_prompt.format(query=query)
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            # Parse the response to extract sub-queries
            # In a production system, you'd use structured output
            lines = response.content.strip().split('\n')
            sub_queries = []
            
            for line in lines:
                line = line.strip()
                if line and not line.startswith(('Query:', 'Sub-queries:', 'Examples:')):
                    # Clean up the sub-query
                    if line.startswith('"') and line.endswith('"'):
                        line = line[1:-1]
                    if line.startswith(('- ', '* ', '1. ', '2. ', '3. ', '4. ', '5. ')):
                        line = line[2:] if line.startswith(('- ', '* ')) else line[3:]
                    if line:
                        sub_queries.append(line)
            
            return DecomposedQuery(
                sub_queries=sub_queries if sub_queries else [query],
                reasoning=f"Decomposed into {len(sub_queries)} sub-queries"
            )
        except Exception as e:
            logger.error(f"Error in query decomposition: {e}")
            return DecomposedQuery(sub_queries=[query], reasoning="Used original query due to error")

# Query router
class QueryRouter:
    def __init__(self, llm):
        self.llm = llm
        self.routing_prompt = PromptTemplate(
            template="""You are an expert query router for a GraphRAG system.
            Your task is to determine the best approach for answering a query.
            
            Routes available:
            - "vector": For semantic similarity searches, concept matching, general knowledge
            - "graph": For relationship-based queries, entity connections, structural analysis
            - "hybrid": For complex queries requiring both semantic and structural analysis
            
            Query: {query}
            
            Consider:
            1. Does the query ask about relationships between entities? → graph
            2. Does the query need semantic understanding of concepts? → vector  
            3. Does the query require both relationship analysis AND semantic understanding? → hybrid
            4. Is the query about finding similar concepts or documents? → vector
            5. Is the query about connections, paths, or network analysis? → graph
            
            Route the query and explain your reasoning:""",
            input_variables=["query"]
        )
    
    async def route_query(self, query: str) -> QueryRoute:
        """Route a query to the appropriate processing path."""
        try:
            prompt = self.routing_prompt.format(query=query)
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            content = response.content.lower()
            
            # Simple routing logic based on response content
            if "hybrid" in content:
                route = "hybrid"
            elif "graph" in content and "vector" not in content:
                route = "graph"
            elif "vector" in content and "graph" not in content:
                route = "vector"
            else:
                # Default to hybrid for complex cases
                route = "hybrid"
            
            return QueryRoute(route=route, reasoning=response.content)
        except Exception as e:
            logger.error(f"Error in query routing: {e}")
            return QueryRoute(route="hybrid", reasoning="Defaulted to hybrid due to error")

# FalkorDB tools for React agent
@tool
async def falkordb_cypher_query(query: str) -> str:
    """Execute a Cypher query against the FalkorDB graph database.
    
    Args:
        query: Cypher query string to execute
        
    Returns:
        Query results as a formatted string
    """
    try:
        db = FalkorDB(host=Config.FALKORDB_HOST, port=Config.FALKORDB_PORT)
        graph = db.select_graph(Config.GRAPH_NAME)
        result = graph.query(query)
        
        # Format results without artificial limits
        if result.result_set:
            formatted_results = []
            total_records = len(result.result_set)
            
            for record in result.result_set:
                formatted_results.append(str(record))
            
            # Return full results with metadata
            return f"Query executed successfully. Found {total_records} records:\n" + "\n".join(formatted_results)
        else:
            return "Query executed successfully but returned no results."
            
    except Exception as e:
        return f"Error executing Cypher query: {str(e)}"

@tool
async def falkordb_vector_search(query: str, limit: Optional[int] = None) -> str:
    """Perform vector similarity search on graph nodes.
    
    Args:
        query: Search query for semantic similarity
        limit: Maximum number of results to return (None for unlimited)
        
    Returns:
        Search results as a formatted string
    """
    try:
        # Note: This is a simplified example. In practice, you'd use FalkorDB's vector index
        limit_clause = f"LIMIT {limit}" if limit else ""
        cypher_query = f"""
        CALL db.idx.fulltext.queryNodes('entity_index', '{query}') 
        YIELD node, score 
        RETURN node.name, node.description, score 
        ORDER BY score DESC
        {limit_clause}
        """
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in vector search: {str(e)}"

@tool
async def falkordb_relationship_analysis(entity: str, depth: int = 2, limit: Optional[int] = None) -> str:
    """Analyze relationships for a given entity up to specified depth.
    
    Args:
        entity: Name of the entity to analyze
        depth: Maximum relationship depth to explore
        limit: Maximum number of results to return (None for unlimited)
        
    Returns:
        Relationship analysis results
    """
    try:
        limit_clause = f"LIMIT {limit}" if limit else ""
        cypher_query = f"""
        MATCH (e {{name: '{entity}'}})-[r*1..{depth}]->(connected)
        RETURN e.name, type(r[0]) as relationship_type, connected.name, length(r) as depth
        ORDER BY depth, relationship_type
        {limit_clause}
        """
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in relationship analysis: {str(e)}"

@tool
async def falkordb_graph_traversal(start_node: str, traversal_pattern: str, max_depth: int = 10) -> str:
    """Perform complex graph traversal for large-scale analysis.
    
    Args:
        start_node: Starting node for traversal
        traversal_pattern: Cypher pattern for traversal (e.g., "-[:CONNECTED_TO*1..5]->")
        max_depth: Maximum traversal depth for large graphs
        
    Returns:
        Traversal results with path information
    """
    try:
        cypher_query = f"""
        MATCH path = (start {{name: '{start_node}'}}){traversal_pattern}(end)
        WHERE length(path) <= {max_depth}
        RETURN 
            start.name as start_node,
            end.name as end_node,
            length(path) as path_length,
            [node in nodes(path) | node.name] as path_nodes,
            [rel in relationships(path) | type(rel)] as relationship_types
        ORDER BY path_length
        """
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in graph traversal: {str(e)}"

@tool
async def falkordb_community_detection(algorithm: str = "leiden", min_community_size: int = 1) -> str:
    """Detect communities in large graphs using graph algorithms.
    
    Args:
        algorithm: Community detection algorithm (leiden, louvain, etc.)
        min_community_size: Minimum size for communities to be returned
        
    Returns:
        Community detection results
    """
    try:
        # Note: This depends on FalkorDB's graph algorithm support
        if algorithm.lower() == "leiden":
            cypher_query = f"""
            CALL algo.leiden.stream()
            YIELD nodeId, community
            MATCH (n) WHERE id(n) = nodeId
            WITH community, collect(n.name) as members
            WHERE size(members) >= {min_community_size}
            RETURN community, members, size(members) as community_size
            ORDER BY community_size DESC
            """
        else:
            cypher_query = f"""
            CALL algo.louvain.stream()
            YIELD nodeId, community
            MATCH (n) WHERE id(n) = nodeId
            WITH community, collect(n.name) as members
            WHERE size(members) >= {min_community_size}
            RETURN community, members, size(members) as community_size
            ORDER BY community_size DESC
            """
        
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in community detection: {str(e)}"

@tool
async def falkordb_centrality_analysis(centrality_type: str = "betweenness", top_k: Optional[int] = None) -> str:
    """Analyze node centrality in large graphs.
    
    Args:
        centrality_type: Type of centrality (betweenness, closeness, pagerank, degree)
        top_k: Number of top nodes to return (None for all)
        
    Returns:
        Centrality analysis results
    """
    try:
        limit_clause = f"LIMIT {top_k}" if top_k else ""
        
        if centrality_type.lower() == "pagerank":
            cypher_query = f"""
            CALL algo.pageRank.stream()
            YIELD nodeId, score
            MATCH (n) WHERE id(n) = nodeId
            RETURN n.name as node, score
            ORDER BY score DESC
            {limit_clause}
            """
        elif centrality_type.lower() == "betweenness":
            cypher_query = f"""
            CALL algo.betweenness.stream()
            YIELD nodeId, centrality
            MATCH (n) WHERE id(n) = nodeId
            RETURN n.name as node, centrality
            ORDER BY centrality DESC
            {limit_clause}
            """
        elif centrality_type.lower() == "degree":
            cypher_query = f"""
            MATCH (n)
            RETURN n.name as node, size((n)--()) as degree
            ORDER BY degree DESC
            {limit_clause}
            """
        else:  # closeness
            cypher_query = f"""
            CALL algo.closeness.stream()
            YIELD nodeId, centrality
            MATCH (n) WHERE id(n) = nodeId
            RETURN n.name as node, centrality
            ORDER BY centrality DESC
            {limit_clause}
            """
        
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in centrality analysis: {str(e)}"

@tool
async def falkordb_subgraph_extraction(entity_list: str, include_relationships: bool = True) -> str:
    """Extract subgraphs for specific entities in large graphs.
    
    Args:
        entity_list: Comma-separated list of entity names
        include_relationships: Whether to include relationships between entities
        
    Returns:
        Subgraph extraction results
    """
    try:
        entities = [e.strip() for e in entity_list.split(',')]
        entity_filter = "', '".join(entities)
        
        if include_relationships:
            cypher_query = f"""
            MATCH (n) WHERE n.name IN ['{entity_filter}']
            OPTIONAL MATCH (n)-[r]-(m) WHERE m.name IN ['{entity_filter}']
            RETURN 
                n.name as source_node,
                CASE WHEN r IS NOT NULL THEN type(r) ELSE 'NO_RELATIONSHIP' END as relationship,
                CASE WHEN m IS NOT NULL THEN m.name ELSE 'ISOLATED' END as target_node
            """
        else:
            cypher_query = f"""
            MATCH (n) WHERE n.name IN ['{entity_filter}']
            RETURN n.name as node, labels(n) as node_types, properties(n) as properties
            """
        
        return await falkordb_cypher_query(cypher_query)
    except Exception as e:
        return f"Error in subgraph extraction: {str(e)}"

# Main GraphRAG Search Engine
class GraphRAGSearchEngine:
    def __init__(self, openai_base_url: Optional[str] = None):
        # Initialize o3-mini with reasoning capabilities
        openai_kwargs = {
            "model": "o3-mini",
            "temperature": 0.1,
            "api_key": Config.OPENAI_API_KEY,
            "model_kwargs": {
                "reasoning_effort": "medium",  # low, medium, high
            }
        }
        
        # Add base_url if provided
        if openai_base_url or Config.OPENAI_BASE_URL:
            openai_kwargs["base_url"] = openai_base_url or Config.OPENAI_BASE_URL
        
        self.reasoning_llm = ChatOpenAI(**openai_kwargs)
        
        # Standard LLM for other tasks
        standard_kwargs = {
            "model": "gpt-4o-mini",
            "temperature": 0.1,
            "api_key": Config.OPENAI_API_KEY,
        }
        
        if openai_base_url or Config.OPENAI_BASE_URL:
            standard_kwargs["base_url"] = openai_base_url or Config.OPENAI_BASE_URL
            
        self.standard_llm = ChatOpenAI(**standard_kwargs)
        
        # Initialize components
        self.query_generator = MultiQueryGenerator(self.standard_llm)
        self.query_router = QueryRouter(self.standard_llm)
        
        # Initialize FalkorDB connection
        self.falkordb_graph = FalkorDBGraph(
            host=Config.FALKORDB_HOST,
            port=Config.FALKORDB_PORT,
            database=Config.GRAPH_NAME
        )
        
        # Initialize tools (expanded for large graph handling)
        self.tools = [
            falkordb_cypher_query,
            falkordb_vector_search, 
            falkordb_relationship_analysis,
            falkordb_graph_traversal,
            falkordb_community_detection,
            falkordb_centrality_analysis,
            falkordb_subgraph_extraction
        ]
        
        # Create React agent with reasoning model
        self.react_agent = create_react_agent(
            model=self.reasoning_llm,
            tools=self.tools,
            state_modifier="""You are a GraphRAG search expert specialized in large-scale graph analysis. 
            
            You have access to powerful tools for analyzing massive knowledge graphs without any query limits:
            - Execute unlimited Cypher queries for complex graph operations
            - Perform unrestricted vector similarity searches
            - Analyze deep relationship patterns across the entire graph
            - Conduct graph traversals with configurable depth
            - Detect communities and analyze centrality in large networks
            - Extract comprehensive subgraphs for detailed analysis
            
            For large graphs, always:
            1. Use the full dataset without artificial limits
            2. Leverage graph algorithms for efficient large-scale analysis
            3. Consider memory and performance implications
            4. Provide comprehensive insights from the complete graph structure
            
            Thoroughly analyze queries and provide comprehensive answers based on the complete graph data."""
        )
        
        # Initialize memory for stateful conversations
        self.memory = MemorySaver()
        
        # Build the main workflow graph
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the LangGraph workflow for GraphRAG processing."""
        
        # Define the workflow graph
        workflow = StateGraph(GraphRAGState)
        
        # Add nodes
        workflow.add_node("decompose_query", self._decompose_query_node)
        workflow.add_node("route_query", self._route_query_node)  
        workflow.add_node("vector_search", self._vector_search_node)
        workflow.add_node("graph_search", self._graph_search_node)
        workflow.add_node("hybrid_search", self._hybrid_search_node)
        workflow.add_node("reasoning_synthesis", self._reasoning_synthesis_node)
        
        # Define the flow
        workflow.add_edge(START, "decompose_query")
        workflow.add_edge("decompose_query", "route_query")
        
        # Conditional routing based on query type
        workflow.add_conditional_edges(
            "route_query",
            self._route_decision,
            {
                "vector": "vector_search",
                "graph": "graph_search", 
                "hybrid": "hybrid_search"
            }
        )
        
        # All paths lead to reasoning synthesis
        workflow.add_edge("vector_search", "reasoning_synthesis")
        workflow.add_edge("graph_search", "reasoning_synthesis")
        workflow.add_edge("hybrid_search", "reasoning_synthesis")
        workflow.add_edge("reasoning_synthesis", END)
        
        return workflow.compile(checkpointer=self.memory)
    
    async def _decompose_query_node(self, state: GraphRAGState) -> GraphRAGState:
        """Decompose the query into sub-queries."""
        query = state.get("original_query", "")
        if not query and state.get("messages"):
            query = state["messages"][-1].content
        
        decomposed = await self.query_generator.decompose_query(query)
        
        state["original_query"] = query
        state["decomposed_queries"] = decomposed.sub_queries
        
        logger.info(f"Decomposed query into: {decomposed.sub_queries}")
        return state
    
    async def _route_query_node(self, state: GraphRAGState) -> GraphRAGState:
        """Route the query to appropriate processing path."""
        query = state["original_query"]
        route_result = await self.query_router.route_query(query)
        
        state["query_route"] = route_result.route
        logger.info(f"Routed query to: {route_result.route}")
        return state
    
    def _route_decision(self, state: GraphRAGState) -> str:
        """Decision function for routing."""
        return state.get("query_route", "hybrid")
    
    async def _vector_search_node(self, state: GraphRAGState) -> GraphRAGState:
        """Execute vector-based search without limits for large graphs."""
        results = []
        for sub_query in state["decomposed_queries"]:
            # Use unlimited vector search for large graphs
            result = await falkordb_vector_search(sub_query, limit=None)
            results.append({"query": sub_query, "results": result})
        
        state["vector_results"] = results
        logger.info(f"Vector search completed for {len(results)} sub-queries (unlimited results)")
        return state
    
    async def _graph_search_node(self, state: GraphRAGState) -> GraphRAGState:
        """Execute graph-based search."""
        results = []
        for sub_query in state["decomposed_queries"]:
            # Use the React agent for complex graph reasoning
            response = await self.react_agent.ainvoke(
                {"messages": [HumanMessage(content=f"Analyze this query using graph relationships: {sub_query}")]},
                config={"configurable": {"thread_id": "graph_search"}}
            )
            
            results.append({
                "query": sub_query, 
                "results": response["messages"][-1].content
            })
        
        state["graph_results"] = results
        logger.info(f"Graph search completed for {len(results)} sub-queries")
        return state
    
    async def _hybrid_search_node(self, state: GraphRAGState) -> GraphRAGState:
        """Execute hybrid vector + graph search."""
        # Execute both vector and graph searches
        await self._vector_search_node(state)
        await self._graph_search_node(state)
        
        logger.info("Hybrid search completed")
        return state
    
    async def _reasoning_synthesis_node(self, state: GraphRAGState) -> GraphRAGState:
        """Synthesize final answer using o3-mini reasoning."""
        
        # Prepare context for reasoning
        context_parts = []
        
        if state.get("vector_results"):
            context_parts.append("Vector Search Results:")
            for result in state["vector_results"]:
                context_parts.append(f"Query: {result['query']}")
                context_parts.append(f"Results: {result['results']}")
        
        if state.get("graph_results"):
            context_parts.append("\nGraph Search Results:")
            for result in state["graph_results"]:
                context_parts.append(f"Query: {result['query']}")
                context_parts.append(f"Results: {result['results']}")
        
        context = "\n".join(context_parts)
        
        # Use o3-mini for final reasoning and synthesis
        reasoning_prompt = f"""
        You are an expert analyst synthesizing information from a GraphRAG search.
        
        Original Query: {state['original_query']}
        Sub-queries: {state['decomposed_queries']}
        Search Route: {state['query_route']}
        
        Available Context:
        {context}
        
        Instructions:
        1. Analyze all the retrieved information carefully
        2. Identify key patterns, relationships, and insights
        3. Synthesize a comprehensive answer to the original query
        4. Ensure accuracy and cite specific evidence from the search results
        5. If information is incomplete, clearly state limitations
        
        Provide a thorough, well-reasoned response:
        """
        
        response = await self.reasoning_llm.ainvoke([HumanMessage(content=reasoning_prompt)])
        
        state["final_answer"] = response.content
        state["reasoning_context"] = context
        
        logger.info("Reasoning synthesis completed")
        return state
    
    async def search(self, query: str, thread_id: Optional[str] = None) -> Dict[str, Any]:
        """Execute a search query through the GraphRAG pipeline."""
        if not thread_id:
            thread_id = f"search_{hash(query)}"
        
        # Initialize state
        initial_state = GraphRAGState(
            messages=[HumanMessage(content=query)],
            original_query=query,
            decomposed_queries=[],
            query_route="",
            vector_results=[],
            graph_results=[],
            reasoning_context="",
            final_answer=""
        )
        
        # Execute the workflow
        config = {"configurable": {"thread_id": thread_id}}
        final_state = await self.workflow.ainvoke(initial_state, config=config)
        
        return {
            "query": query,
            "decomposed_queries": final_state["decomposed_queries"],
            "route": final_state["query_route"],
            "answer": final_state["final_answer"],
            "context": final_state["reasoning_context"],
            "vector_results": final_state.get("vector_results", []),
            "graph_results": final_state.get("graph_results", [])
        }

# Usage example and main function
async def main():
    """Example usage of the GraphRAG Search Engine."""
    
    # Initialize the search engine with optional custom base URL
    # For OpenAI-compatible APIs (e.g., Azure OpenAI, local LLM servers)
    custom_base_url = os.getenv("OPENAI_BASE_URL")  # Optional
    search_engine = GraphRAGSearchEngine(openai_base_url=custom_base_url)
    
    # Example queries optimized for large graphs
    example_queries = [
        "What are the major communities in this knowledge graph and their key characteristics?",
        "Find the most influential entities based on centrality measures across the entire graph",
        "Analyze the complete relationship network between technology companies and their partnerships",
        "What are all the research themes and their interconnections in the academic collaboration network?",
        "Identify key bridge entities that connect different communities in the graph",
        "Perform a comprehensive analysis of the supply chain network including all suppliers and dependencies"
    ]
    
    print("GraphRAG Search Engine Initialized!")
    print("=" * 50)
    
    for query in example_queries:
        print(f"\nQuery: {query}")
        print("-" * 40)
        
        try:
            result = await search_engine.search(query)
            
            print(f"Route: {result['route']}")
            print(f"Sub-queries: {result['decomposed_queries']}")
            print(f"Answer: {result['answer'][:500]}...")  # Truncated for display
            print("-" * 40)
            
        except Exception as e:
            print(f"Error processing query: {e}")
            continue
    
    print("\nSearch engine demo completed!")

if __name__ == "__main__":
    # Set up environment variables first
    os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
    # os.environ["OPENAI_BASE_URL"] = "https://api.openai.com/v1"  # Optional: custom base URL
    os.environ["FALKORDB_HOST"] = "localhost"
    os.environ["FALKORDB_PORT"] = "6379"
    os.environ["GRAPH_NAME"] = "knowledge_graph"
    
    # Run the example
    asyncio.run(main())
