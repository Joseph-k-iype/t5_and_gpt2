#!/usr/bin/env python3
"""
Async Triple-Based RDF to FalkorDB Property Graph Converter

This module converts RDF data to FalkorDB property graphs using a single SPARQL query
with asyncio for handling large datasets without timeout issues.

Query Format: SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass

Features:
- Async processing with falkordb.asyncio and redis.asyncio
- Very long timeouts for large datasets
- Concurrent batch processing
- Connection pooling for optimal performance
- Semaphore-based rate limiting

Dependencies:
    pip install rdflib falkordb urllib3 requests asyncio
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

# Async imports
import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class AsyncTripleConfig:
    """Configuration for async triple-based RDF conversion"""
    # The main SPARQL query with 6 variables
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Async processing settings
    batch_size: int = 1000
    max_concurrent_batches: int = 5  # Control concurrency
    connection_pool_size: int = 20
    sparql_timeout: int = 3600  # 1 hour for large queries
    falkordb_timeout: Optional[int] = None  # Infinite timeout for FalkorDB operations
    max_retries: int = 3
    
    # Transformation settings
    use_shortened_uris: bool = True
    preserve_uri_properties: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    # Filtering
    exclude_rdf_type_properties: bool = True
    literal_properties_only: Set[str] = None
    
    # Monitoring
    export_stats: bool = True
    validate_conversion: bool = True

@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    # Batch processing stats
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    # Created entities
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        # Convert sets to lists for JSON serialization
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        
        return result

class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
            'http://purl.obolibrary.org/obo/': 'obo'
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        # Check cache first
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        # Try namespace mapping
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        # Fallback: extract from URI structure
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)

class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: AsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry"""
        labels = {self.config.default_node_label}
        
        # Add class-based label if available
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            labels.add(class_label)
        
        # Handle blank nodes
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'properties': {}
        }
        
        # Add URI properties if requested
        if self.config.preserve_uri_properties:
            node_data['properties']['uri'] = uri
        
        # Add processed identifier for easier querying
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            # Skip rdf:type if configured to do so
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            # Handle multiple values for the same property
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()

class AsyncFalkorDBManager:
    """Manages async FalkorDB connection and operations"""
    
    def __init__(self, config: AsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB"""
        try:
            # Create Redis connection pool with very long timeout
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'timeout': self.config.falkordb_timeout,  # None = infinite timeout
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
            }
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            # Test the connection
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            logger.info(f"Connected to FalkorDB async at {self.config.falkordb_host}:{self.config.falkordb_port}")
            logger.info(f"Connection pool size: {self.config.connection_pool_size}")
            logger.info(f"Timeout setting: {self.config.falkordb_timeout}")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB async: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def create_indexes(self):
        """Create performance indexes"""
        if not self.config.create_indexes:
            return
        
        index_queries = [
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.uri)",
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.id)",
        ]
        
        for query in index_queries:
            try:
                await self.graph.query(query)
                logger.debug(f"Created index: {query}")
            except Exception as e:
                logger.warning(f"Could not create index '{query}': {e}")
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with async retry logic"""
        for attempt in range(self.config.max_retries):
            try:
                return await self.graph.query(query, params or {})
            except Exception as e:
                # Check if it's a retryable error
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                # Log the retry attempt
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
            except redis_async.ConnectionError as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Connection error after {self.config.max_retries} attempts: {e}")
                    raise
                await asyncio.sleep(2 ** attempt)
                logger.warning(f"Connection error on attempt {attempt + 1}, retrying: {e}")
            except redis_async.TimeoutError as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Timeout error after {self.config.max_retries} attempts: {e}")
                    raise
                await asyncio.sleep(2 ** attempt)
                logger.warning(f"Timeout error on attempt {attempt + 1}, retrying: {e}")
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """Create a batch of nodes concurrently"""
        tasks = []
        for uri, node_data in nodes_batch:
            task = self._create_single_node(uri, node_data)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = 0
        for result in results:
            if not isinstance(result, Exception):
                success_count += 1
            else:
                logger.warning(f"Node creation failed: {result}")
        
        return success_count
    
    async def _create_single_node(self, uri: str, node_data: Dict[str, Any]) -> bool:
        """Create a single node in FalkorDB"""
        try:
            # Prepare labels
            labels = [self.config.default_node_label if not label else label 
                     for label in node_data['labels']]
            labels_str = ':'.join(sorted(set(labels)))
            
            # Prepare properties
            properties = dict(node_data['properties'])
            
            # Build and execute query
            if properties:
                props_clause = ', '.join([f"{k}: ${k}" for k in properties.keys()])
                query = f"CREATE (n:{labels_str} {{{props_clause}}})"
                await self.execute_query_with_retry(query, properties)
            else:
                query = f"CREATE (n:{labels_str})"
                await self.execute_query_with_retry(query)
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating node {uri}: {e}")
            return False
    
    async def create_relationship_batch(self, relationships_batch: List[Tuple[str, str, str]]) -> int:
        """Create a batch of relationships concurrently"""
        tasks = []
        for subject_uri, predicate_uri, object_uri in relationships_batch:
            task = self._create_single_relationship(subject_uri, predicate_uri, object_uri)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = 0
        for result in results:
            if not isinstance(result, Exception):
                success_count += 1
            else:
                logger.warning(f"Relationship creation failed: {result}")
        
        return success_count
    
    async def _create_single_relationship(self, subject_uri: str, predicate_uri: str, object_uri: str) -> bool:
        """Create a single relationship in FalkorDB"""
        try:
            # Process predicate for relationship type
            parsed = urlparse(predicate_uri)
            if parsed.fragment:
                rel_type = parsed.fragment
            else:
                rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
            
            # Clean relationship type
            rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
            if not rel_type:
                rel_type = 'RELATED_TO'
            
            # Create relationship with optional properties
            rel_props = {}
            if self.config.preserve_uri_properties:
                rel_props['predicate_uri'] = predicate_uri
            
            if rel_props:
                props_clause = '{' + ', '.join([f"{k}: ${k}" for k in rel_props.keys()]) + '}'
                query = f"""
                MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
                CREATE (s)-[:{rel_type} {props_clause}]->(o)
                """
            else:
                query = f"""
                MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
                CREATE (s)-[:{rel_type}]->(o)
                """
            
            params = {
                'subject_uri': subject_uri,
                'object_uri': object_uri,
                **rel_props
            }
            
            await self.execute_query_with_retry(query, params)
            return True
            
        except Exception as e:
            logger.error(f"Error creating relationship {subject_uri} -> {object_uri}: {e}")
            return False
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")

class AsyncTripleBasedConverter:
    """Main async converter using 6-variable triple query"""
    
    def __init__(self, config: AsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = AsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        # Processing state
        self.relationships_to_create: List[Tuple[str, str, str]] = []
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
        
        self._setup_rdf_connection()
    
    def _execute_sparql_query(self):
        """Execute SPARQL query synchronously (for use in executor)"""
        try:
            logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
            # Execute the query and convert to list immediately
            query_result = self.rdf_graph.query(self.config.triples_query)
            results = list(query_result)
            logger.info(f"SPARQL query completed, retrieved {len(results)} triples")
            return results
        except Exception as e:
            logger.error(f"SPARQL query execution failed: {e}")
            raise
    
    def _setup_rdf_connection(self):
        """Setup RDF graph connection to SPARQL endpoint"""
        try:
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"Connected to SPARQL endpoint: {self.config.sparql_endpoint}")
            logger.info(f"SPARQL timeout: {self.config.sparql_timeout}s")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method"""
        try:
            logger.info("Starting async triple-based RDF to FalkorDB conversion...")
            
            # Connect to FalkorDB
            await self.falkordb_manager.connect()
            
            # Clear existing data
            await self.falkordb_manager.clear_graph()
            
            # Execute the main query and process results
            await self._execute_and_process_query()
            
            # Create nodes in FalkorDB using async batching
            await self._create_nodes_in_falkordb_async()
            
            # Create relationships in FalkorDB using async batching
            await self._create_relationships_in_falkordb_async()
            
            # Create indexes
            await self.falkordb_manager.create_indexes()
            
            # Validate if requested
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            # Finalize statistics
            await self._finalize_stats()
            
            logger.info("Async conversion completed successfully!")
            return self.stats
            
        except Exception as e:
            logger.error(f"Async conversion failed: {e}")
            raise
        finally:
            # Always close connections
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("Executing triples query with extended timeout...")
            logger.info(f"Query timeout: {self.config.sparql_timeout}s")
            
            # Execute query in executor to avoid blocking event loop
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(
                None, 
                self._execute_sparql_query
            )
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"Retrieved {len(results)} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("Processing triples in async batches...")
            
            # Process in async batches
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with concurrency control"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"Processing {len(results)} triples in {total_batches} batches of {batch_size}")
        
        # Create batches
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        # Process batches concurrently with semaphore control
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        # Process results as they complete
        completed = 0
        failed = 0
        
        for future in asyncio.as_completed(tasks):
            try:
                await future
                completed += 1
                self.stats.completed_batches = completed
                
                if completed % 10 == 0:  # Log every 10 completed batches
                    progress = (completed / total_batches) * 100
                    logger.info(f"Async batch progress: {progress:.1f}% ({completed}/{total_batches})")
            except Exception as e:
                failed += 1
                self.stats.failed_batches = failed
                logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        # Track metadata
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        # Ensure subject node exists
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        # Determine if this creates a property or relationship
        if isinstance(obj, Literal):
            # Object is literal -> add as property
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            # Object is URI/BNode -> create relationship
            object_uri = str(obj)
            
            # Track object class
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            # Ensure object node exists
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            # Queue relationship for creation
            self.relationships_to_create.append((subject_uri, predicate_uri, object_uri))
            self.stats.relationship_triples += 1
        else:
            logger.warning(f"Unknown object type: {type(obj)} for {obj}")
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                elif literal.datatype in (XSD.dateTime, XSD.date, XSD.time):
                    return str(literal)
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB using async batching"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"Creating {len(nodes)} nodes in FalkorDB using async batching...")
        
        # Convert to list of tuples for batching
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        # Create node batches
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        # Process node batches concurrently
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_created = 0
        for result in results:
            if isinstance(result, int):
                total_created += result
            else:
                logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"Successfully created {total_created} nodes using async processing")
    
    async def _create_relationships_in_falkordb_async(self):
        """Create all relationships in FalkorDB using async batching"""
        logger.info(f"Creating {len(self.relationships_to_create)} relationships in FalkorDB using async batching...")
        
        batch_size = self.config.batch_size
        
        # Create relationship batches
        rel_batches = []
        for i in range(0, len(self.relationships_to_create), batch_size):
            batch = self.relationships_to_create[i:i+batch_size]
            rel_batches.append(batch)
        
        # Process relationship batches concurrently
        tasks = []
        for batch in rel_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_relationship_batch(batch)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_created = 0
        for result in results:
            if isinstance(result, int):
                total_created += result
            else:
                logger.error(f"Relationship batch creation failed: {result}")
        
        logger.info(f"Successfully created {total_created} relationships using async processing")
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("Validating async conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("No relationships were created despite processing relationship triples")
        
        logger.info(f"Validation complete: {falkor_stats['nodes']} nodes, {falkor_stats['relationships']} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        # Get final counts from FalkorDB
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.created_nodes = falkor_stats['nodes']
        self.stats.created_relationships = falkor_stats['relationships']
        
        # Calculate unique entities
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        # Count unique objects from relationships
        unique_objects = set()
        for _, _, obj_uri in self.relationships_to_create:
            unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            await self._export_stats()
    
    async def _export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"async_conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['config'] = {
                'batch_size': self.config.batch_size,
                'max_concurrent_batches': self.config.max_concurrent_batches,
                'connection_pool_size': self.config.connection_pool_size,
                'sparql_timeout': self.config.sparql_timeout,
                'falkordb_timeout': self.config.falkordb_timeout
            }
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"Async statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted graph"""
        return [
            "MATCH (n) RETURN labels(n) as node_labels, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN s.id, type(r), o.id LIMIT 10",
            "MATCH (n) WHERE size((n)--()) > 0 RETURN n.id, size((n)--()) as degree ORDER BY degree DESC LIMIT 10"
        ]

# Utility functions
async def test_async_setup(config: AsyncTripleConfig) -> bool:
    """Test the async setup without running full conversion"""
    try:
        logger.info("Testing async setup...")
        
        # Test SPARQL connection
        if not validate_sparql_endpoint(config.sparql_endpoint, config.username, config.password):
            logger.error("SPARQL endpoint test failed")
            return False
        logger.info("‚úÖ SPARQL endpoint accessible")
        
        # Test FalkorDB async connection
        falkordb_manager = AsyncFalkorDBManager(config)
        await falkordb_manager.connect()
        
        # Test basic operations
        await falkordb_manager.clear_graph()
        await falkordb_manager.execute_query_with_retry("CREATE (test:TestNode {name: 'async_test'})")
        stats = await falkordb_manager.get_graph_stats()
        
        if stats['nodes'] > 0:
            logger.info("‚úÖ FalkorDB async operations working")
        
        await falkordb_manager.execute_query_with_retry("MATCH (n:TestNode) DELETE n")
        await falkordb_manager.close()
        
        logger.info("‚úÖ Async setup test completed successfully")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Async setup test failed: {e}")
        return False

def validate_sparql_endpoint(endpoint: str, username: str = None, password: str = None) -> bool:
    """Validate SPARQL endpoint connectivity"""
    try:
        auth = (username, password) if username and password else None
        store = sparqlstore.SPARQLStore(endpoint, auth=auth)
        
        g = Graph(store=store)
        list(g.query("SELECT * WHERE { ?s ?p ?o } LIMIT 1"))
        return True
    except Exception as e:
        logger.error(f"SPARQL endpoint validation failed: {e}")
        return False

def create_sample_query():
    """Create a sample 6-variable SPARQL query"""
    return """
    # Sample 6-variable triple query optimized for large datasets
    PREFIX foaf: <http://xmlns.com/foaf/0.1/>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        
        # Get subject class
        ?subject a ?subjectClass .
        
        # Get predicate class (optional) 
        OPTIONAL {
            ?predicate a ?predicateClass .
        }
        
        # Get object class (optional, only for URI objects)
        OPTIONAL {
            ?object a ?objectClass .
            FILTER(isURI(?object))
        }
        
        # Example filters (customize as needed)
        FILTER(?subject != ?object)  # Avoid self-loops
        
        # Optional: Filter by specific types for performance
        FILTER(?subjectClass IN (foaf:Person, <http://schema.org/Organization>))
        
        # Include relevant predicates
        FILTER(?predicate IN (
            foaf:name, foaf:knows, foaf:mbox, rdfs:label
        ))
    }
    # Note: Remove LIMIT for full dataset conversion
    """

async def main():
    """Example async usage for large datasets with error checking"""
    
    # Sample query optimized for large datasets
    sample_query = create_sample_query()
    
    config = AsyncTripleConfig(
        triples_query=sample_query,
        sparql_endpoint="https://dbpedia.org/sparql",
        username=None,
        password=None,
        
        falkordb_host='localhost',
        falkordb_port=6379,
        graph_name='async_large_graph',
        
        # Conservative settings for testing
        batch_size=1000,  # Start smaller for testing
        max_concurrent_batches=5,  # Lower concurrency for testing
        connection_pool_size=20,  # Fewer connections for testing
        sparql_timeout=7200,  # 2 hours
        falkordb_timeout=None,  # Infinite timeout for FalkorDB operations
        
        use_shortened_uris=True,
        preserve_uri_properties=True,
        create_indexes=True,
        validate_conversion=True,
        export_stats=True
    )
    
    try:
        # Step 1: Test async setup first
        print("üîß Testing async setup...")
        setup_ok = await test_async_setup(config)
        if not setup_ok:
            print("‚ùå Async setup test failed. Please check your configuration.")
            return
        print("‚úÖ Async setup test passed!")
        
        # Step 2: Validate endpoint
        print("üîç Validating SPARQL endpoint...")
        if not validate_sparql_endpoint(config.sparql_endpoint, config.username, config.password):
            logger.error("Cannot connect to SPARQL endpoint")
            return
        print("‚úÖ SPARQL endpoint validated!")
        
        # Step 3: Run async conversion
        print("üöÄ Starting async conversion...")
        converter = AsyncTripleBasedConverter(config)
        stats = await converter.convert()
        
        # Print results
        print("\n" + "="*70)
        print("ASYNC TRIPLE-BASED CONVERSION COMPLETED")
        print("="*70)
        print(f"Total triples retrieved: {stats.total_triples_retrieved}")
        print(f"Property triples (literals): {stats.property_triples}")
        print(f"Relationship triples (URIs): {stats.relationship_triples}")
        print(f"Processed batches: {stats.completed_batches}/{stats.total_batches}")
        print(f"Failed batches: {stats.failed_batches}")
        print(f"Unique subjects: {stats.unique_subjects}")
        print(f"Unique objects: {stats.unique_objects}")
        print(f"Nodes created: {stats.created_nodes}")
        print(f"Relationships created: {stats.created_relationships}")
        
        print(f"\nDiscovered {len(stats.subject_classes)} subject classes")
        print(f"Discovered {len(stats.object_classes)} object classes") 
        print(f"Used {len(stats.predicates_used)} different predicates")
        
        if stats.end_time:
            duration = (stats.end_time - stats.start_time).total_seconds()
            print(f"\nPerformance:")
            print(f"  Query execution: {stats.query_execution_time:.2f}s")
            print(f"  Total duration: {duration:.2f}s")
            print(f"  Processing rate: {stats.processed_triples/duration:.1f} triples/second")
        
        print("\nSample Cypher queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            print(f"{i}. {query}")
        
    except Exception as e:
        logger.error(f"Async conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    # Run the async main function
    asyncio.run(main())
