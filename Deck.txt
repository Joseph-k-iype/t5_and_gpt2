#!/usr/bin/env python3
"""
GDPR Metamodel Knowledge System - Enhanced Version
=================================================

A comprehensive multi-agent system for creating GDPR metamodels with:
- Semantic knowledge graph using PROV-O and SKOS ontologies
- Vector embeddings in both Elasticsearch and FalkorDB
- Comprehensive article-to-concept mapping
- ROPA-focused metamodel generation
- Support for GDPR and UK GDPR across all jurisdictions

Author: AI Assistant
Enhanced: June 2025
Focus: Financial Industry Compliance (HSBC-ready)
"""

import os
import json
import logging
import asyncio
import uuid
import operator
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, Literal, Sequence
from dataclasses import dataclass, field, asdict
from pathlib import Path

# Core libraries
import pandas as pd
import numpy as np
from tqdm import tqdm
import httpx

# Document processing
import pymupdf

# LangChain and LangGraph
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph for multi-agent architecture
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from typing_extensions import TypedDict, Annotated

# Database clients
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from falkordb import FalkorDB

# Direct OpenAI API
from openai import AsyncOpenAI

# Async support
import aiofiles

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Enhanced configuration for GDPR metamodel system"""
    
    # OpenAI Configuration
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL: str = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    REASONING_MODEL: str = "o3-mini"
    REASONING_EFFORT: str = "high"
    EMBEDDING_MODEL: str = "text-embedding-3-large"
    EMBEDDING_DIMENSIONS: int = 3072
    
    # Memory and Persistence
    MEMORY_THREAD_ID: str = os.getenv("MEMORY_THREAD_ID", "gdpr_metamodel_system")
    ENABLE_LONG_TERM_MEMORY: bool = os.getenv("ENABLE_LONG_TERM_MEMORY", "true").lower() == "true"
    KNOWLEDGE_STORE_NAMESPACE: str = "gdpr_metamodel"
    LEARNING_STORE_NAMESPACE: str = "gdpr_learning"
    
    # Elasticsearch Configuration
    ELASTICSEARCH_HOST: str = os.getenv("ES_HOST", "localhost")
    ELASTICSEARCH_PORT: int = int(os.getenv("ES_PORT", "9200"))
    ELASTICSEARCH_USERNAME: str = os.getenv("ES_USERNAME", "elastic")
    ELASTICSEARCH_PASSWORD: str = os.getenv("ES_PASSWORD")
    ELASTICSEARCH_INDEX: str = "gdpr_metamodel_kb"
    ELASTICSEARCH_CA_CERTS: str = os.getenv("ES_CA_CERTS", "")
    ELASTICSEARCH_VERIFY_CERTS: bool = os.getenv("ES_VERIFY_CERTS", "false").lower() == "true"
    
    # FalkorDB Configuration
    FALKORDB_HOST: str = os.getenv("FALKOR_HOST", "localhost")
    FALKORDB_PORT: int = int(os.getenv("FALKOR_PORT", "6379"))
    FALKORDB_PASSWORD: str = os.getenv("FALKOR_PASSWORD", "")
    FALKORDB_GRAPH_NAME: str = "gdpr_metamodel_graph"
    
    # Document Processing
    CHUNK_SIZE: int = int(os.getenv("CHUNK_SIZE", "1500"))
    CHUNK_OVERLAP: int = int(os.getenv("CHUNK_OVERLAP", "300"))
    
    # Streaming and Batch Configuration
    DOCUMENT_BATCH_SIZE: int = int(os.getenv("DOCUMENT_BATCH_SIZE", "50"))
    EMBEDDING_BATCH_SIZE: int = int(os.getenv("EMBEDDING_BATCH_SIZE", "20"))
    ELASTICSEARCH_BATCH_SIZE: int = int(os.getenv("ELASTICSEARCH_BATCH_SIZE", "100"))
    FALKORDB_BATCH_SIZE: int = int(os.getenv("FALKORDB_BATCH_SIZE", "50"))
    CONCEPT_BATCH_SIZE: int = int(os.getenv("CONCEPT_BATCH_SIZE", "25"))
    
    # File Paths
    DOCUMENTS_PATH: Path = Path(os.getenv("DOCS_PATH", "./documents"))
    OUTPUT_PATH: Path = Path(os.getenv("OUTPUT_PATH", "./output"))
    MEMORY_PATH: Path = Path(os.getenv("MEMORY_PATH", "./memory"))
    
    @classmethod
    def validate(cls):
        """Validate configuration"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        valid_efforts = ["low", "medium", "high"]
        if cls.REASONING_EFFORT not in valid_efforts:
            raise ValueError(f"REASONING_EFFORT must be one of {valid_efforts}")
        
        logging.info(f"âœ… Configuration validated for GDPR Metamodel System")
        logging.info(f"ðŸ“Š Batch sizes - Documents: {cls.DOCUMENT_BATCH_SIZE}, Embeddings: {cls.EMBEDDING_BATCH_SIZE}")
        logging.info(f"ðŸ—„ï¸ DB batch sizes - ES: {cls.ELASTICSEARCH_BATCH_SIZE}, Falkor: {cls.FALKORDB_BATCH_SIZE}")
        print(f"âš™ï¸ Configuration: Doc batches={cls.DOCUMENT_BATCH_SIZE}, Embedding batches={cls.EMBEDDING_BATCH_SIZE}")
        print(f"âš™ï¸ DB batches: ES={cls.ELASTICSEARCH_BATCH_SIZE}, Falkor={cls.FALKORDB_BATCH_SIZE}")

# ============================================================================
# MEMORY-ENHANCED PDF PROCESSOR
# ============================================================================

class MemoryEnhancedPDFProcessor:
    """PDF processor with memory of previous analyses"""
    
    def __init__(self, config: Config):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_with_memory(self, file_path: Path, memory_store, session_id: str) -> Dict[str, Any]:
        """Process PDF with memory of previous analyses"""
        try:
            # Open document with pymupdf
            doc = pymupdf.open(str(file_path))
            
            doc_info = {
                "file_path": str(file_path),
                "metadata": doc.metadata,
                "page_count": doc.page_count,
                "pages": [],
                "full_text": "",
                "document_type": self._classify_document(file_path.name),
                "session_id": session_id,
                "processing_insights": []
            }
            
            # Process each page
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text = page.get_text()
                
                try:
                    tables = page.find_tables()
                    table_data = [table.extract() for table in tables]
                except:
                    table_data = []
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": text,
                    "tables": table_data,
                    "word_count": len(text.split())
                }
                
                doc_info["pages"].append(page_info)
                doc_info["full_text"] += text + "\n"
            
            # Create enhanced chunks
            chunks = await self._create_memory_enhanced_chunks(doc_info)
            doc_info["chunks"] = chunks
            
            doc.close()
            return doc_info
            
        except Exception as e:
            logging.error(f"Error processing PDF {file_path}: {e}")
            return {
                "error": str(e), 
                "file_path": str(file_path),
                "page_count": 0,
                "chunks": [],
                "processing_insights": []
            }
    
    async def _create_memory_enhanced_chunks(self, doc_info: Dict[str, Any]) -> List[Document]:
        """Create chunks with memory enhancement"""
        chunks = []
        full_text = doc_info["full_text"]
        text_chunks = self.text_splitter.split_text(full_text)
        
        for i, chunk_text in enumerate(text_chunks):
            metadata = {
                "source": doc_info["file_path"],
                "chunk_id": f"chunk_{i}",
                "chunk_index": i,
                "document_type": doc_info["document_type"],
                "total_chunks": len(text_chunks),
                "session_id": doc_info["session_id"],
                "extracted_at": datetime.now().isoformat()
            }
            
            chunk_doc = Document(
                page_content=chunk_text,
                metadata=metadata
            )
            chunks.append(chunk_doc)
        
        return chunks
    
    def _classify_document(self, filename: str) -> str:
        """Classify document type"""
        filename_lower = filename.lower()
        
        if "gdpr" in filename_lower and "uk" not in filename_lower:
            return "GDPR_REGULATION"
        elif "uk" in filename_lower and "gdpr" in filename_lower:
            return "UK_GDPR_REGULATION"
        elif any(term in filename_lower for term in ["business", "company", "internal"]):
            return "BUSINESS_DOCUMENT"
        else:
            return "UNKNOWN"

# ============================================================================
# MEMORY-ENHANCED ELASTICSEARCH MANAGER
# ============================================================================

class MemoryEnhancedElasticsearchManager:
    """Elasticsearch manager with memory integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self._setup_client()
    
    def _setup_client(self):
        """Setup Elasticsearch client"""
        try:
            connection_params = {
                "request_timeout": 30,
                "max_retries": 3,
                "retry_on_timeout": True,
                "verify_certs": self.config.ELASTICSEARCH_VERIFY_CERTS
            }
            
            if self.config.ELASTICSEARCH_CA_CERTS:
                connection_params["ca_certs"] = self.config.ELASTICSEARCH_CA_CERTS
                protocol = "https"
            else:
                protocol = "https" if self.config.ELASTICSEARCH_VERIFY_CERTS else "http"
            
            host_url = f"{protocol}://{self.config.ELASTICSEARCH_HOST}:{self.config.ELASTICSEARCH_PORT}"
            connection_params["hosts"] = [host_url]
            
            if self.config.ELASTICSEARCH_PASSWORD:
                connection_params["basic_auth"] = (
                    self.config.ELASTICSEARCH_USERNAME, 
                    self.config.ELASTICSEARCH_PASSWORD
                )
            
            self.client = Elasticsearch(**connection_params)
            
            if self.client.ping():
                logging.info(f"âœ… Connected to Elasticsearch at {host_url}")
            else:
                raise ConnectionError("Cannot connect to Elasticsearch")
                
        except Exception as e:
            logging.error(f"âŒ Elasticsearch connection error: {e}")
            raise
    
    async def create_enhanced_index(self):
        """Create index with enhanced mappings"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "content": {"type": "text"},
                    "embeddings": {
                        "type": "dense_vector",
                        "dims": self.config.EMBEDDING_DIMENSIONS,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "session_id": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    "source_file": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "extracted_at": {"type": "date"}
                }
            }
        }
        
        try:
            if self.client.indices.exists(index=self.config.ELASTICSEARCH_INDEX):
                self.client.indices.delete(index=self.config.ELASTICSEARCH_INDEX)
            
            self.client.indices.create(
                index=self.config.ELASTICSEARCH_INDEX,
                **mapping
            )
            logging.info(f"âœ… Created Elasticsearch index: {self.config.ELASTICSEARCH_INDEX}")
            
        except Exception as e:
            logging.error(f"âŒ Error creating index: {e}")
            raise
    
    async def store_with_memory(self, documents: List[Dict[str, Any]], 
                              session_id: str, memory_insights: Dict[str, Any]):
        """Store documents with memory integration"""
        try:
            actions = []
            for doc in documents:
                enhanced_doc = {
                    **doc,
                    "session_id": session_id,
                    "updated_at": datetime.now().isoformat()
                }
                
                action = {
                    "_index": self.config.ELASTICSEARCH_INDEX,
                    "_id": enhanced_doc.get("document_id", str(uuid.uuid4())),
                    "_source": enhanced_doc
                }
                actions.append(action)
            
            success, failed = bulk(self.client, actions)
            logging.info(f"âœ… Stored {success} documents in Elasticsearch")
            
            return {"success": success, "failed": len(failed)}
            
        except Exception as e:
            logging.error(f"âŒ Error storing documents: {e}")
            raise

# ============================================================================
# ENHANCED DATA MODELS
# ============================================================================

class MetamodelState(TypedDict):
    """Enhanced state for metamodel generation"""
    # Document processing
    documents: Annotated[List[Document], operator.add]
    extracted_articles: Dict[str, Any]
    concepts: List[Dict[str, Any]]
    semantic_relationships: Dict[str, List[Dict[str, Any]]]
    
    # Knowledge graph
    knowledge_graph_stats: Dict[str, Any]
    vector_index_stats: Dict[str, Any]
    ontology_mappings: Dict[str, Any]
    
    # Metamodel generation
    metamodel: Dict[str, Any]
    jurisdiction_mappings: Dict[str, Any]
    ropa_framework: Dict[str, Any]
    
    # System state
    current_agent: str
    iteration_count: int
    errors: Annotated[List[str], operator.add]
    session_id: str
    metadata: Dict[str, Any]

@dataclass
class SemanticConcept:
    """Enhanced concept with semantic properties"""
    concept_id: str
    label: str
    definition: str
    category: str
    skos_type: str  # skos:Concept, skos:ConceptScheme, etc.
    prov_type: str  # prov:Entity, prov:Activity, etc.
    
    # GDPR specific
    article_references: List[str]
    regulation_type: str
    jurisdiction: str
    legal_basis: List[str]
    
    # Semantic relationships
    broader_concepts: List[str]  # skos:broader
    narrower_concepts: List[str]  # skos:narrower
    related_concepts: List[str]  # skos:related
    
    # Industry context
    industry_definitions: Dict[str, str]
    compliance_requirements: List[str]
    ropa_relevance: str
    
    # Provenance
    source_document: str
    extracted_from: str
    confidence_score: float
    last_updated: datetime
    
    # Vector embedding
    embedding: Optional[List[float]] = None

@dataclass
class GDPRArticle:
    """Enhanced GDPR article with semantic annotations"""
    article_id: str
    number: Union[int, str]
    title: str
    content: str
    regulation_type: str
    
    # Semantic properties
    concepts: List[str]
    obligations: List[str]
    rights: List[str]
    legal_bases: List[str]
    
    # Jurisdictional context
    jurisdiction: str
    territorial_scope: List[str]
    cross_border_implications: List[str]
    
    # Industry context
    financial_sector_relevance: str
    ropa_implications: List[str]
    
    # Relationships
    related_articles: List[str]
    implementing_guidelines: List[str]
    case_law_references: List[str]
    
    # Provenance
    source_document: str
    confidence_score: float
    last_updated: datetime
    
    # Vector embedding
    embedding: Optional[List[float]] = None

@dataclass
class SemanticRelationship:
    """Semantic relationship between entities"""
    relationship_id: str
    source_id: str
    target_id: str
    relationship_type: str  # PROV-O or SKOS relationship
    properties: Dict[str, Any]
    confidence_score: float
    source_reference: str

# ============================================================================
# ENHANCED FALKORDB MANAGER
# ============================================================================

class EnhancedFalkorDBManager:
    """Enhanced FalkorDB manager with semantic ontology and vector embeddings"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self.graph = None
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
        self._setup_client()
    
    def _setup_client(self):
        """Setup FalkorDB client and graph"""
        try:
            self.client = FalkorDB(
                host=self.config.FALKORDB_HOST,
                port=self.config.FALKORDB_PORT,
                password=self.config.FALKORDB_PASSWORD if self.config.FALKORDB_PASSWORD else None
            )
            
            self.graph = self.client.select_graph(self.config.FALKORDB_GRAPH_NAME)
            logging.info(f"âœ… Connected to FalkorDB: {self.config.FALKORDB_GRAPH_NAME}")
            
        except Exception as e:
            logging.error(f"âŒ FalkorDB connection error: {e}")
            raise
    
    async def create_semantic_schema(self):
        """Create semantic ontology schema with dynamic discovery"""
        try:
            # First, analyze existing data to determine optimal schema
            schema_analysis = await self._analyze_schema_requirements()
            
            print(f"ðŸ—ï¸ Creating schema with {len(schema_analysis['node_types'])} node types")
            
            # Create dynamic vector indexes based on analysis
            for node_type in schema_analysis['node_types']:
                try:
                    # Sanitize node type name for Cypher - remove special characters and ensure valid identifier
                    sanitized_node_type = self._sanitize_node_type(node_type)
                    
                    vector_index_query = f"CREATE VECTOR INDEX FOR (n:{sanitized_node_type}) ON (n.embedding)"
                    print(f"ðŸ”® Creating vector index for: {sanitized_node_type}")
                    self.graph.query(vector_index_query)
                    print(f"âœ… Created vector index for {sanitized_node_type}")
                except Exception as e:
                    print(f"âš ï¸ Vector index for {sanitized_node_type} may already exist or failed: {e}")
                    logging.warning(f"Vector index for {sanitized_node_type} warning: {e}")
            
            # Create property indexes based on discovered properties
            for index_spec in schema_analysis['property_indexes']:
                try:
                    sanitized_node_type = self._sanitize_node_type(index_spec['node_type'])
                    sanitized_property = self._sanitize_property_name(index_spec['property'])
                    
                    property_index_query = f"CREATE INDEX FOR (n:{sanitized_node_type}) ON (n.{sanitized_property})"
                    print(f"ðŸ” Creating property index for: {sanitized_node_type}.{sanitized_property}")
                    self.graph.query(property_index_query)
                    print(f"âœ… Created property index for {sanitized_node_type}.{sanitized_property}")
                except Exception as e:
                    print(f"âš ï¸ Property index may already exist or failed: {e}")
                    logging.warning(f"Property index warning: {e}")
            
            # Create full-text indexes for text properties
            for fulltext_spec in schema_analysis['fulltext_indexes']:
                try:
                    sanitized_node_type = self._sanitize_node_type(fulltext_spec['node_type'])
                    sanitized_property = self._sanitize_property_name(fulltext_spec['property'])
                    
                    fulltext_query = f"CREATE FULLTEXT INDEX FOR (n:{sanitized_node_type}) ON EACH [n.{sanitized_property}]"
                    print(f"ðŸ“ Creating fulltext index for: {sanitized_node_type}.{sanitized_property}")
                    self.graph.query(fulltext_query)
                    print(f"âœ… Created fulltext index for {sanitized_node_type}.{sanitized_property}")
                except Exception as e:
                    print(f"âš ï¸ Fulltext index may already exist or failed: {e}")
                    logging.warning(f"Fulltext index warning: {e}")
            
            logging.info(f"âœ… Created dynamic semantic schema with {len(schema_analysis['node_types'])} node types")
            
        except Exception as e:
            logging.error(f"âŒ Error creating semantic schema: {e}")
            import traceback
            print(f"âŒ Schema creation error: {e}")
            print(f"ðŸ“‹ Traceback: {traceback.format_exc()}")
            raise
    
    def _sanitize_node_type(self, node_type: str) -> str:
        """Sanitize node type name for Cypher compatibility"""
        import re
        # Remove special characters and ensure it starts with a letter
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '', node_type)
        if sanitized and not sanitized[0].isalpha():
            sanitized = 'N' + sanitized
        return sanitized or 'GenericNode'
    
    def _sanitize_property_name(self, property_name: str) -> str:
        """Sanitize property name for Cypher compatibility"""
        import re
        # Replace spaces and special characters with underscores
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', property_name)
        # Ensure it starts with a letter
        if sanitized and not sanitized[0].isalpha():
            sanitized = 'prop_' + sanitized
        return sanitized or 'generic_property'
    
    async def _analyze_schema_requirements(self) -> Dict[str, Any]:
        """Use LLM to analyze and determine optimal schema structure"""
        analysis_prompt = ChatPromptTemplate.from_template("""
        You are a knowledge graph architect specializing in GDPR and legal compliance.
        
        Analyze the requirements for a comprehensive GDPR knowledge graph and determine:
        1. Essential node types for GDPR/UK GDPR compliance
        2. Key properties that need indexing for performance
        3. Text properties that need full-text search
        4. Relationship types for semantic connections
        
        Consider:
        - GDPR articles and concepts
        - Legal bases and obligations
        - Data subject rights
        - Cross-jurisdictional requirements
        - Financial services compliance
        - ROPA (Record of Processing Activities) framework
        
        Return JSON with the optimal schema structure:
        {{
            "node_types": ["Concept", "Article", "Regulation", "..."],
            "property_indexes": [
                {{"node_type": "Concept", "property": "concept_id"}},
                {{"node_type": "Article", "property": "article_id"}},
                ...
            ],
            "fulltext_indexes": [
                {{"node_type": "Concept", "property": "definition"}},
                {{"node_type": "Article", "property": "content"}},
                ...
            ],
            "relationship_types": [
                "skos_broader", "skos_narrower", "skos_related",
                "REFERENCED_IN", "DEFINES", "APPLIES_TO", "..."
            ],
            "essential_properties": {{
                "Concept": ["concept_id", "label", "definition", "..."],
                "Article": ["article_id", "number", "title", "..."],
                ...
            }}
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages()
            response = await self.llm.ainvoke(messages)
            schema_analysis = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-driven schema analysis completed")
            return schema_analysis
            
        except Exception as e:
            logging.error(f"âŒ Error in schema analysis: {e}")
            # Fallback to minimal schema
            return {
                "node_types": ["Concept", "Article", "Regulation"],
                "property_indexes": [
                    {"node_type": "Concept", "property": "concept_id"},
                    {"node_type": "Article", "property": "article_id"}
                ],
                "fulltext_indexes": [
                    {"node_type": "Concept", "property": "definition"},
                    {"node_type": "Article", "property": "content"}
                ],
                "relationship_types": ["RELATED_TO", "REFERENCED_IN", "DEFINES"],
                "essential_properties": {
                    "Concept": ["concept_id", "label", "definition"],
                    "Article": ["article_id", "number", "title", "content"]
                }
            }
    
    async def store_semantic_concepts(self, concepts: List[SemanticConcept], batch_size: int = 50):
        """Store concepts with dynamic semantic relationships using LLM analysis and streaming"""
        try:
            print(f"ðŸ’¾ Starting to store {len(concepts)} concepts in batches of {batch_size}")
            
            # First, analyze the concepts to determine optimal storage structure
            storage_analysis = await self._analyze_concept_storage(concepts[:5])  # Analyze sample
            
            stored_count = 0
            
            # Process concepts in batches for better memory management
            for batch_start in range(0, len(concepts), batch_size):
                batch_end = min(batch_start + batch_size, len(concepts))
                batch = concepts[batch_start:batch_end]
                
                print(f"ðŸ“¦ Processing concept batch {batch_start//batch_size + 1}: concepts {batch_start+1}-{batch_end}")
                
                for concept in batch:
                    try:
                        # Create concept node with dynamically determined properties
                        concept_properties = storage_analysis['concept_properties']
                        property_assignments = []
                        property_params = {'concept_id': concept.concept_id}
                        
                        for prop in concept_properties:
                            if hasattr(concept, prop) and getattr(concept, prop) is not None:
                                property_assignments.append(f"c.{prop} = ${prop}")
                                value = getattr(concept, prop)
                                
                                # Handle different data types properly
                                if hasattr(value, 'isoformat'):  # datetime object
                                    value = value.isoformat()
                                elif isinstance(value, datetime):
                                    value = value.isoformat()
                                elif isinstance(value, list):
                                    value = json.dumps(value) if value else "[]"
                                elif isinstance(value, dict):
                                    value = json.dumps(value) if value else "{}"
                                elif isinstance(value, str):
                                    value = value  # Already a string
                                else:
                                    value = str(value)  # Convert to string
                                    
                                property_params[prop] = value
                        
                        if property_assignments:  # Only create if we have properties to set
                            concept_query = f"""
                            MERGE (c:Concept {{concept_id: $concept_id}})
                            SET {', '.join(property_assignments)}
                            RETURN c
                            """
                            
                            self.graph.query(concept_query, property_params)
                            
                            # Create dynamic relationships based on LLM analysis
                            await self._create_dynamic_concept_relationships(concept, storage_analysis)
                            
                            stored_count += 1
                        else:
                            print(f"âš ï¸ No properties to store for concept: {concept.concept_id}")
                            
                    except Exception as e:
                        print(f"âŒ Error storing concept {concept.concept_id}: {e}")
                        logging.error(f"âŒ Error storing concept {concept.concept_id}: {e}")
                        continue
                
                print(f"âœ… Batch {batch_start//batch_size + 1} completed: {len(batch)} concepts processed")
                
                # Add small delay to prevent overwhelming the database
                await asyncio.sleep(0.1)
            
            logging.info(f"âœ… Stored {stored_count} semantic concepts with dynamic structure")
            return stored_count
            
        except Exception as e:
            logging.error(f"âŒ Error storing semantic concepts: {e}")
            import traceback
            print(f"âŒ Concept storage error: {e}")
            print(f"ðŸ“‹ Traceback: {traceback.format_exc()}")
            raise
    
    async def _analyze_concept_storage(self, concepts: List[SemanticConcept]) -> Dict[str, Any]:
        """Use LLM to analyze concepts and determine optimal storage structure"""
        sample_concepts = concepts[:3]  # Analyze first few concepts
        concept_sample = json.dumps([{
            'concept_id': c.concept_id,
            'label': c.label,
            'definition': c.definition[:200],
            'category': c.category,
            'regulation_type': c.regulation_type,
            'article_references': c.article_references[:3],
            'broader_concepts': c.broader_concepts[:2],
            'related_concepts': c.related_concepts[:2]
        } for c in sample_concepts], indent=2)
        
        analysis_prompt = ChatPromptTemplate.from_template("""
        You are a knowledge graph architect analyzing GDPR concept data for optimal storage.
        
        Sample concepts to analyze:
        {concept_sample}
        
        Determine:
        1. Which concept properties should be stored as graph node properties
        2. What relationship types should be created between concepts
        3. How to handle list properties (as separate nodes or serialized)
        4. What indexes would optimize queries
        
        Return JSON with storage strategy:
        {{
            "concept_properties": ["concept_id", "label", "definition", "..."],
            "relationship_mappings": {{
                "article_references": {{"target_type": "Article", "relationship": "REFERENCED_IN"}},
                "broader_concepts": {{"target_type": "Concept", "relationship": "skos_broader"}},
                "related_concepts": {{"target_type": "Concept", "relationship": "skos_related"}},
                "...": {{"target_type": "...", "relationship": "..."}}
            }},
            "list_properties_as_nodes": ["legal_basis", "compliance_requirements"],
            "serialized_properties": ["industry_definitions", "context_variations"]
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages(concept_sample=concept_sample)
            response = await self.llm.ainvoke(messages)
            storage_analysis = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-driven concept storage analysis completed")
            return storage_analysis
            
        except Exception as e:
            logging.error(f"âŒ Error in concept storage analysis: {e}")
            # Fallback structure
            return {
                "concept_properties": ["concept_id", "label", "definition", "category", "regulation_type", "jurisdiction"],
                "relationship_mappings": {
                    "article_references": {"target_type": "Article", "relationship": "REFERENCED_IN"},
                    "broader_concepts": {"target_type": "Concept", "relationship": "skos_broader"},
                    "related_concepts": {"target_type": "Concept", "relationship": "skos_related"}
                },
                "list_properties_as_nodes": ["legal_basis"],
                "serialized_properties": ["industry_definitions"]
            }
    
    async def _create_dynamic_concept_relationships(self, concept: SemanticConcept, analysis: Dict[str, Any]):
        """Create relationships based on LLM analysis"""
        try:
            relationship_mappings = analysis.get('relationship_mappings', {})
            
            # Create relationships based on analysis
            for property_name, mapping in relationship_mappings.items():
                if hasattr(concept, property_name):
                    property_values = getattr(concept, property_name)
                    if property_values and isinstance(property_values, list):
                        target_type = mapping['target_type']
                        relationship = mapping['relationship']
                        
                        for value in property_values:
                            if value:  # Skip empty values
                                relationship_query = f"""
                                MATCH (source:Concept {{concept_id: $concept_id}})
                                MERGE (target:{target_type} {{id: $target_value}})
                                MERGE (source)-[:{relationship}]->(target)
                                """
                                
                                self.graph.query(relationship_query, {
                                    'concept_id': concept.concept_id,
                                    'target_value': str(value)
                                })
            
            # Handle list properties as separate nodes
            list_as_nodes = analysis.get('list_properties_as_nodes', [])
            for prop_name in list_as_nodes:
                if hasattr(concept, prop_name):
                    prop_values = getattr(concept, prop_name)
                    if prop_values and isinstance(prop_values, list):
                        for value in prop_values:
                            if value:
                                node_type = prop_name.replace('_', '').title().rstrip('s')  # legal_basis -> LegalBasis
                                relationship_query = f"""
                                MATCH (c:Concept {{concept_id: $concept_id}})
                                MERGE (n:{node_type} {{value: $value}})
                                MERGE (c)-[:HAS_{prop_name.upper()}]->(n)
                                """
                                
                                self.graph.query(relationship_query, {
                                    'concept_id': concept.concept_id,
                                    'value': str(value)
                                })
            
        except Exception as e:
            logging.error(f"âŒ Error creating dynamic relationships: {e}")
    
    async def store_gdpr_articles(self, articles: List[GDPRArticle], batch_size: int = 25):
        """Store GDPR articles with dynamic structure based on LLM analysis and streaming"""
        try:
            print(f"ðŸ“š Starting to store {len(articles)} articles in batches of {batch_size}")
            
            # Analyze articles structure
            article_analysis = await self._analyze_article_storage(articles[:3])  # Analyze sample
            
            stored_count = 0
            
            # Process articles in batches for better memory management
            for batch_start in range(0, len(articles), batch_size):
                batch_end = min(batch_start + batch_size, len(articles))
                batch = articles[batch_start:batch_end]
                
                print(f"ðŸ“¦ Processing article batch {batch_start//batch_size + 1}: articles {batch_start+1}-{batch_end}")
                
                for article in batch:
                    try:
                        # Create article node with dynamically determined properties
                        article_properties = article_analysis['article_properties']
                        property_assignments = []
                        property_params = {'article_id': article.article_id}
                        
                        for prop in article_properties:
                            if hasattr(article, prop) and getattr(article, prop) is not None:
                                property_assignments.append(f"a.{prop} = ${prop}")
                                value = getattr(article, prop)
                                
                                # Handle different data types properly
                                if hasattr(value, 'isoformat'):  # datetime object
                                    value = value.isoformat()
                                elif isinstance(value, datetime):
                                    value = value.isoformat()
                                elif isinstance(value, list):
                                    value = json.dumps(value) if value else "[]"
                                elif isinstance(value, dict):
                                    value = json.dumps(value) if value else "{}"
                                elif isinstance(value, str):
                                    value = value  # Already a string
                                else:
                                    value = str(value)  # Convert to string
                                    
                                property_params[prop] = value
                        
                        if property_assignments:  # Only create if we have properties to set
                            article_query = f"""
                            MERGE (a:Article {{article_id: $article_id}})
                            SET {', '.join(property_assignments)}
                            RETURN a
                            """
                            
                            self.graph.query(article_query, property_params)
                            
                            # Create dynamic relationships
                            await self._create_dynamic_article_relationships(article, article_analysis)
                            
                            stored_count += 1
                        else:
                            print(f"âš ï¸ No properties to store for article: {article.article_id}")
                            
                    except Exception as e:
                        print(f"âŒ Error storing article {article.article_id}: {e}")
                        logging.error(f"âŒ Error storing article {article.article_id}: {e}")
                        continue
                
                print(f"âœ… Batch {batch_start//batch_size + 1} completed: {len(batch)} articles processed")
                
                # Add small delay to prevent overwhelming the database
                await asyncio.sleep(0.1)
            
            logging.info(f"âœ… Stored {stored_count} GDPR articles with dynamic structure")
            return stored_count
            
        except Exception as e:
            logging.error(f"âŒ Error storing GDPR articles: {e}")
            import traceback
            print(f"âŒ Article storage error: {e}")
            print(f"ðŸ“‹ Traceback: {traceback.format_exc()}")
            raise
    
    async def _analyze_article_storage(self, articles: List[GDPRArticle]) -> Dict[str, Any]:
        """Use LLM to analyze articles and determine storage structure"""
        sample_articles = articles[:2]
        article_sample = json.dumps([{
            'article_id': a.article_id,
            'number': a.number,
            'title': a.title,
            'regulation_type': a.regulation_type,
            'jurisdiction': a.jurisdiction,
            'concepts': a.concepts[:3],
            'related_articles': a.related_articles[:2]
        } for a in sample_articles], indent=2)
        
        analysis_prompt = ChatPromptTemplate.from_template("""
        You are a knowledge graph architect analyzing GDPR article data for optimal storage.
        
        Sample articles to analyze:
        {article_sample}
        
        Determine the optimal storage structure for GDPR articles including:
        1. Which properties should be stored as node properties
        2. What relationships should connect articles to other entities
        3. How to handle cross-references between articles
        4. How to represent jurisdictional and regulatory relationships
        
        Return JSON with storage strategy:
        {{
            "article_properties": ["article_id", "number", "title", "content", "regulation_type", "jurisdiction", "..."],
            "relationship_mappings": {{
                "concepts": {{"target_type": "Concept", "relationship": "DEFINES"}},
                "related_articles": {{"target_type": "Article", "relationship": "RELATED_TO"}},
                "territorial_scope": {{"target_type": "Jurisdiction", "relationship": "APPLIES_TO"}},
                "...": {{"target_type": "...", "relationship": "..."}}
            }},
            "regulatory_structure": {{
                "regulation_node": "Regulation",
                "jurisdiction_node": "Jurisdiction"
            }}
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages(article_sample=article_sample)
            response = await self.llm.ainvoke(messages)
            article_analysis = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-driven article storage analysis completed")
            return article_analysis
            
        except Exception as e:
            logging.error(f"âŒ Error in article storage analysis: {e}")
            return {
                "article_properties": ["article_id", "number", "title", "content", "regulation_type", "jurisdiction"],
                "relationship_mappings": {
                    "concepts": {"target_type": "Concept", "relationship": "DEFINES"},
                    "related_articles": {"target_type": "Article", "relationship": "RELATED_TO"}
                },
                "regulatory_structure": {
                    "regulation_node": "Regulation",
                    "jurisdiction_node": "Jurisdiction"
                }
            }
    
    async def _create_dynamic_article_relationships(self, article: GDPRArticle, analysis: Dict[str, Any]):
        """Create article relationships based on LLM analysis"""
        try:
            # Create regulation relationship
            reg_structure = analysis.get('regulatory_structure', {})
            regulation_node = reg_structure.get('regulation_node', 'Regulation')
            
            self.graph.query(f"""
                MATCH (a:Article {{article_id: $article_id}})
                MERGE (r:{regulation_node} {{name: $regulation_type, jurisdiction: $jurisdiction}})
                MERGE (a)-[:PART_OF]->(r)
            """, {
                'article_id': article.article_id,
                'regulation_type': article.regulation_type,
                'jurisdiction': article.jurisdiction
            })
            
            # Create dynamic relationships based on analysis
            relationship_mappings = analysis.get('relationship_mappings', {})
            
            for property_name, mapping in relationship_mappings.items():
                if hasattr(article, property_name):
                    property_values = getattr(article, property_name)
                    if property_values and isinstance(property_values, list):
                        target_type = mapping['target_type']
                        relationship = mapping['relationship']
                        
                        for value in property_values:
                            if value:
                                relationship_query = f"""
                                MATCH (source:Article {{article_id: $article_id}})
                                MERGE (target:{target_type} {{id: $target_value}})
                                MERGE (source)-[:{relationship}]->(target)
                                """
                                
                                self.graph.query(relationship_query, {
                                    'article_id': article.article_id,
                                    'target_value': str(value)
                                })
            
        except Exception as e:
            logging.error(f"âŒ Error creating dynamic article relationships: {e}")
    
    async def create_metamodel_structure(self):
        """Create metamodel structure dynamically based on LLM analysis"""
        try:
            # Use LLM to determine what metamodel structure nodes are needed
            structure_analysis = await self._analyze_metamodel_structure_needs()
            
            # Create metamodel nodes based on LLM analysis
            for structure_spec in structure_analysis['metamodel_nodes']:
                node_type = structure_spec['node_type']
                properties = structure_spec['properties']
                
                # Build property assignments
                property_assignments = []
                for prop, value in properties.items():
                    if isinstance(value, str):
                        property_assignments.append(f"{prop}: '{value}'")
                    elif isinstance(value, list):
                        property_assignments.append(f"{prop}: {json.dumps(value)}")
                    else:
                        property_assignments.append(f"{prop}: {value}")
                
                query = f"""
                MERGE (n:{node_type} {{{', '.join(property_assignments)}}})
                """
                
                try:
                    self.graph.query(query)
                except Exception as e:
                    logging.warning(f"âš ï¸ Error creating {node_type}: {e}")
            
            # Create relationships between metamodel components
            for relationship_spec in structure_analysis['metamodel_relationships']:
                rel_query = f"""
                MATCH (source:{relationship_spec['source_type']} {{name: '{relationship_spec['source_name']}'}})
                MATCH (target:{relationship_spec['target_type']} {{name: '{relationship_spec['target_name']}'}})
                MERGE (source)-[:{relationship_spec['relationship']}]->(target)
                """
                
                try:
                    self.graph.query(rel_query)
                except Exception as e:
                    logging.warning(f"âš ï¸ Error creating relationship: {e}")
            
            logging.info("âœ… Created dynamic metamodel structure")
            
        except Exception as e:
            logging.error(f"âŒ Error creating metamodel structure: {e}")
            raise
    
    async def _analyze_metamodel_structure_needs(self) -> Dict[str, Any]:
        """Use LLM to determine what metamodel structure is needed"""
        
        structure_prompt = ChatPromptTemplate.from_template("""
        You are a knowledge graph architect designing a metamodel structure for GDPR compliance in financial services.
        
        Determine what high-level structural nodes and relationships are needed for a comprehensive GDPR metamodel that includes:
        1. The metamodel itself as a central organizing node
        2. Framework nodes for different compliance areas (ROPA, risk assessment, etc.)
        3. Industry context nodes for financial services
        4. Compliance layer organization
        5. Regulatory structure representation
        
        Return JSON with the structural requirements:
        {{
            "metamodel_nodes": [
                {{
                    "node_type": "Metamodel",
                    "properties": {{
                        "name": "GDPR_Financial_Metamodel",
                        "version": "1.0",
                        "industry": "Financial Services",
                        "scope": "GDPR and UK GDPR",
                        "created_at": "datetime()"
                    }}
                }},
                {{
                    "node_type": "ROPAFramework",
                    "properties": {{
                        "name": "GDPR_ROPA",
                        "description": "Record of Processing Activities Framework"
                    }}
                }},
                ...
            ],
            "metamodel_relationships": [
                {{
                    "source_type": "Metamodel",
                    "source_name": "GDPR_Financial_Metamodel",
                    "target_type": "ROPAFramework",
                    "target_name": "GDPR_ROPA",
                    "relationship": "INCLUDES_FRAMEWORK"
                }},
                ...
            ]
        }}
        """)
        
        try:
            messages = structure_prompt.format_messages()
            response = await self.llm.ainvoke(messages)
            structure_analysis = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-determined metamodel structure analysis completed")
            return structure_analysis
            
        except Exception as e:
            logging.error(f"âŒ Error analyzing metamodel structure needs: {e}")
            # Fallback structure
            return {
                "metamodel_nodes": [
                    {
                        "node_type": "Metamodel",
                        "properties": {
                            "name": "GDPR_Financial_Metamodel",
                            "version": "1.0",
                            "industry": "Financial Services"
                        }
                    }
                ],
                "metamodel_relationships": []
            }
    
    async def vector_similarity_search(self, query_embedding: List[float], 
                                     node_type: str = "Concept", 
                                     limit: int = 10) -> List[Dict[str, Any]]:
        """Perform vector similarity search"""
        try:
            query = f"""
            CALL db.idx.vector.queryNodes('{node_type}', 'embedding', $limit, vecf32($query_embedding))
            YIELD node, score
            RETURN node, score
            ORDER BY score DESC
            """
            
            result = self.graph.query(query, {
                'query_embedding': query_embedding,
                'limit': limit
            })
            
            return [{'node': record[0], 'score': record[1]} for record in result.result_set]
            
        except Exception as e:
            logging.error(f"âŒ Error in vector similarity search: {e}")
            return []
    
    async def get_metamodel_statistics(self) -> Dict[str, Any]:
        """Get comprehensive knowledge graph statistics using dynamic discovery"""
        try:
            # Use LLM to determine what statistics to collect
            stats_analysis = await self._analyze_statistics_requirements()
            
            stats = {}
            
            # Execute dynamic queries based on LLM analysis
            for stat_name, query in stats_analysis['stat_queries'].items():
                try:
                    result = self.graph.query(query)
                    stats[stat_name] = result.result_set[0][0] if result.result_set else 0
                except Exception as e:
                    logging.warning(f"âš ï¸ Error executing stat query {stat_name}: {e}")
                    stats[stat_name] = 0
            
            # Add computed statistics
            if stats_analysis.get('computed_stats'):
                for comp_stat_name, computation in stats_analysis['computed_stats'].items():
                    try:
                        # Simple computation example
                        if computation['type'] == 'percentage':
                            numerator = stats.get(computation['numerator'], 0)
                            denominator = stats.get(computation['denominator'], 1)
                            stats[comp_stat_name] = (numerator / denominator * 100) if denominator > 0 else 0
                    except Exception as e:
                        logging.warning(f"âš ï¸ Error computing {comp_stat_name}: {e}")
                        stats[comp_stat_name] = 0
            
            return stats
            
        except Exception as e:
            logging.error(f"âŒ Error getting statistics: {e}")
            return {}
    
    async def _analyze_statistics_requirements(self) -> Dict[str, Any]:
        """Use LLM to determine what statistics to collect from the knowledge graph"""
        
        # First, discover what node types and relationships exist
        discovery_queries = [
            "MATCH (n) RETURN DISTINCT labels(n) as node_types LIMIT 20",
            "MATCH ()-[r]->() RETURN DISTINCT type(r) as relationship_types LIMIT 20"
        ]
        
        node_types = []
        relationship_types = []
        
        try:
            # Discover node types
            result = self.graph.query(discovery_queries[0])
            for record in result.result_set:
                if record[0]:  # labels(n) returns a list
                    node_types.extend(record[0])
            
            # Discover relationship types
            result = self.graph.query(discovery_queries[1])
            for record in result.result_set:
                if record[0]:
                    relationship_types.append(record[0])
                    
        except Exception as e:
            logging.warning(f"âš ï¸ Error discovering graph structure: {e}")
        
        # Use LLM to determine relevant statistics
        stats_prompt = ChatPromptTemplate.from_template("""
        You are a knowledge graph analyst. Based on the discovered graph structure, determine what statistics would be most valuable for a GDPR metamodel knowledge graph.
        
        Discovered Structure:
        - Node Types: {node_types}
        - Relationship Types: {relationship_types}
        
        Generate Cypher queries to collect meaningful statistics for:
        1. Node counts by type
        2. Relationship counts by type
        3. GDPR vs UK GDPR distribution
        4. High-relevance concept counts
        5. Cross-jurisdictional coverage
        6. Any other meaningful metrics
        
        Return JSON with Cypher queries:
        {{
            "stat_queries": {{
                "total_concepts": "MATCH (n:Concept) RETURN count(n)",
                "total_articles": "MATCH (n:Article) RETURN count(n)",
                "...": "..."
            }},
            "computed_stats": {{
                "gdpr_coverage_percentage": {{
                    "type": "percentage",
                    "numerator": "gdpr_concepts",
                    "denominator": "total_concepts"
                }}
            }}
        }}
        """)
        
        try:
            messages = stats_prompt.format_messages(
                node_types=list(set(node_types)),
                relationship_types=list(set(relationship_types))
            )
            response = await self.llm.ainvoke(messages)
            stats_analysis = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-determined statistics analysis completed")
            return stats_analysis
            
        except Exception as e:
            logging.error(f"âŒ Error analyzing statistics requirements: {e}")
            # Fallback queries
            return {
                "stat_queries": {
                    "total_nodes": "MATCH (n) RETURN count(n)",
                    "total_relationships": "MATCH ()-[r]->() RETURN count(r)"
                },
                "computed_stats": {}
            }

# ============================================================================
# ENHANCED CONCEPT EXTRACTOR
# ============================================================================

class SemanticConceptExtractor:
    """Enhanced concept extractor with semantic ontology integration"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
        self.embeddings = DirectOpenAIEmbeddings(config)
    
    async def extract_semantic_concepts(self, documents: List[Document]) -> List[SemanticConcept]:
        """Extract concepts with dynamic structure determined by LLM analysis"""
        
        print(f"ðŸš€ Starting semantic concept extraction from {len(documents)} documents")
        
        # First, analyze document content to determine extraction strategy
        print("ðŸ§  Analyzing extraction strategy...")
        extraction_strategy = await self._analyze_extraction_strategy(documents)
        print(f"âœ… Extraction strategy determined: {json.dumps(extraction_strategy, indent=2)}")
        
        extraction_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR expert specializing in financial services compliance.
        
        Based on the analysis, extract comprehensive semantic concepts from this regulatory text using the following strategy:
        
        Extraction Strategy:
        {extraction_strategy}
        
        Text: {text}
        
        Extract concepts following the determined structure and return as JSON:
        {response_format}
        """)
        
        concepts = []
        concept_map = {}
        
        print(f"ðŸ“„ Processing {len(documents)} documents for concept extraction...")
        
        for doc_idx, doc in enumerate(documents):
            print(f"\nðŸ“‹ Processing document {doc_idx + 1}/{len(documents)}")
            print(f"ðŸ“„ Document source: {doc.metadata.get('source', 'unknown')}")
            print(f"ðŸ“ Document length: {len(doc.page_content)} characters")
            
            try:
                text_chunk = doc.page_content[:8000]
                print(f"ðŸ”¤ Using text chunk: {len(text_chunk)} characters")
                
                messages = extraction_prompt.format_messages(
                    text=text_chunk,
                    extraction_strategy=json.dumps(extraction_strategy, indent=2),
                    response_format=extraction_strategy['response_format']
                )
                
                print(f"ðŸ¤– Sending request to LLM for concept extraction...")
                response = await self.llm.ainvoke(messages)
                print(f"ðŸ“¨ Received LLM response: {len(response.content)} characters")
                
                try:
                    print(f"ðŸ” Parsing JSON response...")
                    extracted_data = json.loads(response.content)
                    print(f"âœ… JSON parsed successfully")
                    print(f"ðŸ“Š Extracted data structure: {list(extracted_data.keys())}")
                    
                    await self._process_extracted_concepts(
                        extracted_data, doc, concept_map, extraction_strategy
                    )
                        
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON parsing failed: {e}")
                    print(f"ðŸ” Raw response content: {response.content[:500]}...")
                    logging.warning(f"âš ï¸ Failed to parse concept extraction: {e}")
                    
            except Exception as e:
                print(f"âŒ Error extracting concepts from document {doc_idx + 1}: {e}")
                logging.error(f"âŒ Error extracting concepts: {e}")
                import traceback
                print(f"ðŸ“‹ Traceback: {traceback.format_exc()}")
        
        final_concepts = list(concept_map.values())
        print(f"\nðŸ Concept extraction completed!")
        print(f"ðŸ“Š Total concepts extracted: {len(final_concepts)}")
        print(f"ðŸ” Concept IDs: {[c.concept_id for c in final_concepts[:5]]}...")
        
        return final_concepts
    
    async def _analyze_extraction_strategy(self, documents: List[Document]) -> Dict[str, Any]:
        """Use LLM to analyze documents and determine optimal extraction strategy"""
        
        print("ðŸ” Analyzing document content for extraction strategy...")
        
        # Sample document content for analysis
        sample_content = "\n".join([doc.page_content[:1000] for doc in documents[:3]])
        document_types = [doc.metadata.get("document_type", "unknown") for doc in documents]
        
        print(f"ðŸ“„ Document types found: {document_types}")
        print(f"ðŸ“ Sample content length: {len(sample_content)} characters")
        
        strategy_prompt = ChatPromptTemplate.from_template("""
        You are an expert in knowledge extraction and ontology design for GDPR compliance.
        
        Analyze this sample content from GDPR-related documents and determine the optimal extraction strategy:
        
        Document Types Found: {document_types}
        
        Sample Content:
        {sample_content}
        
        Determine:
        1. What types of concepts should be extracted (legal, procedural, technical, etc.)
        2. What properties each concept should have
        3. What relationships between concepts should be identified
        4. What ontology standards should be applied (SKOS, PROV-O, etc.)
        5. The optimal JSON response format for extraction
        
        Return a comprehensive extraction strategy as JSON:
        {{
            "concept_categories": ["legal_basis", "data_subject_rights", "processing_activities", "..."],
            "concept_properties": {{
                "required": ["concept_id", "label", "definition"],
                "optional": ["category", "regulation_type", "jurisdiction", "..."],
                "computed": ["skos_type", "prov_type", "ropa_relevance"]
            }},
            "relationship_types": ["broader", "narrower", "related", "referenced_in", "..."],
            "ontology_mappings": {{
                "skos_types": ["skos:Concept", "skos:ConceptScheme"],
                "prov_types": ["prov:Entity", "prov:Activity", "prov:Agent"]
            }},
            "response_format": "{{\"concepts\": [{{\"concept_id\": \"...\", \"label\": \"...\", ...}}]}}",
            "extraction_focus": ["financial_services_relevance", "cross_border_implications", "ropa_requirements"]
        }}
        """)
        
        try:
            print("ðŸ¤– Sending strategy analysis request to LLM...")
            messages = strategy_prompt.format_messages(
                document_types=document_types,
                sample_content=sample_content[:4000]
            )
            response = await self.llm.ainvoke(messages)
            print(f"ðŸ“¨ Received strategy response: {len(response.content)} characters")
            
            print("ðŸ” Parsing strategy JSON...")
            strategy = json.loads(response.content)
            print(f"âœ… Strategy parsed successfully")
            print(f"ðŸ“Š Strategy keys: {list(strategy.keys())}")
            
            logging.info(f"ðŸ§  LLM-determined extraction strategy completed")
            return strategy
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing failed for strategy: {e}")
            print(f"ðŸ” Raw strategy response: {response.content[:500]}...")
            logging.error(f"âŒ Error analyzing extraction strategy: {e}")
            # Fallback strategy
            fallback_strategy = {
                "concept_categories": ["legal_basis", "data_subject_rights", "processing_activities"],
                "concept_properties": {
                    "required": ["concept_id", "label", "definition"],
                    "optional": ["category", "regulation_type", "jurisdiction"],
                    "computed": ["skos_type", "prov_type", "ropa_relevance"]
                },
                "relationship_types": ["broader", "narrower", "related"],
                "response_format": '{"concepts": [{"concept_id": "...", "label": "...", "definition": "..."}]}',
                "extraction_focus": ["financial_services_relevance"]
            }
            print(f"ðŸ”„ Using fallback strategy: {json.dumps(fallback_strategy, indent=2)}")
            return fallback_strategy
        except Exception as e:
            print(f"âŒ Error analyzing extraction strategy: {e}")
            logging.error(f"âŒ Error analyzing extraction strategy: {e}")
            # Fallback strategy
            fallback_strategy = {
                "concept_categories": ["legal_basis", "data_subject_rights", "processing_activities"],
                "concept_properties": {
                    "required": ["concept_id", "label", "definition"],
                    "optional": ["category", "regulation_type", "jurisdiction"],
                    "computed": ["skos_type", "prov_type", "ropa_relevance"]
                },
                "relationship_types": ["broader", "narrower", "related"],
                "response_format": '{"concepts": [{"concept_id": "...", "label": "...", "definition": "..."}]}',
                "extraction_focus": ["financial_services_relevance"]
            }
            print(f"ðŸ”„ Using fallback strategy: {json.dumps(fallback_strategy, indent=2)}")
            return fallback_strategy
    
    async def _process_extracted_concepts(self, extracted_data: Dict, doc: Document, 
                                        concept_map: Dict, strategy: Dict):
        """Process extracted concepts using dynamic strategy"""
        
        print(f"ðŸ” Processing extracted concepts from document: {doc.metadata.get('source', 'unknown')}")
        print(f"ðŸ“Š Strategy: {json.dumps(strategy, indent=2)}")
        print(f"ðŸ“‹ Extracted data keys: {list(extracted_data.keys())}")
        
        required_props = strategy['concept_properties']['required']
        optional_props = strategy['concept_properties']['optional']
        
        # Get valid SemanticConcept field names from the dataclass
        valid_fields = {
            'concept_id', 'label', 'definition', 'category', 'skos_type', 'prov_type',
            'article_references', 'regulation_type', 'jurisdiction', 'legal_basis',
            'broader_concepts', 'narrower_concepts', 'related_concepts',
            'industry_definitions', 'compliance_requirements', 'ropa_relevance',
            'source_document', 'extracted_from', 'confidence_score', 'last_updated', 'embedding'
        }
        
        concepts_data = extracted_data.get("concepts", [])
        print(f"ðŸ”¢ Found {len(concepts_data)} concepts to process")
        
        for i, concept_data in enumerate(concepts_data):
            print(f"\nðŸ“ Processing concept {i+1}: {concept_data.get('concept_id', 'unknown_id')}")
            print(f"ðŸ·ï¸ Concept data keys: {list(concept_data.keys())}")
            
            # Validate required properties
            missing_required = [prop for prop in required_props if prop not in concept_data]
            if missing_required:
                print(f"âš ï¸ Concept missing required properties: {missing_required}")
                print(f"ðŸ” Available data: {concept_data}")
                continue
            
            concept_id = concept_data["concept_id"]
            print(f"ðŸ†” Processing concept ID: {concept_id}")
            
            if concept_id not in concept_map:
                # Generate embedding for the concept
                concept_text = f"{concept_data.get('label', '')}: {concept_data.get('definition', '')}"
                print(f"ðŸ”® Generating embedding for: {concept_text[:100]}...")
                embedding = await self.embeddings.embed_text(concept_text)
                print(f"âœ… Generated embedding with {len(embedding)} dimensions")
                
                # Create concept with dynamic properties - filter to valid fields only
                concept_kwargs = {
                    'concept_id': concept_id,
                    'source_document': doc.metadata.get("source", ""),
                    'extracted_from': doc.metadata.get("chunk_id", ""),
                    'confidence_score': 0.85,
                    'last_updated': datetime.now(),
                    'embedding': embedding
                }
                
                # Add all available properties, but only if they're valid fields
                all_possible_props = required_props + optional_props
                print(f"ðŸ” All possible properties from strategy: {all_possible_props}")
                
                for prop in all_possible_props:
                    if prop in concept_data and prop in valid_fields:
                        value = concept_data[prop]
                        concept_kwargs[prop] = value
                        print(f"âœ… Added property {prop}: {str(value)[:50]}...")
                    elif prop in concept_data and prop not in valid_fields:
                        print(f"âš ï¸ Skipping invalid property {prop}: not in SemanticConcept fields")
                
                # Filter out any unexpected properties that might have been returned by LLM
                filtered_kwargs = {k: v for k, v in concept_kwargs.items() if k in valid_fields}
                
                # Set defaults for missing required SemanticConcept fields
                defaults = {
                    'label': concept_data.get('label', ''),
                    'definition': concept_data.get('definition', ''),
                    'category': concept_data.get('category', 'general'),
                    'skos_type': concept_data.get('skos_type', 'skos:Concept'),
                    'prov_type': concept_data.get('prov_type', 'prov:Entity'),
                    'regulation_type': concept_data.get('regulation_type', 'GDPR'),
                    'jurisdiction': concept_data.get('jurisdiction', 'EU'),
                    'legal_basis': concept_data.get('legal_basis', []),
                    'broader_concepts': concept_data.get('broader_concepts', []),
                    'narrower_concepts': concept_data.get('narrower_concepts', []),
                    'related_concepts': concept_data.get('related_concepts', []),
                    'article_references': concept_data.get('article_references', []),
                    'industry_definitions': concept_data.get('industry_definitions', {}),
                    'compliance_requirements': concept_data.get('compliance_requirements', []),
                    'ropa_relevance': concept_data.get('ropa_relevance', 'medium')
                }
                
                # Apply defaults for missing fields
                for field, default_value in defaults.items():
                    if field not in filtered_kwargs:
                        filtered_kwargs[field] = default_value
                
                print(f"ðŸ—ï¸ Creating SemanticConcept with fields: {list(filtered_kwargs.keys())}")
                
                try:
                    concept = SemanticConcept(**filtered_kwargs)
                    concept_map[concept_id] = concept
                    print(f"âœ… Successfully created concept: {concept.label}")
                except Exception as e:
                    print(f"âŒ Error creating SemanticConcept: {e}")
                    print(f"ðŸ” Problematic kwargs: {filtered_kwargs}")
                    # Show which fields are causing issues
                    for key, value in filtered_kwargs.items():
                        try:
                            # Test if this field would work
                            test_kwargs = {'concept_id': 'test', 'label': 'test', 'definition': 'test', 
                                         'category': 'test', 'skos_type': 'test', 'prov_type': 'test',
                                         'regulation_type': 'test', 'jurisdiction': 'test', 'legal_basis': [],
                                         'broader_concepts': [], 'narrower_concepts': [], 'related_concepts': [],
                                         'article_references': [], 'industry_definitions': {}, 
                                         'compliance_requirements': [], 'ropa_relevance': 'test',
                                         'source_document': 'test', 'extracted_from': 'test',
                                         'confidence_score': 0.8, 'last_updated': datetime.now()}
                            test_kwargs[key] = value
                            print(f"ðŸ§ª Field {key} = {type(value).__name__}: {str(value)[:50]}...")
                        except Exception as field_error:
                            print(f"âŒ Problem with field {key}: {field_error}")
                    raise
            else:
                print(f"â­ï¸ Concept {concept_id} already exists, skipping")
        
        print(f"ðŸ Finished processing concepts. Total in map: {len(concept_map)}")
    
    async def extract_gdpr_articles(self, documents: List[Document]) -> List[GDPRArticle]:
        """Extract GDPR articles with dynamic structure based on LLM analysis"""
        
        # Analyze documents to determine article extraction strategy
        article_strategy = await self._analyze_article_extraction_strategy(documents)
        
        article_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR regulatory expert. Extract GDPR/UK GDPR articles from this text using the determined strategy.
        
        Extraction Strategy:
        {article_strategy}
        
        Text: {text}
        
        Extract articles following the strategy and return as JSON:
        {response_format}
        """)
        
        articles = []
        article_map = {}
        
        regulation_docs = [doc for doc in documents 
                          if "GDPR" in doc.metadata.get("document_type", "")]
        
        for doc in regulation_docs:
            try:
                messages = article_prompt.format_messages(
                    text=doc.page_content[:8000],
                    article_strategy=json.dumps(article_strategy, indent=2),
                    response_format=article_strategy['response_format']
                )
                response = await self.llm.ainvoke(messages)
                
                try:
                    extracted_data = json.loads(response.content)
                    await self._process_extracted_articles(
                        extracted_data, doc, article_map, article_strategy
                    )
                        
                except json.JSONDecodeError as e:
                    logging.warning(f"âš ï¸ Failed to parse article extraction: {e}")
                    
            except Exception as e:
                logging.error(f"âŒ Error extracting articles: {e}")
        
        return list(article_map.values())
    
    async def _analyze_article_extraction_strategy(self, documents: List[Document]) -> Dict[str, Any]:
        """Analyze documents to determine article extraction strategy"""
        
        sample_content = "\n".join([
            doc.page_content[:1000] for doc in documents 
            if "GDPR" in doc.metadata.get("document_type", "")
        ][:2])
        
        strategy_prompt = ChatPromptTemplate.from_template("""
        You are an expert in regulatory document analysis and legal text extraction.
        
        Analyze this sample GDPR regulatory content and determine the optimal strategy for extracting articles:
        
        Sample Content:
        {sample_content}
        
        Determine:
        1. How to identify article boundaries and numbers
        2. What properties each article should have
        3. How to extract cross-references between articles
        4. What regulatory and jurisdictional information to capture
        5. How to identify financial services relevance
        6. The optimal JSON response format
        
        Return a comprehensive article extraction strategy:
        {{
            "article_identification": {{
                "patterns": ["Article \\d+", "Art\\. \\d+", "..."],
                "title_extraction": "method for extracting article titles",
                "content_boundaries": "how to determine article content limits"
            }},
            "article_properties": {{
                "required": ["article_id", "number", "title", "content"],
                "regulatory": ["regulation_type", "jurisdiction", "legal_bases"],
                "relationships": ["concepts", "related_articles", "territorial_scope"],
                "industry_specific": ["financial_sector_relevance", "ropa_implications"]
            }},
            "cross_reference_patterns": ["Article \\d+", "paragraph \\d+", "..."],
            "response_format": "{{\"articles\": [{{\"article_id\": \"...\", ...}}]}}",
            "extraction_priorities": ["financial_industry_relevance", "cross_border_implications"]
        }}
        """)
        
        try:
            messages = strategy_prompt.format_messages(sample_content=sample_content[:4000])
            response = await self.llm.ainvoke(messages)
            strategy = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-determined article extraction strategy completed")
            return strategy
            
        except Exception as e:
            logging.error(f"âŒ Error analyzing article extraction strategy: {e}")
            # Fallback strategy
            return {
                "article_identification": {
                    "patterns": ["Article \\d+"],
                    "title_extraction": "text following article number",
                    "content_boundaries": "until next article or section"
                },
                "article_properties": {
                    "required": ["article_id", "number", "title", "content"],
                    "regulatory": ["regulation_type", "jurisdiction"],
                    "relationships": ["concepts", "related_articles"],
                    "industry_specific": ["financial_sector_relevance"]
                },
                "response_format": '{"articles": [{"article_id": "...", "number": "...", "title": "...", "content": "..."}]}',
                "extraction_priorities": ["financial_industry_relevance"]
            }
    
    async def _process_extracted_articles(self, extracted_data: Dict, doc: Document, 
                                        article_map: Dict, strategy: Dict):
        """Process extracted articles using dynamic strategy"""
        
        print(f"ðŸ” Processing extracted articles from document: {doc.metadata.get('source', 'unknown')}")
        print(f"ðŸ“Š Strategy: {json.dumps(strategy, indent=2)}")
        print(f"ðŸ“‹ Extracted data keys: {list(extracted_data.keys())}")
        
        required_props = strategy['article_properties']['required']
        
        # Get valid GDPRArticle field names from the dataclass
        valid_fields = {
            'article_id', 'number', 'title', 'content', 'regulation_type', 'concepts',
            'obligations', 'rights', 'legal_bases', 'jurisdiction', 'territorial_scope',
            'cross_border_implications', 'financial_sector_relevance', 'ropa_implications',
            'related_articles', 'implementing_guidelines', 'case_law_references',
            'source_document', 'confidence_score', 'last_updated', 'embedding'
        }
        
        articles_data = extracted_data.get("articles", [])
        print(f"ðŸ”¢ Found {len(articles_data)} articles to process")
        
        for i, article_data in enumerate(articles_data):
            print(f"\nðŸ“ Processing article {i+1}: {article_data.get('article_id', 'unknown_id')}")
            print(f"ðŸ·ï¸ Article data keys: {list(article_data.keys())}")
            
            # Validate required properties
            missing_required = [prop for prop in required_props if prop not in article_data]
            if missing_required:
                print(f"âš ï¸ Article missing required properties: {missing_required}")
                print(f"ðŸ” Available data: {article_data}")
                continue
            
            article_id = article_data["article_id"]
            print(f"ðŸ†” Processing article ID: {article_id}")
            
            if article_id not in article_map:
                # Generate embedding for the article
                article_text = f"{article_data.get('title', '')}: {article_data.get('content', '')}"
                print(f"ðŸ”® Generating embedding for: {article_text[:100]}...")
                embedding = await self.embeddings.embed_text(article_text)
                print(f"âœ… Generated embedding with {len(embedding)} dimensions")
                
                # Create article with dynamic properties - filter to valid fields only
                article_kwargs = {
                    'article_id': article_id,
                    'number': article_data["number"],
                    'title': article_data["title"],
                    'content': article_data["content"],
                    'source_document': doc.metadata.get("source", ""),
                    'confidence_score': 0.9,
                    'last_updated': datetime.now(),
                    'embedding': embedding
                }
                
                # Add all available properties dynamically, but only if they're valid fields
                all_props = (strategy['article_properties'].get('regulatory', []) + 
                           strategy['article_properties'].get('relationships', []) + 
                           strategy['article_properties'].get('industry_specific', []))
                
                print(f"ðŸ” All possible properties from strategy: {all_props}")
                
                for prop in all_props:
                    if prop in article_data and prop in valid_fields:
                        value = article_data[prop]
                        article_kwargs[prop] = value
                        print(f"âœ… Added property {prop}: {str(value)[:50]}...")
                    elif prop in article_data and prop not in valid_fields:
                        print(f"âš ï¸ Skipping invalid property {prop}: not in GDPRArticle fields")
                
                # Filter out any unexpected properties
                filtered_kwargs = {k: v for k, v in article_kwargs.items() if k in valid_fields}
                
                # Set defaults for missing required GDPRArticle fields
                defaults = {
                    'regulation_type': article_data.get('regulation_type', 'GDPR'),
                    'concepts': article_data.get('concepts', []),
                    'obligations': article_data.get('obligations', []),
                    'rights': article_data.get('rights', []),
                    'legal_bases': article_data.get('legal_bases', []),
                    'jurisdiction': article_data.get('jurisdiction', 'EU'),
                    'territorial_scope': article_data.get('territorial_scope', []),
                    'cross_border_implications': article_data.get('cross_border_implications', []),
                    'financial_sector_relevance': article_data.get('financial_sector_relevance', 'medium'),
                    'ropa_implications': article_data.get('ropa_implications', []),
                    'related_articles': article_data.get('related_articles', []),
                    'implementing_guidelines': article_data.get('implementing_guidelines', []),
                    'case_law_references': article_data.get('case_law_references', [])
                }
                
                # Apply defaults for missing fields
                for field, default_value in defaults.items():
                    if field not in filtered_kwargs:
                        filtered_kwargs[field] = default_value
                
                print(f"ðŸ—ï¸ Creating GDPRArticle with fields: {list(filtered_kwargs.keys())}")
                
                try:
                    article = GDPRArticle(**filtered_kwargs)
                    article_map[article_id] = article
                    print(f"âœ… Successfully created article: {article.title}")
                except Exception as e:
                    print(f"âŒ Error creating GDPRArticle: {e}")
                    print(f"ðŸ” Problematic kwargs: {filtered_kwargs}")
                    # Show which fields are causing issues
                    for key, value in filtered_kwargs.items():
                        print(f"ðŸ§ª Field {key} = {type(value).__name__}: {str(value)[:50]}...")
                    raise
            else:
                print(f"â­ï¸ Article {article_id} already exists, skipping")
        
        print(f"ðŸ Finished processing articles. Total in map: {len(article_map)}")

# ============================================================================
# METAMODEL GENERATOR
# ============================================================================

class GDPRMetamodelGenerator:
    """Generate comprehensive GDPR metamodel for financial services"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = ChatOpenAI(
            model=config.REASONING_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            openai_api_base=config.OPENAI_BASE_URL
        )
    
    async def generate_metamodel(self, concepts: List[SemanticConcept], 
                               articles: List[GDPRArticle],
                               kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive GDPR metamodel using dynamic analysis"""
        
        # First, analyze the extracted knowledge to determine metamodel structure
        metamodel_structure = await self._analyze_metamodel_requirements(concepts, articles, kg_stats)
        
        metamodel_prompt = ChatPromptTemplate.from_template("""
        You are a GDPR compliance expert for financial services creating a comprehensive metamodel.
        
        Based on the analysis of extracted knowledge, create a metamodel following this structure:
        
        Metamodel Structure Requirements:
        {metamodel_structure}
        
        Knowledge Summary:
        - Total Concepts: {total_concepts}
        - Total Articles: {total_articles}
        - Knowledge Graph Statistics: {kg_stats}
        
        Sample Data:
        {sample_data}
        
        Create a comprehensive metamodel that addresses all structural requirements and return as JSON:
        {response_format}
        """)
        
        # Prepare sample data
        sample_concepts = [
            {
                'label': c.label,
                'category': c.category,
                'regulation_type': c.regulation_type,
                'ropa_relevance': c.ropa_relevance,
                'jurisdiction': c.jurisdiction
            }
            for c in concepts[:10]
        ]
        
        sample_articles = [
            {
                'number': a.number,
                'title': a.title,
                'regulation_type': a.regulation_type,
                'financial_sector_relevance': getattr(a, 'financial_sector_relevance', 'unknown'),
                'jurisdiction': a.jurisdiction
            }
            for a in articles[:10]
        ]
        
        sample_data = {
            'concepts': sample_concepts,
            'articles': sample_articles
        }
        
        try:
            messages = metamodel_prompt.format_messages(
                metamodel_structure=json.dumps(metamodel_structure, indent=2),
                total_concepts=len(concepts),
                total_articles=len(articles),
                kg_stats=json.dumps(kg_stats, indent=2),
                sample_data=json.dumps(sample_data, indent=2),
                response_format=metamodel_structure['response_format']
            )
            
            response = await self.llm.ainvoke(messages)
            metamodel = json.loads(response.content)
            
            # Enhance with computed metrics
            metamodel['computed_metrics'] = await self._compute_metamodel_metrics(concepts, articles, kg_stats)
            
            return metamodel
            
        except Exception as e:
            logging.error(f"âŒ Error generating metamodel: {e}")
            return {}
    
    async def _analyze_metamodel_requirements(self, concepts: List[SemanticConcept], 
                                            articles: List[GDPRArticle],
                                            kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Use LLM to analyze knowledge and determine metamodel structure requirements"""
        
        # Analyze concept categories and regulation types
        concept_categories = list(set(c.category for c in concepts))
        regulation_types = list(set(c.regulation_type for c in concepts))
        jurisdictions = list(set(c.jurisdiction for c in concepts))
        ropa_relevance_levels = list(set(c.ropa_relevance for c in concepts))
        
        analysis_prompt = ChatPromptTemplate.from_template("""
        You are a metamodel architect analyzing GDPR knowledge for financial services compliance.
        
        Analyze this extracted knowledge and determine the optimal metamodel structure:
        
        Knowledge Analysis:
        - Concept Categories: {concept_categories}
        - Regulation Types: {regulation_types}
        - Jurisdictions: {jurisdictions}
        - ROPA Relevance Levels: {ropa_relevance_levels}
        - Total Knowledge Items: {total_items}
        
        Determine the metamodel structure that should include:
        1. Core taxonomies needed for comprehensive coverage
        2. Jurisdiction-specific mappings and variations
        3. Financial services industry-specific elements
        4. ROPA framework alignment requirements
        5. Compliance workflow structures
        6. Cross-jurisdictional considerations
        
        Return JSON with metamodel structure requirements:
        {{
            "required_taxonomies": ["data_categories", "processing_activities", "legal_bases", "..."],
            "jurisdiction_requirements": {{
                "EU_GDPR": ["specific_requirements", "..."],
                "UK_GDPR": ["variations_from_EU", "..."],
                "cross_border": ["transfer_mechanisms", "adequacy_decisions", "..."]
            }},
            "financial_services_elements": ["banking_specific", "risk_management", "regulatory_reporting", "..."],
            "ropa_framework_components": ["mandatory_elements", "documentation_requirements", "..."],
            "compliance_workflows": ["data_lifecycle", "incident_response", "audit_procedures", "..."],
            "response_format": "{{\"metamodel_version\": \"1.0\", \"scope\": \"...\", \"core_taxonomy\": {{...}}, ...}}"
        }}
        """)
        
        try:
            messages = analysis_prompt.format_messages(
                concept_categories=concept_categories,
                regulation_types=regulation_types,
                jurisdictions=jurisdictions,
                ropa_relevance_levels=ropa_relevance_levels,
                total_items=len(concepts) + len(articles)
            )
            response = await self.llm.ainvoke(messages)
            structure_requirements = json.loads(response.content)
            
            logging.info(f"ðŸ§  LLM-determined metamodel structure requirements completed")
            return structure_requirements
            
        except Exception as e:
            logging.error(f"âŒ Error analyzing metamodel requirements: {e}")
            # Fallback structure
            return {
                "required_taxonomies": ["data_categories", "processing_activities", "legal_bases"],
                "jurisdiction_requirements": {
                    "EU_GDPR": ["general_requirements"],
                    "UK_GDPR": ["uk_variations"]
                },
                "financial_services_elements": ["banking_activities"],
                "ropa_framework_components": ["required_elements"],
                "compliance_workflows": ["data_processing_lifecycle"],
                "response_format": '{"metamodel_version": "1.0", "scope": "GDPR Financial Services"}'
            }
    
    async def _compute_metamodel_metrics(self, concepts: List[SemanticConcept], 
                                       articles: List[GDPRArticle],
                                       kg_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Compute comprehensive metrics for the metamodel"""
        try:
            # Dynamic metric computation
            metrics = {
                'concept_coverage': len(concepts),
                'article_coverage': len(articles),
                'jurisdiction_coverage': len(set(c.jurisdiction for c in concepts)),
                'regulation_type_coverage': len(set(c.regulation_type for c in concepts))
            }
            
            # ROPA relevance metrics
            high_ropa_concepts = [c for c in concepts if c.ropa_relevance in ['high', 'critical']]
            metrics['ropa_coverage_percentage'] = (len(high_ropa_concepts) / len(concepts) * 100) if concepts else 0
            
            # Financial relevance metrics
            financial_relevant_articles = [a for a in articles 
                                         if getattr(a, 'financial_sector_relevance', 'medium') in ['high', 'critical']]
            metrics['financial_relevance_score'] = (len(financial_relevant_articles) / len(articles) * 100) if articles else 0
            
            # Category distribution
            category_distribution = {}
            for concept in concepts:
                category = concept.category
                category_distribution[category] = category_distribution.get(category, 0) + 1
            metrics['category_distribution'] = category_distribution
            
            return metrics
            
        except Exception as e:
            logging.error(f"âŒ Error computing metamodel metrics: {e}")
            return {}
    
    async def generate_ropa_report(self, metamodel: Dict[str, Any], 
                                 concepts: List[SemanticConcept],
                                 articles: List[GDPRArticle]) -> Dict[str, Any]:
        """Generate ROPA-focused compliance report"""
        
        ropa_prompt = ChatPromptTemplate.from_template("""
        Generate a comprehensive ROPA (Record of Processing Activities) compliance report
        for financial services based on the GDPR metamodel.
        
        Metamodel Summary: {metamodel_summary}
        ROPA-Relevant Concepts: {ropa_concepts}
        
        Create a detailed ROPA report covering:
        1. Processing activity categories
        2. Legal basis requirements
        3. Data subject rights implications
        4. Cross-border transfer considerations
        5. Financial sector compliance requirements
        6. Documentation templates
        7. Review and audit procedures
        
        Return comprehensive JSON report:
        {{
            "ropa_report_version": "1.0",
            "scope": "Financial Services GDPR/UK GDPR Compliance",
            "processing_categories": {{
                "customer_onboarding": {{...}},
                "transaction_processing": {{...}},
                "risk_assessment": {{...}},
                "regulatory_reporting": {{...}}
            }},
            "compliance_requirements": {{
                "mandatory_elements": [...],
                "documentation_standards": [...],
                "review_frequencies": [...]
            }},
            "risk_assessment": {{
                "high_risk_activities": [...],
                "mitigation_measures": [...],
                "monitoring_requirements": [...]
            }},
            "implementation_roadmap": {{
                "phase_1": {{...}},
                "phase_2": {{...}},
                "phase_3": {{...}}
            }}
        }}
        """)
        
        # Prepare ROPA-relevant data
        ropa_concepts = [c for c in concepts if c.ropa_relevance in ['high', 'critical']]
        metamodel_summary = json.dumps({
            'core_taxonomy': metamodel.get('core_taxonomy', {}),
            'ropa_framework': metamodel.get('ropa_framework', {}),
            'computed_metrics': metamodel.get('computed_metrics', {})
        }, indent=2)
        
        ropa_concepts_summary = json.dumps([
            {
                'label': c.label,
                'definition': c.definition,
                'compliance_requirements': c.compliance_requirements,
                'ropa_relevance': c.ropa_relevance
            }
            for c in ropa_concepts[:10]
        ], indent=2)
        
        try:
            messages = ropa_prompt.format_messages(
                metamodel_summary=metamodel_summary,
                ropa_concepts=ropa_concepts_summary
            )
            
            response = await self.llm.ainvoke(messages)
            ropa_report = json.loads(response.content)
            
            # Add implementation metrics
            ropa_report['implementation_metrics'] = {
                'total_ropa_concepts': len(ropa_concepts),
                'coverage_by_regulation': {
                    'GDPR': len([c for c in ropa_concepts if c.regulation_type == 'GDPR']),
                    'UK_GDPR': len([c for c in ropa_concepts if c.regulation_type == 'UK_GDPR'])
                },
                'compliance_complexity_score': len(set(req for c in ropa_concepts for req in c.compliance_requirements)),
                'financial_sector_coverage': len([c for c in ropa_concepts if 'financial' in c.industry_definitions])
            }
            
            return ropa_report
            
        except Exception as e:
            logging.error(f"âŒ Error generating ROPA report: {e}")
            return {}

# ============================================================================
# DIRECT OPENAI EMBEDDINGS
# ============================================================================

class DirectOpenAIEmbeddings:
    """Direct OpenAI API embeddings without tiktoken overhead"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = AsyncOpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL
        )
        self.model = config.EMBEDDING_MODEL
        self.dimensions = config.EMBEDDING_DIMENSIONS
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            return response.data[0].embedding
            
        except Exception as e:
            logging.error(f"Error generating embedding: {e}")
            return [0.0] * self.dimensions
    
    async def embed_texts(self, texts: List[str], batch_size: int = 20) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently with rate limiting"""
        embeddings = []
        
        print(f"ðŸ”® Generating embeddings for {len(texts)} texts in batches of {batch_size}")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            print(f"ðŸ”® Processing embedding batch {batch_num}/{total_batches} ({len(batch)} texts)")
            
            try:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                    dimensions=self.dimensions
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
                print(f"âœ… Batch {batch_num} completed: {len(batch_embeddings)} embeddings generated")
                
                # Progressive rate limiting - longer delays for larger batches
                if len(batch) >= batch_size:
                    delay = min(0.5, batch_size * 0.02)  # Max 0.5 seconds
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ Error generating embeddings for batch {batch_num}: {e}")
                logging.error(f"Error generating embeddings for batch {i}: {e}")
                # Add zero vectors as fallback
                fallback_embeddings = [[0.0] * self.dimensions] * len(batch)
                embeddings.extend(fallback_embeddings)
                print(f"ðŸ”„ Using fallback embeddings for batch {batch_num}")
        
        print(f"ðŸ Embedding generation completed: {len(embeddings)} total embeddings")
        return embeddings

# ============================================================================
# ENHANCED ORCHESTRATOR
# ============================================================================

class GDPRMetamodelOrchestrator:
    """Main orchestrator for GDPR metamodel generation"""
    
    def __init__(self, config: Config):
        self.config = config
        config.validate()
        
        # Initialize components
        self.pdf_processor = MemoryEnhancedPDFProcessor(config)
        self.concept_extractor = SemanticConceptExtractor(config)
        self.falkor_manager = EnhancedFalkorDBManager(config)
        self.metamodel_generator = GDPRMetamodelGenerator(config)
        self.elasticsearch_manager = MemoryEnhancedElasticsearchManager(config)
        
        self.setup_logging()
    
    def setup_logging(self):
        """Setup comprehensive logging"""
        self.config.OUTPUT_PATH.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.config.OUTPUT_PATH / 'gdpr_metamodel_system.log'),
                logging.StreamHandler()
            ]
        )
    
    async def run_metamodel_pipeline(self) -> Dict[str, Any]:
        """Run the complete metamodel generation pipeline"""
        start_time = datetime.now()
        session_id = f"gdpr_metamodel_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ðŸš€ Starting GDPR Metamodel Pipeline - Session: {session_id}")
        logging.info(f"ðŸš€ Starting GDPR Metamodel Pipeline - Session: {session_id}")
        
        try:
            # Step 1: Process documents
            print("\n" + "="*60)
            print("ðŸ“„ STEP 1: Processing PDF documents...")
            print("="*60)
            logging.info("ðŸ“„ Step 1: Processing PDF documents...")
            documents = await self._process_documents(session_id)
            print(f"âœ… Processed {len(documents)} document chunks")
            
            # Step 2: Extract semantic concepts and articles
            print("\n" + "="*60)
            print("ðŸ§  STEP 2: Extracting semantic concepts and articles...")
            print("="*60)
            logging.info("ðŸ§  Step 2: Extracting semantic concepts and articles...")
            
            print("ðŸ” Extracting concepts...")
            concepts = await self.concept_extractor.extract_semantic_concepts(documents)
            print(f"âœ… Extracted {len(concepts)} concepts")
            
            print("ðŸ“‹ Extracting articles...")
            articles = await self.concept_extractor.extract_gdpr_articles(documents)
            print(f"âœ… Extracted {len(articles)} articles")
            
            # Step 3: Create knowledge graph
            print("\n" + "="*60)
            print("ðŸ•¸ï¸ STEP 3: Creating semantic knowledge graph...")
            print("="*60)
            logging.info("ðŸ•¸ï¸ Step 3: Creating semantic knowledge graph...")
            
            print("ðŸ—ï¸ Creating schema...")
            await self.falkor_manager.create_semantic_schema()
            
            print("ðŸ›ï¸ Creating metamodel structure...")
            await self.falkor_manager.create_metamodel_structure()
            
            print(f"ðŸ’¾ Storing {len(concepts)} concepts in batches of {self.config.FALKORDB_BATCH_SIZE}...")
            concept_count = await self.falkor_manager.store_semantic_concepts(
                concepts, batch_size=self.config.FALKORDB_BATCH_SIZE
            )
            print(f"âœ… Stored {concept_count} concepts")
            
            print(f"ðŸ“š Storing {len(articles)} articles in batches of {self.config.CONCEPT_BATCH_SIZE}...")
            article_count = await self.falkor_manager.store_gdpr_articles(
                articles, batch_size=self.config.CONCEPT_BATCH_SIZE
            )
            print(f"âœ… Stored {article_count} articles")
            
            # Step 4: Create vector index with streaming
            print("\n" + "="*60)
            print("ðŸ” STEP 4: Creating vector embeddings with streaming...")
            print("="*60)
            logging.info("ðŸ” Step 4: Creating vector embeddings...")
            print(f"ðŸ“Š Processing {len(documents)} documents in batches of {self.config.DOCUMENT_BATCH_SIZE}")
            print(f"ðŸ”® Embedding generation batches: {self.config.EMBEDDING_BATCH_SIZE}")
            print(f"ðŸ“¥ Elasticsearch storage batches: {self.config.ELASTICSEARCH_BATCH_SIZE}")
            
            await self._create_vector_index(documents, concepts, articles, session_id)
            
            # Step 5: Generate metamodel
            print("\n" + "="*60)
            print("ðŸ—ï¸ STEP 5: Generating GDPR metamodel...")
            print("="*60)
            logging.info("ðŸ—ï¸ Step 5: Generating GDPR metamodel...")
            kg_stats = await self.falkor_manager.get_metamodel_statistics()
            print(f"ðŸ“Š Knowledge graph statistics: {kg_stats}")
            metamodel = await self.metamodel_generator.generate_metamodel(concepts, articles, kg_stats)
            print(f"âœ… Generated metamodel with {len(metamodel)} sections")
            
            # Step 6: Generate ROPA report
            print("\n" + "="*60)
            print("ðŸ“‹ STEP 6: Generating ROPA compliance report...")
            print("="*60)
            logging.info("ðŸ“‹ Step 6: Generating ROPA compliance report...")
            ropa_report = await self.metamodel_generator.generate_ropa_report(metamodel, concepts, articles)
            print(f"âœ… Generated ROPA report with {len(ropa_report)} sections")
            
            # Step 7: Save results
            print("\n" + "="*60)
            print("ðŸ’¾ STEP 7: Saving results...")
            print("="*60)
            logging.info("ðŸ’¾ Step 7: Saving results...")
            await self._save_results(session_id, metamodel, ropa_report, concepts, articles, kg_stats)
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            results = {
                "status": "success",
                "session_id": session_id,
                "processing_time": processing_time,
                "statistics": {
                    "documents_processed": len(documents),
                    "concepts_extracted": len(concepts),
                    "articles_extracted": len(articles),
                    "knowledge_graph_nodes": kg_stats.get('total_concepts', 0) + kg_stats.get('total_articles', 0),
                    "knowledge_graph_relationships": kg_stats.get('total_relationships', 0),
                    "semantic_relationships": kg_stats.get('semantic_relationships', 0),
                    "ropa_relevant_concepts": kg_stats.get('ropa_relevant_concepts', 0)
                },
                "metamodel": metamodel,
                "ropa_report": ropa_report,
                "knowledge_graph_stats": kg_stats
            }
            
            print("\n" + "="*60)
            print("âœ… GDPR Metamodel Pipeline completed successfully!")
            print("="*60)
            logging.info("âœ… GDPR Metamodel Pipeline completed successfully!")
            self._log_results(results)
            
            return results
            
        except Exception as e:
            error_msg = f"Pipeline execution failed: {e}"
            print(f"\nâŒ {error_msg}")
            logging.error(f"âŒ {error_msg}")
            import traceback
            print(f"ðŸ“‹ Traceback: {traceback.format_exc()}")
            logging.error(f"âŒ Traceback: {traceback.format_exc()}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "session_id": session_id,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
    
    async def _process_documents(self, session_id: str) -> List[Document]:
        """Process all PDF documents"""
        print(f"ðŸ“ Looking for PDF documents in: {self.config.DOCUMENTS_PATH}")
        documents = []
        pdf_files = list(self.config.DOCUMENTS_PATH.glob("*.pdf"))
        print(f"ðŸ“„ Found {len(pdf_files)} PDF files: {[f.name for f in pdf_files]}")
        
        for i, pdf_file in enumerate(pdf_files):
            print(f"\nðŸ“‹ Processing file {i+1}/{len(pdf_files)}: {pdf_file.name}")
            logging.info(f"ðŸ“„ Processing: {pdf_file.name}")
            
            doc_info = await self.pdf_processor.process_with_memory(
                pdf_file, None, session_id
            )
            
            print(f"ðŸ“Š Document info keys: {list(doc_info.keys())}")
            
            if "error" not in doc_info:
                chunks = doc_info.get("chunks", [])
                print(f"âœ… Successfully processed {pdf_file.name}: {len(chunks)} chunks")
                documents.extend(chunks)
                
                # Debug chunk information
                if chunks:
                    first_chunk = chunks[0]
                    print(f"ðŸ” Sample chunk metadata: {first_chunk.metadata}")
                    print(f"ðŸ“ Sample chunk length: {len(first_chunk.page_content)} characters")
            else:
                error_msg = doc_info.get('error', 'Unknown error')
                print(f"âŒ Failed to process {pdf_file.name}: {error_msg}")
                logging.error(f"âŒ Failed to process {pdf_file.name}: {error_msg}")
        
        print(f"\nðŸ Document processing completed!")
        print(f"ðŸ“Š Total documents: {len(pdf_files)}")
        print(f"ðŸ“„ Total chunks: {len(documents)}")
        logging.info(f"âœ… Processed {len(pdf_files)} files, generated {len(documents)} chunks")
        return documents
    
    async def _create_vector_index(self, documents: List[Document], 
                                 concepts: List[SemanticConcept],
                                 articles: List[GDPRArticle],
                                 session_id: str):
        """Create comprehensive vector index with streaming for large documents"""
        print(f"ðŸ” Creating vector index for {len(documents)} documents")
        await self.elasticsearch_manager.create_enhanced_index()
        
        # Process documents in manageable batches
        batch_size = 50  # Smaller batches for embedding generation
        embedding_batch_size = 20  # Even smaller for OpenAI API
        
        print(f"ðŸ“Š Processing in batches of {batch_size} documents")
        print(f"ðŸ”® Generating embeddings in sub-batches of {embedding_batch_size}")
        
        all_indexed_docs = []
        
        # Process documents in batches
        for batch_start in range(0, len(documents), batch_size):
            batch_end = min(batch_start + batch_size, len(documents))
            doc_batch = documents[batch_start:batch_end]
            
            print(f"ðŸ“¦ Processing document batch {batch_start//batch_size + 1}: {batch_start+1}-{batch_end}")
            
            # Prepare documents for indexing with streaming embeddings
            indexed_docs = await self._prepare_documents_with_streaming_embeddings(
                doc_batch, session_id, embedding_batch_size
            )
            
            all_indexed_docs.extend(indexed_docs)
            
            # Store batch in Elasticsearch immediately (streaming)
            if indexed_docs:
                memory_insights = {
                    "insights": [],
                    "concept_evolution": {},
                    "session_context": {"session_id": session_id}
                }
                
                try:
                    result = await self.elasticsearch_manager.store_with_memory(
                        indexed_docs, session_id, memory_insights, batch_size=25
                    )
                    print(f"âœ… Stored batch in Elasticsearch: {result.get('success', 0)} docs")
                except Exception as e:
                    print(f"âŒ Error storing batch in Elasticsearch: {e}")
                    logging.error(f"âŒ Error storing document batch: {e}")
            
            # Add delay between batches to manage API rate limits
            await asyncio.sleep(0.2)
        
        print(f"ðŸ Vector indexing completed: {len(all_indexed_docs)} documents processed")
        logging.info(f"âœ… Vector index created with streaming: {len(all_indexed_docs)} documents")
    
    async def _prepare_documents_with_streaming_embeddings(self, documents: List[Document], 
                                                         session_id: str, 
                                                         embedding_batch_size: int) -> List[Dict[str, Any]]:
        """Prepare documents with embeddings using streaming approach"""
        print(f"ðŸ”® Generating embeddings for {len(documents)} documents")
        
        indexed_docs = []
        
        # Extract texts for embedding generation
        texts = [doc.page_content for doc in documents]
        
        # Generate embeddings in small batches to respect API limits
        all_embeddings = []
        for emb_batch_start in range(0, len(texts), embedding_batch_size):
            emb_batch_end = min(emb_batch_start + embedding_batch_size, len(texts))
            text_batch = texts[emb_batch_start:emb_batch_end]
            
            print(f"ðŸ”® Generating embeddings for sub-batch: {emb_batch_start+1}-{emb_batch_end}")
            
            try:
                # Generate embeddings for this batch
                batch_embeddings = await self.embeddings.embed_texts(text_batch, batch_size=embedding_batch_size)
                all_embeddings.extend(batch_embeddings)
                
                print(f"âœ… Generated {len(batch_embeddings)} embeddings")
                
                # Rate limiting - small delay between embedding batches
                await asyncio.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ Error generating embeddings for batch: {e}")
                # Add zero vectors as fallback
                fallback_embeddings = [[0.0] * self.config.EMBEDDING_DIMENSIONS] * len(text_batch)
                all_embeddings.extend(fallback_embeddings)
                logging.error(f"âŒ Embedding generation failed, using fallback: {e}")
        
        # Create indexed documents
        for doc, embedding in zip(documents, all_embeddings):
            # Enhanced document with memory features
            indexed_doc = {
                "document_id": f"doc_{uuid.uuid4()}",
                "content": doc.page_content,
                "embeddings": embedding,
                
                # Memory features
                "session_id": session_id,
                "has_previous_analysis": doc.metadata.get("has_previous_analysis", False),
                
                # Standard document fields
                "document_type": doc.metadata.get("document_type", "unknown"),
                "source_file": doc.metadata.get("source", ""),
                "chunk_index": doc.metadata.get("chunk_index", 0),
                "extracted_at": datetime.now().isoformat(),
                "confidence_score": 0.8
            }
            
            indexed_docs.append(indexed_doc)
        
        print(f"âœ… Prepared {len(indexed_docs)} documents with embeddings")
        return indexed_docs
    
    async def _save_results(self, session_id: str, metamodel: Dict[str, Any], 
                          ropa_report: Dict[str, Any], concepts: List[SemanticConcept],
                          articles: List[GDPRArticle], kg_stats: Dict[str, Any]):
        """Save all results to files"""
        
        # Save metamodel
        metamodel_file = self.config.OUTPUT_PATH / f"{session_id}_metamodel.json"
        async with aiofiles.open(metamodel_file, 'w') as f:
            await f.write(json.dumps(metamodel, indent=2))
        
        # Save ROPA report
        ropa_file = self.config.OUTPUT_PATH / f"{session_id}_ropa_report.json"
        async with aiofiles.open(ropa_file, 'w') as f:
            await f.write(json.dumps(ropa_report, indent=2))
        
        # Save concepts
        concepts_file = self.config.OUTPUT_PATH / f"{session_id}_concepts.json"
        concepts_data = [asdict(c) for c in concepts]
        # Convert datetime objects to strings
        for concept in concepts_data:
            concept['last_updated'] = concept['last_updated'].isoformat()
        async with aiofiles.open(concepts_file, 'w') as f:
            await f.write(json.dumps(concepts_data, indent=2))
        
        # Save articles
        articles_file = self.config.OUTPUT_PATH / f"{session_id}_articles.json"
        articles_data = [asdict(a) for a in articles]
        # Convert datetime objects to strings
        for article in articles_data:
            article['last_updated'] = article['last_updated'].isoformat()
        async with aiofiles.open(articles_file, 'w') as f:
            await f.write(json.dumps(articles_data, indent=2))
        
        # Save knowledge graph statistics
        stats_file = self.config.OUTPUT_PATH / f"{session_id}_kg_stats.json"
        async with aiofiles.open(stats_file, 'w') as f:
            await f.write(json.dumps(kg_stats, indent=2))
        
        logging.info(f"ðŸ’¾ Saved all results to {self.config.OUTPUT_PATH}")
    
    def _log_results(self, results: Dict[str, Any]):
        """Log comprehensive results"""
        logging.info(f"ðŸ“Š GDPR Metamodel Results:")
        logging.info(f"  - Session ID: {results['session_id']}")
        logging.info(f"  - Processing Time: {results['processing_time']:.2f} seconds")
        logging.info(f"  - Documents Processed: {results['statistics']['documents_processed']}")
        logging.info(f"  - Concepts Extracted: {results['statistics']['concepts_extracted']}")
        logging.info(f"  - Articles Extracted: {results['statistics']['articles_extracted']}")
        logging.info(f"  - Knowledge Graph Nodes: {results['statistics']['knowledge_graph_nodes']}")
        logging.info(f"  - Knowledge Graph Relationships: {results['statistics']['knowledge_graph_relationships']}")
        logging.info(f"  - Semantic Relationships: {results['statistics']['semantic_relationships']}")
        logging.info(f"  - ROPA Relevant Concepts: {results['statistics']['ropa_relevant_concepts']}")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    """Main execution function"""
    try:
        # Load configuration
        config = Config()
        
        # Ensure directories exist
        config.DOCUMENTS_PATH.mkdir(exist_ok=True)
        config.OUTPUT_PATH.mkdir(exist_ok=True)
        config.MEMORY_PATH.mkdir(exist_ok=True)
        
        print("ðŸš€ GDPR Metamodel Knowledge System")
        print("=" * 60)
        print(f"ðŸ“ Documents Path: {config.DOCUMENTS_PATH}")
        print(f"ðŸ“‚ Output Path: {config.OUTPUT_PATH}")
        print(f"ðŸ¤– Reasoning Model: {config.REASONING_MODEL}")
        print(f"âš¡ Reasoning Effort: {config.REASONING_EFFORT}")
        print(f"ðŸ”® Embedding Model: {config.EMBEDDING_MODEL}")
        print(f"ðŸ“Š Embedding Dimensions: {config.EMBEDDING_DIMENSIONS}")
        print(f"ðŸ—„ï¸ FalkorDB Graph: {config.FALKORDB_GRAPH_NAME}")
        print(f"ðŸ” Elasticsearch Index: {config.ELASTICSEARCH_INDEX}")
        
        print(f"\nðŸ“Š Streaming Configuration (for Large Documents):")
        print(f"  - Document Processing Batches: {config.DOCUMENT_BATCH_SIZE}")
        print(f"  - Embedding Generation Batches: {config.EMBEDDING_BATCH_SIZE}")
        print(f"  - Elasticsearch Storage Batches: {config.ELASTICSEARCH_BATCH_SIZE}")
        print(f"  - FalkorDB Storage Batches: {config.FALKORDB_BATCH_SIZE}")
        print(f"  - Concept Processing Batches: {config.CONCEPT_BATCH_SIZE}")
        
        print(f"\nðŸ’¡ For Large Document Sets:")
        print(f"  - Set DOCUMENT_BATCH_SIZE=25 for very large documents")
        print(f"  - Set EMBEDDING_BATCH_SIZE=10 for rate limit management")
        print(f"  - Set ELASTICSEARCH_BATCH_SIZE=50 for memory efficiency")
        print(f"  - Monitor system memory and adjust batch sizes accordingly")
        
        # Create orchestrator and run pipeline
        orchestrator = GDPRMetamodelOrchestrator(config)
        results = await orchestrator.run_metamodel_pipeline()
        
        # Display results
        print("\n" + "=" * 60)
        print("ðŸ“Š METAMODEL GENERATION RESULTS")
        print("=" * 60)
        print(f"Status: {results['status']}")
        print(f"Session ID: {results['session_id']}")
        print(f"Processing Time: {results['processing_time']:.2f} seconds")
        
        if results['status'] == 'success':
            print(f"\nðŸ“ˆ Processing Statistics:")
            for key, value in results['statistics'].items():
                print(f"  - {key.replace('_', ' ').title()}: {value}")
            
            print(f"\nðŸ—ï¸ Metamodel Coverage:")
            metamodel = results['metamodel']
            if 'computed_metrics' in metamodel:
                metrics = metamodel['computed_metrics']
                print(f"  - Concept Coverage: {metrics.get('concept_coverage', 0)}")
                print(f"  - Article Coverage: {metrics.get('article_coverage', 0)}")
                print(f"  - Jurisdiction Coverage: {metrics.get('jurisdiction_coverage', 0)}")
                print(f"  - ROPA Coverage: {metrics.get('ropa_coverage_percentage', 0):.1f}%")
                print(f"  - Financial Relevance: {metrics.get('financial_relevance_score', 0):.1f}%")
            
            print(f"\nðŸ“‹ ROPA Report:")
            ropa_report = results['ropa_report']
            if 'implementation_metrics' in ropa_report:
                metrics = ropa_report['implementation_metrics']
                print(f"  - Total ROPA Concepts: {metrics.get('total_ropa_concepts', 0)}")
                print(f"  - GDPR Coverage: {metrics.get('coverage_by_regulation', {}).get('GDPR', 0)}")
                print(f"  - UK GDPR Coverage: {metrics.get('coverage_by_regulation', {}).get('UK_GDPR', 0)}")
                print(f"  - Compliance Complexity Score: {metrics.get('compliance_complexity_score', 0)}")
            
            print(f"\nðŸ•¸ï¸ Knowledge Graph:")
            kg_stats = results['knowledge_graph_stats']
            print(f"  - Total Concepts: {kg_stats.get('total_concepts', 0)}")
            print(f"  - Total Articles: {kg_stats.get('total_articles', 0)}")
            print(f"  - Total Relationships: {kg_stats.get('total_relationships', 0)}")
            print(f"  - Semantic Relationships: {kg_stats.get('semantic_relationships', 0)}")
            
            print(f"\nðŸ’¾ Files Generated:")
            print(f"  - {results['session_id']}_metamodel.json")
            print(f"  - {results['session_id']}_ropa_report.json")
            print(f"  - {results['session_id']}_concepts.json")
            print(f"  - {results['session_id']}_articles.json")
            print(f"  - {results['session_id']}_kg_stats.json")
            
            print(f"\nðŸ”§ Performance Notes:")
            total_time = results['processing_time']
            docs_processed = results['statistics']['documents_processed']
            if docs_processed > 0:
                print(f"  - Average time per document: {total_time/docs_processed:.2f} seconds")
            print(f"  - Peak memory usage optimized through streaming")
            print(f"  - Batch processing prevented database overload")
            
        else:
            print(f"âŒ Error: {results.get('error', 'Unknown error')}")
        
        print(f"\nðŸŽ‰ GDPR Metamodel System ready for financial services compliance!")
        print(f"ðŸ’¡ System uses streaming processing for efficient large document handling")
        
        return results
        
    except Exception as e:
        print(f"âŒ System failed: {e}")
        logging.error(f"System failure: {e}")
        import traceback
        print(f"ðŸ“‹ Full traceback: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
