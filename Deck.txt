#!/usr/bin/env python3
"""
Complete Optimized Async Triple-Based RDF to FalkorDB Property Graph Converter

This module converts RDF data to FalkorDB property graphs with optimized relationship creation
for handling large datasets efficiently.

Key Optimizations:
- Ultra-fast bulk relationship creation using UNWIND
- Strategic indexing for optimal lookup performance
- Grouped batch processing by relationship type
- Parallel processing with controlled concurrency
- Memory-efficient streaming approach

Dependencies:
    pip install rdflib falkordb redis asyncio
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

# Async imports
import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class OptimizedAsyncTripleConfig:
    """Optimized configuration for async triple-based RDF conversion with fast edge loading"""
    # The main SPARQL query with 6 variables
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Graph management settings
    append_to_existing_graph: bool = True      # Append to existing graph instead of clearing
    clear_existing_graph: bool = False         # Set to True to clear graph before loading
    handle_duplicates: bool = True             # Use MERGE instead of CREATE to handle duplicates
    skip_existing_nodes: bool = True           # Skip nodes that already exist
    skip_existing_relationships: bool = True   # Skip relationships that already exist
    
    # Optimized async processing settings for large datasets
    batch_size: int = 2000              # Larger batches for relationships
    max_concurrent_batches: int = 3     # Controlled concurrency
    connection_pool_size: int = 10      # Moderate pool size for stability
    sparql_timeout: int = 7200          # 2 hours for very large queries
    falkordb_timeout: Optional[int] = None  # Infinite timeout for FalkorDB operations
    max_retries: int = 5                # More retries for large operations
    retry_delay: int = 3                # Delay between retries
    
    # Relationship optimization settings
    preserve_uri_properties: bool = False   # Disable for maximum speed
    disable_relationship_properties: bool = True  # Skip rel properties for speed
    group_relationships_by_type: bool = True      # Group for batch efficiency
    use_bulk_relationship_creation: bool = True   # Use UNWIND for bulk operations
    
    # Performance settings
    use_shortened_uris: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    # Memory and performance optimizations
    exclude_rdf_type_properties: bool = True
    validate_conversion: bool = False      # Skip validation for speed
    export_stats: bool = True
    progress_update_interval: int = 50     # Log progress every N batches

@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process with incremental loading support"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    # Batch processing stats
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    # Graph state tracking (for incremental loading)
    initial_nodes: int = 0              # Nodes in graph before this run
    initial_relationships: int = 0      # Relationships in graph before this run
    
    # Created entities (new in this run)
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0              # New nodes created
    created_relationships: int = 0      # New relationships created
    skipped_nodes: int = 0              # Existing nodes skipped
    skipped_relationships: int = 0      # Existing relationships skipped
    
    # Final totals (after this run)
    final_nodes: int = 0
    final_relationships: int = 0
    
    # Performance metrics
    relationship_creation_time: float = 0.0
    node_creation_time: float = 0.0
    relationship_creation_rate: float = 0.0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    relationship_types_count: Dict[str, int] = None
    
    # Incremental loading info
    append_mode: bool = False
    graph_was_cleared: bool = False
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
        if self.relationship_types_count is None:
            self.relationship_types_count = {}
    
    def get_incremental_summary(self) -> str:
        """Get a summary of incremental changes"""
        if self.append_mode:
            return (f"Added {self.created_nodes:,} nodes and {self.created_relationships:,} relationships. "
                   f"Graph now has {self.final_nodes:,} nodes and {self.final_relationships:,} relationships total.")
        else:
            return (f"Created {self.created_nodes:,} nodes and {self.created_relationships:,} relationships "
                   f"in new graph.")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        # Convert sets to lists for JSON serialization
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        result['incremental_summary'] = self.get_incremental_summary()
        
        return result

class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        # Check cache first
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        # Try namespace mapping
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        # Fallback: extract from URI structure
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)

class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: OptimizedAsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry"""
        labels = {self.config.default_node_label}
        
        # Add class-based label if available
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            labels.add(class_label)
        
        # Handle blank nodes
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'properties': {}
        }
        
        # Add URI properties if requested
        if self.config.preserve_uri_properties:
            node_data['properties']['uri'] = uri
        else:
            # Always add URI for lookup purposes
            node_data['properties']['uri'] = uri
        
        # Add processed identifier for easier querying
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            # Skip rdf:type if configured to do so
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            # Handle multiple values for the same property
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()

class OptimizedAsyncFalkorDBManager:
    """Optimized FalkorDB manager with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB and configure for optimal performance"""
        try:
            # Create Redis connection pool with optimized settings
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'timeout': self.config.falkordb_timeout,
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
                'socket_timeout': None,
                'socket_connect_timeout': 30,
            }
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            # Test the connection
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            # Configure FalkorDB for optimal performance
            await self._configure_for_optimal_performance()
            
            logger.info(f"Optimized FalkorDB connection established")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    async def _configure_for_optimal_performance(self):
        """Configure FalkorDB runtime settings for optimal performance"""
        try:
            configurations = [
                # Remove all timeout restrictions
                ("TIMEOUT_MAX", "0"),
                ("TIMEOUT_DEFAULT", "0"),
                
                # Optimize memory and performance
                ("QUERY_MEM_CAPACITY", str(8 * 1024 * 1024 * 1024)),  # 8GB per query
                ("VKEY_MAX_ENTITY_COUNT", "500000"),  # Larger virtual key size
                ("NODE_CREATION_BUFFER", "65536"),    # Much larger node buffer
                
                # Disable expensive operations during ingestion
                ("CMD_INFO", "no"),
            ]
            
            for param, value in configurations:
                try:
                    await self.graph.query(f"GRAPH.CONFIG SET {param} {value}")
                    logger.info(f"Set FalkorDB config: {param} = {value}")
                except Exception as e:
                    logger.warning(f"Could not set {param}: {e}")
            
            # Additional Redis configurations
            redis_client = redis_async.Redis(connection_pool=self.pool)
            try:
                await redis_client.config_set('timeout', 0)
                await redis_client.config_set('tcp-keepalive', 300)
                await redis_client.config_set('maxmemory-policy', 'allkeys-lru')
                logger.info("Applied Redis optimizations")
            except Exception as e:
                logger.warning(f"Could not apply Redis configurations: {e}")
            finally:
                await redis_client.aclose()
                
        except Exception as e:
            logger.error(f"Error configuring FalkorDB: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data (only if explicitly requested)"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("🗑️  Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def get_initial_graph_stats(self) -> Dict[str, int]:
        """Get initial graph statistics before processing"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            if node_count > 0 or rel_count > 0:
                logger.info(f"📊 Existing graph: {node_count:,} nodes, {rel_count:,} relationships")
            else:
                logger.info("📊 Starting with empty graph")
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting initial statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with async retry logic and timeout override"""
        # Add TIMEOUT 0 for large operations if not already present
        if "TIMEOUT" not in query.upper() and not query.strip().startswith("GRAPH.CONFIG"):
            if any(keyword in query.upper() for keyword in ['CREATE', 'MATCH', 'MERGE', 'DELETE', 'UNWIND']):
                query = f"{query} TIMEOUT 0"
        
        for attempt in range(self.config.max_retries):
            try:
                return await self.graph.query(query, params or {})
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """Create a batch of nodes using MERGE for duplicate handling or CREATE for new graphs"""
        if not nodes_batch:
            return 0
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        try:
            # Prepare data for UNWIND
            nodes_data = []
            for uri, node_data in nodes_batch:
                labels = list(node_data['labels'])
                properties = dict(node_data['properties'])
                
                nodes_data.append({
                    'uri': uri,
                    'labels': labels,
                    'properties': properties
                })
            
            if use_merge:
                # Use MERGE to handle existing nodes
                query = """
                UNWIND $nodes_data AS node_data
                MERGE (n {uri: node_data.properties.uri})
                ON CREATE SET n += node_data.properties
                ON MATCH SET n += node_data.properties
                RETURN count(n) as total_processed
                """
            else:
                # Use CREATE for new graphs (faster)
                query = """
                UNWIND $nodes_data AS node_data
                CREATE (n)
                SET n += node_data.properties
                RETURN count(n) as total_created
                """
            
            try:
                result = await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
                return result.result_set[0][0] if result.result_set else len(nodes_batch)
            except:
                # Fallback to simpler approach if APOC is not available
                return await self._create_nodes_simple_batch(nodes_batch, use_merge)
                
        except Exception as e:
            logger.error(f"Batch node creation failed: {e}")
            return 0
    
    async def _create_nodes_simple_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]], use_merge: bool = False) -> int:
        """Fallback: Create nodes using simple batch approach with optional MERGE"""
        try:
            nodes_data = []
            for uri, node_data in nodes_batch:
                labels_str = ':'.join(sorted(set(node_data['labels'])))
                properties = dict(node_data['properties'])
                
                nodes_data.append({
                    'uri': uri,
                    'labels': labels_str,
                    'properties': properties
                })
            
            if use_merge:
                # MERGE approach for existing graphs
                query = f"""
                UNWIND $nodes_data AS node_data
                MERGE (n:{self.config.default_node_label} {{uri: node_data.properties.uri}})
                ON CREATE SET n += node_data.properties
                ON MATCH SET n += node_data.properties
                """
            else:
                # CREATE approach for new graphs
                query = f"""
                UNWIND $nodes_data AS node_data
                CREATE (n:{self.config.default_node_label})
                SET n += node_data.properties
                """
            
            await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
            return len(nodes_batch)
            
        except Exception as e:
            logger.error(f"Simple batch node creation failed: {e}")
            return 0
    
    async def create_relationships_ultra_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """Ultra-fast relationship creation using optimized bulk operations"""
        if not relationships_by_type:
            return 0
        
        total_relationships = sum(len(rels) for rels in relationships_by_type.values())
        logger.info(f"Creating {total_relationships} relationships using ultra-fast bulk approach...")
        
        start_time = time.time()
        total_created = 0
        
        # Process each relationship type in parallel with controlled concurrency
        semaphore = asyncio.Semaphore(3)  # Limit concurrent operations
        
        async def create_relationships_for_type(rel_type: str, relationships: List[Tuple[str, str, str]]):
            async with semaphore:
                return await self._create_relationships_bulk_by_type(rel_type, relationships)
        
        # Create tasks for all relationship types
        tasks = []
        for rel_type, relationships in relationships_by_type.items():
            task = asyncio.create_task(create_relationships_for_type(rel_type, relationships))
            tasks.append(task)
        
        # Execute all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        for i, result in enumerate(results):
            if isinstance(result, int):
                total_created += result
            else:
                rel_type = list(relationships_by_type.keys())[i]
                logger.error(f"Failed to create relationships for type {rel_type}: {result}")
        
        duration = time.time() - start_time
        rate = total_created / duration if duration > 0 else 0
        
        logger.info(f"Ultra-fast relationship creation completed:")
        logger.info(f"  Created: {total_created:,} relationships")
        logger.info(f"  Time: {duration:.2f} seconds")
        logger.info(f"  Rate: {rate:.1f} relationships/second")
        
        return total_created
    
    async def _create_relationships_bulk_by_type(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Create all relationships of a specific type using optimized batching"""
        if not relationships:
            return 0
        
        logger.info(f"Creating {len(relationships):,} relationships of type '{rel_type}'")
        
        # Adaptive batch sizing
        if len(relationships) > 10000:
            batch_size = 5000
        elif len(relationships) > 1000:
            batch_size = 2000
        else:
            batch_size = len(relationships)  # Process all at once for small sets
        
        total_created = 0
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            
            try:
                created = await self._execute_relationship_batch_query(rel_type, batch)
                total_created += created
                
                # Progress logging for large relationship types
                if len(relationships) > 1000 and (i // batch_size) % 25 == 0:
                    progress = (i + len(batch)) / len(relationships) * 100
                    logger.info(f"  Progress {rel_type}: {progress:.1f}% ({i + len(batch):,}/{len(relationships):,})")
                    
            except Exception as e:
                logger.error(f"Failed to create relationship batch for {rel_type}: {e}")
                # Try smaller batches
                created = await self._create_relationships_smaller_batches(rel_type, batch)
                total_created += created
        
        logger.info(f"Completed {rel_type}: {total_created:,} relationships created")
        return total_created
    
    async def _execute_relationship_batch_query(self, rel_type: str, batch: List[Tuple[str, str, str]]) -> int:
        """Execute optimized batch relationship creation query with duplicate handling"""
        batch_data = []
        for subject_uri, predicate_uri, object_uri in batch:
            rel_data = {
                'subject_uri': subject_uri,
                'object_uri': object_uri
            }
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                rel_data['predicate_uri'] = predicate_uri
            batch_data.append(rel_data)
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        if use_merge:
            # Use MERGE to handle existing relationships
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[r:{rel_type}]->(o)
                ON CREATE SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[:{rel_type}]->(o)
                """
        else:
            # Use CREATE for new graphs (faster)
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{rel_type} {{predicate_uri: rel.predicate_uri}}]->(o)
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{rel_type}]->(o)
                """
        
        await self.execute_query_with_retry(query, {'batch_data': batch_data})
        return len(batch)
    
    async def _create_relationships_smaller_batches(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Fallback: create relationships in much smaller batches"""
        batch_size = 100  # Very small batches for problematic data
        total_created = 0
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            try:
                created = await self._execute_relationship_batch_query(rel_type, batch)
                total_created += created
            except Exception as e:
                logger.warning(f"Small batch failed for {rel_type}: {e}")
                # Try individual creation as last resort
                for subject_uri, predicate_uri, object_uri in batch:
                    try:
                        query = f"""
                        MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
                        CREATE (s)-[:{rel_type}]->(o)
                        """
                        await self.execute_query_with_retry(query, {
                            'subject_uri': subject_uri,
                            'object_uri': object_uri
                        })
                        total_created += 1
                    except:
                        continue  # Skip problematic relationships
        
        return total_created
    
    async def create_indexes(self):
        """Create comprehensive indexes for optimal performance"""
        if not self.config.create_indexes:
            return
        
        index_queries = [
            # Primary indexes
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.uri)",
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.id)",
            
            # Node type indexes
            f"CREATE INDEX IF NOT EXISTS FOR (n:{self.config.default_node_label}) ON (n.uri)",
            "CREATE INDEX IF NOT EXISTS FOR (n:BlankNode) ON (n.uri)",
        ]
        
        logger.info("Creating performance indexes...")
        
        for query in index_queries:
            try:
                await self.execute_query_with_retry(query)
                logger.debug(f"Created index: {query}")
            except Exception as e:
                logger.warning(f"Could not create index '{query}': {e}")
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def monitor_memory_usage(self) -> Dict[str, Any]:
        """Monitor FalkorDB memory usage"""
        try:
            redis_client = redis_async.Redis(connection_pool=self.pool)
            info = await redis_client.info('memory')
            await redis_client.aclose()
            
            return {
                'used_memory': info.get('used_memory', 0),
                'used_memory_human': info.get('used_memory_human', '0B'),
                'used_memory_peak': info.get('used_memory_peak', 0),
                'used_memory_peak_human': info.get('used_memory_peak_human', '0B'),
                'memory_fragmentation_ratio': info.get('mem_fragmentation_ratio', 0),
            }
        except Exception as e:
            logger.warning(f"Could not get memory stats: {e}")
            return {}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")

class OptimizedAsyncTripleBasedConverter:
    """Main async converter with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = OptimizedAsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        # Optimized relationship storage
        self.relationships_by_type: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        self.total_relationships = 0
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
        
        self._setup_rdf_connection()
    
    def _setup_rdf_connection(self):
        """Setup RDF graph connection to SPARQL endpoint"""
        try:
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"Connected to SPARQL endpoint: {self.config.sparql_endpoint}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def _execute_sparql_query(self):
        """Execute SPARQL query synchronously"""
        try:
            logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
            query_result = self.rdf_graph.query(self.config.triples_query)
            results = list(query_result)
            logger.info(f"SPARQL query completed, retrieved {len(results)} triples")
            return results
        except Exception as e:
            logger.error(f"SPARQL query execution failed: {e}")
            raise
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method with incremental loading support"""
        try:
            # Set append mode in stats
            self.stats.append_mode = self.config.append_to_existing_graph
            
            if self.config.append_to_existing_graph:
                logger.info("📈 Starting incremental RDF to FalkorDB conversion (append mode)...")
            else:
                logger.info("🚀 Starting fresh RDF to FalkorDB conversion...")
            
            # Connect to FalkorDB
            await self.falkordb_manager.connect()
            
            # Get initial graph statistics
            initial_stats = await self.falkordb_manager.get_initial_graph_stats()
            self.stats.initial_nodes = initial_stats['nodes']
            self.stats.initial_relationships = initial_stats['relationships']
            
            # Handle graph clearing logic
            if self.config.clear_existing_graph:
                logger.info("🗑️  Clearing existing graph as requested...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            elif not self.config.append_to_existing_graph:
                # If not in append mode and not explicitly keeping graph, clear it
                logger.info("🗑️  Clearing graph for fresh conversion...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            
            # Execute the main query and process results
            await self._execute_and_process_query()
            
            # Create nodes first (fast)
            node_start = time.time()
            await self._create_nodes_in_falkordb_async()
            self.stats.node_creation_time = time.time() - node_start
            
            # Create relationships using ultra-fast approach
            rel_start = time.time()
            await self._create_relationships_ultra_fast()
            self.stats.relationship_creation_time = time.time() - rel_start
            
            # Calculate relationship creation rate
            if self.stats.relationship_creation_time > 0:
                self.stats.relationship_creation_rate = self.stats.created_relationships / self.stats.relationship_creation_time
            
            # Create final indexes
            await self.falkordb_manager.create_indexes()
            
            # Validate if requested
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            # Finalize statistics
            await self._finalize_stats()
            
            # Log performance summary
            await self._log_performance_summary()
            
            if self.config.append_to_existing_graph:
                logger.info("✅ Incremental conversion completed successfully!")
                logger.info(f"📊 {self.stats.get_incremental_summary()}")
            else:
                logger.info("✅ Fresh conversion completed successfully!")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            raise
        finally:
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("📊 Executing triples query...")
            
            # Execute query in executor to avoid blocking event loop
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(None, self._execute_sparql_query)
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"📈 Retrieved {len(results):,} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("⚡ Processing triples in optimized async batches...")
            
            # Process in async batches
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with concurrency control"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"🔄 Processing {len(results):,} triples in {total_batches} batches of {batch_size}")
        
        # Create batches
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        # Process batches concurrently
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        # Process results as they complete
        completed = 0
        failed = 0
        
        for future in asyncio.as_completed(tasks):
            try:
                await future
                completed += 1
                self.stats.completed_batches = completed
                
                if completed % self.config.progress_update_interval == 0:
                    progress = (completed / total_batches) * 100
                    logger.info(f"📊 Batch progress: {progress:.1f}% ({completed}/{total_batches})")
            except Exception as e:
                failed += 1
                self.stats.failed_batches = failed
                logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"✅ Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously with relationship optimization"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        # Track metadata
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        # Ensure subject node exists
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        if isinstance(obj, Literal):
            # Object is literal -> add as property
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            # Object is URI/BNode -> prepare relationship for optimized batch creation
            object_uri = str(obj)
            
            # Track object class
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            # Ensure object node exists
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            # Group relationships by type for ultra-fast batch processing
            rel_type = self._get_relationship_type(predicate_uri)
            self.relationships_by_type[rel_type].append((subject_uri, predicate_uri, object_uri))
            self.total_relationships += 1
            self.stats.relationship_triples += 1
            
            # Track relationship type counts
            self.stats.relationship_types_count[rel_type] = self.stats.relationship_types_count.get(rel_type, 0) + 1
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _get_relationship_type(self, predicate_uri: str) -> str:
        """Extract and clean relationship type from predicate URI"""
        parsed = urlparse(predicate_uri)
        if parsed.fragment:
            rel_type = parsed.fragment
        else:
            rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
        
        # Clean relationship type for Cypher compatibility
        rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        if not rel_type or rel_type.isdigit():
            rel_type = 'RELATED_TO'
        elif rel_type[0].isdigit():
            rel_type = f"_{rel_type}"
        
        return rel_type
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB using optimized async batching"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"🏗️  Creating {len(nodes):,} nodes in FalkorDB...")
        
        if not nodes:
            logger.warning("No nodes to create")
            return
        
        # Convert to list for batching
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        # Create node batches
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        # Process node batches concurrently
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_created = 0
        for result in results:
            if isinstance(result, int):
                total_created += result
            else:
                logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"✅ Successfully created {total_created:,} nodes")
        self.stats.created_nodes = total_created
    
    async def _create_relationships_ultra_fast(self):
        """Create all relationships using ultra-fast optimized approach"""
        if not self.relationships_by_type:
            logger.info("No relationships to create")
            return
        
        logger.info(f"⚡ Creating {self.total_relationships:,} relationships using ultra-fast approach...")
        
        # Log relationship type distribution
        logger.info("📊 Relationship type distribution:")
        for rel_type, count in sorted(self.stats.relationship_types_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  {rel_type}: {count:,} relationships")
        
        # Use the optimized FalkorDB manager method
        created_count = await self.falkordb_manager.create_relationships_ultra_fast(self.relationships_by_type)
        self.stats.created_relationships = created_count
        
        logger.info(f"✅ Successfully created {created_count:,} relationships")
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("🔍 Validating conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("⚠️  No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("⚠️  No relationships were created despite processing relationship triples")
        
        logger.info(f"✅ Validation complete: {falkor_stats['nodes']:,} nodes, {falkor_stats['relationships']:,} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics with incremental loading support"""
        self.stats.end_time = datetime.now()
        
        # Get final counts from FalkorDB
        final_falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.final_nodes = final_falkor_stats['nodes']
        self.stats.final_relationships = final_falkor_stats['relationships']
        
        # Calculate actual nodes and relationships created in this run
        if self.config.append_to_existing_graph and not self.stats.graph_was_cleared:
            # In append mode, calculate the difference
            actual_nodes_created = self.stats.final_nodes - self.stats.initial_nodes
            actual_rels_created = self.stats.final_relationships - self.stats.initial_relationships
            
            # Update stats with actual created counts (may be less than processed due to duplicates)
            if actual_nodes_created >= 0:
                self.stats.created_nodes = actual_nodes_created
            if actual_rels_created >= 0:
                self.stats.created_relationships = actual_rels_created
                
            # Calculate skipped items (duplicates)
            nodes_processed = len(await self.node_manager.get_nodes())
            self.stats.skipped_nodes = max(0, nodes_processed - actual_nodes_created)
            self.stats.skipped_relationships = max(0, self.total_relationships - actual_rels_created)
        else:
            # In fresh mode, use the final counts as created counts
            self.stats.created_nodes = self.stats.final_nodes
            self.stats.created_relationships = self.stats.final_relationships
        
        # Calculate unique entities
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        # Count unique objects from relationships
        unique_objects = set()
        for rel_list in self.relationships_by_type.values():
            for _, _, obj_uri in rel_list:
                unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            await self._export_stats()
    
    async def _log_performance_summary(self):
        """Log comprehensive performance summary with incremental loading info"""
        if not self.stats.end_time:
            return
        
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        if self.stats.append_mode:
            logger.info("🎯 INCREMENTAL LOADING SUMMARY")
        else:
            logger.info("🎯 PERFORMANCE SUMMARY")
        logger.info("=" * 80)
        
        # Graph state information
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            logger.info(f"📊 Graph State:")
            logger.info(f"   Initial: {self.stats.initial_nodes:,} nodes, {self.stats.initial_relationships:,} relationships")
            logger.info(f"   Final: {self.stats.final_nodes:,} nodes, {self.stats.final_relationships:,} relationships")
            logger.info(f"   Added: {self.stats.created_nodes:,} nodes, {self.stats.created_relationships:,} relationships")
            if self.stats.skipped_nodes > 0 or self.stats.skipped_relationships > 0:
                logger.info(f"   Skipped (duplicates): {self.stats.skipped_nodes:,} nodes, {self.stats.skipped_relationships:,} relationships")
            logger.info(f"")
        
        logger.info(f"📊 Data Processed:")
        logger.info(f"   Total triples: {self.stats.total_triples_retrieved:,}")
        logger.info(f"   Property triples: {self.stats.property_triples:,}")
        logger.info(f"   Relationship triples: {self.stats.relationship_triples:,}")
        logger.info(f"")
        
        if not self.stats.append_mode or self.stats.graph_was_cleared:
            logger.info(f"🏗️  Entities Created:")
            logger.info(f"   Nodes: {self.stats.created_nodes:,}")
            logger.info(f"   Relationships: {self.stats.created_relationships:,}")
        else:
            logger.info(f"🏗️  Entities Added:")
            logger.info(f"   New nodes: {self.stats.created_nodes:,}")
            logger.info(f"   New relationships: {self.stats.created_relationships:,}")
            
        logger.info(f"   Relationship types: {len(self.stats.relationship_types_count)}")
        logger.info(f"")
        logger.info(f"⏱️  Performance Metrics:")
        logger.info(f"   Total time: {duration:.2f} seconds")
        logger.info(f"   Query execution: {self.stats.query_execution_time:.2f} seconds")
        logger.info(f"   Node creation: {self.stats.node_creation_time:.2f} seconds")
        logger.info(f"   Relationship creation: {self.stats.relationship_creation_time:.2f} seconds")
        logger.info(f"")
        logger.info(f"🚀 Throughput Rates:")
        logger.info(f"   Overall: {self.stats.processed_triples/duration:.1f} triples/second")
        if self.stats.relationship_creation_rate > 0:
            logger.info(f"   Relationships: {self.stats.relationship_creation_rate:.1f} relationships/second")
        logger.info(f"")
        
        # Performance assessment
        if self.stats.relationship_creation_rate > 1000:
            logger.info("🎉 EXCELLENT: Ultra-fast relationship creation (>1000 rel/sec)")
        elif self.stats.relationship_creation_rate > 500:
            logger.info("✅ GOOD: Fast relationship creation (>500 rel/sec)")
        elif self.stats.relationship_creation_rate > 100:
            logger.info("⚠️  ACCEPTABLE: Moderate relationship creation (>100 rel/sec)")
        else:
            logger.info("❌ SLOW: Consider optimizing configuration (<100 rel/sec)")
        
        # Incremental loading efficiency
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            total_processed = self.stats.unique_subjects + len(set(obj for rels in self.relationships_by_type.values() for _, _, obj in rels))
            actual_added = self.stats.created_nodes + self.stats.created_relationships
            if total_processed > 0:
                efficiency = (actual_added / total_processed) * 100
                logger.info(f"📈 Incremental efficiency: {efficiency:.1f}% new data added")
                
                if efficiency > 80:
                    logger.info("🎉 HIGH efficiency: Mostly new data")
                elif efficiency > 50:
                    logger.info("✅ GOOD efficiency: Good mix of new data")
                elif efficiency > 20:
                    logger.info("⚠️  MODERATE efficiency: Some duplicate data")
                else:
                    logger.info("❌ LOW efficiency: Mostly duplicate data")
        
        logger.info("=" * 80)
    
    async def _export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"optimized_conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['config'] = {
                'batch_size': self.config.batch_size,
                'max_concurrent_batches': self.config.max_concurrent_batches,
                'connection_pool_size': self.config.connection_pool_size,
                'use_bulk_relationship_creation': self.config.use_bulk_relationship_creation,
                'disable_relationship_properties': self.config.disable_relationship_properties,
            }
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"📄 Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted graph"""
        return [
            "MATCH (n) RETURN labels(n) as node_labels, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN s.id, type(r), o.id LIMIT 10",
            "MATCH (n) WHERE size((n)--()) > 0 RETURN n.id, size((n)--()) as degree ORDER BY degree DESC LIMIT 10",
            "CALL db.schema.visualization()",
            "MATCH (n) RETURN count(n) as total_nodes",
            "MATCH ()-[r]->() RETURN count(r) as total_relationships"
        ]

# Utility functions
def create_optimized_config(sparql_endpoint: str, graph_name: str = "optimized_graph", 
                          append_mode: bool = True) -> OptimizedAsyncTripleConfig:
    """Create an optimized configuration for fast edge loading with incremental support"""
    
    # Sample optimized query
    optimized_query = """
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    PREFIX foaf: <http://xmlns.com/foaf/0.1/>
    
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { 
            ?object a ?objectClass .
            FILTER(isURI(?object))
        }
        
        # Optional: Add filters to control dataset size
        FILTER(?subject != ?object)  # Avoid self-loops
    }
    # Remove LIMIT for full dataset conversion
    """
    
    return OptimizedAsyncTripleConfig(
        triples_query=optimized_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        
        # Incremental loading settings
        append_to_existing_graph=append_mode,       # Append by default
        clear_existing_graph=False,                 # Don't clear by default
        handle_duplicates=append_mode,              # Handle duplicates in append mode
        
        # Optimized settings for fast relationship creation
        batch_size=2000,                         # Larger batches for relationships
        max_concurrent_batches=3,                # Controlled concurrency
        connection_pool_size=10,                 # Moderate pool size
        sparql_timeout=7200,                     # 2 hours for large queries
        falkordb_timeout=None,                   # No timeout for FalkorDB
        
        # Speed optimizations
        preserve_uri_properties=False,           # Disable for maximum speed
        disable_relationship_properties=True,    # Skip relationship properties
        group_relationships_by_type=True,        # Enable grouping
        use_bulk_relationship_creation=True,     # Enable bulk operations
        
        # Performance settings
        use_shortened_uris=True,
        create_indexes=True,
        validate_conversion=False,               # Skip validation for speed
        export_stats=True,
        progress_update_interval=25,             # Frequent progress updates
    )

def create_fresh_graph_config(sparql_endpoint: str, graph_name: str = "fresh_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for fresh graph creation (clears existing data)"""
    config = create_optimized_config(sparql_endpoint, graph_name, append_mode=False)
    config.clear_existing_graph = True
    config.handle_duplicates = False  # No need to handle duplicates in fresh graph
    return config

def create_append_config(sparql_endpoint: str, graph_name: str = "incremental_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for appending to existing graph"""
    config = create_optimized_config(sparql_endpoint, graph_name, append_mode=True)
    config.append_to_existing_graph = True
    config.clear_existing_graph = False
    config.handle_duplicates = True
    return config

async def test_optimized_converter(sparql_endpoint: str, sample_size: int = 10000):
    """Test the optimized converter with a sample dataset"""
    
    logger.info(f"🧪 Testing optimized converter with {sample_size:,} triples...")
    
    # Create test configuration
    config = create_optimized_config(sparql_endpoint, "test_optimization")
    
    # Add LIMIT to query for testing
    test_query = config.triples_query.replace("# Remove LIMIT for full dataset conversion", f"LIMIT {sample_size}")
    config.triples_query = test_query
    
    # Run conversion
    converter = OptimizedAsyncTripleBasedConverter(config)
    
    try:
        stats = await converter.convert()
        
        # Display results
        logger.info("🎯 TEST RESULTS:")
        logger.info(f"   Processed: {stats.processed_triples:,} triples")
        logger.info(f"   Created: {stats.created_nodes:,} nodes, {stats.created_relationships:,} relationships")
        
        if stats.relationship_creation_rate > 0:
            logger.info(f"   Relationship rate: {stats.relationship_creation_rate:.1f} rel/sec")
            
            if stats.relationship_creation_rate > 1000:
                logger.info("🎉 EXCELLENT performance! Ready for large datasets.")
            elif stats.relationship_creation_rate > 500:
                logger.info("✅ GOOD performance! Should handle large datasets well.")
            else:
                logger.info("⚠️  Consider optimizing Docker/system configuration.")
        
        return stats
        
    except Exception as e:
        logger.error(f"❌ Test failed: {e}")
        raise

async def main():
    """Example usage of the optimized converter with incremental loading"""
    
    # Configuration
    sparql_endpoint = "https://dbpedia.org/sparql"  # Replace with your endpoint
    graph_name = "my_incremental_graph"
    
    logger.info("🚀 Starting Optimized FalkorDB RDF Conversion with Incremental Loading")
    logger.info("=" * 70)
    
    try:
        # Example 1: Multiple incremental loads with different queries
        logger.info("Example 1: Building graph incrementally with multiple queries")
        
        # First load: People and their basic properties
        people_query = """
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        PREFIX dbo: <http://dbpedia.org/ontology/>
        
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            ?subject a foaf:Person .
            
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { 
                ?object a ?objectClass .
                FILTER(isURI(?object))
            }
            
            FILTER(?predicate IN (foaf:name, dbo:birthDate, dbo:birthPlace))
        }
        LIMIT 1000
        """
        
        config1 = create_append_config(sparql_endpoint, graph_name)
        config1.triples_query = people_query
        
        logger.info("Phase 1: Loading people and basic properties...")
        converter1 = OptimizedAsyncTripleBasedConverter(config1)
        stats1 = await converter1.convert()
        
        logger.info(f"Phase 1 complete: {stats1.get_incremental_summary()}")
        logger.info("")
        
        # Second load: Organizations and relationships
        org_query = """
        PREFIX dbo: <http://dbpedia.org/ontology/>
        PREFIX foaf: <http://xmlns.com/foaf/0.1/>
        
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { 
                ?object a ?objectClass .
                FILTER(isURI(?object))
            }
            
            FILTER(
                ?subjectClass IN (dbo:Organization, dbo:Company) OR
                ?objectClass IN (dbo:Organization, dbo:Company) OR
                ?predicate IN (dbo:employer, dbo:foundedBy, dbo:subsidiary)
            )
        }
        LIMIT 1000
        """
        
        config2 = create_append_config(sparql_endpoint, graph_name)
        config2.triples_query = org_query
        
        logger.info("Phase 2: Adding organizations and relationships...")
        converter2 = OptimizedAsyncTripleBasedConverter(config2)
        stats2 = await converter2.convert()
        
        logger.info(f"Phase 2 complete: {stats2.get_incremental_summary()}")
        logger.info("")
        
        # Third load: Locations
        location_query = """
        PREFIX dbo: <http://dbpedia.org/ontology/>
        
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { 
                ?object a ?objectClass .
                FILTER(isURI(?object))
            }
            
            FILTER(
                ?subjectClass IN (dbo:Place, dbo:City, dbo:Country) OR
                ?objectClass IN (dbo:Place, dbo:City, dbo:Country) OR
                ?predicate IN (dbo:location, dbo:country, dbo:city)
            )
        }
        LIMIT 1000
        """
        
        config3 = create_append_config(sparql_endpoint, graph_name)
        config3.triples_query = location_query
        
        logger.info("Phase 3: Adding locations...")
        converter3 = OptimizedAsyncTripleBasedConverter(config3)
        stats3 = await converter3.convert()
        
        logger.info(f"Phase 3 complete: {stats3.get_incremental_summary()}")
        
        # Final summary
        logger.info("\n" + "=" * 70)
        logger.info("🎉 INCREMENTAL LOADING COMPLETED!")
        logger.info("=" * 70)
        logger.info("Final graph statistics:")
        
        # Get final stats
        final_manager = OptimizedAsyncFalkorDBManager(config3)
        await final_manager.connect()
        final_stats = await final_manager.get_graph_stats()
        await final_manager.close()
        
        logger.info(f"Total nodes: {final_stats['nodes']:,}")
        logger.info(f"Total relationships: {final_stats['relationships']:,}")
        logger.info("")
        
        logger.info("Sample queries to explore your incremental graph:")
        sample_queries = [
            "MATCH (n) RETURN labels(n) as types, count(n) as count ORDER BY count DESC",
            "MATCH ()-[r]->() RETURN type(r) as rel_type, count(r) as count ORDER BY count DESC",
            "MATCH (p:Person)-[r]->(o) RETURN p.name, type(r), o LIMIT 10",
            "MATCH (org:Organization)-[r]->(loc:Place) RETURN org.name, r, loc.name LIMIT 10",
        ]
        
        for i, query in enumerate(sample_queries, 1):
            logger.info(f"{i}. {query}")
        
        return final_stats
        
    except Exception as e:
        logger.error(f"❌ Incremental conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise

# Additional utility functions for different use cases
async def single_query_append(sparql_endpoint: str, query: str, graph_name: str = "default_graph"):
    """Append a single SPARQL query result to existing graph"""
    
    logger.info(f"📈 Appending query results to graph '{graph_name}'...")
    
    config = create_append_config(sparql_endpoint, graph_name)
    config.triples_query = query
    
    converter = OptimizedAsyncTripleBasedConverter(config)
    stats = await converter.convert()
    
    logger.info(f"✅ Append complete: {stats.get_incremental_summary()}")
    return stats

async def fresh_graph_from_query(sparql_endpoint: str, query: str, graph_name: str = "fresh_graph"):
    """Create a fresh graph from a SPARQL query (clears existing data)"""
    
    logger.info(f"🔄 Creating fresh graph '{graph_name}'...")
    
    config = create_fresh_graph_config(sparql_endpoint, graph_name)
    config.triples_query = query
    
    converter = OptimizedAsyncTripleBasedConverter(config)
    stats = await converter.convert()
    
    logger.info(f"✅ Fresh graph created: {stats.created_nodes:,} nodes, {stats.created_relationships:,} relationships")
    return stats

async def multi_source_incremental_loading():
    """Example: Load from multiple SPARQL endpoints into one graph"""
    
    endpoints = [
        "https://dbpedia.org/sparql",
        "https://query.wikidata.org/sparql",
        # Add more endpoints as needed
    ]
    
    graph_name = "multi_source_graph"
    
    for i, endpoint in enumerate(endpoints, 1):
        logger.info(f"Loading from endpoint {i}: {endpoint}")
        
        # Customize query for each endpoint
        if "dbpedia" in endpoint:
            query = """
            PREFIX dbo: <http://dbpedia.org/ontology/>
            SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
                ?subject ?predicate ?object .
                ?subject a dbo:Person .
                OPTIONAL { ?subject a ?subjectClass }
                OPTIONAL { ?predicate a ?predicateClass }
                OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            }
            LIMIT 500
            """
        else:
            # Generic query for other endpoints
            query = """
            SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
                ?subject ?predicate ?object .
                OPTIONAL { ?subject a ?subjectClass }
                OPTIONAL { ?predicate a ?predicateClass }
                OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            }
            LIMIT 500
            """
        
        try:
            await single_query_append(endpoint, query, graph_name)
        except Exception as e:
            logger.error(f"Failed to load from {endpoint}: {e}")
            continue
    
    logger.info("🎉 Multi-source loading completed!")

if __name__ == "__main__":
    # Run the incremental loading example
    asyncio.run(main())
