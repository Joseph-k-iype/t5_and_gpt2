# GDPR Multi-Agent System - PRODUCTION READY with FIXED ERRORS
# Requirements: pip install langchain langgraph langchain-elasticsearch langchain-openai pymupdf==1.26.1 pydantic scikit-learn

import asyncio
import logging
import os
import pickle
import ssl
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Set
from uuid import uuid4

import numpy as np
import pymupdf  # Latest PyMuPDF 1.26.1
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field, ConfigDict
from elasticsearch import Elasticsearch, AsyncElasticsearch
from openai import AsyncOpenAI

# =============================================================================
# GLOBAL CONFIGURATION - ALL CREDENTIALS AND SETTINGS
# =============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
OPENAI_MODEL = "o3-mini"  # ONLY o3-mini everywhere
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
OPENAI_REASONING_EFFORT = "high"

# Elasticsearch Configuration  
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost")
ELASTICSEARCH_PORT = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_CA_CERTS = os.getenv("ELASTICSEARCH_CA_CERTS", "/path/to/http_ca.crt")
ELASTICSEARCH_SSL_FINGERPRINT = os.getenv("ELASTICSEARCH_SSL_FINGERPRINT", None)
ELASTICSEARCH_USE_SSL = True
ELASTICSEARCH_VERIFY_CERTS = True

# Processing Configuration
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 300
SIMILARITY_THRESHOLD = 0.7
BATCH_SIZE = 50
MAX_TOKENS = 4000

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# ENHANCED PYDANTIC MODELS WITH LONG-TERM MEMORY
# =============================================================================

class DocumentType(str, Enum):
    GDPR_EU = "gdpr_eu"
    GDPR_UK = "gdpr_uk"

class ChunkType(str, Enum):
    CHAPTER = "chapter"
    ARTICLE = "article"
    SECTION = "section"
    PARAGRAPH = "paragraph"

class RelationshipType(str, Enum):
    REFERENCES = "references"
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    ELABORATES = "elaborates"
    SUPERSEDES = "supersedes"
    SEMANTICALLY_RELATED = "semantically_related"

class MemoryChunk(BaseModel):
    """Enhanced chunk model with long-term memory capabilities"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    id: str = Field(default_factory=lambda: str(uuid4()))
    document_type: DocumentType
    chunk_type: ChunkType
    title: str
    content: str
    chapter_number: Optional[str] = None
    article_number: Optional[str] = None
    hierarchy_level: int = Field(ge=0, le=5)
    page_number: Optional[int] = None
    position_in_document: int = Field(ge=0)
    embedding: Optional[List[float]] = None
    keywords: List[str] = Field(default_factory=list)
    legal_concepts: List[str] = Field(default_factory=list)
    related_chunk_ids: Set[str] = Field(default_factory=set)
    semantic_similarity_scores: Dict[str, float] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)

class SemanticRelationship(BaseModel):
    """Enhanced relationship model for long-term memory"""
    id: str = Field(default_factory=lambda: str(uuid4()))
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: RelationshipType
    confidence_score: float = Field(ge=0.0, le=1.0)
    semantic_similarity: float = Field(ge=0.0, le=1.0)
    distance_in_document: int = Field(ge=0)
    reasoning: str
    legal_basis: str
    extracted_by_agent: str
    created_at: datetime = Field(default_factory=datetime.now)

class LongTermMemoryState(BaseModel):
    """State with comprehensive long-term memory"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Document processing state
    documents: List[Document] = Field(default_factory=list)
    memory_chunks: List[MemoryChunk] = Field(default_factory=list)
    relationships: List[SemanticRelationship] = Field(default_factory=list)
    
    # Long-term memory components
    global_embedding_index: Dict[str, List[float]] = Field(default_factory=dict)
    concept_memory: Dict[str, Set[str]] = Field(default_factory=dict)
    temporal_memory: List[str] = Field(default_factory=list)
    semantic_clusters: Dict[str, List[str]] = Field(default_factory=dict)
    
    # LangChain components
    vectorstore: Optional[Any] = None
    qa_chain: Optional[Any] = None
    conversational_chain: Optional[Any] = None
    conversation_memory: Optional[Any] = None
    
    # Processing state
    current_agent: Optional[str] = None
    errors: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

# =============================================================================
# FIXED ELASTICSEARCH CONFIGURATION WITH AUTHENTICATION
# =============================================================================

class SecureElasticsearchConfig:
    """Secure Elasticsearch configuration with FIXED authentication"""
    
    def __init__(self, 
                 host: str = ELASTICSEARCH_HOST, 
                 port: int = ELASTICSEARCH_PORT,
                 username: str = ELASTICSEARCH_USERNAME,
                 password: str = ELASTICSEARCH_PASSWORD,
                 ca_certs: str = ELASTICSEARCH_CA_CERTS,
                 use_ssl: bool = ELASTICSEARCH_USE_SSL,
                 verify_certs: bool = ELASTICSEARCH_VERIFY_CERTS,
                 ssl_fingerprint: Optional[str] = ELASTICSEARCH_SSL_FINGERPRINT):
        
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.ca_certs = ca_certs
        self.use_ssl = use_ssl  # ‚úÖ FIXED: Set as instance variable
        self.verify_certs = verify_certs
        self.ssl_fingerprint = ssl_fingerprint
        
        if not self.password or self.password == "your-elasticsearch-password":
            raise ValueError("Elasticsearch password must be provided via parameter or ELASTICSEARCH_PASSWORD env var")
    
    def get_elasticsearch_url(self) -> str:
        """Get properly formatted Elasticsearch URL"""
        scheme = "https" if self.use_ssl else "http"
        return f"{scheme}://{self.host}:{self.port}"
    
    def get_client_config(self) -> Dict[str, Any]:
        """Get Elasticsearch client configuration"""
        config = {
            "hosts": [self.get_elasticsearch_url()],
            "basic_auth": (self.username, self.password),
        }
        
        if self.use_ssl:
            if self.ssl_fingerprint:
                # Use SSL fingerprint method (preferred for self-signed certs)
                config["ssl_assert_fingerprint"] = self.ssl_fingerprint
            elif os.path.exists(self.ca_certs):
                # Use CA certificate method
                config["ca_certs"] = self.ca_certs
                config["verify_certs"] = self.verify_certs
            else:
                logger.warning(f"CA cert file not found at {self.ca_certs}, disabling cert verification")
                config["verify_certs"] = False
        
        return config
    
    def create_client(self) -> Elasticsearch:
        """Create authenticated Elasticsearch client"""
        return Elasticsearch(**self.get_client_config())
    
    def create_async_client(self) -> AsyncElasticsearch:
        """Create authenticated async Elasticsearch client"""
        return AsyncElasticsearch(**self.get_client_config())

# =============================================================================
# FIXED OPENAI REASONING SERVICE - ONLY O3-MINI
# =============================================================================

class OpenAIReasoningService:
    """OpenAI o3-mini reasoning service - ONLY o3-mini everywhere"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided via parameter or OPENAI_API_KEY env var")
        
        self.api_key = api_key
        self.client = AsyncOpenAI(api_key=api_key)
        
        # Create LangChain ChatOpenAI - ONLY o3-mini
        self.llm = ChatOpenAI(
            model=OPENAI_MODEL,  # o3-mini only
            api_key=api_key,
            temperature=0.1,
            model_kwargs={"reasoning_effort": OPENAI_REASONING_EFFORT}  # ‚úÖ FIXED: reasoning_effort here
        )
    
    async def reasoning_completion(self, 
                                 messages: List[Dict[str, str]], 
                                 reasoning_effort: str = OPENAI_REASONING_EFFORT,
                                 max_tokens: int = MAX_TOKENS) -> Dict[str, Any]:
        """Make reasoning completion with o3-mini ONLY"""
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,  # o3-mini only
                messages=messages,
                reasoning_effort=reasoning_effort,  # low, medium, high
                max_completion_tokens=max_tokens,
                temperature=0.1
            )
            
            return {
                "content": response.choices[0].message.content,
                "reasoning_tokens": getattr(response.usage, 'reasoning_tokens', 0),
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
            
        except Exception as e:
            logger.error(f"Error in reasoning completion: {e}")
            raise
    
    async def extract_relationships_with_reasoning(self, chunk1: MemoryChunk, chunk2: MemoryChunk) -> Optional[SemanticRelationship]:
        """Use o3-mini with high reasoning to extract relationships"""
        
        messages = [
            {
                "role": "user",
                "content": f"""You are a legal expert analyzing GDPR document relationships with deep reasoning.
                
                Analyze these two chunks and determine their relationship:
                
                Chunk 1 ({chunk1.chunk_type.value}):
                Title: {chunk1.title}
                Content: {chunk1.content[:800]}
                
                Chunk 2 ({chunk2.chunk_type.value}):
                Title: {chunk2.title}  
                Content: {chunk2.content[:800]}
                
                Use deep legal reasoning to determine:
                1. Is there a meaningful legal relationship? (yes/no)
                2. Relationship type: references, supports, contradicts, elaborates, supersedes, semantically_related
                3. Confidence score (0.0-1.0)
                4. Detailed legal reasoning explaining the connection
                5. Legal basis for the relationship
                
                Respond in JSON format:
                {{
                    "has_relationship": boolean,
                    "relationship_type": "string or null",
                    "confidence_score": float,
                    "reasoning": "detailed legal analysis",
                    "legal_basis": "legal justification"
                }}"""
            }
        ]
        
        try:
            result = await self.reasoning_completion(
                messages=messages,
                reasoning_effort=OPENAI_REASONING_EFFORT
            )
            
            # Parse JSON response
            import json
            parsed_result = json.loads(result["content"])
            
            if parsed_result.get("has_relationship") and parsed_result.get("confidence_score", 0) > 0.6:
                return SemanticRelationship(
                    source_chunk_id=chunk1.id,
                    target_chunk_id=chunk2.id,
                    relationship_type=RelationshipType(parsed_result["relationship_type"]),
                    confidence_score=parsed_result["confidence_score"],
                    semantic_similarity=0.0,  # Will be calculated separately
                    distance_in_document=abs(chunk1.position_in_document - chunk2.position_in_document),
                    reasoning=parsed_result["reasoning"],
                    legal_basis=parsed_result.get("legal_basis", ""),
                    extracted_by_agent="openai_reasoning_service"
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting relationship: {e}")
            return None
    
    async def analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """Analyze GDPR document structure using o3-mini reasoning"""
        
        messages = [
            {
                "role": "user", 
                "content": f"""You are a legal document analysis expert. Use deep reasoning to analyze this GDPR document text.
                
                Text: {text[:2000]}
                
                Identify and extract:
                1. Document type (EU GDPR vs UK GDPR vs other)
                2. Chapters with titles and numbers
                3. Articles with numbers and titles  
                4. Sections and subsections
                5. Key legal concepts and terms
                6. Hierarchical structure
                
                Return detailed JSON structure with hierarchy information.
                
                {{
                    "document_type": "gdpr_eu|gdpr_uk",
                    "structure": {{
                        "chapters": [],
                        "articles": [],
                        "sections": []
                    }},
                    "legal_concepts": [],
                    "hierarchy_levels": {{}}
                }}"""
            }
        ]
        
        try:
            result = await self.reasoning_completion(
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            return json.loads(result["content"])
            
        except Exception as e:
            logger.error(f"Error analyzing structure: {e}")
            return {
                "document_type": "unknown",
                "structure": {"chapters": [], "articles": [], "sections": []},
                "legal_concepts": [],
                "hierarchy_levels": {}
            }
    
    async def extract_legal_concepts(self, text: str) -> List[str]:
        """Extract legal concepts using o3-mini reasoning"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract key legal concepts from this GDPR text using deep legal reasoning.
                
                Focus on:
                - Rights (e.g., right to erasure, right of access)
                - Legal bases (e.g., consent, legitimate interest)
                - Processes (e.g., data processing, profiling)
                - Entities (e.g., data controller, data processor)
                - Principles (e.g., data minimization, purpose limitation)
                - Obligations and requirements
                
                Text: {text[:1000]}
                
                Return as JSON array: ["concept1", "concept2", ...]
                """
            }
        ]
        
        try:
            result = await self.reasoning_completion(
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            concepts = json.loads(result["content"])
            return concepts if isinstance(concepts, list) else []
            
        except Exception as e:
            logger.error(f"Error extracting legal concepts: {e}")
            return []

# =============================================================================
# LONG-TERM MEMORY MANAGER
# =============================================================================

class LongTermMemoryManager:
    """Manages long-term memory across document chunks"""
    
    def __init__(self, similarity_threshold: float = SIMILARITY_THRESHOLD):
        self.similarity_threshold = similarity_threshold
        self.concept_embeddings: Dict[str, List[float]] = {}
        self.temporal_context_window = 10
        
    def add_chunk_to_memory(self, chunk: MemoryChunk, state: LongTermMemoryState):
        """Add chunk to long-term memory with comprehensive indexing"""
        
        # Add to global embedding index
        if chunk.embedding:
            state.global_embedding_index[chunk.id] = chunk.embedding
        
        # Add to concept memory
        for concept in chunk.legal_concepts:
            if concept not in state.concept_memory:
                state.concept_memory[concept] = set()
            state.concept_memory[concept].add(chunk.id)
        
        # Add to temporal memory
        state.temporal_memory.append(chunk.id)
        
        # Find semantic relationships with ALL existing chunks
        self._find_global_semantic_relationships(chunk, state)
        
        # Update semantic clusters
        self._update_semantic_clusters(chunk, state)
    
    def _find_global_semantic_relationships(self, new_chunk: MemoryChunk, state: LongTermMemoryState):
        """Find relationships with ALL chunks in memory, not just adjacent ones"""
        
        if not new_chunk.embedding:
            return
        
        new_embedding = np.array(new_chunk.embedding).reshape(1, -1)
        
        for existing_chunk in state.memory_chunks:
            if existing_chunk.id == new_chunk.id or not existing_chunk.embedding:
                continue
                
            existing_embedding = np.array(existing_chunk.embedding).reshape(1, -1)
            similarity = cosine_similarity(new_embedding, existing_embedding)[0][0]
            
            # Calculate distance in document
            distance = abs(new_chunk.position_in_document - existing_chunk.position_in_document)
            
            # Store similarity score
            new_chunk.semantic_similarity_scores[existing_chunk.id] = similarity
            existing_chunk.semantic_similarity_scores[new_chunk.id] = similarity
            
            # Create relationship if similarity is high enough
            if similarity > self.similarity_threshold:
                relationship_type = self._determine_relationship_type(
                    new_chunk, existing_chunk, similarity, distance
                )
                
                relationship = SemanticRelationship(
                    source_chunk_id=new_chunk.id,
                    target_chunk_id=existing_chunk.id,
                    relationship_type=relationship_type,
                    confidence_score=similarity,
                    semantic_similarity=similarity,
                    distance_in_document=distance,
                    reasoning=f"High semantic similarity ({similarity:.3f}) between chunks {distance} positions apart",
                    legal_basis=self._extract_legal_basis(new_chunk, existing_chunk),
                    extracted_by_agent="long_term_memory_manager"
                )
                
                state.relationships.append(relationship)
                new_chunk.related_chunk_ids.add(existing_chunk.id)
                existing_chunk.related_chunk_ids.add(new_chunk.id)
    
    def _determine_relationship_type(self, chunk1: MemoryChunk, chunk2: MemoryChunk, 
                                   similarity: float, distance: int) -> RelationshipType:
        """Determine relationship type based on content and distance"""
        
        if distance < 5:
            if chunk1.hierarchy_level < chunk2.hierarchy_level:
                return RelationshipType.ELABORATES
            else:
                return RelationshipType.REFERENCES
        
        if distance > 20 and similarity > 0.85:
            return RelationshipType.SEMANTICALLY_RELATED
            
        common_concepts = set(chunk1.legal_concepts) & set(chunk2.legal_concepts)
        if common_concepts:
            return RelationshipType.SUPPORTS
            
        return RelationshipType.SEMANTICALLY_RELATED
    
    def _extract_legal_basis(self, chunk1: MemoryChunk, chunk2: MemoryChunk) -> str:
        """Extract legal justification for relationship"""
        common_concepts = set(chunk1.legal_concepts) & set(chunk2.legal_concepts)
        if common_concepts:
            return f"Shared legal concepts: {', '.join(common_concepts)}"
        
        if chunk1.article_number and chunk2.article_number:
            return f"Cross-reference between Article {chunk1.article_number} and Article {chunk2.article_number}"
            
        return "Semantic similarity in legal context"
    
    def _update_semantic_clusters(self, chunk: MemoryChunk, state: LongTermMemoryState):
        """Update semantic clusters based on similarity"""
        
        best_cluster = None
        best_similarity = 0
        
        for cluster_id, chunk_ids in state.semantic_clusters.items():
            cluster_similarities = []
            for chunk_id in chunk_ids:
                if chunk_id in chunk.semantic_similarity_scores:
                    cluster_similarities.append(chunk.semantic_similarity_scores[chunk_id])
            
            if cluster_similarities:
                avg_similarity = np.mean(cluster_similarities)
                if avg_similarity > best_similarity and avg_similarity > self.similarity_threshold:
                    best_similarity = avg_similarity
                    best_cluster = cluster_id
        
        if best_cluster:
            state.semantic_clusters[best_cluster].append(chunk.id)
        else:
            new_cluster_id = f"cluster_{len(state.semantic_clusters)}"
            state.semantic_clusters[new_cluster_id] = [chunk.id]

# =============================================================================
# FIXED SERVICES WITH PROPER ELASTICSEARCH AUTHENTICATION
# =============================================================================

class EnhancedLangChainElasticsearchService:
    def __init__(self, es_config: SecureElasticsearchConfig):
        self.es_config = es_config
        self.embeddings = OpenAIEmbeddings(
            model=OPENAI_EMBEDDING_MODEL,
            api_key=OPENAI_API_KEY
        )
        # ‚úÖ FIXED: Use ONLY o3-mini, not gpt-4o
        self.llm = ChatOpenAI(
            model=OPENAI_MODEL,  # o3-mini only
            api_key=OPENAI_API_KEY,
            temperature=0.1,
            model_kwargs={"reasoning_effort": OPENAI_REASONING_EFFORT}
        )
        
    def create_vectorstore(self, index_name: str = "gdpr_longterm_memory") -> ElasticsearchStore:
        """Create enhanced vector store with authentication"""
        return ElasticsearchStore(
            elasticsearch_url=self.es_config.get_elasticsearch_url(),
            index_name=index_name,
            embedding=self.embeddings,
            es_user=self.es_config.username,
            es_password=self.es_config.password,
            es_params={
                "ca_certs": self.es_config.ca_certs if os.path.exists(self.es_config.ca_certs) else None,
                "verify_certs": self.es_config.verify_certs,
            }
        )
    
    def create_memory_aware_qa_chains(self, vectorstore: ElasticsearchStore) -> Tuple[RetrievalQA, ConversationalRetrievalChain, ConversationSummaryBufferMemory]:
        """Create QA chains with long-term conversational memory"""
        
        # Create persistent conversation memory
        conversation_memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=2000,
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Enhanced QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "score_threshold": 0.6,
                    "k": 15,
                }
            ),
            return_source_documents=True,
            chain_type_kwargs={
                "prompt": PromptTemplate(
                    template="""You are an expert GDPR legal analyst with perfect memory of all previous discussions.
                    
                    Context from documents: {context}
                    
                    Question: {question}
                    
                    Instructions:
                    1. Provide comprehensive answer citing specific GDPR articles/sections
                    2. Consider relationships between different GDPR provisions  
                    3. Reference any related concepts from distant parts of the regulation
                    4. Explain how this connects to broader GDPR principles
                    
                    Legal Analysis:""",
                    input_variables=["context", "question"]
                )
            }
        )
        
        # Conversational chain with enhanced memory
        conversational_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 12}),
            memory=conversation_memory,
            return_source_documents=True,
            verbose=True
        )
        
        return qa_chain, conversational_chain, conversation_memory

class EnhancedDocumentParsingAgent:
    def __init__(self, reasoning_service: OpenAIReasoningService, memory_manager: LongTermMemoryManager):
        self.reasoning_service = reasoning_service
        self.memory_manager = memory_manager
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
    async def process_documents_with_memory(self, file_paths: List[str]) -> List[Document]:
        """Process documents using latest PyMuPDF with enhanced memory"""
        all_documents = []
        position_counter = 0
        
        for file_path in file_paths:
            try:
                # Extract text with latest PyMuPDF
                text, page_info = self._extract_text_pymupdf_latest(file_path)
                
                # Analyze document structure with o3-mini
                structure = await self.reasoning_service.analyze_document_structure(text)
                
                # Determine document type
                doc_type = self._determine_document_type(file_path, text, structure)
                
                # Create memory-aware chunks
                chunks = await self._create_memory_chunks(
                    text, doc_type, structure, file_path, page_info, position_counter
                )
                
                # Convert to LangChain Documents
                documents = self._chunks_to_documents(chunks)
                all_documents.extend(documents)
                
                position_counter += len(chunks)
                logger.info(f"Processed {file_path}: {len(chunks)} memory-aware chunks")
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
        
        return all_documents
    
    def _extract_text_pymupdf_latest(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text using latest PyMuPDF 1.26.1 - FIXED table detection"""
        doc = pymupdf.open(file_path)
        text = ""
        page_info = {}
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            page_text = page.get_text()
            text += page_text + "\n"
            
            # ‚úÖ FIXED: Proper table detection - check tables attribute, not len()
            try:
                table_finder = page.find_tables()
                has_tables = len(table_finder.tables) > 0 if hasattr(table_finder, 'tables') else False
            except Exception as e:
                logger.warning(f"Table detection failed on page {page_num}: {e}")
                has_tables = False
            
            # Enhanced page information
            page_info[page_num] = {
                "text_length": len(page_text),
                "has_images": len(page.get_images()) > 0,
                "has_tables": has_tables
            }
        
        doc.close()
        return text, page_info
    
    def _determine_document_type(self, file_path: str, text: str, structure: Dict) -> DocumentType:
        """Enhanced document type detection"""
        # Use structure analysis result if available
        if structure.get("document_type") == "gdpr_uk":
            return DocumentType.GDPR_UK
        elif structure.get("document_type") == "gdpr_eu":
            return DocumentType.GDPR_EU
        
        # Fallback to text analysis
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in ["uk gdpr", "data protection act 2018", "ico"]):
            return DocumentType.GDPR_UK
        return DocumentType.GDPR_EU
    
    async def _create_memory_chunks(self, text: str, doc_type: DocumentType, structure: Dict, 
                                  file_path: str, page_info: Dict, position_offset: int) -> List[MemoryChunk]:
        """Create enhanced memory chunks with legal concept extraction"""
        chunks = []
        text_chunks = self.text_splitter.split_text(text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Extract legal concepts using o3-mini
            legal_concepts = await self.reasoning_service.extract_legal_concepts(chunk_text)
            
            # Determine chunk characteristics
            chunk_type = self._determine_chunk_type(chunk_text)
            title = self._extract_title(chunk_text)
            hierarchy_level = self._determine_hierarchy_level(chunk_text)
            article_number = self._extract_article_number(chunk_text)
            chapter_number = self._extract_chapter_number(chunk_text)
            page_number = self._estimate_page_number(i, len(text_chunks), len(page_info))
            
            # Create memory chunk
            memory_chunk = MemoryChunk(
                document_type=doc_type,
                chunk_type=chunk_type,
                title=title,
                content=chunk_text,
                chapter_number=chapter_number,
                article_number=article_number,
                hierarchy_level=hierarchy_level,
                page_number=page_number,
                position_in_document=position_offset + i,
                keywords=self._extract_keywords(chunk_text),
                legal_concepts=legal_concepts,
                metadata={
                    "source_file": file_path,
                    "chunk_index": i,
                    "total_chunks": len(text_chunks),
                    "structure_info": structure
                }
            )
            
            chunks.append(memory_chunk)
        
        return chunks
    
    def _chunks_to_documents(self, chunks: List[MemoryChunk]) -> List[Document]:
        """Convert memory chunks to LangChain Documents - FIXED metadata handling"""
        documents = []
        
        for chunk in chunks:
            # ‚úÖ FIXED: Create proper Document with all metadata
            doc = Document(
                page_content=chunk.content,
                metadata={
                    "chunk_id": chunk.id,
                    "document_type": chunk.document_type.value,
                    "chunk_type": chunk.chunk_type.value,
                    "title": chunk.title,
                    "hierarchy_level": chunk.hierarchy_level,
                    "article_number": chunk.article_number,
                    "chapter_number": chunk.chapter_number,
                    "page_number": chunk.page_number,
                    "position_in_document": chunk.position_in_document,
                    "keywords": chunk.keywords,
                    "legal_concepts": chunk.legal_concepts,
                    # Include all chunk metadata
                    **chunk.metadata
                }
            )
            documents.append(doc)
        
        return documents
    
    # Helper methods
    def _determine_chunk_type(self, text: str) -> ChunkType:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return ChunkType.CHAPTER
        elif "article" in text_lower[:100]:
            return ChunkType.ARTICLE
        elif any(marker in text_lower[:50] for marker in ["section", "¬ß"]):
            return ChunkType.SECTION
        return ChunkType.PARAGRAPH
    
    def _extract_title(self, text: str) -> str:
        lines = text.split('\n')
        for line in lines[:3]:
            if line.strip() and len(line.strip()) < 120:
                return line.strip()
        return text[:60] + "..."
    
    def _determine_hierarchy_level(self, text: str) -> int:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return 1
        elif "article" in text_lower[:100]:
            return 2
        elif "section" in text_lower[:100]:
            return 3
        return 4
    
    def _extract_article_number(self, text: str) -> Optional[str]:
        import re
        match = re.search(r'article\s+(\d+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_chapter_number(self, text: str) -> Optional[str]:
        import re
        match = re.search(r'chapter\s+(\d+|[ivx]+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_keywords(self, text: str) -> List[str]:
        legal_keywords = [
            "personal data", "data subject", "consent", "processing", "controller",
            "processor", "lawful basis", "legitimate interest", "data protection",
            "rights", "erasure", "rectification", "portability", "breach", "profiling"
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in legal_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _estimate_page_number(self, chunk_index: int, total_chunks: int, total_pages: int) -> int:
        if total_pages == 0:
            return 1
        return min(int((chunk_index / total_chunks) * total_pages) + 1, total_pages)

# =============================================================================
# MAIN SYSTEM WITH FIXED ERROR HANDLING
# =============================================================================

class GDPRLongTermMemorySystem:
    def __init__(self, 
                 es_host: str = ELASTICSEARCH_HOST,
                 es_port: int = ELASTICSEARCH_PORT,
                 es_username: str = ELASTICSEARCH_USERNAME,
                 es_password: str = ELASTICSEARCH_PASSWORD,
                 es_ca_certs: str = ELASTICSEARCH_CA_CERTS,
                 es_ssl_fingerprint: Optional[str] = ELASTICSEARCH_SSL_FINGERPRINT):
        
        # Initialize Elasticsearch configuration
        self.es_config = SecureElasticsearchConfig(
            host=es_host,
            port=es_port,
            username=es_username,
            password=es_password,
            ca_certs=es_ca_certs,
            ssl_fingerprint=es_ssl_fingerprint
        )
        
        # Initialize services
        self.reasoning_service = OpenAIReasoningService()
        self.elasticsearch_service = EnhancedLangChainElasticsearchService(self.es_config)
        self.memory_manager = LongTermMemoryManager()
        
        # Initialize agents
        self.document_parser = EnhancedDocumentParsingAgent(
            self.reasoning_service, self.memory_manager
        )
        
        # Create workflow
        self.workflow = self._create_memory_workflow()
    
    def _create_memory_workflow(self) -> StateGraph:
        """Create enhanced workflow with long-term memory"""
        
        workflow = StateGraph(LongTermMemoryState)
        
        # Add nodes
        workflow.add_node("parse_with_memory", self._parse_documents_with_memory_node)
        workflow.add_node("generate_embeddings", self._generate_embeddings_node)
        workflow.add_node("build_memory_index", self._build_memory_index_node)
        workflow.add_node("create_vectorstore", self._create_vectorstore_node)
        workflow.add_node("setup_memory_chains", self._setup_memory_chains_node)
        
        # Define edges
        workflow.add_edge(START, "parse_with_memory")
        workflow.add_edge("parse_with_memory", "generate_embeddings")
        workflow.add_edge("generate_embeddings", "build_memory_index")
        workflow.add_edge("build_memory_index", "create_vectorstore")
        workflow.add_edge("create_vectorstore", "setup_memory_chains")
        workflow.add_edge("setup_memory_chains", END)
        
        return workflow.compile()
    
    async def _parse_documents_with_memory_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Parse documents and create memory chunks"""
        state.current_agent = "enhanced_document_parser"
        try:
            # ‚úÖ FIXED: Proper handling of file paths from state.documents
            file_paths = []
            for doc in state.documents:
                if hasattr(doc, 'metadata') and 'path' in doc.metadata:
                    file_paths.append(doc.metadata['path'])
            
            if not file_paths:
                state.errors.append("No file paths found in documents")
                return state
                
            documents = await self.document_parser.process_documents_with_memory(file_paths)
            state.documents = documents
            
            # Create memory chunks from documents
            state.memory_chunks = [
                self._document_to_memory_chunk(doc) for doc in documents
            ]
            
            logger.info(f"Created {len(state.memory_chunks)} memory-aware chunks")
        except Exception as e:
            error_msg = f"Memory parsing error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _generate_embeddings_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Generate embeddings for all chunks"""
        state.current_agent = "embedding_generator"
        try:
            # Prepare texts for embedding
            texts = [f"{chunk.title}\n\n{chunk.content}" for chunk in state.memory_chunks]
            
            # Generate embeddings in batches
            for i in range(0, len(texts), BATCH_SIZE):
                batch_texts = texts[i:i + BATCH_SIZE]
                batch_chunks = state.memory_chunks[i:i + BATCH_SIZE]
                
                try:
                    embeddings_response = await self.elasticsearch_service.embeddings.aembed_documents(batch_texts)
                    
                    for chunk, embedding in zip(batch_chunks, embeddings_response):
                        chunk.embedding = embedding
                    
                    logger.info(f"Generated embeddings for batch {i//BATCH_SIZE + 1}")
                    
                    # Rate limiting
                    await asyncio.sleep(1.0)
                    
                except Exception as e:
                    logger.error(f"Error in embedding batch {i//BATCH_SIZE + 1}: {e}")
                    # Continue with next batch
            
        except Exception as e:
            error_msg = f"Embedding generation error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _build_memory_index_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Build comprehensive long-term memory index"""
        state.current_agent = "memory_index_builder"
        try:
            # Add each chunk to long-term memory
            for chunk in state.memory_chunks:
                if chunk.embedding:  # Only process chunks with embeddings
                    self.memory_manager.add_chunk_to_memory(chunk, state)
            
            logger.info(f"Built memory index with {len(state.memory_chunks)} chunks")
            logger.info(f"Found {len(state.relationships)} semantic relationships")
            logger.info(f"Created {len(state.semantic_clusters)} semantic clusters")
            
        except Exception as e:
            error_msg = f"Memory indexing error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _create_vectorstore_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Create vector store from memory chunks"""
        state.current_agent = "vectorstore_creator"
        try:
            # Test Elasticsearch connection first
            es_client = self.es_config.create_client()
            info = es_client.info()
            logger.info(f"Connected to Elasticsearch: {info['version']['number']}")
            
            vectorstore = self.elasticsearch_service.create_vectorstore()
            
            # Add documents to vector store in batches
            for i in range(0, len(state.documents), BATCH_SIZE):
                batch = state.documents[i:i + BATCH_SIZE]
                await vectorstore.aadd_documents(batch)
                logger.info(f"Added document batch {i//BATCH_SIZE + 1} to vector store")
            
            state.vectorstore = vectorstore
            logger.info("Created enhanced vector store")
            
        except Exception as e:
            error_msg = f"Vector store error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _setup_memory_chains_node(self, state: LongTermMemoryState) -> LongTermMemoryState:
        """Setup QA chains with long-term memory"""
        state.current_agent = "memory_chain_creator"
        try:
            if state.vectorstore:
                qa_chain, conv_chain, memory = self.elasticsearch_service.create_memory_aware_qa_chains(state.vectorstore)
                state.qa_chain = qa_chain
                state.conversational_chain = conv_chain
                state.conversation_memory = memory
                
                logger.info("Created memory-aware QA chains")
        except Exception as e:
            error_msg = f"QA chain setup error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    def _document_to_memory_chunk(self, doc: Document) -> MemoryChunk:
        """Convert LangChain Document to MemoryChunk"""
        return MemoryChunk(
            id=doc.metadata.get("chunk_id", str(uuid4())),
            document_type=DocumentType(doc.metadata["document_type"]),
            chunk_type=ChunkType(doc.metadata["chunk_type"]),
            title=doc.metadata["title"],
            content=doc.page_content,
            chapter_number=doc.metadata.get("chapter_number"),
            article_number=doc.metadata.get("article_number"),
            hierarchy_level=doc.metadata["hierarchy_level"],
            page_number=doc.metadata.get("page_number"),
            position_in_document=doc.metadata["position_in_document"],
            keywords=doc.metadata.get("keywords", []),
            legal_concepts=doc.metadata.get("legal_concepts", []),
            metadata=doc.metadata
        )
    
    # Public API methods
    async def process_documents(self, file_paths: List[str]) -> LongTermMemoryState:
        """Process documents with comprehensive long-term memory"""
        
        # ‚úÖ FIXED: Proper Document creation with metadata
        initial_state = LongTermMemoryState(
            documents=[
                Document(
                    page_content="", 
                    metadata={"path": path}
                ) for path in file_paths
            ],
            metadata={
                "started_at": datetime.now(),
                "file_paths": file_paths
            }
        )
        
        try:
            final_state = await self.workflow.ainvoke(initial_state)
            final_state.metadata["completed_at"] = datetime.now()
            final_state.metadata["duration"] = (
                final_state.metadata["completed_at"] - final_state.metadata["started_at"]
            ).total_seconds()
            
            logger.info(f"Processing completed in {final_state.metadata['duration']:.2f} seconds")
            return final_state
            
        except Exception as e:
            error_msg = f"Workflow error: {str(e)}"
            logger.error(error_msg)
            initial_state.errors.append(error_msg)
            return initial_state
    
    async def ask_with_memory(self, state: LongTermMemoryState, question: str) -> Dict[str, Any]:
        """Ask question with long-term memory awareness"""
        
        if not state.qa_chain:
            return {"error": "QA chain not available"}
        
        try:
            result = await state.qa_chain.ainvoke({"query": question})
            
            return {
                "answer": result["result"],
                "source_documents": [
                    {
                        "content": doc.page_content[:300] + "...",
                        "metadata": doc.metadata
                    }
                    for doc in result.get("source_documents", [])
                ]
            }
            
        except Exception as e:
            return {"error": f"Memory-aware QA error: {str(e)}"}

# =============================================================================
# FIXED EXAMPLE USAGE WITH PROPER CONFIGURATION
# =============================================================================

async def main():
    """Production-ready example with FIXED authentication and ONLY o3-mini"""
    
    try:
        # Initialize system with proper authentication
        gdpr_system = GDPRLongTermMemorySystem(
            es_host=ELASTICSEARCH_HOST,
            es_port=ELASTICSEARCH_PORT,
            es_username=ELASTICSEARCH_USERNAME,
            es_password=ELASTICSEARCH_PASSWORD,
            es_ca_certs=ELASTICSEARCH_CA_CERTS,
            es_ssl_fingerprint=ELASTICSEARCH_SSL_FINGERPRINT
        )
        
        print("üß† Processing GDPR documents with o3-mini reasoning and long-term memory...")
        print(f"üìä Configuration:")
        print(f"   ü§ñ OpenAI Model: {OPENAI_MODEL}")
        print(f"   üîç Elasticsearch: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
        print(f"   üë§ Username: {ELASTICSEARCH_USERNAME}")
        print(f"   üîí SSL: {ELASTICSEARCH_USE_SSL}")
        
        document_paths = [
            "path/to/gdpr_eu_regulation.pdf",
            "path/to/uk_gdpr_guidance.pdf"
        ]
        
        state = await gdpr_system.process_documents(document_paths)
        
        print(f"‚úÖ Processing completed!")
        print(f"üìÑ Processed {len(state.memory_chunks)} chunks")
        print(f"üîó Found {len(state.relationships)} relationships")
        print(f"üß© Semantic clusters: {len(state.semantic_clusters)}")
        
        if state.errors:
            print(f"‚ö†Ô∏è Errors encountered: {state.errors}")
        
        # Test QA with memory
        if state.qa_chain:
            print("\nü§ñ Testing memory-aware QA with o3-mini:")
            
            question = "What are the lawful bases for processing personal data and how do they relate to consent requirements?"
            answer = await gdpr_system.ask_with_memory(state, question)
            
            if "error" not in answer:
                print(f"üí° A: {answer['answer'][:400]}...")
                print(f"üìö Sources: {len(answer['source_documents'])}")
            else:
                print(f"‚ùå Error: {answer['error']}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
