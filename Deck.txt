#!/usr/bin/env python3
"""
GDPR/UK GDPR Focused Record of Processing Activities (RoPA) Metamodel System
Specialized for GDPR and UK GDPR with Dynamic Jurisdictional Applicability Discovery

Features:
- AI-powered synonym generation using LLM (o3-mini)
- Dynamic GDPR/UK GDPR territorial scope and applicability discovery
- Advanced regulatory concept extraction with reasoning
- Iterative document understanding and metamodel refinement
- Cross-border data transfer compliance mapping
- Financial sector-specific processing activities
- Comprehensive compliance reporting

REQUIRED ENVIRONMENT VARIABLES:
    OPENAI_API_KEY=your_openai_api_key
    OPENAI_BASE_URL=your_custom_openai_endpoint (optional)
    ELASTICSEARCH_HOST=https://your-elasticsearch-cluster.com:9200
    ELASTICSEARCH_USERNAME=your_username (optional)
    ELASTICSEARCH_PASSWORD=your_password (optional)
    FALKORDB_HOST=localhost
    FALKORDB_PORT=6379
    FALKORDB_PASSWORD=your_password (optional)

USAGE:
    python gdpr_ropa_system.py --ingest /path/to/gdpr/documents
    python gdpr_ropa_system.py --analyze --iterations 3
    python gdpr_ropa_system.py --generate-metamodel
    python gdpr_ropa_system.py --generate-report
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Tuple
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
from pathlib import Path

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GDPR and UK GDPR focused frameworks
class GDPRFrameworkType(Enum):
    GDPR_EU = "gdpr_eu"
    UK_GDPR = "uk_gdpr"

@dataclass
class GDPRFramework:
    name: str
    code: str
    territorial_scope: List[str]
    extraterritorial_triggers: List[str]
    adequacy_decisions: List[str]
    transfer_mechanisms: List[str]
    key_principles: List[str]
    penalties: str
    data_subject_rights: List[str]
    financial_sector_specifics: List[str]
    supervisory_authorities: List[str]

# GDPR and UK GDPR frameworks
GDPR_FRAMEWORKS = {
    GDPRFrameworkType.GDPR_EU: GDPRFramework(
        name="EU General Data Protection Regulation",
        code="GDPR",
        territorial_scope=[
            "All 27 EU Member States", "European Economic Area (EEA)", 
            "Organizations established in EU", "Processing in context of EU establishment"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to EU data subjects",
            "Monitoring behavior of EU data subjects",
            "Processing personal data of EU residents",
            "Targeting EU market regardless of controller location",
            "Use of equipment located in EU for processing",
            "Processing in context of activities of EU establishment"
        ],
        adequacy_decisions=[
            "Andorra", "Argentina", "Canada (commercial)", "Faroe Islands", "Guernsey", 
            "Israel", "Isle of Man", "Japan", "Jersey", "New Zealand", "Republic of Korea",
            "Switzerland", "United Kingdom", "Uruguay"
        ],
        transfer_mechanisms=[
            "Standard Contractual Clauses (SCCs)", "Binding Corporate Rules (BCRs)",
            "Certification mechanisms", "Codes of conduct", "Derogations Article 49",
            "Adequacy decisions Article 45"
        ],
        key_principles=[
            "lawfulness", "fairness", "transparency", "purpose_limitation", 
            "data_minimization", "accuracy", "storage_limitation", 
            "integrity_confidentiality", "accountability"
        ],
        penalties="Up to €20 million or 4% of annual global turnover",
        data_subject_rights=[
            "access", "rectification", "erasure", "portability", 
            "restriction", "objection", "automated_decision_making"
        ],
        financial_sector_specifics=[
            "Payment Services Directive (PSD2)", "MiFID II compliance", 
            "AML/CTF requirements", "Banking supervision", "SEPA regulations",
            "Capital Requirements Directive", "Insurance Distribution Directive"
        ],
        supervisory_authorities=[
            "European Data Protection Board (EDPB)", "National Data Protection Authorities",
            "Lead supervisory authority", "One-stop-shop mechanism"
        ]
    ),
    GDPRFrameworkType.UK_GDPR: GDPRFramework(
        name="United Kingdom General Data Protection Regulation",
        code="UK-GDPR",
        territorial_scope=[
            "England", "Wales", "Scotland", "Northern Ireland",
            "Organizations established in UK", "Processing in context of UK establishment",
            "Gibraltar (specific arrangements)"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to UK data subjects",
            "Monitoring behavior of UK data subjects", 
            "Processing personal data of UK residents",
            "Targeting UK market regardless of controller location",
            "Use of equipment located in UK for processing",
            "Processing in context of activities of UK establishment"
        ],
        adequacy_decisions=[
            "EU Member States", "EEA countries", "Gibraltar",
            "Countries with EU adequacy decisions (inherited)",
            "Bridge arrangements for EU transfers"
        ],
        transfer_mechanisms=[
            "International Data Transfer Agreement (IDTA)", "Standard Contractual Clauses",
            "Binding Corporate Rules", "Certification mechanisms", "Derogations",
            "Addendum to EU SCCs", "Adequacy regulations"
        ],
        key_principles=[
            "lawfulness", "fairness", "transparency", "purpose_limitation", 
            "data_minimization", "accuracy", "storage_limitation", 
            "integrity_confidentiality", "accountability"
        ],
        penalties="Up to £17.5 million or 4% of annual global turnover",
        data_subject_rights=[
            "access", "rectification", "erasure", "portability", 
            "restriction", "objection", "automated_decision_making"
        ],
        financial_sector_specifics=[
            "FCA requirements", "PCI DSS compliance", "Open Banking", 
            "Strong Customer Authentication", "Payment Services Regulations",
            "Electronic Money Regulations", "Financial Promotions Order"
        ],
        supervisory_authorities=[
            "Information Commissioner's Office (ICO)", "UK data protection regime",
            "International cooperation mechanisms"
        ]
    )
}

# Global jurisdictions for dynamic applicability discovery
POTENTIAL_AFFECTED_JURISDICTIONS = [
    # Europe (Non-EU/UK)
    "Switzerland", "Norway", "Iceland", "Liechtenstein", "Serbia", "Montenegro", 
    "Albania", "North Macedonia", "Bosnia and Herzegovina", "Moldova", "Ukraine", 
    "Turkey", "Russia", "Belarus",
    
    # Americas
    "United States", "Canada", "Brazil", "Mexico", "Argentina", "Chile", 
    "Colombia", "Peru", "Uruguay", "Costa Rica", "Panama",
    
    # Asia-Pacific
    "China", "Japan", "South Korea", "Singapore", "Australia", "New Zealand", 
    "India", "Hong Kong", "Taiwan", "Malaysia", "Thailand", "Philippines", 
    "Indonesia", "Vietnam", "Myanmar", "Cambodia",
    
    # Middle East & Africa
    "Israel", "United Arab Emirates", "Saudi Arabia", "Qatar", "Kuwait", "Bahrain",
    "South Africa", "Nigeria", "Kenya", "Egypt", "Morocco", "Tunisia",
    
    # Other regions
    "Bermuda", "Cayman Islands", "British Virgin Islands", "Jersey", "Guernsey",
    "Isle of Man", "Dubai International Financial Centre"
]

@dataclass
class ConceptSynonym:
    primary_term: str
    synonyms: List[str]
    context: str
    confidence: float
    gdpr_variants: Dict[str, List[str]]  # GDPR and UK GDPR specific variants
    financial_sector_terms: List[str]

class RopaMetamodelState(TypedDict):
    """Enhanced state for GDPR/UK GDPR RoPA metamodel generation"""
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Dict[str, Any]]
    extracted_concepts: List[Dict[str, Any]]
    generated_synonyms: List[ConceptSynonym]
    territorial_analysis: Dict[str, Any]
    extraterritorial_applicability: List[Dict[str, Any]]
    adequacy_analysis: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    cross_border_transfers: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    metamodel_structure: Dict[str, Any]
    compliance_gaps: List[Dict[str, Any]]
    reasoning_trace: List[str]

class CustomOpenAIEmbeddings(Embeddings):
    """Enhanced OpenAI Embeddings optimized for GDPR/UK GDPR regulatory content"""
    
    def __init__(self, 
                 model: str = "text-embedding-3-large",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 dimensions: Optional[int] = 3072,
                 max_chunk_size: int = 8000):
        
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = OpenAI(**client_kwargs)
        self.model = model
        self.dimensions = dimensions
        self.max_chunk_size = max_chunk_size
        
        logger.info(f"Initialized GDPR-optimized embeddings with model: {model}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed documents with GDPR/UK GDPR regulatory awareness"""
        all_embeddings = []
        
        for text in texts:
            chunks = self._chunk_text_by_regulatory_structure(text)
            
            if len(chunks) == 1:
                embedding = self._get_single_embedding(chunks[0])
                all_embeddings.append(embedding)
            else:
                # Weighted averaging for multi-chunk documents
                chunk_embeddings = []
                weights = []
                
                for chunk in chunks:
                    chunk_embedding = self._get_single_embedding(chunk)
                    chunk_embeddings.append(chunk_embedding)
                    
                    # Weight GDPR/UK GDPR content higher
                    weight = self._calculate_gdpr_weight(chunk)
                    weights.append(weight)
                
                # Compute weighted average
                total_weight = sum(weights)
                if total_weight > 0:
                    avg_embedding = [
                        sum(emb[i] * weights[j] for j, emb in enumerate(chunk_embeddings)) / total_weight
                        for i in range(len(chunk_embeddings[0]))
                    ]
                else:
                    avg_embedding = chunk_embeddings[0]
                
                all_embeddings.append(avg_embedding)
        
        return all_embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query with GDPR regulatory context"""
        chunks = self._chunk_text_by_regulatory_structure(text)
        return self._get_single_embedding(chunks[0])
    
    def _chunk_text_by_regulatory_structure(self, text: str) -> List[str]:
        """Intelligent chunking for GDPR/UK GDPR regulatory documents"""
        if len(text) <= self.max_chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chunk_size
            
            if end < len(text):
                # Look for GDPR-specific break points
                break_points = [
                    ('Article ', -50), ('Section ', -30), ('Clause ', -20),
                    ('Chapter ', -40), ('Recital ', -30), ('Paragraph ', -10),
                    ('.\n\n', 0), ('.\n', 0), ('. ', 0), ('\n', 0), (' ', 0)
                ]
                
                chunk_end = end
                for break_point, offset in break_points:
                    pos = text.rfind(break_point, start, end + offset)
                    if pos > start:
                        chunk_end = pos + len(break_point)
                        break
                
                end = chunk_end
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def _calculate_gdpr_weight(self, text: str) -> float:
        """Calculate weight based on GDPR/UK GDPR content density"""
        gdpr_terms = [
            'gdpr', 'general data protection regulation', 'uk gdpr', 'data protection',
            'personal data', 'processing', 'controller', 'processor', 'consent',
            'lawful basis', 'legitimate interest', 'data subject', 'supervisory authority',
            'adequacy decision', 'standard contractual clauses', 'binding corporate rules',
            'article 6', 'article 9', 'article 30', 'article 44', 'article 45', 'article 46',
            'ropa', 'record of processing', 'cross-border transfer', 'third country',
            'data protection impact assessment', 'dpia', 'data protection officer', 'dpo',
            'breach notification', 'right to erasure', 'right of access', 'data portability'
        ]
        
        text_lower = text.lower()
        matches = sum(1 for term in gdpr_terms if term in text_lower)
        word_count = len(text.split())
        
        base_weight = 1.0
        if word_count > 0:
            density = matches / word_count
            base_weight += min(density * 8, 3.0)  # Higher weighting for GDPR content
        
        return base_weight
    
    def _get_single_embedding(self, text: str) -> List[float]:
        """Get embedding for single text chunk"""
        try:
            params = {"input": text, "model": self.model}
            if self.dimensions:
                params["dimensions"] = self.dimensions
            
            response = self.client.embeddings.create(**params)
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to get embedding: {e}")
            raise

class EnhancedGDPRProcessor:
    """Enhanced processor for GDPR/UK GDPR regulatory and financial documents"""
    
    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 300):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n", "\n", "Article ", "Section ", "Clause ", 
                "Chapter ", "Part ", "Schedule ", "Recital ", "Annex ",
                ".", "!", "?", ";", ":", " "
            ]
        )
        
        # Enhanced GDPR/UK GDPR specific patterns
        self.gdpr_patterns = {
            'gdpr_articles': r'Article\s+(6|7|8|9|10|13|14|15|16|17|18|19|20|21|22|30|32|35|44|45|46|49)',
            'uk_gdpr_sections': r'(Section|Schedule)\s+(\d+)',
            'article_references': r'Article\s+(\d+(?:\.\d+)*)',
            'recital_references': r'Recital\s+(\d+)',
            'legal_bases': r'(Article\s+6\s*\([a-f]\)|legitimate\s+interest|consent|contract|legal\s+obligation|vital\s+interest|public\s+task)',
            'special_categories': r'(Article\s+9|special\s+categor(?:y|ies)|sensitive\s+data|biometric|health|genetic|political|religious|racial|ethnic)',
            'data_categories': r'(personal\s+data|identifiable\s+information|financial\s+data|location\s+data|online\s+identifier)',
            'processing_purposes': r'(processing\s+(?:for|purpose)|purpose(?:s)?\s+of|lawful\s+basis)',
            'retention_periods': r'(\d+\s+(?:days?|months?|years?)|retention\s+period|storage\s+limitation)',
            'cross_border_transfers': r'(third\s+countr(?:y|ies)|international\s+transfer|adequacy\s+decision|standard\s+contractual\s+clauses|binding\s+corporate\s+rules)',
            'data_subject_rights': r'(right\s+to\s+(?:access|rectification|erasure|portability|restriction|object)|data\s+subject\s+rights|right\s+of\s+access)',
            'supervisory_authorities': r'(supervisory\s+authority|data\s+protection\s+authority|ICO|EDPB|lead\s+supervisory\s+authority)',
            'gdpr_principles': r'(lawfulness|fairness|transparency|purpose\s+limitation|data\s+minimization|accuracy|storage\s+limitation|accountability)',
            'financial_regulations': r'(MiFID|PSD2|AML|KYC|Basel|SEPA|open\s+banking|strong\s+customer\s+authentication)',
            'territorial_scope': r'(EU|UK|EEA|European\s+Union|United\s+Kingdom|establishment|targeting|monitoring)',
            'adequacy_countries': r'(Andorra|Argentina|Canada|Faroe\s+Islands|Guernsey|Israel|Isle\s+of\s+Man|Japan|Jersey|New\s+Zealand|Korea|Switzerland|Uruguay)',
            'transfer_mechanisms': r'(SCC|SCCs|BCR|BCRs|adequacy\s+decision|derogation|certification|binding\s+corporate\s+rules)'
        }
    
    def extract_pdf_content(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract and analyze PDF content with enhanced GDPR processing"""
        logger.info(f"Processing GDPR regulatory document: {pdf_path}")
        
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            metadata = {
                "pages": len(doc), 
                "document_type": self._detect_gdpr_document_type(pdf_path),
                "source_file": os.path.basename(pdf_path)
            }
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            
            # Enhanced chunking with GDPR awareness
            text_chunks = self.text_splitter.split_text(full_text)
            
            chunks = []
            for i, chunk_text in enumerate(text_chunks):
                # Extract GDPR patterns
                gdpr_matches = self._extract_gdpr_patterns(chunk_text)
                
                # Calculate GDPR relevance score
                relevance_score = self._calculate_gdpr_relevance_score(chunk_text, gdpr_matches)
                
                chunk = {
                    "chunk_id": f"{Path(pdf_path).stem}_chunk_{i}",
                    "text": chunk_text,
                    "chunk_index": i,
                    "source": pdf_path,
                    "document_type": metadata["document_type"],
                    "gdpr_patterns": gdpr_matches,
                    "metadata": {
                        "word_count": len(chunk_text.split()),
                        "char_count": len(chunk_text),
                        "gdpr_relevance_score": relevance_score,
                        "contains_gdpr_articles": len(gdpr_matches.get("gdpr_articles", [])) > 0,
                        "contains_legal_bases": len(gdpr_matches.get("legal_bases", [])) > 0,
                        "territorial_mentions": gdpr_matches.get("territorial_scope", []),
                        "financial_relevance": len(gdpr_matches.get("financial_regulations", [])) > 0,
                        "transfer_mechanisms": gdpr_matches.get("transfer_mechanisms", [])
                    }
                }
                chunks.append(chunk)
            
            logger.info(f"Created {len(chunks)} GDPR-enhanced chunks from {pdf_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process PDF {pdf_path}: {e}")
            raise
    
    def _detect_gdpr_document_type(self, pdf_path: str) -> str:
        """Enhanced GDPR document type detection"""
        filename = os.path.basename(pdf_path).lower()
        
        gdpr_type_patterns = {
            "gdpr_regulation": ["gdpr", "general data protection regulation", "regulation 2016/679"],
            "uk_gdpr": ["uk gdpr", "data protection act 2018", "uk data protection"],
            "policy": ["privacy policy", "data protection policy", "processing policy"],
            "procedure": ["procedure", "process", "workflow", "guidance"],
            "compliance": ["compliance", "audit", "assessment", "report", "gap analysis"],
            "contract": ["contract", "agreement", "scc", "standard contractual clauses", "dpa"],
            "ropa": ["ropa", "record of processing", "processing activities", "data inventory"],
            "dpia": ["dpia", "data protection impact assessment", "privacy impact"],
            "financial": ["financial", "banking", "payment", "mifid", "psd2"],
            "transfer": ["transfer", "adequacy", "binding corporate rules", "bcr"],
            "breach": ["breach", "incident", "notification", "security"],
            "training": ["training", "awareness", "education", "guidelines"]
        }
        
        for doc_type, patterns in gdpr_type_patterns.items():
            if any(pattern in filename for pattern in patterns):
                return doc_type
        
        return "general_gdpr"
    
    def _extract_gdpr_patterns(self, text: str) -> Dict[str, List[str]]:
        """Enhanced GDPR pattern extraction"""
        matches = {}
        
        for pattern_name, pattern in self.gdpr_patterns.items():
            found_matches = re.findall(pattern, text, re.IGNORECASE)
            # Flatten tuples and remove duplicates
            if found_matches:
                if isinstance(found_matches[0], tuple):
                    matches[pattern_name] = list(set([match for match_tuple in found_matches for match in match_tuple if match]))
                else:
                    matches[pattern_name] = list(set(found_matches))
            else:
                matches[pattern_name] = []
        
        return matches
    
    def _calculate_gdpr_relevance_score(self, text: str, patterns: Dict[str, List[str]]) -> float:
        """Calculate GDPR relevance score"""
        score = 0.0
        
        # Pattern-based scoring with GDPR-specific weights
        weights = {
            'gdpr_articles': 5.0,
            'uk_gdpr_sections': 4.5,
            'legal_bases': 3.0,
            'special_categories': 3.5,
            'data_categories': 2.5,
            'cross_border_transfers': 4.0,
            'data_subject_rights': 3.0,
            'supervisory_authorities': 2.5,
            'adequacy_countries': 3.5,
            'transfer_mechanisms': 4.0,
            'territorial_scope': 3.0
        }
        
        for pattern, weight in weights.items():
            matches = patterns.get(pattern, [])
            score += len(matches) * weight
        
        # Normalize by text length
        word_count = len(text.split())
        if word_count > 0:
            score = score / word_count * 100  # Scale to percentage-like score
        
        return min(score, 15.0)  # Cap at 15.0 for very high relevance

@tool
def gdpr_synonym_generation_agent(concept_text: str, context: str = "") -> Dict[str, Any]:
    """Enhanced agent for generating GDPR/UK GDPR specific synonyms using LLM reasoning"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""As an expert in GDPR and UK GDPR regulations, generate comprehensive synonyms and related terms for the following regulatory concept:

Concept: {concept_text}
Context: {context}

Focus specifically on GDPR and UK GDPR terminology and variations. Generate synonyms considering:

1. EU GDPR terminology (Regulation 2016/679)
2. UK GDPR variations (Data Protection Act 2018)
3. Pre and post-Brexit terminology differences
4. Financial sector specific applications under GDPR/UK GDPR
5. Cross-border and territorial scope variations
6. Technical and legal variations within GDPR framework
7. Supervisory authority specific terminology
8. Transfer mechanism terminology

For each primary concept, provide:
- Direct GDPR/UK GDPR synonyms and abbreviations
- EU vs UK terminology variations
- Financial sector specific terms under GDPR
- Technical variations within GDPR context
- Related GDPR concepts often used interchangeably

Return detailed JSON with comprehensive GDPR-focused synonym mappings:

{{
    "primary_concept": "{concept_text}",
    "direct_synonyms": ["synonym1", "synonym2", "..."],
    "abbreviations": ["abbrev1", "abbrev2", "..."],
    "gdpr_variants": {{
        "eu_gdpr": ["EU GDPR specific term1", "term2"],
        "uk_gdpr": ["UK GDPR specific term1", "term2"],
        "pre_brexit": ["pre-Brexit term1", "term2"],
        "post_brexit": ["post-Brexit term1", "term2"]
    }},
    "financial_sector_terms": ["GDPR banking term1", "GDPR insurance term2", "GDPR payment term3"],
    "technical_variations": ["GDPR tech term1", "GDPR tech term2"],
    "related_gdpr_concepts": ["related GDPR concept1", "related concept2"],
    "gdpr_acronyms": ["GDPR ACRONYM1", "ACRONYM2"],
    "formal_gdpr_terms": ["formal Article reference1", "formal term2"],
    "informal_business_terms": ["business GDPR term1", "business term2"],
    "cross_reference_terms": ["Article cross-ref1", "regulatory cross-ref2"],
    "contextual_usage": {{
        "data_processing": ["GDPR processing term1", "processing term2"],
        "compliance": ["GDPR compliance term1", "compliance term2"],
        "audit": ["GDPR audit term1", "audit term2"],
        "transfer": ["GDPR transfer term1", "transfer term2"],
        "breach_management": ["GDPR breach term1", "breach term2"]
    }},
    "territorial_variations": {{
        "eu_context": ["EU specific usage1", "usage2"],
        "uk_context": ["UK specific usage1", "usage2"],
        "cross_border": ["cross-border usage1", "usage2"]
    }},
    "confidence_score": 0.95,
    "reasoning": "Detailed explanation of why these GDPR/UK GDPR synonyms were chosen and their regulatory context"
}}

Focus on creating the most comprehensive GDPR/UK GDPR synonym map possible for global compliance where these regulations apply."""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        # Extract JSON from response
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate and ensure all required fields exist
            required_fields = [
                "primary_concept", "direct_synonyms", "abbreviations", 
                "gdpr_variants", "financial_sector_terms", 
                "technical_variations", "related_gdpr_concepts", "confidence_score"
            ]
            
            for field in required_fields:
                if field not in result:
                    if field == "confidence_score":
                        result[field] = 0.8
                    elif field == "gdpr_variants":
                        result[field] = {"eu_gdpr": [], "uk_gdpr": []}
                    else:
                        result[field] = []
            
            return result
        else:
            logger.warning(f"Could not extract JSON from GDPR synonym generation response for: {concept_text}")
            return {
                "primary_concept": concept_text,
                "direct_synonyms": [],
                "abbreviations": [],
                "gdpr_variants": {"eu_gdpr": [], "uk_gdpr": []},
                "financial_sector_terms": [],
                "technical_variations": [],
                "related_gdpr_concepts": [],
                "confidence_score": 0.0,
                "reasoning": "Failed to parse LLM response"
            }
    
    except Exception as e:
        logger.error(f"GDPR synonym generation failed for '{concept_text}': {e}")
        return {
            "primary_concept": concept_text,
            "direct_synonyms": [],
            "abbreviations": [],
            "gdpr_variants": {"eu_gdpr": [], "uk_gdpr": []},
            "financial_sector_terms": [],
            "technical_variations": [],
            "related_gdpr_concepts": [],
            "confidence_score": 0.0,
            "reasoning": f"Error during generation: {str(e)}"
        }

@tool
def gdpr_territorial_analysis_agent(text: str) -> Dict[str, Any]:
    """Agent for analyzing GDPR/UK GDPR territorial scope and extraterritorial applicability"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze this text to identify GDPR and UK GDPR territorial scope and extraterritorial applicability. Focus on dynamically discovering where these regulations apply:

Text: {text}

Perform comprehensive analysis to extract:

1. GDPR TERRITORIAL SCOPE (Article 3):
   - Organizations established in EU/EEA
   - Processing in context of EU establishment
   - Extraterritorial triggers for non-EU entities

2. UK GDPR TERRITORIAL SCOPE:
   - Organizations established in UK
   - Processing in context of UK establishment  
   - Extraterritorial triggers for non-UK entities

3. EXTRATERRITORIAL APPLICABILITY DISCOVERY:
   - Which non-EU/UK jurisdictions are affected by GDPR/UK GDPR
   - Specific triggering conditions (targeting, monitoring, offering services)
   - Cross-border business operations subject to GDPR/UK GDPR

4. ADEQUACY DECISIONS AND TRANSFERS:
   - Countries with adequacy decisions
   - Transfer mechanisms required for different jurisdictions
   - Bridge arrangements between EU and UK

5. DYNAMIC JURISDICTION DISCOVERY:
   - Countries/regions mentioned in processing context
   - Business operations that trigger GDPR/UK GDPR compliance
   - Global entities subject to these regulations

6. FINANCIAL SECTOR TERRITORIAL IMPLICATIONS:
   - Cross-border banking under GDPR/UK GDPR
   - International payment processing compliance
   - Global financial service providers affected

Return comprehensive JSON analysis focusing on territorial applicability:

{{
    "gdpr_territorial_analysis": {{
        "eu_scope": {{
            "direct_applicability": ["EU member states", "EEA countries"],
            "establishment_triggers": ["conditions for EU establishment"],
            "processing_context": ["when processing is in EU context"]
        }},
        "extraterritorial_triggers": [
            {{
                "trigger_type": "offering_goods_services|monitoring_behavior|targeting_market",
                "affected_jurisdiction": "country/region name",
                "trigger_conditions": ["specific conditions that trigger GDPR"],
                "compliance_implications": ["what compliance is required"]
            }}
        ]
    }},
    "uk_gdpr_territorial_analysis": {{
        "uk_scope": {{
            "direct_applicability": ["England", "Wales", "Scotland", "Northern Ireland"],
            "establishment_triggers": ["conditions for UK establishment"],
            "processing_context": ["when processing is in UK context"]
        }},
        "extraterritorial_triggers": [
            {{
                "trigger_type": "offering_goods_services|monitoring_behavior|targeting_market",
                "affected_jurisdiction": "country/region name", 
                "trigger_conditions": ["specific conditions that trigger UK GDPR"],
                "compliance_implications": ["what compliance is required"]
            }}
        ]
    }},
    "adequacy_and_transfers": {{
        "eu_adequacy_decisions": [
            {{
                "country": "country name",
                "adequacy_status": "adequate|under_review|suspended",
                "transfer_implications": "how this affects transfers",
                "special_conditions": ["any special conditions"]
            }}
        ],
        "uk_adequacy_arrangements": [
            {{
                "country": "country name",
                "adequacy_status": "adequate|bridge_arrangement|under_review",
                "transfer_implications": "how this affects transfers",
                "special_conditions": ["any special conditions"]
            }}
        ],
        "transfer_mechanisms_required": [
            {{
                "origin": "data origin jurisdiction",
                "destination": "data destination jurisdiction", 
                "gdpr_mechanism": "SCC|BCR|adequacy|derogation|certification",
                "uk_gdpr_mechanism": "IDTA|SCC|BCR|adequacy|derogation",
                "additional_safeguards": ["required additional measures"]
            }}
        ]
    }},
    "dynamically_affected_jurisdictions": [
        {{
            "jurisdiction": "country/region name",
            "gdpr_applicability": "direct|extraterritorial|transfer_only|not_applicable",
            "uk_gdpr_applicability": "direct|extraterritorial|transfer_only|not_applicable",
            "applicability_basis": "establishment|targeting|monitoring|transfers",
            "business_context": "why this jurisdiction is affected",
            "compliance_requirements": ["specific requirements for this jurisdiction"],
            "data_flows": ["types of data flows that trigger compliance"]
        }}
    ],
    "financial_sector_territorial_impacts": [
        {{
            "financial_activity": "banking|insurance|payments|investment",
            "affected_jurisdictions": ["jurisdictions where activity triggers GDPR/UK GDPR"],
            "territorial_triggers": ["what makes this activity subject to GDPR/UK GDPR"],
            "compliance_implications": ["specific compliance requirements"],
            "cross_border_considerations": ["cross-border transfer implications"]
        }}
    ],
    "processing_activities_territorial": [
        {{
            "activity": "processing activity name",
            "gdpr_territorial_scope": "where GDPR applies for this activity",
            "uk_gdpr_territorial_scope": "where UK GDPR applies for this activity",
            "affected_jurisdictions": ["jurisdictions involved in this activity"],
            "territorial_complexity": "assessment of territorial complexity"
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate required fields
            required_fields = [
                "gdpr_territorial_analysis", "uk_gdpr_territorial_analysis", 
                "adequacy_and_transfers", "dynamically_affected_jurisdictions",
                "financial_sector_territorial_impacts", "processing_activities_territorial"
            ]
            
            for field in required_fields:
                if field not in result:
                    if field.endswith("_analysis"):
                        result[field] = {"eu_scope": {}, "extraterritorial_triggers": []}
                    else:
                        result[field] = []
            
            return result
        else:
            return {field: [] if not field.endswith("_analysis") else {"eu_scope": {}, "extraterritorial_triggers": []} 
                   for field in ["gdpr_territorial_analysis", "uk_gdpr_territorial_analysis", 
                                "adequacy_and_transfers", "dynamically_affected_jurisdictions",
                                "financial_sector_territorial_impacts", "processing_activities_territorial"]}
    
    except Exception as e:
        logger.error(f"GDPR territorial analysis failed: {e}")
        return {field: [] if not field.endswith("_analysis") else {"eu_scope": {}, "extraterritorial_triggers": []} 
               for field in ["gdpr_territorial_analysis", "uk_gdpr_territorial_analysis", 
                            "adequacy_and_transfers", "dynamically_affected_jurisdictions",
                            "financial_sector_territorial_impacts", "processing_activities_territorial"]}

@tool
def comprehensive_gdpr_extraction_agent(text: str) -> Dict[str, Any]:
    """Comprehensive agent for extracting GDPR and UK GDPR regulatory concepts with RoPA focus"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze this regulatory/financial document text to extract comprehensive GDPR and UK GDPR compliance information with focus on Record of Processing Activities (RoPA):

Text: {text}

Extract and analyze for RoPA metamodel creation:

1. PROCESSING ACTIVITIES (Article 30 GDPR):
   - Specific data processing operations
   - Purposes of processing
   - Categories of data subjects
   - Categories of personal data
   - Retention periods
   - Recipients of data
   - International transfers

2. DATA CATEGORIES AND CLASSIFICATION:
   - Personal data types
   - Special categories (Article 9)
   - Financial data categories
   - Sensitivity classifications
   - Data minimization considerations

3. LEGAL BASES (Article 6 & 9):
   - Lawful bases for processing
   - Conditions for special categories
   - Consent mechanisms
   - Legitimate interests assessments

4. ORGANIZATIONAL ENTITIES AND ROLES:
   - Data controllers and processors
   - Joint controllers
   - Data Protection Officers (DPOs)
   - Representatives
   - Supervisory authorities

5. SECURITY AND RISK MANAGEMENT (Article 32):
   - Technical and organizational measures
   - Security safeguards
   - Risk assessment procedures
   - Breach management procedures

6. CROSS-BORDER TRANSFERS (Chapter V):
   - International data transfers
   - Transfer mechanisms and safeguards
   - Adequacy decisions
   - Third country assessments

7. FINANCIAL SECTOR GDPR SPECIFICS:
   - Banking and payment processing under GDPR
   - Investment and trading data compliance
   - Insurance data handling
   - Regulatory reporting requirements

8. COMPLIANCE FRAMEWORKS:
   - GDPR compliance obligations
   - UK GDPR variations
   - Audit and monitoring requirements
   - Documentation standards

Return comprehensive JSON analysis for RoPA metamodel:

{{
    "processing_activities": [
        {{
            "activity_name": "specific processing activity",
            "purpose": "purpose of processing",
            "lawful_basis": "Article 6 basis",
            "special_category_condition": "Article 9 condition if applicable",
            "data_categories": ["category1", "category2"],
            "data_subjects": ["subject category1", "subject category2"],
            "recipients": ["recipient1", "recipient2"],
            "retention_period": "retention description",
            "international_transfers": ["country1", "country2"],
            "automated_decision_making": true/false,
            "gdpr_article_30_compliant": true/false
        }}
    ],
    "data_categories": [
        {{
            "category": "data category name",
            "gdpr_classification": "personal_data|special_category|financial_data",
            "sensitivity_level": "normal|sensitive|highly_sensitive",
            "examples": ["example1", "example2"],
            "article_9_applicable": true/false,
            "protection_requirements": ["requirement1", "requirement2"],
            "minimization_considerations": ["consideration1", "consideration2"]
        }}
    ],
    "legal_bases": [
        {{
            "basis_type": "consent|contract|legal_obligation|vital_interests|public_task|legitimate_interests",
            "gdpr_article": "Article 6(1)(a-f)",
            "description": "basis description",
            "conditions": ["condition1", "condition2"],
            "withdrawal_mechanism": "how consent can be withdrawn if applicable",
            "balancing_test": "legitimate interests balancing if applicable"
        }}
    ],
    "organizational_entities": [
        {{
            "entity_name": "entity name",
            "role": "controller|processor|joint_controller|dpo|representative",
            "gdpr_responsibilities": ["responsibility1", "responsibility2"],
            "establishment": "EU|UK|third_country",
            "contact_details": "contact information",
            "supervisory_authority": "relevant SA"
        }}
    ],
    "security_measures": [
        {{
            "measure_type": "technical|organizational",
            "measure_name": "specific measure",
            "article_32_compliance": true/false,
            "description": "measure description",
            "implementation_requirements": ["requirement1", "requirement2"],
            "risk_mitigation": ["risk1", "risk2"]
        }}
    ],
    "cross_border_transfers": [
        {{
            "origin": "origin jurisdiction",
            "destination": "destination jurisdiction",
            "transfer_mechanism": "adequacy|scc|bcr|derogation|certification",
            "adequacy_decision_exists": true/false,
            "safeguards": ["safeguard1", "safeguard2"],
            "transfer_impact_assessment": "TIA requirements",
            "chapter_5_compliance": true/false
        }}
    ],
    "financial_specifics": [
        {{
            "financial_activity": "banking|insurance|investment|payments",
            "gdpr_requirements": ["requirement1", "requirement2"],
            "data_types": ["financial data type1", "type2"],
            "regulatory_overlap": ["overlapping regulation1", "regulation2"],
            "compliance_complexity": "low|medium|high"
        }}
    ],
    "compliance_obligations": [
        {{
            "obligation_type": "ropa|dpia|breach_notification|dpo_appointment|policy",
            "gdpr_article": "relevant GDPR article",
            "uk_gdpr_variation": "any UK GDPR differences",
            "deadline": "compliance deadline",
            "documentation_required": ["document1", "document2"]
        }}
    ],
    "key_concepts": [
        {{
            "concept": "GDPR/UK GDPR concept name",
            "definition": "regulatory definition",
            "article_reference": "GDPR/UK GDPR article",
            "ropa_relevance": "how this relates to RoPA",
            "importance": "high|medium|low"
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate required fields
            required_fields = [
                "processing_activities", "data_categories", "legal_bases",
                "organizational_entities", "security_measures", "cross_border_transfers",
                "financial_specifics", "compliance_obligations", "key_concepts"
            ]
            
            for field in required_fields:
                if field not in result:
                    result[field] = []
            
            return result
        else:
            return {field: [] for field in [
                "processing_activities", "data_categories", "legal_bases",
                "organizational_entities", "security_measures", "cross_border_transfers",
                "financial_specifics", "compliance_obligations", "key_concepts"
            ]}
    
    except Exception as e:
        logger.error(f"Comprehensive GDPR extraction failed: {e}")
        return {field: [] for field in [
            "processing_activities", "data_categories", "legal_bases",
            "organizational_entities", "security_measures", "cross_border_transfers",
            "financial_specifics", "compliance_obligations", "key_concepts"
        ]}

@tool
def gdpr_metamodel_generation_agent(consolidated_data: str) -> Dict[str, Any]:
    """Agent for generating comprehensive GDPR/UK GDPR RoPA metamodel"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Based on comprehensive analysis of GDPR and UK GDPR regulatory documents, create a detailed metamodel for Record of Processing Activities (RoPA) that supports global compliance where GDPR/UK GDPR applies.

Consolidated Analysis Data: {consolidated_data}

Create a comprehensive RoPA metamodel based on Article 30 GDPR and UK GDPR equivalent that includes:

1. CORE ROPA METAMODEL ENTITIES (Article 30 compliant):
   - Processing Activity (controller and processor variants)
   - Data Categories and Special Categories
   - Legal Bases and Conditions
   - Data Subjects and Recipients
   - International Transfers
   - Security Measures

2. TERRITORIAL SCOPE FRAMEWORK:
   - GDPR territorial applicability mechanisms
   - UK GDPR territorial scope
   - Extraterritorial application triggers
   - Cross-border compliance requirements

3. FINANCIAL SECTOR GDPR SPECIALIZATIONS:
   - Banking processing activities under GDPR
   - Insurance data handling compliance
   - Investment management requirements
   - Payment processing frameworks
   - Cross-border financial services

4. COMPLIANCE VALIDATION FRAMEWORK:
   - Article 30 compliance validation
   - UK GDPR variations handling
   - Supervisory authority requirements
   - Audit and documentation standards

Return comprehensive RoPA metamodel specification:

{{
    "ropa_metamodel_specification": {{
        "version": "3.0.0",
        "scope": "GDPR and UK GDPR Record of Processing Activities with Global Applicability",
        "compliance_framework": "Article 30 GDPR and UK GDPR equivalent",
        "core_entities": [
            {{
                "entity_name": "ProcessingActivity",
                "entity_type": "controller|processor",
                "gdpr_article": "Article 30(1) or 30(2)",
                "required_attributes": [
                    {{
                        "name": "activity_name",
                        "type": "string",
                        "gdpr_requirement": "Article 30(1)(a) - purposes of processing",
                        "validation_rules": ["rule1", "rule2"],
                        "uk_gdpr_variation": "any differences in UK GDPR"
                    }}
                ],
                "optional_attributes": [
                    {{
                        "name": "attribute_name", 
                        "type": "string|date|array",
                        "gdpr_context": "which GDPR provision this supports",
                        "territorial_variations": ["EU specific", "UK specific"]
                    }}
                ],
                "relationships": [
                    {{
                        "target_entity": "DataCategory",
                        "relationship_type": "processes",
                        "cardinality": "1..*",
                        "gdpr_requirement": "Article 30(1)(c) - categories of personal data"
                    }}
                ],
                "territorial_extensions": {{
                    "eu_gdpr": ["EU specific requirements"],
                    "uk_gdpr": ["UK specific requirements"],
                    "extraterritorial": ["requirements for non-EU/UK entities"]
                }}
            }}
        ],
        "territorial_compliance_framework": [
            {{
                "territory": "EU_GDPR|UK_GDPR|EXTRATERRITORIAL",
                "applicability_conditions": ["condition1", "condition2"],
                "mandatory_ropa_elements": ["element1", "element2"],
                "supervisory_authority_requirements": ["requirement1", "requirement2"],
                "documentation_language": ["acceptable languages"],
                "submission_requirements": ["when and how to submit to SA"]
            }}
        ],
        "financial_sector_extensions": [
            {{
                "financial_domain": "banking|insurance|investment|payments",
                "additional_ropa_requirements": ["requirement1", "requirement2"],
                "gdpr_financial_considerations": ["consideration1", "consideration2"],
                "regulatory_overlap": ["overlapping financial regulations"],
                "specialized_processing_activities": ["activity1", "activity2"]
            }}
        ],
        "transfer_framework": {{
            "international_transfer_entity": {{
                "required_attributes": ["destination_country", "transfer_mechanism", "safeguards"],
                "adequacy_decision_handling": "how to handle adequacy decisions",
                "scc_requirements": "SCC specific requirements",
                "bcr_requirements": "BCR specific requirements",
                "derogation_conditions": "Article 49 derogation conditions"
            }},
            "adequacy_mapping": {{
                "adequate_countries": ["list of adequate countries"],
                "partial_adequacy": ["countries with partial adequacy"],
                "bridge_arrangements": ["UK-EU bridge arrangements"]
            }}
        }}
    }},
    "implementation_guidance": {{
        "ropa_creation_process": [
            "step1: Identify all processing activities",
            "step2: Determine controller/processor role", 
            "step3: Map territorial applicability",
            "step4: Document Article 30 requirements",
            "step5: Validate compliance across territories"
        ],
        "territorial_assessment_process": [
            "step1: Assess establishment-based applicability",
            "step2: Evaluate extraterritorial triggers",
            "step3: Determine transfer requirements",
            "step4: Map supervisory authority jurisdiction"
        ],
        "validation_framework": {{
            "article_30_validation": ["validation rule1", "rule2"],
            "territorial_validation": ["territorial rule1", "rule2"],
            "financial_validation": ["financial sector rule1", "rule2"],
            "cross_border_validation": ["transfer rule1", "rule2"]
        }}
    }},
    "compliance_matrices": {{
        "gdpr_uk_gdpr_comparison": [
            {{
                "element": "RoPA element name",
                "gdpr_requirement": "EU GDPR requirement",
                "uk_gdpr_requirement": "UK GDPR requirement", 
                "differences": ["key differences"],
                "harmonization_approach": "how to handle both requirements"
            }}
        ],
        "territorial_requirements": [
            {{
                "jurisdiction": "jurisdiction name",
                "gdpr_applicability": "how GDPR applies",
                "uk_gdpr_applicability": "how UK GDPR applies",
                "ropa_obligations": ["specific RoPA obligations"],
                "documentation_requirements": ["what documentation is needed"]
            }}
        ]
    }}
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {"error": "Failed to parse GDPR RoPA metamodel specification"}
    
    except Exception as e:
        logger.error(f"GDPR metamodel generation failed: {e}")
        return {"error": f"GDPR metamodel generation failed: {str(e)}"}

class EnhancedGDPRVectorEngine:
    """Enhanced vector engine with GDPR-focused AI-powered synonym storage"""
    
    def __init__(self, 
                 host: str = "http://localhost:9200",
                 index_name: str = "gdpr_ropa_metamodel",
                 username: str = None,
                 password: str = None,
                 ca_certs: str = None,
                 verify_certs: bool = True,
                 openai_api_key: str = None,
                 openai_base_url: str = None):
        
        self.index_name = index_name
        self.synonym_index = f"{index_name}_gdpr_synonyms"
        
        # Initialize GDPR-optimized embeddings
        self.embeddings = CustomOpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=openai_api_key,
            base_url=openai_base_url,
            dimensions=3072
        )
        
        # Configure Elasticsearch client
        self.client = self._create_elasticsearch_client(
            host, username, password, ca_certs, verify_certs
        )
        
        self._create_gdpr_indices()
    
    def _create_elasticsearch_client(self, host, username, password, ca_certs, verify_certs):
        """Create Elasticsearch client with enhanced configuration"""
        if not host.startswith(('http://', 'https://')):
            raise ValueError(f"Elasticsearch host must include schema. Got: {host}")
        
        client_config = {
            "hosts": [host],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # Configure SSL/TLS
        if host.startswith('https://'):
            client_config["use_ssl"] = True
            client_config["verify_certs"] = verify_certs
            
            if ca_certs:
                client_config["ca_certs"] = ca_certs
                logger.info(f"Using CA certificate: {ca_certs}")
            
            if not verify_certs:
                client_config["ssl_show_warn"] = False
                logger.warning("SSL certificate verification disabled")
        
        # Configure authentication
        if username and password:
            client_config["basic_auth"] = (username, password)
            logger.info(f"Using basic authentication for user: {username}")
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch for GDPR processing")
                info = client.info()
                logger.info(f"Elasticsearch version: {info.body['version']['number']}")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch cluster")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_gdpr_indices(self):
        """Create GDPR-optimized indices for documents and synonyms"""
        
        # Main GDPR document index
        doc_mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer",
                                "keyword_repeat",
                                "remove_duplicates"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "document_type": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "gdpr_relevance_score": {"type": "float"},
                    
                    # GDPR-specific extractions
                    "gdpr_territorial_analysis": {"type": "nested"},
                    "uk_gdpr_territorial_analysis": {"type": "nested"},
                    "processing_activities": {"type": "nested"},
                    "data_categories": {"type": "nested"},
                    "legal_bases": {"type": "nested"},
                    "organizational_entities": {"type": "nested"},
                    "security_measures": {"type": "nested"},
                    "cross_border_transfers": {"type": "nested"},
                    "financial_specifics": {"type": "nested"},
                    "compliance_obligations": {"type": "nested"},
                    "key_concepts": {"type": "nested"},
                    "dynamically_affected_jurisdictions": {"type": "nested"},
                    "adequacy_and_transfers": {"type": "nested"},
                    
                    # GDPR patterns and metadata
                    "gdpr_patterns": {"type": "object"},
                    "metadata": {"type": "object"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # GDPR synonym index
        synonym_mapping = {
            "mappings": {
                "properties": {
                    "primary_concept": {"type": "keyword"},
                    "direct_synonyms": {"type": "keyword"},
                    "abbreviations": {"type": "keyword"},
                    "gdpr_variants": {
                        "type": "object",
                        "properties": {
                            "eu_gdpr": {"type": "keyword"},
                            "uk_gdpr": {"type": "keyword"},
                            "pre_brexit": {"type": "keyword"},
                            "post_brexit": {"type": "keyword"}
                        }
                    },
                    "financial_sector_terms": {"type": "keyword"},
                    "technical_variations": {"type": "keyword"},
                    "related_gdpr_concepts": {"type": "keyword"},
                    "territorial_variations": {
                        "type": "object",
                        "properties": {
                            "eu_context": {"type": "keyword"},
                            "uk_context": {"type": "keyword"},
                            "cross_border": {"type": "keyword"}
                        }
                    },
                    "confidence_score": {"type": "float"},
                    "context": {"type": "text"},
                    "reasoning": {"type": "text"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            # Create main index
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **doc_mapping)
                logger.info(f"Created GDPR document index: {self.index_name}")
            
            # Create synonym index
            if not self.client.indices.exists(index=self.synonym_index):
                self.client.indices.create(index=self.synonym_index, **synonym_mapping)
                logger.info(f"Created GDPR synonym index: {self.synonym_index}")
        
        except Exception as e:
            logger.error(f"Failed to create GDPR indices: {e}")
            raise
    
    def generate_and_store_gdpr_synonyms(self, extracted_concepts: List[Dict[str, Any]]) -> List[ConceptSynonym]:
        """Generate GDPR-specific synonyms using LLM and store them in Elasticsearch"""
        logger.info("Generating and storing GDPR-focused AI-powered synonyms...")
        
        synonyms = []
        processed_concepts = set()
        
        for extraction in extracted_concepts:
            # Extract unique GDPR concepts from various fields
            concepts_to_process = []
            
            # From key concepts
            for concept in extraction.get("key_concepts", []):
                concept_name = concept.get("concept", "").strip()
                if concept_name and concept_name not in processed_concepts:
                    contexts = [
                        concept.get("definition", ""),
                        concept.get("ropa_relevance", ""),
                        f"Article reference: {concept.get('article_reference', '')}"
                    ]
                    context = " | ".join([c for c in contexts if c])
                    concepts_to_process.append((concept_name, context))
            
            # From processing activities
            for activity in extraction.get("processing_activities", []):
                activity_name = activity.get("activity_name", "").strip()
                if activity_name and activity_name not in processed_concepts:
                    context = f"Purpose: {activity.get('purpose', '')} | Legal basis: {activity.get('lawful_basis', '')}"
                    concepts_to_process.append((activity_name, context))
            
            # From data categories
            for category in extraction.get("data_categories", []):
                category_name = category.get("category", "").strip()
                if category_name and category_name not in processed_concepts:
                    context = f"Classification: {category.get('gdpr_classification', '')} | Examples: {', '.join(category.get('examples', []))}"
                    concepts_to_process.append((category_name, context))
            
            # From legal bases
            for basis in extraction.get("legal_bases", []):
                basis_type = basis.get("basis_type", "").strip()
                if basis_type and basis_type not in processed_concepts:
                    context = f"GDPR Article: {basis.get('gdpr_article', '')} | Description: {basis.get('description', '')}"
                    concepts_to_process.append((basis_type, context))
            
            # Generate synonyms for each GDPR concept
            for concept_name, context in concepts_to_process:
                if concept_name in processed_concepts:
                    continue
                
                try:
                    # Generate GDPR-specific synonyms using LLM
                    synonym_data = gdpr_synonym_generation_agent.invoke(concept_name, context)
                    
                    # Create ConceptSynonym object
                    concept_synonym = ConceptSynonym(
                        primary_term=concept_name,
                        synonyms=synonym_data.get("direct_synonyms", []) + synonym_data.get("abbreviations", []),
                        context=context,
                        confidence=synonym_data.get("confidence_score", 0.8),
                        gdpr_variants=synonym_data.get("gdpr_variants", {}),
                        financial_sector_terms=synonym_data.get("financial_sector_terms", [])
                    )
                    
                    synonyms.append(concept_synonym)
                    processed_concepts.add(concept_name)
                    
                    # Store in Elasticsearch
                    self._store_gdpr_synonym_in_elasticsearch(synonym_data)
                    
                    logger.info(f"Generated GDPR synonyms for: {concept_name}")
                
                except Exception as e:
                    logger.error(f"Failed to generate GDPR synonyms for '{concept_name}': {e}")
                    continue
        
        logger.info(f"Generated and stored {len(synonyms)} GDPR concept synonyms")
        return synonyms
    
    def _store_gdpr_synonym_in_elasticsearch(self, synonym_data: Dict[str, Any]):
        """Store GDPR synonym data in Elasticsearch"""
        try:
            doc = {
                **synonym_data,
                "timestamp": datetime.now()
            }
            
            self.client.index(
                index=self.synonym_index,
                id=f"gdpr_synonym_{synonym_data['primary_concept']}",
                document=doc
            )
        except Exception as e:
            logger.error(f"Failed to store GDPR synonym: {e}")
    
    def index_documents_with_gdpr_extractions(self, chunks: List[Dict[str, Any]], 
                                              extractions: List[Dict[str, Any]], 
                                              territorial_analyses: List[Dict[str, Any]]):
        """Index documents with comprehensive GDPR extractions and territorial analysis"""
        logger.info("Indexing documents with comprehensive GDPR extractions...")
        
        for i, chunk in enumerate(chunks):
            extraction = extractions[i] if i < len(extractions) else {}
            territorial = territorial_analyses[i] if i < len(territorial_analyses) else {}
            
            # Generate embedding
            embedding = self.embeddings.embed_query(chunk["text"])
            
            doc = {
                "text": chunk["text"],
                "embedding": embedding,
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "document_type": chunk.get("document_type", "general_gdpr"),
                "gdpr_relevance_score": chunk.get("metadata", {}).get("gdpr_relevance_score", 0.0),
                
                # Store GDPR extractions
                "processing_activities": extraction.get("processing_activities", []),
                "data_categories": extraction.get("data_categories", []),
                "legal_bases": extraction.get("legal_bases", []),
                "organizational_entities": extraction.get("organizational_entities", []),
                "security_measures": extraction.get("security_measures", []),
                "cross_border_transfers": extraction.get("cross_border_transfers", []),
                "financial_specifics": extraction.get("financial_specifics", []),
                "compliance_obligations": extraction.get("compliance_obligations", []),
                "key_concepts": extraction.get("key_concepts", []),
                
                # Store territorial analysis
                "gdpr_territorial_analysis": territorial.get("gdpr_territorial_analysis", {}),
                "uk_gdpr_territorial_analysis": territorial.get("uk_gdpr_territorial_analysis", {}),
                "adequacy_and_transfers": territorial.get("adequacy_and_transfers", {}),
                "dynamically_affected_jurisdictions": territorial.get("dynamically_affected_jurisdictions", []),
                "financial_sector_territorial_impacts": territorial.get("financial_sector_territorial_impacts", []),
                "processing_activities_territorial": territorial.get("processing_activities_territorial", []),
                
                "gdpr_patterns": chunk.get("gdpr_patterns", {}),
                "metadata": chunk.get("metadata", {}),
                "timestamp": datetime.now()
            }
            
            try:
                self.client.index(index=self.index_name, id=chunk["chunk_id"], document=doc)
            except Exception as e:
                logger.error(f"Failed to index chunk {chunk['chunk_id']}: {e}")
        
        self.client.indices.refresh(index=self.index_name)
        logger.info(f"Successfully indexed {len(chunks)} GDPR documents")
    
    def search_with_gdpr_synonym_expansion(self, query: str, filters: Dict[str, Any] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Search with GDPR-specific automatic synonym expansion"""
        # Get GDPR synonyms for query terms
        expanded_query = self._expand_query_with_gdpr_synonyms(query)
        
        # Generate query embedding
        query_embedding = self.embeddings.embed_query(query)
        
        # Build search query
        must_clauses = [
            {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            }
        ]
        
        should_clauses = [
            {
                "multi_match": {
                    "query": expanded_query,
                    "fields": [
                        "text^1.0",
                        "processing_activities.activity_name^3.0",
                        "data_categories.category^2.5",
                        "key_concepts.concept^3.0",
                        "legal_bases.basis_type^2.0",
                        "dynamically_affected_jurisdictions.jurisdiction^2.0",
                        "cross_border_transfers.origin^1.5",
                        "cross_border_transfers.destination^1.5",
                        "compliance_obligations.obligation_type^2.0"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            }
        ]
        
        # Add filters if provided
        filter_clauses = []
        if filters:
            for field, value in filters.items():
                if isinstance(value, list):
                    filter_clauses.append({"terms": {field: value}})
                else:
                    filter_clauses.append({"term": {field: value}})
        
        search_body = {
            "query": {
                "bool": {
                    "must": must_clauses,
                    "should": should_clauses,
                    "filter": filter_clauses,
                    "minimum_should_match": 1
                }
            },
            "size": top_k,
            "_source": {
                "excludes": ["embedding"]
            },
            "sort": [
                {"_score": {"order": "desc"}},
                {"gdpr_relevance_score": {"order": "desc"}}
            ]
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_gdpr_search_results(response)
        except Exception as e:
            logger.error(f"GDPR search failed: {e}")
            return []
    
    def _expand_query_with_gdpr_synonyms(self, query: str) -> str:
        """Expand query with stored GDPR synonyms"""
        try:
            # Search for GDPR synonyms
            synonym_search = {
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"primary_concept": query}},
                            {"match": {"direct_synonyms": query}},
                            {"match": {"abbreviations": query}},
                            {"nested": {
                                "path": "gdpr_variants",
                                "query": {
                                    "bool": {
                                        "should": [
                                            {"match": {"gdpr_variants.eu_gdpr": query}},
                                            {"match": {"gdpr_variants.uk_gdpr": query}}
                                        ]
                                    }
                                }
                            }}
                        ]
                    }
                },
                "size": 10
            }
            
            response = self.client.search(index=self.synonym_index, **synonym_search)
            
            expanded_terms = [query]
            
            if hasattr(response, 'body'):
                hits = response.body["hits"]["hits"]
            else:
                hits = response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                expanded_terms.extend(source.get("direct_synonyms", []))
                expanded_terms.extend(source.get("abbreviations", []))
                expanded_terms.extend(source.get("technical_variations", []))
                expanded_terms.extend(source.get("related_gdpr_concepts", []))
                
                # Add GDPR variants
                gdpr_variants = source.get("gdpr_variants", {})
                for variant_type, terms in gdpr_variants.items():
                    expanded_terms.extend(terms)
                
                # Add territorial variations
                territorial_vars = source.get("territorial_variations", {})
                for territory, terms in territorial_vars.items():
                    expanded_terms.extend(terms)
            
            # Remove duplicates and return
            unique_terms = list(set(expanded_terms))
            return " ".join(unique_terms)
        
        except Exception as e:
            logger.warning(f"GDPR synonym expansion failed: {e}")
            return query
    
    def _format_gdpr_search_results(self, response: Dict) -> List[Dict[str, Any]]:
        """Format GDPR search results"""
        results = []
        
        try:
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "text": source["text"],
                    "chunk_id": source["chunk_id"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "document_type": source.get("document_type", "general_gdpr"),
                    "gdpr_relevance_score": source.get("gdpr_relevance_score", 0.0),
                    "gdpr_territorial_analysis": source.get("gdpr_territorial_analysis", {}),
                    "uk_gdpr_territorial_analysis": source.get("uk_gdpr_territorial_analysis", {}),
                    "processing_activities": source.get("processing_activities", []),
                    "key_concepts": source.get("key_concepts", []),
                    "dynamically_affected_jurisdictions": source.get("dynamically_affected_jurisdictions", []),
                    "metadata": source.get("metadata", {})
                }
                results.append(result)
        except Exception as e:
            logger.error(f"Error formatting GDPR search results: {e}")
        
        return results

class EnhancedGDPRGraphEngine:
    """Enhanced graph engine with GDPR-focused synonym integration"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, password: str = None):
        try:
            if password:
                self.db = FalkorDB(host=host, port=port, password=password)
            else:
                self.db = FalkorDB(host=host, port=port)
            
            self.graph = self.db.select_graph("gdpr_ropa_knowledge_graph")
            logger.info(f"Connected to FalkorDB for GDPR graph at {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def build_gdpr_knowledge_graph(self, 
                                   chunks: List[Dict[str, Any]], 
                                   extractions: List[Dict[str, Any]], 
                                   territorial_analyses: List[Dict[str, Any]],
                                   synonyms: List[ConceptSynonym]):
        """Build comprehensive GDPR knowledge graph with synonyms and territorial analysis"""
        logger.info("Building comprehensive GDPR knowledge graph...")
        
        try:
            # Clear existing graph
            self.graph.query("MATCH (n) DETACH DELETE n")
            
            # Create GDPR framework nodes
            self._create_gdpr_framework_nodes()
            
            # Create synonym nodes and relationships
            self._create_gdpr_synonym_nodes(synonyms)
            
            # Process document chunks, extractions, and territorial analysis
            for i, chunk in enumerate(chunks):
                extraction = extractions[i] if i < len(extractions) else {}
                territorial = territorial_analyses[i] if i < len(territorial_analyses) else {}
                self._process_gdpr_chunk_extraction(chunk, extraction, territorial)
            
            # Create advanced GDPR relationships
            self._create_gdpr_relationships()
            
            logger.info("GDPR knowledge graph built successfully")
        
        except Exception as e:
            logger.error(f"Failed to build GDPR knowledge graph: {e}")
            raise
    
    def _create_gdpr_framework_nodes(self):
        """Create GDPR and UK GDPR framework nodes"""
        for framework_type, framework in GDPR_FRAMEWORKS.items():
            framework_dict = asdict(framework)
            
            # Escape single quotes for Cypher
            for key, value in framework_dict.items():
                if isinstance(value, str):
                    framework_dict[key] = value.replace("'", "\\'")
                elif isinstance(value, list):
                    framework_dict[key] = [v.replace("'", "\\'") if isinstance(v, str) else v for v in value]
            
            query = f"""
            CREATE (g:GDPRFramework {{
                type: '{framework_type.value}',
                code: '{framework_dict['code']}',
                name: '{framework_dict['name']}',
                territorial_scope: {json.dumps(framework_dict['territorial_scope'])},
                extraterritorial_triggers: {json.dumps(framework_dict['extraterritorial_triggers'])},
                adequacy_decisions: {json.dumps(framework_dict['adequacy_decisions'])},
                transfer_mechanisms: {json.dumps(framework_dict['transfer_mechanisms'])},
                penalties: '{framework_dict['penalties']}',
                data_subject_rights: {json.dumps(framework_dict['data_subject_rights'])},
                financial_sector_specifics: {json.dumps(framework_dict['financial_sector_specifics'])},
                supervisory_authorities: {json.dumps(framework_dict['supervisory_authorities'])}
            }})
            """
            self.graph.query(query)
        
        # Create adequacy decision nodes
        adequate_countries = set()
        for framework in GDPR_FRAMEWORKS.values():
            adequate_countries.update(framework.adequacy_decisions)
        
        for country in adequate_countries:
            if country:
                country_escaped = country.replace("'", "\\'")
                adequacy_query = f"""
                CREATE (a:AdequacyDecision {{
                    country: '{country_escaped}',
                    status: 'adequate'
                }})
                """
                self.graph.query(adequacy_query)
    
    def _create_gdpr_synonym_nodes(self, synonyms: List[ConceptSynonym]):
        """Create GDPR synonym nodes and relationships"""
        for synonym in synonyms:
            # Create primary GDPR concept node
            primary_term = synonym.primary_term.replace("'", "\\'")
            context = synonym.context.replace("'", "\\'")
            
            concept_query = f"""
            MERGE (c:GDPRConcept {{
                name: '{primary_term}',
                context: '{context}',
                confidence: {synonym.confidence}
            }})
            """
            self.graph.query(concept_query)
            
            # Create synonym nodes and relationships
            all_synonyms = (synonym.synonyms + synonym.financial_sector_terms + 
                          [term for terms in synonym.gdpr_variants.values() for term in terms])
            
            for syn in set(all_synonyms):  # Remove duplicates
                if syn and syn != primary_term:
                    syn_escaped = syn.replace("'", "\\'")
                    
                    synonym_query = f"""
                    MERGE (s:GDPRSynonym {{name: '{syn_escaped}'}})
                    """
                    self.graph.query(synonym_query)
                    
                    # Create relationship
                    relationship_query = f"""
                    MATCH (c:GDPRConcept {{name: '{primary_term}'}})
                    MATCH (s:GDPRSynonym {{name: '{syn_escaped}'}})
                    MERGE (c)-[:HAS_GDPR_SYNONYM]->(s)
                    """
                    self.graph.query(relationship_query)
            
            # Create GDPR variant relationships
            for variant_type, terms in synonym.gdpr_variants.items():
                for term in terms:
                    if term:
                        term_escaped = term.replace("'", "\\'")
                        variant_type_escaped = variant_type.replace("'", "\\'")
                        
                        variant_query = f"""
                        MERGE (gv:GDPRVariant {{
                            name: '{term_escaped}',
                            variant_type: '{variant_type_escaped}'
                        }})
                        """
                        self.graph.query(variant_query)
                        
                        # Link to primary concept
                        link_query = f"""
                        MATCH (c:GDPRConcept {{name: '{primary_term}'}})
                        MATCH (gv:GDPRVariant {{name: '{term_escaped}', variant_type: '{variant_type_escaped}'}})
                        MERGE (c)-[:HAS_GDPR_VARIANT]->(gv)
                        """
                        self.graph.query(link_query)
    
    def _process_gdpr_chunk_extraction(self, chunk: Dict[str, Any], extraction: Dict[str, Any], territorial: Dict[str, Any]):
        """Process individual chunk with GDPR extraction and territorial data"""
        chunk_id = chunk["chunk_id"]
        source = chunk["source"].replace("'", "\\'")
        
        # Create document node
        doc_query = f"""
        MERGE (d:GDPRDocument {{
            id: '{chunk_id}',
            source: '{source}',
            document_type: '{chunk.get('document_type', 'general_gdpr')}',
            gdpr_relevance_score: {chunk.get('metadata', {}).get('gdpr_relevance_score', 0.0)}
        }})
        """
        self.graph.query(doc_query)
        
        # Process GDPR processing activities
        for activity in extraction.get("processing_activities", []):
            self._create_gdpr_processing_activity_node(activity, chunk_id)
        
        # Process GDPR data categories
        for category in extraction.get("data_categories", []):
            self._create_gdpr_data_category_node(category, chunk_id)
        
        # Process legal bases
        for basis in extraction.get("legal_bases", []):
            self._create_gdpr_legal_basis_node(basis, chunk_id)
        
        # Process organizational entities
        for entity in extraction.get("organizational_entities", []):
            self._create_gdpr_organizational_entity_node(entity, chunk_id)
        
        # Process cross-border transfers
        for transfer in extraction.get("cross_border_transfers", []):
            self._create_gdpr_cross_border_transfer_node(transfer, chunk_id)
        
        # Process dynamically affected jurisdictions
        for jurisdiction in territorial.get("dynamically_affected_jurisdictions", []):
            self._create_affected_jurisdiction_node(jurisdiction, chunk_id)
    
    def _create_gdpr_processing_activity_node(self, activity: Dict[str, Any], chunk_id: str):
        """Create GDPR processing activity node"""
        activity_name = activity.get("activity_name", "").replace("'", "\\'")
        purpose = activity.get("purpose", "").replace("'", "\\'")
        lawful_basis = activity.get("lawful_basis", "").replace("'", "\\'")
        
        if activity_name:
            query = f"""
            MERGE (pa:GDPRProcessingActivity {{
                name: '{activity_name}',
                purpose: '{purpose}',
                lawful_basis: '{lawful_basis}',
                gdpr_article: '{activity.get('gdpr_article', '')}',
                data_categories: {json.dumps(activity.get('data_categories', []))},
                data_subjects: {json.dumps(activity.get('data_subjects', []))},
                retention_period: '{activity.get('retention_period', '')}',
                automated_decision_making: {str(activity.get('automated_decision_making', False)).lower()},
                gdpr_article_30_compliant: {str(activity.get('gdpr_article_30_compliant', False)).lower()}
            }})
            """
            self.graph.query(query)
            
            # Link to document
            link_query = f"""
            MATCH (d:GDPRDocument {{id: '{chunk_id}'}})
            MATCH (pa:GDPRProcessingActivity {{name: '{activity_name}'}})
            MERGE (d)-[:DESCRIBES_GDPR_ACTIVITY]->(pa)
            """
            self.graph.query(link_query)
    
    def _create_gdpr_data_category_node(self, category: Dict[str, Any], chunk_id: str):
        """Create GDPR data category node"""
        category_name = category.get("category", "").replace("'", "\\'")
        gdpr_classification = category.get("gdpr_classification", "personal_data")
        
        if category_name:
            query = f"""
            MERGE (dc:GDPRDataCategory {{
                name: '{category_name}',
                gdpr_classification: '{gdpr_classification}',
                sensitivity_level: '{category.get('sensitivity_level', 'normal')}',
                article_9_applicable: {str(category.get('article_9_applicable', False)).lower()},
                examples: {json.dumps(category.get('examples', []))},
                protection_requirements: {json.dumps(category.get('protection_requirements', []))}
            }})
            """
            self.graph.query(query)
            
            # Link to document
            link_query = f"""
            MATCH (d:GDPRDocument {{id: '{chunk_id}'}})
            MATCH (dc:GDPRDataCategory {{name: '{category_name}'}})
            MERGE (d)-[:DEFINES_GDPR_CATEGORY]->(dc)
            """
            self.graph.query(link_query)
    
    def _create_gdpr_legal_basis_node(self, basis: Dict[str, Any], chunk_id: str):
        """Create GDPR legal basis node"""
        basis_type = basis.get("basis_type", "").replace("'", "\\'")
        gdpr_article = basis.get("gdpr_article", "").replace("'", "\\'")
        
        if basis_type:
            query = f"""
            MERGE (lb:GDPRLegalBasis {{
                type: '{basis_type}',
                gdpr_article: '{gdpr_article}',
                description: '{basis.get('description', '').replace("'", "\\'")}',
                conditions: {json.dumps(basis.get('conditions', []))}
            }})
            """
            self.graph.query(query)
    
    def _create_gdpr_organizational_entity_node(self, entity: Dict[str, Any], chunk_id: str):
        """Create GDPR organizational entity node"""
        entity_name = entity.get("entity_name", "").replace("'", "\\'")
        role = entity.get("role", "").replace("'", "\\'")
        
        if entity_name:
            query = f"""
            MERGE (oe:GDPROrganizationalEntity {{
                name: '{entity_name}',
                role: '{role}',
                establishment: '{entity.get('establishment', '')}',
                gdpr_responsibilities: {json.dumps(entity.get('gdpr_responsibilities', []))}
            }})
            """
            self.graph.query(query)
    
    def _create_gdpr_cross_border_transfer_node(self, transfer: Dict[str, Any], chunk_id: str):
        """Create GDPR cross-border transfer node"""
        origin = transfer.get("origin", "").replace("'", "\\'")
        destination = transfer.get("destination", "").replace("'", "\\'")
        mechanism = transfer.get("transfer_mechanism", "").replace("'", "\\'")
        
        if origin and destination:
            query = f"""
            MERGE (cbt:GDPRCrossBorderTransfer {{
                origin: '{origin}',
                destination: '{destination}',
                transfer_mechanism: '{mechanism}',
                adequacy_decision_exists: {str(transfer.get('adequacy_decision_exists', False)).lower()},
                safeguards: {json.dumps(transfer.get('safeguards', []))},
                chapter_5_compliance: {str(transfer.get('chapter_5_compliance', False)).lower()}
            }})
            """
            self.graph.query(query)
    
    def _create_affected_jurisdiction_node(self, jurisdiction: Dict[str, Any], chunk_id: str):
        """Create dynamically affected jurisdiction node"""
        jurisdiction_name = jurisdiction.get("jurisdiction", "").replace("'", "\\'")
        
        if jurisdiction_name:
            query = f"""
            MERGE (aj:AffectedJurisdiction {{
                name: '{jurisdiction_name}',
                gdpr_applicability: '{jurisdiction.get('gdpr_applicability', '')}',
                uk_gdpr_applicability: '{jurisdiction.get('uk_gdpr_applicability', '')}',
                applicability_basis: '{jurisdiction.get('applicability_basis', '')}',
                business_context: '{jurisdiction.get('business_context', '').replace("'", "\\'")}',
                compliance_requirements: {json.dumps(jurisdiction.get('compliance_requirements', []))}
            }})
            """
            self.graph.query(query)
    
    def _create_gdpr_relationships(self):
        """Create advanced GDPR relationships between entities"""
        
        # Link processing activities to data categories
        self.graph.query("""
        MATCH (pa:GDPRProcessingActivity), (dc:GDPRDataCategory)
        WHERE dc.name IN pa.data_categories
        MERGE (pa)-[:PROCESSES_GDPR_DATA]->(dc)
        """)
        
        # Link processing activities to legal bases
        self.graph.query("""
        MATCH (pa:GDPRProcessingActivity), (lb:GDPRLegalBasis)
        WHERE pa.lawful_basis = lb.type
        MERGE (pa)-[:BASED_ON_GDPR_BASIS]->(lb)
        """)
        
        # Link transfers to adequacy decisions
        self.graph.query("""
        MATCH (cbt:GDPRCrossBorderTransfer), (ad:AdequacyDecision)
        WHERE cbt.destination = ad.country
        MERGE (cbt)-[:HAS_ADEQUACY_DECISION]->(ad)
        """)
        
        # Link entities to GDPR frameworks
        self.graph.query("""
        MATCH (pa:GDPRProcessingActivity), (gf:GDPRFramework)
        WHERE gf.type = 'gdpr_eu'
        MERGE (pa)-[:SUBJECT_TO_GDPR]->(gf)
        """)
        
        # Link affected jurisdictions to frameworks
        self.graph.query("""
        MATCH (aj:AffectedJurisdiction), (gf:GDPRFramework)
        WHERE (aj.gdpr_applicability CONTAINS 'extraterritorial' OR aj.gdpr_applicability CONTAINS 'direct') 
        AND gf.type = 'gdpr_eu'
        MERGE (aj)-[:AFFECTED_BY_GDPR]->(gf)
        """)
        
        self.graph.query("""
        MATCH (aj:AffectedJurisdiction), (gf:GDPRFramework)
        WHERE (aj.uk_gdpr_applicability CONTAINS 'extraterritorial' OR aj.uk_gdpr_applicability CONTAINS 'direct') 
        AND gf.type = 'uk_gdpr'
        MERGE (aj)-[:AFFECTED_BY_UK_GDPR]->(gf)
        """)
        
        # Create synonym relationships with GDPR concepts
        self.graph.query("""
        MATCH (c:GDPRConcept), (pa:GDPRProcessingActivity)
        WHERE toLower(c.name) = toLower(pa.name)
        MERGE (c)-[:REPRESENTS_GDPR_ACTIVITY]->(pa)
        """)
        
        self.graph.query("""
        MATCH (c:GDPRConcept), (dc:GDPRDataCategory)
        WHERE toLower(c.name) = toLower(dc.name)
        MERGE (c)-[:REPRESENTS_GDPR_CATEGORY]->(dc)
        """)
    
    def search_with_gdpr_synonyms(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """Search GDPR graph using synonym expansion"""
        query_terms = self._extract_gdpr_search_terms(query)
        results = []
        
        for term in query_terms:
            term_escaped = term.replace("'", "\\'")
            
            # Search with GDPR synonym expansion
            search_query = f"""
            MATCH path = (start)-[*1..3]-(related)
            WHERE (
                (start:GDPRProcessingActivity AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)<-[:REPRESENTS_GDPR_ACTIVITY]-(:GDPRConcept)-[:HAS_GDPR_SYNONYM]->(:GDPRSynonym {{name: '{term_escaped}'}}))
                 )
                ) OR
                (start:GDPRDataCategory AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)<-[:REPRESENTS_GDPR_CATEGORY]-(:GDPRConcept)-[:HAS_GDPR_SYNONYM]->(:GDPRSynonym {{name: '{term_escaped}'}}))
                 )
                ) OR
                (start:GDPRConcept AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)-[:HAS_GDPR_SYNONYM]->(:GDPRSynonym {{name: '{term_escaped}'}}))
                 )
                ) OR
                (start:AffectedJurisdiction AND toLower(start.name) CONTAINS '{term_escaped}') OR
                (start:GDPRLegalBasis AND toLower(start.type) CONTAINS '{term_escaped}') OR
                (start:GDPRFramework AND toLower(start.name) CONTAINS '{term_escaped}')
            )
            RETURN DISTINCT start, related, relationships(path), length(path) as distance
            ORDER BY distance, start.name
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(search_query)
                
                for record in result.result_set:
                    graph_result = {
                        "start_node": self._format_gdpr_node(record[0]),
                        "related_node": self._format_gdpr_node(record[1]),
                        "relationships": [str(rel) for rel in record[2]],
                        "distance": record[3],
                        "search_term": term,
                        "search_type": "gdpr_synonym_enhanced_graph"
                    }
                    results.append(graph_result)
            
            except Exception as e:
                logger.warning(f"GDPR graph search failed for '{term}': {e}")
        
        return results[:top_k]
    
    def _extract_gdpr_search_terms(self, query: str) -> List[str]:
        """Extract meaningful GDPR search terms from query"""
        words = re.findall(r'\b\w+\b', query.lower())
        stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an', 'is', 'are'}
        return [word for word in words if len(word) > 2 and word not in stopwords]
    
    def _format_gdpr_node(self, node) -> Dict[str, Any]:
        """Format GDPR graph node for output"""
        if hasattr(node, 'properties'):
            return dict(node.properties)
        return {"id": str(node)}

class GlobalGDPRRopaMetamodelSystem:
    """Complete Global GDPR/UK GDPR RoPA Metamodel System with Dynamic Jurisdictional Discovery"""
    
    def __init__(self,
                 openai_api_key: str,
                 openai_base_url: str = None,
                 elasticsearch_host: str = "http://localhost:9200",
                 elasticsearch_username: str = None,
                 elasticsearch_password: str = None,
                 elasticsearch_ca_certs: str = None,
                 falkordb_host: str = "localhost",
                 falkordb_port: int = 6379,
                 falkordb_password: str = None):
        
        # Set environment variables
        os.environ["OPENAI_API_KEY"] = openai_api_key
        if openai_base_url:
            os.environ["OPENAI_BASE_URL"] = openai_base_url
        
        # Initialize GDPR-focused components
        self.processor = EnhancedGDPRProcessor()
        
        self.vector_engine = EnhancedGDPRVectorEngine(
            host=elasticsearch_host,
            username=elasticsearch_username,
            password=elasticsearch_password,
            ca_certs=elasticsearch_ca_certs,
            openai_api_key=openai_api_key,
            openai_base_url=openai_base_url
        )
        
        self.graph_engine = EnhancedGDPRGraphEngine(
            host=falkordb_host,
            port=falkordb_port,
            password=falkordb_password
        )
        
        # Initialize system state
        self.system_state = {
            "processed_documents": [],
            "extracted_concepts": [],
            "territorial_analyses": [],
            "generated_synonyms": [],
            "metamodel_iterations": [],
            "final_metamodel": None,
            "analysis_statistics": {}
        }
        
        logger.info("Global GDPR/UK GDPR RoPA Metamodel System initialized")
        if openai_base_url:
            logger.info(f"Using custom OpenAI endpoint: {openai_base_url}")
    
    def ingest_documents(self, document_paths: List[str]) -> Dict[str, Any]:
        """Ingest and process GDPR/UK GDPR regulatory documents with comprehensive analysis"""
        logger.info(f"Ingesting {len(document_paths)} GDPR regulatory documents")
        
        all_chunks = []
        all_extractions = []
        all_territorial_analyses = []
        
        for doc_path in document_paths:
            try:
                # Process document with GDPR focus
                chunks = self.processor.extract_pdf_content(doc_path)
                
                # Extract GDPR regulatory concepts from each chunk
                extractions = []
                territorial_analyses = []
                
                for chunk in chunks:
                    # Comprehensive GDPR extraction
                    extraction = comprehensive_gdpr_extraction_agent.invoke(chunk["text"])
                    extractions.append(extraction)
                    
                    # Territorial analysis for GDPR/UK GDPR applicability
                    territorial = gdpr_territorial_analysis_agent.invoke(chunk["text"])
                    territorial_analyses.append(territorial)
                
                all_chunks.extend(chunks)
                all_extractions.extend(extractions)
                all_territorial_analyses.extend(territorial_analyses)
                
                self.system_state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "timestamp": datetime.now()
                })
                
                logger.info(f"Processed {doc_path}: {len(chunks)} chunks with GDPR analysis")
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Store extractions and analyses
        self.system_state["extracted_concepts"] = all_extractions
        self.system_state["territorial_analyses"] = all_territorial_analyses
        
        # Generate and store GDPR-specific synonyms using AI
        logger.info("Generating GDPR-focused AI-powered synonyms...")
        synonyms = self.vector_engine.generate_and_store_gdpr_synonyms(all_extractions)
        self.system_state["generated_synonyms"] = synonyms
        
        # Index documents with GDPR extractions and territorial analysis
        self.vector_engine.index_documents_with_gdpr_extractions(all_chunks, all_extractions, all_territorial_analyses)
        
        # Build GDPR knowledge graph
        self.graph_engine.build_gdpr_knowledge_graph(all_chunks, all_extractions, all_territorial_analyses, synonyms)
        
        # Calculate statistics
        stats = self._calculate_gdpr_ingestion_statistics(all_chunks, all_extractions, all_territorial_analyses, synonyms)
        self.system_state["analysis_statistics"] = stats
        
        return {
            "status": "success",
            "documents_processed": len(document_paths),
            "total_chunks": len(all_chunks),
            "gdpr_synonyms_generated": len(synonyms),
            **stats
        }
    
    def _calculate_gdpr_ingestion_statistics(self, chunks: List[Dict], extractions: List[Dict], territorial_analyses: List[Dict], synonyms: List[ConceptSynonym]) -> Dict[str, Any]:
        """Calculate comprehensive GDPR ingestion statistics"""
        
        # Process GDPR extractions statistics
        total_processing_activities = sum(len(ext.get("processing_activities", [])) for ext in extractions)
        total_data_categories = sum(len(ext.get("data_categories", [])) for ext in extractions)
        total_legal_bases = sum(len(ext.get("legal_bases", [])) for ext in extractions)
        total_cross_border_transfers = sum(len(ext.get("cross_border_transfers", [])) for ext in extractions)
        total_compliance_obligations = sum(len(ext.get("compliance_obligations", [])) for ext in extractions)
        
        # Territorial analysis statistics
        affected_jurisdictions = set()
        gdpr_applicable_jurisdictions = set()
        uk_gdpr_applicable_jurisdictions = set()
        
        for territorial in territorial_analyses:
            for jurisdiction in territorial.get("dynamically_affected_jurisdictions", []):
                if isinstance(jurisdiction, dict):
                    jurisdiction_name = jurisdiction.get("jurisdiction", "")
                    affected_jurisdictions.add(jurisdiction_name)
                    
                    if jurisdiction.get("gdpr_applicability") in ["direct", "extraterritorial"]:
                        gdpr_applicable_jurisdictions.add(jurisdiction_name)
                    
                    if jurisdiction.get("uk_gdpr_applicability") in ["direct", "extraterritorial"]:
                        uk_gdpr_applicable_jurisdictions.add(jurisdiction_name)
        
        # Document type distribution
        doc_types = {}
        for chunk in chunks:
            doc_type = chunk.get("document_type", "unknown")
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        # GDPR synonym statistics
        total_synonyms = sum(len(syn.synonyms) for syn in synonyms)
        gdpr_variants = sum(len(variants) for syn in synonyms for variants in syn.gdpr_variants.values())
        
        # Article 30 compliance indicators
        article_30_activities = sum(1 for ext in extractions for activity in ext.get("processing_activities", []) if activity.get("gdpr_article_30_compliant"))
        
        return {
            "processing_activities_discovered": total_processing_activities,
            "article_30_compliant_activities": article_30_activities,
            "data_categories_discovered": total_data_categories,
            "legal_bases_identified": total_legal_bases,
            "cross_border_transfers_found": total_cross_border_transfers,
            "compliance_obligations_identified": total_compliance_obligations,
            "affected_jurisdictions_discovered": len(affected_jurisdictions),
            "gdpr_applicable_jurisdictions": len(gdpr_applicable_jurisdictions),
            "uk_gdpr_applicable_jurisdictions": len(uk_gdpr_applicable_jurisdictions),
            "document_type_distribution": doc_types,
            "total_gdpr_synonyms_generated": total_synonyms,
            "gdpr_variant_terms_created": gdpr_variants,
            "average_gdpr_relevance_score": sum(chunk.get("metadata", {}).get("gdpr_relevance_score", 0) for chunk in chunks) / len(chunks) if chunks else 0
        }
    
    def iterative_analysis(self, iterations: int = 3) -> Dict[str, Any]:
        """Perform iterative analysis with enhanced GDPR/UK GDPR AI reasoning"""
        logger.info(f"Starting iterative GDPR analysis with {iterations} iterations")
        
        gdpr_analysis_queries = [
            "What are the mandatory Article 30 Record of Processing Activities requirements under GDPR?",
            "How does UK GDPR differ from EU GDPR in terms of RoPA requirements?",
            "What are the territorial scope triggers for GDPR and UK GDPR extraterritorial application?",
            "Which adequacy decisions affect cross-border transfers from EU and UK?",
            "What are the specific GDPR compliance requirements for financial institutions?",
            "How do cross-border data transfers work between EU, UK, and third countries under GDPR?",
            "What processing activities require specific documentation under Article 30 GDPR?",
            "How do data subject rights vary between GDPR and UK GDPR frameworks?",
            "What are the security measures required under Article 32 GDPR for financial data?",
            "How should organizations handle GDPR compliance across multiple jurisdictions?",
            "What are the specific legal bases available under GDPR Article 6 and Article 9?",
            "How do supervisory authorities coordinate GDPR enforcement across borders?",
            "What transfer mechanisms are available under GDPR Chapter V for different countries?",
            "How does GDPR territorial scope apply to cloud computing and SaaS providers?",
            "What are the specific documentation requirements for controllers vs processors under Article 30?"
        ]
        
        iteration_results = []
        
        for iteration in range(iterations):
            logger.info(f"GDPR analysis iteration {iteration + 1}/{iterations}")
            
            iteration_insights = []
            
            for query in gdpr_analysis_queries:
                # Search vector store with GDPR synonym expansion
                vector_results = self.vector_engine.search_with_gdpr_synonym_expansion(query, top_k=5)
                
                # Search GDPR knowledge graph with synonyms
                graph_results = self.graph_engine.search_with_gdpr_synonyms(query, top_k=3)
                
                # Combine and analyze results
                combined_analysis = {
                    "query": query,
                    "vector_results": vector_results,
                    "graph_results": graph_results,
                    "iteration": iteration + 1,
                    "gdpr_focus": True
                }
                
                iteration_insights.append(combined_analysis)
            
            iteration_results.append({
                "iteration": iteration + 1,
                "insights": iteration_insights,
                "timestamp": datetime.now()
            })
        
        self.system_state["metamodel_iterations"] = iteration_results
        
        return {
            "iterations_completed": iterations,
            "total_gdpr_queries_analyzed": len(gdpr_analysis_queries) * iterations,
            "insights_per_iteration": len(gdpr_analysis_queries),
            "focus": "GDPR and UK GDPR territorial applicability",
            "status": "gdpr_iterative_analysis_complete"
        }
    
    def generate_final_metamodel(self) -> Dict[str, Any]:
        """Generate final comprehensive GDPR/UK GDPR RoPA metamodel"""
        logger.info("Generating final comprehensive GDPR/UK GDPR RoPA metamodel")
        
        # Consolidate all GDPR analysis data
        consolidated_data = {
            "processed_documents": self.system_state["processed_documents"],
            "extracted_concepts": self.system_state["extracted_concepts"],
            "territorial_analyses": self.system_state["territorial_analyses"],
            "generated_synonyms": [asdict(syn) for syn in self.system_state["generated_synonyms"]],
            "metamodel_iterations": self.system_state["metamodel_iterations"],
            "analysis_statistics": self.system_state["analysis_statistics"],
            "gdpr_frameworks": {k.value: asdict(v) for k, v in GDPR_FRAMEWORKS.items()},
            "potential_affected_jurisdictions": POTENTIAL_AFFECTED_JURISDICTIONS
        }
        
        # Generate GDPR RoPA metamodel using AI
        metamodel = gdpr_metamodel_generation_agent.invoke(json.dumps(consolidated_data, default=str))
        
        self.system_state["final_metamodel"] = metamodel
        
        return metamodel
    
    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive GDPR/UK GDPR analysis report"""
        logger.info("Generating comprehensive GDPR/UK GDPR analysis report")
        
        if not self.system_state["final_metamodel"]:
            self.generate_final_metamodel()
        
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        report_data = {
            "system_state": self.system_state,
            "gdpr_frameworks": {k.value: asdict(v) for k, v in GDPR_FRAMEWORKS.items()},
            "timestamp": datetime.now().isoformat()
        }
        
        prompt = f"""Generate a comprehensive executive report on the GDPR/UK GDPR Record of Processing Activities (RoPA) Metamodel System with Dynamic Jurisdictional Discovery for Financial Institutions.

System Analysis Data: {json.dumps(report_data, default=str)[:50000]}...

Create a detailed report with the following structure:

# Executive Summary
- Project overview focusing on GDPR and UK GDPR compliance
- Key findings from AI-enhanced territorial scope discovery
- Critical compliance gaps identified through AI analysis
- Implementation recommendations for global financial institutions

# Enhanced AI-Powered Methodology
- AI-powered document analysis with o3-mini reasoning specifically for GDPR
- GDPR-specific synonym generation using LLM
- Territorial applicability discovery with machine learning
- Vector and graph database integration optimized for GDPR compliance
- Iterative refinement focused on Article 30 requirements

# GDPR and UK GDPR Regulatory Framework Analysis
- EU GDPR comprehensive requirements (Regulation 2016/679)
- UK GDPR post-Brexit considerations (Data Protection Act 2018)
- Territorial scope under Article 3 GDPR
- Extraterritorial application triggers and conditions
- Cross-border data transfer mechanisms under Chapter V
- Adequacy decisions and their impact on global operations

# Dynamic Jurisdictional Applicability Discovery
- AI-powered discovery of jurisdictions affected by GDPR/UK GDPR
- Extraterritorial application scenarios for global organizations
- Financial institutions' territorial compliance obligations
- Cross-border service provision under GDPR framework
- Targeting and monitoring provisions for non-EU/UK entities

# AI-Enhanced GDPR Synonym Generation
- LLM-powered GDPR concept expansion with o3-mini reasoning
- EU GDPR vs UK GDPR terminology variations
- Financial sector specific GDPR terminology
- Article-specific legal terminology mapping
- Cross-reference and semantic relationships within GDPR framework

# Article 30 RoPA Metamodel Architecture
- Core RoPA entities compliant with Article 30 GDPR
- Controller vs processor RoPA requirements
- Territorial scope integration in RoPA structure
- Financial sector specializations within GDPR framework
- Cross-border transfer documentation in RoPA
- Validation framework for Article 30 compliance

# Financial Sector GDPR Specializations
- Banking regulatory compliance under GDPR (PSD2, MiFID II integration)
- Insurance data protection requirements under GDPR
- Investment management and trading data under GDPR
- Payment processing compliance with GDPR
- Cross-border financial services territorial compliance
- AML/KYC data processing under GDPR framework

# Knowledge Graph and Vector Integration for GDPR
- GDPR-specific semantic search capabilities
- Synonym-enhanced queries for regulatory compliance
- Cross-reference resolution within GDPR articles
- Territorial relationship discovery
- Compliance gap identification through AI analysis

# Territorial Compliance Implementation Roadmap
- Phase 1: GDPR/UK GDPR foundation and AI integration
- Phase 2: Core Article 30 RoPA implementation
- Phase 3: Territorial scope extensions and discovery
- Phase 4: Financial sector GDPR specializations
- Phase 5: Cross-border compliance optimization

# Risk Assessment and GDPR Compliance Validation
- AI-identified GDPR compliance risks across territories
- Cross-jurisdictional conflicts with GDPR requirements
- Implementation challenges for global financial institutions
- Mitigation strategies with AI-powered monitoring

# Technology Architecture for GDPR Compliance
- Enhanced vector database with GDPR-optimized indexing
- Knowledge graph with GDPR synonym expansion
- AI-powered territorial analysis pipeline
- Scalability for global GDPR compliance monitoring
- Security and privacy safeguards for sensitive regulatory data

# Governance and Continuous GDPR Compliance
- AI-enhanced monitoring of GDPR regulatory updates
- Automated Article 30 compliance checking
- GDPR synonym evolution and maintenance
- Regular territorial scope review and updates
- Supervisory authority guidance integration

# Conclusions and Strategic Value
- Key achievements in GDPR territorial compliance discovery
- Scalability and adaptability for global financial operations
- Future AI enhancement opportunities for GDPR compliance
- Long-term strategic value for regulatory compliance management

Provide a comprehensive, professional report that demonstrates the focused GDPR/UK GDPR capabilities and global territorial compliance coverage of this specialized system.
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            return response.content
        
        except Exception as e:
            logger.error(f"Failed to generate comprehensive GDPR report: {e}")
            return f"Error generating GDPR report: {e}"
    
    def query_system(self, query: str, search_type: str = "both") -> Dict[str, Any]:
        """Query the GDPR system using enhanced search capabilities"""
        results = {}
        
        if search_type in ["vector", "both"]:
            vector_results = self.vector_engine.search_with_gdpr_synonym_expansion(query, top_k=10)
            results["vector_search"] = vector_results
        
        if search_type in ["graph", "both"]:
            graph_results = self.graph_engine.search_with_gdpr_synonyms(query, top_k=10)
            results["graph_search"] = graph_results
        
        return {
            "query": query,
            "search_type": search_type,
            "gdpr_focus": True,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get comprehensive GDPR system statistics"""
        try:
            # Elasticsearch stats
            es_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.index_name)
            doc_count = es_stats.body["indices"][self.vector_engine.index_name]["total"]["docs"]["count"]
            
            # GDPR Synonym stats
            synonym_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.synonym_index)
            synonym_count = synonym_stats.body["indices"][self.vector_engine.synonym_index]["total"]["docs"]["count"]
            
            # Graph stats
            node_stats = self.graph_engine.graph.query("MATCH (n) RETURN count(n) as nodes")
            node_count = node_stats.result_set[0][0] if node_stats.result_set else 0
            
            rel_stats = self.graph_engine.graph.query("MATCH ()-[r]->() RETURN count(r) as rels")
            rel_count = rel_stats.result_set[0][0] if rel_stats.result_set else 0
            
            # Detailed GDPR entity counts
            gdpr_entity_counts = {}
            gdpr_entity_types = ["GDPRProcessingActivity", "GDPRDataCategory", "GDPRLegalBasis", "GDPRConcept", "GDPRSynonym", "AffectedJurisdiction", "GDPRFramework"]
            
            for entity_type in gdpr_entity_types:
                try:
                    count_query = f"MATCH (n:{entity_type}) RETURN count(n) as count"
                    result = self.graph_engine.graph.query(count_query)
                    gdpr_entity_counts[entity_type.lower() + "_count"] = result.result_set[0][0] if result.result_set else 0
                except:
                    gdpr_entity_counts[entity_type.lower() + "_count"] = 0
            
            return {
                "elasticsearch_documents": doc_count,
                "gdpr_synonyms": synonym_count,
                "graph_nodes": node_count,
                "graph_relationships": rel_count,
                **gdpr_entity_counts,
                "processed_documents": len(self.system_state["processed_documents"]),
                "generated_gdpr_synonyms": len(self.system_state["generated_synonyms"]),
                "metamodel_iterations": len(self.system_state["metamodel_iterations"]),
                "has_final_metamodel": self.system_state["final_metamodel"] is not None,
                "analysis_statistics": self.system_state.get("analysis_statistics", {}),
                "gdpr_focus": True,
                "system_status": "operational"
            }
        except Exception as e:
            return {"error": str(e), "system_status": "error"}
    
    def export_metamodel(self, output_path: str = "gdpr_ropa_metamodel.json") -> bool:
        """Export the final GDPR RoPA metamodel to file"""
        try:
            if not self.system_state["final_metamodel"]:
                logger.warning("No final GDPR metamodel available. Generating...")
                self.generate_final_metamodel()
            
            export_data = {
                "metamodel": self.system_state["final_metamodel"],
                "system_state": self.system_state,
                "gdpr_frameworks": {k.value: asdict(v) for k, v in GDPR_FRAMEWORKS.items()},
                "export_timestamp": datetime.now().isoformat(),
                "system_version": "3.0.0",
                "focus": "GDPR and UK GDPR with dynamic territorial discovery"
            }
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2, default=str, ensure_ascii=False)
            
            logger.info(f"GDPR RoPA metamodel exported to: {output_path}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to export GDPR metamodel: {e}")
            return False


def main():
    """Main execution function for the GDPR/UK GDPR Focused RoPA Metamodel System"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="GDPR/UK GDPR Focused RoPA Metamodel System with Dynamic Jurisdictional Discovery",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python gdpr_ropa_system.py --ingest /path/to/gdpr/docs/*.pdf
  python gdpr_ropa_system.py --analyze --iterations 5
  python gdpr_ropa_system.py --generate-metamodel
  python gdpr_ropa_system.py --generate-report
  python gdpr_ropa_system.py --query "Article 30 record of processing activities"
  python gdpr_ropa_system.py --stats
        """
    )
    
    # Main operations
    parser.add_argument("--ingest", nargs="+", help="Paths to GDPR regulatory documents to ingest")
    parser.add_argument("--analyze", action="store_true", help="Perform iterative GDPR metamodel analysis")
    parser.add_argument("--generate-metamodel", action="store_true", help="Generate final GDPR RoPA metamodel")
    parser.add_argument("--generate-report", action="store_true", help="Generate comprehensive GDPR report")
    parser.add_argument("--query", type=str, help="Query the GDPR system with natural language")
    parser.add_argument("--stats", action="store_true", help="Show GDPR system statistics")
    parser.add_argument("--export", type=str, help="Export GDPR metamodel to specified file path")
    
    # Configuration options
    parser.add_argument("--iterations", type=int, default=3, help="Number of analysis iterations")
    parser.add_argument("--search-type", choices=["vector", "graph", "both"], default="both", 
                       help="Type of search to perform")
    parser.add_argument("--output-dir", type=str, default=".", help="Output directory for reports and exports")
    
    args = parser.parse_args()
    
    # Load and validate configuration
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "openai_base_url": os.getenv("OPENAI_BASE_URL"),
        "elasticsearch_host": os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        "elasticsearch_username": os.getenv("ELASTICSEARCH_USERNAME"),
        "elasticsearch_password": os.getenv("ELASTICSEARCH_PASSWORD"),
        "elasticsearch_ca_certs": os.getenv("ELASTICSEARCH_CA_CERTS"),
        "falkordb_host": os.getenv("FALKORDB_HOST", "localhost"),
        "falkordb_port": int(os.getenv("FALKORDB_PORT", 6379)),
        "falkordb_password": os.getenv("FALKORDB_PASSWORD")
    }
    
    # Validate required configuration
    if not config["openai_api_key"]:
        print("❌ Error: OPENAI_API_KEY environment variable is required")
        print("   Set it with: export OPENAI_API_KEY=your_api_key")
        return 1
    
    if not config["elasticsearch_host"].startswith(('http://', 'https://')):
        print("❌ Error: ELASTICSEARCH_HOST must include schema (http:// or https://)")
        print(f"   Current value: {config['elasticsearch_host']}")
        return 1
    
    # Display configuration
    print("🚀 GDPR/UK GDPR Focused RoPA Metamodel System")
    print("=" * 60)
    print("Configuration:")
    print(f"  • OpenAI Endpoint: {config['openai_base_url'] or 'Standard OpenAI API'}")
    print(f"  • Elasticsearch: {config['elasticsearch_host']}")
    print(f"  • FalkorDB: {config['falkordb_host']}:{config['falkordb_port']}")
    print(f"  • Focus: GDPR & UK GDPR with Dynamic Territorial Discovery")
    print(f"  • Target: Financial Institutions, Article 30 Compliance")
    print()
    
    try:
        # Initialize the GDPR-focused system
        print("🔧 Initializing GDPR/UK GDPR RoPA Metamodel System...")
        system = GlobalGDPRRopaMetamodelSystem(
            openai_api_key=config["openai_api_key"],
            openai_base_url=config["openai_base_url"],
            elasticsearch_host=config["elasticsearch_host"],
            elasticsearch_username=config["elasticsearch_username"],
            elasticsearch_password=config["elasticsearch_password"],
            elasticsearch_ca_certs=config["elasticsearch_ca_certs"],
            falkordb_host=config["falkordb_host"],
            falkordb_port=config["falkordb_port"],
            falkordb_password=config["falkordb_password"]
        )
        
        print("✅ GDPR-focused system initialized successfully")
        print()
        
        # Execute requested operations
        if args.ingest:
            print(f"📄 Ingesting {len(args.ingest)} GDPR regulatory documents with AI analysis...")
            result = system.ingest_documents(args.ingest)
            print("✅ GDPR document ingestion completed:")
            print(f"   • Documents processed: {result['documents_processed']}")
            print(f"   • Total chunks: {result['total_chunks']}")
            print(f"   • GDPR synonyms generated: {result['gdpr_synonyms_generated']}")
            print(f"   • Processing activities: {result.get('processing_activities_discovered', 0)}")
            print(f"   • Article 30 compliant activities: {result.get('article_30_compliant_activities', 0)}")
            print(f"   • Data categories: {result.get('data_categories_discovered', 0)}")
            print(f"   • Affected jurisdictions: {result.get('affected_jurisdictions_discovered', 0)}")
            print(f"   • GDPR applicable jurisdictions: {result.get('gdpr_applicable_jurisdictions', 0)}")
            print(f"   • UK GDPR applicable jurisdictions: {result.get('uk_gdpr_applicable_jurisdictions', 0)}")
            print()
        
        if args.analyze:
            print(f"🔍 Performing iterative GDPR analysis with {args.iterations} iterations...")
            result = system.iterative_analysis(args.iterations)
            print("✅ GDPR iterative analysis completed:")
            print(f"   • Iterations completed: {result['iterations_completed']}")
            print(f"   • Total GDPR queries analyzed: {result['total_gdpr_queries_analyzed']}")
            print(f"   • Focus: {result['focus']}")
            print(f"   • Insights per iteration: {result['insights_per_iteration']}")
            print()
        
        if args.generate_metamodel:
            print("🏗️ Generating final comprehensive GDPR/UK GDPR RoPA metamodel...")
            result = system.generate_final_metamodel()
            print("✅ Final GDPR RoPA metamodel generated successfully")
            
            # Save metamodel to output directory
            output_file = os.path.join(args.output_dir, "gdpr_ropa_metamodel.json")
            system.export_metamodel(output_file)
            print(f"💾 GDPR RoPA metamodel saved to: {output_file}")
            print()
        
        if args.generate_report:
            print("📊 Generating comprehensive GDPR/UK GDPR analysis report...")
            report = system.generate_comprehensive_report()
            
            # Save report to output directory
            report_file = os.path.join(args.output_dir, "gdpr_ropa_analysis_report.md")
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            print(f"💾 GDPR report saved to: {report_file}")
            
            # Display executive summary
            lines = report.split('\n')
            summary_start = -1
            summary_end = -1
            
            for i, line in enumerate(lines):
                if "# Executive Summary" in line:
                    summary_start = i
                elif summary_start != -1 and line.startswith("# ") and "Executive Summary" not in line:
                    summary_end = i
                    break
            
            if summary_start != -1:
                end_idx = summary_end if summary_end != -1 else min(summary_start + 20, len(lines))
                summary = '\n'.join(lines[summary_start:end_idx])
                
                print("\n" + "="*80)
                print("GDPR EXECUTIVE SUMMARY PREVIEW")
                print("="*80)
                print(summary[:2000] + "..." if len(summary) > 2000 else summary)
                print("="*80)
            print()
        
        if args.query:
            print(f"🔎 Querying GDPR system: '{args.query}'")
            print(f"   Search type: {args.search_type}")
            result = system.query_system(args.query, args.search_type)
            
            print("✅ GDPR query results:")
            if "vector_search" in result["results"]:
                vector_results = result["results"]["vector_search"]
                print(f"   • Vector search: {len(vector_results)} results")
                for i, res in enumerate(vector_results[:3]):
                    print(f"     {i+1}. {res['chunk_id']} (score: {res['score']:.3f}, GDPR relevance: {res['gdpr_relevance_score']:.3f})")
            
            if "graph_search" in result["results"]:
                graph_results = result["results"]["graph_search"]
                print(f"   • Graph search: {len(graph_results)} results")
                for i, res in enumerate(graph_results[:3]):
                    start_node = res.get('start_node', {})
                    node_name = start_node.get('name', 'Unknown')
                    print(f"     {i+1}. {node_name} (distance: {res.get('distance', 'N/A')})")
            print()
        
        if args.stats:
            print("📈 GDPR System Statistics:")
            stats = system.get_system_statistics()
            
            if "error" in stats:
                print(f"   ❌ Error: {stats['error']}")
            else:
                print(f"   • System Status: {stats['system_status']}")
                print(f"   • GDPR Focus: {stats.get('gdpr_focus', 'Yes')}")
                print(f"   • Elasticsearch Documents: {stats.get('elasticsearch_documents', 0):,}")
                print(f"   • GDPR Synonyms: {stats.get('gdpr_synonyms', 0):,}")
                print(f"   • Knowledge Graph Nodes: {stats.get('graph_nodes', 0):,}")
                print(f"   • Knowledge Graph Relationships: {stats.get('graph_relationships', 0):,}")
                print(f"   • GDPR Processing Activities: {stats.get('gdprprocessingactivity_count', 0):,}")
                print(f"   • GDPR Data Categories: {stats.get('gdprdatacategory_count', 0):,}")
                print(f"   • GDPR Concepts: {stats.get('gdprconcept_count', 0):,}")
                print(f"   • GDPR Synonyms: {stats.get('gdprsynonym_count', 0):,}")
                print(f"   • Affected Jurisdictions: {stats.get('affectedjurisdiction_count', 0):,}")
                print(f"   • GDPR Frameworks: {stats.get('gdprframework_count', 0):,}")
                print(f"   • Processed Documents: {stats.get('processed_documents', 0)}")
                print(f"   • Metamodel Iterations: {stats.get('metamodel_iterations', 0)}")
                print(f"   • Has Final Metamodel: {'Yes' if stats.get('has_final_metamodel') else 'No'}")
                
                analysis_stats = stats.get('analysis_statistics', {})
                if analysis_stats:
                    print("   GDPR Analysis Statistics:")
                    for key, value in analysis_stats.items():
                        if isinstance(value, dict):
                            print(f"     • {key.replace('_', ' ').title()}:")
                            for k, v in value.items():
                                print(f"       - {k}: {v}")
                        else:
                            print(f"     • {key.replace('_', ' ').title()}: {value}")
            print()
        
        if args.export:
            print(f"💾 Exporting GDPR metamodel to: {args.export}")
            success = system.export_metamodel(args.export)
            if success:
                print("✅ GDPR export completed successfully")
            else:
                print("❌ GDPR export failed")
            print()
        
        # If no specific action requested, show help and system info
        if not any([args.ingest, args.analyze, args.generate_metamodel, 
                   args.generate_report, args.query, args.stats, args.export]):
            parser.print_help()
            print("\n" + "="*80)
            print("GDPR/UK GDPR FOCUSED SYSTEM CAPABILITIES")
            print("="*80)
            print("🤖 AI-Powered GDPR Features:")
            print("   • LLM-based GDPR synonym generation with o3-mini reasoning")
            print("   • Comprehensive GDPR regulatory concept extraction")
            print("   • GDPR/UK GDPR territorial scope discovery")
            print("   • Article 30 RoPA compliance analysis")
            print("   • Financial sector GDPR specialization")
            print("   • Semantic search with GDPR synonym expansion")
            print()
            print("🌍 GDPR Territorial Coverage:")
            print("   • EU GDPR (Regulation 2016/679) full compliance")
            print("   • UK GDPR (Data Protection Act 2018)")
            print("   • Dynamic extraterritorial applicability discovery")
            print("   • Cross-border transfer mechanisms (Chapter V)")
            print("   • Adequacy decision mapping and monitoring")
            print("   • Financial sector territorial compliance")
            print()
            print("🏗️ GDPR-Focused Architecture:")
            print("   • Elasticsearch with GDPR-optimized analyzers")
            print("   • FalkorDB knowledge graph with GDPR entities")
            print("   • Vector embeddings optimized for regulatory content")
            print("   • AI-powered GDPR synonym expansion")
            print("   • Article 30 compliance validation framework")
            print()
            print("💡 Quick Start:")
            print("   1. Set environment variables (OPENAI_API_KEY, ELASTICSEARCH_HOST, etc.)")
            print("   2. Ingest GDPR documents: --ingest /path/to/gdpr/docs/*.pdf")
            print("   3. Analyze: --analyze --iterations 3")
            print("   4. Generate RoPA metamodel: --generate-metamodel")
            print("   5. Create report: --generate-report")
            print("="*80)
    
    except KeyboardInterrupt:
        print("\n⚠️ Operation cancelled by user")
        return 1
    except Exception as e:
        print(f"\n❌ System error: {e}")
        logger.exception("Detailed error information:")
        return 1
    
    print("🎉 GDPR/UK GDPR RoPA Metamodel System operation completed successfully!")
    return 0


if __name__ == "__main__":
    exit(main())
