def prepare_text(self, row: pd.Series, is_source: bool = True) -> str:
    """Combine name and description for semantic matching with improved context.
    Ensures all inputs are converted to strings and properly stripped.
    """
    if is_source:
        name = str(row['name']).strip() if pd.notna(row['name']) else ""
        description = str(row['description']).strip() if pd.notna(row['description']) else ""
        return f"Title: {name}\nDescription: {description}"
    else:
        pbt_name = str(row['pbt-name']).strip() if pd.notna(row['pbt-name']) else ""
        pbt_definition = str(row['pbt-definition']).strip() if pd.notna(row['pbt-definition']) else ""
        return f"Title: {pbt_name}\nDefinition: {pbt_definition}"

def find_top_k_matches(self, source_df: pd.DataFrame, target_df: pd.DataFrame, k: int = 4) -> pd.DataFrame:
    """Find top k semantic matches using LanceDB."""
    logger.info("Processing semantic matches...")
    
    # Convert all relevant columns to string type first
    source_df = source_df.copy()
    target_df = target_df.copy()
    
    # Convert source columns to string
    source_df['name'] = source_df['name'].astype(str)
    source_df['description'] = source_df['description'].astype(str)
    
    # Convert target columns to string
    target_df['pbt-name'] = target_df['pbt-name'].astype(str)
    target_df['pbt-definition'] = target_df['pbt-definition'].astype(str)
    
    # Process target entries
    logger.info("Processing target entries...")
    target_data = self.process_embeddings(
        target_df, 
        is_source=False, 
        desc="Processing target entries"
    )
    
    # Create target table in LanceDB
    target_table = self.db.create_table(
        "target_vectors",
        data=target_data,
        mode="overwrite"
    )
    
    # Process source entries and find matches
    results = []
    batch_size = min(self.batch_size, len(source_df))
    
    for i in tqdm(range(0, len(source_df), batch_size), desc="Finding matches"):
        # Process batch of source entries
        batch_df = source_df.iloc[i:i + batch_size]
        source_data = self.process_embeddings(
            batch_df,
            is_source=True,
            desc=f"Processing batch {i//batch_size + 1}"
        )
        
        # Find matches for each source item
        for source_item in source_data:
            try:
                # Search using vector similarity
                matches = target_table.search(source_item['vector']).limit(k).to_list()
                
                result = {
                    'name': source_item['name'],
                    'description': source_item['description']
                }
                
                # Add matches
                for rank, match in enumerate(matches, 1):
                    result.update({
                        f'match_{rank}_pbt_name': match['pbt_name'],
                        f'match_{rank}_score': float(match['_distance']),
                        f'match_{rank}_definition': match['pbt_definition']
                    })
                
                # If we have fewer matches than k, fill in the remaining slots
                for rank in range(len(matches) + 1, k + 1):
                    result.update({
                        f'match_{rank}_pbt_name': 'No match',
                        f'match_{rank}_score': 0.0,
                        f'match_{rank}_definition': 'No match found'
                    })
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing match: {str(e)}")
                # Add error entry with empty matches
                result = {
                    'name': source_item['name'],
                    'description': source_item['description']
                }
                for rank in range(1, k + 1):
                    result.update({
                        f'match_{rank}_pbt_name': 'Error',
                        f'match_{rank}_score': 0.0,
                        f'match_{rank}_definition': f'Error: {str(e)}'
                    })
                results.append(result)
    
    # Convert to DataFrame
    result_df = pd.DataFrame(results)
    
    # Add summary statistics
    score_cols = [f'match_{i}_score' for i in range(1, k+1)]
    result_df['avg_score'] = result_df[score_cols].mean(axis=1)
    result_df['max_score'] = result_df[score_cols].max(axis=1)
    result_df['min_score'] = result_df[score_cols].min(axis=1)
    
    # Sort by best match score
    result_df.sort_values(by='match_1_score', ascending=False, inplace=True)
    
    return result_df

def process_embeddings(self, df: pd.DataFrame, is_source: bool = True, desc: str = "") -> List[Dict]:
    """Process dataframe and return data with embeddings."""
    processed_data = []
    
    try:
        # Process in batches
        for i in tqdm(range(0, len(df), self.batch_size), desc=desc):
            batch_df = df.iloc[i:i + self.batch_size]
            batch_texts = []
            
            # Safely create texts
            for _, row in batch_df.iterrows():
                try:
                    text = self.prepare_text(row, is_source)
                    batch_texts.append(text)
                except Exception as e:
                    logger.error(f"Error preparing text: {str(e)}")
                    text = "Error in text preparation"
                    batch_texts.append(text)
            
            # Get embeddings
            batch_embeddings = self.get_embeddings_batch(batch_texts)
            
            # Create processed items
            for idx, (_, row) in enumerate(batch_df.iterrows()):
                try:
                    item = {
                        'vector': batch_embeddings[idx],
                        'text': batch_texts[idx]
                    }
                    
                    if is_source:
                        item.update({
                            'name': str(row['name']),
                            'description': str(row['description'])
                        })
                    else:
                        item.update({
                            'pbt_name': str(row['pbt-name']),
                            'pbt_definition': str(row['pbt-definition'])
                        })
                    
                    processed_data.append(item)
                    
                except Exception as e:
                    logger.error(f"Error processing row: {str(e)}")
                    # Add error item if needed
                    error_item = {
                        'vector': [0] * self.dimension,  # zero vector
                        'text': 'Error processing item'
                    }
                    if is_source:
                        error_item.update({
                            'name': 'Error',
                            'description': str(e)
                        })
                    else:
                        error_item.update({
                            'pbt_name': 'Error',
                            'pbt_definition': str(e)
                        })
                    processed_data.append(error_item)
            
    except Exception as e:
        logger.error(f"Error in batch processing: {str(e)}")
        raise
    
    return processed_data
