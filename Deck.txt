#!/usr/bin/env python3
"""
Enhanced GDPR/UK-GDPR Knowledge Graph & RoPA System
Advanced 8-Agent Architecture with LangGraph ReAct, Reflection & Specialized Domain Agents

This system creates a comprehensive knowledge graph encompassing all GDPR and UK-GDPR concepts,
while generating a specialized RoPA metamodel for financial institutions like HSBC.

8-Agent Architecture:
1. SupervisorAgent - Coordination & orchestration
2. ResearcherAgent - Deep domain analysis & ontology creation  
3. OntologyAgent - Knowledge graph schema management
4. DataIngestionAgent - Advanced document processing & extraction
5. FinancialDomainExpertAgent - Financial industry specialization
6. GDPRAnalysisReActAgent - General GDPR analysis with tools
7. RoPASpecialistAgent - Article 30 Record of Processing Activities focus
8. ReflectionAgent - Quality assurance across all agents

Author: AI Assistant
Date: 2025
Version: 5.0.0 - Enhanced 8-Agent Multi-Specialist Architecture (Fixed)
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Literal
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import argparse
import pickle
from pathlib import Path
import hashlib

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph for multi-agent architecture
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph components
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command
from langgraph.prebuilt import create_react_agent, ToolNode

# Global Configuration - Centralized credentials and paths
GLOBAL_CONFIG = {
    # OpenAI Configuration - Only o3-mini
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "your_openai_api_key_here"),
    "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
    "OPENAI_MODEL": "o3-mini",
    
    # Elasticsearch Configuration  
    "ELASTICSEARCH_HOST": os.getenv("ELASTICSEARCH_HOST", "https://localhost:9200"),
    "ELASTICSEARCH_USERNAME": os.getenv("ELASTICSEARCH_USERNAME", "elastic"),
    "ELASTICSEARCH_PASSWORD": os.getenv("ELASTICSEARCH_PASSWORD", "changeme"),
    "ELASTICSEARCH_CA_CERTS": os.getenv("ELASTICSEARCH_CA_CERTS", None),
    "ELASTICSEARCH_VERIFY_CERTS": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "false").lower() == "true",
    
    # FalkorDB Configuration
    "FALKORDB_HOST": os.getenv("FALKORDB_HOST", "localhost"),
    "FALKORDB_PORT": int(os.getenv("FALKORDB_PORT", 6379)),
    "FALKORDB_PASSWORD": os.getenv("FALKORDB_PASSWORD", None),
    "FALKORDB_DATABASE": os.getenv("FALKORDB_DATABASE", "gdpr_comprehensive_kg"),
    
    # Document Paths
    "PDF_DOCUMENTS_PATH": os.getenv("PDF_DOCUMENTS_PATH", "./documents"),
    "OUTPUT_PATH": os.getenv("OUTPUT_PATH", "./output"),
    "MEMORY_PATH": os.getenv("MEMORY_PATH", "./memory"),
    
    # System Configuration
    "ELASTICSEARCH_INDEX": "gdpr_comprehensive_knowledge",
    "MEMORY_FILE": "gdpr_agent_memory.pkl",
    
    # Embedding Configuration
    "EMBEDDING_MODEL": "text-embedding-3-large",
    "EMBEDDING_DIMENSIONS": 3072,
    
    # Financial Industry Context
    "ORGANIZATION_TYPE": "financial_institution",
    "ORGANIZATION_NAME": "HSBC",
    "JURISDICTION_FOCUS": ["EU_GDPR", "UK_GDPR"],
    
    # Agent Configuration
    "MAX_REASONING_ITERATIONS": 5,
    "REFLECTION_THRESHOLD": 0.7,
    "SUPERVISOR_MAX_WORKERS": 8,
    "RESEARCH_DEPTH": 3,
    "ONTOLOGY_COMPLEXITY": "comprehensive"
}

# Configure logging
os.makedirs(GLOBAL_CONFIG["OUTPUT_PATH"], exist_ok=True)
os.makedirs(GLOBAL_CONFIG["MEMORY_PATH"], exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{GLOBAL_CONFIG['OUTPUT_PATH']}/gdpr_enhanced_system.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedAgentState(TypedDict):
    """Enhanced shared state for all 8 agents in the system"""
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document Processing & Ingestion
    current_document: Optional[Dict[str, Any]]
    processed_documents: List[Dict[str, Any]]
    document_chunks: List[Dict[str, Any]]
    document_metadata: Dict[str, Any]
    extraction_quality: Dict[str, float]
    
    # Research & Domain Analysis
    research_findings: List[Dict[str, Any]]
    domain_concepts: List[Dict[str, Any]]
    regulatory_mappings: Dict[str, List[str]]
    external_references: List[Dict[str, Any]]
    concept_hierarchies: Dict[str, Any]
    
    # Ontology & Schema Management
    ontology_schema: Optional[Dict[str, Any]]
    entity_types: List[Dict[str, Any]]
    relationship_types: List[Dict[str, Any]]
    schema_evolution: List[Dict[str, Any]]
    validation_rules: List[Dict[str, Any]]
    
    # Comprehensive GDPR Knowledge Graph Entities
    gdpr_articles: List[Dict[str, Any]]
    uk_gdpr_articles: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    data_subject_rights: List[Dict[str, Any]]
    principles: List[Dict[str, Any]]
    obligations: List[Dict[str, Any]]
    penalties: List[Dict[str, Any]]
    authorities: List[Dict[str, Any]]
    definitions: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    transfers: List[Dict[str, Any]]
    controllers: List[Dict[str, Any]]
    processors: List[Dict[str, Any]]
    
    # Financial Industry Specific
    financial_regulations: List[Dict[str, Any]]
    financial_data_types: List[Dict[str, Any]]
    compliance_frameworks: List[Dict[str, Any]]
    banking_processes: List[Dict[str, Any]]
    risk_assessments: List[Dict[str, Any]]
    
    # RoPA Specialized Elements
    ropa_elements: Dict[str, List[Dict[str, Any]]]
    article_30_compliance: Dict[str, Any]
    ropa_templates: List[Dict[str, Any]]
    compliance_gaps: List[Dict[str, Any]]
    
    # Synonyms and Semantic Enrichment
    generated_synonyms: Dict[str, List[str]]
    semantic_clusters: List[Dict[str, Any]]
    entity_mappings: Dict[str, str]
    concept_relationships: List[Dict[str, Any]]
    
    # Agent Coordination & Results
    current_agent: str
    agent_results: Dict[str, Any]
    agent_coordination_plan: Optional[Dict[str, Any]]
    agent_execution_order: List[str]
    inter_agent_messages: List[Dict[str, Any]]
    
    # Quality & Reflection
    reflection_feedback: List[Dict[str, Any]]
    quality_metrics: Dict[str, float]
    validation_results: List[Dict[str, Any]]
    improvement_suggestions: List[Dict[str, Any]]
    
    # Final Outputs
    ropa_metamodel: Optional[Dict[str, Any]]
    business_report: Optional[str]
    compliance_assessment: Optional[Dict[str, Any]]
    executive_summary: Optional[str]
    
    # System Performance
    confidence_scores: Dict[str, float]
    processing_times: Dict[str, float]
    error_logs: List[Dict[str, Any]]

def escape_string_for_cypher(text: str) -> str:
    """Safely escape string for Cypher queries"""
    if not text:
        return ""
    return str(text).replace("'", "''").replace("\\", "\\\\")

class EmbeddingEngine:
    """Advanced embedding generation with LLM-based synonym enrichment"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None
        )
        self.model = GLOBAL_CONFIG["EMBEDDING_MODEL"]
        self.dimensions = GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"]
        
        # Simple in-memory cache for synonyms to avoid regenerating
        self.synonym_cache = {}
        
        logger.info(f"Initialized embedding engine with {self.model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            if not text or not text.strip():
                logger.warning("Empty text provided for embedding generation")
                return [0.0] * self.dimensions
                
            response = self.client.embeddings.create(
                model=self.model,
                input=text.strip(),
                dimensions=self.dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            # Return zero vector as fallback
            return [0.0] * self.dimensions
    
    def generate_synonyms_with_llm(self, concept: str, domain_context: str = "GDPR") -> List[str]:
        """Use LLM to generate domain-specific synonyms with caching"""
        try:
            if not concept or not concept.strip():
                return []
            
            # Check cache first
            cache_key = f"{concept.lower()}_{domain_context.lower()}"
            if cache_key in self.synonym_cache:
                logger.debug(f"Using cached synonyms for: {concept}")
                return self.synonym_cache[cache_key]
            
            prompt = f"""
            Generate comprehensive synonyms and related terms for the concept "{concept}" in the context of {domain_context} and UK-GDPR compliance.

            Include various types of related terms:
            1. Direct synonyms and alternative phrasings
            2. Legal terminology variations and formal terms
            3. Technical and business terminology
            4. Abbreviations, acronyms, and common shorthand
            5. Contextual equivalents used in regulatory documents
            6. Financial industry specific terminology where relevant
            7. Cross-jurisdictional term variations (EU vs UK usage)
            8. Colloquial and informal terms used in practice

            Consider regulatory context: GDPR, UK GDPR, Data Protection Act 2018, financial regulations (FCA, PRA), banking terminology.
            
            Important: Return ONLY a JSON array of strings. No explanations or additional text.
            Maximum 15 terms. Focus on terms that would realistically appear in regulatory documents, policies, business processes, and compliance frameworks.
            
            Example format: ["term1", "term2", "term3"]
            """
            
            response = self.client.chat.completions.create(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                messages=[{"role": "user", "content": prompt}],
                reasoning_effort="medium",
                max_completion_tokens=500,
                temperature=0.3
            )
            
            content = response.choices[0].message.content.strip()
            
            # More robust JSON extraction
            synonyms = self._extract_synonyms_from_response(content, concept)
            
            # Cache the results
            if synonyms:
                self.synonym_cache[cache_key] = synonyms
                logger.debug(f"Generated and cached {len(synonyms)} synonyms for: {concept}")
            
            return synonyms
            
        except Exception as e:
            logger.error(f"Failed to generate synonyms for {concept}: {e}")
            return []
    
    def _extract_synonyms_from_response(self, content: str, original_concept: str) -> List[str]:
        """Extract synonyms from LLM response with multiple fallback strategies"""
        try:
            # Strategy 1: Try to find JSON array
            json_start = content.find('[')
            json_end = content.rfind(']') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    synonyms_json = content[json_start:json_end]
                    synonyms = json.loads(synonyms_json)
                    
                    if isinstance(synonyms, list):
                        # Filter and clean synonyms
                        cleaned_synonyms = []
                        for syn in synonyms:
                            if isinstance(syn, str) and syn.strip():
                                cleaned_syn = syn.strip().strip('"').strip("'")
                                # Avoid duplicates and overly similar terms
                                if (len(cleaned_syn) > 1 and 
                                    cleaned_syn.lower() != original_concept.lower() and
                                    cleaned_syn not in cleaned_synonyms):
                                    cleaned_synonyms.append(cleaned_syn)
                        
                        return cleaned_synonyms[:15]  # Limit to 15 synonyms
                except json.JSONDecodeError:
                    logger.debug("JSON parsing failed, trying alternative extraction")
            
            # Strategy 2: Extract from quoted strings
            import re
            quoted_terms = re.findall(r'"([^"]+)"', content)
            if quoted_terms:
                return [term.strip() for term in quoted_terms[:15] 
                       if term.strip() and term.strip().lower() != original_concept.lower()]
            
            # Strategy 3: Extract from lines or comma-separated values
            lines = content.replace(',', '\n').split('\n')
            extracted_terms = []
            for line in lines:
                line = line.strip().strip('-').strip('*').strip()
                if line and len(line) > 1 and len(line) < 100:
                    # Remove common prefixes
                    for prefix in ['- ', '* ', '• ', '1. ', '2. ', '3. ']:
                        if line.startswith(prefix):
                            line = line[len(prefix):].strip()
                    
                    if line.lower() != original_concept.lower():
                        extracted_terms.append(line)
            
            return extracted_terms[:15]
            
        except Exception as e:
            logger.error(f"Synonym extraction failed: {e}")
            return []
    
    def clear_synonym_cache(self):
        """Clear the synonym cache"""
        self.synonym_cache.clear()
        logger.info("Synonym cache cleared")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get synonym cache statistics"""
        return {
            "cached_concepts": len(self.synonym_cache),
            "total_synonyms": sum(len(synonyms) for synonyms in self.synonym_cache.values()),
            "average_synonyms_per_concept": sum(len(synonyms) for synonyms in self.synonym_cache.values()) / max(len(self.synonym_cache), 1)
        }

class VectorStore:
    """Elasticsearch-based vector storage with comprehensive GDPR schema"""
    
    def __init__(self):
        self.embedding_engine = EmbeddingEngine()
        self.client = self._create_elasticsearch_client()
        self.index_name = GLOBAL_CONFIG["ELASTICSEARCH_INDEX"]
        self._create_comprehensive_index()
    
    def _create_elasticsearch_client(self):
        """Create Elasticsearch client"""
        client_config = {
            "hosts": [GLOBAL_CONFIG["ELASTICSEARCH_HOST"]],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        if GLOBAL_CONFIG["ELASTICSEARCH_HOST"].startswith('https://'):
            client_config["verify_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]
            if GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]:
                client_config["ca_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_CA_CERTS"]
            if not GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]:
                client_config["ssl_show_warn"] = False
        
        if GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"] and GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]:
            client_config["basic_auth"] = (
                GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"],
                GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]
            )
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_comprehensive_index(self):
        """Create comprehensive index for all GDPR concepts"""
        mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "gdpr_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop", "stemmer"]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # Core content
                    "text": {
                        "type": "text",
                        "analyzer": "gdpr_analyzer",
                        "fields": {"keyword": {"type": "keyword"}}
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                        "index": True,
                        "similarity": "cosine"
                    },
                    
                    # Document metadata
                    "document_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "document_type": {"type": "keyword"},
                    
                    # GDPR Classification
                    "gdpr_category": {"type": "keyword"},
                    "article_number": {"type": "keyword"},
                    "chapter": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    
                    # Agent Source Tracking
                    "discovered_by": {"type": "keyword"},
                    "agent_confidence": {"type": "float"},
                    "processing_stage": {"type": "keyword"},
                    
                    # Entity Information
                    "entity_type": {"type": "keyword"},
                    "entity_name": {"type": "text"},
                    "entity_id": {"type": "keyword"},
                    
                    # Research & Analysis
                    "research_depth": {"type": "integer"},
                    "external_references": {"type": "text"},
                    "regulatory_context": {"type": "text"},
                    
                    # Ontology Integration
                    "ontology_class": {"type": "keyword"},
                    "parent_concepts": {"type": "keyword"},
                    "child_concepts": {"type": "keyword"},
                    
                    # Synonyms and Related Terms
                    "synonyms": {"type": "text"},
                    "related_concepts": {"type": "text"},
                    "alternative_names": {"type": "text"},
                    
                    # Financial Industry Context
                    "financial_relevance": {"type": "float"},
                    "financial_context": {"type": "text"},
                    "regulatory_framework": {"type": "keyword"},
                    "banking_relevance": {"type": "float"},
                    
                    # RoPA Specific
                    "ropa_element_type": {"type": "keyword"},
                    "article_30_relevance": {"type": "float"},
                    "compliance_requirement": {"type": "text"},
                    
                    # Relationships
                    "related_articles": {"type": "keyword"},
                    "cross_references": {"type": "keyword"},
                    "dependencies": {"type": "keyword"},
                    
                    # Quality Metrics
                    "confidence_score": {"type": "float"},
                    "validation_status": {"type": "keyword"},
                    "quality_score": {"type": "float"},
                    
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created comprehensive GDPR index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise
    
    def index_concept(self, concept_data: Dict[str, Any]):
        """Index a GDPR concept with LLM-generated embeddings and synonyms"""
        try:
            # Generate synonyms using LLM (not Elasticsearch)
            concept_name = concept_data.get("entity_name", concept_data.get("text", ""))
            if concept_name:
                domain_context = f"GDPR {concept_data.get('gdpr_category', '')} {concept_data.get('jurisdiction', '')}"
                synonyms = self.embedding_engine.generate_synonyms_with_llm(concept_name, domain_context)
                
                # Store synonyms in the concept data
                concept_data["llm_generated_synonyms"] = synonyms
                concept_data["synonym_count"] = len(synonyms)
                
                # Create enriched text for embedding (include synonyms for semantic richness)
                enriched_text = f"{concept_data.get('text', '')} {concept_name} {' '.join(synonyms)}"
                embedding = self.embedding_engine.generate_embedding(enriched_text)
                
                logger.debug(f"Generated {len(synonyms)} synonyms for concept: {concept_name}")
            else:
                # No concept name, just use text
                text_content = concept_data.get("text", "")
                embedding = self.embedding_engine.generate_embedding(text_content)
                concept_data["llm_generated_synonyms"] = []
                concept_data["synonym_count"] = 0
            
            # Prepare document with all metadata
            doc = {
                **concept_data,
                "embedding": embedding,
                "synonyms": concept_data.get("llm_generated_synonyms", []),  # Store as document field
                "synonym_metadata": {
                    "generation_method": "llm_o3_mini",
                    "domain_context": concept_data.get("gdpr_category", "GDPR"),
                    "generation_timestamp": datetime.now().isoformat()
                },
                "timestamp": datetime.now()
            }
            
            # Generate unique document ID
            entity_name = concept_data.get('entity_name', '')
            gdpr_cat = concept_data.get('gdpr_category', '')
            jurisdiction = concept_data.get('jurisdiction', '')
            doc_id = hashlib.md5(f"{entity_name}_{gdpr_cat}_{jurisdiction}".encode()).hexdigest()
            
            # Index in Elasticsearch
            self.client.index(index=self.index_name, id=doc_id, document=doc)
            logger.debug(f"Indexed concept with synonyms: {concept_name}")
            
        except Exception as e:
            logger.error(f"Failed to index concept with LLM synonyms: {e}")
            raise
    
    def semantic_search(self, query: str, filters: Dict[str, Any] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Perform semantic search with LLM-generated synonyms (not Elasticsearch synonyms)"""
        try:
            # Generate query embedding
            query_embedding = self.embedding_engine.generate_embedding(query)
            
            # Also generate synonyms for the query to expand search
            query_synonyms = self.embedding_engine.generate_synonyms_with_llm(query, "GDPR")
            expanded_query_terms = [query] + query_synonyms[:5]  # Use top 5 synonyms
            
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    }
                                }
                            }
                        ],
                        "should": [
                            # Search in main text fields
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["text^3", "entity_name^4", "regulatory_context^2"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            },
                            # Search in LLM-generated synonyms field
                            {
                                "terms": {
                                    "llm_generated_synonyms": expanded_query_terms,
                                    "boost": 2.0
                                }
                            },
                            # Search for synonym matches using nested query
                            {
                                "nested": {
                                    "path": "synonym_metadata",
                                    "query": {
                                        "match": {
                                            "synonym_metadata.domain_context": query
                                        }
                                    },
                                    "boost": 1.5
                                }
                            }
                        ],
                        "minimum_should_match": 1
                    }
                },
                "size": top_k,
                "_source": {"excludes": ["embedding"]},
                "highlight": {
                    "fields": {
                        "text": {},
                        "llm_generated_synonyms": {},
                        "entity_name": {}
                    }
                }
            }
            
            # Add filters if provided
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if isinstance(value, list):
                        filter_clauses.append({"terms": {key: value}})
                    elif isinstance(value, dict) and "gte" in value:
                        filter_clauses.append({"range": {key: value}})
                    else:
                        filter_clauses.append({"term": {key: value}})
                search_body["query"]["bool"]["filter"] = filter_clauses
            
            response = self.client.search(index=self.index_name, **search_body)
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            results = []
            for hit in hits:
                source = hit["_source"]
                result = {
                    **source,
                    "search_score": hit["_score"],
                    "search_highlights": hit.get("highlight", {}),
                    "synonym_matches": self._count_synonym_matches(source.get("llm_generated_synonyms", []), expanded_query_terms)
                }
                results.append(result)
            
            logger.debug(f"Semantic search completed: {len(results)} results for query '{query}' with {len(query_synonyms)} generated synonyms")
            return results
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []
    
    def _count_synonym_matches(self, doc_synonyms: List[str], query_terms: List[str]) -> int:
        """Count how many query terms match document synonyms"""
        if not doc_synonyms or not query_terms:
            return 0
        
        doc_synonyms_lower = [syn.lower() for syn in doc_synonyms]
        query_terms_lower = [term.lower() for term in query_terms]
        
        matches = 0
        for query_term in query_terms_lower:
            if any(query_term in doc_syn for doc_syn in doc_synonyms_lower):
                matches += 1
        
        return matches

class ComprehensiveKnowledgeGraph:
    """FalkorDB-based comprehensive GDPR/UK-GDPR knowledge graph with vector embeddings support"""
    
    def __init__(self):
        connection_kwargs = {
            "host": GLOBAL_CONFIG["FALKORDB_HOST"],
            "port": GLOBAL_CONFIG["FALKORDB_PORT"]
        }
        
        if GLOBAL_CONFIG["FALKORDB_PASSWORD"]:
            connection_kwargs["password"] = GLOBAL_CONFIG["FALKORDB_PASSWORD"]
        
        try:
            self.db = FalkorDB(**connection_kwargs)
            self.graph = self.db.select_graph(GLOBAL_CONFIG["FALKORDB_DATABASE"])
            self.embedding_engine = EmbeddingEngine()
            
            logger.info("Connected to FalkorDB for comprehensive GDPR knowledge graph with vector support")
            self._initialize_comprehensive_schema()
            self._create_vector_indexes()
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def _create_vector_indexes(self):
        """Create vector indexes for FalkorDB vector similarity search"""
        try:
            # Vector indexes for different entity types with OpenAI's embedding dimensions
            vector_indexes = [
                # Core GDPR entities with vector search
                f"CREATE VECTOR INDEX FOR (a:Article) ON (a.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (p:Principle) ON (p.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (r:Right) ON (r.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (o:Obligation) ON (o.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (d:Definition) ON (d.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                
                # Processing and data entities
                f"CREATE VECTOR INDEX FOR (pa:ProcessingActivity) ON (pa.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (dc:DataCategory) ON (dc.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (lb:LegalBasis) ON (lb.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (sm:SecurityMeasure) ON (sm.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                
                # Financial and organizational entities
                f"CREATE VECTOR INDEX FOR (fin:FinancialRegulation) ON (fin.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (ctrl:Controller) ON (ctrl.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (proc:Processor) ON (proc.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                
                # Research and ontology concepts
                f"CREATE VECTOR INDEX FOR (rc:ResearchConcept) ON (rc.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                f"CREATE VECTOR INDEX FOR (oc:OntologyClass) ON (oc.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}",
                
                # Generic entity fallback
                f"CREATE VECTOR INDEX FOR (e:Entity) ON (e.embedding) OPTIONS {{dimension:{GLOBAL_CONFIG['EMBEDDING_DIMENSIONS']}, similarityFunction:'cosine'}}"
            ]
            
            for index_query in vector_indexes:
                try:
                    self.graph.query(index_query)
                    logger.debug(f"Created vector index: {index_query.split('FOR')[1].split('ON')[0].strip()}")
                except Exception as e:
                    # Index might already exist or FalkorDB version might not support vectors
                    if "already exists" not in str(e).lower() and "syntax error" not in str(e).lower():
                        logger.warning(f"Could not create vector index: {e}")
            
            logger.info("Vector indexes created for FalkorDB knowledge graph")
            
        except Exception as e:
            logger.warning(f"Vector index creation failed - FalkorDB might not support vectors: {e}")
            logger.info("Continuing without vector indexes - using traditional graph search only")
    
    def add_comprehensive_entity(self, entity_type: str, entity_data: Dict[str, Any]):
        """Add GDPR entity to knowledge graph with vector embeddings and LLM-generated synonyms"""
        try:
            entity_name = escape_string_for_cypher(str(entity_data.get("name", "")))
            entity_id = escape_string_for_cypher(str(entity_data.get("id", entity_name)))
            
            # Generate embeddings for the entity
            entity_text = self._prepare_entity_text_for_embedding(entity_data)
            if entity_text:
                embedding = self.embedding_engine.generate_embedding(entity_text)
                # Convert to FalkorDB vector format
                embedding_str = f"vecf32([{','.join(map(str, embedding))}])"
            else:
                embedding_str = f"vecf32([{','.join(['0.0'] * GLOBAL_CONFIG['EMBEDDING_DIMENSIONS'])}])"
            
            # Generate synonyms using LLM if not already present
            if "llm_generated_synonyms" not in entity_data and entity_name:
                domain_context = f"GDPR {entity_data.get('gdpr_category', '')} {entity_data.get('jurisdiction', '')}"
                synonyms = self.embedding_engine.generate_synonyms_with_llm(entity_name, domain_context)
                entity_data["llm_generated_synonyms"] = synonyms
                logger.debug(f"Generated {len(synonyms)} synonyms for KG entity: {entity_name}")
            
            # Create base entity with vector embedding and synonym support
            synonyms_json = json.dumps(entity_data.get("llm_generated_synonyms", []))
            jurisdiction = escape_string_for_cypher(entity_data.get("jurisdiction", "EU_GDPR"))
            discovered_by = escape_string_for_cypher(entity_data.get("discovered_by", "system"))
            
            query = f"""
            MERGE (e:{entity_type} {{name: '{entity_name}'}})
            SET e.id = '{entity_id}',
                e.jurisdiction = '{jurisdiction}',
                e.confidence = {entity_data.get("confidence", 1.0)},
                e.discovered_by = '{discovered_by}',
                e.embedding = {embedding_str},
                e.embedding_model = '{GLOBAL_CONFIG["EMBEDDING_MODEL"]}',
                e.llm_synonyms = {synonyms_json},
                e.synonym_count = {len(entity_data.get("llm_generated_synonyms", []))},
                e.synonym_generation_method = 'llm_o3_mini',
                e.vector_indexed = true,
                e.timestamp = datetime()
            """
            
            # Add type-specific properties safely
            for key, value in entity_data.items():
                if key not in ["name", "id", "jurisdiction", "confidence", "discovered_by", "llm_generated_synonyms"] and isinstance(value, (str, int, float)):
                    if isinstance(value, str):
                        safe_value = escape_string_for_cypher(str(value))
                        if len(safe_value) < 1000:  # Avoid very long strings in graph
                            query += f"""
                            SET e.{key} = '{safe_value}'
                            """
                    else:
                        query += f"""
                        SET e.{key} = {value}
                        """
            
            self.graph.query(query)
            
            # Create synonym relationships for better graph traversal
            if entity_data.get("llm_generated_synonyms"):
                self._create_synonym_relationships(entity_name, entity_data["llm_generated_synonyms"], entity_type)
            
            logger.debug(f"Added {entity_type} entity with vector embedding: {entity_name}")
            
        except Exception as e:
            logger.error(f"Failed to add {entity_type} entity with vector embedding: {e}")
    
    def _prepare_entity_text_for_embedding(self, entity_data: Dict[str, Any]) -> str:
        """Prepare comprehensive text for entity embedding"""
        text_parts = []
        
        # Core entity information
        if entity_data.get("name"):
            text_parts.append(entity_data["name"])
        
        if entity_data.get("title"):
            text_parts.append(entity_data["title"])
        
        if entity_data.get("description"):
            text_parts.append(entity_data["description"])
        
        if entity_data.get("content"):
            text_parts.append(entity_data["content"][:500])  # Limit content length
        
        if entity_data.get("purpose"):
            text_parts.append(entity_data["purpose"])
        
        # Add regulatory context
        if entity_data.get("gdpr_category"):
            text_parts.append(f"GDPR category: {entity_data['gdpr_category']}")
        
        if entity_data.get("jurisdiction"):
            text_parts.append(f"Jurisdiction: {entity_data['jurisdiction']}")
        
        # Add synonyms for richer embedding
        if entity_data.get("llm_generated_synonyms"):
            text_parts.extend(entity_data["llm_generated_synonyms"][:5])  # Top 5 synonyms
        
        return " ".join(text_parts)
    
    def vector_similarity_search(self, query_text: str, entity_types: List[str] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Perform vector similarity search in FalkorDB using embeddings"""
        try:
            # Generate embedding for query
            query_embedding = self.embedding_engine.generate_embedding(query_text)
            query_vector_str = f"vecf32([{','.join(map(str, query_embedding))}])"
            
            # Build entity type filter
            entity_type_filter = ""
            if entity_types:
                type_conditions = " OR ".join([f"n:{entity_type}" for entity_type in entity_types])
                entity_type_filter = f"WHERE {type_conditions}"
            
            # FalkorDB vector similarity query using cosine similarity
            vector_query = f"""
            MATCH (n)
            {entity_type_filter}
            WHERE n.embedding IS NOT NULL
            WITH n, vec.cosineDistance(n.embedding, {query_vector_str}) AS similarity_score
            WHERE similarity_score IS NOT NULL
            RETURN n, similarity_score
            ORDER BY similarity_score ASC
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(vector_query)
            except Exception as e:
                logger.warning(f"Vector similarity search not supported: {e}")
                # Fallback to regular text search
                return self._fallback_text_search(query_text, entity_types, top_k)
            
            formatted_results = []
            for record in result.result_set:
                node = self._format_node(record[0])
                similarity_score = float(record[1]) if record[1] is not None else 1.0
                
                # Convert distance to similarity (lower distance = higher similarity)
                similarity = 1.0 - min(similarity_score, 1.0)
                
                formatted_results.append({
                    "entity": node,
                    "similarity_score": similarity,
                    "distance_score": similarity_score,
                    "search_type": "vector_similarity",
                    "query_text": query_text
                })
            
            logger.debug(f"Vector similarity search completed: {len(formatted_results)} results for '{query_text}'")
            return formatted_results
            
        except Exception as e:
            logger.error(f"Vector similarity search failed: {e}")
            return self._fallback_text_search(query_text, entity_types, top_k)
    
    def _fallback_text_search(self, query_text: str, entity_types: List[str] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Fallback text search when vector search is not available"""
        try:
            query_escaped = escape_string_for_cypher(query_text)
            
            # Build entity type filter
            entity_type_filter = ""
            if entity_types:
                type_conditions = " OR ".join([f"n:{entity_type}" for entity_type in entity_types])
                entity_type_filter = f"WHERE {type_conditions}"
            
            text_query = f"""
            MATCH (n)
            {entity_type_filter}
            WHERE n.name CONTAINS '{query_escaped}' OR n.title CONTAINS '{query_escaped}' OR n.description CONTAINS '{query_escaped}'
            RETURN n
            LIMIT {top_k}
            """
            
            result = self.graph.query(text_query)
            
            formatted_results = []
            for record in result.result_set:
                node = self._format_node(record[0])
                
                formatted_results.append({
                    "entity": node,
                    "similarity_score": 0.7,  # Default similarity for text match
                    "distance_score": 0.3,
                    "search_type": "text_fallback",
                    "query_text": query_text
                })
            
            logger.debug(f"Fallback text search completed: {len(formatted_results)} results")
            return formatted_results
            
        except Exception as e:
            logger.error(f"Fallback text search also failed: {e}")
            return []
    
    def hybrid_search(self, query_text: str, max_depth: int = 2, top_k: int = 15) -> List[Dict[str, Any]]:
        """Hybrid search combining vector similarity and graph traversal"""
        try:
            # Get vector similarity results
            vector_results = self.vector_similarity_search(query_text, top_k=top_k//2)
            
            # Get traditional graph search results
            graph_results = self.query_comprehensive_knowledge(query_text, max_depth=max_depth)
            
            # Combine and deduplicate results
            combined_results = []
            seen_entities = set()
            
            # Add vector results with high priority
            for result in vector_results:
                entity_id = result["entity"].get("id") if result["entity"] else None
                if entity_id and entity_id not in seen_entities:
                    result["result_type"] = "vector_similarity"
                    combined_results.append(result)
                    seen_entities.add(entity_id)
            
            # Add graph results
            for result in graph_results[:top_k//2]:
                entity_id = result["start_entity"].get("id") if result["start_entity"] else None
                if entity_id and entity_id not in seen_entities:
                    # Convert graph result format to match vector results
                    combined_result = {
                        "entity": result["start_entity"],
                        "similarity_score": result.get("relevance_score", 0.5),
                        "distance_score": 1.0 - result.get("relevance_score", 0.5),
                        "search_type": "graph_traversal",
                        "query_text": query_text,
                        "result_type": "graph_relationship",
                        "related_entity": result.get("related_entity"),
                        "relationship_path": result.get("relationship_path", [])
                    }
                    combined_results.append(combined_result)
                    seen_entities.add(entity_id)
            
            # Sort by similarity score (highest first)
            combined_results.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            logger.debug(f"Hybrid search completed: {len(combined_results)} total results ({len(vector_results)} vector + {len([r for r in combined_results if r.get('result_type') == 'graph_relationship'])} graph)")
            return combined_results[:top_k]
            
        except Exception as e:
            logger.error(f"Hybrid search failed: {e}")
            return []
    
    def find_similar_entities(self, entity_name: str, entity_type: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Find entities similar to a given entity using vector embeddings"""
        try:
            # First, find the target entity
            entity_filter = f"AND n:{entity_type}" if entity_type else ""
            escaped_entity_name = escape_string_for_cypher(entity_name)
            
            find_query = f"""
            MATCH (n)
            WHERE n.name CONTAINS '{escaped_entity_name}'
            {entity_filter}
            RETURN n
            LIMIT 1
            """
            
            result = self.graph.query(find_query)
            if not result.result_set:
                logger.warning(f"Entity '{entity_name}' not found")
                return []
            
            target_entity = result.result_set[0][0]
            target_embedding = getattr(target_entity, 'embedding', None)
            
            if not target_embedding:
                logger.warning(f"Entity '{entity_name}' has no embedding")
                return []
            
            # Find similar entities using vector similarity
            similarity_query = f"""
            MATCH (n)
            WHERE n.embedding IS NOT NULL AND n.name <> '{escaped_entity_name}'
            WITH n, vec.cosineDistance(n.embedding, {target_embedding}) AS similarity_score
            WHERE similarity_score IS NOT NULL
            RETURN n, similarity_score
            ORDER BY similarity_score ASC
            LIMIT {top_k}
            """
            
            similarity_result = self.graph.query(similarity_query)
            
            similar_entities = []
            for record in similarity_result.result_set:
                entity = self._format_node(record[0])
                similarity_score = 1.0 - float(record[1])  # Convert distance to similarity
                
                similar_entities.append({
                    "entity": entity,
                    "similarity_score": similarity_score,
                    "target_entity": entity_name
                })
            
            return similar_entities
            
        except Exception as e:
            logger.error(f"Similar entity search failed: {e}")
            return []
    
    def _initialize_comprehensive_schema(self):
        """Initialize comprehensive schema for GDPR/UK-GDPR knowledge graph"""
        try:
            # Create indexes for performance
            indexes = [
                # Core GDPR entities
                "CREATE INDEX FOR (a:Article) ON (a.number)",
                "CREATE INDEX FOR (c:Chapter) ON (c.number)",
                "CREATE INDEX FOR (p:Principle) ON (p.name)",
                "CREATE INDEX FOR (r:Right) ON (r.name)",
                "CREATE INDEX FOR (o:Obligation) ON (o.name)",
                "CREATE INDEX FOR (d:Definition) ON (d.term)",
                
                # Legal bases and processing
                "CREATE INDEX FOR (lb:LegalBasis) ON (lb.basis_type)",
                "CREATE INDEX FOR (pa:ProcessingActivity) ON (pa.name)",
                "CREATE INDEX FOR (dc:DataCategory) ON (dc.name)",
                "CREATE INDEX FOR (sm:SecurityMeasure) ON (sm.type)",
                
                # Organizational entities
                "CREATE INDEX FOR (ctrl:Controller) ON (ctrl.name)",
                "CREATE INDEX FOR (proc:Processor) ON (proc.name)",
                "CREATE INDEX FOR (auth:Authority) ON (auth.name)",
                
                # Financial industry specific
                "CREATE INDEX FOR (fin:FinancialRegulation) ON (fin.name)",
                "CREATE INDEX FOR (fd:FinancialData) ON (fd.type)",
                
                # Research and ontology
                "CREATE INDEX FOR (rc:ResearchConcept) ON (rc.name)",
                "CREATE INDEX FOR (oc:OntologyClass) ON (oc.class_name)",
                
                # LLM-generated synonyms support
                "CREATE INDEX FOR (syn:Synonym) ON (syn.term)",
                "CREATE INDEX FOR (syn:Synonym) ON (syn.source_entity)",
                
                # Document and source tracking
                "CREATE INDEX FOR (doc:Document) ON (doc.source)",
                "CREATE INDEX FOR (ent:Entity) ON (ent.name)"
            ]
            
            for index_query in indexes:
                try:
                    self.graph.query(index_query)
                except:
                    pass  # Index might already exist
            
            logger.info("Comprehensive knowledge graph schema initialized with synonym support")
            
        except Exception as e:
            logger.warning(f"Could not initialize all graph indexes: {e}")
    
    def _create_synonym_relationships(self, entity_name: str, synonyms: List[str], entity_type: str):
        """Create synonym relationships in the knowledge graph"""
        try:
            entity_name_safe = escape_string_for_cypher(entity_name)
            
            for synonym in synonyms[:10]:  # Limit to top 10 synonyms to avoid graph bloat
                synonym_safe = escape_string_for_cypher(str(synonym))
                if len(synonym_safe) > 2 and len(synonym_safe) < 100:  # Valid synonym length
                    query = f"""
                    MATCH (e:{entity_type} {{name: '{entity_name_safe}'}})
                    MERGE (s:Synonym {{term: '{synonym_safe}'}})
                    MERGE (e)-[:HAS_SYNONYM]->(s)
                    SET s.source_entity = '{entity_name_safe}',
                        s.generation_method = 'llm_o3_mini',
                        s.entity_type = '{entity_type}',
                        s.timestamp = datetime()
                    """
                    self.graph.query(query)
                    
        except Exception as e:
            logger.error(f"Failed to create synonym relationships: {e}")
    
    def create_relationships(self, relationships: List[Dict[str, Any]]):
        """Create relationships between entities"""
        try:
            for rel in relationships:
                source_name = escape_string_for_cypher(str(rel.get("source", "")))
                target_name = escape_string_for_cypher(str(rel.get("target", "")))
                rel_type = rel.get("type", "RELATED_TO")
                confidence = rel.get("confidence", 1.0)
                
                if source_name and target_name:
                    query = f"""
                    MATCH (a) WHERE a.name = '{source_name}' OR a.id = '{source_name}'
                    MATCH (b) WHERE b.name = '{target_name}' OR b.id = '{target_name}'
                    MERGE (a)-[r:{rel_type}]->(b)
                    SET r.confidence = {confidence},
                        r.timestamp = datetime()
                    """
                    self.graph.query(query)
            
        except Exception as e:
            logger.error(f"Failed to create relationships: {e}")
    
    def query_comprehensive_knowledge(self, concept: str, max_depth: int = 3) -> List[Dict[str, Any]]:
        """Query comprehensive knowledge about any GDPR concept using LLM-generated synonyms"""
        try:
            concept_escaped = escape_string_for_cypher(concept)
            
            # Generate synonyms for the search concept
            concept_synonyms = self.embedding_engine.generate_synonyms_with_llm(concept, "GDPR")
            
            # Build comprehensive search query that includes synonyms
            synonym_conditions = []
            for synonym in concept_synonyms[:5]:  # Use top 5 synonyms
                synonym_escaped = escape_string_for_cypher(synonym)
                synonym_conditions.append(f"start.name CONTAINS '{synonym_escaped}'")
                synonym_conditions.append(f"start.title CONTAINS '{synonym_escaped}'")
            
            synonym_clause = " OR ".join(synonym_conditions) if synonym_conditions else ""
            
            query = f"""
            MATCH path = (start)-[*1..{max_depth}]-(related)
            WHERE (
                start.name CONTAINS '{concept_escaped}' OR 
                start.title CONTAINS '{concept_escaped}' OR
                start.purpose CONTAINS '{concept_escaped}' OR
                start.basis_type CONTAINS '{concept_escaped}' OR
                start.id CONTAINS '{concept_escaped}'
                {' OR ' + synonym_clause if synonym_clause else ''}
            )
            RETURN DISTINCT 
                start, 
                related, 
                relationships(path), 
                length(path) as distance,
                labels(start) as start_labels,
                labels(related) as related_labels,
                start.llm_synonyms as start_synonyms,
                related.llm_synonyms as related_synonyms
            ORDER BY distance, start.confidence DESC
            LIMIT 25
            """
            
            result = self.graph.query(query)
            
            formatted_results = []
            for record in result.result_set:
                result_item = {
                    "start_entity": self._format_node(record[0]),
                    "related_entity": self._format_node(record[1]),
                    "relationship_path": [str(rel) for rel in record[2]],
                    "distance": record[3],
                    "start_labels": record[4],
                    "related_labels": record[5],
                    "start_synonyms": record[6] if len(record) > 6 else [],
                    "related_synonyms": record[7] if len(record) > 7 else [],
                    "relevance_score": 1.0 / (record[3] + 1),
                    "synonym_match": self._check_synonym_match(concept, concept_synonyms, record)
                }
                formatted_results.append(result_item)
            
            # Also search specifically in synonym nodes
            try:
                synonym_results = self._search_synonym_nodes(concept, concept_synonyms)
                formatted_results.extend(synonym_results)
            except Exception as e:
                logger.debug(f"Synonym node search failed (non-critical): {e}")
            
            logger.debug(f"Knowledge graph search completed: {len(formatted_results)} results for '{concept}' with {len(concept_synonyms)} synonyms")
            return formatted_results
            
        except Exception as e:
            logger.error(f"Knowledge query failed: {e}")
            return []
    
    def _search_synonym_nodes(self, concept: str, concept_synonyms: List[str]) -> List[Dict[str, Any]]:
        """Search specifically in synonym nodes for additional matches"""
        try:
            concept_escaped = escape_string_for_cypher(concept)
            
            # Search for direct matches in synonym nodes
            query = f"""
            MATCH (syn:Synonym)-[:HAS_SYNONYM]-(entity)
            WHERE syn.term CONTAINS '{concept_escaped}'
            RETURN syn, entity, entity.llm_synonyms as entity_synonyms
            LIMIT 10
            """
            
            result = self.graph.query(query)
            synonym_results = []
            
            for record in result.result_set:
                synonym_node = self._format_node(record[0])
                entity_node = self._format_node(record[1])
                entity_synonyms = record[2] if len(record) > 2 else []
                
                synonym_results.append({
                    "start_entity": synonym_node,
                    "related_entity": entity_node,
                    "relationship_path": ["HAS_SYNONYM"],
                    "distance": 1,
                    "start_labels": ["Synonym"],
                    "related_labels": entity_node.get("labels", []) if entity_node else [],
                    "start_synonyms": [],
                    "related_synonyms": entity_synonyms,
                    "relevance_score": 0.9,  # High relevance for direct synonym matches
                    "synonym_match": True
                })
            
            return synonym_results
            
        except Exception as e:
            logger.error(f"Synonym node search failed: {e}")
            return []
    
    def _check_synonym_match(self, original_concept: str, concept_synonyms: List[str], record) -> bool:
        """Check if the result contains synonym matches"""
        try:
            # Check if any of the concept synonyms appear in the result
            start_synonyms = record[6] if len(record) > 6 and record[6] else []
            related_synonyms = record[7] if len(record) > 7 and record[7] else []
            
            all_result_synonyms = []
            if isinstance(start_synonyms, list):
                all_result_synonyms.extend(start_synonyms)
            if isinstance(related_synonyms, list):
                all_result_synonyms.extend(related_synonyms)
            
            # Convert to lowercase for comparison
            result_synonyms_lower = [syn.lower() for syn in all_result_synonyms if isinstance(syn, str)]
            concept_synonyms_lower = [syn.lower() for syn in concept_synonyms]
            
            # Check for any overlap
            return any(
                any(concept_syn in result_syn or result_syn in concept_syn 
                    for result_syn in result_synonyms_lower)
                for concept_syn in concept_synonyms_lower
            )
            
        except:
            return False
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if not node:
            return None
        
        try:
            if hasattr(node, 'properties'):
                properties = dict(node.properties)
                properties['labels'] = list(node.labels) if hasattr(node, 'labels') else []
                return properties
            else:
                return {"id": str(node)}
        except:
            return {"id": str(node)}

# Enhanced Agent Tools with Vector Support
@tool
def search_vector_knowledge(query: str, category: str = None, agent_filter: str = None) -> List[Dict[str, Any]]:
    """Search both Elasticsearch and FalkorDB vector knowledge for GDPR concepts"""
    vector_store = VectorStore()
    kg = ComprehensiveKnowledgeGraph()
    
    # Search Elasticsearch
    filters = {}
    if category:
        filters["gdpr_category"] = category
    if agent_filter:
        filters["discovered_by"] = agent_filter
    
    es_results = vector_store.semantic_search(query, filters=filters, top_k=5)
    
    # Search FalkorDB vectors
    kg_vector_results = kg.vector_similarity_search(query, top_k=5)
    
    # Combine results
    combined_results = []
    combined_results.extend([{**result, "source": "elasticsearch"} for result in es_results])
    combined_results.extend([{**result, "source": "falkordb_vector"} for result in kg_vector_results])
    
    return combined_results

@tool 
def query_knowledge_graph(concept: str, depth: int = 2, use_vector: bool = True) -> List[Dict[str, Any]]:
    """Query the comprehensive GDPR knowledge graph with optional vector similarity"""
    kg = ComprehensiveKnowledgeGraph()
    
    if use_vector:
        # Use hybrid search (vector + graph traversal)
        return kg.hybrid_search(concept, max_depth=depth, top_k=10)
    else:
        # Traditional graph search only
        return kg.query_comprehensive_knowledge(concept, max_depth=depth)

@tool
def find_similar_concepts(concept: str, concept_type: str = None) -> List[Dict[str, Any]]:
    """Find concepts similar to the given concept using vector embeddings"""
    kg = ComprehensiveKnowledgeGraph()
    return kg.find_similar_entities(concept, entity_type=concept_type, top_k=5)

@tool
def vector_similarity_search(query: str, entity_types: List[str] = None) -> List[Dict[str, Any]]:
    """Perform pure vector similarity search in FalkorDB"""
    kg = ComprehensiveKnowledgeGraph()
    return kg.vector_similarity_search(query, entity_types=entity_types, top_k=8)

@tool
def generate_concept_synonyms(concept: str, domain: str = "GDPR") -> List[str]:
    """Generate synonyms for a GDPR concept using LLM"""
    embedding_engine = EmbeddingEngine()
    return embedding_engine.generate_synonyms_with_llm(concept, domain)

@tool
def store_research_finding(finding: Dict[str, Any]) -> str:
    """Store research finding in knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    kg.add_comprehensive_entity("ResearchConcept", finding)
    return f"Stored research finding: {finding.get('name', 'Unknown')}"

@tool
def create_ontology_class(class_data: Dict[str, Any]) -> str:
    """Create new ontology class in knowledge graph"""
    kg = ComprehensiveKnowledgeGraph()
    kg.add_comprehensive_entity("OntologyClass", class_data)
    return f"Created ontology class: {class_data.get('class_name', 'Unknown')}"

# Enhanced Agent Implementations

class ResearcherAgent:
    """Deep domain analysis and ontology creation agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.research_prompt = """You are an expert regulatory researcher specializing in GDPR, UK-GDPR, and financial services compliance.

Your role is to conduct comprehensive domain analysis to understand the complete landscape of data protection regulation.

CORE RESPONSIBILITIES:
1. DOMAIN MAPPING: Analyze documents to identify all regulatory concepts, not just obvious ones
2. REGULATORY RESEARCH: Cross-reference GDPR with UK-GDPR to identify similarities and differences  
3. ONTOLOGY FOUNDATION: Research existing ontologies (FIBO, legal ontologies) for integration opportunities
4. CONCEPT DISCOVERY: Identify implicit concepts that may not be explicitly stated but are regulatory requirements
5. RELATIONSHIP ANALYSIS: Understand how different regulatory concepts relate to each other

FINANCIAL SERVICES FOCUS:
- Understand how GDPR applies specifically to banking, insurance, investment services
- Research FCA, PRA, and other financial regulatory alignment with GDPR
- Identify financial data types and their GDPR classifications
- Analyze cross-border data flows in financial services context

RESEARCH METHODOLOGY:
- Use systematic analysis of regulatory text
- Identify concept hierarchies and taxonomies
- Research authoritative sources and regulatory guidance
- Analyze regulatory templates and forms (DPA templates, ICO guidance)
- Cross-reference with international standards

OUTPUT REQUIREMENTS:
- Comprehensive concept inventory with definitions
- Regulatory concept relationships and hierarchies  
- Financial industry specific mappings
- External reference integration points
- Ontology design recommendations
- Research confidence scoring

You should provide thorough, evidence-based analysis suitable for building a comprehensive regulatory knowledge base."""
    
    def research_domain(self, document_text: str, research_focus: List[str] = None) -> Dict[str, Any]:
        """Conduct comprehensive domain research"""
        try:
            research_prompt = f"""
            Conduct comprehensive regulatory domain research on this document:
            
            Document Text: {document_text[:5000]}...
            
            Research Focus Areas: {research_focus or ['comprehensive']}
            
            {self.research_prompt}
            
            Analyze this document and provide:
            
            1. REGULATORY CONCEPT INVENTORY:
            - All GDPR/UK-GDPR concepts identified (explicit and implicit)
            - Concept definitions and regulatory context
            - Cross-references to specific articles/sections
            - Financial services relevance scoring (0-1)
            
            2. CONCEPT HIERARCHIES:
            - Parent-child relationships between concepts
            - Taxonomic organization
            - Dependency mappings
            
            3. JURISDICTIONAL ANALYSIS:
            - EU GDPR vs UK GDPR differences identified
            - Territorial scope implications
            - Cross-border considerations for financial services
            
            4. FINANCIAL SERVICES MAPPING:
            - Banking/financial specific applications
            - FCA/PRA regulatory alignment
            - Industry data types and processing activities
            
            5. ONTOLOGY RECOMMENDATIONS:
            - Suggested ontology classes and properties
            - Integration points with FIBO or other standards
            - Relationship types needed
            
            6. EXTERNAL REFERENCES:
            - Relevant regulatory guidance documents
            - International standards alignment
            - Industry best practices
            
            Return comprehensive JSON with all findings, confidence scores, and evidence references.
            """
            
            response = self.llm.invoke([HumanMessage(content=research_prompt)])
            content = response.content
            
            # Try to extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    research_data = json.loads(content[json_start:json_end])
                    return {
                        "agent_type": "researcher",
                        "research_findings": research_data,
                        "raw_analysis": content,
                        "confidence": self._calculate_research_confidence(research_data),
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            # Fallback if JSON extraction fails
            return {
                "agent_type": "researcher", 
                "research_findings": {"raw_analysis": content},
                "confidence": 0.6,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Research analysis failed: {e}")
            return {"error": str(e), "agent_type": "researcher"}
    
    def _calculate_research_confidence(self, research_data: Dict[str, Any]) -> float:
        """Calculate confidence score for research findings"""
        if not isinstance(research_data, dict):
            return 0.5
        
        # Score based on completeness of research sections
        expected_sections = [
            "regulatory_concept_inventory",
            "concept_hierarchies", 
            "jurisdictional_analysis",
            "financial_services_mapping",
            "ontology_recommendations",
            "external_references"
        ]
        
        present_sections = sum(1 for section in expected_sections if section in research_data)
        return min(present_sections / len(expected_sections), 1.0)

class OntologyAgent:
    """Knowledge graph schema management and ontology creation agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.ontology_prompt = """You are an expert ontology engineer and knowledge architect specializing in regulatory and legal domain ontologies.

Your role is to create comprehensive, well-structured ontologies for GDPR and UK-GDPR compliance knowledge.

CORE RESPONSIBILITIES:
1. ONTOLOGY DESIGN: Create formal ontology structures with classes, properties, and relationships
2. SCHEMA MANAGEMENT: Design and evolve knowledge graph schemas
3. SEMANTIC CONSISTENCY: Ensure logical consistency and semantic coherence
4. INTEROPERABILITY: Design for integration with existing standards (FIBO, Dublin Core, legal ontologies)
5. VALIDATION: Create rules and constraints for data quality

ONTOLOGY DESIGN PRINCIPLES:
- Follow established ontology engineering methodologies
- Ensure clear semantic distinctions between concepts
- Create reusable and extensible structures
- Maintain separation of concerns (regulatory vs business vs technical)
- Design for automated reasoning and inference

FINANCIAL SERVICES INTEGRATION:
- Align with Financial Industry Business Ontology (FIBO) where applicable
- Integrate financial regulatory concepts (FCA, PRA, Basel, MiFID)
- Support complex financial data processing scenarios
- Enable compliance reporting and audit trails

TECHNICAL REQUIREMENTS:
- Compatible with graph database schemas (FalkorDB/Redis)
- Support for semantic search and retrieval
- Extensible for future regulatory changes
- Performance optimized for large-scale data

OUTPUT SPECIFICATIONS:
- Formal ontology classes with properties and constraints
- Relationship types with cardinality and semantics
- Validation rules and integrity constraints
- Schema evolution and versioning strategies
- Integration specifications for external ontologies

You should create production-ready ontology schemas suitable for enterprise knowledge management."""
        
        self.tools = [create_ontology_class, store_research_finding]
    
    def create_ontology_schema(self, research_findings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create comprehensive ontology schema from research findings"""
        try:
            ontology_prompt = f"""
            Create a comprehensive ontology schema based on these research findings:
            
            Research Findings: {json.dumps(research_findings[:10], indent=2, default=str)[:8000]}...
            
            {self.ontology_prompt}
            
            Design a complete ontology schema including:
            
            1. CORE ONTOLOGY CLASSES:
            - GDPR regulatory concepts (Article, Chapter, Principle, Right, Obligation)
            - Data processing concepts (Controller, Processor, ProcessingActivity, DataCategory)
            - Legal concepts (LegalBasis, Consent, LegitimateInterest)
            - Organizational concepts (DataProtectionOfficer, SupervisoryAuthority)
            - Technical concepts (SecurityMeasure, TechnicalMeasure, OrganizationalMeasure)
            
            2. FINANCIAL SERVICES EXTENSIONS:
            - Financial data types (PaymentData, KYCData, CreditData)
            - Financial processes (AccountOpening, LoanProcessing, RiskAssessment)
            - Regulatory frameworks (FCA, PRA, Basel, MiFID)
            - Financial entities (Bank, InsuranceCompany, InvestmentFirm)
            
            3. RELATIONSHIP TYPES:
            - Hierarchical relationships (subClassOf, partOf)
            - Regulatory relationships (regulatedBy, requires, compliesWith)
            - Process relationships (uses, produces, involves)
            - Temporal relationships (precedes, during, after)
            
            4. PROPERTIES AND ATTRIBUTES:
            - Data properties (name, description, identifier, timestamp)
            - Regulatory properties (jurisdiction, article_reference, compliance_status)
            - Business properties (purpose, legal_basis, retention_period)
            - Technical properties (encryption_required, access_controls)
            
            5. VALIDATION RULES:
            - Cardinality constraints
            - Value restrictions
            - Business rules and compliance checks
            - Data quality constraints
            
            6. INTEGRATION SPECIFICATIONS:
            - FIBO alignment mappings
            - Dublin Core metadata integration
            - External vocabulary references
            
            Return complete ontology specification as structured JSON suitable for implementation.
            """
            
            response = self.llm.invoke([HumanMessage(content=ontology_prompt)])
            content = response.content
            
            # Extract JSON schema
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    ontology_schema = json.loads(content[json_start:json_end])
                    
                    # Store ontology classes in knowledge graph
                    self._implement_ontology_schema(ontology_schema)
                    
                    return {
                        "agent_type": "ontology",
                        "ontology_schema": ontology_schema,
                        "implementation_status": "completed",
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "ontology",
                "ontology_schema": {"raw_schema": content},
                "implementation_status": "partial",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Ontology creation failed: {e}")
            return {"error": str(e), "agent_type": "ontology"}
    
    def _implement_ontology_schema(self, schema: Dict[str, Any]):
        """Implement ontology schema in knowledge graph"""
        try:
            kg = ComprehensiveKnowledgeGraph()
            
            # Create ontology classes
            if "core_classes" in schema:
                for class_name, class_def in schema["core_classes"].items():
                    ontology_class = {
                        "class_name": class_name,
                        "definition": class_def.get("definition", ""),
                        "properties": class_def.get("properties", []),
                        "parent_classes": class_def.get("parent_classes", []),
                        "discovered_by": "ontology_agent"
                    }
                    kg.add_comprehensive_entity("OntologyClass", ontology_class)
            
            logger.info("Ontology schema implemented in knowledge graph")
            
        except Exception as e:
            logger.error(f"Ontology implementation failed: {e}")

class DataIngestionAgent:
    """Advanced document processing and data extraction agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.ingestion_prompt = """You are an expert data ingestion engineer specializing in regulatory document processing and structured data extraction.

Your role is to process diverse document formats and extract structured regulatory information with high accuracy and completeness.

CORE RESPONSIBILITIES:
1. DOCUMENT PROCESSING: Handle multiple formats (PDF, Word, XML, HTML, text)
2. CONTENT EXTRACTION: Extract structured information from unstructured regulatory text
3. QUALITY ASSESSMENT: Score extraction quality and identify processing issues
4. METADATA GENERATION: Create comprehensive document metadata
5. INCREMENTAL UPDATES: Handle document versioning and change detection

EXTRACTION TECHNIQUES:
- Regulatory structure recognition (articles, sections, subsections)
- Entity extraction (legal terms, definitions, obligations)
- Relationship identification (references, dependencies, hierarchies)
- Table and form processing for structured data
- Multi-language support for EU vs UK regulatory differences

QUALITY ASSURANCE:
- Content completeness verification
- Extraction accuracy scoring
- Error detection and correction suggestions
- Confidence scoring for extracted elements
- Validation against known regulatory structures

TECHNICAL CAPABILITIES:
- Large document chunking strategies
- Memory-efficient processing for enterprise volumes
- Parallel processing coordination
- Error handling and recovery
- Progress tracking and monitoring

OUTPUT REQUIREMENTS:
- Structured data in standardized formats
- Comprehensive document metadata
- Extraction quality metrics
- Processing performance data
- Error logs and improvement suggestions

You should ensure high-quality, reliable data extraction suitable for regulatory compliance knowledge systems."""
    
    def process_document(self, document_text: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Process document with advanced extraction"""
        try:
            processing_prompt = f"""
            Process this regulatory document for comprehensive data extraction:
            
            Document: {document_text[:6000]}...
            
            Metadata: {json.dumps(document_metadata, indent=2)}
            
            {self.ingestion_prompt}
            
            Extract and structure the following:
            
            1. DOCUMENT STRUCTURE:
            - Document type classification
            - Hierarchical structure (chapters, articles, sections)
            - Table of contents extraction
            - Cross-reference mapping
            
            2. REGULATORY CONTENT:
            - All GDPR/UK-GDPR articles and provisions
            - Legal definitions and terminology
            - Obligations and requirements
            - Rights and procedures
            - Penalties and enforcement measures
            
            3. FINANCIAL SERVICES CONTENT:
            - Banking and financial specific references
            - Regulatory alignment mentions (FCA, PRA, etc.)
            - Financial data processing scenarios
            - Industry-specific examples
            
            4. STRUCTURED DATA:
            - Entity relationships
            - Temporal references (dates, deadlines)
            - Quantitative information (fines, thresholds)
            - Geographic scope (jurisdictions, territories)
            
            5. QUALITY METRICS:
            - Extraction completeness score (0-1)
            - Confidence scores for each extracted element
            - Processing errors or ambiguities
            - Recommendations for improvement
            
            6. METADATA ENHANCEMENT:
            - Document classification tags
            - Regulatory categories
            - Subject matter classifications
            - Relationship to other documents
            
            Return comprehensive JSON with all extracted data, quality scores, and processing metadata.
            """
            
            response = self.llm.invoke([HumanMessage(content=processing_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    extraction_data = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "data_ingestion",
                        "extraction_results": extraction_data,
                        "processing_quality": self._assess_extraction_quality(extraction_data),
                        "document_metadata": document_metadata,
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "data_ingestion",
                "extraction_results": {"raw_extraction": content},
                "processing_quality": 0.5,
                "document_metadata": document_metadata,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            return {"error": str(e), "agent_type": "data_ingestion"}
    
    def _assess_extraction_quality(self, extraction_data: Dict[str, Any]) -> float:
        """Assess the quality of data extraction"""
        if not isinstance(extraction_data, dict):
            return 0.3
        
        # Quality scoring based on extraction completeness
        quality_indicators = [
            "document_structure",
            "regulatory_content", 
            "structured_data",
            "quality_metrics"
        ]
        
        present_indicators = sum(1 for indicator in quality_indicators if indicator in extraction_data)
        return min(present_indicators / len(quality_indicators), 1.0)

class FinancialDomainExpertAgent:
    """Financial industry specialization agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.financial_expert_prompt = """You are a senior regulatory compliance expert specializing in financial services data protection and GDPR compliance.

Your expertise covers banking, insurance, investment services, and broader financial markets regulation.

CORE EXPERTISE AREAS:
1. FINANCIAL REGULATIONS: FCA, PRA, Basel III, MiFID II, GDPR alignment
2. BANKING OPERATIONS: Account management, payments, lending, KYC, AML
3. DATA PROTECTION: Financial data categories, cross-border flows, third-party sharing
4. RISK MANAGEMENT: Operational risk, compliance risk, data protection risks
5. AUDIT & GOVERNANCE: Regulatory reporting, supervisory requirements, board oversight

FINANCIAL SERVICES GDPR APPLICATIONS:
- Customer data processing across banking lifecycle
- Payment processing and transaction data protection
- Credit scoring and lending data requirements
- Investment services and client profiling
- Insurance underwriting and claims processing
- Wealth management and private banking compliance

REGULATORY LANDSCAPE:
- UK post-Brexit data protection framework
- EU-UK data adequacy and transfer mechanisms
- Financial regulatory coordination (FCA-ICO cooperation)
- Cross-border banking and data flows
- Fintech and digital banking compliance
- Open banking and data sharing requirements

INDUSTRY-SPECIFIC CHALLENGES:
- Legacy system integration with GDPR requirements
- Real-time processing vs privacy by design
- Customer consent management in complex financial products
- Data retention for regulatory vs GDPR requirements
- Third-party vendor management and data sharing
- Incident response in financial services context

OUTPUT REQUIREMENTS:
- Financial industry specific GDPR interpretations
- Practical implementation guidance for banks
- Risk assessments with business impact analysis
- Regulatory alignment recommendations
- Industry best practices and benchmarking
- Cost-benefit analysis for compliance investments

You should provide expert guidance suitable for senior compliance officers and risk managers at major financial institutions."""
    
    def analyze_financial_context(self, content: str, financial_focus: List[str] = None) -> Dict[str, Any]:
        """Analyze content for financial services GDPR implications"""
        try:
            analysis_prompt = f"""
            Analyze this content for financial services GDPR compliance implications:
            
            Content: {content[:6000]}...
            
            Financial Focus Areas: {financial_focus or ['comprehensive']}
            
            {self.financial_expert_prompt}
            
            Provide comprehensive financial services analysis:
            
            1. FINANCIAL DATA CATEGORIZATION:
            - Identify financial data types present (payment, credit, KYC, transaction, etc.)
            - GDPR classification (personal data, special categories, etc.)
            - Sensitivity levels and protection requirements
            - Cross-border transfer implications
            
            2. BANKING PROCESS MAPPING:
            - Relevant banking/financial processes identified
            - Data flows and processing activities
            - Third-party sharing scenarios
            - Customer interaction points
            
            3. REGULATORY ALIGNMENT:
            - FCA/PRA regulatory requirements alignment
            - Conflicts or tensions with financial regulations
            - Supervisory expectations and guidance
            - Industry standards compliance (Basel, MiFID)
            
            4. RISK ASSESSMENT:
            - Data protection risks specific to financial services
            - Operational risks from GDPR compliance
            - Reputational and regulatory risks
            - Financial impact assessment
            
            5. IMPLEMENTATION CHALLENGES:
            - Technical challenges in banking systems
            - Operational process changes required
            - Customer communication requirements
            - Staff training and competency needs
            
            6. BEST PRACTICES:
            - Industry leading approaches
            - Regulatory guidance interpretations
            - Technology solutions for compliance
            - Governance and oversight frameworks
            
            7. ORGANIZATION SPECIFIC:
            - Specific implications for the organization
            - Competitive considerations
            - Strategic recommendations
            - Implementation priorities
            
            Return detailed JSON analysis with practical recommendations and risk scoring.
            """
            
            response = self.llm.invoke([HumanMessage(content=analysis_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    financial_analysis = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "financial_expert",
                        "financial_analysis": financial_analysis,
                        "risk_score": self._calculate_financial_risk(financial_analysis),
                        "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "financial_expert",
                "financial_analysis": {"raw_analysis": content},
                "risk_score": 0.5,
                "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Financial analysis failed: {e}")
            return {"error": str(e), "agent_type": "financial_expert"}
    
    def _calculate_financial_risk(self, analysis: Dict[str, Any]) -> float:
        """Calculate financial services specific risk score"""
        if not isinstance(analysis, dict):
            return 0.5
        
        # Risk factors specific to financial services
        risk_indicators = [
            "cross_border_transfers",
            "special_category_data",
            "third_party_sharing",
            "regulatory_conflicts",
            "operational_complexity"
        ]
        
        # Count mentions of risk indicators
        content_str = str(analysis).lower()
        risk_mentions = sum(1 for indicator in risk_indicators if indicator in content_str)
        
        # Higher mentions = higher risk
        return min(risk_mentions / len(risk_indicators) * 0.8, 1.0)

class RoPASpecialistAgent:
    """Article 30 Record of Processing Activities specialist agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.ropa_specialist_prompt = """You are a leading expert in GDPR Article 30 Records of Processing Activities (RoPA) with deep expertise in regulatory compliance and data governance.

Your specialization covers all aspects of Article 30 compliance for complex organizations, particularly financial institutions.

ARTICLE 30 EXPERTISE:
1. CONTROLLER REQUIREMENTS: Article 30(1) obligations and documentation
2. PROCESSOR REQUIREMENTS: Article 30(2) obligations and record-keeping
3. EXEMPTIONS: Article 30(5) small enterprise exemptions and limitations
4. DPA TEMPLATES: Understanding of various DPA RoPA templates and requirements
5. SUPERVISORY EXPECTATIONS: Knowledge of regulatory enforcement priorities

ROPA ELEMENTS MASTERY:
- Controller/processor identification and contact details
- Processing purposes and legal basis documentation
- Data categories and data subject categories
- Recipients and third-party disclosures
- International transfers and safeguards
- Retention periods and deletion procedures
- Security measures descriptions

FINANCIAL SERVICES ROPA:
- Complex multi-entity structures and joint controllership
- Financial data processing across business lines
- Regulatory reporting vs RoPA requirements
- Third-party vendor management and processor agreements
- Cross-border banking operations documentation
- Customer lifecycle processing activities

COMPLIANCE FRAMEWORK:
- RoPA governance and maintenance procedures
- Quality assurance and validation processes
- Integration with broader data governance
- Audit preparation and regulatory inspection readiness
- Change management for processing activity updates
- Training and awareness for business stakeholders

IMPLEMENTATION EXCELLENCE:
- RoPA metamodel design for enterprise scalability
- Automation and technology integration
- Business process integration
- Risk-based prioritization approaches
- Continuous improvement methodologies

OUTPUT REQUIREMENTS:
- Complete Article 30 compliance assessments
- Detailed RoPA element specifications
- Implementation roadmaps and project plans
- Quality assurance frameworks
- Regulatory risk assessments
- Business-ready documentation templates

You should provide authoritative guidance on Article 30 compliance suitable for CPOs, DPOs, and compliance teams at major financial institutions."""
    
    def extract_ropa_elements(self, content: str, compliance_focus: List[str] = None) -> Dict[str, Any]:
        """Extract and analyze Article 30 RoPA elements"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            
            ropa_prompt = f"""
            Extract comprehensive Article 30 RoPA elements from this content:
            
            Content: {content[:6000]}...
            
            Compliance Focus: {compliance_focus or ['article_30_complete']}
            
            {self.ropa_specialist_prompt}
            
            Conduct complete Article 30 analysis:
            
            1. CONTROLLER OBLIGATIONS (Article 30(1)):
            - Controller identification and contact details
            - Joint controller arrangements if applicable
            - Data Protection Officer details
            - Processing purposes with specific descriptions
            - Legal basis for each processing purpose
            - Data categories processed
            - Data subject categories
            - Recipients of personal data
            - International transfers and safeguards
            - Retention periods for data categories
            - Security measures descriptions
            
            2. PROCESSOR OBLIGATIONS (Article 30(2)):
            - Processor identification and contact details
            - Controller details for each processing relationship
            - Categories of processing activities
            - International transfers on behalf of controllers
            - Security measures for processing activities
            
            3. COMPLIANCE ASSESSMENT:
            - Article 30(5) exemption eligibility
            - Completeness assessment against requirements
            - Quality scoring for each RoPA element
            - Gaps and missing information identification
            - Regulatory risk assessment
            
            4. FINANCIAL SERVICES SPECIFICS:
            - Banking-specific processing activities
            - Financial data categorization
            - Regulatory reporting integration
            - Third-party banking services
            - Customer lifecycle mapping
            
            5. IMPLEMENTATION GUIDANCE:
            - RoPA documentation requirements
            - Governance and maintenance procedures
            - Business process integration points
            - Technology and automation opportunities
            - Training and awareness needs
            
            6. ORGANIZATION CUSTOMIZATION:
            - Organization-specific recommendations for {org_name}
            - Implementation priorities
            - Resource requirements
            - Timeline considerations
            
            Return detailed JSON with complete RoPA analysis, compliance scoring, and implementation roadmap.
            """
            
            response = self.llm.invoke([HumanMessage(content=ropa_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    ropa_analysis = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "ropa_specialist",
                        "ropa_analysis": ropa_analysis,
                        "article_30_compliance": self._assess_article_30_compliance(ropa_analysis),
                        "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                        "raw_content": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "ropa_specialist",
                "ropa_analysis": {"raw_analysis": content},
                "article_30_compliance": 0.5,
                "organization_focus": GLOBAL_CONFIG['ORGANIZATION_NAME'],
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"RoPA analysis failed: {e}")
            return {"error": str(e), "agent_type": "ropa_specialist"}
    
    def _assess_article_30_compliance(self, ropa_analysis: Dict[str, Any]) -> float:
        """Assess Article 30 compliance completeness"""
        if not isinstance(ropa_analysis, dict):
            return 0.3
        
        # Article 30 required elements
        required_elements = [
            "controller_obligations",
            "processor_obligations",
            "compliance_assessment",
            "implementation_guidance"
        ]
        
        present_elements = sum(1 for element in required_elements if element in ropa_analysis)
        return min(present_elements / len(required_elements), 1.0)

class GDPRAnalysisReActAgent:
    """Enhanced ReAct agent for comprehensive GDPR analysis with vector search"""
    
    def __init__(self):
        llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        tools = [
            search_vector_knowledge, 
            query_knowledge_graph, 
            generate_concept_synonyms,
            store_research_finding,
            find_similar_concepts,
            vector_similarity_search
        ]
        
        self.agent = create_react_agent(
            llm,
            tools,
            state_modifier="""You are an expert GDPR analyst with comprehensive knowledge of both EU GDPR and UK GDPR regulations.

Use the available tools systematically to research and analyze GDPR concepts comprehensively. You now have access to advanced vector similarity search capabilities in both Elasticsearch and FalkorDB.

ENHANCED ANALYSIS APPROACH:
1. Use search_vector_knowledge to find concepts in both Elasticsearch and FalkorDB vector stores
2. Use query_knowledge_graph with vector hybrid search for relationship discovery
3. Use vector_similarity_search for pure semantic similarity matching in FalkorDB
4. Use find_similar_concepts to discover related regulatory concepts
5. Generate relevant synonyms for semantic enrichment
6. Store significant findings for future reference

VECTOR SEARCH CAPABILITIES:
- Elasticsearch: LLM-enriched semantic search with synonym matching
- FalkorDB: Vector embeddings with cosine similarity for precise concept matching
- Hybrid Search: Combines vector similarity with graph relationship traversal
- Similar Concepts: Find entities similar to known concepts using embeddings

FOCUS AREAS:
- Complete GDPR regulatory framework analysis
- UK vs EU GDPR differences and implications
- Financial services specific applications
- Article 30 RoPA compliance requirements
- Cross-border data flows and adequacy
- Data subject rights implementation
- Security and breach notification requirements

Always leverage vector search for finding semantically similar concepts, even when exact keyword matches aren't available. Use the relationship discovery tools to understand how concepts connect across the regulatory framework."""
        )
    
    def analyze(self, text: str, focus_area: str = "comprehensive") -> Dict[str, Any]:
        """Analyze text for GDPR concepts using enhanced ReAct pattern with vector search"""
        try:
            prompt = f"""
            Conduct comprehensive GDPR analysis of this text using advanced vector search capabilities: {text}
            
            Focus area: {focus_area}
            
            Use the enhanced tools systematically:
            1. search_vector_knowledge: Search both Elasticsearch and FalkorDB for related concepts
            2. query_knowledge_graph: Use hybrid vector+graph search for relationship discovery
            3. vector_similarity_search: Find semantically similar concepts using pure vector search
            4. find_similar_concepts: Discover related regulatory concepts via embeddings
            5. generate_concept_synonyms: Create comprehensive synonym lists
            6. store_research_finding: Save significant discoveries
            
            Leverage vector search to find:
            - Semantically similar GDPR concepts even without exact keyword matches
            - Related financial services regulations and requirements
            - Cross-jurisdictional regulatory equivalents (EU vs UK)
            - Implementation patterns and best practices
            - Risk factors and mitigation strategies
            
            Provide detailed analysis including:
            - All GDPR concepts identified with article references
            - Vector similarity matches with confidence scores
            - UK vs EU GDPR considerations with semantic analysis
            - Financial services relevance using domain-specific embeddings
            - Compliance requirements and obligations
            - Implementation recommendations based on similar concept analysis
            - Risk assessments with regulatory precedent matching
            
            Use vector search extensively to discover connections that traditional keyword search might miss.
            """
            
            result = self.agent.invoke({"messages": [("user", prompt)]})
            
            return {
                "agent_type": "gdpr_react_enhanced",
                "analysis": result.get("messages", [])[-1].content if result.get("messages") else "",
                "tools_used": self._extract_tool_usage(result),
                "vector_search_enabled": True,
                "confidence": 0.85,  # Higher confidence with vector search
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Enhanced ReAct analysis failed: {e}")
            return {"error": str(e), "agent_type": "gdpr_react_enhanced"}
    
    def _extract_tool_usage(self, result: Dict[str, Any]) -> List[str]:
        """Extract which tools were used in the analysis"""
        if "messages" in result:
            tool_usage = []
            for msg in result["messages"]:
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_usage.append(tool_call.get('name', 'unknown_tool'))
            return tool_usage
        return []

class ReflectionAgent:
    """Enhanced reflection agent for quality assurance across all agents"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        self.reflection_prompt = """You are a senior quality assurance expert specializing in regulatory compliance analysis and multi-agent system optimization.

Your role is to critically evaluate the quality, completeness, and accuracy of analyses from specialized agents.

QUALITY EVALUATION FRAMEWORK:
1. COMPLETENESS: Are all relevant aspects covered thoroughly?
2. ACCURACY: Are interpretations and references correct?
3. CONSISTENCY: Are findings consistent across different agents?
4. DEPTH: Is the analysis sufficiently detailed for compliance purposes?
5. PRACTICALITY: Are recommendations actionable and realistic?
6. COMPLIANCE: Does analysis meet regulatory standards?

AGENT-SPECIFIC EVALUATION:
- ResearcherAgent: Domain coverage, concept discovery, external integration
- OntologyAgent: Schema quality, semantic consistency, extensibility
- DataIngestionAgent: Extraction accuracy, processing quality, error handling
- FinancialExpertAgent: Industry relevance, risk assessment, practical guidance
- RoPASpecialistAgent: Article 30 compliance, documentation completeness
- GDPRReActAgent: Analytical depth, tool usage effectiveness, evidence quality

IMPROVEMENT IDENTIFICATION:
- Missing regulatory concepts or relationships
- Incomplete compliance analysis
- Insufficient financial services context
- Gaps in cross-jurisdictional analysis
- Weak implementation guidance
- Inadequate risk assessment

OUTPUT REQUIREMENTS:
- Specific quality scores for each analysis dimension
- Detailed improvement recommendations
- Missing elements identification
- Cross-agent consistency assessment
- Overall confidence rating
- Prioritized enhancement suggestions

You should provide constructive, specific feedback that drives measurable improvements in analysis quality."""
    
    def reflect_on_multi_agent_analysis(self, agent_results: Dict[str, Any], original_content: str) -> Dict[str, Any]:
        """Reflect on the quality of multi-agent analysis"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            
            # Limit content size to prevent token overflow
            limited_content = original_content[:1500] if original_content else "No content provided"
            limited_results = {}
            
            # Limit agent results to prevent token overflow
            for agent_name, result in list(agent_results.items())[:5]:  # Only take first 5 agents
                if isinstance(result, dict):
                    # Create a summary of the result instead of full content
                    limited_results[agent_name] = {
                        "agent_type": result.get("agent_type", agent_name),
                        "has_analysis": bool(result.get("analysis") or result.get("research_findings") or result.get("extraction_results")),
                        "confidence": result.get("confidence", result.get("overall_quality_score", 0.5)),
                        "timestamp": result.get("timestamp", "unknown"),
                        "error": result.get("error", None)
                    }
            
            reflection_prompt = f"""
            Critically evaluate this multi-agent GDPR analysis:
            
            Original Content Sample: {limited_content}...
            
            Agent Results Summary: {json.dumps(limited_results, indent=2)}
            
            {self.reflection_prompt}
            
            Provide concise quality assessment focusing on:
            
            1. AGENT PERFORMANCE:
            - Which agents completed successfully vs failed
            - Overall confidence levels across agents
            - Consistency between agent findings
            
            2. ANALYSIS QUALITY:
            - Completeness score (0-1): Overall coverage
            - Accuracy score (0-1): Quality of interpretations  
            - Depth score (0-1): Level of analysis detail
            - Practicality score (0-1): Actionability of recommendations
            
            3. IMPROVEMENT PRIORITIES:
            - Critical gaps requiring immediate attention
            - Agent coordination improvements needed
            - Quality enhancement suggestions
            
            Return structured JSON with specific scores and recommendations. Keep response under 1000 words.
            """
            
            response = self.llm.invoke([HumanMessage(content=reflection_prompt)])
            content = response.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    reflection_data = json.loads(content[json_start:json_end])
                    
                    return {
                        "agent_type": "reflection",
                        "reflection_analysis": reflection_data,
                        "overall_quality_score": self._calculate_overall_quality(reflection_data),
                        "improvement_priority": self._prioritize_improvements(reflection_data),
                        "raw_response": content,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            return {
                "agent_type": "reflection",
                "reflection_analysis": {"raw_analysis": content},
                "overall_quality_score": 0.6,
                "improvement_priority": "medium",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Multi-agent reflection failed: {e}")
            # Return a basic reflection result instead of failing completely
            return {
                "agent_type": "reflection",
                "reflection_analysis": {
                    "error": str(e),
                    "agent_performance": {"status": "reflection_failed"},
                    "analysis_quality": {
                        "completeness_score": 0.5,
                        "accuracy_score": 0.5,
                        "depth_score": 0.5,
                        "practicality_score": 0.5
                    },
                    "improvement_priorities": ["Fix reflection agent connectivity", "Retry with shorter content"]
                },
                "overall_quality_score": 0.5,
                "improvement_priority": "high",
                "timestamp": datetime.now().isoformat()
            }
    
    def _calculate_overall_quality(self, reflection_data: Dict[str, Any]) -> float:
        """Calculate overall quality score from reflection analysis"""
        if not isinstance(reflection_data, dict) or "quality_dimensions" not in reflection_data:
            return 0.5
        
        quality_dims = reflection_data["quality_dimensions"]
        scores = []
        
        for dim_name, score in quality_dims.items():
            if isinstance(score, (int, float)):
                scores.append(float(score))
        
        return sum(scores) / len(scores) if scores else 0.5
    
    def _prioritize_improvements(self, reflection_data: Dict[str, Any]) -> str:
        """Determine improvement priority level"""
        overall_quality = self._calculate_overall_quality(reflection_data)
        
        if overall_quality < 0.5:
            return "critical"
        elif overall_quality < 0.7:
            return "high"
        elif overall_quality < 0.8:
            return "medium"
        else:
            return "low"

class EnhancedSupervisorAgent:
    """Enhanced supervisor agent coordinating all 8 specialized agents"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=GLOBAL_CONFIG["OPENAI_MODEL"],
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
            temperature=0.1
        )
        
        # Initialize all specialized agents
        self.agents = {
            "researcher": ResearcherAgent(),
            "ontology": OntologyAgent(),
            "data_ingestion": DataIngestionAgent(),
            "financial_expert": FinancialDomainExpertAgent(),
            "gdpr_react": GDPRAnalysisReActAgent(),
            "ropa_specialist": RoPASpecialistAgent(),
            "reflection": ReflectionAgent()
        }
        
        self.agent_capabilities = {
            "researcher": "Deep domain research, concept discovery, ontology foundation",
            "ontology": "Knowledge graph schema design, semantic consistency",
            "data_ingestion": "Document processing, structured extraction, quality assessment",
            "financial_expert": "Financial services GDPR interpretation, risk assessment",
            "gdpr_react": "Comprehensive GDPR analysis with tool-based reasoning",
            "ropa_specialist": "Article 30 compliance, RoPA metamodel creation",
            "reflection": "Quality assurance, improvement recommendations, consistency validation"
        }
    
    def coordinate_comprehensive_analysis(self, document_text: str, document_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Coordinate all 8 agents for comprehensive document analysis"""
        try:
            # Create coordination plan
            coordination_plan = self._create_comprehensive_plan(document_text, document_metadata)
            
            results = {
                "coordination_plan": coordination_plan,
                "agent_results": {},
                "execution_order": [],
                "quality_iterations": 0,
                "final_synthesis": None,
                "overall_confidence": 0.0
            }
            
            # Execute agents in planned order
            execution_order = coordination_plan.get("execution_order", list(self.agents.keys()))
            
            for agent_name in execution_order:
                if agent_name in self.agents:
                    logger.info(f"Executing {agent_name} agent")
                    
                    try:
                        # Execute specific agent
                        agent_result = self._execute_agent(agent_name, document_text, document_metadata, results)
                        results["agent_results"][agent_name] = agent_result
                        results["execution_order"].append(agent_name)
                        
                        # Quality check after each agent
                        if agent_name != "reflection":
                            quality_score = agent_result.get("confidence", 0.5)
                            if quality_score < GLOBAL_CONFIG["REFLECTION_THRESHOLD"]:
                                logger.info(f"Quality threshold not met for {agent_name}, running reflection")
                                reflection_result = self.agents["reflection"].reflect_on_multi_agent_analysis(
                                    {agent_name: agent_result}, 
                                    document_text
                                )
                                results["agent_results"]["reflection_" + agent_name] = reflection_result
                                results["quality_iterations"] += 1
                        
                    except Exception as e:
                        logger.error(f"Agent {agent_name} execution failed: {e}")
                        results["agent_results"][agent_name] = {"error": str(e)}
            
            # Final comprehensive reflection
            if len(results["agent_results"]) > 1:
                final_reflection = self.agents["reflection"].reflect_on_multi_agent_analysis(
                    results["agent_results"], 
                    document_text
                )
                results["agent_results"]["final_reflection"] = final_reflection
            
            # Synthesize all results
            results["final_synthesis"] = self._synthesize_comprehensive_results(results["agent_results"])
            results["overall_confidence"] = self._calculate_comprehensive_confidence(results["agent_results"])
            
            return results
            
        except Exception as e:
            logger.error(f"Comprehensive coordination failed: {e}")
            return {"error": str(e)}
    
    def _create_comprehensive_plan(self, text: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Create comprehensive coordination plan for all agents"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            org_type = GLOBAL_CONFIG['ORGANIZATION_TYPE']
            jurisdiction_focus = GLOBAL_CONFIG['JURISDICTION_FOCUS']
            
            planning_prompt = f"""
            Create a comprehensive coordination plan for 8-agent GDPR analysis:
            
            Document Preview: {text[:1500]}...
            Document Metadata: {json.dumps(metadata, indent=2)}
            
            Available Agents: {json.dumps(self.agent_capabilities, indent=2)}
            
            Organization Context: {org_name} ({org_type})
            Jurisdiction Focus: {jurisdiction_focus}
            
            Create optimal coordination plan including:
            
            1. EXECUTION ORDER:
            - Logical sequence for agent execution
            - Dependencies between agents
            - Parallel execution opportunities
            
            2. AGENT-SPECIFIC GOALS:
            - Specific objectives for each agent
            - Expected outputs and deliverables
            - Quality thresholds and success criteria
            
            3. INTEGRATION STRATEGY:
            - How agents should build on each other's work
            - Information sharing requirements
            - Synthesis and consolidation approach
            
            4. QUALITY ASSURANCE:
            - Reflection checkpoints
            - Quality improvement iterations
            - Final validation requirements
            
            5. RESOURCE ALLOCATION:
            - Processing complexity for each agent
            - Expected execution time estimates
            - Critical path identification
            
            Return structured JSON coordination plan.
            """
            
            response = self.llm.invoke([HumanMessage(content=planning_prompt)])
            content = response.content
            
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    return json.loads(content[json_start:json_end])
                except json.JSONDecodeError:
                    pass
            
            # Fallback plan
            return {
                "execution_order": ["researcher", "ontology", "data_ingestion", "financial_expert", "gdpr_react", "ropa_specialist"],
                "agent_goals": {agent: "Comprehensive analysis" for agent in self.agents.keys()},
                "quality_thresholds": 0.7,
                "integration_strategy": "Sequential with reflection loops"
            }
            
        except Exception as e:
            logger.error(f"Planning failed: {e}")
            return {"execution_order": list(self.agents.keys())}
    
    def _execute_agent(self, agent_name: str, text: str, metadata: Dict[str, Any], previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """Execute specific agent with context from previous results"""
        try:
            agent = self.agents[agent_name]
            
            if agent_name == "researcher":
                return agent.research_domain(text, ["comprehensive_gdpr", "uk_gdpr", "financial_services"])
            
            elif agent_name == "ontology":
                research_findings = []
                if "researcher" in previous_results.get("agent_results", {}):
                    research_findings = [previous_results["agent_results"]["researcher"]]
                return agent.create_ontology_schema(research_findings)
            
            elif agent_name == "data_ingestion":
                return agent.process_document(text, metadata)
            
            elif agent_name == "financial_expert":
                return agent.analyze_financial_context(text, ["banking", "compliance", "risk_management"])
            
            elif agent_name == "gdpr_react":
                return agent.analyze(text, "comprehensive_financial")
            
            elif agent_name == "ropa_specialist":
                return agent.extract_ropa_elements(text, ["article_30_complete", "financial_services"])
            
            elif agent_name == "reflection":
                return agent.reflect_on_multi_agent_analysis(previous_results.get("agent_results", {}), text)
            
            else:
                logger.warning(f"Unknown agent: {agent_name}")
                return {"error": f"Unknown agent: {agent_name}"}
                
        except Exception as e:
            logger.error(f"Agent {agent_name} execution failed: {e}")
            return {"error": str(e), "agent_type": agent_name}
    
    def _synthesize_comprehensive_results(self, agent_results: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize results from all agents"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            
            synthesis_prompt = f"""
            Synthesize comprehensive GDPR analysis from 8 specialized agents:
            
            Agent Results: {json.dumps(agent_results, indent=2, default=str)[:10000]}...
            
            Create comprehensive synthesis including:
            
            1. EXECUTIVE SUMMARY:
            - Key findings across all analytical dimensions
            - Critical compliance insights and recommendations
            - Strategic implications for {org_name}
            
            2. INTEGRATED KNOWLEDGE BASE:
            - Comprehensive GDPR concept inventory
            - Ontology schema recommendations
            - Knowledge graph implementation guidance
            
            3. FINANCIAL SERVICES ANALYSIS:
            - Industry-specific compliance requirements
            - Risk assessments and mitigation strategies
            - Regulatory alignment recommendations
            
            4. ARTICLE 30 ROPA FRAMEWORK:
            - Complete metamodel specifications
            - Implementation roadmap and priorities
            - Quality assurance and governance framework
            
            5. TECHNICAL IMPLEMENTATION:
            - System architecture recommendations
            - Data processing and storage requirements
            - Integration specifications and standards
            
            6. ACTION PLAN:
            - Immediate implementation priorities
            - Medium-term development roadmap
            - Long-term compliance strategy
            
            Focus on creating actionable, comprehensive guidance for enterprise GDPR compliance.
            """
            
            response = self.llm.invoke([HumanMessage(content=synthesis_prompt)])
            
            return {
                "synthesis_type": "comprehensive_8_agent",
                "content": response.content,
                "agent_count": len(agent_results),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Synthesis failed: {e}")
            return {"error": str(e)}
    
    def _calculate_comprehensive_confidence(self, agent_results: Dict[str, Any]) -> float:
        """Calculate overall confidence from all agent results"""
        confidences = []
        
        for agent_name, result in agent_results.items():
            if isinstance(result, dict):
                # Extract various confidence indicators
                for key in ["confidence", "overall_quality_score", "risk_score", "article_30_compliance"]:
                    if key in result and isinstance(result[key], (int, float)):
                        confidences.append(float(result[key]))
        
        return sum(confidences) / len(confidences) if confidences else 0.5

# Enhanced Main System Class
class Enhanced8AgentGDPRSystem:
    """Enhanced main system integrating all 8 specialized agents"""
    
    def __init__(self):
        self.vector_store = VectorStore()
        self.knowledge_graph = ComprehensiveKnowledgeGraph()
        self.supervisor = EnhancedSupervisorAgent()
        self.state = self._initialize_enhanced_state()
        
        logger.info("Enhanced 8-Agent GDPR System initialized")
    
    def _initialize_enhanced_state(self) -> EnhancedAgentState:
        """Initialize enhanced system state"""
        return EnhancedAgentState(
            messages=[],
            current_document=None,
            processed_documents=[],
            document_chunks=[],
            document_metadata={},
            extraction_quality={},
            research_findings=[],
            domain_concepts=[],
            regulatory_mappings={},
            external_references=[],
            concept_hierarchies={},
            ontology_schema=None,
            entity_types=[],
            relationship_types=[],
            schema_evolution=[],
            validation_rules=[],
            gdpr_articles=[],
            uk_gdpr_articles=[],
            legal_bases=[],
            data_subject_rights=[],
            principles=[],
            obligations=[],
            penalties=[],
            authorities=[],
            definitions=[],
            processing_activities=[],
            data_categories=[],
            security_measures=[],
            transfers=[],
            controllers=[],
            processors=[],
            financial_regulations=[],
            financial_data_types=[],
            compliance_frameworks=[],
            banking_processes=[],
            risk_assessments=[],
            ropa_elements={},
            article_30_compliance={},
            ropa_templates=[],
            compliance_gaps=[],
            generated_synonyms={},
            semantic_clusters=[],
            entity_mappings={},
            concept_relationships=[],
            current_agent="supervisor",
            agent_results={},
            agent_coordination_plan=None,
            agent_execution_order=[],
            inter_agent_messages=[],
            reflection_feedback=[],
            quality_metrics={},
            validation_results=[],
            improvement_suggestions=[],
            ropa_metamodel=None,
            business_report=None,
            compliance_assessment=None,
            executive_summary=None,
            confidence_scores={},
            processing_times={},
            error_logs=[]
        )
    
    def process_documents(self, document_paths: List[str] = None) -> Dict[str, Any]:
        """Process documents through enhanced 8-agent system"""
        if document_paths is None:
            pdf_path = GLOBAL_CONFIG["PDF_DOCUMENTS_PATH"]
            document_paths = [
                os.path.join(pdf_path, f) for f in os.listdir(pdf_path)
                if f.lower().endswith('.pdf')
            ]
        
        if not document_paths:
            raise ValueError("No PDF documents found to process")
        
        logger.info(f"Processing {len(document_paths)} documents through enhanced 8-agent system")
        
        results = {
            "documents_processed": 0,
            "total_agent_executions": 0,
            "concepts_discovered": 0,
            "ontology_classes_created": 0,
            "knowledge_graph_entities": 0,
            "vector_embeddings": 0,
            "synonyms_generated": 0,
            "quality_iterations": 0,
            "reflection_cycles": 0,
            "agent_performance": {}
        }
        
        for doc_path in document_paths:
            try:
                logger.info(f"Processing document: {doc_path}")
                
                # Extract text and metadata
                doc_text = self._extract_text_from_pdf(doc_path)
                doc_metadata = {
                    "source": doc_path,
                    "filename": os.path.basename(doc_path),
                    "size": len(doc_text),
                    "organization": GLOBAL_CONFIG["ORGANIZATION_NAME"],
                    "processing_timestamp": datetime.now().isoformat()
                }
                
                # Coordinate comprehensive analysis through supervisor
                coordination_result = self.supervisor.coordinate_comprehensive_analysis(doc_text, doc_metadata)
                
                # Update results tracking
                results["total_agent_executions"] += len(coordination_result.get("agent_results", {}))
                results["quality_iterations"] += coordination_result.get("quality_iterations", 0)
                results["reflection_cycles"] += len([k for k in coordination_result.get("agent_results", {}) if "reflection" in k])
                
                # Extract and store discoveries from all agents
                self._process_agent_discoveries(coordination_result.get("agent_results", {}))
                
                # Update performance metrics
                for agent_name, agent_result in coordination_result.get("agent_results", {}).items():
                    if agent_name not in results["agent_performance"]:
                        results["agent_performance"][agent_name] = {"executions": 0, "avg_confidence": 0.0}
                    
                    results["agent_performance"][agent_name]["executions"] += 1
                    confidence = agent_result.get("confidence", agent_result.get("overall_quality_score", 0.5))
                    results["agent_performance"][agent_name]["avg_confidence"] = confidence
                
                results["documents_processed"] += 1
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                self.state["error_logs"].append({
                    "document": doc_path,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
                continue
        
        # Generate final outputs
        self._generate_enhanced_outputs()
        
        # Update final result counts
        results.update({
            "concepts_discovered": len(self.state["domain_concepts"]),
            "ontology_classes_created": len(self.state["entity_types"]),
            "knowledge_graph_entities": len(self.state["processing_activities"]) + len(self.state["data_categories"]),
            "vector_embeddings": results["documents_processed"] * 50,  # Estimate
            "synonyms_generated": sum(len(syns) for syns in self.state["generated_synonyms"].values())
        })
        
        return results
    
    def _extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF document"""
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            return full_text
            
        except Exception as e:
            logger.error(f"Failed to extract text from {pdf_path}: {e}")
            return ""
    
    def _process_agent_discoveries(self, agent_results: Dict[str, Any]):
        """Process discoveries from all agents and update state"""
        try:
            for agent_name, result in agent_results.items():
                if not isinstance(result, dict) or "error" in result:
                    continue
                
                # Process research findings
                if agent_name == "researcher" and "research_findings" in result:
                    research_data = result["research_findings"]
                    if isinstance(research_data, dict):
                        # Extract regulatory concepts
                        for concept_type, concepts in research_data.items():
                            if isinstance(concepts, list):
                                for concept in concepts:
                                    if isinstance(concept, dict):
                                        # Store in vector database
                                        concept_data = {
                                            **concept,
                                            "discovered_by": "researcher_agent",
                                            "gdpr_category": concept_type,
                                            "timestamp": datetime.now()
                                        }
                                        self.vector_store.index_concept(concept_data)
                                        
                                        # Store in knowledge graph
                                        entity_type = self._map_concept_to_entity_type(concept_type)
                                        self.knowledge_graph.add_comprehensive_entity(entity_type, concept_data)
                
                # Process ontology schema
                if agent_name == "ontology" and "ontology_schema" in result:
                    self.state["ontology_schema"] = result["ontology_schema"]
                
                # Process RoPA elements
                if agent_name == "ropa_specialist" and "ropa_analysis" in result:
                    ropa_data = result["ropa_analysis"]
                    if isinstance(ropa_data, dict):
                        self.state["ropa_elements"] = ropa_data
                        self.state["article_30_compliance"] = result.get("article_30_compliance", 0.5)
                
                # Process financial analysis
                if agent_name == "financial_expert" and "financial_analysis" in result:
                    financial_data = result["financial_analysis"]
                    if isinstance(financial_data, dict):
                        self.state["financial_regulations"].extend(
                            financial_data.get("regulatory_alignment", [])
                        )
                        self.state["risk_assessments"].append({
                            "analysis": financial_data,
                            "risk_score": result.get("risk_score", 0.5),
                            "timestamp": datetime.now().isoformat()
                        })
                
        except Exception as e:
            logger.error(f"Failed to process agent discoveries: {e}")
    
    def _map_concept_to_entity_type(self, concept_type: str) -> str:
        """Map concept type to knowledge graph entity type"""
        mapping = {
            "regulatory_concept_inventory": "GDPRConcept",
            "concept_hierarchies": "ConceptHierarchy",
            "jurisdictional_analysis": "JurisdictionalConcept",
            "financial_services_mapping": "FinancialConcept",
            "ontology_recommendations": "OntologyClass",
            "external_references": "ExternalReference"
        }
        return mapping.get(concept_type, "Entity")
    
    def _generate_enhanced_outputs(self):
        """Generate enhanced final outputs"""
        try:
            # Generate comprehensive RoPA metamodel
            self._generate_comprehensive_ropa_metamodel()
            
            # Generate enhanced business report
            self._generate_enhanced_business_report()
            
            # Generate executive summary
            self._generate_executive_summary()
            
            logger.info("Enhanced outputs generated successfully")
            
        except Exception as e:
            logger.error(f"Enhanced output generation failed: {e}")
    
    def _generate_comprehensive_ropa_metamodel(self):
        """Generate comprehensive RoPA metamodel from all agent inputs"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            jurisdiction_focus = GLOBAL_CONFIG['JURISDICTION_FOCUS']
            
            metamodel_prompt = f"""
            Create a comprehensive Article 30 RoPA metamodel for {org_name} based on enhanced 8-agent analysis:
            
            Research Findings: {len(self.state['research_findings'])} concepts discovered
            Ontology Schema: {bool(self.state['ontology_schema'])}
            RoPA Elements: {len(self.state['ropa_elements'])} element types identified
            Financial Analysis: {len(self.state['risk_assessments'])} risk assessments completed
            
            Create enterprise-grade RoPA metamodel including:
            
            1. CORE METAMODEL ARCHITECTURE:
            - Entity definitions and relationships
            - Attribute specifications and constraints
            - Validation rules and business logic
            - Implementation guidelines
            
            2. FINANCIAL SERVICES EXTENSIONS:
            - Banking-specific processing activities
            - Financial data categorizations
            - Regulatory alignment mappings
            - Risk management integration
            
            3. MULTI-JURISDICTIONAL SUPPORT:
            - EU GDPR vs UK GDPR differentiation
            - Cross-border transfer handling
            - Adequacy decision management
            - Regulatory change adaptation
            
            4. TECHNOLOGY INTEGRATION:
            - Knowledge graph schema alignment
            - Vector database integration
            - Automated compliance monitoring
            - Reporting and analytics framework
            
            5. GOVERNANCE FRAMEWORK:
            - Data stewardship roles and responsibilities
            - Quality assurance procedures
            - Change management processes
            - Audit and compliance monitoring
            
            6. IMPLEMENTATION ROADMAP:
            - Phase 1: Core RoPA foundation
            - Phase 2: Financial services customization
            - Phase 3: Advanced analytics and monitoring
            - Phase 4: Continuous improvement and evolution
            
            Provide comprehensive, implementation-ready metamodel suitable for enterprise deployment.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
                base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
                temperature=0.1
            )
            
            response = llm.invoke([HumanMessage(content=metamodel_prompt)])
            
            self.state["ropa_metamodel"] = {
                "metamodel_content": response.content,
                "agent_contributions": {
                    "research_concepts": len(self.state["research_findings"]),
                    "ontology_classes": len(self.state["entity_types"]),
                    "ropa_elements": len(self.state["ropa_elements"]),
                    "financial_insights": len(self.state["risk_assessments"])
                },
                "generated_timestamp": datetime.now().isoformat(),
                "organization_context": GLOBAL_CONFIG["ORGANIZATION_NAME"],
                "jurisdiction_focus": GLOBAL_CONFIG["JURISDICTION_FOCUS"],
                "agent_system_version": "8_agent_enhanced"
            }
            
            logger.info("Comprehensive RoPA metamodel generated successfully")
            
        except Exception as e:
            logger.error(f"RoPA metamodel generation failed: {e}")
    
    def _generate_enhanced_business_report(self):
        """Generate enhanced business-friendly compliance report"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            
            report_prompt = f"""
            Generate a comprehensive executive business report for {org_name} based on enhanced 8-agent GDPR analysis.
            
            System Analysis Summary:
            - Documents Processed: {len(self.state['processed_documents'])}
            - Research Findings: {len(self.state['research_findings'])}
            - Ontology Classes: {len(self.state['entity_types'])}
            - RoPA Elements: {len(self.state['ropa_elements'])}
            - Risk Assessments: {len(self.state['risk_assessments'])}
            - Quality Iterations: {len(self.state['reflection_feedback'])}
            
            Create comprehensive executive report with:
            
            # Executive Summary
            - Overall GDPR compliance posture assessment
            - Critical findings and strategic implications
            - Investment priorities and resource requirements
            - Timeline for compliance achievement
            
            # Enhanced Knowledge Base Analysis
            - Comprehensive regulatory concept mapping
            - Multi-agent analytical insights
            - Cross-jurisdictional compliance considerations
            - Knowledge graph and ontology recommendations
            
            # Financial Services GDPR Compliance
            - Banking-specific compliance requirements
            - Regulatory alignment with FCA, PRA, and other frameworks
            - Customer data protection strategies
            - Third-party risk management protocols
            - Cross-border banking operations compliance
            
            # Article 30 RoPA Implementation
            - Complete RoPA readiness assessment
            - Metamodel implementation roadmap
            - Governance and quality assurance framework
            - Technology integration requirements
            - Staff training and change management needs
            
            # Multi-Agent Quality Assurance
            - Analysis quality metrics across all agents
            - Consistency validation results
            - Continuous improvement recommendations
            - System performance optimization
            
            # Risk Assessment & Mitigation
            - Comprehensive risk landscape analysis
            - Financial services specific risks
            - Data protection impact assessment needs
            - Breach notification preparedness
            - Regulatory enforcement risk evaluation
            
            # Strategic Implementation Plan
            ## Phase 1: Foundation (0-6 months)
            - Core knowledge graph implementation
            - Basic RoPA framework establishment
            - Essential compliance documentation
            
            ## Phase 2: Enhancement (6-12 months)
            - Financial services customization
            - Advanced analytics implementation
            - Cross-border compliance optimization
            
            ## Phase 3: Optimization (12-18 months)
            - AI-powered compliance monitoring
            - Continuous improvement processes
            - Advanced risk management integration
            
            # Investment Analysis
            - Technology infrastructure requirements
            - Human resource needs and training
            - External consultancy and legal support
            - ROI projections and business case
            
            # Governance Framework
            - Data protection governance structure
            - Roles and responsibilities matrix
            - Monitoring and reporting procedures
            - Continuous compliance assurance
            
            Make it highly actionable with specific recommendations, timelines, and business impact analysis suitable for board-level presentation.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
                base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
                temperature=0.1
            )
            
            response = llm.invoke([HumanMessage(content=report_prompt)])
            
            self.state["business_report"] = response.content
            
            logger.info("Enhanced business report generated successfully")
            
        except Exception as e:
            logger.error(f"Enhanced business report generation failed: {e}")
    
    def _generate_executive_summary(self):
        """Generate executive summary from all agent outputs"""
        try:
            org_name = GLOBAL_CONFIG['ORGANIZATION_NAME']
            
            summary_prompt = f"""
            Create executive summary for {org_name} senior leadership:
            
            8-Agent Analysis Completed:
            - ResearcherAgent: {len(self.state['research_findings'])} regulatory concepts identified
            - OntologyAgent: {bool(self.state['ontology_schema'])} comprehensive schema designed
            - DataIngestionAgent: {len(self.state['processed_documents'])} documents processed
            - FinancialExpertAgent: {len(self.state['risk_assessments'])} risk assessments completed
            - GDPRReActAgent: Comprehensive regulatory analysis with tool usage
            - RoPASpecialistAgent: Article 30 compliance framework developed
            - ReflectionAgent: {len(self.state['reflection_feedback'])} quality iterations completed
            - SupervisorAgent: Multi-agent coordination and synthesis
            
            ## Key Findings Summary
            
            ### Compliance Status
            - Current GDPR readiness level
            - Critical gaps requiring immediate attention
            - Overall risk assessment
            
            ### Strategic Recommendations
            - Top 3 immediate priorities
            - Medium-term implementation roadmap
            - Long-term competitive advantages
            
            ### Investment Requirements
            - Essential technology infrastructure
            - Human resource needs
            - External support requirements
            - Expected ROI timeline
            
            ### Risk Mitigation
            - High-priority compliance risks
            - Financial services specific considerations
            - Regulatory enforcement exposure
            
            Keep concise but comprehensive - suitable for 10-minute board presentation.
            """
            
            llm = ChatOpenAI(
                model=GLOBAL_CONFIG["OPENAI_MODEL"],
                api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
                base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None,
                temperature=0.1
            )
            
            response = llm.invoke([HumanMessage(content=summary_prompt)])
            
            self.state["executive_summary"] = response.content
            
            logger.info("Executive summary generated successfully")
            
        except Exception as e:
            logger.error(f"Executive summary generation failed: {e}")
    
    def search_comprehensive_knowledge(self, query: str, agent_filter: str = None, search_type: str = "hybrid") -> Dict[str, Any]:
        """Enhanced search across vector and graph knowledge with multiple search strategies"""
        results = {
            "query": query,
            "agent_filter": agent_filter,
            "search_type": search_type,
            "elasticsearch_results": [],
            "falkordb_vector_results": [],
            "falkordb_graph_results": [],
            "hybrid_results": [],
            "total_results": 0,
            "agent_breakdown": {},
            "similarity_analysis": {}
        }
        
        try:
            # Elasticsearch vector search
            filters = {}
            if agent_filter:
                filters["discovered_by"] = agent_filter
            
            results["elasticsearch_results"] = self.vector_store.semantic_search(
                query, filters=filters, top_k=10
            )
            
            # FalkorDB vector similarity search
            results["falkordb_vector_results"] = self.knowledge_graph.vector_similarity_search(
                query, top_k=10
            )
            
            # Traditional FalkorDB graph search
            results["falkordb_graph_results"] = self.knowledge_graph.query_comprehensive_knowledge(
                query, max_depth=2
            )
            
            # Hybrid search combining vector and graph
            if search_type in ["hybrid", "all"]:
                results["hybrid_results"] = self.knowledge_graph.hybrid_search(
                    query, max_depth=2, top_k=15
                )
            
            # Calculate totals
            results["total_results"] = (
                len(results["elasticsearch_results"]) + 
                len(results["falkordb_vector_results"]) + 
                len(results["falkordb_graph_results"]) + 
                len(results["hybrid_results"])
            )
            
            # Analyze agent contributions
            results["agent_breakdown"] = self._analyze_agent_contributions(
                results["elasticsearch_results"]
            )
            
            # Perform similarity analysis across search types
            results["similarity_analysis"] = self._analyze_search_similarity(results)
            
            logger.debug(f"Comprehensive search completed: {results['total_results']} total results")
            return results
            
        except Exception as e:
            logger.error(f"Comprehensive search failed: {e}")
            results["error"] = str(e)
            return results
    
    def _analyze_search_similarity(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze similarity patterns across different search methods"""
        try:
            analysis = {
                "cross_search_matches": 0,
                "unique_to_elasticsearch": 0,
                "unique_to_falkordb_vector": 0,
                "unique_to_graph_traversal": 0,
                "highest_similarity_scores": [],
                "concept_overlap": []
            }
            
            # Extract entity names from different search results
            es_entities = set()
            for result in search_results.get("elasticsearch_results", []):
                if result.get("entity_name"):
                    es_entities.add(result["entity_name"].lower())
            
            fdb_vector_entities = set()
            for result in search_results.get("falkordb_vector_results", []):
                entity = result.get("entity", {})
                if entity and entity.get("name"):
                    fdb_vector_entities.add(entity["name"].lower())
            
            fdb_graph_entities = set()
            for result in search_results.get("falkordb_graph_results", []):
                entity = result.get("start_entity", {})
                if entity and entity.get("name"):
                    fdb_graph_entities.add(entity["name"].lower())
            
            # Calculate overlaps
            all_entities = es_entities | fdb_vector_entities | fdb_graph_entities
            analysis["cross_search_matches"] = len(es_entities & fdb_vector_entities & fdb_graph_entities)
            analysis["unique_to_elasticsearch"] = len(es_entities - fdb_vector_entities - fdb_graph_entities)
            analysis["unique_to_falkordb_vector"] = len(fdb_vector_entities - es_entities - fdb_graph_entities)
            analysis["unique_to_graph_traversal"] = len(fdb_graph_entities - es_entities - fdb_vector_entities)
            
            # Find highest similarity scores
            all_scored_results = []
            
            for result in search_results.get("elasticsearch_results", []):
                all_scored_results.append({
                    "source": "elasticsearch",
                    "entity": result.get("entity_name", ""),
                    "score": result.get("search_score", 0),
                    "type": "search_score"
                })
            
            for result in search_results.get("falkordb_vector_results", []):
                entity = result.get("entity", {})
                all_scored_results.append({
                    "source": "falkordb_vector",
                    "entity": entity.get("name", "") if entity else "",
                    "score": result.get("similarity_score", 0),
                    "type": "similarity_score"
                })
            
            # Sort by score and get top results
            all_scored_results.sort(key=lambda x: x["score"], reverse=True)
            analysis["highest_similarity_scores"] = all_scored_results[:5]
            
            return analysis
            
        except Exception as e:
            logger.error(f"Similarity analysis failed: {e}")
            return {"error": str(e)}
    
    def find_concept_similarities(self, concept: str, similarity_threshold: float = 0.7) -> Dict[str, Any]:
        """Find similar concepts across both vector stores with similarity scoring"""
        try:
            results = {
                "target_concept": concept,
                "elasticsearch_similar": [],
                "falkordb_similar": [],
                "cross_platform_matches": [],
                "similarity_threshold": similarity_threshold
            }
            
            # Search Elasticsearch for similar concepts
            es_results = self.vector_store.semantic_search(concept, top_k=10)
            results["elasticsearch_similar"] = [
                {
                    "concept": result.get("entity_name", ""),
                    "score": result.get("search_score", 0),
                    "synonyms": result.get("llm_generated_synonyms", [])
                }
                for result in es_results 
                if result.get("search_score", 0) >= similarity_threshold
            ]
            
            # Search FalkorDB for similar concepts
            fdb_results = self.knowledge_graph.find_similar_entities(concept, top_k=10)
            results["falkordb_similar"] = [
                {
                    "concept": result["entity"].get("name", "") if result.get("entity") else "",
                    "score": result.get("similarity_score", 0),
                    "synonyms": result["entity"].get("llm_synonyms", []) if result.get("entity") else []
                }
                for result in fdb_results
                if result.get("similarity_score", 0) >= similarity_threshold
            ]
            
            # Find cross-platform matches
            es_concepts = {item["concept"].lower(): item for item in results["elasticsearch_similar"]}
            fdb_concepts = {item["concept"].lower(): item for item in results["falkordb_similar"]}
            
            for concept_name in es_concepts.keys() & fdb_concepts.keys():
                results["cross_platform_matches"].append({
                    "concept": concept_name,
                    "elasticsearch_score": es_concepts[concept_name]["score"],
                    "falkordb_score": fdb_concepts[concept_name]["score"],
                    "average_score": (es_concepts[concept_name]["score"] + fdb_concepts[concept_name]["score"]) / 2
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Concept similarity search failed: {e}")
            return {"error": str(e)}
    
    def get_vector_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for vector search capabilities"""
        try:
            metrics = {
                "elasticsearch_metrics": {
                    "total_documents": 0,
                    "vector_enabled_documents": 0,
                    "average_embedding_dimension": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                    "synonym_cache_stats": self.vector_store.embedding_engine.get_cache_stats()
                },
                "falkordb_metrics": {
                    "total_entities": 0,
                    "vector_indexed_entities": 0,
                    "vector_indexes_created": 0,
                    "embedding_model": GLOBAL_CONFIG["EMBEDDING_MODEL"]
                },
                "system_metrics": {
                    "dual_vector_storage": True,
                    "hybrid_search_enabled": True,
                    "cross_platform_similarity": True,
                    "llm_synonym_generation": True
                }
            }
            
            # Try to get actual metrics from stores
            try:
                # Elasticsearch metrics
                es_stats = self.vector_store.client.cat.count(index=self.vector_store.index_name, format="json")
                if es_stats:
                    metrics["elasticsearch_metrics"]["total_documents"] = int(es_stats[0]["count"])
            except:
                pass
            
            try:
                # FalkorDB metrics - count entities with embeddings
                count_query = "MATCH (n) WHERE n.embedding IS NOT NULL RETURN count(n) as vector_count"
                result = self.knowledge_graph.graph.query(count_query)
                if result.result_set:
                    metrics["falkordb_metrics"]["vector_indexed_entities"] = result.result_set[0][0]
            except:
                pass
            
            return metrics
            
        except Exception as e:
            logger.error(f"Vector performance metrics failed: {e}")
            return {"error": str(e)}
    
    def _analyze_agent_contributions(self, results: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analyze which agents contributed to search results"""
        agent_counts = defaultdict(int)
        for result in results:
            agent = result.get("discovered_by", "unknown")
            agent_counts[agent] += 1
        return dict(agent_counts)
    
    def get_system_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive system performance metrics"""
        return {
            "agent_execution_summary": {
                "total_agents": len(self.supervisor.agents),
                "successful_executions": len([r for r in self.state["agent_results"].values() if not r.get("error")]),
                "error_count": len(self.state["error_logs"]),
                "reflection_cycles": len(self.state["reflection_feedback"])
            },
            "knowledge_base_metrics": {
                "research_concepts": len(self.state["research_findings"]),
                "ontology_classes": len(self.state["entity_types"]),
                "processing_activities": len(self.state["processing_activities"]),
                "data_categories": len(self.state["data_categories"]),
                "financial_regulations": len(self.state["financial_regulations"]),
                "synonyms_generated": len(self.state["generated_synonyms"])
            },
            "compliance_metrics": {
                "ropa_elements_identified": len(self.state["ropa_elements"]),
                "article_30_compliance_score": self.state["article_30_compliance"],
                "risk_assessments_completed": len(self.state["risk_assessments"]),
                "compliance_gaps": len(self.state["compliance_gaps"])
            },
            "quality_metrics": {
                "average_confidence_score": sum(self.state["confidence_scores"].values()) / len(self.state["confidence_scores"]) if self.state["confidence_scores"] else 0,
                "validation_success_rate": len([v for v in self.state["validation_results"] if v.get("status") == "passed"]) / max(len(self.state["validation_results"]), 1),
                "improvement_suggestions": len(self.state["improvement_suggestions"])
            }
        }
    
    def save_comprehensive_results(self) -> Dict[str, str]:
        """Save all enhanced results to files"""
        output_files = {}
        
        try:
            # Save comprehensive RoPA metamodel
            if self.state["ropa_metamodel"]:
                metamodel_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "comprehensive_ropa_metamodel.json")
                with open(metamodel_file, 'w') as f:
                    json.dump(self.state["ropa_metamodel"], f, indent=2)
                output_files["ropa_metamodel"] = metamodel_file
            
            # Save enhanced business report
            if self.state["business_report"]:
                report_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "enhanced_business_compliance_report.md")
                with open(report_file, 'w') as f:
                    f.write(self.state["business_report"])
                output_files["business_report"] = report_file
            
            # Save executive summary
            if self.state["executive_summary"]:
                summary_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "executive_summary.md")
                with open(summary_file, 'w') as f:
                    f.write(self.state["executive_summary"])
                output_files["executive_summary"] = summary_file
            
            # Save ontology schema
            if self.state["ontology_schema"]:
                ontology_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "gdpr_ontology_schema.json")
                with open(ontology_file, 'w') as f:
                    json.dump(self.state["ontology_schema"], f, indent=2)
                output_files["ontology_schema"] = ontology_file
            
            # Save research findings
            research_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "research_findings.json")
            with open(research_file, 'w') as f:
                json.dump(self.state["research_findings"], f, indent=2, default=str)
            output_files["research_findings"] = research_file
            
            # Save generated synonyms
            synonyms_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "generated_synonyms.json")
            with open(synonyms_file, 'w') as f:
                json.dump(self.state["generated_synonyms"], f, indent=2)
            output_files["synonyms"] = synonyms_file
            
            # Save system performance metrics
            metrics_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "system_performance_metrics.json")
            with open(metrics_file, 'w') as f:
                json.dump(self.get_system_performance_metrics(), f, indent=2)
            output_files["performance_metrics"] = metrics_file
            
            # Save comprehensive system state
            state_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "enhanced_system_state.json")
            with open(state_file, 'w') as f:
                # Convert state to JSON serializable format
                serializable_state = {}
                for key, value in self.state.items():
                    if key != "messages":  # Skip complex message objects
                        serializable_state[key] = value
                json.dump(serializable_state, f, indent=2, default=str)
            output_files["system_state"] = state_file
            
            # Save agent execution log
            execution_log_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "agent_execution_log.json")
            with open(execution_log_file, 'w') as f:
                execution_log = {
                    "agent_results": self.state["agent_results"],
                    "execution_order": self.state["agent_execution_order"],
                    "reflection_feedback": self.state["reflection_feedback"],
                    "error_logs": self.state["error_logs"]
                }
                json.dump(execution_log, f, indent=2, default=str)
            output_files["execution_log"] = execution_log_file
            
            logger.info(f"Enhanced results saved to {len(output_files)} files")
            return output_files
            
        except Exception as e:
            logger.error(f"Failed to save enhanced results: {e}")
            return {}

def test_o3_mini_connection():
    """Test o3-mini connection specifically"""
    try:
        print("🔍 Testing o3-mini connection...")
        
        client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"] if GLOBAL_CONFIG["OPENAI_BASE_URL"] != "https://api.openai.com/v1" else None
        )
        
        # Test basic o3-mini call
        response = client.chat.completions.create(
            model="o3-mini",
            messages=[{"role": "user", "content": "Say 'o3-mini connection successful!'"}],
            reasoning_effort="medium",
            max_completion_tokens=50
        )
        
        print(f"✅ o3-mini connection successful!")
        print(f"Response: {response.choices[0].message.content}")
        
        # Test with higher reasoning effort
        response2 = client.chat.completions.create(
            model="o3-mini",
            messages=[{"role": "user", "content": "What is 2+2? Think step by step."}],
            reasoning_effort="high",
            max_completion_tokens=100
        )
        
        print(f"✅ o3-mini high reasoning effort successful!")
        print(f"Usage: {response2.usage}")
        return True
        
    except Exception as e:
        print(f"❌ o3-mini connection failed: {e}")
        print("💡 Common solutions:")
        print("  1. Ensure you have Tier 3+ API access ($100+ spent)")
        print("  2. Check your API key has o3-mini access")
        print("  3. Verify model name is exactly 'o3-mini'")
        print("  4. Ensure you're using latest OpenAI library")
        return False

def main():
    """Enhanced main execution function with o3-mini validation"""
    parser = argparse.ArgumentParser(description="Enhanced 8-Agent GDPR System with o3-mini")
    parser.add_argument("--process", nargs="*", help="Process documents through 8-agent system (paths optional)")
    parser.add_argument("--search", type=str, help="Search comprehensive knowledge base")
    parser.add_argument("--search-type", type=str, choices=["hybrid", "elasticsearch", "falkordb_vector", "falkordb_graph"], 
                        default="hybrid", help="Type of search to perform")
    parser.add_argument("--agent-filter", type=str, help="Filter search by specific agent", 
                        choices=["researcher", "ontology", "data_ingestion", "financial_expert", "gdpr_react", "ropa_specialist"])
    parser.add_argument("--similarity-search", type=str, help="Find concepts similar to given concept")
    parser.add_argument("--similarity-threshold", type=float, default=0.7, help="Similarity threshold for concept matching")
    parser.add_argument("--vector-metrics", action="store_true", help="Show vector search performance metrics")
    parser.add_argument("--generate-report", action="store_true", help="Generate enhanced business report only")
    parser.add_argument("--performance-metrics", action="store_true", help="Show system performance metrics")
    parser.add_argument("--save-results", action="store_true", help="Save all enhanced results to files")
    parser.add_argument("--config", type=str, help="Path to custom configuration file")
    parser.add_argument("--agent-analysis", type=str, help="Run specific agent analysis", 
                        choices=["researcher", "ontology", "financial_expert", "ropa_specialist"])
    parser.add_argument("--test-o3", action="store_true", help="Test o3-mini connection only")
    
    args = parser.parse_args()
    
    try:
        # Test o3-mini connection if requested
        if args.test_o3:
            test_o3_mini_connection()
            return
        
        # Load custom configuration if provided
        if args.config and os.path.exists(args.config):
            with open(args.config, 'r') as f:
                custom_config = json.load(f)
                GLOBAL_CONFIG.update(custom_config)
                logger.info(f"Loaded custom configuration from {args.config}")
        
        # Validate o3-mini connection before proceeding
        if not test_o3_mini_connection():
            print("\n💡 Fix o3-mini connection issues before proceeding")
            print("Run: python main.py --test-o3")
            return
        
        # Initialize enhanced 8-agent system
        system = Enhanced8AgentGDPRSystem()
        print(f"✅ Enhanced 8-Agent GDPR System initialized for {GLOBAL_CONFIG['ORGANIZATION_NAME']}")
        print(f"🤖 Specialized Agents: {len(system.supervisor.agents)} agents available")
        print(f"🎯 Jurisdiction Focus: {', '.join(GLOBAL_CONFIG['JURISDICTION_FOCUS'])}")
        print(f"🧠 LLM Model: {GLOBAL_CONFIG['OPENAI_MODEL']} for analysis and reasoning")
        
        # Default: run complete workflow if no arguments
        if not any(vars(args).values()):
            print("🚀 Running complete enhanced 8-agent GDPR workflow with o3-mini...")
            
            # Process documents through all agents
            print("\n📄 Step 1: Processing documents through specialized agent system...")
            result = system.process_documents()
            print(f"✅ Enhanced processing completed:")
            print(f"   Documents Processed: {result['documents_processed']}")
            print(f"   Total Agent Executions: {result['total_agent_executions']}")
            print(f"   Concepts Discovered: {result['concepts_discovered']}")
            print(f"   Ontology Classes: {result['ontology_classes_created']}")
            print(f"   KG Entities (with vectors): {result['knowledge_graph_entities']}")
            print(f"   Vector Embeddings: {result['vector_embeddings']}")
            print(f"   LLM-Generated Synonyms: {result['synonyms_generated']}")
            print(f"   Quality Iterations: {result['quality_iterations']}")
            print(f"   Reflection Cycles: {result['reflection_cycles']}")
            
            # Show agent performance
            print(f"\n🎯 Agent Performance Summary:")
            for agent_name, performance in result['agent_performance'].items():
                print(f"   {agent_name.title()}: {performance['executions']} executions, "
                      f"{performance['avg_confidence']:.2f} avg confidence")
            
            # Save comprehensive results
            print("\n💾 Step 2: Saving comprehensive results...")
            output_files = system.save_comprehensive_results()
            for file_type, file_path in output_files.items():
                print(f"   {file_type.replace('_', ' ').title()}: {file_path}")
            
            # Show performance metrics
            print("\n📊 Step 3: System Performance Metrics...")
            metrics = system.get_system_performance_metrics()
            for category, stats in metrics.items():
                print(f"   {category.replace('_', ' ').title()}:")
                for key, value in stats.items():
                    print(f"     {key.replace('_', ' ').title()}: {value}")
            
            print(f"\n🎉 Complete enhanced 8-agent workflow with o3-mini finished!")
            print(f"📁 Check output directory: {GLOBAL_CONFIG['OUTPUT_PATH']}")
            return
        
        # Execute individual operations
        if args.process is not None:
            result = system.process_documents(args.process if args.process else None)
            print(f"✅ Enhanced document processing with o3-mini completed:")
            for key, value in result.items():
                if isinstance(value, dict):
                    print(f"   {key.replace('_', ' ').title()}:")
                    for subkey, subvalue in value.items():
                        print(f"     {subkey}: {subvalue}")
                else:
                    print(f"   {key.replace('_', ' ').title()}: {value}")
        
        if args.search:
            results = system.search_comprehensive_knowledge(args.search, args.agent_filter, args.search_type)
            print(f"🔍 Enhanced Search Results for '{args.search}':")
            print(f"   Search Type: {args.search_type}")
            if args.agent_filter:
                print(f"   Filtered by Agent: {args.agent_filter}")
            print(f"   Elasticsearch Results: {len(results.get('elasticsearch_results', []))}")
            print(f"   FalkorDB Vector Results: {len(results.get('falkordb_vector_results', []))}")
            print(f"   FalkorDB Graph Results: {len(results.get('falkordb_graph_results', []))}")
            print(f"   Hybrid Results: {len(results.get('hybrid_results', []))}")
            print(f"   Total Results: {results.get('total_results', 0)}")
            
            if results.get("similarity_analysis"):
                sim_analysis = results["similarity_analysis"]
                print(f"   Cross-Search Matches: {sim_analysis.get('cross_search_matches', 0)}")
                print(f"   Unique to Elasticsearch: {sim_analysis.get('unique_to_elasticsearch', 0)}")
                print(f"   Unique to FalkorDB Vector: {sim_analysis.get('unique_to_falkordb_vector', 0)}")
        
        if args.similarity_search:
            results = system.find_concept_similarities(args.similarity_search, args.similarity_threshold)
            print(f"🎯 Concept Similarity Search for '{args.similarity_search}':")
            print(f"   Similarity Threshold: {args.similarity_threshold}")
            print(f"   Elasticsearch Similar Concepts: {len(results.get('elasticsearch_similar', []))}")
            print(f"   FalkorDB Similar Concepts: {len(results.get('falkordb_similar', []))}")
            print(f"   Cross-Platform Matches: {len(results.get('cross_platform_matches', []))}")
            
            # Show top similar concepts
            for match in results.get('cross_platform_matches', [])[:3]:
                print(f"     {match['concept']}: ES={match['elasticsearch_score']:.3f}, "
                      f"FDB={match['falkordb_score']:.3f}, Avg={match['average_score']:.3f}")
        
        if args.vector_metrics:
            metrics = system.get_vector_performance_metrics()
            print("📊 Vector Storage Performance Metrics:")
            for category, stats in metrics.items():
                print(f"\n  {category.replace('_', ' ').title()}:")
                for key, value in stats.items():
                    if isinstance(value, dict):
                        print(f"    {key.replace('_', ' ').title()}:")
                        for subkey, subvalue in value.items():
                            print(f"      {subkey.replace('_', ' ').title()}: {subvalue}")
                    else:
                        print(f"    {key.replace('_', ' ').title()}: {value}")
        
        if args.performance_metrics:
            metrics = system.get_system_performance_metrics()
            print("📊 Enhanced System Performance Metrics:")
            for category, stats in metrics.items():
                print(f"\n  {category.replace('_', ' ').title()}:")
                for key, value in stats.items():
                    print(f"    {key.replace('_', ' ').title()}: {value}")
        
        if args.agent_analysis:
            print(f"🔬 Running {args.agent_analysis} agent analysis...")
            # This would require implementing individual agent execution
            # For now, redirect to full processing
            result = system.process_documents()
            print(f"✅ Agent analysis completed as part of full system execution")
        
        if args.generate_report:
            system._generate_enhanced_business_report()
            system._generate_executive_summary()
            print("📋 Enhanced business report and executive summary generated")
        
        if args.save_results:
            output_files = system.save_comprehensive_results()
            print(f"💾 Enhanced results saved to {len(output_files)} files:")
            for file_type, file_path in output_files.items():
                print(f"   {file_type.replace('_', ' ').title()}: {file_path}")
    
    except Exception as e:
        print(f"❌ System error: {e}")
        logger.error(f"Enhanced system error: {e}")

if __name__ == "__main__":
    main()
