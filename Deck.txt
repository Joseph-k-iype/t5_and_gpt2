#!/usr/bin/env python3
"""
TTL to CSV to FalkorDB Ultra-Fast Pipeline
Converts TTL to CSV format then uses FalkorDB's native bulk loader for maximum speed
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import subprocess
from typing import Dict, Set, List, Tuple, Any
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TTLToCSVConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to CSV converter"""
        self.output_dir = output_dir
        self.nodes_by_type = {}  # type -> list of nodes
        self.edges_by_type = {}  # relationship -> list of edges
        self.node_id_map = {}    # URI -> unique_id
        self.next_id = 1
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
    def clean_identifier(self, uri_or_literal: str) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            parsed = urlparse(str(uri_or_literal))
            if parsed.fragment:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.fragment)
            elif parsed.path:
                local_name = parsed.path.split('/')[-1]
                if local_name:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', local_name)
                else:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.netloc)
            else:
                return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
        else:
            return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique sequential ID for a resource"""
        uri = str(resource)
        if uri not in self.node_id_map:
            self.node_id_map[uri] = str(self.next_id)
            self.next_id += 1
        return self.node_id_map[uri]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, Any]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path:
            parts = parsed.path.strip('/').split('/')
            if parts and parts[-1]:
                properties['local_name'] = parts[-1]
                properties['namespace'] = uri_str.replace(parts[-1], '').rstrip('/')
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[Any, str]:
        """Process literal value and return (value, datatype)"""
        if literal.datatype:
            datatype = str(literal.datatype)
            # Handle common datatypes
            if 'integer' in datatype or 'int' in datatype:
                try:
                    return int(literal), 'integer'
                except:
                    return str(literal), 'string'
            elif 'decimal' in datatype or 'double' in datatype or 'float' in datatype:
                try:
                    return float(literal), 'float'
                except:
                    return str(literal), 'string'
            elif 'boolean' in datatype:
                return str(literal).lower() in ('true', '1'), 'boolean'
            elif 'date' in datatype:
                return str(literal), 'date'
            else:
                return str(literal), 'string'
        else:
            return str(literal), 'string'
    
    def convert_ttl_to_csv(self, ttl_file_path: str):
        """Convert TTL file to CSV format optimized for FalkorDB bulk loader"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Parse TTL file
        graph = Graph()
        try:
            graph.parse(ttl_file_path, format='turtle')
            logger.info(f"Successfully parsed TTL file. Found {len(graph)} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        # Track properties for each node type
        node_properties = {}  # node_type -> set of property names
        edge_properties = {}  # edge_type -> set of property names
        
        total_triples = len(graph)
        processed = 0
        
        logger.info("First pass: Analyzing schema and collecting data...")
        with tqdm(total=total_triples, desc="Analyzing triples") as pbar:
            for subject, predicate, obj in graph:
                processed += 1
                pbar.update(1)
                
                subject_id = self.get_or_create_node_id(subject)
                subject_type = self.clean_identifier(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Initialize node type tracking
                if subject_type not in self.nodes_by_type:
                    self.nodes_by_type[subject_type] = {}
                    node_properties[subject_type] = set()
                
                # Initialize node if not exists
                if subject_id not in self.nodes_by_type[subject_type]:
                    if isinstance(subject, URIRef):
                        base_props = self.extract_properties_from_uri(subject)
                    else:  # BNode
                        base_props = {'uri': str(subject), 'blank_node_id': str(subject)}
                    
                    self.nodes_by_type[subject_type][subject_id] = base_props
                    
                    # Track all base properties (excluding 'id' since it will be first column)
                    for prop_name in base_props.keys():
                        if prop_name != 'id':  # 'id' will be handled separately
                            node_properties[subject_type].add(prop_name)
                
                # Handle object
                if isinstance(obj, Literal):
                    # Add as property to subject node
                    value, datatype = self.process_literal_value(obj)
                    
                    # Clean property name and avoid conflicts
                    clean_prop = predicate_clean
                    if clean_prop == 'id':  # Avoid conflict with ID column
                        clean_prop = f"{predicate_clean}_value"
                    
                    self.nodes_by_type[subject_type][subject_id][clean_prop] = value
                    node_properties[subject_type].add(clean_prop)
                    
                    # Also store language if present
                    if obj.language:
                        lang_prop = f"{clean_prop}_lang"
                        self.nodes_by_type[subject_type][subject_id][lang_prop] = obj.language
                        node_properties[subject_type].add(lang_prop)
                
                else:
                    # Object is a resource - create edge
                    object_id = self.get_or_create_node_id(obj)
                    object_type = self.clean_identifier(obj)
                    
                    # Initialize object node if not exists
                    if object_type not in self.nodes_by_type:
                        self.nodes_by_type[object_type] = {}
                        node_properties[object_type] = set()
                    
                    if object_id not in self.nodes_by_type[object_type]:
                        if isinstance(obj, URIRef):
                            base_props = self.extract_properties_from_uri(obj)
                        else:  # BNode
                            base_props = {'uri': str(obj), 'blank_node_id': str(obj)}
                        
                        self.nodes_by_type[object_type][object_id] = base_props
                        
                        # Track all base properties
                        for prop_name in base_props.keys():
                            if prop_name != 'id':
                                node_properties[object_type].add(prop_name)
                    
                    # Create edge with proper format for bulk loader
                    if predicate_clean not in self.edges_by_type:
                        self.edges_by_type[predicate_clean] = []
                        edge_properties[predicate_clean] = set(['predicate_uri'])
                    
                    edge = {
                        'source_id': subject_id,  # Must be first column
                        'target_id': object_id,   # Must be second column  
                        'predicate_uri': str(predicate)
                    }
                    self.edges_by_type[predicate_clean].append(edge)
        
        logger.info(f"Schema analysis complete:")
        logger.info(f"  Node types: {list(self.nodes_by_type.keys())}")
        logger.info(f"  Edge types: {list(self.edges_by_type.keys())}")
        
        # Write CSV files in proper format for FalkorDB bulk loader
        csv_files = []
        
        # Write node CSV files with ID as first column (required by bulk loader)
        logger.info("Writing node CSV files...")
        for node_type, nodes in self.nodes_by_type.items():
            if not nodes:
                continue
                
            filename = f"{self.output_dir}/{node_type}.csv"
            csv_files.append(('nodes', filename))
            
            # ID must be first column, then other properties in consistent order
            other_props = sorted(node_properties[node_type])  # Consistent ordering
            headers = ['id'] + other_props  # ID first (required by bulk loader)
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(headers)
                
                for node_id, node_data in nodes.items():
                    row = [node_id]  # ID first
                    for prop in other_props:
                        value = node_data.get(prop, '')
                        # Handle different data types for CSV
                        if isinstance(value, (list, dict)):
                            value = json.dumps(value)
                        elif value is None or value == '':
                            value = ''  # Empty string instead of None (avoid NULL issues)
                        elif isinstance(value, bool):
                            value = str(value).lower()  # 'true'/'false' format
                        else:
                            value = str(value).strip()  # Clean whitespace
                        row.append(value)
                    writer.writerow(row)
            
            logger.info(f"  Written {len(nodes)} {node_type} nodes to {filename}")
        
        # Write edge CSV files with source_id, target_id as first two columns (required)
        logger.info("Writing edge CSV files...")
        for edge_type, edges in self.edges_by_type.items():
            if not edges:
                continue
                
            filename = f"{self.output_dir}/{edge_type}.csv"
            csv_files.append(('relationships', filename))
            
            # source_id and target_id must be first two columns (required by bulk loader)
            other_props = sorted(edge_properties[edge_type] - {'source_id', 'target_id'})
            headers = ['source_id', 'target_id'] + other_props
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(headers)
                
                for edge in edges:
                    row = [edge['source_id'], edge['target_id']]  # First two columns
                    for prop in other_props:
                        value = edge.get(prop, '')
                        if isinstance(value, (list, dict)):
                            value = json.dumps(value)
                        elif value is None or value == '':
                            value = ''
                        elif isinstance(value, bool):
                            value = str(value).lower()
                        else:
                            value = str(value).strip()
                        row.append(value)
                    writer.writerow(row)
            
            logger.info(f"  Written {len(edges)} {edge_type} edges to {filename}")
        
        # Validate CSV format
        self.validate_csv_format(csv_files)
        
        logger.info(f"CSV conversion completed! Files written to {self.output_dir}/")
        return csv_files
    
    def validate_csv_format(self, csv_files: List[Tuple[str, str]]):
        """Validate CSV files match FalkorDB bulk loader requirements"""
        logger.info("Validating CSV format for FalkorDB bulk loader...")
        
        all_node_ids = set()
        edge_references = set()
        
        for file_type, filename in csv_files:
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    headers = next(reader)
                    
                    if file_type == 'nodes':
                        # Node files: ID must be first column
                        if headers[0] != 'id':
                            logger.warning(f"Node file {filename}: First column should be 'id', found '{headers[0]}'")
                        
                        # Collect all node IDs for reference validation
                        for row in reader:
                            if row and row[0]:  # Non-empty ID
                                all_node_ids.add(row[0])
                    
                    elif file_type == 'relationships':
                        # Edge files: First two columns must be source_id, target_id
                        if len(headers) < 2:
                            logger.warning(f"Edge file {filename}: Must have at least 2 columns")
                        elif headers[0] != 'source_id' or headers[1] != 'target_id':
                            logger.warning(f"Edge file {filename}: First two columns should be 'source_id', 'target_id'")
                            logger.warning(f"  Found: {headers[0]}, {headers[1]}")
                        
                        # Collect edge references for validation
                        for row in reader:
                            if len(row) >= 2 and row[0] and row[1]:
                                edge_references.add(row[0])  # source
                                edge_references.add(row[1])  # target
                
            except Exception as e:
                logger.error(f"Error validating {filename}: {e}")
        
        # Check if all edge references have corresponding nodes
        missing_nodes = edge_references - all_node_ids
        if missing_nodes:
            logger.warning(f"Found {len(missing_nodes)} edge references without corresponding nodes")
            logger.warning(f"Sample missing node IDs: {list(missing_nodes)[:5]}")
        else:
            logger.info("‚úÖ All edge references have corresponding nodes")
        
        logger.info(f"‚úÖ CSV validation complete:")
        logger.info(f"  Total unique node IDs: {len(all_node_ids)}")
        logger.info(f"  Total edge references: {len(edge_references)}")
        logger.info(f"  Missing node references: {len(missing_nodes)}")
    
    def check_bulk_loader_installed(self):
        """Check if falkordb-bulk-loader is available"""
        try:
            result = subprocess.run(['falkordb-bulk-insert', '--help'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                logger.info("‚úÖ falkordb-bulk-loader is available")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # Try alternative command names
        for cmd in ['falkordb-bulk-insert', 'python3 -m falkordb_bulk_loader', 'python -m falkordb_bulk_loader']:
            try:
                result = subprocess.run(cmd.split() + ['--help'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    logger.info(f"‚úÖ Found bulk loader via: {cmd}")
                    return cmd.split()[0] if len(cmd.split()) == 1 else cmd
            except:
                continue
        
        return False
    
    def run_falkor_bulk_loader(self, csv_files: List[Tuple[str, str]], graph_name: str, 
                              host: str = 'localhost', port: int = 6379, password: str = None):
        """Run FalkorDB bulk loader on the generated CSV files"""
        
        # Check if bulk loader is available (DO NOT try to install)
        bulk_cmd = self.check_bulk_loader_installed()
        
        if not bulk_cmd:
            logger.error("‚ùå falkordb-bulk-loader not found!")
            logger.error("The CSV files have been created successfully, but bulk loader is not accessible.")
            self.print_manual_instructions(csv_files, graph_name, host, port, password)
            return False
        
        logger.info(f"‚úÖ Found bulk loader: {bulk_cmd}")
        
        # Build command
        if isinstance(bulk_cmd, str) and ' ' in bulk_cmd:
            # Handle commands like "python3 -m falkordb_bulk_loader"
            cmd = bulk_cmd.split() + [graph_name]
        else:
            cmd = ['falkordb-bulk-insert', graph_name]
        
        # Add connection parameters
        if host != 'localhost':
            cmd.extend(['--host', host])
        if port != 6379:
            cmd.extend(['--port', str(port)])
        if password:
            cmd.extend(['--password', password])
        
        # Add CSV files in proper order (nodes first, then relationships)
        node_files = [f for f_type, f in csv_files if f_type == 'nodes']
        rel_files = [f for f_type, f in csv_files if f_type == 'relationships']
        
        for filename in node_files:
            cmd.extend(['--nodes', filename])
        
        for filename in rel_files:
            cmd.extend(['--relationships', filename])
        
        logger.info(f"üöÄ Executing bulk loader command:")
        logger.info(f"  {' '.join(cmd)}")
        logger.info(f"  Input files: {len(node_files)} node files, {len(rel_files)} relationship files")
        
        try:
            start_time = time.time()
            
            # Run with verbose output and longer timeout
            result = subprocess.run(
                cmd, 
                check=True, 
                capture_output=True, 
                text=True, 
                timeout=1200  # 20 minutes timeout
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"‚úÖ Bulk loading completed successfully in {duration:.2f} seconds!")
            
            if result.stdout:
                logger.info("üìã Bulk loader output:")
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        logger.info(f"  {line}")
            
            if result.stderr:
                logger.info("‚ö†Ô∏è  Bulk loader warnings:")
                for line in result.stderr.strip().split('\n'):
                    if line.strip():
                        logger.info(f"  {line}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Bulk loading failed with exit code {e.returncode}")
            logger.error("üìã Error details:")
            
            if e.stdout:
                logger.error("STDOUT:")
                for line in e.stdout.strip().split('\n'):
                    if line.strip():
                        logger.error(f"  {line}")
            
            if e.stderr:
                logger.error("STDERR:")
                for line in e.stderr.strip().split('\n'):
                    if line.strip():
                        logger.error(f"  {line}")
            
            # Provide troubleshooting suggestions
            logger.error("\nüîß Troubleshooting suggestions:")
            logger.error("1. Check if FalkorDB is running and accessible")
            logger.error("2. Verify CSV file format and content")
            logger.error("3. Try manual command with --help to see all options")
            logger.error("4. Check FalkorDB logs: docker logs falkordb")
            
            self.print_manual_instructions(csv_files, graph_name, host, port, password)
            return False
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Bulk loading timed out (>20 minutes)")
            logger.error("This usually indicates:")
            logger.error("  - Very large dataset")
            logger.error("  - Connection issues")
            logger.error("  - CSV format problems")
            
            self.print_manual_instructions(csv_files, graph_name, host, port, password)
            return False
        
        except Exception as e:
            logger.error(f"‚ùå Unexpected error: {e}")
            self.print_manual_instructions(csv_files, graph_name, host, port, password)
            return False
    
    def print_manual_instructions(self, csv_files: List[Tuple[str, str]], graph_name: str, 
                                host: str, port: int, password: str):
        """Print manual instructions for running the bulk loader"""
        logger.info("\n" + "="*60)
        logger.info("üìã MANUAL BULK LOADER INSTRUCTIONS")
        logger.info("="*60)
        logger.info("The CSV files have been created successfully!")
        logger.info("You can run the bulk loader manually with this command:")
        logger.info("")
        
        # Build manual command
        cmd_parts = ['falkordb-bulk-insert', graph_name]
        
        if host != 'localhost':
            cmd_parts.extend(['--host', host])
        if port != 6379:
            cmd_parts.extend(['--port', str(port)])
        if password:
            cmd_parts.extend(['--password', password])
        
        for file_type, filename in csv_files:
            if file_type == 'nodes':
                cmd_parts.extend(['--nodes', filename])
            elif file_type == 'relationships':
                cmd_parts.extend(['--relationships', filename])
        
        logger.info("COMMAND TO RUN:")
        logger.info(f"  {' '.join(cmd_parts)}")
        logger.info("")
        logger.info("Or try these alternatives:")
        logger.info(f"  python3 -m falkordb_bulk_loader {' '.join(cmd_parts[1:])}")
        logger.info(f"  python -m falkordb_bulk_loader {' '.join(cmd_parts[1:])}")
        logger.info("")
        logger.info("CSV files created:")
        for file_type, filename in csv_files:
            logger.info(f"  {file_type}: {filename}")
        logger.info("")
        logger.info("If bulk loader still doesn't work, try:")
        logger.info("  pip install --upgrade falkordb-bulk-loader")
        logger.info("  pip install --force-reinstall falkordb-bulk-loader")
        logger.info("="*60)

def main():
    """Main function for TTL to CSV to FalkorDB pipeline"""
    parser = argparse.ArgumentParser(description='Convert TTL to CSV and bulk load into FalkorDB')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='localhost', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--password', help='FalkorDB password')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--keep_csv', action='store_true', help='Keep CSV files after loading')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        return
    
    file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
    logger.info(f"Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    
    # Create converter
    converter = TTLToCSVConverter(args.output_dir)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        csv_files = converter.convert_ttl_to_csv(args.ttl_file)
        conversion_time = time.time() - start_time
        
        logger.info(f"‚úÖ TTL to CSV conversion completed in {conversion_time:.2f} seconds")
        logger.info(f"   Performance: {file_size_mb/conversion_time:.1f} MB/second")
        
        # Run bulk loader
        success = converter.run_falkor_bulk_loader(
            csv_files, args.graph_name, args.host, args.port, args.password
        )
        
        if success:
            total_time = time.time() - start_time
            logger.info(f"üöÄ COMPLETE PIPELINE SUCCESS in {total_time:.2f} seconds!")
            logger.info(f"   Overall performance: {file_size_mb/total_time:.1f} MB/second")
            
            # Cleanup CSV files if requested
            if not args.keep_csv:
                import shutil
                shutil.rmtree(args.output_dir)
                logger.info(f"Cleaned up temporary CSV files in {args.output_dir}")
        else:
            logger.error("Pipeline failed during bulk loading")
            
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    main()
