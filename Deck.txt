# GDPR Multi-Agent System - PRODUCTION READY with FIXED ERRORS and ENHANCED LINKING
# Requirements: pip install langchain langgraph langchain-elasticsearch langchain-openai pymupdf==1.26.1 pydantic scikit-learn

import asyncio
import logging
import os
import pickle
import re
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Set
from uuid import uuid4

import numpy as np
import pymupdf  # Latest PyMuPDF 1.26.1
from sklearn.metrics.pairwise import cosine_similarity
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import ElasticsearchStore
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.memory.chat_message_histories import FileChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field, ConfigDict
from elasticsearch import Elasticsearch, AsyncElasticsearch
from openai import AsyncOpenAI

# =============================================================================
# GLOBAL CONFIGURATION - ALL CREDENTIALS AND SETTINGS
# =============================================================================

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key-here")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", None)  # For Azure OpenAI, proxies, etc.
OPENAI_MODEL = "o3-mini"  # ONLY o3-mini everywhere
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"
OPENAI_REASONING_EFFORT = "high"

# Elasticsearch Configuration - FIXED without SSL complications
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "localhost")
ELASTICSEARCH_PORT = int(os.getenv("ELASTICSEARCH_PORT", "9200"))
ELASTICSEARCH_USERNAME = os.getenv("ELASTICSEARCH_USERNAME", "elastic")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD", "your-elasticsearch-password")
ELASTICSEARCH_USE_SSL = False  # Disabled to avoid SSL issues
ELASTICSEARCH_INDEX_NAME = "gdpr_comprehensive_knowledge"

# Processing Configuration
CHUNK_SIZE = 2000  # Larger chunks for full context
CHUNK_OVERLAP = 400  # More overlap for better continuity
SIMILARITY_THRESHOLD = 0.75
BATCH_SIZE = 20  # Smaller batches for stability

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# ENHANCED PYDANTIC MODELS WITH COMPREHENSIVE ARTICLE LINKING
# =============================================================================

class DocumentType(str, Enum):
    GDPR_EU = "gdpr_eu"
    GDPR_UK = "gdpr_uk"

class ChunkType(str, Enum):
    FULL_ARTICLE = "full_article"
    ARTICLE_SECTION = "article_section"
    CHAPTER = "chapter"
    SECTION = "section"
    PARAGRAPH = "paragraph"
    CROSS_REFERENCE = "cross_reference"

class RelationshipType(str, Enum):
    REFERENCES = "references"
    SUPPORTS = "supports"
    CONTRADICTS = "contradicts"
    ELABORATES = "elaborates"
    SUPERSEDES = "supersedes"
    CROSS_REFERENCES = "cross_references"
    DEFINES = "defines"
    APPLIES_TO = "applies_to"
    SEMANTICALLY_RELATED = "semantically_related"

class ArticleReference(BaseModel):
    """Enhanced article reference with full context"""
    article_number: str
    article_title: str
    section_reference: Optional[str] = None
    paragraph_reference: Optional[str] = None
    context: str  # The context in which this article is referenced
    reference_type: str  # "direct", "implied", "definition", "procedure"

class LegalConcept(BaseModel):
    """Enhanced legal concept with article mappings"""
    concept_name: str
    definition: str
    primary_article: str
    related_articles: List[str] = Field(default_factory=list)
    context_chunks: List[str] = Field(default_factory=list)  # Chunk IDs where this concept appears

class ComprehensiveChunk(BaseModel):
    """Enhanced chunk model with comprehensive article linking"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    id: str = Field(default_factory=lambda: str(uuid4()))
    document_type: DocumentType
    chunk_type: ChunkType
    title: str
    content: str
    full_article_content: Optional[str] = None  # Full article text if this is part of an article
    
    # Article structure
    chapter_number: Optional[str] = None
    article_number: Optional[str] = None
    section_number: Optional[str] = None
    paragraph_number: Optional[str] = None
    hierarchy_level: int = Field(ge=0, le=6)
    
    # Position and navigation
    page_number: Optional[int] = None
    position_in_document: int = Field(ge=0)
    
    # Direct OpenAI embeddings
    embedding: Optional[List[float]] = None
    
    # Enhanced linking
    article_references: List[ArticleReference] = Field(default_factory=list)
    legal_concepts: List[LegalConcept] = Field(default_factory=list)
    cross_references: List[str] = Field(default_factory=list)  # "See Article X", "As defined in Y"
    
    # Relationships
    related_chunk_ids: Set[str] = Field(default_factory=set)
    semantic_similarity_scores: Dict[str, float] = Field(default_factory=dict)
    
    # Keywords and concepts
    keywords: List[str] = Field(default_factory=list)
    legal_terms: List[str] = Field(default_factory=list)
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.now)

class SemanticRelationship(BaseModel):
    """Enhanced relationship model with legal context"""
    id: str = Field(default_factory=lambda: str(uuid4()))
    source_chunk_id: str
    target_chunk_id: str
    relationship_type: RelationshipType
    confidence_score: float = Field(ge=0.0, le=1.0)
    semantic_similarity: float = Field(ge=0.0, le=1.0)
    distance_in_document: int = Field(ge=0)
    reasoning: str
    legal_basis: str
    article_connection: Optional[str] = None  # How articles are connected
    extracted_by_agent: str
    created_at: datetime = Field(default_factory=datetime.now)

class ComprehensiveMemoryState(BaseModel):
    """State with comprehensive legal knowledge"""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # Document processing state
    documents: List[Document] = Field(default_factory=list)
    comprehensive_chunks: List[ComprehensiveChunk] = Field(default_factory=list)
    relationships: List[SemanticRelationship] = Field(default_factory=list)
    
    # Article mapping
    article_index: Dict[str, str] = Field(default_factory=dict)  # article_number -> chunk_id
    concept_index: Dict[str, List[str]] = Field(default_factory=dict)  # concept -> chunk_ids
    cross_reference_index: Dict[str, List[str]] = Field(default_factory=dict)  # reference -> chunk_ids
    
    # Elasticsearch components
    vectorstore: Optional[Any] = None
    qa_chain: Optional[Any] = None
    conversational_chain: Optional[Any] = None
    conversation_memory: Optional[Any] = None
    
    # Processing state
    current_agent: Optional[str] = None
    errors: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

# =============================================================================
# FIXED ELASTICSEARCH CONFIGURATION WITHOUT SSL
# =============================================================================

class SimpleElasticsearchConfig:
    """Simple Elasticsearch configuration without SSL complications"""
    
    def __init__(self, 
                 host: str = ELASTICSEARCH_HOST, 
                 port: int = ELASTICSEARCH_PORT,
                 username: str = ELASTICSEARCH_USERNAME,
                 password: str = ELASTICSEARCH_PASSWORD):
        
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        
        if not self.password or self.password == "your-elasticsearch-password":
            raise ValueError("Elasticsearch password must be provided")
    
    def get_elasticsearch_url(self) -> str:
        """Get HTTP Elasticsearch URL (no SSL)"""
        return f"http://{self.host}:{self.port}"
    
    def get_client_config(self) -> Dict[str, Any]:
        """Get simple Elasticsearch client configuration"""
        return {
            "hosts": [f"{self.host}:{self.port}"],
            "basic_auth": (self.username, self.password),
            "verify_certs": False,
            "ssl_show_warn": False
        }
    
    def create_client(self) -> Elasticsearch:
        """Create simple Elasticsearch client"""
        return Elasticsearch(**self.get_client_config())

# =============================================================================
# DIRECT OPENAI EMBEDDING SERVICE
# =============================================================================

class DirectOpenAIEmbeddingService:
    """Direct OpenAI embedding service without LangChain wrapper"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = AsyncOpenAI(**client_kwargs)
        self.model = OPENAI_EMBEDDING_MODEL
    
    async def embed_text(self, text: str) -> List[float]:
        """Generate embedding for single text"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return []
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            return [[] for _ in texts]

# =============================================================================
# ENHANCED REASONING SERVICE WITH COMPREHENSIVE ARTICLE ANALYSIS
# =============================================================================

class ComprehensiveReasoningService:
    """Enhanced reasoning service for comprehensive article analysis"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY, base_url: Optional[str] = OPENAI_BASE_URL):
        if not api_key or api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API key must be provided")
        
        client_kwargs = {"api_key": api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = AsyncOpenAI(**client_kwargs)
        
        # LangChain ChatOpenAI for chains
        langchain_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": api_key
        }
        if base_url:
            langchain_kwargs["base_url"] = base_url
            
        self.llm = ChatOpenAI(**langchain_kwargs)
    
    async def analyze_comprehensive_structure(self, text: str) -> Dict[str, Any]:
        """Comprehensive GDPR document structure analysis"""
        
        messages = [
            {
                "role": "user",
                "content": f"""You are a legal expert analyzing GDPR documents. Perform comprehensive structural analysis.

Text: {text[:3000]}

Extract and analyze:
1. Document type (EU GDPR, UK GDPR, guidance, etc.)
2. ALL articles with their complete numbers, titles, and full content
3. ALL cross-references between articles (e.g., "as defined in Article X", "see Article Y")
4. Chapter structure with titles and article mappings
5. Legal concepts and their defining articles
6. Hierarchical relationships between provisions
7. Cross-reference patterns and linking structure

Provide comprehensive JSON structure:
{{
    "document_type": "gdpr_eu|gdpr_uk|guidance",
    "articles": [
        {{
            "number": "string",
            "title": "string",
            "full_content": "string",
            "sections": ["list of sections"],
            "cross_references": ["Article X", "Article Y"],
            "legal_concepts": ["concept1", "concept2"],
            "hierarchy_level": number
        }}
    ],
    "chapters": [
        {{
            "number": "string",
            "title": "string", 
            "articles": ["list of article numbers"]
        }}
    ],
    "cross_reference_map": {{
        "Article X": ["referenced by articles"],
        "Article Y": ["referenced by articles"]
    }},
    "legal_concepts": {{
        "concept_name": {{
            "definition": "string",
            "primary_article": "string",
            "related_articles": ["list"]
        }}
    }}
}}"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort=OPENAI_REASONING_EFFORT
            )
            
            import json
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return self._default_structure()
    
    async def extract_article_references(self, text: str) -> List[ArticleReference]:
        """Extract all article references from text"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract ALL article references from this GDPR text. Find every mention of other articles.

Text: {text[:1500]}

Look for patterns like:
- "Article 6" (direct reference)
- "as defined in Article 7"
- "pursuant to Article 8"
- "see Article 9(1)"
- "in accordance with Articles 10 and 11"
- "Article 12, paragraph 2"

For each reference, determine:
1. Referenced article number
2. Section/paragraph if specified
3. Context of the reference
4. Type of reference (definition, procedure, requirement, etc.)

Return JSON array:
[
    {{
        "article_number": "6",
        "section_reference": null,
        "paragraph_reference": null,
        "context": "legal basis for processing",
        "reference_type": "definition"
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            references_data = json.loads(response.choices[0].message.content)
            
            return [
                ArticleReference(
                    article_number=ref["article_number"],
                    article_title="",  # Will be filled later
                    section_reference=ref.get("section_reference"),
                    paragraph_reference=ref.get("paragraph_reference"),
                    context=ref["context"],
                    reference_type=ref["reference_type"]
                )
                for ref in references_data
            ]
            
        except Exception as e:
            logger.error(f"Error extracting references: {e}")
            return []
    
    async def extract_legal_concepts_comprehensive(self, text: str) -> List[LegalConcept]:
        """Extract comprehensive legal concepts with article mappings"""
        
        messages = [
            {
                "role": "user",
                "content": f"""Extract legal concepts from GDPR text with comprehensive article mappings.

Text: {text[:1500]}

Identify:
1. Legal concepts (rights, obligations, processes, entities, principles)
2. Their definitions in the text
3. Primary article that defines each concept
4. Related articles that mention or expand on the concept

Focus on key GDPR concepts like:
- Rights (access, rectification, erasure, portability, etc.)
- Legal bases (consent, contract, legal obligation, etc.)
- Entities (controller, processor, supervisory authority, etc.)
- Processes (processing, profiling, automated decision-making)
- Principles (lawfulness, fairness, transparency, etc.)

Return JSON array:
[
    {{
        "concept_name": "right of access",
        "definition": "extracted definition",
        "primary_article": "15",
        "related_articles": ["12", "13", "14"]
    }}
]"""
            }
        ]
        
        try:
            response = await self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                reasoning_effort="medium"
            )
            
            import json
            concepts_data = json.loads(response.choices[0].message.content)
            
            return [
                LegalConcept(
                    concept_name=concept["concept_name"],
                    definition=concept["definition"],
                    primary_article=concept["primary_article"],
                    related_articles=concept.get("related_articles", [])
                )
                for concept in concepts_data
            ]
            
        except Exception as e:
            logger.error(f"Error extracting legal concepts: {e}")
            return []
    
    def _default_structure(self) -> Dict[str, Any]:
        """Default structure when analysis fails"""
        return {
            "document_type": "unknown",
            "articles": [],
            "chapters": [],
            "cross_reference_map": {},
            "legal_concepts": {}
        }

# =============================================================================
# FIXED ELASTICSEARCH SERVICE
# =============================================================================

class FixedElasticsearchService:
    """Fixed Elasticsearch service with correct parameters"""
    
    def __init__(self, es_config: SimpleElasticsearchConfig, embedding_service: DirectOpenAIEmbeddingService):
        self.es_config = es_config
        self.embedding_service = embedding_service
        
        # LangChain ChatOpenAI
        llm_kwargs = {
            "model": OPENAI_MODEL,
            "api_key": OPENAI_API_KEY
        }
        if OPENAI_BASE_URL:
            llm_kwargs["base_url"] = OPENAI_BASE_URL
            
        self.llm = ChatOpenAI(**llm_kwargs)
    
    def create_vectorstore(self, index_name: str = ELASTICSEARCH_INDEX_NAME) -> ElasticsearchStore:
        """Create ElasticsearchStore with correct parameters"""
        
        # Create a dummy embedding function for ElasticsearchStore
        class DummyEmbedding:
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                # This won't be used as we'll add embeddings separately
                return [[0.0] * 1536 for _ in texts]
            
            def embed_query(self, text: str) -> List[float]:
                # This won't be used as we'll handle queries separately
                return [0.0] * 1536
        
        dummy_embedding = DummyEmbedding()
        
        # Get Elasticsearch client
        es_client = self.es_config.create_client()
        
        # Create ElasticsearchStore with correct parameters
        try:
            vectorstore = ElasticsearchStore(
                index_name=index_name,
                embedding=dummy_embedding,
                es_connection=es_client
            )
            logger.info(f"Created ElasticsearchStore with index: {index_name}")
            return vectorstore
            
        except Exception as e:
            logger.error(f"Error creating ElasticsearchStore: {e}")
            # Try alternative constructor
            vectorstore = ElasticsearchStore(
                index_name=index_name,
                embedding=dummy_embedding,
                elasticsearch_url=self.es_config.get_elasticsearch_url(),
                es_user=self.es_config.username,
                es_password=self.es_config.password
            )
            return vectorstore
    
    def create_comprehensive_qa_chains(self, vectorstore: ElasticsearchStore) -> Tuple[RetrievalQA, ConversationalRetrievalChain, ConversationSummaryBufferMemory]:
        """Create QA chains with comprehensive legal analysis prompts"""
        
        # Enhanced conversation memory
        conversation_memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=3000,
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Comprehensive QA prompt
        qa_prompt = PromptTemplate(
            template="""You are an expert GDPR legal analyst with comprehensive knowledge of all articles, cross-references, and legal concepts.

Context from documents: {context}

Question: {question}

Instructions for comprehensive analysis:
1. Cite specific GDPR articles, sections, and paragraphs with exact numbers
2. Include ALL relevant cross-references mentioned in the source articles
3. Explain relationships between different articles and provisions
4. Reference related legal concepts and their definitions
5. Mention any "as defined in Article X" or "see Article Y" references
6. Provide the full legal context with supporting and related provisions
7. If an article references procedures in other articles, include those details
8. Explain how different articles work together to address the query

Legal Analysis with Full Context and Cross-References:""",
            input_variables=["context", "question"]
        )
        
        # Enhanced QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": 25,  # More documents for comprehensive coverage
                }
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": qa_prompt}
        )
        
        # Conversational chain
        conversational_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 20}),
            memory=conversation_memory,
            return_source_documents=True,
            verbose=True
        )
        
        return qa_chain, conversational_chain, conversation_memory

# =============================================================================
# COMPREHENSIVE DOCUMENT PROCESSING AGENT
# =============================================================================

class ComprehensiveDocumentProcessor:
    """Enhanced document processor with comprehensive article analysis"""
    
    def __init__(self, 
                 reasoning_service: ComprehensiveReasoningService,
                 embedding_service: DirectOpenAIEmbeddingService):
        self.reasoning_service = reasoning_service
        self.embedding_service = embedding_service
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    async def process_documents_comprehensive(self, file_paths: List[str]) -> List[Document]:
        """Process documents with comprehensive article analysis and linking"""
        all_documents = []
        position_counter = 0
        
        for file_path in file_paths:
            try:
                # Extract text
                text, page_info = self._extract_text_pymupdf(file_path)
                
                # Comprehensive structure analysis
                structure = await self.reasoning_service.analyze_comprehensive_structure(text)
                
                # Determine document type
                doc_type = self._determine_document_type(file_path, text, structure)
                
                # Create comprehensive chunks
                chunks = await self._create_comprehensive_chunks(
                    text, doc_type, structure, file_path, page_info, position_counter
                )
                
                # Convert to LangChain Documents
                documents = self._chunks_to_documents(chunks)
                all_documents.extend(documents)
                
                position_counter += len(chunks)
                logger.info(f"Processed {file_path}: {len(chunks)} comprehensive chunks")
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {e}")
        
        return all_documents
    
    def _extract_text_pymupdf(self, file_path: str) -> Tuple[str, Dict]:
        """Extract text using PyMuPDF"""
        doc = pymupdf.open(file_path)
        text = ""
        page_info = {}
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            page_text = page.get_text()
            text += page_text + "\n"
            
            page_info[page_num] = {
                "text_length": len(page_text),
                "has_images": len(page.get_images()) > 0
            }
        
        doc.close()
        return text, page_info
    
    def _determine_document_type(self, file_path: str, text: str, structure: Dict) -> DocumentType:
        """Determine document type"""
        if structure.get("document_type") == "gdpr_uk":
            return DocumentType.GDPR_UK
        elif structure.get("document_type") == "gdpr_eu":
            return DocumentType.GDPR_EU
        
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in ["uk gdpr", "data protection act 2018", "ico"]):
            return DocumentType.GDPR_UK
        return DocumentType.GDPR_EU
    
    async def _create_comprehensive_chunks(self, text: str, doc_type: DocumentType, structure: Dict, 
                                         file_path: str, page_info: Dict, position_offset: int) -> List[ComprehensiveChunk]:
        """Create comprehensive chunks with full article context and linking"""
        chunks = []
        
        # Create article-based chunks first
        articles = structure.get("articles", [])
        article_chunks = await self._create_article_chunks(articles, doc_type, file_path, position_offset)
        chunks.extend(article_chunks)
        
        # Create standard text chunks for non-article content
        text_chunks = self.text_splitter.split_text(text)
        standard_chunks = await self._create_standard_chunks(
            text_chunks, doc_type, structure, file_path, page_info, 
            position_offset + len(article_chunks)
        )
        chunks.extend(standard_chunks)
        
        return chunks
    
    async def _create_article_chunks(self, articles: List[Dict], doc_type: DocumentType, 
                                   file_path: str, position_offset: int) -> List[ComprehensiveChunk]:
        """Create chunks for full articles with comprehensive linking"""
        chunks = []
        
        for i, article in enumerate(articles):
            article_number = article.get("number", "")
            article_title = article.get("title", "")
            full_content = article.get("full_content", "")
            
            if not full_content:
                continue
            
            # Extract article references
            article_refs = await self.reasoning_service.extract_article_references(full_content)
            
            # Extract legal concepts
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(full_content)
            
            # Create main article chunk
            main_chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=ChunkType.FULL_ARTICLE,
                title=f"Article {article_number}: {article_title}",
                content=full_content,
                full_article_content=full_content,
                article_number=article_number,
                hierarchy_level=2,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=article.get("cross_references", []),
                keywords=self._extract_keywords(full_content),
                legal_terms=self._extract_legal_terms(full_content),
                metadata={
                    "source_file": file_path,
                    "article_sections": article.get("sections", []),
                    "is_full_article": True
                }
            )
            
            chunks.append(main_chunk)
            
            # Create section chunks if article has sections
            sections = article.get("sections", [])
            for j, section in enumerate(sections):
                section_chunk = ComprehensiveChunk(
                    document_type=doc_type,
                    chunk_type=ChunkType.ARTICLE_SECTION,
                    title=f"Article {article_number}, Section {j+1}",
                    content=section,
                    full_article_content=full_content,
                    article_number=article_number,
                    section_number=str(j+1),
                    hierarchy_level=3,
                    position_in_document=position_offset + i + j + 1,
                    article_references=article_refs,
                    legal_concepts=legal_concepts,
                    metadata={
                        "source_file": file_path,
                        "parent_article_chunk_id": main_chunk.id,
                        "is_article_section": True
                    }
                )
                chunks.append(section_chunk)
        
        return chunks
    
    async def _create_standard_chunks(self, text_chunks: List[str], doc_type: DocumentType, 
                                    structure: Dict, file_path: str, page_info: Dict, 
                                    position_offset: int) -> List[ComprehensiveChunk]:
        """Create standard chunks for non-article content"""
        chunks = []
        
        for i, chunk_text in enumerate(text_chunks):
            # Extract references and concepts
            article_refs = await self.reasoning_service.extract_article_references(chunk_text)
            legal_concepts = await self.reasoning_service.extract_legal_concepts_comprehensive(chunk_text)
            
            # Determine chunk characteristics
            chunk_type = self._determine_chunk_type(chunk_text)
            title = self._extract_title(chunk_text)
            hierarchy_level = self._determine_hierarchy_level(chunk_text)
            article_number = self._extract_article_number(chunk_text)
            chapter_number = self._extract_chapter_number(chunk_text)
            
            chunk = ComprehensiveChunk(
                document_type=doc_type,
                chunk_type=chunk_type,
                title=title,
                content=chunk_text,
                chapter_number=chapter_number,
                article_number=article_number,
                hierarchy_level=hierarchy_level,
                position_in_document=position_offset + i,
                article_references=article_refs,
                legal_concepts=legal_concepts,
                cross_references=self._extract_cross_references(chunk_text),
                keywords=self._extract_keywords(chunk_text),
                legal_terms=self._extract_legal_terms(chunk_text),
                metadata={
                    "source_file": file_path,
                    "chunk_index": i,
                    "is_standard_chunk": True
                }
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _chunks_to_documents(self, chunks: List[ComprehensiveChunk]) -> List[Document]:
        """Convert comprehensive chunks to LangChain Documents"""
        documents = []
        
        for chunk in chunks:
            # Create enhanced content for embedding
            enhanced_content = self._create_enhanced_content(chunk)
            
            doc = Document(
                page_content=enhanced_content,
                metadata={
                    "chunk_id": chunk.id,
                    "document_type": chunk.document_type.value,
                    "chunk_type": chunk.chunk_type.value,
                    "title": chunk.title,
                    "original_content": chunk.content,
                    "full_article_content": chunk.full_article_content,
                    "article_number": chunk.article_number,
                    "chapter_number": chunk.chapter_number,
                    "section_number": chunk.section_number,
                    "hierarchy_level": chunk.hierarchy_level,
                    "position_in_document": chunk.position_in_document,
                    "article_references": [ref.dict() for ref in chunk.article_references],
                    "legal_concepts": [concept.dict() for concept in chunk.legal_concepts],
                    "cross_references": chunk.cross_references,
                    "keywords": chunk.keywords,
                    "legal_terms": chunk.legal_terms,
                    **chunk.metadata
                }
            )
            documents.append(doc)
        
        return documents
    
    def _create_enhanced_content(self, chunk: ComprehensiveChunk) -> str:
        """Create enhanced content for better embedding and retrieval"""
        parts = []
        
        # Title and article info
        parts.append(f"TITLE: {chunk.title}")
        
        if chunk.article_number:
            parts.append(f"ARTICLE: {chunk.article_number}")
        
        if chunk.chapter_number:
            parts.append(f"CHAPTER: {chunk.chapter_number}")
        
        # Main content
        parts.append(f"CONTENT: {chunk.content}")
        
        # Full article context if available
        if chunk.full_article_content and chunk.chunk_type != ChunkType.FULL_ARTICLE:
            parts.append(f"FULL ARTICLE CONTEXT: {chunk.full_article_content[:500]}...")
        
        # Article references
        if chunk.article_references:
            ref_text = ", ".join([f"Article {ref.article_number}" for ref in chunk.article_references])
            parts.append(f"REFERENCES: {ref_text}")
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_text = ", ".join([concept.concept_name for concept in chunk.legal_concepts])
            parts.append(f"LEGAL CONCEPTS: {concept_text}")
        
        # Cross-references
        if chunk.cross_references:
            parts.append(f"CROSS REFERENCES: {', '.join(chunk.cross_references)}")
        
        return "\n\n".join(parts)
    
    # Helper methods
    def _determine_chunk_type(self, text: str) -> ChunkType:
        text_lower = text.lower()
        if "article" in text_lower[:100]:
            return ChunkType.FULL_ARTICLE
        elif "chapter" in text_lower[:100]:
            return ChunkType.CHAPTER
        elif "section" in text_lower[:100]:
            return ChunkType.SECTION
        return ChunkType.PARAGRAPH
    
    def _extract_title(self, text: str) -> str:
        lines = text.split('\n')
        for line in lines[:3]:
            if line.strip() and len(line.strip()) < 150:
                return line.strip()
        return text[:80] + "..."
    
    def _determine_hierarchy_level(self, text: str) -> int:
        text_lower = text.lower()
        if "chapter" in text_lower[:100]:
            return 1
        elif "article" in text_lower[:100]:
            return 2
        elif "section" in text_lower[:100]:
            return 3
        return 4
    
    def _extract_article_number(self, text: str) -> Optional[str]:
        match = re.search(r'article\s+(\d+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_chapter_number(self, text: str) -> Optional[str]:
        match = re.search(r'chapter\s+(\d+|[ivx]+)', text.lower())
        return match.group(1) if match else None
    
    def _extract_cross_references(self, text: str) -> List[str]:
        """Extract cross-references like 'see Article X', 'as defined in Article Y'"""
        patterns = [
            r'see Article (\d+)',
            r'as defined in Article (\d+)',
            r'pursuant to Article (\d+)',
            r'in accordance with Article (\d+)',
            r'Article (\d+)\([^)]*\)',
        ]
        
        references = []
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                references.append(f"Article {match.group(1)}")
        
        return list(set(references))  # Remove duplicates
    
    def _extract_keywords(self, text: str) -> List[str]:
        gdpr_keywords = [
            "personal data", "data subject", "consent", "processing", "controller",
            "processor", "lawful basis", "legitimate interest", "data protection",
            "rights", "erasure", "rectification", "portability", "breach", "profiling",
            "automated decision-making", "supervisory authority", "cross-border processing"
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in gdpr_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _extract_legal_terms(self, text: str) -> List[str]:
        legal_terms = [
            "shall", "must", "may", "should", "obligation", "requirement",
            "prohibition", "permission", "duty", "responsibility", "liability",
            "compliance", "violation", "penalty", "fine", "sanction"
        ]
        
        found_terms = []
        text_lower = text.lower()
        for term in legal_terms:
            if term in text_lower:
                found_terms.append(term)
        
        return found_terms

# =============================================================================
# MAIN COMPREHENSIVE SYSTEM
# =============================================================================

class ComprehensiveGDPRSystem:
    """Comprehensive GDPR system with enhanced article linking and direct embeddings"""
    
    def __init__(self, 
                 es_host: str = ELASTICSEARCH_HOST,
                 es_port: int = ELASTICSEARCH_PORT,
                 es_username: str = ELASTICSEARCH_USERNAME,
                 es_password: str = ELASTICSEARCH_PASSWORD):
        
        # Initialize services
        self.es_config = SimpleElasticsearchConfig(es_host, es_port, es_username, es_password)
        self.embedding_service = DirectOpenAIEmbeddingService()
        self.reasoning_service = ComprehensiveReasoningService()
        self.elasticsearch_service = FixedElasticsearchService(self.es_config, self.embedding_service)
        
        # Initialize document processor
        self.document_processor = ComprehensiveDocumentProcessor(
            self.reasoning_service, self.embedding_service
        )
        
        # Create workflow
        self.workflow = self._create_comprehensive_workflow()
    
    def _create_comprehensive_workflow(self) -> StateGraph:
        """Create comprehensive workflow"""
        
        workflow = StateGraph(ComprehensiveMemoryState)
        
        # Add nodes
        workflow.add_node("parse_comprehensive", self._parse_comprehensive_node)
        workflow.add_node("generate_direct_embeddings", self._generate_direct_embeddings_node)
        workflow.add_node("build_indices", self._build_indices_node)
        workflow.add_node("create_vectorstore", self._create_vectorstore_node)
        workflow.add_node("setup_qa_chains", self._setup_qa_chains_node)
        
        # Define edges
        workflow.add_edge(START, "parse_comprehensive")
        workflow.add_edge("parse_comprehensive", "generate_direct_embeddings")
        workflow.add_edge("generate_direct_embeddings", "build_indices")
        workflow.add_edge("build_indices", "create_vectorstore")
        workflow.add_edge("create_vectorstore", "setup_qa_chains")
        workflow.add_edge("setup_qa_chains", END)
        
        return workflow.compile()
    
    async def _parse_comprehensive_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Parse documents comprehensively"""
        state.current_agent = "comprehensive_parser"
        try:
            file_paths = []
            for doc in state.documents:
                if hasattr(doc, 'metadata') and 'path' in doc.metadata:
                    file_paths.append(doc.metadata['path'])
            
            documents = await self.document_processor.process_documents_comprehensive(file_paths)
            state.documents = documents
            
            # Convert to comprehensive chunks
            state.comprehensive_chunks = [
                self._document_to_comprehensive_chunk(doc) for doc in documents
            ]
            
            logger.info(f"Created {len(state.comprehensive_chunks)} comprehensive chunks")
            
        except Exception as e:
            error_msg = f"Comprehensive parsing error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _generate_direct_embeddings_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Generate embeddings using direct OpenAI API"""
        state.current_agent = "direct_embedding_generator"
        try:
            # Prepare enhanced texts for embedding
            enhanced_texts = []
            for chunk in state.comprehensive_chunks:
                enhanced_text = self._create_embedding_text(chunk)
                enhanced_texts.append(enhanced_text)
            
            # Generate embeddings in batches
            for i in range(0, len(enhanced_texts), BATCH_SIZE):
                batch_texts = enhanced_texts[i:i + BATCH_SIZE]
                batch_chunks = state.comprehensive_chunks[i:i + BATCH_SIZE]
                
                try:
                    embeddings = await self.embedding_service.embed_texts(batch_texts)
                    
                    for chunk, embedding in zip(batch_chunks, embeddings):
                        chunk.embedding = embedding
                    
                    logger.info(f"Generated direct embeddings for batch {i//BATCH_SIZE + 1}")
                    await asyncio.sleep(0.5)  # Rate limiting
                    
                except Exception as e:
                    logger.error(f"Error in embedding batch {i//BATCH_SIZE + 1}: {e}")
            
        except Exception as e:
            error_msg = f"Direct embedding error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _build_indices_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Build comprehensive indices"""
        state.current_agent = "index_builder"
        try:
            # Build article index
            for chunk in state.comprehensive_chunks:
                if chunk.article_number:
                    state.article_index[chunk.article_number] = chunk.id
            
            # Build concept index
            for chunk in state.comprehensive_chunks:
                for concept in chunk.legal_concepts:
                    if concept.concept_name not in state.concept_index:
                        state.concept_index[concept.concept_name] = []
                    state.concept_index[concept.concept_name].append(chunk.id)
            
            # Build cross-reference index
            for chunk in state.comprehensive_chunks:
                for ref in chunk.cross_references:
                    if ref not in state.cross_reference_index:
                        state.cross_reference_index[ref] = []
                    state.cross_reference_index[ref].append(chunk.id)
            
            logger.info(f"Built indices: {len(state.article_index)} articles, {len(state.concept_index)} concepts")
            
        except Exception as e:
            error_msg = f"Index building error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _create_vectorstore_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Create vector store"""
        state.current_agent = "vectorstore_creator"
        try:
            vectorstore = self.elasticsearch_service.create_vectorstore()
            
            # Add documents with embeddings
            documents_with_embeddings = []
            for doc, chunk in zip(state.documents, state.comprehensive_chunks):
                if chunk.embedding:
                    # Add embedding to document metadata
                    doc.metadata["embedding"] = chunk.embedding
                    documents_with_embeddings.append(doc)
            
            # Add to vectorstore in batches
            for i in range(0, len(documents_with_embeddings), BATCH_SIZE):
                batch = documents_with_embeddings[i:i + BATCH_SIZE]
                await vectorstore.aadd_documents(batch)
                logger.info(f"Added document batch {i//BATCH_SIZE + 1} to vectorstore")
            
            state.vectorstore = vectorstore
            logger.info("Created comprehensive vectorstore")
            
        except Exception as e:
            error_msg = f"Vectorstore creation error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    async def _setup_qa_chains_node(self, state: ComprehensiveMemoryState) -> ComprehensiveMemoryState:
        """Setup comprehensive QA chains"""
        state.current_agent = "qa_chain_creator"
        try:
            if state.vectorstore:
                qa_chain, conv_chain, memory = self.elasticsearch_service.create_comprehensive_qa_chains(state.vectorstore)
                state.qa_chain = qa_chain
                state.conversational_chain = conv_chain
                state.conversation_memory = memory
                
                logger.info("Created comprehensive QA chains")
                
        except Exception as e:
            error_msg = f"QA chain setup error: {str(e)}"
            logger.error(error_msg)
            state.errors.append(error_msg)
        return state
    
    def _document_to_comprehensive_chunk(self, doc: Document) -> ComprehensiveChunk:
        """Convert Document to ComprehensiveChunk"""
        
        # Extract article references from metadata
        article_refs = []
        if "article_references" in doc.metadata:
            for ref_data in doc.metadata["article_references"]:
                article_refs.append(ArticleReference(**ref_data))
        
        # Extract legal concepts from metadata
        legal_concepts = []
        if "legal_concepts" in doc.metadata:
            for concept_data in doc.metadata["legal_concepts"]:
                legal_concepts.append(LegalConcept(**concept_data))
        
        return ComprehensiveChunk(
            id=doc.metadata.get("chunk_id", str(uuid4())),
            document_type=DocumentType(doc.metadata["document_type"]),
            chunk_type=ChunkType(doc.metadata["chunk_type"]),
            title=doc.metadata["title"],
            content=doc.metadata.get("original_content", doc.page_content),
            full_article_content=doc.metadata.get("full_article_content"),
            article_number=doc.metadata.get("article_number"),
            chapter_number=doc.metadata.get("chapter_number"),
            section_number=doc.metadata.get("section_number"),
            hierarchy_level=doc.metadata["hierarchy_level"],
            position_in_document=doc.metadata["position_in_document"],
            article_references=article_refs,
            legal_concepts=legal_concepts,
            cross_references=doc.metadata.get("cross_references", []),
            keywords=doc.metadata.get("keywords", []),
            legal_terms=doc.metadata.get("legal_terms", []),
            metadata=doc.metadata
        )
    
    def _create_embedding_text(self, chunk: ComprehensiveChunk) -> str:
        """Create optimized text for embedding"""
        parts = []
        
        # Core information
        parts.append(f"Document Type: {chunk.document_type.value}")
        parts.append(f"Title: {chunk.title}")
        
        # Article information
        if chunk.article_number:
            parts.append(f"Article {chunk.article_number}")
        
        # Main content
        parts.append(chunk.content)
        
        # Full article context for sections
        if chunk.full_article_content and chunk.chunk_type == ChunkType.ARTICLE_SECTION:
            parts.append(f"Full Article Context: {chunk.full_article_content}")
        
        # Referenced articles
        if chunk.article_references:
            ref_info = []
            for ref in chunk.article_references:
                ref_info.append(f"Article {ref.article_number} ({ref.reference_type}): {ref.context}")
            parts.append("Referenced Articles: " + "; ".join(ref_info))
        
        # Legal concepts
        if chunk.legal_concepts:
            concept_info = []
            for concept in chunk.legal_concepts:
                concept_info.append(f"{concept.concept_name}: {concept.definition}")
            parts.append("Legal Concepts: " + "; ".join(concept_info))
        
        return "\n\n".join(parts)
    
    # Public API
    async def process_documents(self, file_paths: List[str]) -> ComprehensiveMemoryState:
        """Process documents comprehensively"""
        
        initial_state = ComprehensiveMemoryState(
            documents=[
                Document(page_content="", metadata={"path": path}) 
                for path in file_paths
            ],
            metadata={
                "started_at": datetime.now(),
                "file_paths": file_paths
            }
        )
        
        try:
            final_state = await self.workflow.ainvoke(initial_state)
            final_state.metadata["completed_at"] = datetime.now()
            final_state.metadata["duration"] = (
                final_state.metadata["completed_at"] - final_state.metadata["started_at"]
            ).total_seconds()
            
            logger.info(f"Comprehensive processing completed in {final_state.metadata['duration']:.2f} seconds")
            return final_state
            
        except Exception as e:
            error_msg = f"Comprehensive workflow error: {str(e)}"
            logger.error(error_msg)
            initial_state.errors.append(error_msg)
            return initial_state
    
    async def ask_comprehensive(self, state: ComprehensiveMemoryState, question: str) -> Dict[str, Any]:
        """Ask question with comprehensive analysis"""
        
        if not state.qa_chain:
            return {"error": "QA chain not available"}
        
        try:
            result = await state.qa_chain.ainvoke({"query": question})
            
            # Enhanced response with related information
            response = {
                "answer": result["result"],
                "source_documents": [],
                "related_articles": [],
                "legal_concepts": [],
                "cross_references": []
            }
            
            # Process source documents
            for doc in result.get("source_documents", []):
                source_info = {
                    "content": doc.page_content[:400] + "...",
                    "article_number": doc.metadata.get("article_number"),
                    "title": doc.metadata.get("title"),
                    "chunk_type": doc.metadata.get("chunk_type"),
                    "metadata": doc.metadata
                }
                response["source_documents"].append(source_info)
                
                # Extract related articles
                if doc.metadata.get("article_references"):
                    for ref in doc.metadata["article_references"]:
                        if ref not in response["related_articles"]:
                            response["related_articles"].append(ref)
                
                # Extract legal concepts
                if doc.metadata.get("legal_concepts"):
                    for concept in doc.metadata["legal_concepts"]:
                        if concept not in response["legal_concepts"]:
                            response["legal_concepts"].append(concept)
                
                # Extract cross-references
                if doc.metadata.get("cross_references"):
                    response["cross_references"].extend(doc.metadata["cross_references"])
            
            # Remove duplicates
            response["cross_references"] = list(set(response["cross_references"]))
            
            return response
            
        except Exception as e:
            return {"error": f"Comprehensive QA error: {str(e)}"}

# =============================================================================
# FIXED EXAMPLE USAGE
# =============================================================================

async def main():
    """Production example with comprehensive GDPR analysis"""
    
    try:
        print(" Initializing Comprehensive GDPR System with Direct OpenAI Embeddings...")
        print(f" Configuration:")
        print(f"    OpenAI Model: {OPENAI_MODEL}")
        print(f"    Embedding Model: {OPENAI_EMBEDDING_MODEL}")
        print(f"    Elasticsearch: {ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}")
        print(f"    SSL: {ELASTICSEARCH_USE_SSL}")
        
        # Initialize system
        gdpr_system = ComprehensiveGDPRSystem(
            es_host=ELASTICSEARCH_HOST,
            es_port=ELASTICSEARCH_PORT,
            es_username=ELASTICSEARCH_USERNAME,
            es_password=ELASTICSEARCH_PASSWORD
        )
        
        # Process documents
        document_paths = [
            "path/to/gdpr_regulation.pdf",
            "path/to/gdpr_guidance.pdf"
        ]
        
        print(" Processing documents with comprehensive analysis...")
        state = await gdpr_system.process_documents(document_paths)
        
        print(f" Processing completed!")
        print(f" Processed {len(state.comprehensive_chunks)} comprehensive chunks")
        print(f" Article index: {len(state.article_index)} articles")
        print(f" Concept index: {len(state.concept_index)} legal concepts")
        print(f" Cross-reference index: {len(state.cross_reference_index)} references")
        
        if state.errors:
            print(f" Errors: {state.errors}")
        
        # Test comprehensive QA
        if state.qa_chain:
            print("\n Testing comprehensive QA:")
            
            question = "What are the lawful bases for processing personal data under Article 6, and how do they relate to the consent requirements in other articles?"
            
            print(f" Q: {question}")
            
            answer = await gdpr_system.ask_comprehensive(state, question)
            
            if "error" not in answer:
                print(f" A: {answer['answer'][:500]}...")
                print(f" Sources: {len(answer['source_documents'])}")
                print(f" Related Articles: {len(answer['related_articles'])}")
                print(f" Legal Concepts: {len(answer['legal_concepts'])}")
                print(f" Cross-References: {answer['cross_references'][:5]}")
            else:
                print(f" Error: {answer['error']}")
        
    except Exception as e:
        print(f" Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
