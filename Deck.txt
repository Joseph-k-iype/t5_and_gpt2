#!/usr/bin/env python3
"""
Enhanced Global GDPR Record of Processing Activities (RoPA) Metamodel System
Specialized for GDPR, UK GDPR, and Global Jurisdictional Compliance in Financial Institutions

Features:
- AI-powered synonym generation using LLM (o3-mini)
- Comprehensive global jurisdictional compliance analysis
- Advanced regulatory concept extraction with reasoning
- Iterative document understanding and metamodel refinement
- Cross-border data transfer compliance mapping
- Financial sector-specific processing activities
- Comprehensive compliance reporting

REQUIRED ENVIRONMENT VARIABLES:
    OPENAI_API_KEY=your_openai_api_key
    OPENAI_BASE_URL=your_custom_openai_endpoint (optional)
    ELASTICSEARCH_HOST=https://your-elasticsearch-cluster.com:9200
    ELASTICSEARCH_USERNAME=your_username (optional)
    ELASTICSEARCH_PASSWORD=your_password (optional)
    FALKORDB_HOST=localhost
    FALKORDB_PORT=6379
    FALKORDB_PASSWORD=your_password (optional)

USAGE:
    python enhanced_gdpr_ropa_system.py --ingest /path/to/gdpr/documents
    python enhanced_gdpr_ropa_system.py --analyze --iterations 3
    python enhanced_gdpr_ropa_system.py --generate-metamodel
    python enhanced_gdpr_ropa_system.py --generate-report
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Tuple
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
from pathlib import Path

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Global jurisdiction frameworks
class JurisdictionType(Enum):
    GDPR_EU = "gdpr_eu"
    UK_GDPR = "uk_gdpr"
    CCPA = "ccpa"
    PIPEDA = "pipeda"
    LGPD = "lgpd"
    PIPL = "pipl"
    PDPA_SINGAPORE = "pdpa_singapore"
    OTHER = "other"

@dataclass
class JurisdictionFramework:
    name: str
    code: str
    territorial_scope: List[str]
    extraterritorial_triggers: List[str]
    adequacy_status: str
    transfer_mechanisms: List[str]
    key_principles: List[str]
    penalties: str
    data_subject_rights: List[str]
    financial_sector_specifics: List[str]

# Comprehensive jurisdiction frameworks
JURISDICTION_FRAMEWORKS = {
    JurisdictionType.GDPR_EU: JurisdictionFramework(
        name="EU General Data Protection Regulation",
        code="GDPR",
        territorial_scope=[
            "All 27 EU Member States", "European Economic Area (EEA)", 
            "Organizations established in EU", "Processing in context of EU establishment"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to EU data subjects",
            "Monitoring behavior of EU data subjects",
            "Processing personal data of EU residents",
            "Targeting EU market regardless of location"
        ],
        adequacy_status="Source jurisdiction",
        transfer_mechanisms=[
            "Standard Contractual Clauses (SCCs)", "Binding Corporate Rules (BCRs)",
            "Certification mechanisms", "Codes of conduct", "Derogations Article 49"
        ],
        key_principles=[
            "lawfulness", "fairness", "transparency", "purpose_limitation", 
            "data_minimization", "accuracy", "storage_limitation", 
            "integrity_confidentiality", "accountability"
        ],
        penalties="Up to €20 million or 4% of annual global turnover",
        data_subject_rights=[
            "access", "rectification", "erasure", "portability", 
            "restriction", "objection", "automated_decision_making"
        ],
        financial_sector_specifics=[
            "Payment Services Directive", "MiFID II compliance", 
            "AML/CTF requirements", "Banking supervision"
        ]
    ),
    JurisdictionType.UK_GDPR: JurisdictionFramework(
        name="United Kingdom GDPR",
        code="UK-GDPR",
        territorial_scope=[
            "England", "Wales", "Scotland", "Northern Ireland",
            "Organizations established in UK", "Processing in context of UK establishment"
        ],
        extraterritorial_triggers=[
            "Offering goods/services to UK data subjects",
            "Monitoring behavior of UK data subjects", 
            "Processing personal data of UK residents",
            "Targeting UK market regardless of location"
        ],
        adequacy_status="Adequate for EU transfers",
        transfer_mechanisms=[
            "International Data Transfer Agreement (IDTA)", "Standard Contractual Clauses",
            "Binding Corporate Rules", "Certification mechanisms", "Derogations"
        ],
        key_principles=[
            "lawfulness", "fairness", "transparency", "purpose_limitation", 
            "data_minimization", "accuracy", "storage_limitation", 
            "integrity_confidentiality", "accountability"
        ],
        penalties="Up to £17.5 million or 4% of annual global turnover",
        data_subject_rights=[
            "access", "rectification", "erasure", "portability", 
            "restriction", "objection", "automated_decision_making"
        ],
        financial_sector_specifics=[
            "FCA requirements", "PCI DSS compliance", "Open Banking", 
            "Strong Customer Authentication"
        ]
    ),
    JurisdictionType.CCPA: JurisdictionFramework(
        name="California Consumer Privacy Act",
        code="CCPA",
        territorial_scope=[
            "California residents", "Businesses operating in California",
            "Organizations processing California residents' data"
        ],
        extraterritorial_triggers=[
            "Processing California residents' personal information",
            "Selling to California consumers", "Offering services to California residents"
        ],
        adequacy_status="No EU adequacy decision",
        transfer_mechanisms=[
            "Privacy Shield successor", "Standard Contractual Clauses", 
            "Company-specific adequacy assessments"
        ],
        key_principles=[
            "transparency", "consumer_control", "data_minimization", 
            "purpose_limitation", "security"
        ],
        penalties="Up to $7,500 per violation",
        data_subject_rights=[
            "right_to_know", "right_to_delete", "right_to_opt_out", 
            "right_to_non_discrimination"
        ],
        financial_sector_specifics=[
            "GLBA alignment", "FCRA compliance", "State banking regulations"
        ]
    ),
    JurisdictionType.PIPL: JurisdictionFramework(
        name="Personal Information Protection Law",
        code="PIPL",
        territorial_scope=[
            "China mainland", "Organizations processing personal information in China",
            "Foreign organizations processing Chinese residents' data"
        ],
        extraterritorial_triggers=[
            "Targeting Chinese residents", "Analyzing behavior of Chinese residents",
            "Providing products/services to Chinese residents"
        ],
        adequacy_status="No EU adequacy decision",
        transfer_mechanisms=[
            "CAC approval", "Certification", "Standard contracts", "Other measures"
        ],
        key_principles=[
            "legitimacy", "proportionality", "necessity", "good_faith", 
            "transparency", "purpose_limitation"
        ],
        penalties="Up to RMB 50 million or 5% of previous year's turnover",
        data_subject_rights=[
            "right_to_know", "right_to_decision", "right_to_access", 
            "right_to_rectification", "right_to_deletion"
        ],
        financial_sector_specifics=[
            "PBOC requirements", "Banking regulations", "Securities law compliance"
        ]
    )
}

# Global jurisdictions that may be affected by GDPR/privacy laws
GLOBAL_JURISDICTIONS = [
    # Europe
    "Austria", "Belgium", "Bulgaria", "Croatia", "Cyprus", "Czech Republic", 
    "Denmark", "Estonia", "Finland", "France", "Germany", "Greece", "Hungary", 
    "Ireland", "Italy", "Latvia", "Lithuania", "Luxembourg", "Malta", 
    "Netherlands", "Poland", "Portugal", "Romania", "Slovakia", "Slovenia", 
    "Spain", "Sweden", "United Kingdom", "Norway", "Iceland", "Switzerland",
    
    # Americas
    "United States", "Canada", "Brazil", "Mexico", "Argentina", "Chile", 
    "Colombia", "Peru", "Uruguay",
    
    # Asia-Pacific
    "China", "Japan", "South Korea", "Singapore", "Australia", "New Zealand", 
    "India", "Hong Kong", "Taiwan", "Malaysia", "Thailand", "Philippines", 
    "Indonesia", "Vietnam",
    
    # Middle East & Africa
    "Israel", "United Arab Emirates", "Saudi Arabia", "Qatar", "South Africa", 
    "Nigeria", "Kenya", "Egypt", "Morocco"
]

@dataclass
class ConceptSynonym:
    primary_term: str
    synonyms: List[str]
    context: str
    confidence: float
    jurisdictional_variants: Dict[str, List[str]]
    financial_sector_terms: List[str]

class RopaMetamodelState(TypedDict):
    """Enhanced state for RoPA metamodel generation"""
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Dict[str, Any]]
    extracted_concepts: List[Dict[str, Any]]
    generated_synonyms: List[ConceptSynonym]
    territorial_analysis: Dict[str, Any]
    jurisdictional_mappings: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    cross_border_transfers: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    metamodel_structure: Dict[str, Any]
    compliance_gaps: List[Dict[str, Any]]
    reasoning_trace: List[str]

class CustomOpenAIEmbeddings(Embeddings):
    """Enhanced OpenAI Embeddings with financial/regulatory domain optimization"""
    
    def __init__(self, 
                 model: str = "text-embedding-3-large",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 dimensions: Optional[int] = 3072,
                 max_chunk_size: int = 8000):
        
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = OpenAI(**client_kwargs)
        self.model = model
        self.dimensions = dimensions
        self.max_chunk_size = max_chunk_size
        
        logger.info(f"Initialized CustomOpenAIEmbeddings with model: {model}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed documents with regulatory domain awareness"""
        all_embeddings = []
        
        for text in texts:
            chunks = self._chunk_text_by_characters(text)
            
            if len(chunks) == 1:
                embedding = self._get_single_embedding(chunks[0])
                all_embeddings.append(embedding)
            else:
                # Weighted averaging for multi-chunk documents
                chunk_embeddings = []
                weights = []
                
                for chunk in chunks:
                    chunk_embedding = self._get_single_embedding(chunk)
                    chunk_embeddings.append(chunk_embedding)
                    
                    # Weight regulatory content higher
                    weight = self._calculate_regulatory_weight(chunk)
                    weights.append(weight)
                
                # Compute weighted average
                total_weight = sum(weights)
                if total_weight > 0:
                    avg_embedding = [
                        sum(emb[i] * weights[j] for j, emb in enumerate(chunk_embeddings)) / total_weight
                        for i in range(len(chunk_embeddings[0]))
                    ]
                else:
                    avg_embedding = chunk_embeddings[0]
                
                all_embeddings.append(avg_embedding)
        
        return all_embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query with regulatory context"""
        chunks = self._chunk_text_by_characters(text)
        return self._get_single_embedding(chunks[0])
    
    def _chunk_text_by_characters(self, text: str) -> List[str]:
        """Intelligent chunking for regulatory documents"""
        if len(text) <= self.max_chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chunk_size
            
            if end < len(text):
                # Look for natural break points in regulatory documents
                break_points = [
                    ('Article ', -50), ('Section ', -30), ('Clause ', -20),
                    ('.\n\n', 0), ('.\n', 0), ('. ', 0), ('\n', 0), (' ', 0)
                ]
                
                chunk_end = end
                for break_point, offset in break_points:
                    pos = text.rfind(break_point, start, end + offset)
                    if pos > start:
                        chunk_end = pos + len(break_point)
                        break
                
                end = chunk_end
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def _calculate_regulatory_weight(self, text: str) -> float:
        """Calculate weight based on regulatory content density"""
        regulatory_terms = [
            'gdpr', 'data protection', 'privacy', 'personal data', 'processing',
            'controller', 'processor', 'consent', 'lawful basis', 'legitimate interest',
            'data subject', 'supervisory authority', 'compliance', 'breach',
            'retention', 'security', 'accountability', 'transparency', 'transfer',
            'adequacy', 'safeguards', 'article', 'regulation', 'directive'
        ]
        
        text_lower = text.lower()
        matches = sum(1 for term in regulatory_terms if term in text_lower)
        word_count = len(text.split())
        
        base_weight = 1.0
        if word_count > 0:
            density = matches / word_count
            base_weight += min(density * 5, 2.0)  # Cap at 3.0 total weight
        
        return base_weight
    
    def _get_single_embedding(self, text: str) -> List[float]:
        """Get embedding for single text chunk"""
        try:
            params = {"input": text, "model": self.model}
            if self.dimensions:
                params["dimensions"] = self.dimensions
            
            response = self.client.embeddings.create(**params)
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to get embedding: {e}")
            raise

class EnhancedRegulatoryProcessor:
    """Enhanced processor for regulatory and financial documents"""
    
    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 300):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n", "\n", "Article ", "Section ", "Clause ", 
                "Chapter ", "Part ", "Schedule ", "Annex ",
                ".", "!", "?", ";", ":", " "
            ]
        )
        
        # Enhanced regulatory patterns for global compliance
        self.regulatory_patterns = {
            'article_references': r'Article\s+(\d+(?:\.\d+)*)',
            'section_references': r'Section\s+(\d+(?:\.\d+)*)',
            'gdpr_articles': r'Article\s+(6|7|8|9|10|13|14|15|16|17|18|19|20|21|22|30|32|35|44|45|46|49)',
            'legal_bases': r'(Article\s+6\s*\([a-f]\)|legitimate\s+interest|consent|contract|legal\s+obligation|vital\s+interest|public\s+task)',
            'data_categories': r'(personal\s+data|special\s+categor(?:y|ies)|biometric|health|financial|location|identifier|genetic|political|religious)',
            'processing_purposes': r'(processing\s+(?:for|purpose)|purpose(?:s)?\s+of|lawful\s+basis)',
            'retention_periods': r'(\d+\s+(?:days?|months?|years?)|retention\s+period|storage\s+limitation)',
            'cross_border_transfers': r'(third\s+countr(?:y|ies)|international\s+transfer|adequacy\s+decision|standard\s+contractual\s+clauses)',
            'data_subject_rights': r'(right\s+to\s+(?:access|rectification|erasure|portability|restriction|object)|data\s+subject\s+rights)',
            'supervisory_authorities': r'(supervisory\s+authority|data\s+protection\s+authority|ICO|CNIL|EDPB)',
            'financial_regulations': r'(Basel\s+III|MiFID|PSD2|AML|KYC|SWIFT|correspondent\s+banking)',
            'jurisdiction_references': r'(EU|UK|USA|China|Canada|Australia|Singapore|Hong Kong)',
        }
    
    def extract_pdf_content(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract and analyze PDF content with enhanced regulatory processing"""
        logger.info(f"Processing regulatory document: {pdf_path}")
        
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            metadata = {
                "pages": len(doc), 
                "document_type": self._detect_document_type(pdf_path),
                "source_file": os.path.basename(pdf_path)
            }
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n{page_text}"
            
            doc.close()
            
            # Enhanced chunking with regulatory awareness
            text_chunks = self.text_splitter.split_text(full_text)
            
            chunks = []
            for i, chunk_text in enumerate(text_chunks):
                # Extract regulatory patterns
                regulatory_matches = self._extract_regulatory_patterns(chunk_text)
                
                # Calculate regulatory relevance score
                relevance_score = self._calculate_relevance_score(chunk_text, regulatory_matches)
                
                chunk = {
                    "chunk_id": f"{Path(pdf_path).stem}_chunk_{i}",
                    "text": chunk_text,
                    "chunk_index": i,
                    "source": pdf_path,
                    "document_type": metadata["document_type"],
                    "regulatory_patterns": regulatory_matches,
                    "metadata": {
                        "word_count": len(chunk_text.split()),
                        "char_count": len(chunk_text),
                        "relevance_score": relevance_score,
                        "contains_articles": len(regulatory_matches.get("article_references", [])) > 0,
                        "contains_legal_bases": len(regulatory_matches.get("legal_bases", [])) > 0,
                        "jurisdiction_mentions": regulatory_matches.get("jurisdiction_references", []),
                        "financial_relevance": len(regulatory_matches.get("financial_regulations", [])) > 0
                    }
                }
                chunks.append(chunk)
            
            logger.info(f"Created {len(chunks)} enhanced regulatory chunks from {pdf_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process PDF {pdf_path}: {e}")
            raise
    
    def _detect_document_type(self, pdf_path: str) -> str:
        """Enhanced document type detection"""
        filename = os.path.basename(pdf_path).lower()
        
        type_patterns = {
            "regulation": ["gdpr", "regulation", "directive", "law", "act"],
            "policy": ["policy", "procedure", "guideline", "framework"],
            "compliance": ["compliance", "audit", "assessment", "report"],
            "business_process": ["business", "process", "workflow", "procedure"],
            "financial": ["financial", "banking", "payment", "basel", "mifid"],
            "contract": ["contract", "agreement", "terms", "conditions"],
            "guidance": ["guidance", "guidance", "interpretation", "opinion"]
        }
        
        for doc_type, patterns in type_patterns.items():
            if any(pattern in filename for pattern in patterns):
                return doc_type
        
        return "general"
    
    def _extract_regulatory_patterns(self, text: str) -> Dict[str, List[str]]:
        """Enhanced regulatory pattern extraction"""
        matches = {}
        
        for pattern_name, pattern in self.regulatory_patterns.items():
            found_matches = re.findall(pattern, text, re.IGNORECASE)
            # Flatten tuples and remove duplicates
            if found_matches:
                if isinstance(found_matches[0], tuple):
                    matches[pattern_name] = list(set([match for match_tuple in found_matches for match in match_tuple if match]))
                else:
                    matches[pattern_name] = list(set(found_matches))
            else:
                matches[pattern_name] = []
        
        return matches
    
    def _calculate_relevance_score(self, text: str, patterns: Dict[str, List[str]]) -> float:
        """Calculate regulatory relevance score"""
        score = 0.0
        
        # Base score from pattern matches
        weights = {
            'gdpr_articles': 3.0,
            'article_references': 2.0,
            'legal_bases': 2.5,
            'data_categories': 2.0,
            'cross_border_transfers': 2.5,
            'data_subject_rights': 2.0,
            'financial_regulations': 2.0,
            'jurisdiction_references': 1.5
        }
        
        for pattern, weight in weights.items():
            matches = patterns.get(pattern, [])
            score += len(matches) * weight
        
        # Normalize by text length
        word_count = len(text.split())
        if word_count > 0:
            score = score / word_count * 100  # Scale to percentage-like score
        
        return min(score, 10.0)  # Cap at 10.0

@tool
def enhanced_synonym_generation_agent(concept_text: str, context: str = "") -> Dict[str, Any]:
    """Enhanced agent for generating comprehensive synonyms using LLM reasoning"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""As an expert in global data protection and privacy laws, generate comprehensive synonyms and related terms for the following regulatory concept:

Concept: {concept_text}
Context: {context}

Generate synonyms considering:
1. GDPR and EU terminology
2. UK GDPR variations post-Brexit
3. US privacy law terminology (CCPA, GLBA, FCRA)
4. International privacy frameworks (APEC CBPR, PIPEDA, LGPD, PIPL)
5. Financial sector specific terminology
6. Cross-jurisdictional variations
7. Technical and legal variations
8. Industry-specific adaptations

For each primary concept, provide:
- Direct synonyms and abbreviations
- Jurisdictional variations (how the same concept is referred to in different jurisdictions)
- Financial sector specific terms
- Technical variations
- Related concepts that are often used interchangeably

Return detailed JSON with comprehensive synonym mappings:

{{
    "primary_concept": "{concept_text}",
    "direct_synonyms": ["synonym1", "synonym2", "..."],
    "abbreviations": ["abbrev1", "abbrev2", "..."],
    "jurisdictional_variants": {{
        "eu_gdpr": ["term1", "term2"],
        "uk_gdpr": ["term1", "term2"],
        "us_ccpa": ["term1", "term2"],
        "canada_pipeda": ["term1", "term2"],
        "brazil_lgpd": ["term1", "term2"],
        "china_pipl": ["term1", "term2"],
        "singapore_pdpa": ["term1", "term2"],
        "australia_privacy_act": ["term1", "term2"]
    }},
    "financial_sector_terms": ["banking_term1", "insurance_term2", "investment_term3"],
    "technical_variations": ["tech_term1", "tech_term2"],
    "related_concepts": ["related1", "related2"],
    "acronyms": ["ACRONYM1", "ACRONYM2"],
    "formal_legal_terms": ["formal_term1", "formal_term2"],
    "informal_business_terms": ["business_term1", "business_term2"],
    "cross_reference_terms": ["cross_ref1", "cross_ref2"],
    "contextual_usage": {{
        "data_processing": ["usage_term1", "usage_term2"],
        "compliance": ["compliance_term1", "compliance_term2"],
        "audit": ["audit_term1", "audit_term2"],
        "breach_management": ["breach_term1", "breach_term2"]
    }},
    "confidence_score": 0.95,
    "reasoning": "Detailed explanation of why these synonyms were chosen and their contextual relevance"
}}

Focus on creating the most comprehensive synonym map possible for global GDPR and privacy compliance."""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        # Extract JSON from response
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate and ensure all required fields exist
            required_fields = [
                "primary_concept", "direct_synonyms", "abbreviations", 
                "jurisdictional_variants", "financial_sector_terms", 
                "technical_variations", "related_concepts", "confidence_score"
            ]
            
            for field in required_fields:
                if field not in result:
                    if field == "confidence_score":
                        result[field] = 0.8
                    elif field == "jurisdictional_variants":
                        result[field] = {}
                    else:
                        result[field] = []
            
            return result
        else:
            logger.warning(f"Could not extract JSON from synonym generation response for: {concept_text}")
            return {
                "primary_concept": concept_text,
                "direct_synonyms": [],
                "abbreviations": [],
                "jurisdictional_variants": {},
                "financial_sector_terms": [],
                "technical_variations": [],
                "related_concepts": [],
                "confidence_score": 0.0,
                "reasoning": "Failed to parse LLM response"
            }
    
    except Exception as e:
        logger.error(f"Synonym generation failed for '{concept_text}': {e}")
        return {
            "primary_concept": concept_text,
            "direct_synonyms": [],
            "abbreviations": [],
            "jurisdictional_variants": {},
            "financial_sector_terms": [],
            "technical_variations": [],
            "related_concepts": [],
            "confidence_score": 0.0,
            "reasoning": f"Error during generation: {str(e)}"
        }

@tool
def comprehensive_regulatory_extraction_agent(text: str) -> Dict[str, Any]:
    """Comprehensive agent for extracting GDPR and global privacy regulatory concepts"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze this regulatory/financial document text to extract comprehensive GDPR and global privacy compliance information:

Text: {text}

Extract and analyze:

1. TERRITORIAL SCOPE AND JURISDICTIONAL ANALYSIS:
   - Which jurisdictions are mentioned or affected
   - Cross-border processing scenarios
   - Extraterritorial application triggers
   - Adequacy decisions and transfer mechanisms

2. PROCESSING ACTIVITIES:
   - Specific data processing operations
   - Business processes involving personal data
   - Automated decision-making
   - Profiling activities

3. DATA CATEGORIES AND CLASSIFICATION:
   - Types of personal data processed
   - Special categories of data
   - Financial data categories
   - Sensitivity classifications

4. LEGAL BASES AND COMPLIANCE FRAMEWORKS:
   - Lawful bases for processing
   - Consent mechanisms
   - Legitimate interests assessments
   - Compliance obligations

5. ORGANIZATIONAL ENTITIES AND ROLES:
   - Data controllers and processors
   - Joint controllers
   - Data Protection Officers
   - Supervisory authorities

6. SECURITY AND RISK MANAGEMENT:
   - Technical and organizational measures
   - Security safeguards
   - Risk assessment procedures
   - Breach management

7. CROSS-BORDER TRANSFERS:
   - International data transfers
   - Transfer mechanisms and safeguards
   - Third country considerations
   - Adequacy assessments

8. FINANCIAL SECTOR SPECIFICS:
   - Banking and payment processing
   - Investment and trading data
   - Insurance and underwriting
   - Regulatory reporting

Return comprehensive JSON analysis:

{{
    "territorial_analysis": {{
        "affected_jurisdictions": [
            {{
                "jurisdiction": "jurisdiction_name",
                "applicability_basis": "establishment|targeting|monitoring|other",
                "compliance_requirements": ["requirement1", "requirement2"],
                "transfer_implications": "implications_description"
            }}
        ],
        "cross_border_scenarios": ["scenario1", "scenario2"],
        "adequacy_considerations": ["consideration1", "consideration2"]
    }},
    "processing_activities": [
        {{
            "activity_name": "processing_activity",
            "purpose": "purpose_description",
            "data_categories": ["category1", "category2"],
            "legal_basis": "legal_basis_type",
            "retention_period": "retention_description",
            "automated_processing": true/false,
            "jurisdictional_scope": ["jurisdiction1", "jurisdiction2"]
        }}
    ],
    "data_categories": [
        {{
            "category": "data_category_name",
            "sensitivity": "normal|special|financial|restricted",
            "examples": ["example1", "example2"],
            "protection_requirements": ["requirement1", "requirement2"],
            "cross_border_restrictions": ["restriction1", "restriction2"]
        }}
    ],
    "legal_bases": [
        {{
            "basis_type": "consent|contract|legal_obligation|vital_interests|public_task|legitimate_interests",
            "description": "basis_description",
            "conditions": ["condition1", "condition2"],
            "jurisdictional_variations": ["variation1", "variation2"]
        }}
    ],
    "organizational_entities": [
        {{
            "entity_name": "entity_name",
            "role": "controller|processor|joint_controller|dpo|authority",
            "responsibilities": ["responsibility1", "responsibility2"],
            "jurisdictional_scope": ["jurisdiction1", "jurisdiction2"]
        }}
    ],
    "security_measures": [
        {{
            "measure_type": "technical|organizational|administrative",
            "description": "measure_description",
            "implementation_requirements": ["requirement1", "requirement2"],
            "compliance_standards": ["standard1", "standard2"]
        }}
    ],
    "cross_border_transfers": [
        {{
            "origin": "origin_jurisdiction",
            "destination": "destination_jurisdiction",
            "transfer_mechanism": "adequacy|scc|bcr|derogation|certification",
            "safeguards": ["safeguard1", "safeguard2"],
            "risk_assessment": "risk_level"
        }}
    ],
    "financial_specifics": [
        {{
            "area": "banking|insurance|investment|payments",
            "regulations": ["regulation1", "regulation2"],
            "data_types": ["type1", "type2"],
            "compliance_requirements": ["requirement1", "requirement2"]
        }}
    ],
    "key_concepts": [
        {{
            "concept": "concept_name",
            "definition": "concept_definition",
            "regulatory_context": "context_description",
            "importance": "high|medium|low"
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Validate required fields
            required_fields = [
                "territorial_analysis", "processing_activities", "data_categories",
                "legal_bases", "organizational_entities", "security_measures",
                "cross_border_transfers", "financial_specifics", "key_concepts"
            ]
            
            for field in required_fields:
                if field not in result:
                    if field == "territorial_analysis":
                        result[field] = {"affected_jurisdictions": [], "cross_border_scenarios": [], "adequacy_considerations": []}
                    else:
                        result[field] = []
            
            return result
        else:
            return {field: [] if field != "territorial_analysis" else {"affected_jurisdictions": [], "cross_border_scenarios": [], "adequacy_considerations": []} for field in [
                "territorial_analysis", "processing_activities", "data_categories",
                "legal_bases", "organizational_entities", "security_measures",
                "cross_border_transfers", "financial_specifics", "key_concepts"
            ]}
    
    except Exception as e:
        logger.error(f"Comprehensive regulatory extraction failed: {e}")
        return {field: [] if field != "territorial_analysis" else {"affected_jurisdictions": [], "cross_border_scenarios": [], "adequacy_considerations": []} for field in [
            "territorial_analysis", "processing_activities", "data_categories",
            "legal_bases", "organizational_entities", "security_measures",
            "cross_border_transfers", "financial_specifics", "key_concepts"
        ]}

@tool
def metamodel_generation_agent(consolidated_data: str) -> Dict[str, Any]:
    """Agent for generating comprehensive metamodel from consolidated analysis"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Based on comprehensive analysis of GDPR and global privacy regulatory documents, create a detailed metamodel for Record of Processing Activities (RoPA) that supports global jurisdictional compliance for financial institutions.

Consolidated Analysis Data: {consolidated_data}

Create a metamodel that includes:

1. CORE METAMODEL ENTITIES:
   - Abstract base classes for universal concepts
   - Concrete implementation classes for specific jurisdictions
   - Inheritance hierarchies and polymorphism
   - Relationship definitions and cardinalities

2. JURISDICTIONAL ADAPTATION FRAMEWORK:
   - Template patterns for jurisdiction-specific extensions
   - Configuration mechanisms for different privacy laws
   - Validation rules per jurisdiction
   - Compliance mapping matrices

3. FINANCIAL SECTOR SPECIALIZATIONS:
   - Banking-specific processing activities
   - Insurance data handling requirements
   - Investment management compliance
   - Payment processing frameworks
   - Cross-border financial services

4. IMPLEMENTATION ARCHITECTURE:
   - Data models and schemas
   - API specifications
   - Integration patterns
   - Validation frameworks

Return comprehensive metamodel specification:

{{
    "metamodel_specification": {{
        "version": "2.0.0",
        "scope": "Global GDPR and Privacy Compliance for Financial Institutions",
        "core_entities": [
            {{
                "entity_name": "ProcessingActivity",
                "entity_type": "abstract|concrete",
                "attributes": [
                    {{
                        "name": "attribute_name",
                        "type": "string|integer|boolean|date|array|object",
                        "required": true/false,
                        "jurisdiction_specific": true/false,
                        "validation_rules": ["rule1", "rule2"]
                    }}
                ],
                "relationships": [
                    {{
                        "target_entity": "target_entity_name",
                        "relationship_type": "one-to-one|one-to-many|many-to-many",
                        "cardinality": "0..1|1..1|0..*|1..*",
                        "constraints": ["constraint1", "constraint2"]
                    }}
                ],
                "jurisdiction_extensions": {{
                    "gdpr_eu": ["extension1", "extension2"],
                    "uk_gdpr": ["extension1", "extension2"],
                    "ccpa": ["extension1", "extension2"]
                }}
            }}
        ],
        "jurisdictional_frameworks": [
            {{
                "jurisdiction": "jurisdiction_code",
                "framework_name": "framework_name",
                "mandatory_attributes": ["attr1", "attr2"],
                "optional_attributes": ["attr3", "attr4"],
                "validation_rules": ["rule1", "rule2"],
                "compliance_requirements": ["req1", "req2"]
            }}
        ],
        "financial_specializations": [
            {{
                "sector": "banking|insurance|investment|payments",
                "specialized_entities": ["entity1", "entity2"],
                "additional_attributes": ["attr1", "attr2"],
                "compliance_frameworks": ["framework1", "framework2"],
                "reporting_requirements": ["req1", "req2"]
            }}
        ],
        "cross_border_framework": {{
            "transfer_entities": ["entity1", "entity2"],
            "adequacy_mapping": {{
                "adequate_countries": ["country1", "country2"],
                "transfer_mechanisms": ["mechanism1", "mechanism2"]
            }},
            "risk_assessment_framework": ["factor1", "factor2"]
        }}
    }},
    "implementation_guidance": {{
        "data_models": [
            {{
                "model_name": "model_name",
                "schema_definition": {{}},
                "validation_rules": ["rule1", "rule2"],
                "persistence_requirements": ["req1", "req2"]
            }}
        ],
        "api_specifications": [
            {{
                "endpoint": "endpoint_path",
                "method": "GET|POST|PUT|DELETE",
                "parameters": ["param1", "param2"],
                "response_format": "response_description"
            }}
        ],
        "integration_patterns": [
            {{
                "pattern_name": "pattern_name",
                "use_case": "use_case_description",
                "implementation_steps": ["step1", "step2"],
                "best_practices": ["practice1", "practice2"]
            }}
        ]
    }},
    "compliance_validation": {{
        "validation_rules": [
            {{
                "rule_name": "rule_name",
                "jurisdiction": "jurisdiction_code",
                "validation_logic": "validation_description",
                "error_messages": ["message1", "message2"]
            }}
        ],
        "compliance_matrices": [
            {{
                "framework": "framework_name",
                "requirements": ["req1", "req2"],
                "implementation_mapping": {{
                    "req1": "implementation_approach",
                    "req2": "implementation_approach"
                }}
            }}
        ]
    }}
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {"error": "Failed to parse metamodel specification"}
    
    except Exception as e:
        logger.error(f"Metamodel generation failed: {e}")
        return {"error": f"Metamodel generation failed: {str(e)}"}

class EnhancedVectorEngine:
    """Enhanced vector engine with AI-powered synonym storage"""
    
    def __init__(self, 
                 host: str = "http://localhost:9200",
                 index_name: str = "enhanced_gdpr_ropa_metamodel",
                 username: str = None,
                 password: str = None,
                 ca_certs: str = None,
                 verify_certs: bool = True,
                 openai_api_key: str = None,
                 openai_base_url: str = None):
        
        self.index_name = index_name
        self.synonym_index = f"{index_name}_synonyms"
        
        # Initialize embeddings
        self.embeddings = CustomOpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=openai_api_key,
            base_url=openai_base_url,
            dimensions=3072
        )
        
        # Configure Elasticsearch client
        self.client = self._create_elasticsearch_client(
            host, username, password, ca_certs, verify_certs
        )
        
        self._create_enhanced_indices()
    
    def _create_elasticsearch_client(self, host, username, password, ca_certs, verify_certs):
        """Create Elasticsearch client with enhanced configuration"""
        if not host.startswith(('http://', 'https://')):
            raise ValueError(f"Elasticsearch host must include schema. Got: {host}")
        
        client_config = {
            "hosts": [host],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        # Configure SSL/TLS
        if host.startswith('https://'):
            client_config["use_ssl"] = True
            client_config["verify_certs"] = verify_certs
            
            if ca_certs:
                client_config["ca_certs"] = ca_certs
                logger.info(f"Using CA certificate: {ca_certs}")
            
            if not verify_certs:
                client_config["ssl_show_warn"] = False
                logger.warning("SSL certificate verification disabled")
        
        # Configure authentication
        if username and password:
            client_config["basic_auth"] = (username, password)
            logger.info(f"Using basic authentication for user: {username}")
        elif username or password:
            logger.warning("Both username and password must be provided for authentication")
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                info = client.info()
                logger.info(f"Elasticsearch version: {info.body['version']['number']}")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch cluster")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_enhanced_indices(self):
        """Create enhanced indices for documents and synonyms"""
        
        # Main document index
        doc_mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "regulatory_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer",
                                "keyword_repeat",
                                "remove_duplicates"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "regulatory_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "document_type": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "relevance_score": {"type": "float"},
                    
                    # Regulatory extractions
                    "territorial_analysis": {"type": "nested"},
                    "processing_activities": {"type": "nested"},
                    "data_categories": {"type": "nested"},
                    "legal_bases": {"type": "nested"},
                    "organizational_entities": {"type": "nested"},
                    "security_measures": {"type": "nested"},
                    "cross_border_transfers": {"type": "nested"},
                    "financial_specifics": {"type": "nested"},
                    "key_concepts": {"type": "nested"},
                    
                    # Metadata
                    "regulatory_patterns": {"type": "object"},
                    "metadata": {"type": "object"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        # Synonym index
        synonym_mapping = {
            "mappings": {
                "properties": {
                    "primary_concept": {"type": "keyword"},
                    "direct_synonyms": {"type": "keyword"},
                    "abbreviations": {"type": "keyword"},
                    "jurisdictional_variants": {
                        "type": "object",
                        "properties": {
                            "eu_gdpr": {"type": "keyword"},
                            "uk_gdpr": {"type": "keyword"},
                            "us_ccpa": {"type": "keyword"},
                            "canada_pipeda": {"type": "keyword"},
                            "brazil_lgpd": {"type": "keyword"},
                            "china_pipl": {"type": "keyword"}
                        }
                    },
                    "financial_sector_terms": {"type": "keyword"},
                    "technical_variations": {"type": "keyword"},
                    "related_concepts": {"type": "keyword"},
                    "confidence_score": {"type": "float"},
                    "context": {"type": "text"},
                    "reasoning": {"type": "text"},
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            # Create main index
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **doc_mapping)
                logger.info(f"Created document index: {self.index_name}")
            
            # Create synonym index
            if not self.client.indices.exists(index=self.synonym_index):
                self.client.indices.create(index=self.synonym_index, **synonym_mapping)
                logger.info(f"Created synonym index: {self.synonym_index}")
        
        except Exception as e:
            logger.error(f"Failed to create indices: {e}")
            raise
    
    def generate_and_store_synonyms(self, extracted_concepts: List[Dict[str, Any]]) -> List[ConceptSynonym]:
        """Generate synonyms using LLM and store them in Elasticsearch"""
        logger.info("Generating and storing AI-powered synonyms...")
        
        synonyms = []
        processed_concepts = set()
        
        for extraction in extracted_concepts:
            # Extract unique concepts from various fields
            concepts_to_process = []
            
            # From key concepts
            for concept in extraction.get("key_concepts", []):
                concept_name = concept.get("concept", "").strip()
                if concept_name and concept_name not in processed_concepts:
                    concepts_to_process.append((concept_name, concept.get("definition", "")))
            
            # From processing activities
            for activity in extraction.get("processing_activities", []):
                activity_name = activity.get("activity_name", "").strip()
                if activity_name and activity_name not in processed_concepts:
                    concepts_to_process.append((activity_name, activity.get("purpose", "")))
            
            # From data categories
            for category in extraction.get("data_categories", []):
                category_name = category.get("category", "").strip()
                if category_name and category_name not in processed_concepts:
                    concepts_to_process.append((category_name, ", ".join(category.get("examples", []))))
            
            # Generate synonyms for each concept
            for concept_name, context in concepts_to_process:
                if concept_name in processed_concepts:
                    continue
                
                try:
                    # Generate synonyms using LLM
                    synonym_data = enhanced_synonym_generation_agent.invoke(concept_name, context)
                    
                    # Create ConceptSynonym object
                    concept_synonym = ConceptSynonym(
                        primary_term=concept_name,
                        synonyms=synonym_data.get("direct_synonyms", []) + synonym_data.get("abbreviations", []),
                        context=context,
                        confidence=synonym_data.get("confidence_score", 0.8),
                        jurisdictional_variants=synonym_data.get("jurisdictional_variants", {}),
                        financial_sector_terms=synonym_data.get("financial_sector_terms", [])
                    )
                    
                    synonyms.append(concept_synonym)
                    processed_concepts.add(concept_name)
                    
                    # Store in Elasticsearch
                    self._store_synonym_in_elasticsearch(synonym_data)
                    
                    logger.info(f"Generated synonyms for: {concept_name}")
                
                except Exception as e:
                    logger.error(f"Failed to generate synonyms for '{concept_name}': {e}")
                    continue
        
        logger.info(f"Generated and stored {len(synonyms)} concept synonyms")
        return synonyms
    
    def _store_synonym_in_elasticsearch(self, synonym_data: Dict[str, Any]):
        """Store synonym data in Elasticsearch"""
        try:
            doc = {
                **synonym_data,
                "timestamp": datetime.now()
            }
            
            self.client.index(
                index=self.synonym_index,
                id=f"synonym_{synonym_data['primary_concept']}",
                document=doc
            )
        except Exception as e:
            logger.error(f"Failed to store synonym: {e}")
    
    def index_documents_with_extractions(self, chunks: List[Dict[str, Any]], extractions: List[Dict[str, Any]]):
        """Index documents with comprehensive extractions"""
        logger.info("Indexing documents with comprehensive extractions...")
        
        for i, chunk in enumerate(chunks):
            extraction = extractions[i] if i < len(extractions) else {}
            
            # Generate embedding
            embedding = self.embeddings.embed_query(chunk["text"])
            
            doc = {
                "text": chunk["text"],
                "embedding": embedding,
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "document_type": chunk.get("document_type", "general"),
                "relevance_score": chunk.get("metadata", {}).get("relevance_score", 0.0),
                
                # Store all extractions
                "territorial_analysis": extraction.get("territorial_analysis", {}),
                "processing_activities": extraction.get("processing_activities", []),
                "data_categories": extraction.get("data_categories", []),
                "legal_bases": extraction.get("legal_bases", []),
                "organizational_entities": extraction.get("organizational_entities", []),
                "security_measures": extraction.get("security_measures", []),
                "cross_border_transfers": extraction.get("cross_border_transfers", []),
                "financial_specifics": extraction.get("financial_specifics", []),
                "key_concepts": extraction.get("key_concepts", []),
                
                "regulatory_patterns": chunk.get("regulatory_patterns", {}),
                "metadata": chunk.get("metadata", {}),
                "timestamp": datetime.now()
            }
            
            try:
                self.client.index(index=self.index_name, id=chunk["chunk_id"], document=doc)
            except Exception as e:
                logger.error(f"Failed to index chunk {chunk['chunk_id']}: {e}")
        
        self.client.indices.refresh(index=self.index_name)
        logger.info(f"Successfully indexed {len(chunks)} documents")
    
    def search_with_synonym_expansion(self, query: str, filters: Dict[str, Any] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Search with automatic synonym expansion"""
        # Get synonyms for query terms
        expanded_query = self._expand_query_with_synonyms(query)
        
        # Generate query embedding
        query_embedding = self.embeddings.embed_query(query)
        
        # Build search query
        must_clauses = [
            {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            }
        ]
        
        should_clauses = [
            {
                "multi_match": {
                    "query": expanded_query,
                    "fields": [
                        "text^1.0",
                        "processing_activities.activity_name^2.0",
                        "data_categories.category^1.8",
                        "key_concepts.concept^2.5",
                        "territorial_analysis.affected_jurisdictions.jurisdiction^1.5",
                        "cross_border_transfers.origin^1.2",
                        "cross_border_transfers.destination^1.2"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            }
        ]
        
        # Add filters if provided
        filter_clauses = []
        if filters:
            for field, value in filters.items():
                if isinstance(value, list):
                    filter_clauses.append({"terms": {field: value}})
                else:
                    filter_clauses.append({"term": {field: value}})
        
        search_body = {
            "query": {
                "bool": {
                    "must": must_clauses,
                    "should": should_clauses,
                    "filter": filter_clauses,
                    "minimum_should_match": 1
                }
            },
            "size": top_k,
            "_source": {
                "excludes": ["embedding"]
            },
            "sort": [
                {"_score": {"order": "desc"}},
                {"relevance_score": {"order": "desc"}}
            ]
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_search_results(response)
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
    
    def _expand_query_with_synonyms(self, query: str) -> str:
        """Expand query with stored synonyms"""
        try:
            # Search for synonyms
            synonym_search = {
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"primary_concept": query}},
                            {"match": {"direct_synonyms": query}},
                            {"match": {"abbreviations": query}}
                        ]
                    }
                },
                "size": 10
            }
            
            response = self.client.search(index=self.synonym_index, **synonym_search)
            
            expanded_terms = [query]
            
            if hasattr(response, 'body'):
                hits = response.body["hits"]["hits"]
            else:
                hits = response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                expanded_terms.extend(source.get("direct_synonyms", []))
                expanded_terms.extend(source.get("abbreviations", []))
                expanded_terms.extend(source.get("technical_variations", []))
                
                # Add jurisdictional variants
                for jurisdiction, terms in source.get("jurisdictional_variants", {}).items():
                    expanded_terms.extend(terms)
            
            # Remove duplicates and return
            unique_terms = list(set(expanded_terms))
            return " ".join(unique_terms)
        
        except Exception as e:
            logger.warning(f"Synonym expansion failed: {e}")
            return query
    
    def _format_search_results(self, response: Dict) -> List[Dict[str, Any]]:
        """Format search results"""
        results = []
        
        try:
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "text": source["text"],
                    "chunk_id": source["chunk_id"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "document_type": source.get("document_type", "general"),
                    "relevance_score": source.get("relevance_score", 0.0),
                    "territorial_analysis": source.get("territorial_analysis", {}),
                    "processing_activities": source.get("processing_activities", []),
                    "key_concepts": source.get("key_concepts", []),
                    "metadata": source.get("metadata", {})
                }
                results.append(result)
        except Exception as e:
            logger.error(f"Error formatting search results: {e}")
        
        return results

class EnhancedGraphEngine:
    """Enhanced graph engine with synonym integration"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, password: str = None):
        try:
            if password:
                self.db = FalkorDB(host=host, port=port, password=password)
            else:
                self.db = FalkorDB(host=host, port=port)
            
            self.graph = self.db.select_graph("enhanced_gdpr_ropa_graph")
            logger.info(f"Connected to FalkorDB at {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def build_comprehensive_knowledge_graph(self, 
                                          chunks: List[Dict[str, Any]], 
                                          extractions: List[Dict[str, Any]], 
                                          synonyms: List[ConceptSynonym]):
        """Build comprehensive knowledge graph with synonyms"""
        logger.info("Building comprehensive knowledge graph with synonyms...")
        
        try:
            # Clear existing graph
            self.graph.query("MATCH (n) DETACH DELETE n")
            
            # Create jurisdiction nodes
            self._create_jurisdiction_nodes()
            
            # Create synonym nodes and relationships
            self._create_synonym_nodes(synonyms)
            
            # Process document chunks and extractions
            for i, chunk in enumerate(chunks):
                extraction = extractions[i] if i < len(extractions) else {}
                self._process_chunk_extraction(chunk, extraction)
            
            # Create cross-references and advanced relationships
            self._create_advanced_relationships()
            
            logger.info("Knowledge graph built successfully")
        
        except Exception as e:
            logger.error(f"Failed to build knowledge graph: {e}")
            raise
    
    def _create_jurisdiction_nodes(self):
        """Create jurisdiction framework nodes"""
        for jurisdiction_type, framework in JURISDICTION_FRAMEWORKS.items():
            framework_dict = asdict(framework)
            
            # Escape single quotes for Cypher
            for key, value in framework_dict.items():
                if isinstance(value, str):
                    framework_dict[key] = value.replace("'", "\\'")
                elif isinstance(value, list):
                    framework_dict[key] = [v.replace("'", "\\'") if isinstance(v, str) else v for v in value]
            
            query = f"""
            CREATE (j:Jurisdiction:Framework {{
                code: '{framework_dict['code']}',
                name: '{framework_dict['name']}',
                territorial_scope: {json.dumps(framework_dict['territorial_scope'])},
                extraterritorial_triggers: {json.dumps(framework_dict['extraterritorial_triggers'])},
                adequacy_status: '{framework_dict['adequacy_status']}',
                transfer_mechanisms: {json.dumps(framework_dict['transfer_mechanisms'])},
                penalties: '{framework_dict['penalties']}',
                data_subject_rights: {json.dumps(framework_dict['data_subject_rights'])},
                financial_sector_specifics: {json.dumps(framework_dict['financial_sector_specifics'])}
            }})
            """
            self.graph.query(query)
    
    def _create_synonym_nodes(self, synonyms: List[ConceptSynonym]):
        """Create synonym nodes and relationships"""
        for synonym in synonyms:
            # Create primary concept node
            primary_term = synonym.primary_term.replace("'", "\\'")
            context = synonym.context.replace("'", "\\'")
            
            concept_query = f"""
            MERGE (c:Concept {{
                name: '{primary_term}',
                context: '{context}',
                confidence: {synonym.confidence}
            }})
            """
            self.graph.query(concept_query)
            
            # Create synonym nodes and relationships
            all_synonyms = (synonym.synonyms + synonym.financial_sector_terms + 
                          [term for terms in synonym.jurisdictional_variants.values() for term in terms])
            
            for syn in set(all_synonyms):  # Remove duplicates
                if syn and syn != primary_term:
                    syn_escaped = syn.replace("'", "\\'")
                    
                    synonym_query = f"""
                    MERGE (s:Synonym {{name: '{syn_escaped}'}})
                    """
                    self.graph.query(synonym_query)
                    
                    # Create relationship
                    relationship_query = f"""
                    MATCH (c:Concept {{name: '{primary_term}'}})
                    MATCH (s:Synonym {{name: '{syn_escaped}'}})
                    MERGE (c)-[:HAS_SYNONYM]->(s)
                    """
                    self.graph.query(relationship_query)
            
            # Create jurisdictional variant relationships
            for jurisdiction, terms in synonym.jurisdictional_variants.items():
                for term in terms:
                    if term:
                        term_escaped = term.replace("'", "\\'")
                        jurisdiction_escaped = jurisdiction.replace("'", "\\'")
                        
                        variant_query = f"""
                        MERGE (jv:JurisdictionalVariant {{
                            name: '{term_escaped}',
                            jurisdiction: '{jurisdiction_escaped}'
                        }})
                        """
                        self.graph.query(variant_query)
                        
                        # Link to primary concept
                        link_query = f"""
                        MATCH (c:Concept {{name: '{primary_term}'}})
                        MATCH (jv:JurisdictionalVariant {{name: '{term_escaped}', jurisdiction: '{jurisdiction_escaped}'}})
                        MERGE (c)-[:HAS_JURISDICTIONAL_VARIANT]->(jv)
                        """
                        self.graph.query(link_query)
    
    def _process_chunk_extraction(self, chunk: Dict[str, Any], extraction: Dict[str, Any]):
        """Process individual chunk and extraction data"""
        chunk_id = chunk["chunk_id"]
        source = chunk["source"].replace("'", "\\'")
        
        # Create document node
        doc_query = f"""
        MERGE (d:Document {{
            id: '{chunk_id}',
            source: '{source}',
            document_type: '{chunk.get('document_type', 'general')}',
            relevance_score: {chunk.get('metadata', {}).get('relevance_score', 0.0)}
        }})
        """
        self.graph.query(doc_query)
        
        # Process processing activities
        for activity in extraction.get("processing_activities", []):
            self._create_processing_activity_node(activity, chunk_id)
        
        # Process data categories
        for category in extraction.get("data_categories", []):
            self._create_data_category_node(category, chunk_id)
        
        # Process legal bases
        for basis in extraction.get("legal_bases", []):
            self._create_legal_basis_node(basis, chunk_id)
        
        # Process organizational entities
        for entity in extraction.get("organizational_entities", []):
            self._create_organizational_entity_node(entity, chunk_id)
        
        # Process cross-border transfers
        for transfer in extraction.get("cross_border_transfers", []):
            self._create_cross_border_transfer_node(transfer, chunk_id)
    
    def _create_processing_activity_node(self, activity: Dict[str, Any], chunk_id: str):
        """Create processing activity node"""
        activity_name = activity.get("activity_name", "").replace("'", "\\'")
        purpose = activity.get("purpose", "").replace("'", "\\'")
        legal_basis = activity.get("legal_basis", "").replace("'", "\\'")
        
        if activity_name:
            query = f"""
            MERGE (pa:ProcessingActivity {{
                name: '{activity_name}',
                purpose: '{purpose}',
                legal_basis: '{legal_basis}',
                data_categories: {json.dumps(activity.get('data_categories', []))},
                retention_period: '{activity.get('retention_period', '')}',
                automated_processing: {str(activity.get('automated_processing', False)).lower()},
                jurisdictional_scope: {json.dumps(activity.get('jurisdictional_scope', []))}
            }})
            """
            self.graph.query(query)
            
            # Link to document
            link_query = f"""
            MATCH (d:Document {{id: '{chunk_id}'}})
            MATCH (pa:ProcessingActivity {{name: '{activity_name}'}})
            MERGE (d)-[:DESCRIBES]->(pa)
            """
            self.graph.query(link_query)
    
    def _create_data_category_node(self, category: Dict[str, Any], chunk_id: str):
        """Create data category node"""
        category_name = category.get("category", "").replace("'", "\\'")
        sensitivity = category.get("sensitivity", "normal")
        
        if category_name:
            query = f"""
            MERGE (dc:DataCategory {{
                name: '{category_name}',
                sensitivity: '{sensitivity}',
                examples: {json.dumps(category.get('examples', []))},
                protection_requirements: {json.dumps(category.get('protection_requirements', []))},
                cross_border_restrictions: {json.dumps(category.get('cross_border_restrictions', []))}
            }})
            """
            self.graph.query(query)
            
            # Link to document
            link_query = f"""
            MATCH (d:Document {{id: '{chunk_id}'}})
            MATCH (dc:DataCategory {{name: '{category_name}'}})
            MERGE (d)-[:DEFINES]->(dc)
            """
            self.graph.query(link_query)
    
    def _create_legal_basis_node(self, basis: Dict[str, Any], chunk_id: str):
        """Create legal basis node"""
        basis_type = basis.get("basis_type", "").replace("'", "\\'")
        description = basis.get("description", "").replace("'", "\\'")
        
        if basis_type:
            query = f"""
            MERGE (lb:LegalBasis {{
                type: '{basis_type}',
                description: '{description}',
                conditions: {json.dumps(basis.get('conditions', []))},
                jurisdictional_variations: {json.dumps(basis.get('jurisdictional_variations', []))}
            }})
            """
            self.graph.query(query)
    
    def _create_organizational_entity_node(self, entity: Dict[str, Any], chunk_id: str):
        """Create organizational entity node"""
        entity_name = entity.get("entity_name", "").replace("'", "\\'")
        role = entity.get("role", "").replace("'", "\\'")
        
        if entity_name:
            query = f"""
            MERGE (oe:OrganizationalEntity {{
                name: '{entity_name}',
                role: '{role}',
                responsibilities: {json.dumps(entity.get('responsibilities', []))},
                jurisdictional_scope: {json.dumps(entity.get('jurisdictional_scope', []))}
            }})
            """
            self.graph.query(query)
    
    def _create_cross_border_transfer_node(self, transfer: Dict[str, Any], chunk_id: str):
        """Create cross-border transfer node"""
        origin = transfer.get("origin", "").replace("'", "\\'")
        destination = transfer.get("destination", "").replace("'", "\\'")
        mechanism = transfer.get("transfer_mechanism", "").replace("'", "\\'")
        
        if origin and destination:
            query = f"""
            MERGE (cbt:CrossBorderTransfer {{
                origin: '{origin}',
                destination: '{destination}',
                transfer_mechanism: '{mechanism}',
                safeguards: {json.dumps(transfer.get('safeguards', []))},
                risk_assessment: '{transfer.get('risk_assessment', '')}'
            }})
            """
            self.graph.query(query)
    
    def _create_advanced_relationships(self):
        """Create advanced relationships between entities"""
        
        # Link processing activities to data categories
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (dc:DataCategory)
        WHERE pa.name IN pa.data_categories OR dc.name IN pa.data_categories
        MERGE (pa)-[:PROCESSES]->(dc)
        """)
        
        # Link processing activities to legal bases
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (lb:LegalBasis)
        WHERE pa.legal_basis = lb.type
        MERGE (pa)-[:BASED_ON]->(lb)
        """)
        
        # Link entities to jurisdictions
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (j:Jurisdiction)
        WHERE j.code IN pa.jurisdictional_scope
        MERGE (pa)-[:SUBJECT_TO]->(j)
        """)
        
        # Create synonym relationships with concepts
        self.graph.query("""
        MATCH (c:Concept), (pa:ProcessingActivity)
        WHERE toLower(c.name) = toLower(pa.name)
        MERGE (c)-[:REPRESENTS]->(pa)
        """)
        
        self.graph.query("""
        MATCH (c:Concept), (dc:DataCategory)
        WHERE toLower(c.name) = toLower(dc.name)
        MERGE (c)-[:REPRESENTS]->(dc)
        """)
    
    def search_with_synonyms(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """Search graph using synonym expansion"""
        query_terms = self._extract_search_terms(query)
        results = []
        
        for term in query_terms:
            term_escaped = term.replace("'", "\\'")
            
            # Search with synonym expansion
            search_query = f"""
            MATCH path = (start)-[*1..3]-(related)
            WHERE (
                (start:ProcessingActivity AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)<-[:REPRESENTS]-(:Concept)-[:HAS_SYNONYM]->(:Synonym {{name: '{term_escaped}'}}))
                 )
                ) OR
                (start:DataCategory AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)<-[:REPRESENTS]-(:Concept)-[:HAS_SYNONYM]->(:Synonym {{name: '{term_escaped}'}}))
                 )
                ) OR
                (start:Concept AND 
                 (toLower(start.name) CONTAINS '{term_escaped}' OR 
                  EXISTS((start)-[:HAS_SYNONYM]->(:Synonym {{name: '{term_escaped}'}}))
                 )
                )
            )
            RETURN DISTINCT start, related, relationships(path), length(path) as distance
            ORDER BY distance, start.name
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(search_query)
                
                for record in result.result_set:
                    graph_result = {
                        "start_node": self._format_node(record[0]),
                        "related_node": self._format_node(record[1]),
                        "relationships": [str(rel) for rel in record[2]],
                        "distance": record[3],
                        "search_term": term,
                        "search_type": "synonym_enhanced_graph"
                    }
                    results.append(graph_result)
            
            except Exception as e:
                logger.warning(f"Graph search failed for '{term}': {e}")
        
        return results[:top_k]
    
    def _extract_search_terms(self, query: str) -> List[str]:
        """Extract meaningful search terms from query"""
        words = re.findall(r'\b\w+\b', query.lower())
        stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an'}
        return [word for word in words if len(word) > 2 and word not in stopwords]
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if hasattr(node, 'properties'):
            return dict(node.properties)
        return {"id": str(node)}

class GlobalRopaMetamodelSystem:
    """Complete Global GDPR RoPA Metamodel System with Enhanced AI Capabilities"""
    
    def __init__(self,
                 openai_api_key: str,
                 openai_base_url: str = None,
                 elasticsearch_host: str = "http://localhost:9200",
                 elasticsearch_username: str = None,
                 elasticsearch_password: str = None,
                 elasticsearch_ca_certs: str = None,
                 falkordb_host: str = "localhost",
                 falkordb_port: int = 6379,
                 falkordb_password: str = None):
        
        # Set environment variables
        os.environ["OPENAI_API_KEY"] = openai_api_key
        if openai_base_url:
            os.environ["OPENAI_BASE_URL"] = openai_base_url
        
        # Initialize components
        self.processor = EnhancedRegulatoryProcessor()
        
        self.vector_engine = EnhancedVectorEngine(
            host=elasticsearch_host,
            username=elasticsearch_username,
            password=elasticsearch_password,
            ca_certs=elasticsearch_ca_certs,
            openai_api_key=openai_api_key,
            openai_base_url=openai_base_url
        )
        
        self.graph_engine = EnhancedGraphEngine(
            host=falkordb_host,
            port=falkordb_port,
            password=falkordb_password
        )
        
        # Initialize state
        self.system_state = {
            "processed_documents": [],
            "extracted_concepts": [],
            "generated_synonyms": [],
            "metamodel_iterations": [],
            "final_metamodel": None,
            "analysis_statistics": {}
        }
        
        logger.info("Enhanced Global RoPA Metamodel System initialized")
        if openai_base_url:
            logger.info(f"Using custom OpenAI endpoint: {openai_base_url}")
    
    def ingest_documents(self, document_paths: List[str]) -> Dict[str, Any]:
        """Ingest and process regulatory documents with comprehensive analysis"""
        logger.info(f"Ingesting {len(document_paths)} regulatory documents")
        
        all_chunks = []
        all_extractions = []
        
        for doc_path in document_paths:
            try:
                # Process document
                chunks = self.processor.extract_pdf_content(doc_path)
                
                # Extract regulatory concepts from each chunk
                extractions = []
                for chunk in chunks:
                    extraction = comprehensive_regulatory_extraction_agent.invoke(chunk["text"])
                    extractions.append(extraction)
                
                all_chunks.extend(chunks)
                all_extractions.extend(extractions)
                
                self.system_state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "timestamp": datetime.now()
                })
                
                logger.info(f"Processed {doc_path}: {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Store extractions
        self.system_state["extracted_concepts"] = all_extractions
        
        # Generate and store synonyms using AI
        logger.info("Generating AI-powered synonyms...")
        synonyms = self.vector_engine.generate_and_store_synonyms(all_extractions)
        self.system_state["generated_synonyms"] = synonyms
        
        # Index documents with extractions
        self.vector_engine.index_documents_with_extractions(all_chunks, all_extractions)
        
        # Build knowledge graph with synonyms
        self.graph_engine.build_comprehensive_knowledge_graph(all_chunks, all_extractions, synonyms)
        
        # Calculate statistics
        stats = self._calculate_ingestion_statistics(all_chunks, all_extractions, synonyms)
        self.system_state["analysis_statistics"] = stats
        
        return {
            "status": "success",
            "documents_processed": len(document_paths),
            "total_chunks": len(all_chunks),
            "synonyms_generated": len(synonyms),
            **stats
        }
    
    def _calculate_ingestion_statistics(self, chunks: List[Dict], extractions: List[Dict], synonyms: List[ConceptSynonym]) -> Dict[str, Any]:
        """Calculate comprehensive ingestion statistics"""
        
        # Process extractions statistics
        total_processing_activities = sum(len(ext.get("processing_activities", [])) for ext in extractions)
        total_data_categories = sum(len(ext.get("data_categories", [])) for ext in extractions)
        total_legal_bases = sum(len(ext.get("legal_bases", [])) for ext in extractions)
        total_cross_border_transfers = sum(len(ext.get("cross_border_transfers", [])) for ext in extractions)
        
        # Unique jurisdictions mentioned
        unique_jurisdictions = set()
        for extraction in extractions:
            territorial_analysis = extraction.get("territorial_analysis", {})
            for jurisdiction in territorial_analysis.get("affected_jurisdictions", []):
                if isinstance(jurisdiction, dict):
                    unique_jurisdictions.add(jurisdiction.get("jurisdiction", ""))
                else:
                    unique_jurisdictions.add(str(jurisdiction))
        
        # Document type distribution
        doc_types = {}
        for chunk in chunks:
            doc_type = chunk.get("document_type", "unknown")
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        # Synonym statistics
        total_synonyms = sum(len(syn.synonyms) for syn in synonyms)
        jurisdictional_variants = sum(len(variants) for syn in synonyms for variants in syn.jurisdictional_variants.values())
        
        return {
            "processing_activities_discovered": total_processing_activities,
            "data_categories_discovered": total_data_categories,
            "legal_bases_identified": total_legal_bases,
            "cross_border_transfers_found": total_cross_border_transfers,
            "unique_jurisdictions_mentioned": len(unique_jurisdictions),
            "document_type_distribution": doc_types,
            "total_synonyms_generated": total_synonyms,
            "jurisdictional_variants_created": jurisdictional_variants,
            "average_relevance_score": sum(chunk.get("metadata", {}).get("relevance_score", 0) for chunk in chunks) / len(chunks) if chunks else 0
        }
    
    def iterative_analysis(self, iterations: int = 3) -> Dict[str, Any]:
        """Perform iterative analysis with enhanced AI reasoning"""
        logger.info(f"Starting iterative analysis with {iterations} iterations")
        
        analysis_queries = [
            "What are the key GDPR compliance requirements for financial institutions operating globally?",
            "How do cross-border data transfers work between EU, UK, US, and other major jurisdictions?",
            "What are the mandatory elements for Record of Processing Activities across different jurisdictions?",
            "How do adequacy decisions affect international data transfers for financial services?",
            "What are the specific data protection requirements for different types of financial data?",
            "How do supervisory authorities coordinate across borders for GDPR enforcement?",
            "What are the key differences between GDPR, UK GDPR, CCPA, and other privacy frameworks?",
            "How should financial institutions handle consent and legitimate interests across jurisdictions?",
            "What are the technical and organizational measures required for different jurisdictions?",
            "How do data subject rights vary across global privacy frameworks?"
        ]
        
        iteration_results = []
        
        for iteration in range(iterations):
            logger.info(f"Iteration {iteration + 1}/{iterations}")
            
            iteration_insights = []
            
            for query in analysis_queries:
                # Search vector store with synonym expansion
                vector_results = self.vector_engine.search_with_synonym_expansion(query, top_k=5)
                
                # Search knowledge graph with synonyms
                graph_results = self.graph_engine.search_with_synonyms(query, top_k=3)
                
                # Combine and analyze results
                combined_analysis = {
                    "query": query,
                    "vector_results": vector_results,
                    "graph_results": graph_results,
                    "iteration": iteration + 1
                }
                
                iteration_insights.append(combined_analysis)
            
            iteration_results.append({
                "iteration": iteration + 1,
                "insights": iteration_insights,
                "timestamp": datetime.now()
            })
        
        self.system_state["metamodel_iterations"] = iteration_results
        
        return {
            "iterations_completed": iterations,
            "total_queries_analyzed": len(analysis_queries) * iterations,
            "insights_per_iteration": len(analysis_queries),
            "status": "iterative_analysis_complete"
        }
    
    def generate_final_metamodel(self) -> Dict[str, Any]:
        """Generate final comprehensive metamodel"""
        logger.info("Generating final comprehensive metamodel")
        
        # Consolidate all analysis data
        consolidated_data = {
            "processed_documents": self.system_state["processed_documents"],
            "extracted_concepts": self.system_state["extracted_concepts"],
            "generated_synonyms": [asdict(syn) for syn in self.system_state["generated_synonyms"]],
            "metamodel_iterations": self.system_state["metamodel_iterations"],
            "analysis_statistics": self.system_state["analysis_statistics"],
            "jurisdiction_frameworks": {k.value: asdict(v) for k, v in JURISDICTION_FRAMEWORKS.items()},
            "global_jurisdictions": GLOBAL_JURISDICTIONS
        }
        
        # Generate metamodel using AI
        metamodel = metamodel_generation_agent.invoke(json.dumps(consolidated_data, default=str))
        
        self.system_state["final_metamodel"] = metamodel
        
        return metamodel
    
    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive analysis report"""
        logger.info("Generating comprehensive analysis report")
        
        if not self.system_state["final_metamodel"]:
            self.generate_final_metamodel()
        
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        report_data = {
            "system_state": self.system_state,
            "jurisdiction_frameworks": {k.value: asdict(v) for k, v in JURISDICTION_FRAMEWORKS.items()},
            "timestamp": datetime.now().isoformat()
        }
        
        prompt = f"""Generate a comprehensive executive report on the Enhanced Global GDPR Record of Processing Activities (RoPA) Metamodel System for Financial Institutions.

System Analysis Data: {json.dumps(report_data, default=str)[:50000]}...

Create a detailed report with the following structure:

# Executive Summary
- Project overview and enhanced AI capabilities
- Key findings and AI-generated insights
- Critical compliance gaps identified through AI analysis
- Implementation recommendations and priorities

# Enhanced Methodology
- AI-powered document analysis with o3-mini reasoning
- Comprehensive synonym generation using LLM
- Vector and graph database integration
- Iterative refinement with machine learning

# Global Regulatory Landscape Analysis
- GDPR (EU) comprehensive requirements
- UK GDPR post-Brexit considerations
- CCPA and US state privacy laws
- International frameworks (PIPL, PIPEDA, LGPD, etc.)
- Cross-border data transfer mechanisms
- Adequacy decisions and their implications

# AI-Enhanced Synonym Generation
- LLM-powered concept expansion
- Jurisdictional terminology variations
- Financial sector specific terminology
- Cross-reference and semantic relationships
- Confidence scoring and validation

# Financial Sector Specializations
- Banking regulatory compliance (Basel, MiFID, PSD2)
- Insurance data protection requirements
- Investment management and trading data
- Payment processing and fintech
- Cross-border financial services
- Regulatory reporting across jurisdictions

# Metamodel Architecture and Design
- Core entity definitions with AI enhancement
- Relationship mappings and semantic connections
- Jurisdictional extension patterns
- Financial sector specializations
- Implementation framework with AI integration
- Validation and compliance mechanisms

# Knowledge Graph and Vector Integration
- Semantic search capabilities
- Synonym-enhanced queries
- Cross-reference resolution
- Relationship discovery
- Compliance gap identification

# Implementation Roadmap
- Phase 1: Foundation and AI integration
- Phase 2: Core metamodel implementation
- Phase 3: Jurisdictional extensions
- Phase 4: Financial specializations
- Phase 5: Advanced AI features and optimization

# Risk Assessment and Compliance Validation
- AI-identified compliance risks
- Cross-jurisdictional conflicts
- Implementation challenges
- Mitigation strategies with AI support

# Technology Architecture
- Enhanced vector database integration
- Knowledge graph with synonym expansion
- AI-powered analysis pipeline
- Scalability and performance optimization
- Security and privacy safeguards

# Governance and Continuous Improvement
- AI-enhanced monitoring and updates
- Automated compliance checking
- Synonym evolution and maintenance
- Regular review and enhancement processes

# Conclusions and Future Enhancements
- Key achievements and AI capabilities
- Scalability and adaptability
- Future AI enhancement opportunities
- Long-term strategic value

Provide a comprehensive, professional report that demonstrates the enhanced AI capabilities and global compliance coverage of this advanced system.
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            return response.content
        
        except Exception as e:
            logger.error(f"Failed to generate comprehensive report: {e}")
            return f"Error generating report: {e}"
    
    def query_system(self, query: str, search_type: str = "both") -> Dict[str, Any]:
        """Query the system using enhanced search capabilities"""
        results = {}
        
        if search_type in ["vector", "both"]:
            vector_results = self.vector_engine.search_with_synonym_expansion(query, top_k=10)
            results["vector_search"] = vector_results
        
        if search_type in ["graph", "both"]:
            graph_results = self.graph_engine.search_with_synonyms(query, top_k=10)
            results["graph_search"] = graph_results
        
        return {
            "query": query,
            "search_type": search_type,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        try:
            # Elasticsearch stats
            es_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.index_name)
            doc_count = es_stats.body["indices"][self.vector_engine.index_name]["total"]["docs"]["count"]
            
            # Synonym stats
            synonym_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.synonym_index)
            synonym_count = synonym_stats.body["indices"][self.vector_engine.synonym_index]["total"]["docs"]["count"]
            
            # Graph stats
            node_stats = self.graph_engine.graph.query("MATCH (n) RETURN count(n) as nodes")
            node_count = node_stats.result_set[0][0] if node_stats.result_set else 0
            
            rel_stats = self.graph_engine.graph.query("MATCH ()-[r]->() RETURN count(r) as rels")
            rel_count = rel_stats.result_set[0][0] if rel_stats.result_set else 0
            
            # Detailed entity counts
            entity_counts = {}
            entity_types = ["ProcessingActivity", "DataCategory", "LegalBasis", "Concept", "Synonym"]
            
            for entity_type in entity_types:
                try:
                    count_query = f"MATCH (n:{entity_type}) RETURN count(n) as count"
                    result = self.graph_engine.graph.query(count_query)
                    entity_counts[entity_type.lower() + "_count"] = result.result_set[0][0] if result.result_set else 0
                except:
                    entity_counts[entity_type.lower() + "_count"] = 0
            
            return {
                "elasticsearch_documents": doc_count,
                "elasticsearch_synonyms": synonym_count,
                "graph_nodes": node_count,
                "graph_relationships": rel_count,
                **entity_counts,
                "processed_documents": len(self.system_state["processed_documents"]),
                "generated_synonyms": len(self.system_state["generated_synonyms"]),
                "metamodel_iterations": len(self.system_state["metamodel_iterations"]),
                "has_final_metamodel": self.system_state["final_metamodel"] is not None,
                "analysis_statistics": self.system_state.get("analysis_statistics", {}),
                "system_status": "operational"
            }
        except Exception as e:
            return {"error": str(e), "system_status": "error"}
    
    def export_metamodel(self, output_path: str = "enhanced_gdpr_ropa_metamodel.json") -> bool:
        """Export the final metamodel to file"""
        try:
            if not self.system_state["final_metamodel"]:
                logger.warning("No final metamodel available. Generating...")
                self.generate_final_metamodel()
            
            export_data = {
                "metamodel": self.system_state["final_metamodel"],
                "system_state": self.system_state,
                "jurisdiction_frameworks": {k.value: asdict(v) for k, v in JURISDICTION_FRAMEWORKS.items()},
                "export_timestamp": datetime.now().isoformat(),
                "system_version": "2.0.0"
            }
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2, default=str, ensure_ascii=False)
            
            logger.info(f"Metamodel exported to: {output_path}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to export metamodel: {e}")
            return False


def main():
    """Main execution function for the Enhanced Global RoPA Metamodel System"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Enhanced Global GDPR RoPA Metamodel System with AI-Powered Synonym Generation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python enhanced_gdpr_ropa_system.py --ingest /path/to/gdpr/docs/*.pdf
  python enhanced_gdpr_ropa_system.py --analyze --iterations 5
  python enhanced_gdpr_ropa_system.py --generate-metamodel
  python enhanced_gdpr_ropa_system.py --generate-report
  python enhanced_gdpr_ropa_system.py --query "cross-border data transfers"
  python enhanced_gdpr_ropa_system.py --stats
        """
    )
    
    # Main operations
    parser.add_argument("--ingest", nargs="+", help="Paths to regulatory documents to ingest")
    parser.add_argument("--analyze", action="store_true", help="Perform iterative metamodel analysis")
    parser.add_argument("--generate-metamodel", action="store_true", help="Generate final metamodel")
    parser.add_argument("--generate-report", action="store_true", help="Generate comprehensive report")
    parser.add_argument("--query", type=str, help="Query the system with natural language")
    parser.add_argument("--stats", action="store_true", help="Show system statistics")
    parser.add_argument("--export", type=str, help="Export metamodel to specified file path")
    
    # Configuration options
    parser.add_argument("--iterations", type=int, default=3, help="Number of analysis iterations")
    parser.add_argument("--search-type", choices=["vector", "graph", "both"], default="both", 
                       help="Type of search to perform")
    parser.add_argument("--output-dir", type=str, default=".", help="Output directory for reports and exports")
    
    args = parser.parse_args()
    
    # Load and validate configuration
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "openai_base_url": os.getenv("OPENAI_BASE_URL"),
        "elasticsearch_host": os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        "elasticsearch_username": os.getenv("ELASTICSEARCH_USERNAME"),
        "elasticsearch_password": os.getenv("ELASTICSEARCH_PASSWORD"),
        "elasticsearch_ca_certs": os.getenv("ELASTICSEARCH_CA_CERTS"),
        "falkordb_host": os.getenv("FALKORDB_HOST", "localhost"),
        "falkordb_port": int(os.getenv("FALKORDB_PORT", 6379)),
        "falkordb_password": os.getenv("FALKORDB_PASSWORD")
    }
    
    # Validate required configuration
    if not config["openai_api_key"]:
        print("❌ Error: OPENAI_API_KEY environment variable is required")
        print("   Set it with: export OPENAI_API_KEY=your_api_key")
        return 1
    
    if not config["elasticsearch_host"].startswith(('http://', 'https://')):
        print("❌ Error: ELASTICSEARCH_HOST must include schema (http:// or https://)")
        print(f"   Current value: {config['elasticsearch_host']}")
        return 1
    
    # Display configuration
    print("🚀 Enhanced Global GDPR RoPA Metamodel System")
    print("=" * 60)
    print("Configuration:")
    print(f"  • OpenAI Endpoint: {config['openai_base_url'] or 'Standard OpenAI API'}")
    print(f"  • Elasticsearch: {config['elasticsearch_host']}")
    print(f"  • FalkorDB: {config['falkordb_host']}:{config['falkordb_port']}")
    print(f"  • Enhanced Features: AI Synonym Generation, Cross-Jurisdictional Analysis")
    print(f"  • Target: Financial Institutions, Global Compliance")
    print()
    
    try:
        # Initialize the enhanced system
        print("🔧 Initializing Enhanced Global RoPA Metamodel System...")
        system = GlobalRopaMetamodelSystem(
            openai_api_key=config["openai_api_key"],
            openai_base_url=config["openai_base_url"],
            elasticsearch_host=config["elasticsearch_host"],
            elasticsearch_username=config["elasticsearch_username"],
            elasticsearch_password=config["elasticsearch_password"],
            elasticsearch_ca_certs=config["elasticsearch_ca_certs"],
            falkordb_host=config["falkordb_host"],
            falkordb_port=config["falkordb_port"],
            falkordb_password=config["falkordb_password"]
        )
        
        print("✅ Enhanced system initialized successfully")
        print()
        
        # Execute requested operations
        if args.ingest:
            print(f"📄 Ingesting {len(args.ingest)} regulatory documents with AI analysis...")
            result = system.ingest_documents(args.ingest)
            print("✅ Document ingestion completed:")
            print(f"   • Documents processed: {result['documents_processed']}")
            print(f"   • Total chunks: {result['total_chunks']}")
            print(f"   • AI synonyms generated: {result['synonyms_generated']}")
            print(f"   • Processing activities: {result.get('processing_activities_discovered', 0)}")
            print(f"   • Data categories: {result.get('data_categories_discovered', 0)}")
            print(f"   • Jurisdictions identified: {result.get('unique_jurisdictions_mentioned', 0)}")
            print()
        
        if args.analyze:
            print(f"🔍 Performing iterative analysis with {args.iterations} iterations...")
            result = system.iterative_analysis(args.iterations)
            print("✅ Iterative analysis completed:")
            print(f"   • Iterations completed: {result['iterations_completed']}")
            print(f"   • Total queries analyzed: {result['total_queries_analyzed']}")
            print(f"   • Insights per iteration: {result['insights_per_iteration']}")
            print()
        
        if args.generate_metamodel:
            print("🏗️ Generating final comprehensive metamodel with AI insights...")
            result = system.generate_final_metamodel()
            print("✅ Final metamodel generated successfully")
            
            # Save metamodel to output directory
            output_file = os.path.join(args.output_dir, "enhanced_gdpr_ropa_metamodel.json")
            system.export_metamodel(output_file)
            print(f"💾 Metamodel saved to: {output_file}")
            print()
        
        if args.generate_report:
            print("📊 Generating comprehensive analysis report...")
            report = system.generate_comprehensive_report()
            
            # Save report to output directory
            report_file = os.path.join(args.output_dir, "enhanced_gdpr_ropa_analysis_report.md")
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            
            print(f"💾 Report saved to: {report_file}")
            
            # Display executive summary
            lines = report.split('\n')
            summary_start = -1
            summary_end = -1
            
            for i, line in enumerate(lines):
                if "# Executive Summary" in line:
                    summary_start = i
                elif summary_start != -1 and line.startswith("# ") and "Executive Summary" not in line:
                    summary_end = i
                    break
            
            if summary_start != -1:
                end_idx = summary_end if summary_end != -1 else min(summary_start + 20, len(lines))
                summary = '\n'.join(lines[summary_start:end_idx])
                
                print("\n" + "="*80)
                print("EXECUTIVE SUMMARY PREVIEW")
                print("="*80)
                print(summary[:2000] + "..." if len(summary) > 2000 else summary)
                print("="*80)
            print()
        
        if args.query:
            print(f"🔎 Querying system: '{args.query}'")
            print(f"   Search type: {args.search_type}")
            result = system.query_system(args.query, args.search_type)
            
            print("✅ Query results:")
            if "vector_search" in result["results"]:
                vector_results = result["results"]["vector_search"]
                print(f"   • Vector search: {len(vector_results)} results")
                for i, res in enumerate(vector_results[:3]):
                    print(f"     {i+1}. {res['chunk_id']} (score: {res['score']:.3f})")
            
            if "graph_search" in result["results"]:
                graph_results = result["results"]["graph_search"]
                print(f"   • Graph search: {len(graph_results)} results")
                for i, res in enumerate(graph_results[:3]):
                    start_node = res.get('start_node', {})
                    node_name = start_node.get('name', 'Unknown')
                    print(f"     {i+1}. {node_name} (distance: {res.get('distance', 'N/A')})")
            print()
        
        if args.stats:
            print("📈 System Statistics:")
            stats = system.get_system_statistics()
            
            if "error" in stats:
                print(f"   ❌ Error: {stats['error']}")
            else:
                print(f"   • System Status: {stats['system_status']}")
                print(f"   • Elasticsearch Documents: {stats.get('elasticsearch_documents', 0):,}")
                print(f"   • AI-Generated Synonyms: {stats.get('elasticsearch_synonyms', 0):,}")
                print(f"   • Knowledge Graph Nodes: {stats.get('graph_nodes', 0):,}")
                print(f"   • Knowledge Graph Relationships: {stats.get('graph_relationships', 0):,}")
                print(f"   • Processing Activities: {stats.get('processingactivity_count', 0):,}")
                print(f"   • Data Categories: {stats.get('datacategory_count', 0):,}")
                print(f"   • Concepts: {stats.get('concept_count', 0):,}")
                print(f"   • Synonyms: {stats.get('synonym_count', 0):,}")
                print(f"   • Processed Documents: {stats.get('processed_documents', 0)}")
                print(f"   • Metamodel Iterations: {stats.get('metamodel_iterations', 0)}")
                print(f"   • Has Final Metamodel: {'Yes' if stats.get('has_final_metamodel') else 'No'}")
                
                analysis_stats = stats.get('analysis_statistics', {})
                if analysis_stats:
                    print("   Analysis Statistics:")
                    for key, value in analysis_stats.items():
                        if isinstance(value, dict):
                            print(f"     • {key.replace('_', ' ').title()}:")
                            for k, v in value.items():
                                print(f"       - {k}: {v}")
                        else:
                            print(f"     • {key.replace('_', ' ').title()}: {value}")
            print()
        
        if args.export:
            print(f"💾 Exporting metamodel to: {args.export}")
            success = system.export_metamodel(args.export)
            if success:
                print("✅ Export completed successfully")
            else:
                print("❌ Export failed")
            print()
        
        # If no specific action requested, show help and system info
        if not any([args.ingest, args.analyze, args.generate_metamodel, 
                   args.generate_report, args.query, args.stats, args.export]):
            parser.print_help()
            print("\n" + "="*80)
            print("ENHANCED SYSTEM CAPABILITIES")
            print("="*80)
            print("🤖 AI-Powered Features:")
            print("   • LLM-based synonym generation with o3-mini reasoning")
            print("   • Comprehensive regulatory concept extraction")
            print("   • Cross-jurisdictional terminology mapping")
            print("   • Financial sector specialization")
            print("   • Semantic search with synonym expansion")
            print()
            print("🌍 Global Compliance Coverage:")
            print("   • GDPR (EU) and UK GDPR")
            print("   • CCPA, PIPEDA, LGPD, PIPL")
            print("   • Cross-border transfer mechanisms")
            print("   • Adequacy decision mapping")
            print("   • Financial sector regulations")
            print()
            print("🏗️ Architecture:")
            print("   • Elasticsearch with custom analyzers")
            print("   • FalkorDB knowledge graph")
            print("   • Vector embeddings with text-embedding-3-large")
            print("   • AI-powered synonym expansion")
            print("   • Iterative refinement with reasoning")
            print()
            print("💡 Quick Start:")
            print("   1. Set environment variables (OPENAI_API_KEY, ELASTICSEARCH_HOST, etc.)")
            print("   2. Ingest documents: --ingest /path/to/docs/*.pdf")
            print("   3. Analyze: --analyze --iterations 3")
            print("   4. Generate metamodel: --generate-metamodel")
            print("   5. Create report: --generate-report")
            print("="*80)
    
    except KeyboardInterrupt:
        print("\n⚠️ Operation cancelled by user")
        return 1
    except Exception as e:
        print(f"\n❌ System error: {e}")
        logger.exception("Detailed error information:")
        return 1
    
    print("🎉 Enhanced Global RoPA Metamodel System operation completed successfully!")
    return 0


if __name__ == "__main__":
    exit(main())
