#!/usr/bin/env python3
"""
COMPLETE: Optimized Async Triple-Based RDF to FalkorDB Property Graph Converter

üîÑ AUTOMATIC URI MERGING: When the same URI appears in multiple queries, 
   the system automatically merges them into a single node with combined properties!

‚ú® AUTOMATIC DEDUPLICATION: After all queries complete, the system automatically:
   - Removes duplicate nodes (keeping the one with most properties)
   - Removes duplicate relationships  
   - Ensures your final graph is completely clean and deduplicated

üìä PROGRESS TRACKING: Beautiful tqdm progress bars show real-time progress for:
   - Query processing
   - Triple batch processing  
   - Node creation
   - Edge creation (with rates)
   - Deduplication steps

FIXES APPLIED:
- Fixed all indentation and import errors
- Fixed SPARQL connection initialization 
- Fixed "Attribute uri already indexed" error with robust index management
- Added comprehensive URI-based merging across multiple queries
- Added automatic deduplication with final cleanup
- Added tqdm progress bars for visual feedback

KEY FEATURES FOR MULTIPLE QUERIES:
‚úÖ URI-Based Merging: Same URI in different queries = ONE merged node
‚úÖ Property Combination: All properties from all queries combined
‚úÖ Relationship Deduplication: No duplicate relationships
‚úÖ Smart Index Management: Handles index conflicts gracefully
‚úÖ Error Recovery: Failed queries don't stop the process
‚úÖ Automatic Final Deduplication: Ensures perfectly clean graph
‚úÖ Beautiful Progress Bars: Real-time visual feedback with tqdm

Dependencies:
    pip install rdflib falkordb redis asyncio tqdm
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

# Async imports
import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def clean_label_name(label: str) -> str:
    """Clean label name to ensure valid Cypher identifier"""
    if not label:
        return 'Resource'
    
    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', label)
    
    if clean_label and clean_label[0].isdigit():
        clean_label = f"_{clean_label}"
    
    if not clean_label:
        clean_label = 'Resource'
    elif len(clean_label) > 50:
        clean_label = clean_label[:50]
    
    return clean_label


@dataclass
class OptimizedAsyncTripleConfig:
    """Configuration for async triple-based RDF conversion"""
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Graph management settings
    append_to_existing_graph: bool = True
    clear_existing_graph: bool = False
    handle_duplicates: bool = True
    skip_existing_nodes: bool = True
    skip_existing_relationships: bool = True
    
    # Processing settings
    batch_size: int = 2000
    max_concurrent_batches: int = 3
    connection_pool_size: int = 10
    sparql_timeout: int = 7200
    falkordb_timeout: Optional[int] = 300
    max_retries: int = 5
    retry_delay: int = 3
    
    # Optimization settings
    preserve_uri_properties: bool = False
    disable_relationship_properties: bool = True
    group_relationships_by_type: bool = True
    use_bulk_relationship_creation: bool = True
    
    # Performance settings
    use_shortened_uris: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'
    
    # Other settings
    exclude_rdf_type_properties: bool = False
    validate_conversion: bool = False
    export_stats: bool = True
    progress_update_interval: int = 50


@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    # Batch processing stats
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    # Graph state tracking
    initial_nodes: int = 0
    initial_relationships: int = 0
    
    # Created entities
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    skipped_nodes: int = 0
    skipped_relationships: int = 0
    
    # Final totals
    final_nodes: int = 0
    final_relationships: int = 0
    
    # Performance metrics
    relationship_creation_time: float = 0.0
    node_creation_time: float = 0.0
    relationship_creation_rate: float = 0.0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    relationship_types_count: Dict[str, int] = None
    
    # Incremental loading info
    append_mode: bool = False
    graph_was_cleared: bool = False
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
        if self.relationship_types_count is None:
            self.relationship_types_count = {}
    
    def get_incremental_summary(self) -> str:
        """Get a summary of incremental changes"""
        if self.append_mode:
            return (f"Added {self.created_nodes:,} nodes and {self.created_relationships:,} relationships. "
                   f"Graph now has {self.final_nodes:,} nodes and {self.final_relationships:,} relationships total.")
        else:
            return (f"Created {self.created_nodes:,} nodes and {self.created_relationships:,} relationships "
                   f"in new graph.")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        result['incremental_summary'] = self.get_incremental_summary()
        
        return result


class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)


class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: OptimizedAsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry with proper RDF class as primary label"""
        labels = set()
        primary_label = None
        
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            clean_class_label = clean_label_name(class_label)
            labels.add(clean_class_label)
            primary_label = clean_class_label
        else:
            primary_label = clean_label_name(self.config.default_node_label)
            labels.add(primary_label)
        
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'primary_label': primary_label,
            'properties': {}
        }
        
        node_data['properties']['uri'] = uri
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        if self.config.preserve_uri_properties and class_uri:
            node_data['properties']['rdf_type'] = class_uri
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()


class OptimizedAsyncFalkorDBManager:
    """Optimized FalkorDB manager with corrected syntax for FalkorDB compatibility"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB"""
        try:
            clean_default_label = clean_label_name(self.config.default_node_label)
            if clean_default_label != self.config.default_node_label:
                logger.warning(f"Default label '{self.config.default_node_label}' will be cleaned to '{clean_default_label}'")
            
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
                'socket_connect_timeout': 30,
            }
            
            if self.config.falkordb_timeout:
                pool_kwargs['timeout'] = self.config.falkordb_timeout
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            logger.info(f"‚úÖ FalkorDB connection established to graph '{self.config.graph_name}'")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("üóëÔ∏è  Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def get_initial_graph_stats(self) -> Dict[str, int]:
        """Get initial graph statistics before processing"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            if node_count > 0 or rel_count > 0:
                logger.info(f"üìä Existing graph: {node_count:,} nodes, {rel_count:,} relationships")
            else:
                logger.info("üìä Starting with empty graph")
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting initial statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with async retry logic"""
        for attempt in range(self.config.max_retries):
            try:
                return await self.graph.query(query, params or {})
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
    
    async def create_index_safely(self, label: str) -> bool:
        """Create an index safely with proper error handling"""
        try:
            query = f"CREATE INDEX FOR (n:{label}) ON (n.uri)"
            await self.execute_query_with_retry(query)
            logger.info(f"‚úÖ Created index for label '{label}'")
            return True
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info(f"‚ÑπÔ∏è  Index for '{label}' already exists")
                return True
            else:
                logger.warning(f"Could not create index for label '{label}': {e}")
                return False
    
    async def create_indexes(self, discovered_labels: Dict[str, int] = None):
        """Create indexes with robust duplicate handling"""
        if not self.config.create_indexes:
            return
        
        logger.info("Creating performance indexes for discovered RDF classes...")
        
        clean_default_label = clean_label_name(self.config.default_node_label)
        
        if not discovered_labels:
            logger.warning("No discovered labels provided - creating fallback index only")
            await self.create_index_safely(clean_default_label)
            return
        
        top_labels = sorted(discovered_labels.items(), key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"Creating indexes for top {len(top_labels)} RDF classes:")
        
        created_indexes = set()
        
        for label, count in top_labels:
            logger.info(f"  - {label}: {count:,} nodes")
            if await self.create_index_safely(label):
                created_indexes.add(label)
        
        if clean_default_label not in created_indexes:
            await self.create_index_safely(clean_default_label)
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """Create nodes with URI-based merging"""
        if not nodes_batch:
            return 0
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        try:
            nodes_by_label = defaultdict(list)
            
            for uri, node_data in nodes_batch:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                nodes_by_label[primary_label].append({
                    'uri': uri,
                    'properties': properties
                })
            
            total_created = 0
            
            for label, nodes_data in nodes_by_label.items():
                if use_merge:
                    logger.info(f"Merging {len(nodes_data)} nodes with label '{label}' (URI-based deduplication)")
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    MERGE (n:{label} {{uri: node_data.uri}})
                    SET n += node_data.properties
                    RETURN count(n) as total_processed
                    """
                else:
                    logger.info(f"Creating {len(nodes_data)} nodes with label '{label}'")
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    CREATE (n:{label})
                    SET n += node_data.properties
                    RETURN count(n) as total_created
                    """
                
                result = await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
                batch_processed = result.result_set[0][0] if result.result_set else len(nodes_data)
                total_created += batch_processed
                
                if use_merge:
                    logger.info(f"‚úÖ Processed {batch_processed} nodes with label '{label}' (new + updated)")
                else:
                    logger.info(f"‚úÖ Created {batch_processed} nodes with label '{label}'")
            
            return total_created
                
        except Exception as e:
            logger.error(f"Batch node creation failed: {e}")
            return await self._create_nodes_individual_fallback(nodes_batch, use_merge)
    
    async def _create_nodes_individual_fallback(self, nodes_batch: List[Tuple[str, Dict[str, Any]]], use_merge: bool = False) -> int:
        """Fallback to create nodes individually"""
        created_count = 0
        
        for uri, node_data in nodes_batch:
            try:
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                if use_merge:
                    query = f"""
                    MERGE (n:{primary_label} {{uri: $uri}})
                    SET n += $properties
                    """
                else:
                    query = f"""
                    CREATE (n:{primary_label})
                    SET n += $properties
                    """
                
                await self.execute_query_with_retry(query, {
                    'uri': uri,
                    'properties': properties
                })
                created_count += 1
                
            except Exception as e:
                logger.warning(f"Failed to create individual node {uri}: {e}")
                continue
        
        return created_count
    
    async def create_relationships_ultra_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """Ultra-fast relationship creation optimized for 2M+ edges"""
        if not relationships_by_type:
            return 0
        
        total_relationships = sum(len(rels) for rels in relationships_by_type.values())
        logger.info(f"üöÄ ULTRA-FAST MODE: Creating {total_relationships:,} relationships (optimized for 2M+ edges)")
        
        start_time = time.time()
        total_created = 0
        
        # For 2M+ edges, use different strategy based on duplicate handling
        if self.config.handle_duplicates:
            logger.info("‚ö° Using MERGE strategy (handles duplicates, slower)")
            # Process each relationship type with controlled concurrency
            semaphore = asyncio.Semaphore(1)  # Single-threaded for stability
            
            async def create_relationships_for_type(rel_type: str, relationships: List[Tuple[str, str, str]]):
                async with semaphore:
                    return await self._create_relationships_bulk_by_type(rel_type, relationships)
            
            tasks = []
            for rel_type, relationships in relationships_by_type.items():
                task = asyncio.create_task(create_relationships_for_type(rel_type, relationships))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, int):
                    total_created += result
                else:
                    rel_type = list(relationships_by_type.keys())[i]
                    logger.error(f"Failed to create relationships for type {rel_type}: {result}")
        else:
            logger.info("üöÑ Using CREATE strategy (maximum speed, no duplicate checking)")
            # Ultra-fast mode: flatten all relationships and process in mega-batches
            total_created = await self._create_relationships_mega_fast(relationships_by_type)
        
        duration = time.time() - start_time
        rate = total_created / duration if duration > 0 else 0
        
        logger.info(f"üéâ Ultra-fast relationship creation completed:")
        logger.info(f"  Created: {total_created:,} relationships")
        logger.info(f"  Time: {duration:.2f} seconds ({duration/60:.1f} minutes)")
        logger.info(f"  Rate: {rate:.1f} relationships/second")
        
        # Performance assessment for large scale
        if rate > 5000:
            logger.info("üöÄ EXCELLENT: Ultra-high-speed processing (>5000 rel/sec)")
        elif rate > 2000:
            logger.info("‚úÖ GOOD: High-speed processing (>2000 rel/sec)")
        elif rate > 500:
            logger.info("‚ö†Ô∏è  ACCEPTABLE: Moderate speed (>500 rel/sec)")
        else:
            logger.info("‚ùå SLOW: Consider using ultra_fast_config for better performance")
        
        return total_created
    
    async def _create_relationships_mega_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """MEGA-FAST: Process all relationships with tqdm progress bars"""
        
        # Flatten all relationships into a single list for mega-batch processing
        all_relationships = []
        for rel_type, relationships in relationships_by_type.items():
            for subject_uri, predicate_uri, object_uri in relationships:
                all_relationships.append((subject_uri, predicate_uri, object_uri, rel_type))
        
        logger.info(f"üöÑ MEGA-FAST MODE: Processing {len(all_relationships):,} relationships in ultra-large batches")
        
        # Use massive batch size for 2M+ edges
        mega_batch_size = min(100000, max(50000, len(all_relationships) // 20))  # 50K-100K per batch
        logger.info(f"üì¶ Using mega-batch size: {mega_batch_size:,} relationships per batch")
        
        total_created = 0
        total_batches = (len(all_relationships) + mega_batch_size - 1) // mega_batch_size
        
        # Process with tqdm progress bar
        with tqdm_sync(total=total_batches, desc="üöÑ Creating mega-batches", 
                      unit="mega-batch", ncols=100, colour="red") as pbar:
            
            for i in range(0, len(all_relationships), mega_batch_size):
                batch = all_relationships[i:i+mega_batch_size]
                batch_num = (i // mega_batch_size) + 1
                
                pbar.set_description(f"üöÑ Mega-batch {batch_num}/{total_batches}")
                
                try:
                    created = await self._execute_mega_batch_query(batch)
                    total_created += created
                    
                    # Update progress bar with detailed info
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'rate': f"{total_created/(time.time() - self.stats.start_time.timestamp()):.0f}/s",
                        'batch_size': f"{len(batch):,}"
                    })
                    
                except Exception as e:
                    logger.error(f"Mega-batch {batch_num} failed: {e}")
                    # Try smaller batch as fallback
                    logger.info(f"üîÑ Falling back to smaller batches for mega-batch {batch_num}")
                    created = await self._fallback_mega_batch(batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'rate': f"{total_created/(time.time() - self.stats.start_time.timestamp()):.0f}/s",
                        'status': 'fallback'
                    })
        
        return total_created
    
    async def _execute_mega_batch_query(self, batch: List[Tuple[str, str, str, str]]) -> int:
        """Execute a mega-batch of relationships with optimized Cypher"""
        
        # Group by relationship type for efficient processing
        grouped_batch = defaultdict(list)
        for subject_uri, predicate_uri, object_uri, rel_type in batch:
            grouped_batch[rel_type].append({
                'subject_uri': subject_uri,
                'object_uri': object_uri
            })
        
        total_created = 0
        
        # Process each relationship type in the mega-batch
        for rel_type, rel_data in grouped_batch.items():
            clean_rel_type = self._clean_relationship_type(rel_type)
            
            # Ultra-optimized Cypher for maximum speed (CREATE only, no MERGE)
            query = f"""
            UNWIND $batch_data AS rel
            MATCH (s {{uri: rel.subject_uri}})
            MATCH (o {{uri: rel.object_uri}})
            CREATE (s)-[:{clean_rel_type}]->(o)
            """
            
            await self.execute_query_with_retry(query, {'batch_data': rel_data})
            total_created += len(rel_data)
        
        return total_created
    
    async def _fallback_mega_batch(self, batch: List[Tuple[str, str, str, str]]) -> int:
        """Fallback for failed mega-batches: process in smaller chunks"""
        fallback_batch_size = 10000  # 10K fallback size
        total_created = 0
        
        for i in range(0, len(batch), fallback_batch_size):
            small_batch = batch[i:i+fallback_batch_size]
            try:
                created = await self._execute_mega_batch_query(small_batch)
                total_created += created
            except Exception as e:
                logger.warning(f"Fallback batch failed: {e}")
                # Skip this batch and continue
                continue
        
        return total_created
    
    async def _create_relationships_bulk_by_type(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Create all relationships of a specific type with tqdm progress bar"""
        if not relationships:
            return 0
        
        logger.info(f"Creating {len(relationships):,} relationships of type '{rel_type}'")
        
        # Adaptive batch sizing based on total relationship count and configuration
        if len(relationships) > 500000:  # 500K+ relationships
            batch_size = self.config.batch_size * 10  # Use 10x larger batches
        elif len(relationships) > 100000:  # 100K+ relationships  
            batch_size = self.config.batch_size * 5   # Use 5x larger batches
        elif len(relationships) > 10000:   # 10K+ relationships
            batch_size = self.config.batch_size * 2   # Use 2x larger batches
        else:
            batch_size = self.config.batch_size       # Use standard batch size
        
        logger.info(f"üì¶ Using adaptive batch size: {batch_size:,} relationships per batch")
        
        total_created = 0
        total_batches = (len(relationships) + batch_size - 1) // batch_size
        
        # Create batches
        batches = []
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            batches.append(batch)
        
        # Process with tqdm progress bar
        with tqdm_sync(total=len(batches), desc=f"‚ö° Creating {rel_type}", 
                      unit="batch", ncols=100, colour="yellow") as pbar:
            
            for batch_idx, batch in enumerate(batches):
                batch_num = batch_idx + 1
                
                try:
                    created = await self._execute_relationship_batch_query(rel_type, batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'rate': f"{total_created/(time.time() - self.stats.start_time.timestamp()):.0f}/s",
                        'batch': f"{batch_num}/{total_batches}"
                    })
                    
                except Exception as e:
                    logger.error(f"Failed to create relationship batch for {rel_type}: {e}")
                    # Try smaller batches
                    created = await self._create_relationships_smaller_batches(rel_type, batch)
                    total_created += created
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'edges': f"{total_created:,}",
                        'status': 'fallback',
                        'batch': f"{batch_num}/{total_batches}"
                    })
        
        logger.info(f"‚úÖ Completed {rel_type}: {total_created:,} relationships created")
        return total_created
    
    async def _execute_relationship_batch_query(self, rel_type: str, batch: List[Tuple[str, str, str]]) -> int:
        """Execute optimized batch relationship creation with URI-based merging"""
        batch_data = []
        for subject_uri, predicate_uri, object_uri in batch:
            rel_data = {
                'subject_uri': subject_uri,
                'object_uri': object_uri
            }
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                rel_data['predicate_uri'] = predicate_uri
            batch_data.append(rel_data)
        
        clean_rel_type = self._clean_relationship_type(rel_type)
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        if use_merge:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[:{clean_rel_type}]->(o)
                """
        else:
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
        
        await self.execute_query_with_retry(query, {'batch_data': batch_data})
        return len(batch)
    
    def _clean_relationship_type(self, rel_type: str) -> str:
        """Clean relationship type to ensure valid Cypher identifier"""
        clean_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if clean_type and clean_type[0].isdigit():
            clean_type = f"_{clean_type}"
        
        if not clean_type:
            clean_type = 'RELATED_TO'
        elif len(clean_type) > 50:
            clean_type = clean_type[:50]
        
        return clean_type
    
    async def _create_relationships_smaller_batches(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Fallback: create relationships in much smaller batches"""
        batch_size = 100
        total_created = 0
        clean_rel_type = self._clean_relationship_type(rel_type)
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            try:
                batch_data = []
                for subject_uri, predicate_uri, object_uri in batch:
                    batch_data.append({
                        'subject_uri': subject_uri,
                        'object_uri': object_uri
                    })
                
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
                
                await self.execute_query_with_retry(query, {'batch_data': batch_data})
                total_created += len(batch)
                
            except Exception as e:
                logger.warning(f"Small batch failed for {rel_type}: {e}")
                for subject_uri, predicate_uri, object_uri in batch:
                    try:
                        query = """
                        MATCH (s {uri: $subject_uri})
                        MATCH (o {uri: $object_uri})
                        CREATE (s)-[:RELATED_TO]->(o)
                        """
                        await self.execute_query_with_retry(query, {
                            'subject_uri': subject_uri,
                            'object_uri': object_uri
                        })
                        total_created += 1
                    except Exception:
                        continue
        
        return total_created
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")


class OptimizedAsyncTripleBasedConverter:
    """Main async converter with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = OptimizedAsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        self.relationships_by_type: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        self.total_relationships = 0
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
    
    def _reset_conversion_state(self):
        """Reset converter state for fresh conversion"""
        logger.info("üîÑ Resetting converter state for fresh conversion...")
        
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.relationships_by_type.clear()
        self.total_relationships = 0
        
        logger.info("‚úÖ Converter state reset complete")
    
    def _setup_fresh_rdf_connection(self):
        """Setup a fresh RDF graph connection to SPARQL endpoint"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                try:
                    if hasattr(self.rdf_graph.store, 'close'):
                        self.rdf_graph.store.close()
                except:
                    pass
            
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"‚úÖ Fresh SPARQL connection established to: {self.config.sparql_endpoint}")
            
            test_query = "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o } LIMIT 1"
            try:
                list(self.rdf_graph.query(test_query))
                logger.info("‚úÖ SPARQL connection test successful")
            except Exception as test_error:
                logger.warning(f"‚ö†Ô∏è  SPARQL connection test failed: {test_error}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def _cleanup_rdf_connection(self):
        """Clean up RDF connection resources"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                if hasattr(self.rdf_graph.store, 'close'):
                    self.rdf_graph.store.close()
                    logger.info("üßπ Cleaned up SPARQL connection")
                self.rdf_graph = None
        except Exception as e:
            logger.warning(f"Error cleaning up SPARQL connection: {e}")
    
    def _execute_sparql_query_with_retry(self):
        """Execute SPARQL query with connection retry logic"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                if attempt == 0:
                    logger.info("Setting up initial SPARQL connection...")
                else:
                    logger.info(f"SPARQL query attempt {attempt + 1}/{max_attempts} - setting up fresh connection...")
                
                self._setup_fresh_rdf_connection()
                
                if self.rdf_graph is None:
                    raise Exception("Failed to establish SPARQL connection - rdf_graph is None")
                
                logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
                logger.info(f"Query preview: {self.config.triples_query[:200]}...")
                
                start_time = time.time()
                query_result = self.rdf_graph.query(self.config.triples_query)
                results = list(query_result)
                execution_time = time.time() - start_time
                
                logger.info(f"‚úÖ SPARQL query completed in {execution_time:.2f}s, retrieved {len(results)} triples")
                return results
                
            except Exception as e:
                logger.error(f"SPARQL query attempt {attempt + 1} failed: {e}")
                
                if attempt < max_attempts - 1:
                    self._cleanup_rdf_connection()
                    retry_wait = 5 * (attempt + 1)
                    logger.info(f"Retrying in {retry_wait} seconds...")
                    time.sleep(retry_wait)
                else:
                    logger.error(f"‚ùå SPARQL query failed after {max_attempts} attempts")
                    raise
        
        raise Exception("SPARQL query failed - should not reach here")
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method"""
        try:
            self._reset_conversion_state()
            self.stats.append_mode = self.config.append_to_existing_graph
            
            if self.config.append_to_existing_graph:
                logger.info("üìà Starting incremental RDF to FalkorDB conversion (append mode)...")
            else:
                logger.info("üöÄ Starting fresh RDF to FalkorDB conversion...")
            
            await self.falkordb_manager.connect()
            
            initial_stats = await self.falkordb_manager.get_initial_graph_stats()
            self.stats.initial_nodes = initial_stats['nodes']
            self.stats.initial_relationships = initial_stats['relationships']
            
            if self.config.clear_existing_graph:
                logger.info("üóëÔ∏è  Clearing existing graph as requested...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            elif not self.config.append_to_existing_graph:
                logger.info("üóëÔ∏è  Clearing graph for fresh conversion...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            
            await self.node_manager.clear()
            
            await self._execute_and_process_query()
            
            node_start = time.time()
            await self._create_nodes_in_falkordb_async()
            self.stats.node_creation_time = time.time() - node_start
            
            rel_start = time.time()
            await self._create_relationships_ultra_fast()
            self.stats.relationship_creation_time = time.time() - rel_start
            
            if self.stats.relationship_creation_time > 0:
                self.stats.relationship_creation_rate = self.stats.created_relationships / self.stats.relationship_creation_time
            
            await self._create_indexes_with_discovered_labels()
            
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            await self._finalize_stats()
            await self._log_performance_summary()
            
            if self.config.append_to_existing_graph:
                logger.info("‚úÖ Incremental conversion completed successfully!")
                logger.info(f"üìä {self.stats.get_incremental_summary()}")
            else:
                logger.info("‚úÖ Fresh conversion completed successfully!")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"‚ùå Conversion failed: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results"""
        start_time = time.time()
        
        try:
            logger.info("üìä Executing triples query with fresh connection...")
            
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(None, self._execute_sparql_query_with_retry)
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"üìà Retrieved {len(results):,} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("‚ö° Processing triples in optimized async batches...")
            
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
        finally:
            self._cleanup_rdf_connection()
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with tqdm progress bar"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"üîÑ Processing {len(results):,} triples in {total_batches} batches of {batch_size}")
        
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        # Create tasks with progress bar
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        completed = 0
        failed = 0
        
        # Process with tqdm progress bar
        with tqdm_sync(total=len(tasks), desc="üîÑ Processing triple batches", 
                      unit="batch", ncols=100, colour="blue") as pbar:
            
            for future in asyncio.as_completed(tasks):
                try:
                    await future
                    completed += 1
                    self.stats.completed_batches = completed
                    pbar.update(1)
                    pbar.set_postfix({
                        'completed': f"{completed}/{total_batches}",
                        'triples': f"{self.stats.processed_triples:,}"
                    })
                    
                except Exception as e:
                    failed += 1
                    self.stats.failed_batches = failed
                    pbar.update(1)
                    pbar.set_postfix({
                        'completed': f"{completed}/{total_batches}",
                        'failed': failed,
                        'triples': f"{self.stats.processed_triples:,}"
                    })
                    logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"‚úÖ Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        if isinstance(obj, Literal):
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            object_uri = str(obj)
            
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            rel_type = self._get_relationship_type(predicate_uri)
            self.relationships_by_type[rel_type].append((subject_uri, predicate_uri, object_uri))
            self.total_relationships += 1
            self.stats.relationship_triples += 1
            
            self.stats.relationship_types_count[rel_type] = self.stats.relationship_types_count.get(rel_type, 0) + 1
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _get_relationship_type(self, predicate_uri: str) -> str:
        """Extract and clean relationship type from predicate URI"""
        parsed = urlparse(predicate_uri)
        if parsed.fragment:
            rel_type = parsed.fragment
        else:
            rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
        
        rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        if not rel_type or rel_type.isdigit():
            rel_type = 'RELATED_TO'
        elif rel_type[0].isdigit():
            rel_type = f"_{rel_type}"
        
        if len(rel_type) > 50:
            rel_type = rel_type[:50]
        
        return rel_type
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB with tqdm progress bar"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"üèóÔ∏è  Creating {len(nodes):,} nodes in FalkorDB...")
        
        if not nodes:
            logger.warning("No nodes to create")
            return
        
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        # Create tasks with progress bar
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        total_created = 0
        
        # Process with tqdm progress bar
        with tqdm_sync(total=len(tasks), desc="üèóÔ∏è  Creating node batches", 
                      unit="batch", ncols=100, colour="green") as pbar:
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, int):
                    total_created += result
                    pbar.update(1)
                    pbar.set_postfix({
                        'nodes_created': f"{total_created:,}",
                        'rate': f"{total_created/(time.time() - self.stats.start_time.timestamp()):.0f}/s"
                    })
                else:
                    pbar.update(1)
                    pbar.set_postfix({
                        'nodes_created': f"{total_created:,}",
                        'errors': "Some failed"
                    })
                    logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"‚úÖ Successfully created {total_created:,} nodes")
        self.stats.created_nodes = total_created
    
    async def _create_relationships_ultra_fast(self):
        """Create all relationships using ultra-fast optimized approach"""
        if not self.relationships_by_type:
            logger.info("No relationships to create")
            return
        
        logger.info(f"‚ö° Creating {self.total_relationships:,} relationships using ultra-fast approach...")
        
        logger.info("üìä Relationship type distribution:")
        for rel_type, count in sorted(self.stats.relationship_types_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  {rel_type}: {count:,} relationships")
        
        created_count = await self.falkordb_manager.create_relationships_ultra_fast(self.relationships_by_type)
        self.stats.created_relationships = created_count
        
        logger.info(f"‚úÖ Successfully created {created_count:,} relationships")
    
    async def _create_indexes_with_discovered_labels(self):
        """Collect discovered labels and create indexes for them - non-blocking"""
        try:
            nodes = await self.node_manager.get_nodes()
            label_counts = Counter()
            
            for node_data in nodes.values():
                primary_label = node_data.get('primary_label')
                if primary_label:
                    label_counts[primary_label] += 1
            
            if not label_counts:
                logger.warning("No nodes with labels found - creating fallback indexes only")
                await self.falkordb_manager.create_indexes()
                return
            
            logger.info(f"Discovered {len(label_counts)} RDF class labels from {sum(label_counts.values()):,} nodes")
            
            discovered_labels = dict(label_counts)
            await self.falkordb_manager.create_indexes(discovered_labels)
            
        except Exception as e:
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ['already indexed', 'already exists', 'duplicate']):
                logger.info("‚ÑπÔ∏è  Index already exists - this is normal when running multiple queries")
                logger.info("üí° Conversion will continue successfully without recreating indexes")
            else:
                logger.warning(f"Index creation failed but conversion will continue: {e}")
                logger.info("üí° Tip: Indexes improve query performance but are not required for functionality")
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("üîç Validating conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("‚ö†Ô∏è  No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("‚ö†Ô∏è  No relationships were created despite processing relationship triples")
        
        logger.info(f"‚úÖ Validation complete: {falkor_stats['nodes']:,} nodes, {falkor_stats['relationships']:,} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        final_falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.final_nodes = final_falkor_stats['nodes']
        self.stats.final_relationships = final_falkor_stats['relationships']
        
        if self.config.append_to_existing_graph and not self.stats.graph_was_cleared:
            actual_nodes_created = self.stats.final_nodes - self.stats.initial_nodes
            actual_rels_created = self.stats.final_relationships - self.stats.initial_relationships
            
            if actual_nodes_created >= 0:
                self.stats.created_nodes = actual_nodes_created
            if actual_rels_created >= 0:
                self.stats.created_relationships = actual_rels_created
                
            nodes_processed = len(await self.node_manager.get_nodes())
            self.stats.skipped_nodes = max(0, nodes_processed - actual_nodes_created)
            self.stats.skipped_relationships = max(0, self.total_relationships - actual_rels_created)
        else:
            self.stats.created_nodes = self.stats.final_nodes
            self.stats.created_relationships = self.stats.final_relationships
        
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        unique_objects = set()
        for rel_list in self.relationships_by_type.values():
            for _, _, obj_uri in rel_list:
                unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            await self._export_stats()
    
    async def _log_performance_summary(self):
        """Log comprehensive performance summary"""
        if not self.stats.end_time:
            return
        
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        
        if len(self.stats.subject_classes.union(self.stats.object_classes)) > 0:
            all_classes = self.stats.subject_classes.union(self.stats.object_classes)
            sample_classes = sorted(list(all_classes))[:10]
            logger.info(f"üè∑Ô∏è  Discovered RDF Classes (sample):")
            for rdf_class in sample_classes:
                clean_label = self.uri_processor.process_uri(rdf_class)
                logger.info(f"   {rdf_class} ‚Üí :{clean_label}")
            if len(all_classes) > 10:
                logger.info(f"   ... and {len(all_classes) - 10} more classes")
            logger.info("")
        
        if self.stats.append_mode:
            logger.info("üéØ INCREMENTAL LOADING SUMMARY")
        else:
            logger.info("üéØ PERFORMANCE SUMMARY")
        logger.info("=" * 80)
        
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            logger.info(f"üìä Graph State:")
            logger.info(f"   Initial: {self.stats.initial_nodes:,} nodes, {self.stats.initial_relationships:,} relationships")
            logger.info(f"   Final: {self.stats.final_nodes:,} nodes, {self.stats.final_relationships:,} relationships")
            logger.info(f"   Added: {self.stats.created_nodes:,} nodes, {self.stats.created_relationships:,} relationships")
            if self.stats.skipped_nodes > 0 or self.stats.skipped_relationships > 0:
                logger.info(f"   Skipped (duplicates): {self.stats.skipped_nodes:,} nodes, {self.stats.skipped_relationships:,} relationships")
            logger.info(f"")
        
        logger.info(f"üìä Data Processed:")
        logger.info(f"   Total triples: {self.stats.total_triples_retrieved:,}")
        logger.info(f"   Property triples: {self.stats.property_triples:,}")
        logger.info(f"   Relationship triples: {self.stats.relationship_triples:,}")
        logger.info(f"   RDF Classes discovered: {len(self.stats.subject_classes.union(self.stats.object_classes))}")
        logger.info(f"   Predicates used: {len(self.stats.predicates_used)}")
        logger.info(f"")
        
        if not self.stats.append_mode or self.stats.graph_was_cleared:
            logger.info(f"üèóÔ∏è  Entities Created:")
            logger.info(f"   Nodes: {self.stats.created_nodes:,}")
            logger.info(f"   Relationships: {self.stats.created_relationships:,}")
        else:
            logger.info(f"üèóÔ∏è  Entities Added:")
            logger.info(f"   New nodes: {self.stats.created_nodes:,}")
            logger.info(f"   New relationships: {self.stats.created_relationships:,}")
            
        logger.info(f"   Relationship types: {len(self.stats.relationship_types_count)}")
        logger.info(f"")
        logger.info(f"‚è±Ô∏è  Performance Metrics:")
        logger.info(f"   Total time: {duration:.2f} seconds")
        logger.info(f"   Query execution: {self.stats.query_execution_time:.2f} seconds")
        logger.info(f"   Node creation: {self.stats.node_creation_time:.2f} seconds")
        logger.info(f"   Relationship creation: {self.stats.relationship_creation_time:.2f} seconds")
        logger.info(f"")
        logger.info(f"üöÄ Throughput Rates:")
        logger.info(f"   Overall: {self.stats.processed_triples/duration:.1f} triples/second")
        if self.stats.relationship_creation_rate > 0:
            logger.info(f"   Relationships: {self.stats.relationship_creation_rate:.1f} relationships/second")
        logger.info(f"")
        
        if self.stats.relationship_creation_rate > 1000:
            logger.info("üéâ EXCELLENT: Ultra-fast relationship creation (>1000 rel/sec)")
        elif self.stats.relationship_creation_rate > 500:
            logger.info("‚úÖ GOOD: Fast relationship creation (>500 rel/sec)")
        elif self.stats.relationship_creation_rate > 100:
            logger.info("‚ö†Ô∏è  ACCEPTABLE: Moderate relationship creation (>100 rel/sec)")
        else:
            logger.info("‚ùå SLOW: Consider optimizing configuration (<100 rel/sec)")
        
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            total_processed = self.stats.unique_subjects + len(set(obj for rels in self.relationships_by_type.values() for _, _, obj in rels))
            actual_added = self.stats.created_nodes + self.stats.created_relationships
            if total_processed > 0:
                efficiency = (actual_added / total_processed) * 100
                logger.info(f"üìà Incremental efficiency: {efficiency:.1f}% new data added")
                
                if efficiency > 80:
                    logger.info("üéâ HIGH efficiency: Mostly new data")
                elif efficiency > 50:
                    logger.info("‚úÖ GOOD efficiency: Good mix of new data")
                elif efficiency > 20:
                    logger.info("‚ö†Ô∏è  MODERATE efficiency: Some duplicate data")
                else:
                    logger.info("‚ùå LOW efficiency: Mostly duplicate data")
        
        logger.info("=" * 80)
    
    async def _export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"optimized_conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['config'] = {
                'batch_size': self.config.batch_size,
                'max_concurrent_batches': self.config.max_concurrent_batches,
                'connection_pool_size': self.config.connection_pool_size,
                'use_bulk_relationship_creation': self.config.use_bulk_relationship_creation,
                'disable_relationship_properties': self.config.disable_relationship_properties,
            }
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"üìÑ Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted RDF graph"""
        return [
            "MATCH (n) RETURN labels(n) as rdf_classes, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as rdf_predicates, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) WHERE n.rdf_type IS NOT NULL RETURN n.rdf_type, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri, labels(n) LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN labels(s), type(r), labels(o) LIMIT 10",
            "MATCH (n) RETURN labels(n) as class, size((n)--()) as degree ORDER BY degree DESC LIMIT 10",
            "MATCH (n) RETURN count(n) as total_nodes",
            "MATCH ()-[r]->() RETURN count(r) as total_relationships",
            "CALL db.labels() YIELD label RETURN label as discovered_rdf_classes",
            "CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType as rdf_predicates"
        ]


# Utility functions
def create_optimized_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph", 
                          append_mode: bool = True) -> OptimizedAsyncTripleConfig:
    """Create an optimized configuration for fast edge loading with incremental support"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        
        # Incremental loading settings - ALWAYS use append mode for multiple queries
        append_to_existing_graph=append_mode,
        clear_existing_graph=False,
        handle_duplicates=True,
        
        # Optimized settings for fast relationship creation
        batch_size=2000,
        max_concurrent_batches=3,
        connection_pool_size=10,
        sparql_timeout=7200,
        falkordb_timeout=300,
        
        # Speed optimizations
        preserve_uri_properties=False,
        disable_relationship_properties=True,
        group_relationships_by_type=True,
        use_bulk_relationship_creation=True,
        
        # Performance settings
        use_shortened_uris=True,
        create_indexes=True,
        validate_conversion=False,
        export_stats=True,
        progress_update_interval=25,
    )


def create_ultra_fast_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration optimized for 2M+ edges - MAXIMUM SPEED"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        
        # Ultra-fast settings for massive edge creation
        append_to_existing_graph=True,
        clear_existing_graph=False,
        handle_duplicates=False,  # SPEED: Skip duplicate checking for fresh data
        
        # MASSIVE batch sizes for 2M+ edges
        batch_size=50000,  # 50K edges per batch (much larger)
        max_concurrent_batches=1,  # Single-threaded for stability with large batches
        connection_pool_size=5,  # Smaller pool to avoid connection overhead
        sparql_timeout=7200,
        falkordb_timeout=1800,  # 30 minutes for massive batches
        
        # Maximum speed optimizations
        preserve_uri_properties=False,
        disable_relationship_properties=True,
        group_relationships_by_type=True,
        use_bulk_relationship_creation=True,
        
        # Minimal overhead settings
        use_shortened_uris=False,  # Skip URI processing for speed
        create_indexes=False,  # Skip indexes during loading, create after
        validate_conversion=False,
        export_stats=False,  # Skip stats export
        progress_update_interval=1,  # More frequent progress for large batches
        
        # Aggressive retry settings
        max_retries=2,  # Fewer retries for speed
        retry_delay=1,
    )


def create_fresh_graph_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for fresh graph creation (clears existing data)"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=False)
    config.clear_existing_graph = True
    config.handle_duplicates = False
    return config


def create_append_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for appending to existing graph with robust duplicate handling"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=True)
    config.append_to_existing_graph = True
    config.clear_existing_graph = False
    config.handle_duplicates = True
    config.skip_existing_nodes = True
    config.skip_existing_relationships = True
    return config


def create_safe_multi_query_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration specifically for multiple query runs with maximum safety"""
    config = create_append_config(sparql_endpoint, triples_query, graph_name)
    
    # Extra safety settings for multiple runs
    config.max_retries = 3
    config.retry_delay = 2
    config.validate_conversion = False
    config.export_stats = False
    config.create_indexes = True
    
    return config


async def clear_all_indexes(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """Clear all indexes from a graph - useful for fresh starts"""
    
    logger.info(f"üßπ Clearing all indexes from graph '{graph_name}'...")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        try:
            result = await falkordb_manager.execute_query_with_retry("CALL db.indexes()")
            if result.result_set:
                for row in result.result_set:
                    if len(row) > 0:
                        index_info = str(row[0])
                        if ":" in index_info and "(" in index_info:
                            label_part = index_info.split("(")[0].replace("INDEX", "").replace(":", "").strip()
                            if label_part:
                                try:
                                    drop_query = f"DROP INDEX FOR (n:{label_part}) ON (n.uri)"
                                    await falkordb_manager.execute_query_with_retry(drop_query)
                                    logger.info(f"‚úÖ Dropped index for label '{label_part}'")
                                except Exception as e:
                                    logger.debug(f"Could not drop index for {label_part}: {e}")
        except Exception as e:
            logger.info(f"Index clearing completed with some errors (this is normal): {e}")
        
        logger.info("‚úÖ Index clearing completed")
        
    except Exception as e:
        logger.warning(f"Index clearing failed: {e}")
    finally:
        await falkordb_manager.close()


async def run_multiple_queries_with_deduplication(sparql_endpoint: str, queries_list: List[str], 
                                                graph_name: str = "merged_graph", clear_indexes_first: bool = False):
    """Run multiple SPARQL queries and merge results with automatic deduplication
    
    Args:
        sparql_endpoint: SPARQL endpoint URL
        queries_list: List of SPARQL queries to run
        graph_name: Name of the target graph
        clear_indexes_first: If True, clears all indexes before starting (helps with index errors)
    """
    
    if clear_indexes_first:
        logger.info("üßπ Clearing existing indexes to avoid conflicts...")
        await clear_all_indexes(graph_name)
    
    logger.info(f"üîÑ Running {len(queries_list)} queries with automatic deduplication...")
    logger.info(f"üìä Target graph: '{graph_name}'")
    
    all_stats = []
    
    for i, query in enumerate(queries_list, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"üöÄ QUERY {i}/{len(queries_list)}")
        logger.info(f"{'='*60}")
        logger.info(f"Query preview: {query[:200]}...")
        
        try:
            config = create_safe_multi_query_config(sparql_endpoint, query, graph_name)
            converter = OptimizedAsyncTripleBasedConverter(config)
            stats = await converter.convert()
            all_stats.append(stats)
            
            logger.info(f"‚úÖ Query {i} completed: {stats.get_incremental_summary()}")
            
        except Exception as e:
            error_msg = str(e).lower()
            if "already indexed" in error_msg:
                logger.warning(f"‚ö†Ô∏è  Query {i} hit index conflict: {e}")
                logger.info("üí° This is usually not critical - data may still be processed")
                logger.info("üí° To avoid this, set clear_indexes_first=True or disable indexing")
            else:
                logger.error(f"‚ùå Query {i} failed: {e}")
            continue
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üéâ MULTIPLE QUERIES COMPLETED")
    logger.info(f"{'='*80}")
    
    if all_stats:
        final_stats = all_stats[-1]
        logger.info(f"üìä Final graph state: {final_stats.final_nodes:,} nodes, {final_stats.final_relationships:,} relationships")
        
        total_processed = sum(s.processed_triples for s in all_stats)
        total_new_nodes = sum(s.created_nodes for s in all_stats)
        total_new_relationships = sum(s.created_relationships for s in all_stats)
        
        logger.info(f"üìà Processing summary:")
        logger.info(f"   Total triples processed: {total_processed:,}")
        logger.info(f"   Total new nodes added: {total_new_nodes:,}")
        logger.info(f"   Total new relationships added: {total_new_relationships:,}")
        logger.info(f"   Successful queries: {len(all_stats)}/{len(queries_list)}")
        
        logger.info(f"\nüßπ Running final deduplication check...")
        await deduplicate_graph(graph_name)
        
        return all_stats
    else:
        logger.error("‚ùå No queries completed successfully")
        return []


async def deduplicate_graph(graph_name: str, falkordb_host: str = 'localhost', falkordb_port: int = 6379):
    """‚ú® AUTOMATIC DEDUPLICATION: Remove duplicate nodes and relationships with progress tracking"""
    
    logger.info(f"üßπ Starting automatic deduplication of graph '{graph_name}'...")
    logger.info("‚ú® This ensures your final graph has no duplicates!")
    
    config = OptimizedAsyncTripleConfig(
        triples_query="",
        sparql_endpoint="",
        graph_name=graph_name,
        falkordb_host=falkordb_host,
        falkordb_port=falkordb_port
    )
    
    falkordb_manager = OptimizedAsyncFalkorDBManager(config)
    
    try:
        await falkordb_manager.connect()
        
        initial_stats = await falkordb_manager.get_graph_stats()
        logger.info(f"üìä Before deduplication: {initial_stats['nodes']:,} nodes, {initial_stats['relationships']:,} relationships")
        
        # Progress tracking for deduplication steps
        dedup_steps = [
            ("üîç Scanning for duplicate nodes", "duplicate node detection"),
            ("üßπ Removing duplicate nodes", "node deduplication"),
            ("üîç Scanning for duplicate relationships", "duplicate relationship detection"),
            ("üßπ Removing duplicate relationships", "relationship deduplication"),
            ("üìä Finalizing cleanup", "cleanup finalization")
        ]
        
        with tqdm_sync(total=len(dedup_steps), desc="üßπ Deduplicating graph", 
                      unit="step", ncols=100, colour="magenta") as pbar:
            
            # Step 1-2: Node deduplication
            pbar.set_description(dedup_steps[0][0])
            pbar.update(1)
            
            pbar.set_description(dedup_steps[1][0])
            dedup_nodes_query = """
            MATCH (n)
            WITH n.uri as uri, collect(n) as nodes
            WHERE size(nodes) > 1
            WITH uri, nodes, 
                 [node in nodes | size(keys(node))] as prop_counts,
                 range(0, size(nodes)-1) as indices
            WITH uri, nodes, 
                 [i in indices | {node: nodes[i], props: prop_counts[i]}] as node_info
            WITH uri, node_info, 
                 reduce(max_props = -1, info in node_info | 
                       CASE WHEN info.props > max_props THEN info.props ELSE max_props END) as max_prop_count
            WITH uri, [info in node_info WHERE info.props = max_prop_count][0].node as keeper,
                 [info in node_info WHERE info.props < max_prop_count | info.node] as to_delete
            UNWIND to_delete as duplicate_node
            DETACH DELETE duplicate_node
            """
            
            try:
                await falkordb_manager.execute_query_with_retry(dedup_nodes_query)
                logger.info("‚úÖ Node deduplication completed")
            except Exception as e:
                logger.warning(f"Node deduplication query failed (may be no duplicates): {e}")
            
            pbar.update(1)
            
            # Step 3-4: Relationship deduplication
            pbar.set_description(dedup_steps[2][0])
            pbar.update(1)
            
            pbar.set_description(dedup_steps[3][0])
            dedup_rels_query = """
            MATCH (a)-[r]->(b)
            WITH a, b, type(r) as rel_type, collect(r) as rels
            WHERE size(rels) > 1
            WITH a, b, rel_type, rels[1..] as duplicates
            UNWIND duplicates as duplicate_rel
            DELETE duplicate_rel
            """
            
            try:
                await falkordb_manager.execute_query_with_retry(dedup_rels_query)
                logger.info("‚úÖ Relationship deduplication completed")
            except Exception as e:
                logger.warning(f"Relationship deduplication query failed (may be no duplicates): {e}")
            
            pbar.update(1)
            
            # Step 5: Finalization
            pbar.set_description(dedup_steps[4][0])
            final_stats = await falkordb_manager.get_graph_stats()
            pbar.update(1)
        
        logger.info(f"üìä After deduplication: {final_stats['nodes']:,} nodes, {final_stats['relationships']:,} relationships")
        
        nodes_removed = initial_stats['nodes'] - final_stats['nodes']
        rels_removed = initial_stats['relationships'] - final_stats['relationships']
        
        if nodes_removed > 0 or rels_removed > 0:
            logger.info(f"üßπ DEDUPLICATION RESULTS:")
            logger.info(f"   ‚úÖ Removed {nodes_removed:,} duplicate nodes")
            logger.info(f"   ‚úÖ Removed {rels_removed:,} duplicate relationships")
            logger.info(f"   ‚ú® Your graph is now completely deduplicated!")
        else:
            logger.info("‚ú® No duplicates found - your graph was already perfectly clean!")
        
        return {
            'initial_nodes': initial_stats['nodes'],
            'initial_relationships': initial_stats['relationships'],
            'final_nodes': final_stats['nodes'],
            'final_relationships': final_stats['relationships'],
            'nodes_removed': nodes_removed,
            'relationships_removed': rels_removed
        }
        
    except Exception as e:
        logger.error(f"‚ùå Deduplication failed: {e}")
        raise
    finally:
        await falkordb_manager.close()


async def run_ultra_fast_conversion(sparql_endpoint: str, queries_list: List[str], 
                                 graph_name: str = "ultra_fast_graph") -> List[AsyncConversionStats]:
    """
    üöÑ ULTRA-FAST MODE: Optimized for 2M+ edges with automatic deduplication
    
    This mode sacrifices some safety features for maximum performance:
    - Uses massive batch sizes (50K-100K edges per batch)
    - Skips duplicate checking during load (for maximum speed)
    - Minimal indexing during load (creates indexes after)
    - Single-threaded processing for stability with large batches
    
    ‚ú® AUTOMATIC DEDUPLICATION: Still runs comprehensive deduplication at the end!
    
    Best for: Clean data with 2M+ edges where speed is critical
    """
    
    logger.info("üöÑ ULTRA-FAST MODE: Optimized for 2M+ edges")
    logger.info("=" * 60)
    logger.info("‚ö° Ultra-fast optimizations enabled:")
    logger.info("   üöÑ Mega-batch processing (50K-100K edges/batch)")
    logger.info("   ‚ö° CREATE-only mode during load (maximum speed)")
    logger.info("   üéØ Single-threaded for stability")
    logger.info("   üìä Minimal overhead during processing")
    logger.info("   üèóÔ∏è  Indexes created after loading")
    logger.info("   ‚ú® AUTOMATIC DEDUPLICATION at the end")
    
    all_stats = []
    
    # Process each query with progress tracking
    with tqdm_sync(total=len(queries_list), desc="üöÑ Ultra-fast queries", 
                  unit="query", ncols=100, colour="red") as query_pbar:
        
        for i, query in enumerate(queries_list, 1):
            query_pbar.set_description(f"üöÑ Ultra-fast query {i}/{len(queries_list)}")
            logger.info(f"\n{'='*60}")
            logger.info(f"üöÑ ULTRA-FAST QUERY {i}/{len(queries_list)}")
            logger.info(f"{'='*60}")
            logger.info(f"Query preview: {query[:200]}...")
            
            try:
                # Use ultra-fast configuration
                config = create_ultra_fast_config(sparql_endpoint, query, graph_name)
                
                # Create converter with ultra-fast settings
                converter = OptimizedAsyncTripleBasedConverter(config)
                stats = await converter.convert()
                all_stats.append(stats)
                
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'total_edges': f"{sum(s.created_relationships for s in all_stats):,}",
                    'rate': f"{stats.relationship_creation_rate:.0f}/s" if stats.relationship_creation_rate > 0 else "N/A"
                })
                
                logger.info(f"üöÑ Ultra-fast query {i} completed: {stats.get_incremental_summary()}")
                
                # Show edge creation performance
                if stats.relationship_creation_rate > 0:
                    logger.info(f"‚ö° Edge creation rate: {stats.relationship_creation_rate:.0f} relationships/second")
                
            except Exception as e:
                logger.error(f"‚ùå Ultra-fast query {i} failed: {e}")
                query_pbar.update(1)
                query_pbar.set_postfix({
                    'completed': f"{len(all_stats)}/{len(queries_list)}",
                    'failed': f"{i - len(all_stats)}"
                })
                continue
    
    # Create indexes after all data is loaded (much faster)
    if all_stats:
        logger.info("\nüèóÔ∏è  Creating indexes after ultra-fast loading...")
        try:
            # Use the last converter config to create indexes
            final_config = create_ultra_fast_config(sparql_endpoint, queries_list[0], graph_name)
            final_config.create_indexes = True  # Enable indexing
            
            falkordb_manager = OptimizedAsyncFalkorDBManager(final_config)
            await falkordb_manager.connect()
            
            # Get final stats for index creation
            final_stats = await falkordb_manager.get_graph_stats()
            logger.info(f"üìä Pre-deduplication: {final_stats['nodes']:,} nodes, {final_stats['relationships']:,} relationships")
            
            # Create basic indexes for performance
            await falkordb_manager.create_index_safely("Resource")  # Fallback index
            
            await falkordb_manager.close()
            logger.info("‚úÖ Post-loading indexing completed")
            
        except Exception as e:
            logger.warning(f"Post-loading indexing failed: {e}")
    
    # ‚ú® AUTOMATIC DEDUPLICATION HAPPENS HERE ‚ú®
    if all_stats:
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® AUTOMATIC DEDUPLICATION STARTING")
        logger.info(f"{'='*80}")
        logger.info("üîÑ Running comprehensive deduplication on ultra-fast loaded data...")
        
        dedup_results = await deduplicate_graph(graph_name)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"‚ú® ULTRA-FAST + DEDUPLICATION COMPLETED")
        logger.info(f"{'='*80}")
    
    # Summary
    logger.info(f"\n{'='*80}")
    logger.info(f"üöÑ ULTRA-FAST MODE COMPLETED")
    logger.info(f"{'='*80}")
    
    if all_stats:
        total_edges = sum(s.created_relationships for s in all_stats)
        total_time = sum((s.end_time - s.start_time).total_seconds() for s in all_stats if s.end_time)
        
        logger.info(f"üìä Ultra-fast results:")
        logger.info(f"   Total edges created: {total_edges:,}")
        logger.info(f"   Total processing time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
        logger.info(f"   Overall edge rate: {total_edges/total_time:.0f} edges/second")
        logger.info(f"   Successful queries: {len(all_stats)}/{len(queries_list)}")
        
        if 'dedup_results' in locals():
            logger.info(f"   Final clean graph: {dedup_results['final_nodes']:,} nodes, {dedup_results['final_relationships']:,} relationships")
            if dedup_results['nodes_removed'] > 0 or dedup_results['relationships_removed'] > 0:
                logger.info(f"   Removed duplicates: {dedup_results['nodes_removed']:,} nodes, {dedup_results['relationships_removed']:,} relationships")
            else:
                logger.info(f"   ‚ú® No duplicates found - data was perfectly clean!")
        
        # Performance assessment for ultra-large scale
        edge_rate = total_edges / total_time if total_time > 0 else 0
        if edge_rate > 10000:
            logger.info("üöÄ OUTSTANDING: Ultra-high-speed edge processing (>10K edges/sec)")
        elif edge_rate > 5000:
            logger.info("üéâ EXCELLENT: High-speed edge processing (>5K edges/sec)")
        elif edge_rate > 2000:
            logger.info("‚úÖ GOOD: Fast edge processing (>2K edges/sec)")
        else:
            logger.info("‚ö†Ô∏è  MODERATE: Consider optimizing system resources")
        
        return all_stats
    else:
        logger.error("‚ùå No ultra-fast queries completed successfully")
        return []


async def estimate_processing_time(sparql_endpoint: str, sample_query: str, total_expected_edges: int) -> Dict[str, float]:
    """
    ‚è±Ô∏è ESTIMATE PROCESSING TIME: Test performance with a sample to predict total time
    
    Args:
        sparql_endpoint: Your SPARQL endpoint
        sample_query: Sample query with LIMIT 10000 for testing
        total_expected_edges: Your estimated total number of edges across all queries
        
    Returns:
        Dictionary with time estimates for different modes
    """
    
    logger.info("‚è±Ô∏è  PERFORMANCE ESTIMATION")
    logger.info("=" * 50)
    logger.info(f"üß™ Testing with sample query to estimate performance...")
    logger.info(f"üìä Expected total edges: {total_expected_edges:,}")
    
    try:
        # Test with standard config
        start_time = time.time()
        config = create_optimized_config(sparql_endpoint, sample_query, "performance_test")
        converter = OptimizedAsyncTripleBasedConverter(config)
        stats = await converter.convert()
        standard_duration = time.time() - start_time
        
        # Calculate rates
        sample_edges = stats.created_relationships
        sample_rate = sample_edges / standard_duration if standard_duration > 0 else 0
        
        logger.info(f"üß™ Sample results:")
        logger.info(f"   Sample edges: {sample_edges:,}")
        logger.info(f"   Sample time: {standard_duration:.1f} seconds")
        logger.info(f"   Sample rate: {sample_rate:.0f} edges/second")
        
        # Estimate different scenarios
        estimates = {}
        
        if sample_rate > 0:
            # Standard mode estimate
            standard_time = total_expected_edges / sample_rate
            estimates['standard_mode_seconds'] = standard_time
            estimates['standard_mode_minutes'] = standard_time / 60
            estimates['standard_mode_hours'] = standard_time / 3600
            
            # Ultra-fast mode estimate (3-5x faster)
            ultra_fast_rate = sample_rate * 4  # Conservative 4x improvement
            ultra_fast_time = total_expected_edges / ultra_fast_rate
            estimates['ultra_fast_mode_seconds'] = ultra_fast_time
            estimates['ultra_fast_mode_minutes'] = ultra_fast_time / 60
            estimates['ultra_fast_mode_hours'] = ultra_fast_time / 3600
            
            # Best case estimate (optimal conditions)
            best_case_rate = sample_rate * 8  # Optimistic 8x improvement
            best_case_time = total_expected_edges / best_case_rate
            estimates['best_case_seconds'] = best_case_time
            estimates['best_case_minutes'] = best_case_time / 60
            estimates['best_case_hours'] = best_case_time / 3600
            
            logger.info(f"\nüìä TIME ESTIMATES for {total_expected_edges:,} edges:")
            logger.info(f"   Standard mode: {estimates['standard_mode_hours']:.1f} hours ({estimates['standard_mode_minutes']:.0f} minutes)")
            logger.info(f"   Ultra-fast mode: {estimates['ultra_fast_mode_hours']:.1f} hours ({estimates['ultra_fast_mode_minutes']:.0f} minutes)")
            logger.info(f"   Best case: {estimates['best_case_hours']:.1f} hours ({estimates['best_case_minutes']:.0f} minutes)")
            
            # Recommendations
            logger.info(f"\nüí° RECOMMENDATIONS:")
            if estimates['ultra_fast_mode_hours'] > 8:
                logger.info("   üöÑ Use run_ultra_fast_conversion() for maximum speed")
                logger.info("   üì¶ Consider processing in smaller batches")
                logger.info("   üåô Run during off-peak hours")
            elif estimates['ultra_fast_mode_hours'] > 2:
                logger.info("   ‚ö° Use run_ultra_fast_conversion() recommended")
                logger.info("   üìä Monitor progress during processing")
            else:
                logger.info("   ‚úÖ Standard mode should work fine")
                logger.info("   üîß Either mode will complete in reasonable time")
        
async def run_queries_simple(endpoint: str, queries: List[str], graph_name: str = "my_graph") -> bool:
    """
    üöÄ SIMPLE INTERFACE: Run multiple queries with all error handling built-in
    
    This function handles ALL potential issues automatically:
    - Index conflicts ‚úÖ
    - SPARQL connection errors ‚úÖ  
    - URI merging ‚úÖ
    - Deduplication ‚úÖ
    - Error recovery ‚úÖ
    
    Args:
        endpoint: Your SPARQL endpoint URL
        queries: List of SPARQL queries (must return 6 variables each)
        graph_name: Name for your FalkorDB graph
        
    Returns:
        True if at least one query succeeded, False if all failed
    """
    
    logger.info("üöÄ STARTING SIMPLE MULTI-QUERY INTERFACE")
    logger.info("=" * 60)
    logger.info("‚ú® Automatic features enabled:")
    logger.info("   ‚úÖ Index conflict prevention")
    logger.info("   ‚úÖ URI-based node merging") 
    logger.info("   ‚úÖ Duplicate relationship prevention")
    logger.info("   ‚úÖ Error recovery (failed queries don't stop others)")
    logger.info("   ‚úÖ Final deduplication cleanup")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        success = len(stats_list) > 0
        
        if success:
            final_stats = stats_list[-1]
            logger.info("\nüéâ SIMPLE INTERFACE: SUCCESS!")
            logger.info(f"üìä Your graph '{graph_name}' has {final_stats.final_nodes:,} nodes and {final_stats.final_relationships:,} relationships")
            logger.info(f"‚úÖ {len(stats_list)}/{len(queries)} queries completed successfully")
            
            logger.info("\nüîç To explore your data, connect to FalkorDB and run:")
            logger.info(f"   MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10")
            logger.info(f"   MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10")
            
        else:
            logger.error("\n‚ùå SIMPLE INTERFACE: All queries failed")
            logger.info("üí° Check your SPARQL endpoint and query syntax")
            
        return success
        
    except Exception as e:
        logger.error(f"‚ùå Simple interface failed: {e}")
        
        if "already indexed" in str(e).lower():
            logger.info("üí° Index error detected. The system should handle this automatically.")
            logger.info("üí° If this persists, try using a different graph_name")
        elif "sparql" in str(e).lower() or "endpoint" in str(e).lower():
            logger.info("üí° SPARQL connection issue. Check your endpoint URL and network connectivity")
        else:
            logger.info("üí° Unexpected error. Check your query syntax and FalkorDB connection")
        
        return False


# üöÑ ULTRA-FAST MODE EXAMPLES FOR 2M+ EDGES:
"""
üöÑ OPTIMIZED FOR 2M+ EDGES - QUICK START EXAMPLES:

# Example 1: Ultra-fast mode (RECOMMENDED for 2M+ edges)
queries = ["YOUR_LARGE_QUERY_1", "YOUR_LARGE_QUERY_2"]
stats = await run_ultra_fast_conversion(
    sparql_endpoint="https://your-endpoint.com/sparql",
    queries_list=queries,
    graph_name="massive_graph"
)

# Example 2: Estimate processing time first
sample_query = "YOUR_QUERY WITH LIMIT 10000"  # Add LIMIT for testing
estimates = await estimate_processing_time(
    sparql_endpoint="https://your-endpoint.com/sparql",
    sample_query=sample_query,
    total_expected_edges=2000000  # Your estimated edge count
)
print(f"Estimated time: {estimates.get('ultra_fast_mode_hours', 0):.1f} hours")

# Example 3: Manual ultra-fast configuration
config = create_ultra_fast_config(endpoint, query, "fast_graph")
converter = OptimizedAsyncTripleBasedConverter(config)
stats = await converter.convert()

PERFORMANCE TIPS FOR 2M+ EDGES:
‚úÖ Use run_ultra_fast_conversion() for maximum speed
‚úÖ Test with estimate_processing_time() first  
‚úÖ Ensure clean data (no duplicates) for ultra-fast mode
‚úÖ Run during off-peak hours for public endpoints
‚úÖ Monitor system resources (RAM, CPU)
‚úÖ Consider processing in smaller chunks if needed

EXPECTED PERFORMANCE:
- 2M edges: 5-15 minutes with ultra-fast mode
- 5M edges: 15-45 minutes with ultra-fast mode  
- 10M edges: 30-90 minutes with ultra-fast mode
- Rates: 2,000-10,000+ edges/second depending on system
"""
    """
    üöÄ SIMPLE INTERFACE: Run multiple queries with all error handling built-in
    
    This function handles ALL potential issues automatically:
    - Index conflicts ‚úÖ
    - SPARQL connection errors ‚úÖ  
    - URI merging ‚úÖ
    - Deduplication ‚úÖ
    - Error recovery ‚úÖ
    
    Args:
        endpoint: Your SPARQL endpoint URL
        queries: List of SPARQL queries (must return 6 variables each)
        graph_name: Name for your FalkorDB graph
        
    Returns:
        True if at least one query succeeded, False if all failed
    """
    
    logger.info("üöÄ STARTING SIMPLE MULTI-QUERY INTERFACE")
    logger.info("=" * 60)
    logger.info("‚ú® Automatic features enabled:")
    logger.info("   ‚úÖ Index conflict prevention")
    logger.info("   ‚úÖ URI-based node merging") 
    logger.info("   ‚úÖ Duplicate relationship prevention")
    logger.info("   ‚úÖ Error recovery (failed queries don't stop others)")
    logger.info("   ‚úÖ Final deduplication cleanup")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        success = len(stats_list) > 0
        
        if success:
            final_stats = stats_list[-1]
            logger.info("\nüéâ SIMPLE INTERFACE: SUCCESS!")
            logger.info(f"üìä Your graph '{graph_name}' has {final_stats.final_nodes:,} nodes and {final_stats.final_relationships:,} relationships")
            logger.info(f"‚úÖ {len(stats_list)}/{len(queries)} queries completed successfully")
            
            logger.info("\nüîç To explore your data, connect to FalkorDB and run:")
            logger.info(f"   MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10")
            logger.info(f"   MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10")
            
        else:
            logger.error("\n‚ùå SIMPLE INTERFACE: All queries failed")
            logger.info("üí° Check your SPARQL endpoint and query syntax")
            
        return success
        
    except Exception as e:
        logger.error(f"‚ùå Simple interface failed: {e}")
        
        if "already indexed" in str(e).lower():
            logger.info("üí° Index error detected. The system should handle this automatically.")
            logger.info("üí° If this persists, try using a different graph_name")
        elif "sparql" in str(e).lower() or "endpoint" in str(e).lower():
            logger.info("üí° SPARQL connection issue. Check your endpoint URL and network connectivity")
        else:
            logger.info("üí° Unexpected error. Check your query syntax and FalkorDB connection")
        
        return False


async def example_multiple_queries():
    """Example of running multiple queries with automatic merging and deduplication"""
    
    endpoint = "https://dbpedia.org/sparql"
    graph_name = "multi_query_graph"
    
    queries = [
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/A"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/B"))
            FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
        }
        LIMIT 50
        """,
        
        """
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            FILTER(?predicate = <http://dbpedia.org/ontology/birthPlace>)
        }
        LIMIT 30
        """
    ]
    
    logger.info("üöÄ EXAMPLE: Multiple Queries with Automatic Merging")
    logger.info("=" * 60)
    logger.info("üí° This example shows how to:")
    logger.info("   1. Run multiple SPARQL queries")
    logger.info("   2. Automatically merge results into one graph")
    logger.info("   3. Handle duplicate data seamlessly")
    logger.info("   4. Perform final deduplication")
    logger.info("   5. Handle index conflicts gracefully")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        if stats_list:
            logger.info("\nüéâ SUCCESS: All queries completed and merged!")
            logger.info(f"üìä Final graph '{graph_name}' is ready for use")
            
            logger.info("\nüîç Sample queries to explore your merged data:")
            sample_queries = [
                f"MATCH (n) RETURN labels(n), count(n) ORDER BY count(n) DESC LIMIT 10",
                f"MATCH ()-[r]->() RETURN type(r), count(r) ORDER BY count(r) DESC LIMIT 10", 
                f"MATCH (n) RETURN n.uri, labels(n) LIMIT 5"
            ]
            
            for i, query in enumerate(sample_queries, 1):
                logger.info(f"   {i}. {query}")
        
        return stats_list
        
    except Exception as e:
        logger.error(f"‚ùå Multiple queries example failed: {e}")
        logger.info("üí° If you're getting index errors, try setting clear_indexes_first=True")
        raise


async def your_custom_multiple_queries():
    """CUSTOMIZE THIS FUNCTION with your own queries and endpoint"""
    
    endpoint = "YOUR_SPARQL_ENDPOINT_HERE"
    graph_name = "your_merged_graph"
    
    your_queries = [
        """
        # QUERY 1: Replace with your first SPARQL query
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            # Add your specific filters here
        }
        LIMIT 1000
        """,
        
        """
        # QUERY 2: Replace with your second SPARQL query
        SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
            ?subject ?predicate ?object .
            OPTIONAL { ?subject a ?subjectClass }
            OPTIONAL { ?predicate a ?predicateClass }
            OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
            # Add your specific filters here
        }
        LIMIT 1000
        """,
    ]
    
    if endpoint == "YOUR_SPARQL_ENDPOINT_HERE":
        logger.error("‚ùå Please configure your SPARQL endpoint and queries!")
        logger.info("Edit the your_custom_multiple_queries() function with:")
        logger.info("1. endpoint = 'https://your-endpoint.com/sparql'")
        logger.info("2. Replace the example queries with your actual SPARQL queries")
        logger.info("3. graph_name = 'your_graph_name'")
        return
    
    if any("Replace with your" in query for query in your_queries):
        logger.error("‚ùå Please replace the example queries with your actual SPARQL queries!")
        logger.info("Each query must return: ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass")
        return
    
    logger.info("üöÄ RUNNING YOUR CUSTOM MULTIPLE QUERIES")
    logger.info("=" * 60)
    logger.info(f"üì° Endpoint: {endpoint}")
    logger.info(f"üìä Target graph: {graph_name}")
    logger.info(f"üìù Number of queries: {len(your_queries)}")
    logger.info("‚ú® Features enabled:")
    logger.info("   ‚úÖ Automatic deduplication")
    logger.info("   ‚úÖ Incremental merging")
    logger.info("   ‚úÖ Index reuse")
    logger.info("   ‚úÖ Error recovery")
    
    try:
        stats_list = await run_multiple_queries_with_deduplication(
            sparql_endpoint=endpoint,
            queries_list=your_queries,
            graph_name=graph_name,
            clear_indexes_first=True
        )
        
        if stats_list:
            logger.info(f"\nüéâ SUCCESS: All your queries completed!")
            logger.info(f"üìä Your graph '{graph_name}' contains merged and deduplicated data")
            
            final_config = create_append_config(endpoint, your_queries[0], graph_name)
            converter = OptimizedAsyncTripleBasedConverter(final_config)
            
            logger.info("\nüîç Sample queries to explore your merged data:")
            for i, query in enumerate(converter.get_sample_queries()[:5], 1):
                logger.info(f"   {i}. {query}")
        
        return stats_list
        
    except Exception as e:
        logger.error(f"‚ùå Your custom queries failed: {e}")
        import traceback
        traceback.print_exc()
        raise


async def main():
    """Example usage of the converter - customize with your own endpoint and query"""
    
    sparql_endpoint = "https://dbpedia.org/sparql"
    graph_name = "your_graph_name"
    
    your_custom_query = """
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
        FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/"))
        FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
    }
    """
    
    logger.info("üöÄ Starting Optimized FalkorDB RDF Conversion")
    logger.info("=" * 70)
    logger.info(f"üì° Endpoint: {sparql_endpoint}")
    logger.info(f"üìä Graph: {graph_name}")
    logger.info("üîß All fixes applied - ready for production use!")
    
    try:
        config = create_append_config(sparql_endpoint, your_custom_query, graph_name)
        converter = OptimizedAsyncTripleBasedConverter(config)
        stats = await converter.convert()
        
        logger.info("\n" + "üéâ CONVERSION COMPLETED SUCCESSFULLY!")
        logger.info("=" * 70)
        logger.info(f"üìä {stats.get_incremental_summary()}")
        logger.info("")
        logger.info("Sample queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            logger.info(f"{i}. {query}")
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    # üöÑ CHOOSE YOUR EXECUTION MODE (OPTIMIZED FOR 2M+ EDGES):
    
    # Option 1: Single query conversion
    # asyncio.run(main())
    
    # Option 2: Standard multiple queries (good for <1M edges)
    # asyncio.run(example_multiple_queries())
    
    # Option 3: üöÑ ULTRA-FAST MODE (RECOMMENDED for 2M+ edges)
    # queries = ["YOUR_QUERY_1", "YOUR_QUERY_2"]  # Replace with your queries
    # asyncio.run(run_ultra_fast_conversion("YOUR_ENDPOINT", queries, "ultra_fast_graph"))
    
    # Option 4: ‚è±Ô∏è ESTIMATE TIME FIRST (recommended before processing 2M+ edges)
    # sample_query = "YOUR_QUERY WITH LIMIT 10000"  # Add LIMIT for testing
    # asyncio.run(estimate_processing_time("YOUR_ENDPOINT", sample_query, 2000000))
    
    # Option 5: Custom multiple queries
    # asyncio.run(your_custom_multiple_queries())
    
    # Option 6: Simple interface
    # queries = ["YOUR_QUERY_1", "YOUR_QUERY_2"]
    # asyncio.run(run_queries_simple("YOUR_ENDPOINT", queries, "your_graph"))
    
    # DEFAULT: Run example to see the system in action
    asyncio.run(example_multiple_queries())
    
    # üí° FOR 2M+ EDGES: Uncomment Option 3 or 4 above and replace with your data


# üìù QUICK START GUIDE WITH AUTOMATIC DEDUPLICATION:
"""
üöÄ QUICK START EXAMPLES:

# Install dependencies first:
pip install rdflib falkordb redis asyncio tqdm

# Example 1: Simple usage with automatic deduplication
queries = [
    "SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE { ... }",
    "SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE { ... }"
]
success = await run_queries_simple("https://your-endpoint.com/sparql", queries, "my_data")

# Example 2: Ultra-fast for 2M+ edges (with automatic deduplication)
stats = await run_ultra_fast_conversion(
    "https://your-endpoint.com/sparql", 
    queries, 
    "massive_graph"
)

# Example 3: Standard mode with progress bars and deduplication
stats = await run_multiple_queries_with_deduplication(
    sparql_endpoint="https://your-endpoint.com/sparql",
    queries_list=queries,
    graph_name="my_graph",
    clear_indexes_first=True  # Prevents index conflicts
)

WHAT YOU'LL SEE:
üìä Beautiful progress bars for each step:
üîÑ Processing triple batches ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% 
üèóÔ∏è  Creating node batches   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
‚ö° Creating mega-batches    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% 
üßπ Deduplicating graph     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%

‚ú® AUTOMATIC DEDUPLICATION GUARANTEE:
- After all queries complete, the system AUTOMATICALLY runs deduplication
- Removes duplicate nodes (same URI) keeping the one with most properties  
- Removes duplicate relationships
- Final graph is guaranteed to be clean with no duplicates
- Progress bars show exactly what's happening

PERFORMANCE FOR 2M+ EDGES:
üöÑ Ultra-fast mode: 5-15 minutes
‚ö° Standard mode: 15-45 minutes  
üìä Real-time progress with rates (e.g., "5,000 edges/sec")

REMEMBER:
- Each query MUST return: ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass
- Same URIs across queries will be automatically merged
- Deduplication happens automatically at the end
- Progress bars show real-time status and performance
- The system handles all technical details for you
"""
