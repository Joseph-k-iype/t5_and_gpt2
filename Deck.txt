#!/usr/bin/env python3
"""
Enhanced GDPR Record of Processing Activities (RoPA) Metamodel RAG System
Specialized for Global Financial Institutions

Features:
- Global data protection law analysis (GDPR, CCPA, PIPEDA, LGPD, etc.)
- Financial sector specific RoPA requirements
- Multi-jurisdictional metamodel generation
- Advanced regulatory concept extraction
- Iterative document understanding
- Comprehensive compliance reporting

REQUIRED ENVIRONMENT VARIABLES:
    OPENAI_API_KEY=your_openai_api_key
    OPENAI_BASE_URL=your_custom_openai_endpoint (optional)
    ELASTICSEARCH_HOST=https://your-elasticsearch-cluster.com:9200
    FALKORDB_HOST=localhost
    FALKORDB_PORT=6379

USAGE:
    python gdpr_ropa_system.py --ingest /path/to/gdpr/documents
    python gdpr_ropa_system.py --analyze
    python gdpr_ropa_system.py --generate-metamodel
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, TypedDict, Annotated
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum

# Core dependencies
import pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GDPR-Based Data Protection Laws and Jurisdictions
class DataProtectionJurisdiction(Enum):
    GDPR_EU = "gdpr_eu"
    UK_GDPR = "uk_gdpr"
    GDPR_BASED_CALIFORNIA = "gdpr_based_california"  # CCPA mapped to GDPR principles
    GDPR_BASED_CANADA = "gdpr_based_canada"  # PIPEDA mapped to GDPR principles
    GDPR_BASED_BRAZIL = "gdpr_based_brazil"  # LGPD mapped to GDPR principles
    GDPR_BASED_ASIA = "gdpr_based_asia"  # APPI, PDPA mapped to GDPR principles
    GDPR_BASED_OTHERS = "gdpr_based_others"  # Other jurisdictions using GDPR as base

@dataclass
class JurisdictionRequirements:
    name: str
    key_principles: List[str]
    ropa_requirements: List[str]
    financial_specifics: List[str]
    penalties: str
    data_subject_rights: List[str]

# GDPR-centric jurisdiction mappings
JURISDICTION_REQUIREMENTS = {
    DataProtectionJurisdiction.GDPR_EU: JurisdictionRequirements(
        name="European Union GDPR",
        key_principles=["lawfulness", "fairness", "transparency", "purpose_limitation", "data_minimization", "accuracy", "storage_limitation", "integrity_confidentiality", "accountability"],
        ropa_requirements=["controller_details", "purposes", "data_categories", "data_subjects", "recipients", "transfers", "retention", "security_measures"],
        financial_specifics=["financial_data_categories", "credit_scoring", "fraud_detection", "know_your_customer", "anti_money_laundering"],
        penalties="Up to €20 million or 4% of annual global turnover",
        data_subject_rights=["access", "rectification", "erasure", "portability", "restriction", "objection", "automated_decision_making"]
    ),
    DataProtectionJurisdiction.UK_GDPR: JurisdictionRequirements(
        name="United Kingdom GDPR",
        key_principles=["lawfulness", "fairness", "transparency", "purpose_limitation", "data_minimization", "accuracy", "storage_limitation", "integrity_confidentiality", "accountability"],
        ropa_requirements=["controller_details", "purposes", "data_categories", "data_subjects", "recipients", "transfers", "retention", "security_measures"],
        financial_specifics=["financial_data_categories", "credit_scoring", "fraud_detection", "know_your_customer", "anti_money_laundering"],
        penalties="Up to £17.5 million or 4% of annual global turnover",
        data_subject_rights=["access", "rectification", "erasure", "portability", "restriction", "objection", "automated_decision_making"]
    ),
    DataProtectionJurisdiction.GDPR_BASED_CALIFORNIA: JurisdictionRequirements(
        name="GDPR-based California Consumer Privacy Act",
        key_principles=["transparency", "consumer_control", "non_discrimination", "gdpr_alignment"],
        ropa_requirements=["categories_collected", "purposes", "sources", "third_parties", "business_purposes", "gdpr_style_documentation"],
        financial_specifics=["financial_information", "account_details", "transaction_history", "credit_information"],
        penalties="Up to $7,500 per violation (mapped to GDPR framework)",
        data_subject_rights=["know", "delete", "opt_out", "non_discrimination", "gdpr_equivalent_rights"]
    ),
    DataProtectionJurisdiction.GDPR_BASED_CANADA: JurisdictionRequirements(
        name="GDPR-based Personal Information Protection Act",
        key_principles=["accountability", "identifying_purposes", "consent", "limiting_collection", "limiting_use_disclosure", "accuracy", "safeguards", "openness", "individual_access", "challenging_compliance"],
        ropa_requirements=["personal_information_types", "collection_purposes", "use_disclosure", "consent_mechanisms", "retention_periods", "gdpr_alignment"],
        financial_specifics=["banking_information", "investment_data", "insurance_records", "credit_reporting"],
        penalties="Up to CAD $100,000 (with GDPR-style escalation)",
        data_subject_rights=["access", "correction", "withdrawal_consent", "gdpr_equivalent_rights"]
    ),
    DataProtectionJurisdiction.GDPR_BASED_BRAZIL: JurisdictionRequirements(
        name="GDPR-based Lei Geral de Proteção de Dados",
        key_principles=["good_faith", "purpose", "adequacy", "necessity", "free_access", "data_quality", "transparency", "security", "prevention", "non_discrimination", "accountability"],
        ropa_requirements=["processing_purposes", "data_categories", "data_subjects", "data_sharing", "retention", "security_measures", "gdpr_alignment"],
        financial_specifics=["financial_transactions", "credit_analysis", "risk_assessment", "payment_processing"],
        penalties="Up to 2% of revenue or R$50 million (GDPR-style calculation)",
        data_subject_rights=["confirmation", "access", "correction", "anonymization", "deletion", "portability", "information"]
    ),
    DataProtectionJurisdiction.GDPR_BASED_ASIA: JurisdictionRequirements(
        name="GDPR-based Asian Privacy Laws (APPI, PDPA)",
        key_principles=["consent", "purpose_limitation", "data_minimization", "security", "accountability", "gdpr_alignment"],
        ropa_requirements=["processing_purposes", "data_categories", "consent_records", "security_measures", "gdpr_style_documentation"],
        financial_specifics=["financial_services_data", "banking_records", "payment_information"],
        penalties="Jurisdiction-specific penalties with GDPR-style framework",
        data_subject_rights=["access", "correction", "deletion", "data_portability", "gdpr_equivalent_rights"]
    )
}

class RopaMetamodelState(TypedDict):
    """Enhanced state for RoPA metamodel generation"""
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Dict[str, Any]]
    extracted_concepts: List[Dict[str, Any]]
    jurisdictional_mappings: Dict[str, Any]
    regulatory_entities: List[Dict[str, Any]]
    processing_activities: List[Dict[str, Any]]
    data_categories: List[Dict[str, Any]]
    legal_bases: List[Dict[str, Any]]
    cross_border_transfers: List[Dict[str, Any]]
    security_measures: List[Dict[str, Any]]
    retention_policies: List[Dict[str, Any]]
    metamodel_structure: Dict[str, Any]
    compliance_gaps: List[Dict[str, Any]]
    reasoning_trace: List[str]

class CustomOpenAIEmbeddings(Embeddings):
    """Custom OpenAI Embeddings for enhanced regulatory document processing"""
    
    def __init__(self, 
                 model: str = "text-embedding-3-large",
                 api_key: Optional[str] = None,
                 base_url: Optional[str] = None,
                 dimensions: Optional[int] = 3072,
                 max_chunk_size: int = 8000):
        
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if base_url:
            client_kwargs["base_url"] = base_url
            logger.info(f"Using custom OpenAI base URL: {base_url}")
        
        self.client = OpenAI(**client_kwargs)
        self.model = model
        self.dimensions = dimensions
        self.max_chunk_size = max_chunk_size
        
        logger.info(f"Initialized CustomOpenAIEmbeddings with model: {model}")
    
    def _chunk_text_by_characters(self, text: str) -> List[str]:
        """Character-based chunking for regulatory documents"""
        if len(text) <= self.max_chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chunk_size
            
            if end < len(text):
                # Look for regulatory section boundaries
                chunk_end = text.rfind('Article ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('Section ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('Clause ', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('.', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind('\n', start, end)
                if chunk_end == -1:
                    chunk_end = text.rfind(' ', start, end)
                
                if chunk_end > start:
                    end = chunk_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed regulatory documents with enhanced processing"""
        all_embeddings = []
        
        for text in texts:
            chunks = self._chunk_text_by_characters(text)
            
            if len(chunks) == 1:
                embedding = self._get_single_embedding(chunks[0])
                all_embeddings.append(embedding)
            else:
                # For regulatory documents, average embeddings with weighting
                chunk_embeddings = []
                weights = []
                
                for i, chunk in enumerate(chunks):
                    chunk_embedding = self._get_single_embedding(chunk)
                    chunk_embeddings.append(chunk_embedding)
                    
                    # Weight chunks with regulatory keywords higher
                    weight = 1.0
                    regulatory_keywords = ['gdpr', 'article', 'section', 'data protection', 'privacy', 'controller', 'processor', 'consent', 'lawful basis']
                    for keyword in regulatory_keywords:
                        if keyword.lower() in chunk.lower():
                            weight += 0.2
                    weights.append(weight)
                
                # Weighted average
                total_weight = sum(weights)
                avg_embedding = [
                    sum(emb[i] * weights[j] for j, emb in enumerate(chunk_embeddings)) / total_weight
                    for i in range(len(chunk_embeddings[0]))
                ]
                all_embeddings.append(avg_embedding)
        
        return all_embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed regulatory queries"""
        chunks = self._chunk_text_by_characters(text)
        return self._get_single_embedding(chunks[0])
    
    def _get_single_embedding(self, text: str) -> List[float]:
        """Get embedding for single text chunk"""
        try:
            params = {"input": text, "model": self.model}
            if self.dimensions:
                params["dimensions"] = self.dimensions
            
            response = self.client.embeddings.create(**params)
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to get embedding: {e}")
            raise

class EnhancedRegulatoryProcessor:
    """Enhanced processor for regulatory and financial documents"""
    
    def __init__(self, chunk_size: int = 1500, chunk_overlap: int = 300):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "Article ", "Section ", "Clause ", ".", "!", "?", ";", ":", " "]
        )
        self.regulatory_patterns = {
            'article_references': r'Article\s+(\d+(?:\.\d+)*)',
            'legal_bases': r'(Article\s+6\s*\([a-f]\)|legitimate\s+interest|consent|contract|legal\s+obligation|vital\s+interest|public\s+task)',
            'data_categories': r'(personal\s+data|special\s+categor(?:y|ies)|biometric|health|financial|location|identifier)',
            'processing_purposes': r'(processing\s+(?:for|purpose)|purpose(?:s)?\s+of)',
            'retention_periods': r'(\d+\s+(?:days?|months?|years?)|retention\s+period|storage\s+limitation)',
            'transfers': r'(third\s+countr(?:y|ies)|international\s+transfer|adequacy\s+decision)',
            'rights': r'(right\s+to\s+(?:access|rectification|erasure|portability|restriction|object)|data\s+subject\s+rights)'
        }
    
    def extract_pdf_content(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract and enhance PDF content for regulatory analysis"""
        logger.info(f"Processing regulatory document: {pdf_path}")
        
        doc = pymupdf.open(pdf_path)
        full_text = ""
        metadata = {"pages": len(doc), "document_type": self._detect_document_type(pdf_path)}
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            page_text = page.get_text()
            full_text += f"\n\n{page_text}"
        
        doc.close()
        
        # Enhanced chunking with regulatory context
        text_chunks = self.text_splitter.split_text(full_text)
        
        chunks = []
        for i, chunk_text in enumerate(text_chunks):
            # Extract regulatory patterns
            regulatory_matches = self._extract_regulatory_patterns(chunk_text)
            
            chunk = {
                "chunk_id": f"chunk_{i}",
                "text": chunk_text,
                "chunk_index": i,
                "source": pdf_path,
                "document_type": metadata["document_type"],
                "regulatory_patterns": regulatory_matches,
                "metadata": {
                    "word_count": len(chunk_text.split()),
                    "char_count": len(chunk_text),
                    "contains_article_refs": len(regulatory_matches.get("article_references", [])) > 0,
                    "contains_legal_bases": len(regulatory_matches.get("legal_bases", [])) > 0,
                    "regulatory_density": self._calculate_regulatory_density(chunk_text)
                }
            }
            chunks.append(chunk)
        
        logger.info(f"Created {len(chunks)} enhanced regulatory chunks")
        return chunks
    
    def _detect_document_type(self, pdf_path: str) -> str:
        """Detect the type of regulatory document"""
        filename = os.path.basename(pdf_path).lower()
        
        if any(term in filename for term in ['gdpr', 'regulation', 'directive']):
            return "regulation"
        elif any(term in filename for term in ['policy', 'procedure', 'guideline']):
            return "policy"
        elif any(term in filename for term in ['compliance', 'audit', 'assessment']):
            return "compliance"
        elif any(term in filename for term in ['business', 'process', 'procedure']):
            return "business_process"
        else:
            return "general"
    
    def _extract_regulatory_patterns(self, text: str) -> Dict[str, List[str]]:
        """Extract regulatory patterns from text"""
        matches = {}
        
        for pattern_name, pattern in self.regulatory_patterns.items():
            matches[pattern_name] = re.findall(pattern, text, re.IGNORECASE)
        
        return matches
    
    def _calculate_regulatory_density(self, text: str) -> float:
        """Calculate density of regulatory terms in text"""
        regulatory_terms = [
            'gdpr', 'data protection', 'privacy', 'controller', 'processor', 'consent',
            'lawful basis', 'legitimate interest', 'personal data', 'processing',
            'data subject', 'supervisory authority', 'compliance', 'breach',
            'retention', 'security', 'accountability', 'transparency'
        ]
        
        text_lower = text.lower()
        term_count = sum(1 for term in regulatory_terms if term in text_lower)
        word_count = len(text.split())
        
        return term_count / word_count if word_count > 0 else 0.0

@tool
def enhanced_regulatory_extraction_agent(text: str) -> Dict[str, Any]:
    """Enhanced agent for extracting regulatory concepts and RoPA elements"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze this regulatory/financial document text for GDPR Record of Processing Activities (RoPA) metamodel creation:

Text: {text}

Extract comprehensive information for a global RoPA metamodel covering financial institutions:

1. REGULATORY ENTITIES:
   - Data controllers, processors, joint controllers
   - Supervisory authorities
   - Data Protection Officers
   - Third-party service providers

2. PROCESSING ACTIVITIES:
   - Purpose of processing
   - Categories of data subjects
   - Categories of personal data
   - Recipients or categories of recipients
   - International transfers
   - Retention periods
   - Security measures

3. DATA CATEGORIES (Financial Sector Specific):
   - Customer identification data
   - Financial transaction data
   - Credit scoring data
   - Risk assessment data
   - Anti-money laundering data
   - Know Your Customer (KYC) data
   - Payment data
   - Investment data
   - Insurance data

4. LEGAL BASES:
   - Consent mechanisms
   - Contract performance
   - Legal obligations
   - Legitimate interests
   - Vital interests
   - Public tasks

5. JURISDICTIONAL REQUIREMENTS (GDPR-Based Framework):
   - GDPR (EU) specific requirements
   - UK GDPR requirements  
   - GDPR-based California (CCPA mapped to GDPR principles)
   - GDPR-based Canada (PIPEDA mapped to GDPR principles)
   - GDPR-based Brazil (LGPD mapped to GDPR principles)
   - GDPR-based Asian jurisdictions (APPI, PDPA mapped to GDPR)
   - Other jurisdictions using GDPR as base framework

6. COMPLIANCE CONCEPTS:
   - Privacy by Design principles
   - Data minimization requirements
   - Purpose limitation
   - Accountability measures
   - Transparency obligations

7. TECHNICAL AND ORGANIZATIONAL MEASURES:
   - Security safeguards
   - Access controls
   - Encryption requirements
   - Incident response procedures
   - Regular auditing

Return comprehensive JSON with detailed mappings:

{{
    "regulatory_entities": [
        {{"name": "entity_name", "type": "controller|processor|dpo|authority", "role": "detailed_role", "jurisdiction": "applicable_jurisdiction"}}
    ],
    "processing_activities": [
        {{"activity_name": "name", "purpose": "purpose", "legal_basis": "basis", "data_categories": ["categories"], "data_subjects": ["subjects"], "retention": "period", "jurisdiction_applicability": ["jurisdictions"]}}
    ],
    "data_categories": [
        {{"category": "category_name", "sensitivity": "normal|special", "financial_specific": true|false, "examples": ["examples"], "protection_requirements": ["requirements"]}}
    ],
    "legal_bases": [
        {{"basis": "basis_name", "description": "description", "requirements": ["requirements"], "jurisdiction": "applicable_jurisdiction"}}
    ],
    "jurisdictional_mappings": [
        {{"jurisdiction": "jurisdiction_name", "requirements": ["specific_requirements"], "penalties": "penalty_structure", "data_subject_rights": ["rights"]}}
    ],
    "security_measures": [
        {{"measure": "measure_name", "type": "technical|organizational", "description": "description", "mandatory_jurisdictions": ["jurisdictions"]}}
    ],
    "cross_border_transfers": [
        {{"transfer_mechanism": "mechanism", "requirements": ["requirements"], "applicable_jurisdictions": ["jurisdictions"]}}
    ],
    "compliance_concepts": [
        {{"concept": "concept_name", "principle": "underlying_principle", "implementation": "how_to_implement", "jurisdictions": ["applicable_jurisdictions"]}}
    ],
    "concept_synonyms": [
        {{
            "primary_term": "main_regulatory_term",
            "synonyms": ["alternative_terms", "industry_terms", "jurisdictional_variants"],
            "context": "regulatory_context_and_definition",
            "confidence": 0.95,
            "category": "ropa_category"
        }}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            result = json.loads(json_str)
            
            # Ensure all required fields exist
            required_fields = [
                "regulatory_entities", "processing_activities", "data_categories", 
                "legal_bases", "jurisdictional_mappings", "security_measures",
                "cross_border_transfers", "compliance_concepts", "concept_synonyms"
            ]
            
            for field in required_fields:
                if field not in result:
                    result[field] = []
            
            return result
        else:
            return {field: [] for field in [
                "regulatory_entities", "processing_activities", "data_categories", 
                "legal_bases", "jurisdictional_mappings", "security_measures",
                "cross_border_transfers", "compliance_concepts", "concept_synonyms"
            ]}
    
    except Exception as e:
        logger.warning(f"Enhanced regulatory extraction failed: {e}")
        return {field: [] for field in [
            "regulatory_entities", "processing_activities", "data_categories", 
            "legal_bases", "jurisdictional_mappings", "security_measures",
            "cross_border_transfers", "compliance_concepts", "concept_synonyms"
        ]}

@tool
def metamodel_analysis_agent(extracted_data: str) -> Dict[str, Any]:
    """Agent for analyzing extracted data and building metamodel structure"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Analyze the extracted regulatory data to build a comprehensive GDPR RoPA metamodel structure for global financial institutions:

Extracted Data: {extracted_data}

Create a comprehensive metamodel that includes:

1. CORE METAMODEL ENTITIES:
   - Abstract base classes
   - Concrete implementation classes
   - Relationship mappings
   - Constraint definitions

2. JURISDICTIONAL EXTENSIONS:
   - Jurisdiction-specific requirements
   - Variation points for different laws
   - Inheritance hierarchies
   - Compatibility matrices

3. FINANCIAL SECTOR SPECIALIZATIONS:
   - Banking-specific processing
   - Insurance data handling
   - Investment management
   - Payment processing
   - Credit scoring
   - Risk management

4. COMPLIANCE FRAMEWORKS:
   - Audit trail requirements
   - Reporting structures
   - Documentation standards
   - Certification processes

5. OPERATIONAL MAPPINGS:
   - Business process integration
   - System architecture alignment
   - Data flow modeling
   - Control frameworks

Return structured metamodel design:

{{
    "metamodel_structure": {{
        "core_entities": [
            {{"entity": "entity_name", "type": "abstract|concrete", "attributes": ["attributes"], "relationships": ["relationships"], "constraints": ["constraints"]}}
        ],
        "jurisdictional_variants": [
            {{"jurisdiction": "jurisdiction_name", "extensions": ["specific_extensions"], "overrides": ["overridden_elements"], "additional_requirements": ["requirements"]}}
        ],
        "financial_specializations": [
            {{"domain": "banking|insurance|investment|payments", "specific_entities": ["entities"], "processing_types": ["types"], "data_categories": ["categories"]}}
        ],
        "relationship_matrix": [
            {{"entity1": "entity_name", "entity2": "entity_name", "relationship_type": "one-to-one|one-to-many|many-to-many", "constraints": ["constraints"]}}
        ]
    }},
    "implementation_guidance": {{
        "mandatory_elements": ["elements_required_in_all_jurisdictions"],
        "optional_elements": ["elements_for_specific_contexts"],
        "extensibility_points": ["areas_for_customization"],
        "validation_rules": ["rules_for_metamodel_consistency"]
    }},
    "compliance_mapping": {{
        "audit_requirements": ["requirements_per_jurisdiction"],
        "reporting_obligations": ["reporting_structures"],
        "documentation_standards": ["documentation_requirements"],
        "certification_processes": ["certification_frameworks"]
    }}
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {
                "metamodel_structure": {"core_entities": [], "jurisdictional_variants": [], "financial_specializations": [], "relationship_matrix": []},
                "implementation_guidance": {"mandatory_elements": [], "optional_elements": [], "extensibility_points": [], "validation_rules": []},
                "compliance_mapping": {"audit_requirements": [], "reporting_obligations": [], "documentation_standards": [], "certification_processes": []}
            }
    
    except Exception as e:
        logger.warning(f"Metamodel analysis failed: {e}")
        return {
            "metamodel_structure": {"core_entities": [], "jurisdictional_variants": [], "financial_specializations": [], "relationship_matrix": []},
            "implementation_guidance": {"mandatory_elements": [], "optional_elements": [], "extensibility_points": [], "validation_rules": []},
            "compliance_mapping": {"audit_requirements": [], "reporting_obligations": [], "documentation_standards": [], "certification_processes": []}
        }

@tool
def compliance_gap_analysis_agent(metamodel_data: str, jurisdiction_requirements: str) -> Dict[str, Any]:
    """Agent for identifying compliance gaps across jurisdictions"""
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
    if base_url:
        llm_kwargs["base_url"] = base_url
    if api_key:
        llm_kwargs["api_key"] = api_key
    
    llm = ChatOpenAI(**llm_kwargs)
    
    prompt = f"""Perform comprehensive compliance gap analysis for the RoPA metamodel across global jurisdictions:

Metamodel Data: {metamodel_data}

Jurisdiction Requirements: {jurisdiction_requirements}

Analyze and identify:

1. COVERAGE GAPS:
   - Missing elements for specific jurisdictions
   - Incomplete mappings
   - Unaddressed requirements

2. COMPLIANCE CONFLICTS:
   - Contradictory requirements between jurisdictions
   - Incompatible approaches
   - Resolution strategies

3. IMPLEMENTATION CHALLENGES:
   - Complex mapping scenarios
   - Resource-intensive requirements
   - Technical difficulties

4. PRIORITY RECOMMENDATIONS:
   - Critical gaps to address
   - Quick wins
   - Long-term improvements

Return comprehensive gap analysis:

{{
    "coverage_gaps": [
        {{"jurisdiction": "jurisdiction_name", "missing_elements": ["elements"], "impact": "high|medium|low", "remediation": "recommended_action"}}
    ],
    "compliance_conflicts": [
        {{"jurisdictions": ["conflicting_jurisdictions"], "conflict_area": "area_of_conflict", "severity": "high|medium|low", "resolution_strategy": "strategy"}}
    ],
    "implementation_challenges": [
        {{"challenge": "challenge_description", "affected_areas": ["areas"], "complexity": "high|medium|low", "recommended_approach": "approach"}}
    ],
    "priority_recommendations": [
        {{"recommendation": "recommendation_text", "priority": "critical|high|medium|low", "effort": "high|medium|low", "impact": "high|medium|low", "timeline": "timeframe"}}
    ],
    "risk_assessment": [
        {{"risk": "risk_description", "probability": "high|medium|low", "impact": "high|medium|low", "mitigation": "mitigation_strategy"}}
    ]
}}
"""
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content
        
        json_start = content.find('{')
        json_end = content.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        else:
            return {
                "coverage_gaps": [], "compliance_conflicts": [], 
                "implementation_challenges": [], "priority_recommendations": [],
                "risk_assessment": []
            }
    
    except Exception as e:
        logger.warning(f"Compliance gap analysis failed: {e}")
        return {
            "coverage_gaps": [], "compliance_conflicts": [], 
            "implementation_challenges": [], "priority_recommendations": [],
            "risk_assessment": []
        }

class EnhancedRegulatoryVectorEngine:
    """Enhanced vector engine for regulatory document processing"""
    
    def __init__(self, 
                 host: str = "http://localhost:9200", 
                 index_name: str = "gdpr_ropa_metamodel",
                 username: str = None,
                 password: str = None,
                 ca_certs: str = None,
                 verify_certs: bool = True,
                 openai_api_key: str = None,
                 openai_base_url: str = None):
        
        self.index_name = index_name
        
        # Initialize enhanced embeddings for regulatory processing
        self.embeddings = CustomOpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=openai_api_key,
            base_url=openai_base_url,
            dimensions=3072
        )
        
        # Configure Elasticsearch client
        self.client = self._create_elasticsearch_client(
            host, username, password, ca_certs, verify_certs
        )
        
        self._create_regulatory_index()
    
    def _create_elasticsearch_client(self, host, username, password, ca_certs, verify_certs):
        """Create Elasticsearch client for regulatory data"""
        if not host.startswith(('http://', 'https://')):
            raise ValueError(f"Elasticsearch host must include schema. Got: {host}")
        
        client_config = {
            "hosts": [host],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        if ca_certs or verify_certs is False:
            client_config["verify_certs"] = verify_certs
            if ca_certs:
                client_config["ca_certs"] = ca_certs
        
        if username and password:
            client_config["basic_auth"] = (username, password)
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch for regulatory processing")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch cluster")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_regulatory_index(self):
        """Create enhanced index for regulatory documents"""
        mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "regulatory_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer",
                                "keyword_repeat",
                                "remove_duplicates",
                                "regulatory_synonyms"
                            ]
                        }
                    },
                    "filter": {
                        "regulatory_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "gdpr,general data protection regulation",
                                "controller,data controller",
                                "processor,data processor",
                                "dpo,data protection officer",
                                "ropa,record of processing activities",
                                "ccpa,california consumer privacy act",
                                "pipeda,personal information protection and electronic documents act",
                                "lgpd,lei geral de proteção de dados"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "text": {
                        "type": "text",
                        "analyzer": "regulatory_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 3072,
                        "index": True,
                        "similarity": "cosine"
                    },
                    "document_type": {"type": "keyword"},
                    "jurisdiction": {"type": "keyword"},
                    "regulatory_patterns": {"type": "object"},
                    "regulatory_density": {"type": "float"},
                    
                    # RoPA specific fields
                    "processing_activities": {"type": "nested"},
                    "data_categories": {"type": "nested"},
                    "legal_bases": {"type": "nested"},
                    "regulatory_entities": {"type": "nested"},
                    "security_measures": {"type": "nested"},
                    "jurisdictional_mappings": {"type": "nested"},
                    
                    # Metamodel fields
                    "metamodel_entity": {"type": "keyword"},
                    "compliance_relevance": {"type": "float"},
                    "implementation_complexity": {"type": "keyword"},
                    
                    "chunk_id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "timestamp": {"type": "date"},
                    "metadata": {"type": "object"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created regulatory index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create regulatory index: {e}")
            raise
    
    def index_regulatory_documents(self, chunks: List[Dict[str, Any]], extractions: List[Dict[str, Any]]):
        """Index regulatory documents with enhanced metadata"""
        logger.info("Indexing regulatory documents with enhanced RoPA metadata...")
        
        for i, chunk in enumerate(chunks):
            extraction = extractions[i] if i < len(extractions) else {}
            
            # Create primary document embedding
            embedding = self.embeddings.embed_query(chunk["text"])
            
            doc = {
                "text": chunk["text"],
                "embedding": embedding,
                "chunk_id": chunk["chunk_id"],
                "source": chunk["source"],
                "document_type": chunk.get("document_type", "general"),
                "regulatory_patterns": chunk.get("regulatory_patterns", {}),
                "regulatory_density": chunk.get("metadata", {}).get("regulatory_density", 0.0),
                
                # Enhanced RoPA extractions
                "processing_activities": extraction.get("processing_activities", []),
                "data_categories": extraction.get("data_categories", []),
                "legal_bases": extraction.get("legal_bases", []),
                "regulatory_entities": extraction.get("regulatory_entities", []),
                "security_measures": extraction.get("security_measures", []),
                "jurisdictional_mappings": extraction.get("jurisdictional_mappings", []),
                
                # Metamodel relevance
                "metamodel_entity": self._determine_metamodel_relevance(chunk, extraction),
                "compliance_relevance": self._calculate_compliance_relevance(chunk, extraction),
                "implementation_complexity": self._assess_implementation_complexity(extraction),
                
                "metadata": chunk["metadata"],
                "timestamp": datetime.now()
            }
            
            try:
                self.client.index(index=self.index_name, id=chunk["chunk_id"], document=doc)
            except Exception as e:
                logger.error(f"Failed to index chunk {chunk['chunk_id']}: {e}")
        
        self.client.indices.refresh(index=self.index_name)
        logger.info(f"Successfully indexed {len(chunks)} regulatory chunks")
    
    def _determine_metamodel_relevance(self, chunk: Dict, extraction: Dict) -> str:
        """Determine the primary metamodel entity this chunk relates to"""
        if extraction.get("processing_activities"):
            return "processing_activity"
        elif extraction.get("data_categories"):
            return "data_category"
        elif extraction.get("legal_bases"):
            return "legal_basis"
        elif extraction.get("regulatory_entities"):
            return "regulatory_entity"
        elif extraction.get("security_measures"):
            return "security_measure"
        else:
            return "general_compliance"
    
    def _calculate_compliance_relevance(self, chunk: Dict, extraction: Dict) -> float:
        """Calculate compliance relevance score"""
        score = 0.0
        
        # Base regulatory density
        score += chunk.get("metadata", {}).get("regulatory_density", 0.0) * 0.3
        
        # Extraction richness
        extraction_count = sum(len(v) if isinstance(v, list) else 1 for v in extraction.values())
        score += min(extraction_count / 10.0, 0.4)
        
        # Regulatory pattern matches
        pattern_count = sum(len(v) if isinstance(v, list) else 1 for v in chunk.get("regulatory_patterns", {}).values())
        score += min(pattern_count / 5.0, 0.3)
        
        return min(score, 1.0)
    
    def _assess_implementation_complexity(self, extraction: Dict) -> str:
        """Assess implementation complexity based on extractions"""
        complexity_indicators = 0
        
        # Multiple jurisdictions
        jurisdictions = set()
        for mapping in extraction.get("jurisdictional_mappings", []):
            jurisdictions.add(mapping.get("jurisdiction", ""))
        complexity_indicators += len(jurisdictions)
        
        # Complex processing activities
        complexity_indicators += len(extraction.get("processing_activities", []))
        
        # Multiple legal bases
        complexity_indicators += len(extraction.get("legal_bases", []))
        
        if complexity_indicators >= 10:
            return "high"
        elif complexity_indicators >= 5:
            return "medium"
        else:
            return "low"
    
    def enhanced_regulatory_search(self, query: str, jurisdiction: str = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Enhanced search for regulatory concepts"""
        query_embedding = self.embeddings.embed_query(query)
        
        # Build complex search query
        must_clauses = [
            {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            }
        ]
        
        should_clauses = [
            {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "text^1.0",
                        "processing_activities.activity_name^2.0",
                        "data_categories.category^1.8",
                        "legal_bases.basis^1.5",
                        "regulatory_entities.name^1.5"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            }
        ]
        
        # Add jurisdiction filter if specified
        filter_clauses = []
        if jurisdiction:
            filter_clauses.append({
                "nested": {
                    "path": "jurisdictional_mappings",
                    "query": {"term": {"jurisdictional_mappings.jurisdiction": jurisdiction}}
                }
            })
        
        search_body = {
            "query": {
                "bool": {
                    "must": must_clauses,
                    "should": should_clauses,
                    "filter": filter_clauses
                }
            },
            "size": top_k,
            "_source": {
                "excludes": ["embedding"]
            }
        }
        
        try:
            response = self.client.search(index=self.index_name, **search_body)
            return self._format_regulatory_results(response)
        except Exception as e:
            logger.error(f"Enhanced regulatory search failed: {e}")
            return []
    
    def _format_regulatory_results(self, response: Dict) -> List[Dict[str, Any]]:
        """Format enhanced regulatory search results"""
        results = []
        
        try:
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "text": source["text"],
                    "chunk_id": source["chunk_id"],
                    "source": source["source"],
                    "score": hit["_score"],
                    "document_type": source.get("document_type", "general"),
                    "metamodel_entity": source.get("metamodel_entity", "general"),
                    "compliance_relevance": source.get("compliance_relevance", 0.0),
                    "implementation_complexity": source.get("implementation_complexity", "unknown"),
                    "processing_activities": source.get("processing_activities", []),
                    "data_categories": source.get("data_categories", []),
                    "legal_bases": source.get("legal_bases", []),
                    "jurisdictional_mappings": source.get("jurisdictional_mappings", [])
                }
                results.append(result)
        except Exception as e:
            logger.error(f"Error formatting regulatory search results: {e}")
        
        return results

class RegulatoryGraphEngine:
    """Enhanced graph engine for regulatory knowledge representation"""
    
    def __init__(self, host: str = "localhost", port: int = 6379, password: str = None):
        try:
            if password:
                self.db = FalkorDB(host=host, port=port, password=password)
            else:
                self.db = FalkorDB(host=host, port=port)
            
            self.graph = self.db.select_graph("gdpr_ropa_metamodel_graph")
            logger.info(f"Connected to FalkorDB for regulatory graph at {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def build_regulatory_knowledge_graph(self, chunks: List[Dict[str, Any]], extractions: List[Dict[str, Any]]):
        """Build comprehensive regulatory knowledge graph"""
        logger.info("Building comprehensive regulatory knowledge graph...")
        
        try:
            # Clear existing graph
            self.graph.query("MATCH (n) DETACH DELETE n")
            
            # Create jurisdiction nodes
            for jurisdiction, requirements in JURISDICTION_REQUIREMENTS.items():
                jurisdiction_query = f"""
                CREATE (j:Jurisdiction {{
                    id: '{jurisdiction.value}',
                    name: '{requirements.name}',
                    penalties: '{requirements.penalties}',
                    principles: {json.dumps(requirements.key_principles)},
                    ropa_requirements: {json.dumps(requirements.ropa_requirements)},
                    financial_specifics: {json.dumps(requirements.financial_specifics)},
                    data_subject_rights: {json.dumps(requirements.data_subject_rights)}
                }})
                """
                self.graph.query(jurisdiction_query)
            
            # Process each document chunk
            for i, chunk in enumerate(chunks):
                extraction = extractions[i] if i < len(extractions) else {}
                
                # Create document node
                doc_query = f"""
                CREATE (d:Document {{
                    id: '{chunk['chunk_id']}',
                    source: '{chunk['source']}',
                    document_type: '{chunk.get('document_type', 'general')}',
                    regulatory_density: {chunk.get('metadata', {}).get('regulatory_density', 0.0)}
                }})
                """
                self.graph.query(doc_query)
                
                # Create processing activity nodes
                for activity in extraction.get("processing_activities", []):
                    activity_name = activity.get("activity_name", "").replace("'", "\\'")
                    purpose = activity.get("purpose", "").replace("'", "\\'")
                    legal_basis = activity.get("legal_basis", "").replace("'", "\\'")
                    
                    if activity_name:
                        activity_query = f"""
                        MERGE (pa:ProcessingActivity {{
                            name: '{activity_name}',
                            purpose: '{purpose}',
                            legal_basis: '{legal_basis}',
                            data_categories: {json.dumps(activity.get('data_categories', []))},
                            data_subjects: {json.dumps(activity.get('data_subjects', []))},
                            retention: '{activity.get('retention', '')}',
                            jurisdiction_applicability: {json.dumps(activity.get('jurisdiction_applicability', []))}
                        }})
                        """
                        self.graph.query(activity_query)
                        
                        # Link to document
                        link_query = f"""
                        MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                        MATCH (pa:ProcessingActivity {{name: '{activity_name}'}})
                        MERGE (d)-[:DESCRIBES]->(pa)
                        """
                        self.graph.query(link_query)
                        
                        # Link to applicable jurisdictions
                        for jurisdiction in activity.get('jurisdiction_applicability', []):
                            jurisdiction = jurisdiction.replace("'", "\\'")
                            jurisdiction_link = f"""
                            MATCH (pa:ProcessingActivity {{name: '{activity_name}'}})
                            MATCH (j:Jurisdiction {{id: '{jurisdiction}'}})
                            MERGE (pa)-[:SUBJECT_TO]->(j)
                            """
                            self.graph.query(jurisdiction_link)
                
                # Create data category nodes
                for category in extraction.get("data_categories", []):
                    category_name = category.get("category", "").replace("'", "\\'")
                    sensitivity = category.get("sensitivity", "normal")
                    financial_specific = category.get("financial_specific", False)
                    
                    if category_name:
                        category_query = f"""
                        MERGE (dc:DataCategory {{
                            name: '{category_name}',
                            sensitivity: '{sensitivity}',
                            financial_specific: {str(financial_specific).lower()},
                            examples: {json.dumps(category.get('examples', []))},
                            protection_requirements: {json.dumps(category.get('protection_requirements', []))}
                        }})
                        """
                        self.graph.query(category_query)
                        
                        # Link to document
                        link_query = f"""
                        MATCH (d:Document {{id: '{chunk['chunk_id']}'}})
                        MATCH (dc:DataCategory {{name: '{category_name}'}})
                        MERGE (d)-[:DEFINES]->(dc)
                        """
                        self.graph.query(link_query)
                
                # Create legal basis nodes
                for basis in extraction.get("legal_bases", []):
                    basis_name = basis.get("basis", "").replace("'", "\\'")
                    description = basis.get("description", "").replace("'", "\\'")
                    jurisdiction = basis.get("jurisdiction", "").replace("'", "\\'")
                    
                    if basis_name:
                        basis_query = f"""
                        MERGE (lb:LegalBasis {{
                            name: '{basis_name}',
                            description: '{description}',
                            jurisdiction: '{jurisdiction}',
                            requirements: {json.dumps(basis.get('requirements', []))}
                        }})
                        """
                        self.graph.query(basis_query)
                        
                        # Link to jurisdiction
                        if jurisdiction:
                            jurisdiction_link = f"""
                            MATCH (lb:LegalBasis {{name: '{basis_name}'}})
                            MATCH (j:Jurisdiction {{id: '{jurisdiction}'}})
                            MERGE (lb)-[:APPLIES_IN]->(j)
                            """
                            self.graph.query(jurisdiction_link)
                
                # Create regulatory entity nodes
                for entity in extraction.get("regulatory_entities", []):
                    entity_name = entity.get("name", "").replace("'", "\\'")
                    entity_type = entity.get("type", "").replace("'", "\\'")
                    role = entity.get("role", "").replace("'", "\\'")
                    jurisdiction = entity.get("jurisdiction", "").replace("'", "\\'")
                    
                    if entity_name:
                        entity_query = f"""
                        MERGE (re:RegulatoryEntity {{
                            name: '{entity_name}',
                            type: '{entity_type}',
                            role: '{role}',
                            jurisdiction: '{jurisdiction}'
                        }})
                        """
                        self.graph.query(entity_query)
                
                # Create security measure nodes
                for measure in extraction.get("security_measures", []):
                    measure_name = measure.get("measure", "").replace("'", "\\'")
                    measure_type = measure.get("type", "").replace("'", "\\'")
                    description = measure.get("description", "").replace("'", "\\'")
                    
                    if measure_name:
                        measure_query = f"""
                        MERGE (sm:SecurityMeasure {{
                            name: '{measure_name}',
                            type: '{measure_type}',
                            description: '{description}',
                            mandatory_jurisdictions: {json.dumps(measure.get('mandatory_jurisdictions', []))}
                        }})
                        """
                        self.graph.query(measure_query)
            
            # Create cross-references and relationships
            self._create_cross_references()
            
            logger.info("Regulatory knowledge graph built successfully")
        
        except Exception as e:
            logger.error(f"Failed to build regulatory knowledge graph: {e}")
            raise
    
    def _create_cross_references(self):
        """Create complex cross-references between regulatory concepts"""
        
        # Link processing activities to data categories
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (dc:DataCategory)
        WHERE ANY(category IN pa.data_categories WHERE category = dc.name)
        MERGE (pa)-[:PROCESSES]->(dc)
        """)
        
        # Link processing activities to legal bases
        self.graph.query("""
        MATCH (pa:ProcessingActivity), (lb:LegalBasis)
        WHERE pa.legal_basis = lb.name
        MERGE (pa)-[:BASED_ON]->(lb)
        """)
        
        # Create compliance relationships
        self.graph.query("""
        MATCH (j:Jurisdiction), (pa:ProcessingActivity)
        WHERE ANY(jurisdiction IN pa.jurisdiction_applicability WHERE jurisdiction = j.id)
        MERGE (pa)-[:MUST_COMPLY_WITH]->(j)
        """)
    
    def enhanced_regulatory_graph_search(self, query: str, jurisdiction: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Enhanced graph search for regulatory concepts"""
        results = []
        
        query_terms = self._extract_regulatory_terms(query)
        
        for term in query_terms[:3]:
            term = term.replace("'", "\\'")
            
            # Multi-level graph traversal
            graph_query = f"""
            MATCH path = (start)-[*1..3]-(related)
            WHERE (
                (start:ProcessingActivity AND toLower(start.name) CONTAINS '{term}') OR
                (start:DataCategory AND toLower(start.name) CONTAINS '{term}') OR
                (start:LegalBasis AND toLower(start.name) CONTAINS '{term}') OR
                (start:RegulatoryEntity AND toLower(start.name) CONTAINS '{term}') OR
                (start:SecurityMeasure AND toLower(start.name) CONTAINS '{term}')
            )
            RETURN DISTINCT start, related, relationships(path), length(path) as distance
            ORDER BY distance
            LIMIT {top_k}
            """
            
            try:
                result = self.graph.query(graph_query)
                
                for record in result.result_set:
                    graph_result = {
                        "start_node": self._format_node(record[0]),
                        "related_node": self._format_node(record[1]),
                        "relationships": [str(rel) for rel in record[2]],
                        "distance": record[3],
                        "search_term": term,
                        "search_type": "enhanced_regulatory_graph"
                    }
                    results.append(graph_result)
            
            except Exception as e:
                logger.warning(f"Enhanced graph query failed for '{term}': {e}")
        
        return results[:top_k]
    
    def _extract_regulatory_terms(self, query: str) -> List[str]:
        """Extract regulatory terms from query"""
        words = re.findall(r'\b\w+\b', query.lower())
        regulatory_stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return [word for word in words if len(word) > 2 and word not in regulatory_stopwords]
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if hasattr(node, 'properties'):
            return dict(node.properties)
        return {"id": str(node)}

class GlobalRopaMetamodelSystem:
    """Complete Global GDPR RoPA Metamodel System for Financial Institutions"""
    
    def __init__(self,
                 openai_api_key: str,
                 openai_base_url: str = None,
                 elasticsearch_host: str = "http://localhost:9200",
                 elasticsearch_username: str = None,
                 elasticsearch_password: str = None,
                 falkordb_host: str = "localhost",
                 falkordb_port: int = 6379,
                 falkordb_password: str = None):
        
        os.environ["OPENAI_API_KEY"] = openai_api_key
        if openai_base_url:
            os.environ["OPENAI_BASE_URL"] = openai_base_url
        
        # Initialize components
        self.processor = EnhancedRegulatoryProcessor()
        
        self.vector_engine = EnhancedRegulatoryVectorEngine(
            host=elasticsearch_host,
            username=elasticsearch_username,
            password=elasticsearch_password,
            openai_api_key=openai_api_key,
            openai_base_url=openai_base_url
        )
        
        self.graph_engine = RegulatoryGraphEngine(
            host=falkordb_host,
            port=falkordb_port,
            password=falkordb_password
        )
        
        # State for iterative analysis
        self.analysis_state = {
            "processed_documents": [],
            "extracted_concepts": [],
            "metamodel_iterations": [],
            "compliance_gaps": [],
            "final_metamodel": None
        }
        
        logger.info("Global RoPA Metamodel System initialized for financial institutions")
        if openai_base_url:
            logger.info(f"Using custom OpenAI endpoint: {openai_base_url}")
    
    def ingest_regulatory_documents(self, document_paths: List[str]) -> Dict[str, Any]:
        """Ingest and process regulatory documents"""
        logger.info(f"Ingesting {len(document_paths)} regulatory documents")
        
        all_chunks = []
        all_extractions = []
        
        for doc_path in document_paths:
            try:
                # Process document
                chunks = self.processor.extract_pdf_content(doc_path)
                self.analysis_state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "timestamp": datetime.now()
                })
                
                # Extract regulatory concepts
                extractions = []
                for chunk in chunks:
                    extraction = enhanced_regulatory_extraction_agent.invoke(chunk["text"])
                    extractions.append(extraction)
                    self.analysis_state["extracted_concepts"].extend(extraction.get("concept_synonyms", []))
                
                all_chunks.extend(chunks)
                all_extractions.extend(extractions)
                
                logger.info(f"Processed {doc_path}: {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"Failed to process {doc_path}: {e}")
                continue
        
        # Index in vector store
        self.vector_engine.index_regulatory_documents(all_chunks, all_extractions)
        
        # Build knowledge graph
        self.graph_engine.build_regulatory_knowledge_graph(all_chunks, all_extractions)
        
        # Calculate statistics
        total_processing_activities = sum(len(ext.get("processing_activities", [])) for ext in all_extractions)
        total_data_categories = sum(len(ext.get("data_categories", [])) for ext in all_extractions)
        total_legal_bases = sum(len(ext.get("legal_bases", [])) for ext in all_extractions)
        total_jurisdictions = len(set(
            mapping.get("jurisdiction", "") 
            for ext in all_extractions 
            for mapping in ext.get("jurisdictional_mappings", [])
        ))
        
        return {
            "status": "success",
            "documents_processed": len(document_paths),
            "total_chunks": len(all_chunks),
            "processing_activities_discovered": total_processing_activities,
            "data_categories_discovered": total_data_categories,
            "legal_bases_discovered": total_legal_bases,
            "jurisdictions_covered": total_jurisdictions,
            "regulatory_concepts": len(self.analysis_state["extracted_concepts"]),
            "timestamp": datetime.now()
        }
    
    def iterative_metamodel_analysis(self, iterations: int = 3) -> Dict[str, Any]:
        """Perform iterative analysis to refine the metamodel"""
        logger.info(f"Starting iterative metamodel analysis with {iterations} iterations")
        
        for iteration in range(iterations):
            logger.info(f"Metamodel iteration {iteration + 1}/{iterations}")
            
            # Query the system for comprehensive understanding with GDPR focus
            analysis_queries = [
                "What are the core processing activities in financial institutions under GDPR?",
                "What data categories are specific to banking and financial services under GDPR framework?",
                "What legal bases apply under GDPR and UK GDPR for financial institutions?",
                "How do GDPR and UK GDPR requirements compare with GDPR-based jurisdictions?",
                "What security measures are mandatory for financial institutions under GDPR framework?",
                "What are the GDPR retention requirements for different types of financial data?",
                "How do GDPR cross-border transfer mechanisms apply to global financial institutions?",
                "What are the GDPR-based compliance frameworks for global operations?",
                "How do data subject rights under GDPR apply to financial services?",
                "What are the GDPR accountability requirements for financial institutions?"
            ]
            
            iteration_insights = []
            
            for query in analysis_queries:
                # Search vector store
                vector_results = self.vector_engine.enhanced_regulatory_search(query, top_k=5)
                
                # Search knowledge graph
                graph_results = self.graph_engine.enhanced_regulatory_graph_search(query, top_k=3)
                
                # Analyze results
                combined_data = {
                    "query": query,
                    "vector_results": vector_results,
                    "graph_results": graph_results
                }
                
                metamodel_analysis = metamodel_analysis_agent.invoke(json.dumps(combined_data))
                iteration_insights.append(metamodel_analysis)
            
            # Store iteration results
            self.analysis_state["metamodel_iterations"].append({
                "iteration": iteration + 1,
                "insights": iteration_insights,
                "timestamp": datetime.now()
            })
        
        # Perform compliance gap analysis
        all_jurisdiction_requirements = {
            jurisdiction.value: requirements.__dict__ 
            for jurisdiction, requirements in JURISDICTION_REQUIREMENTS.items()
        }
        
        combined_metamodel = {
            "iterations": self.analysis_state["metamodel_iterations"],
            "extracted_concepts": self.analysis_state["extracted_concepts"]
        }
        
        gap_analysis = compliance_gap_analysis_agent.invoke(
            json.dumps(combined_metamodel),
            json.dumps(all_jurisdiction_requirements)
        )
        
        self.analysis_state["compliance_gaps"] = gap_analysis
        
        return {
            "iterations_completed": iterations,
            "insights_generated": len(iteration_insights) * iterations,
            "compliance_gaps_identified": len(gap_analysis.get("coverage_gaps", [])),
            "conflicts_identified": len(gap_analysis.get("compliance_conflicts", [])),
            "recommendations_generated": len(gap_analysis.get("priority_recommendations", [])),
            "status": "analysis_complete"
        }
    
    def generate_final_metamodel(self) -> Dict[str, Any]:
        """Generate the final comprehensive metamodel"""
        logger.info("Generating final comprehensive RoPA metamodel")
        
        # Consolidate all analysis
        consolidated_analysis = {
            "processed_documents": self.analysis_state["processed_documents"],
            "extracted_concepts": self.analysis_state["extracted_concepts"],
            "metamodel_iterations": self.analysis_state["metamodel_iterations"],
            "compliance_gaps": self.analysis_state["compliance_gaps"],
            "jurisdiction_requirements": {
                jurisdiction.value: requirements.__dict__ 
                for jurisdiction, requirements in JURISDICTION_REQUIREMENTS.items()
            }
        }
        
        # Generate final metamodel structure
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        prompt = f"""Based on comprehensive analysis of regulatory documents and iterative refinement, create the final GDPR-based Record of Processing Activities (RoPA) metamodel for global financial institutions.

This metamodel should use GDPR and UK GDPR as the foundation, with other jurisdictions mapped to GDPR principles and requirements.

Consolidated Analysis: {json.dumps(consolidated_analysis, default=str)}

Create a comprehensive GDPR-centric metamodel that includes:

1. CORE METAMODEL STRUCTURE (GDPR-Based):
   - Abstract base classes following GDPR Article 30 requirements
   - Concrete implementation classes for GDPR compliance
   - Relationship definitions based on GDPR framework
   - Constraint specifications aligned with GDPR principles
   - Validation rules for GDPR compliance

2. JURISDICTIONAL VARIANTS (GDPR as Foundation):
   - GDPR (EU) core requirements
   - UK GDPR specific adaptations
   - GDPR-based California (CCPA mapped to GDPR structure)
   - GDPR-based Canada (PIPEDA aligned with GDPR principles)
   - GDPR-based Brazil (LGPD harmonized with GDPR)
   - GDPR-based Asian jurisdictions (APPI, PDPA using GDPR framework)
   - Extension framework for other GDPR-based jurisdictions

3. FINANCIAL SECTOR SPECIALIZATIONS (GDPR-Compliant):
   - Banking-specific processing activities under GDPR
   - Insurance data handling with GDPR requirements
   - Investment management GDPR compliance
   - Payment processing GDPR obligations
   - Credit scoring and risk assessment under GDPR
   - Anti-money laundering (AML) with GDPR alignment
   - Know Your Customer (KYC) processes under GDPR

4. IMPLEMENTATION FRAMEWORK (GDPR-Centric):
   - GDPR-compliant deployment guidelines
   - Integration patterns with GDPR requirements
   - Validation procedures for GDPR compliance
   - Audit trail requirements per GDPR Article 5(2)
   - Reporting mechanisms aligned with GDPR

5. COMPLIANCE VALIDATION (GDPR-Based):
   - Automated GDPR compliance checks
   - Gap identification against GDPR requirements
   - Risk assessment frameworks using GDPR criteria
   - Remediation guidance based on GDPR principles

Return the complete metamodel specification:

{{
    "metamodel_specification": {{
        "version": "1.0.0",
        "scope": "Global Financial Institutions RoPA Compliance",
        "core_entities": [
            {{
                "entity": "ProcessingActivity",
                "type": "core",
                "attributes": ["id", "name", "purpose", "legal_basis", "data_categories", "data_subjects", "recipients", "retention_period", "security_measures"],
                "relationships": [
                    {{"target": "DataCategory", "type": "processes", "cardinality": "many-to-many"}},
                    {{"target": "LegalBasis", "type": "based_on", "cardinality": "many-to-one"}},
                    {{"target": "Jurisdiction", "type": "subject_to", "cardinality": "many-to-many"}}
                ],
                "constraints": ["purpose_limitation", "data_minimization", "storage_limitation"],
                "validation_rules": ["legal_basis_validation", "retention_period_compliance", "cross_border_transfer_validation"]
            }}
        ],
        "jurisdictional_extensions": [
            {{
                "jurisdiction": "GDPR_EU",
                "additional_attributes": ["lawfulness_assessment", "data_protection_impact_assessment", "legitimate_interest_assessment"],
                "specific_constraints": ["consent_requirements", "special_category_restrictions"],
                "reporting_requirements": ["supervisory_authority_notifications", "breach_notifications"]
            }}
        ],
        "financial_specializations": [
            {{
                "domain": "banking",
                "specific_entities": ["BankingTransaction", "CustomerDueDiligence", "CreditAssessment"],
                "regulatory_requirements": ["basel_compliance", "pci_dss_compliance", "aml_requirements"],
                "data_categories": ["account_information", "transaction_history", "credit_scores", "kyc_data"]
            }}
        ],
        "implementation_framework": {{
            "deployment_patterns": ["centralized", "federated", "hybrid"],
            "integration_standards": ["api_specifications", "data_exchange_formats", "security_protocols"],
            "validation_procedures": ["automated_compliance_checks", "manual_review_processes", "audit_trails"],
            "reporting_mechanisms": ["regulatory_reports", "internal_dashboards", "audit_reports"]
        }},
        "compliance_validation": {{
            "automated_checks": ["data_category_validation", "legal_basis_verification", "retention_compliance"],
            "risk_assessment": ["privacy_impact_assessment", "cross_border_transfer_risk", "security_risk_evaluation"],
            "gap_identification": ["missing_legal_basis", "inadequate_security_measures", "non_compliant_retention"],
            "remediation_guidance": ["priority_recommendations", "implementation_steps", "timeline_guidance"]
        }}
    }},
    "implementation_guidance": {{
        "quick_start_guide": ["initial_setup_steps", "basic_configuration", "first_processing_activity"],
        "advanced_configuration": ["multi_jurisdiction_setup", "financial_sector_customization", "integration_patterns"],
        "best_practices": ["data_governance_alignment", "privacy_by_design_implementation", "continuous_compliance_monitoring"],
        "troubleshooting": ["common_issues", "error_resolution", "support_contacts"]
    }},
    "validation_and_certification": {{
        "self_assessment_tools": ["compliance_checklists", "gap_analysis_templates", "risk_assessment_forms"],
        "third_party_validation": ["audit_frameworks", "certification_processes", "independent_assessments"],
        "continuous_monitoring": ["automated_compliance_monitoring", "regular_reviews", "update_procedures"]
    }}
}}
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            content = response.content
            
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = content[json_start:json_end]
                final_metamodel = json.loads(json_str)
                
                self.analysis_state["final_metamodel"] = final_metamodel
                
                return final_metamodel
            
        except Exception as e:
            logger.error(f"Failed to generate final metamodel: {e}")
            return {"error": "Failed to generate metamodel"}
    
    def generate_comprehensive_report(self) -> str:
        """Generate comprehensive analysis and metamodel report"""
        logger.info("Generating comprehensive RoPA metamodel report")
        
        if not self.analysis_state["final_metamodel"]:
            self.generate_final_metamodel()
        
        # Generate detailed report using all collected data
        base_url = os.getenv("OPENAI_BASE_URL")
        api_key = os.getenv("OPENAI_API_KEY")
        
        llm_kwargs = {"model": "o3-mini", "reasoning_effort": "high"}
        if base_url:
            llm_kwargs["base_url"] = base_url
        if api_key:
            llm_kwargs["api_key"] = api_key
        
        llm = ChatOpenAI(**llm_kwargs)
        
        report_data = {
            "analysis_state": self.analysis_state,
            "jurisdiction_requirements": {
                jurisdiction.value: requirements.__dict__ 
                for jurisdiction, requirements in JURISDICTION_REQUIREMENTS.items()
            },
            "timestamp": datetime.now().isoformat()
        }
        
        prompt = f"""Generate a comprehensive executive report on the Global GDPR Record of Processing Activities (RoPA) Metamodel for Financial Institutions.

Analysis Data: {json.dumps(report_data, default=str)}

Create a detailed report with the following structure:

# Executive Summary
- Project overview and objectives
- Key findings and recommendations
- Critical compliance gaps identified
- Implementation timeline and priorities

# Methodology
- Document analysis approach
- AI-enhanced extraction techniques
- Iterative refinement process
- Validation procedures

# Global Regulatory Landscape Analysis (GDPR-Based Framework)
- GDPR (EU) requirements and implications
- UK GDPR specific considerations  
- GDPR-based California requirements (CCPA mapped to GDPR)
- GDPR-based Canada alignment (PIPEDA harmonized with GDPR)
- GDPR-based Brazil compliance (LGPD aligned with GDPR principles)
- GDPR-based Asian jurisdictions (APPI, PDPA using GDPR framework)
- Cross-border data transfer implications under GDPR Article 44-49

# Financial Sector Specific Requirements
- Banking industry processing activities
- Insurance data handling requirements
- Investment management compliance
- Payment processing obligations
- Credit scoring and risk assessment
- Anti-money laundering (AML) considerations
- Know Your Customer (KYC) requirements

# Metamodel Architecture
- Core entity definitions
- Relationship mappings
- Jurisdictional extension patterns
- Financial sector specializations
- Implementation framework
- Validation and compliance mechanisms

# Implementation Roadmap
- Phase 1: Foundation setup
- Phase 2: Core implementation
- Phase 3: Jurisdictional extensions
- Phase 4: Financial specializations
- Phase 5: Advanced features and optimization

# Risk Assessment and Mitigation
- Compliance risks identified
- Implementation challenges
- Mitigation strategies
- Contingency planning

# Technology Architecture
- System integration requirements
- Data storage and processing
- Security and privacy safeguards
- Scalability considerations
- Performance optimization

# Governance and Maintenance
- Ongoing governance structure
- Regular review processes
- Update and maintenance procedures
- Training and change management

# Conclusion and Next Steps
- Key deliverables
- Success metrics
- Future enhancements
- Support and maintenance plan

Provide a comprehensive, professional report that can be used by executives, compliance officers, and technical teams to understand and implement the global RoPA metamodel.
"""
        
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            return response.content
        
        except Exception as e:
            logger.error(f"Failed to generate comprehensive report: {e}")
            return f"Error generating report: {e}"
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        try:
            # Elasticsearch stats
            es_stats = self.vector_engine.client.indices.stats(index=self.vector_engine.index_name)
            doc_count = es_stats.body["indices"][self.vector_engine.index_name]["total"]["docs"]["count"]
            
            # Graph stats
            node_stats = self.graph_engine.graph.query("MATCH (n) RETURN count(n) as nodes")
            node_count = node_stats.result_set[0][0] if node_stats.result_set else 0
            
            rel_stats = self.graph_engine.graph.query("MATCH ()-[r]->() RETURN count(r) as rels")
            rel_count = rel_stats.result_set[0][0] if rel_stats.result_set else 0
            
            # Processing activity stats
            pa_stats = self.graph_engine.graph.query("MATCH (pa:ProcessingActivity) RETURN count(pa) as activities")
            pa_count = pa_stats.result_set[0][0] if pa_stats.result_set else 0
            
            # Data category stats
            dc_stats = self.graph_engine.graph.query("MATCH (dc:DataCategory) RETURN count(dc) as categories")
            dc_count = dc_stats.result_set[0][0] if dc_stats.result_set else 0
            
            # Jurisdiction stats
            j_stats = self.graph_engine.graph.query("MATCH (j:Jurisdiction) RETURN count(j) as jurisdictions")
            j_count = j_stats.result_set[0][0] if j_stats.result_set else 0
            
            return {
                "elasticsearch_documents": doc_count,
                "graph_nodes": node_count,
                "graph_relationships": rel_count,
                "processing_activities": pa_count,
                "data_categories": dc_count,
                "jurisdictions": j_count,
                "processed_documents": len(self.analysis_state["processed_documents"]),
                "extracted_concepts": len(self.analysis_state["extracted_concepts"]),
                "metamodel_iterations": len(self.analysis_state["metamodel_iterations"]),
                "compliance_gaps": len(self.analysis_state.get("compliance_gaps", {}).get("coverage_gaps", [])),
                "has_final_metamodel": self.analysis_state["final_metamodel"] is not None,
                "system_status": "operational"
            }
        except Exception as e:
            return {"error": str(e), "system_status": "error"}

def main():
    """Main execution function for the Global RoPA Metamodel System"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Global GDPR RoPA Metamodel System for Financial Institutions")
    parser.add_argument("--ingest", nargs="+", help="Paths to regulatory documents to ingest")
    parser.add_argument("--analyze", action="store_true", help="Perform iterative metamodel analysis")
    parser.add_argument("--generate-metamodel", action="store_true", help="Generate final metamodel")
    parser.add_argument("--generate-report", action="store_true", help="Generate comprehensive report")
    parser.add_argument("--stats", action="store_true", help="Show system statistics")
    parser.add_argument("--iterations", type=int, default=3, help="Number of analysis iterations")
    
    args = parser.parse_args()
    
    # Load configuration
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "openai_base_url": os.getenv("OPENAI_BASE_URL"),
        "elasticsearch_host": os.getenv("ELASTICSEARCH_HOST", "http://localhost:9200"),
        "elasticsearch_username": os.getenv("ELASTICSEARCH_USERNAME"),
        "elasticsearch_password": os.getenv("ELASTICSEARCH_PASSWORD"),
        "falkordb_host": os.getenv("FALKORDB_HOST", "localhost"),
        "falkordb_port": int(os.getenv("FALKORDB_PORT", 6379)),
        "falkordb_password": os.getenv("FALKORDB_PASSWORD")
    }
    
    # Validate configuration
    if not config["openai_api_key"]:
        print("❌ Error: OPENAI_API_KEY environment variable is required")
        return
    
    if not config["elasticsearch_host"].startswith(('http://', 'https://')):
        print("❌ Error: ELASTICSEARCH_HOST must include schema (http:// or https://)")
        return
    
    print("🔧 Global RoPA Metamodel System Configuration:")
    print(f"  OpenAI Endpoint: {config['openai_base_url'] or 'Standard OpenAI API'}")
    print(f"  Elasticsearch: {config['elasticsearch_host']}")
    print(f"  FalkorDB: {config['falkordb_host']}:{config['falkordb_port']}")
    print("  Enhanced for: Financial Institutions, Global Compliance")
    
    try:
        # Initialize system
        system = GlobalRopaMetamodelSystem(
            openai_api_key=config["openai_api_key"],
            openai_base_url=config["openai_base_url"],
            elasticsearch_host=config["elasticsearch_host"],
            elasticsearch_username=config["elasticsearch_username"],
            elasticsearch_password=config["elasticsearch_password"],
            falkordb_host=config["falkordb_host"],
            falkordb_port=config["falkordb_port"],
            falkordb_password=config["falkordb_password"]
        )
        
        print("✅ Global RoPA Metamodel System initialized successfully")
        
        # Execute requested operations
        if args.ingest:
            print(f"\n📄 Ingesting {len(args.ingest)} regulatory documents...")
            result = system.ingest_regulatory_documents(args.ingest)
            print(f"✅ Ingestion completed: {result}")
        
        if args.analyze:
            print(f"\n🔍 Performing iterative metamodel analysis ({args.iterations} iterations)...")
            result = system.iterative_metamodel_analysis(args.iterations)
            print(f"✅ Analysis completed: {result}")
        
        if args.generate_metamodel:
            print("\n🏗️ Generating final comprehensive metamodel...")
            result = system.generate_final_metamodel()
            print("✅ Final metamodel generated successfully")
            
            # Save metamodel to file
            with open("gdpr_ropa_metamodel.json", "w") as f:
                json.dump(result, f, indent=2, default=str)
            print("💾 Metamodel saved to gdpr_ropa_metamodel.json")
        
        if args.generate_report:
            print("\n📊 Generating comprehensive report...")
            report = system.generate_comprehensive_report()
            
            # Save report to file
            with open("gdpr_ropa_analysis_report.md", "w") as f:
                f.write(report)
            print("💾 Report saved to gdpr_ropa_analysis_report.md")
            
            # Display summary
            print("\n" + "="*80)
            print("EXECUTIVE SUMMARY")
            print("="*80)
            print(report[:1000] + "..." if len(report) > 1000 else report)
        
        if args.stats:
            print("\n📈 System Statistics:")
            stats = system.get_system_statistics()
            for key, value in stats.items():
                print(f"  {key.replace('_', ' ').title()}: {value}")
        
        # If no specific action requested, show help
        if not any([args.ingest, args.analyze, args.generate_metamodel, args.generate_report, args.stats]):
            parser.print_help()
            print("\n💡 Example usage:")
            print("  python gdpr_ropa_system.py --ingest /path/to/gdpr/docs/*.pdf")
            print("  python gdpr_ropa_system.py --analyze --iterations 5")
            print("  python gdpr_ropa_system.py --generate-metamodel")
            print("  python gdpr_ropa_system.py --generate-report")
            print("  python gdpr_ropa_system.py --stats")
    
    except Exception as e:
        print(f"❌ System error: {e}")
        return

if __name__ == "__main__":
    main()
