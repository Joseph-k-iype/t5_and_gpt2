#!/usr/bin/env python3
"""
TTL to FalkorDB Property Graph Converter
Converts RDF triples from TTL files to property graph model and ingests into FalkorDB
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import redis
import falkordb
import hashlib
import json
import time
import logging
from typing import Dict, Set, List, Tuple, Any
from urllib.parse import urlparse
import re
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TTLToFalkorDB:
    def __init__(self, redis_host='localhost', redis_port=6379, db_name='knowledge_graph', password=None):
        """Initialize the converter with FalkorDB connection"""
        # Set longer timeouts for large operations
        socket_timeout = 300  # 5 minutes
        socket_connect_timeout = 30  # 30 seconds
        
        if password:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                password=password, 
                decode_responses=True,
                socket_timeout=socket_timeout,
                socket_connect_timeout=socket_connect_timeout,
                retry_on_timeout=True,
                health_check_interval=30
            )
            self.db = falkordb.FalkorDB(
                host=redis_host, 
                port=redis_port, 
                password=password
            ).select_graph(db_name)
        else:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                decode_responses=True,
                socket_timeout=socket_timeout,
                socket_connect_timeout=socket_connect_timeout,
                retry_on_timeout=True,
                health_check_interval=30
            )
            self.db = falkordb.FalkorDB(
                host=redis_host, 
                port=redis_port
            ).select_graph(db_name)
        self.batch_size = 1000
        self.node_cache = set()
        self.edge_cache = set()
        
    def clean_identifier(self, uri_or_literal: str) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            # Extract meaningful part from URI for label
            parsed = urlparse(str(uri_or_literal))
            if parsed.fragment:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.fragment)
            elif parsed.path:
                local_name = parsed.path.split('/')[-1]
                if local_name:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', local_name)
                else:
                    # Use domain name if no local name
                    return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.netloc)
            else:
                return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
        else:
            return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
    
    def create_node_id(self, resource) -> str:
        """Create node ID - use URI directly for URIRefs, special handling for blank nodes"""
        if isinstance(resource, BNode):
            return f"_:bnode_{str(resource)}"
        else:
            # Use the actual URI as the ID
            return str(resource)
    
    def create_node_label(self, resource) -> str:
        """Create node label from URI"""
        if isinstance(resource, BNode):
            return "BlankNode"
        else:
            # Extract meaningful label from URI
            uri_str = str(resource)
            parsed = urlparse(uri_str)
            
            # Try to get a meaningful label
            if parsed.fragment:
                return self.clean_identifier(URIRef(parsed.fragment))
            elif parsed.path:
                parts = parsed.path.strip('/').split('/')
                if parts and parts[-1]:
                    return self.clean_identifier(URIRef(parts[-1]))
                else:
                    # Use domain name
                    return self.clean_identifier(URIRef(parsed.netloc))
            else:
                # Fallback to cleaned full URI
                return self.clean_identifier(resource)
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, Any]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path:
            parts = parsed.path.strip('/').split('/')
            if parts and parts[-1]:
                properties['local_name'] = parts[-1]
                properties['namespace'] = uri_str.replace(parts[-1], '').rstrip('/')
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[Any, str]:
        """Process literal value and return (value, datatype)"""
        if literal.datatype:
            datatype = str(literal.datatype)
            # Handle common datatypes
            if 'integer' in datatype or 'int' in datatype:
                try:
                    return int(literal), 'integer'
                except:
                    return str(literal), 'string'
            elif 'decimal' in datatype or 'double' in datatype or 'float' in datatype:
                try:
                    return float(literal), 'float'
                except:
                    return str(literal), 'string'
            elif 'boolean' in datatype:
                return str(literal).lower() in ('true', '1'), 'boolean'
            elif 'date' in datatype:
                return str(literal), 'date'
            else:
                return str(literal), 'string'
        else:
            return str(literal), 'string'
    
    def parse_ttl_streaming(self, ttl_file_path: str):
        """Parse TTL file in streaming fashion"""
        logger.info(f"Starting to parse {ttl_file_path}")
        
        graph = Graph()
        
        # Parse the file
        try:
            graph.parse(ttl_file_path, format='turtle')
            logger.info(f"Successfully parsed TTL file. Found {len(graph)} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        return graph
    
    def convert_to_property_graph(self, graph: Graph):
        """Convert RDF graph to property graph model"""
        logger.info("Converting RDF triples to property graph model...")
        
        nodes = {}
        edges = []
        literal_properties = {}  # subject -> {predicate: value}
        
        total_triples = len(graph)
        processed = 0
        
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                processed += 1
                pbar.update(1)
                
                subject_id = self.create_node_id(subject)
                subject_label = self.create_node_label(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Create subject node if not exists
                if subject_id not in nodes:
                    if isinstance(subject, URIRef):
                        properties = self.extract_properties_from_uri(subject)
                        # Add the URI as the primary identifier
                        properties['uri'] = str(subject)
                    else:  # BNode
                        properties = {'uri': str(subject), 'blank_node_id': str(subject)}
                    
                    nodes[subject_id] = {
                        'id': subject_id,
                        'label': subject_label,
                        'properties': properties
                    }
                
                # Handle object
                if isinstance(obj, Literal):
                    # Object is a literal - add as property to subject node
                    if subject_id not in literal_properties:
                        literal_properties[subject_id] = {}
                    
                    value, datatype = self.process_literal_value(obj)
                    literal_properties[subject_id][predicate_clean] = {
                        'value': value,
                        'datatype': datatype
                    }
                    
                    # Also store language if present
                    if obj.language:
                        literal_properties[subject_id][f"{predicate_clean}_lang"] = obj.language
                
                else:
                    # Object is a resource - create edge
                    object_id = self.create_node_id(obj)
                    object_label = self.create_node_label(obj)
                    
                    # Create object node if not exists
                    if object_id not in nodes:
                        if isinstance(obj, URIRef):
                            properties = self.extract_properties_from_uri(obj)
                            properties['uri'] = str(obj)
                        else:  # BNode
                            properties = {'uri': str(obj), 'blank_node_id': str(obj)}
                        
                        nodes[object_id] = {
                            'id': object_id,
                            'label': object_label,
                            'properties': properties
                        }
                    
                    # Create edge
                    edge = {
                        'from': subject_id,
                        'to': object_id,
                        'from_label': subject_label,
                        'to_label': object_label,
                        'relationship': predicate_clean,
                        'properties': {
                            'predicate_uri': str(predicate)
                        }
                    }
                    edges.append(edge)
        
        # Merge literal properties into nodes
        for node_id, props in literal_properties.items():
            if node_id in nodes:
                for prop_name, prop_data in props.items():
                    nodes[node_id]['properties'][prop_name] = prop_data['value']
                    nodes[node_id]['properties'][f"{prop_name}_datatype"] = prop_data['datatype']
        
        logger.info(f"Conversion complete. Created {len(nodes)} nodes and {len(edges)} edges.")
        return list(nodes.values()), edges
    
    def escape_cypher_string(self, value: str) -> str:
        """Escape string for Cypher query"""
        if isinstance(value, str):
            # Escape single quotes, double quotes, backslashes, and newlines
            escaped = value.replace("\\", "\\\\")  # Escape backslashes first
            escaped = escaped.replace("'", "\\'")
            escaped = escaped.replace('"', '\\"')
            escaped = escaped.replace('\n', '\\n')
            escaped = escaped.replace('\r', '\\r')
            escaped = escaped.replace('\t', '\\t')
            return escaped
        return str(value)
    
    def escape_uri_for_cypher(self, uri: str) -> str:
        """Escape URI specifically for use in Cypher queries"""
        # For URIs used as node IDs, we need to be extra careful with escaping
        escaped = self.escape_cypher_string(uri)
        return escaped
    
    def create_cypher_properties(self, properties: Dict[str, Any]) -> str:
        """Convert properties dict to Cypher properties string"""
        prop_parts = []
        for key, value in properties.items():
            # Ensure key is valid Cypher identifier
            clean_key = re.sub(r'[^a-zA-Z0-9_]', '_', str(key))
            if clean_key != key:
                logger.debug(f"Property key changed from '{key}' to '{clean_key}'")
            
            if isinstance(value, str):
                escaped_value = self.escape_cypher_string(value)
                prop_parts.append(f"{clean_key}: '{escaped_value}'")
            elif isinstance(value, (int, float)):
                prop_parts.append(f"{clean_key}: {value}")
            elif isinstance(value, bool):
                prop_parts.append(f"{clean_key}: {str(value).lower()}")
            else:
                escaped_value = self.escape_cypher_string(str(value))
                prop_parts.append(f"{clean_key}: '{escaped_value}'")
        
        return "{" + ", ".join(prop_parts) + "}"
    
    def ingest_nodes_bulk(self, nodes: List[Dict]):
        """Ingest all nodes using bulk operations for maximum speed"""
        if not nodes:
            return
            
        logger.info("Using bulk ingestion strategy for maximum speed...")
        
        # Group nodes by label for efficient bulk operations
        nodes_by_label = {}
        for node in nodes:
            label = node['label']
            if label not in nodes_by_label:
                nodes_by_label[label] = []
            nodes_by_label[label].append(node)
        
        total_processed = 0
        
        for label, label_nodes in nodes_by_label.items():
            logger.info(f"Bulk inserting {len(label_nodes)} nodes with label '{label}'")
            
            # Process in chunks to avoid memory issues
            chunk_size = 1000  # Larger chunks for bulk operations
            
            for i in range(0, len(label_nodes), chunk_size):
                chunk = label_nodes[i:i + chunk_size]
                
                try:
                    # Build bulk CREATE query using UNWIND (much faster than individual MERGEs)
                    # Prepare node data for UNWIND
                    node_data = []
                    for node in chunk:
                        node_props = dict(node['properties'])
                        node_props['uri'] = node['id']  # Add URI as a property
                        node_data.append(node_props)
                    
                    # Create parameterized query for bulk insert
                    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', label)
                    if not clean_label or clean_label[0].isdigit():
                        clean_label = f"Node_{clean_label}"
                    
                    # Use UNWIND for bulk operations - much faster than individual queries
                    cypher = f"""
                    UNWIND $nodes AS nodeData
                    CREATE (n:{clean_label})
                    SET n = nodeData
                    """
                    
                    # Execute bulk insert with parameters
                    result = self.execute_query_with_retry(cypher, node_data)
                    total_processed += len(chunk)
                    
                    if total_processed % 5000 == 0:
                        logger.info(f"Bulk inserted {total_processed}/{len(nodes)} nodes")
                    
                except Exception as e:
                    logger.error(f"Error in bulk node insertion for label '{label}': {e}")
                    # Fallback to individual inserts for this chunk
                    self._fallback_individual_nodes(chunk, clean_label)
                    total_processed += len(chunk)
        
        logger.info(f"Bulk node ingestion completed: {total_processed} nodes")
    
    def ingest_edges_bulk(self, edges: List[Dict]):
        """Ingest all edges using bulk operations for maximum speed"""
        if not edges:
            return
            
        logger.info("Using bulk edge ingestion strategy...")
        
        # Group edges by relationship type for efficient bulk operations
        edges_by_relationship = {}
        for edge in edges:
            rel_type = edge['relationship']
            if rel_type not in edges_by_relationship:
                edges_by_relationship[rel_type] = []
            edges_by_relationship[rel_type].append(edge)
        
        total_processed = 0
        
        for rel_type, rel_edges in edges_by_relationship.items():
            logger.info(f"Bulk inserting {len(rel_edges)} edges with relationship '{rel_type}'")
            
            # Process in chunks
            chunk_size = 500  # Smaller chunks for edges due to complexity
            
            for i in range(0, len(rel_edges), chunk_size):
                chunk = rel_edges[i:i + chunk_size]
                
                try:
                    # Prepare edge data for bulk insert
                    edge_data = []
                    for edge in chunk:
                        edge_item = {
                            'from_uri': edge['from'],
                            'to_uri': edge['to'],
                            'properties': edge['properties']
                        }
                        edge_data.append(edge_item)
                    
                    clean_rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
                    if not clean_rel_type or clean_rel_type[0].isdigit():
                        clean_rel_type = f"REL_{clean_rel_type}"
                    
                    # Use UNWIND for bulk edge creation
                    cypher = f"""
                    UNWIND $edges AS edgeData
                    MATCH (from {{uri: edgeData.from_uri}})
                    MATCH (to {{uri: edgeData.to_uri}})
                    CREATE (from)-[r:{clean_rel_type}]->(to)
                    SET r = edgeData.properties
                    """
                    
                    result = self.execute_query_with_retry(cypher, edge_data)
                    total_processed += len(chunk)
                    
                    if total_processed % 2500 == 0:
                        logger.info(f"Bulk inserted {total_processed}/{len(edges)} edges")
                    
                except Exception as e:
                    logger.error(f"Error in bulk edge insertion for relationship '{rel_type}': {e}")
                    # Fallback to individual inserts for this chunk
                    self._fallback_individual_edges(chunk, clean_rel_type)
                    total_processed += len(chunk)
        
        logger.info(f"Bulk edge ingestion completed: {total_processed} edges")
    
    def _fallback_individual_nodes(self, nodes: List[Dict], clean_label: str):
        """Fallback to individual node inserts if bulk fails"""
        logger.warning("Falling back to individual node inserts...")
        for node in nodes:
            try:
                escaped_uri = self.escape_uri_for_cypher(node['id'])
                properties_str = self.create_cypher_properties(node['properties'])
                
                cypher = f"CREATE (n:{clean_label} {{uri: '{escaped_uri}'}}) SET n += {properties_str}"
                self.execute_query_with_retry(cypher)
            except Exception as e:
                logger.error(f"Failed to insert individual node {node['id']}: {e}")
    
    def _fallback_individual_edges(self, edges: List[Dict], clean_rel_type: str):
        """Fallback to individual edge inserts if bulk fails"""
        logger.warning("Falling back to individual edge inserts...")
        for edge in edges:
            try:
                from_uri = self.escape_uri_for_cypher(edge['from'])
                to_uri = self.escape_uri_for_cypher(edge['to'])
                properties_str = self.create_cypher_properties(edge['properties'])
                
                cypher = f"""
                MATCH (from {{uri: '{from_uri}'}})
                MATCH (to {{uri: '{to_uri}'}})
                CREATE (from)-[r:{clean_rel_type}]->(to)
                SET r += {properties_str}
                """
                self.execute_query_with_retry(cypher.strip())
            except Exception as e:
                logger.error(f"Failed to insert individual edge {edge['from']}->{edge['to']}: {e}")
    
    def execute_query_with_retry(self, query, parameters=None, max_retries=3, delay=1):
        """Execute query with retry logic and parameter support"""
        for attempt in range(max_retries):
            try:
                if parameters:
                    # For parameterized queries, pass parameters correctly
                    if isinstance(parameters, list):
                        # For UNWIND operations, wrap in a dict
                        param_dict = {"nodes": parameters} if "UNWIND $nodes" in query else {"edges": parameters}
                        result = self.db.query(query, param_dict)
                    else:
                        result = self.db.query(query, parameters)
                else:
                    result = self.db.query(query)
                return result
            except (redis.exceptions.TimeoutError, redis.exceptions.ConnectionError) as e:
                logger.warning(f"Query timeout/connection error (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay * (attempt + 1))  # Exponential backoff
                    # Try to reconnect
                    try:
                        self.redis_client.ping()
                    except:
                        logger.warning("Reconnecting to FalkorDB...")
                        time.sleep(2)
                else:
                    logger.error(f"Query failed after {max_retries} attempts")
                    raise
            except Exception as e:
                logger.error(f"Unexpected error in query: {e}")
                if parameters:
                    logger.error(f"Query: {query[:200]}... (with {len(parameters) if isinstance(parameters, list) else 1} parameters)")
                else:
                    logger.error(f"Query: {query[:200]}...")
                raise
    
    def ingest_nodes_batch(self, nodes: List[Dict], batch_start: int, batch_end: int):
        """Fallback batch method for individual node inserts"""
        batch_nodes = nodes[batch_start:batch_end]
        
        if not batch_nodes:
            return
        
        for node in batch_nodes:
            try:
                node_id = node['id']  
                node_label = node['label']  
                escaped_node_id = self.escape_uri_for_cypher(node_id)
                
                clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', node_label)
                if not clean_label or clean_label[0].isdigit():
                    clean_label = f"Node_{clean_label}"
                
                properties_str = self.create_cypher_properties(node['properties'])
                
                cypher = f"MERGE (n:{clean_label} {{uri: '{escaped_node_id}'}}) SET n += {properties_str}"
                self.execute_query_with_retry(cypher)
                
            except Exception as e:
                logger.error(f"Error inserting node {node_id}: {e}")
                continue
    
    def ingest_edges_batch(self, edges: List[Dict], batch_start: int, batch_end: int):
        """Fallback batch method for individual edge inserts"""
        batch_edges = edges[batch_start:batch_end]
        
        if not batch_edges:
            return
        
        for edge in batch_edges:
            try:
                from_id = edge['from']  
                to_id = edge['to']      
                from_label = edge['from_label']
                to_label = edge['to_label']
                relationship = edge['relationship']
                
                clean_from_label = re.sub(r'[^a-zA-Z0-9_]', '_', from_label)
                clean_to_label = re.sub(r'[^a-zA-Z0-9_]', '_', to_label)
                clean_relationship = re.sub(r'[^a-zA-Z0-9_]', '_', relationship)
                
                if not clean_from_label or clean_from_label[0].isdigit():
                    clean_from_label = f"Node_{clean_from_label}"
                if not clean_to_label or clean_to_label[0].isdigit():
                    clean_to_label = f"Node_{clean_to_label}"
                if not clean_relationship or clean_relationship[0].isdigit():
                    clean_relationship = f"REL_{clean_relationship}"
                
                escaped_from_id = self.escape_uri_for_cypher(from_id)
                escaped_to_id = self.escape_uri_for_cypher(to_id)
                
                properties_str = self.create_cypher_properties(edge['properties'])
                
                cypher = f"""
                MATCH (from:{clean_from_label} {{uri: '{escaped_from_id}'}})
                MATCH (to:{clean_to_label} {{uri: '{escaped_to_id}'}})
                MERGE (from)-[r:{clean_relationship}]->(to)
                SET r += {properties_str}
                """
                
                self.execute_query_with_retry(cypher.strip())
                
            except Exception as e:
                logger.error(f"Error inserting edge {from_id}->{to_id}: {e}")
                continue
    
    def ingest_to_falkordb(self, nodes: List[Dict], edges: List[Dict]):
        """Ingest nodes and edges into FalkorDB with batching"""
        logger.info("Starting ingestion into FalkorDB...")
        
        # Create indexes for better performance
        try:
            self.db.query("CREATE INDEX ON :Node(id)")
            self.db.query("CREATE INDEX ON :Resource(uri)")
        except Exception as e:
            logger.warning(f"Index creation warning (may already exist): {e}")
        
        # Ingest nodes in batches
        logger.info(f"Ingesting {len(nodes)} nodes in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(nodes), self.batch_size), desc="Ingesting nodes"):
            batch_end = min(i + self.batch_size, len(nodes))
            self.ingest_nodes_batch(nodes, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        # Ingest edges in batches
        logger.info(f"Ingesting {len(edges)} edges in batches of {self.batch_size}...")
        for i in tqdm(range(0, len(edges), self.batch_size), desc="Ingesting edges"):
            batch_end = min(i + self.batch_size, len(edges))
            self.ingest_edges_batch(edges, i, batch_end)
            time.sleep(0.01)  # Small delay to prevent overwhelming the database
        
        logger.info("Ingestion completed successfully!")
    
    def convert_and_ingest(self, ttl_file_path: str):
        """Main method to convert TTL file and ingest into FalkorDB"""
        try:
            # Parse TTL file
            graph = self.parse_ttl_streaming(ttl_file_path)
            
            # Convert to property graph
            nodes, edges = self.convert_to_property_graph(graph)
            
            # Ingest into FalkorDB
            self.ingest_to_falkordb(nodes, edges)
            
            # Print summary
            result = self.db.query("MATCH (n) RETURN count(n) as node_count")
            node_count = result.result_set[0][0] if result.result_set else 0
            
            result = self.db.query("MATCH ()-[r]->() RETURN count(r) as edge_count")
            edge_count = result.result_set[0][0] if result.result_set else 0
            
            logger.info(f"Final database statistics:")
            logger.info(f"  Nodes: {node_count}")
            logger.info(f"  Edges: {edge_count}")
            
        except Exception as e:
            logger.error(f"Error in conversion and ingestion: {e}")
            raise

def main():
    """Main function with example usage"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description='Convert TTL file to FalkorDB property graph')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--host', default='localhost', help='Redis/FalkorDB host (default: localhost)')
    parser.add_argument('--port', type=int, default=6379, help='Redis/FalkorDB port (default: 6379)')
    parser.add_argument('--db_name', default='knowledge_graph', help='Database name (default: knowledge_graph)')
    parser.add_argument('--batch_size', type=int, default=0, help='Batch size for ingestion (0=auto-detect)')
    parser.add_argument('--password', help='Redis password if required')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        return
    
    # Auto-detect batch size based on file size (not used for bulk operations but kept for fallback)
    batch_size = args.batch_size
    if batch_size == 0:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        if file_size_mb > 1000:  # >1GB
            batch_size = 100  # Conservative for fallback operations
            logger.info(f"Large file detected ({file_size_mb:.1f}MB), fallback batch_size={batch_size}")
        elif file_size_mb > 100:  # >100MB
            batch_size = 200  # Medium files
            logger.info(f"Medium file detected ({file_size_mb:.1f}MB), fallback batch_size={batch_size}")
        else:
            batch_size = 500  # Small files
            logger.info(f"Small file detected ({file_size_mb:.1f}MB), fallback batch_size={batch_size}")
    
    logger.info("=== BULK INGESTION MODE ===")
    logger.info("Using optimized BULK operations:")
    logger.info("• UNWIND for massive performance gains")
    logger.info("• Grouped by label/relationship type")
    logger.info("• CREATE instead of MERGE where possible")
    logger.info("• Fallback to batch mode if needed")
    logger.info("• Expected speed: 1000-10000+ items/second")
    logger.info("==============================")
    
    # Test connection first
    try:
        if args.password:
            test_client = redis.Redis(host=args.host, port=args.port, password=args.password, decode_responses=True)
        else:
            test_client = redis.Redis(host=args.host, port=args.port, decode_responses=True)
        test_client.ping()
        logger.info(f"Successfully connected to FalkorDB at {args.host}:{args.port}")
    except redis.exceptions.AuthenticationError:
        logger.error(f"Authentication failed. If using password 'falkordb', try:")
        logger.error(f"  python3 ttl_converter.py {args.ttl_file} --password falkordb")
        logger.error(f"  OR with username: --password falkordb (username 'default' is used by default)")
        return
    except Exception as e:
        logger.error(f"Failed to connect to FalkorDB at {args.host}:{args.port}: {e}")
        logger.error("Please ensure FalkorDB is running and accessible")
        logger.error("If using authentication, add --password your_password")
        return
    
    # Create converter
    converter = TTLToFalkorDB(
        redis_host=args.host,
        redis_port=args.port,
        db_name=args.db_name,
        password=args.password
    )
    converter.batch_size = batch_size
    
    # Convert and ingest
    converter.convert_and_ingest(args.ttl_file)

if __name__ == "__main__":
    main()
