#!/usr/bin/env python3
"""
Dynamic GDPR Knowledge Discovery System - Multi-Agent Architecture

This system dynamically discovers ALL concepts, relationships, and structures from PDF documents
without any hardcoded assumptions. Everything is extracted by LLM analysis of actual document content.

Features:
- Dynamic concept discovery from documents
- LLM-driven relationship extraction
- Flexible knowledge graph construction
- Document-driven ontology building
- No hardcoded regulatory structures
- Multi-agent ReAct, Reflection, and Supervisor architecture

Author: AI Assistant
Date: 2025
Version: 5.0.0 - Dynamic Discovery System
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Union
from datetime import datetime
import re
from collections import defaultdict
from dataclasses import dataclass, asdict
from enum import Enum
import argparse
import pickle
from pathlib import Path
import uuid

# Core dependencies
import PyMuPDF as pymupdf
from elasticsearch import Elasticsearch
from falkordb import FalkorDB
from openai import OpenAI

# LangChain and LangGraph for multi-agent architecture
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command
from langgraph.prebuilt import create_react_agent

# =============================================================================
# GLOBAL CONFIGURATION
# =============================================================================

GLOBAL_CONFIG = {
    # OpenAI Configuration - Only o3-mini
    "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", "your_openai_api_key_here"),
    "OPENAI_BASE_URL": os.getenv("OPENAI_BASE_URL", None),
    
    # Elasticsearch Configuration
    "ELASTICSEARCH_HOST": os.getenv("ELASTICSEARCH_HOST", "https://localhost:9200"),
    "ELASTICSEARCH_USERNAME": os.getenv("ELASTICSEARCH_USERNAME", "elastic"),
    "ELASTICSEARCH_PASSWORD": os.getenv("ELASTICSEARCH_PASSWORD", "changeme"),
    "ELASTICSEARCH_CA_CERTS": os.getenv("ELASTICSEARCH_CA_CERTS", None),
    "ELASTICSEARCH_VERIFY_CERTS": os.getenv("ELASTICSEARCH_VERIFY_CERTS", "false").lower() == "true",
    
    # FalkorDB Configuration
    "FALKORDB_HOST": os.getenv("FALKORDB_HOST", "localhost"),
    "FALKORDB_PORT": int(os.getenv("FALKORDB_PORT", 6379)),
    "FALKORDB_USERNAME": os.getenv("FALKORDB_USERNAME", ""),
    "FALKORDB_PASSWORD": os.getenv("FALKORDB_PASSWORD", ""),
    
    # Document Paths
    "PDF_DOCUMENTS_PATH": os.getenv("PDF_DOCUMENTS_PATH", "./documents"),
    "OUTPUT_PATH": os.getenv("OUTPUT_PATH", "./output"),
    "MEMORY_PATH": os.getenv("MEMORY_PATH", "./memory"),
    
    # System Configuration
    "INDEX_NAME": "dynamic_regulatory_knowledge",
    "GRAPH_NAME": "dynamic_regulatory_ontology",
    "MEMORY_FILE": "dynamic_agent_memory.pkl",
    
    # Embedding Configuration
    "EMBEDDING_MODEL": "text-embedding-3-large",
    "EMBEDDING_DIMENSIONS": 3072,
    
    # Industry Configuration
    "INDUSTRY_TYPE": os.getenv("INDUSTRY_TYPE", "financial_services"),
    "COMPANY_SIZE": os.getenv("COMPANY_SIZE", "large_enterprise")
}

# Configure logging
os.makedirs(GLOBAL_CONFIG["OUTPUT_PATH"], exist_ok=True)
os.makedirs(GLOBAL_CONFIG["MEMORY_PATH"], exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{GLOBAL_CONFIG['OUTPUT_PATH']}/dynamic_discovery_system.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# STATE DEFINITIONS
# =============================================================================

class DynamicDiscoveryState(TypedDict):
    """Dynamic state for regulatory knowledge discovery"""
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document Processing
    processed_documents: List[Dict[str, Any]]
    current_document_chunk: Optional[Dict[str, Any]]
    
    # Dynamically Discovered Concepts (no hardcoding)
    discovered_concepts: Dict[str, List[Dict[str, Any]]]  # keyed by concept type
    discovered_relationships: List[Dict[str, Any]]
    discovered_hierarchies: List[Dict[str, Any]]
    discovered_synonyms: Dict[str, List[str]]
    
    # Multi-agent workflow
    agent_discoveries: Dict[str, List[Dict[str, Any]]]
    reflection_results: List[Dict[str, Any]]
    supervisor_decisions: List[Dict[str, Any]]
    
    # Knowledge enrichment
    concept_clusters: List[Dict[str, Any]]
    semantic_mappings: List[Dict[str, Any]]
    
    # Final outputs
    dynamic_ontology: Optional[Dict[str, Any]]
    ropa_metamodel: Optional[Dict[str, Any]]
    compliance_framework: Optional[Dict[str, Any]]
    industry_analysis: Optional[Dict[str, Any]]

# =============================================================================
# DYNAMIC EMBEDDING ENGINE
# =============================================================================

class DynamicEmbeddingEngine:
    """Dynamic embedding generation with LLM-based synonym discovery"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=GLOBAL_CONFIG["OPENAI_API_KEY"],
            base_url=GLOBAL_CONFIG["OPENAI_BASE_URL"]
        )
        self.model = GLOBAL_CONFIG["EMBEDDING_MODEL"]
        self.dimensions = GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"]
        self.max_chars = 8000
        
        logger.info(f"Initialized dynamic embedding engine with {self.model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            if len(text) > self.max_chars:
                text = text[:self.max_chars]
            
            response = self.client.embeddings.create(
                model=self.model,
                input=text,
                dimensions=self.dimensions
            )
            
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise
    
    def discover_synonyms_and_variants(self, concept: str, context_text: str = "") -> Dict[str, Any]:
        """Dynamically discover synonyms and concept variants using LLM"""
        try:
            prompt = f"""
            Analyze the concept "{concept}" in the following regulatory/legal context and discover all possible synonyms, variants, and related terms that might appear in regulatory documents.

            Context: {context_text[:2000]}

            Discover and return JSON with:
            1. "exact_synonyms": Direct synonyms and alternate spellings
            2. "conceptual_variants": Related terms that refer to similar concepts
            3. "regulatory_terms": Official regulatory language variations
            4. "abbreviations": Common abbreviations and acronyms
            5. "cross_jurisdictional": Terms used in different jurisdictions (EU, UK, US, etc.)
            6. "industry_terms": Industry-specific variations
            7. "formal_informal": Both formal legal terms and informal business terms

            Only include terms that could realistically appear in regulatory documents. Do not invent or hallucinate terms.

            Return only valid JSON:
            """
            
            response = self.client.chat.completions.create(
                model="o3-mini",
                messages=[{"role": "user", "content": prompt}],
                reasoning_effort="high"
            )
            
            content = response.choices[0].message.content
            
            # Extract JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    result = json.loads(content[json_start:json_end])
                    return result
                except json.JSONDecodeError:
                    pass
            
            # Fallback
            return {"exact_synonyms": [], "conceptual_variants": [], "regulatory_terms": []}
            
        except Exception as e:
            logger.error(f"Failed to discover synonyms for {concept}: {e}")
            return {"exact_synonyms": [], "conceptual_variants": [], "regulatory_terms": []}

# =============================================================================
# DYNAMIC VECTOR DATABASE
# =============================================================================

class DynamicVectorDatabase:
    """Elasticsearch-based dynamic vector storage without hardcoded schemas"""
    
    def __init__(self):
        self.embedding_engine = DynamicEmbeddingEngine()
        self.client = self._create_elasticsearch_client()
        self.index_name = GLOBAL_CONFIG["INDEX_NAME"]
        self._create_flexible_index()
    
    def _create_elasticsearch_client(self):
        """Create Elasticsearch client"""
        client_config = {
            "hosts": [GLOBAL_CONFIG["ELASTICSEARCH_HOST"]],
            "request_timeout": 60,
            "max_retries": 3,
            "retry_on_timeout": True
        }
        
        if GLOBAL_CONFIG["ELASTICSEARCH_HOST"].startswith('https://'):
            client_config["use_ssl"] = True
            client_config["verify_certs"] = GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]
            
            if not GLOBAL_CONFIG["ELASTICSEARCH_VERIFY_CERTS"]:
                client_config["ssl_show_warn"] = False
        
        if GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"] and GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]:
            client_config["basic_auth"] = (
                GLOBAL_CONFIG["ELASTICSEARCH_USERNAME"],
                GLOBAL_CONFIG["ELASTICSEARCH_PASSWORD"]
            )
        
        try:
            client = Elasticsearch(**client_config)
            if client.ping():
                logger.info("Successfully connected to Elasticsearch")
                return client
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
        except Exception as e:
            logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise
    
    def _create_flexible_index(self):
        """Create flexible index that can accommodate any discovered concepts"""
        mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "analysis": {
                    "analyzer": {
                        "dynamic_regulatory_analyzer": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "stemmer"
                            ]
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # Core content
                    "text": {
                        "type": "text",
                        "analyzer": "dynamic_regulatory_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "raw": {"type": "text", "analyzer": "standard"}
                        }
                    },
                    "title": {
                        "type": "text",
                        "analyzer": "dynamic_regulatory_analyzer",
                        "boost": 2.0
                    },
                    
                    # Vector embedding
                    "embedding": {
                        "type": "dense_vector",
                        "dims": GLOBAL_CONFIG["EMBEDDING_DIMENSIONS"],
                        "index": True,
                        "similarity": "cosine"
                    },
                    
                    # Document metadata
                    "chunk_id": {"type": "keyword"},
                    "source_document": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    
                    # Dynamically discovered classifications
                    "discovered_concept_type": {"type": "keyword"},
                    "discovered_categories": {"type": "keyword"},
                    "discovered_entities": {"type": "keyword"},
                    "discovered_relationships": {"type": "keyword"},
                    
                    # LLM-generated enrichments
                    "dynamic_synonyms": {"type": "keyword"},
                    "concept_variants": {"type": "keyword"},
                    "regulatory_terms": {"type": "keyword"},
                    "cross_references": {"type": "keyword"},
                    
                    # Discovery metadata
                    "discovery_confidence": {"type": "float"},
                    "agent_source": {"type": "keyword"},
                    "discovery_method": {"type": "keyword"},
                    
                    # Flexible dynamic fields for any discovered attributes
                    "dynamic_attributes": {
                        "type": "object",
                        "dynamic": True
                    },
                    
                    # Agent analysis results
                    "agent_insights": {
                        "type": "nested",
                        "properties": {
                            "agent_name": {"type": "keyword"},
                            "insight_type": {"type": "keyword"},
                            "insight": {"type": "text"},
                            "confidence": {"type": "float"},
                            "timestamp": {"type": "date"}
                        }
                    },
                    
                    "timestamp": {"type": "date"}
                }
            }
        }
        
        try:
            if not self.client.indices.exists(index=self.index_name):
                self.client.indices.create(index=self.index_name, **mapping)
                logger.info(f"Created dynamic index: {self.index_name}")
        except Exception as e:
            logger.error(f"Failed to create index: {e}")
            raise
    
    def index_discovered_concept(self, concept_data: Dict[str, Any], agent_insights: List[Dict[str, Any]] = None):
        """Index dynamically discovered concept"""
        try:
            # Generate embedding
            text_content = f"{concept_data.get('title', '')} {concept_data.get('text', '')} {concept_data.get('description', '')}"
            embedding = self.embedding_engine.generate_embedding(text_content)
            
            # Discover synonyms dynamically
            concept_name = concept_data.get('name', concept_data.get('title', ''))
            if concept_name:
                synonym_data = self.embedding_engine.discover_synonyms_and_variants(
                    concept_name, 
                    concept_data.get('text', '')
                )
            else:
                synonym_data = {}
            
            # Prepare document for indexing
            doc = {
                "text": concept_data.get('text', ''),
                "title": concept_data.get('title', concept_data.get('name', '')),
                "embedding": embedding,
                "chunk_id": concept_data.get('chunk_id', str(uuid.uuid4())),
                "source_document": concept_data.get('source_document', ''),
                "chunk_index": concept_data.get('chunk_index', 0),
                
                # Dynamically discovered classifications
                "discovered_concept_type": concept_data.get('concept_type', 'unknown'),
                "discovered_categories": concept_data.get('categories', []),
                "discovered_entities": concept_data.get('entities', []),
                "discovered_relationships": concept_data.get('relationships', []),
                
                # LLM-generated enrichments
                "dynamic_synonyms": synonym_data.get('exact_synonyms', []),
                "concept_variants": synonym_data.get('conceptual_variants', []),
                "regulatory_terms": synonym_data.get('regulatory_terms', []),
                "cross_references": concept_data.get('cross_references', []),
                
                # Discovery metadata
                "discovery_confidence": concept_data.get('confidence', 0.5),
                "agent_source": concept_data.get('agent_source', 'unknown'),
                "discovery_method": concept_data.get('discovery_method', 'llm_analysis'),
                
                # Store any additional discovered attributes dynamically
                "dynamic_attributes": {k: v for k, v in concept_data.items() 
                                     if k not in ['text', 'title', 'chunk_id', 'source_document']},
                
                "timestamp": datetime.now()
            }
            
            # Add agent insights
            if agent_insights:
                doc["agent_insights"] = agent_insights
            
            # Index document
            self.client.index(index=self.index_name, id=doc["chunk_id"], document=doc)
            
            return doc["chunk_id"]
            
        except Exception as e:
            logger.error(f"Failed to index discovered concept: {e}")
            raise
    
    def search_dynamic_knowledge(self, query: str, filters: Dict[str, Any] = None, top_k: int = 20) -> List[Dict[str, Any]]:
        """Search dynamically discovered knowledge"""
        try:
            # Generate query embedding
            query_embedding = self.embedding_engine.generate_embedding(query)
            
            # Build search query
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "script_score": {
                                    "query": {"match_all": {}},
                                    "script": {
                                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                                        "params": {"query_vector": query_embedding}
                                    },
                                    "boost": 2.0
                                }
            },
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": [
                                        "title^3",
                                        "text^1.5",
                                        "dynamic_synonyms^2.5",
                                        "concept_variants^2",
                                        "regulatory_terms^2.5",
                                        "discovered_entities^1.5"
                                    ],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            }
                        ]
                    }
                },
                "size": top_k,
                "_source": {"excludes": ["embedding"]},
                "highlight": {
                    "fields": {
                        "text": {"fragment_size": 150, "number_of_fragments": 3},
                        "title": {}
                    }
                }
            }
            
            # Add filters if provided
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if isinstance(value, list):
                        filter_clauses.append({"terms": {key: value}})
                    else:
                        filter_clauses.append({"term": {key: value}})
                search_body["query"]["bool"]["filter"] = filter_clauses
            
            response = self.client.search(index=self.index_name, **search_body)
            
            # Format results
            results = []
            hits = response.body["hits"]["hits"] if hasattr(response, 'body') else response["hits"]["hits"]
            
            for hit in hits:
                source = hit["_source"]
                result = {
                    "chunk_id": source["chunk_id"],
                    "title": source.get("title", ""),
                    "text": source["text"],
                    "source_document": source.get("source_document", ""),
                    "score": hit["_score"],
                    "discovered_concept_type": source.get("discovered_concept_type", ""),
                    "discovered_categories": source.get("discovered_categories", []),
                    "dynamic_synonyms": source.get("dynamic_synonyms", []),
                    "concept_variants": source.get("concept_variants", []),
                    "discovery_confidence": source.get("discovery_confidence", 0.0),
                    "agent_source": source.get("agent_source", ""),
                    "dynamic_attributes": source.get("dynamic_attributes", {}),
                    "highlight": hit.get("highlight", {})
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Dynamic knowledge search failed: {e}")
            return []

# =============================================================================
# DYNAMIC KNOWLEDGE GRAPH
# =============================================================================

class DynamicKnowledgeGraph:
    """FalkorDB-based dynamic knowledge graph without hardcoded ontology"""
    
    def __init__(self):
        try:
            connection_kwargs = {
                "host": GLOBAL_CONFIG["FALKORDB_HOST"],
                "port": GLOBAL_CONFIG["FALKORDB_PORT"]
            }
            
            if GLOBAL_CONFIG["FALKORDB_USERNAME"]:
                connection_kwargs["username"] = GLOBAL_CONFIG["FALKORDB_USERNAME"]
            if GLOBAL_CONFIG["FALKORDB_PASSWORD"]:
                connection_kwargs["password"] = GLOBAL_CONFIG["FALKORDB_PASSWORD"]
            
            self.db = FalkorDB(**connection_kwargs)
            self.graph = self.db.select_graph(GLOBAL_CONFIG["GRAPH_NAME"])
            
            # Initialize minimal flexible schema
            self._initialize_flexible_schema()
            
            logger.info("Dynamic knowledge graph initialized - no hardcoded concepts")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def _initialize_flexible_schema(self):
        """Initialize minimal flexible schema that can accommodate any discovered concepts"""
        try:
            logger.info("Initializing flexible knowledge graph schema...")
            
            # Create minimal indexes for performance
            minimal_indexes = [
                # Generic concept indexing
                "CREATE INDEX FOR (n:Concept) ON (n.name)",
                "CREATE INDEX FOR (n:Concept) ON (n.type)",
                "CREATE INDEX FOR (n:Entity) ON (n.name)",
                "CREATE INDEX FOR (n:Entity) ON (n.category)",
                
                # Document source tracking
                "CREATE INDEX FOR (n:DocumentSource) ON (n.filename)",
                "CREATE INDEX FOR (n:DocumentChunk) ON (n.chunk_id)",
                
                # Discovery metadata
                "CREATE INDEX FOR (n) ON (n.discovered_by)",
                "CREATE INDEX FOR (n) ON (n.discovery_timestamp)",
                "CREATE INDEX FOR (n) ON (n.confidence_score)"
            ]
            
            for index_query in minimal_indexes:
                try:
                    self.graph.query(index_query)
                except Exception as e:
                    logger.debug(f"Index creation note: {e}")
            
            logger.info("Flexible schema initialized - ready for dynamic discovery")
            
        except Exception as e:
            logger.warning(f"Schema initialization warning: {e}")
    
    def add_discovered_concept(self, concept_data: Dict[str, Any], relationships: List[Dict[str, Any]] = None):
        """Add dynamically discovered concept to knowledge graph"""
        try:
            concept_name = str(concept_data.get("name", "")).replace("'", "\\'")
            concept_type = concept_data.get("type", "Concept")
            
            if not concept_name:
                return
            
            # Build properties dynamically from discovered data
            properties = {
                "name": concept_name,
                "discovery_timestamp": "datetime()",
                "discovered_by": concept_data.get("discovered_by", "llm_agent"),
                "confidence_score": float(concept_data.get("confidence", 0.5)),
                "source_document": concept_data.get("source_document", ""),
                "chunk_id": concept_data.get("chunk_id", "")
            }
            
            # Add all discovered properties dynamically
            for key, value in concept_data.items():
                if key not in ["name", "type"] and value is not None:
                    if isinstance(value, (list, dict)):
                        properties[key] = json.dumps(value).replace("'", "\\'")
                    else:
                        properties[key] = str(value).replace("'", "\\'")
            
            # Build property string for Cypher query
            props_list = []
            for k, v in properties.items():
                if k == "discovery_timestamp":
                    props_list.append(f"{k}: {v}")
                else:
                    props_list.append(f"{k}: '{v}'")
            
            props_str = ", ".join(props_list)
            
            # Create node with discovered type
            query = f"""
            MERGE (n:{concept_type} {{ {props_str} }})
            RETURN n
            """
            self.graph.query(query)
            
            # Add relationships if discovered
            if relationships:
                for rel in relationships:
                    self._create_dynamic_relationship(concept_name, concept_type, rel)
            
            logger.debug(f"Added discovered concept: {concept_type}:{concept_name}")
            
        except Exception as e:
            logger.error(f"Failed to add discovered concept {concept_data.get('name', 'unknown')}: {e}")
    
    def _create_dynamic_relationship(self, source_name: str, source_type: str, relationship: Dict[str, Any]):
        """Create dynamically discovered relationship"""
        try:
            target_name = relationship.get("target_name", "").replace("'", "\\'")
            target_type = relationship.get("target_type", "Concept")
            rel_type = relationship.get("relationship_type", "RELATED_TO").upper().replace(" ", "_")
            rel_properties = relationship.get("properties", {})
            
            if not target_name or not rel_type:
                return
            
            # Build relationship properties
            rel_props = []
            for k, v in rel_properties.items():
                rel_props.append(f"{k}: '{str(v).replace(chr(39), chr(92)+chr(39))}'")
            
            rel_props_str = "{" + ", ".join(rel_props) + "}" if rel_props else ""
            
            query = f"""
            MATCH (s:{source_type} {{name: '{source_name}'}})
            MATCH (t:{target_type} {{name: '{target_name}'}})
            MERGE (s)-[:{rel_type} {rel_props_str}]->(t)
            """
            self.graph.query(query)
            
        except Exception as e:
            logger.debug(f"Could not create dynamic relationship: {e}")
    
    def discover_concept_clusters(self, similarity_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """Discover clusters of related concepts using graph analysis"""
        try:
            query = f"""
            MATCH (n:Concept)-[r]-(m:Concept)
            WHERE n.confidence_score >= {similarity_threshold} AND m.confidence_score >= {similarity_threshold}
            RETURN n, r, m, 
                   size((n)-[]-(m)) as connection_strength
            ORDER BY connection_strength DESC
            LIMIT 100
            """
            
            result = self.graph.query(query)
            
            clusters = []
            for record in result.result_set:
                source_node = self._format_node(record[0])
                relationship = str(record[1]) if record[1] else ""
                target_node = self._format_node(record[2])
                connection_strength = record[3] if len(record) > 3 else 0
                
                clusters.append({
                    "source_concept": source_node,
                    "target_concept": target_node,
                    "relationship": relationship,
                    "connection_strength": connection_strength,
                    "cluster_type": "discovered"
                })
            
            return clusters
            
        except Exception as e:
            logger.error(f"Concept cluster discovery failed: {e}")
            return []
    
    def search_dynamic_graph(self, query_concept: str, max_depth: int = 3, limit: int = 50) -> List[Dict[str, Any]]:
        """Search dynamically built knowledge graph"""
        try:
            concept_escaped = query_concept.replace("'", "\\'")
            
            search_query = f"""
            MATCH path = (start)-[*1..{max_depth}]-(related)
            WHERE (
                toLower(start.name) CONTAINS toLower('{concept_escaped}') OR 
                toLower(start.description) CONTAINS toLower('{concept_escaped}') OR
                ANY(key IN keys(start) WHERE toLower(toString(start[key])) CONTAINS toLower('{concept_escaped}'))
            )
            RETURN DISTINCT 
                start,
                related, 
                relationships(path) as path_relationships,
                length(path) as distance,
                labels(start) as start_labels,
                labels(related) as related_labels,
                start.confidence_score as start_confidence,
                related.confidence_score as related_confidence
            ORDER BY start_confidence DESC, distance ASC
            LIMIT {limit}
            """
            
            result = self.graph.query(query)
            
            formatted_results = []
            for record in result.result_set:
                start_node = self._format_node(record[0])
                related_node = self._format_node(record[1])
                relationships = [str(rel) for rel in record[2]] if record[2] else []
                distance = record[3] if len(record) > 3 else 0
                start_labels = record[4] if len(record) > 4 else []
                related_labels = record[5] if len(record) > 5 else []
                start_confidence = record[6] if len(record) > 6 else 0.0
                related_confidence = record[7] if len(record) > 7 else 0.0
                
                formatted_results.append({
                    "start_node": start_node,
                    "related_node": related_node,
                    "relationships": relationships,
                    "distance": distance,
                    "start_labels": start_labels,
                    "related_labels": related_labels,
                    "start_confidence": start_confidence,
                    "related_confidence": related_confidence,
                    "relevance_score": (start_confidence + related_confidence) / (distance + 1)
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Dynamic graph search failed: {e}")
            return []
    
    def _format_node(self, node) -> Dict[str, Any]:
        """Format graph node for output"""
        if not node:
            return {}
            
        try:
            if hasattr(node, 'properties'):
                return dict(node.properties)
            else:
                return {"id": str(node)}
        except Exception as e:
            logger.debug(f"Node formatting error: {e}")
            return {"id": str(node)}

# =============================================================================
# MULTI-AGENT DISCOVERY ARCHITECTURE
# =============================================================================

class BaseDynamicAgent:
    """Base agent for dynamic discovery using o3-mini reasoning"""
    
    def __init__(self, agent_name: str, system_prompt: str):
        self.agent_name = agent_name
        self.system_prompt = system_prompt
        
        # Initialize o3-mini client
        client_kwargs = {"api_key": GLOBAL_CONFIG["OPENAI_API_KEY"]}
        if GLOBAL_CONFIG["OPENAI_BASE_URL"]:
            client_kwargs["base_url"] = GLOBAL_CONFIG["OPENAI_BASE_URL"]
        
        self.client = OpenAI(**client_kwargs)
        
    def analyze_and_discover(self, content: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analyze content and discover concepts dynamically"""
        try:
            # Build context if provided
            context_str = ""
            if context:
                if "vector_results" in context:
                    context_str += "\n\nPrevious Discoveries (Vector Search):\n"
                    for i, result in enumerate(context["vector_results"][:3]):
                        context_str += f"[{i+1}] {result.get('title', '')}: {result.get('discovered_concept_type', 'unknown')}\n"
                
                if "graph_results" in context:
                    context_str += "\n\nRelated Concepts (Graph):\n"
                    for i, result in enumerate(context["graph_results"][:3]):
                        context_str += f"[{i+1}] {result.get('start_node', {}).get('name', '')}\n"
            
            full_prompt = f"{self.system_prompt}\n\nContent to Analyze:\n{content}\n{context_str}"
            
            # Use o3-mini with high reasoning effort
            response = self.client.chat.completions.create(
                model="o3-mini",
                messages=[{"role": "user", "content": full_prompt}],
                reasoning_effort="high"
            )
            
            content_response = response.choices[0].message.content
            
            # Try to extract JSON from response
            json_start = content_response.find('{')
            json_end = content_response.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                try:
                    json_str = content_response[json_start:json_end]
                    result = json.loads(json_str)
                    return {
                        "agent": self.agent_name,
                        "reasoning": content_response[:json_start].strip() if json_start > 0 else "",
                        "discoveries": result,
                        "raw_response": content_response,
                        "timestamp": datetime.now().isoformat()
                    }
                except json.JSONDecodeError:
                    pass
            
            # If no JSON, return as text analysis
            return {
                "agent": self.agent_name,
                "reasoning": content_response,
                "discoveries": {"raw_analysis": content_response},
                "raw_response": content_response,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Agent {self.agent_name} analysis failed: {e}")
            return {
                "agent": self.agent_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

class ConceptDiscoveryAgent(BaseDynamicAgent):
    """Agent for discovering concepts from documents dynamically"""
    
    def __init__(self):
        system_prompt = """You are a Concept Discovery Agent specializing in dynamically identifying and extracting regulatory and legal concepts from documents.

Your mission is to discover ALL concepts present in the document content without making ANY assumptions about what should or should not be there. You discover what IS actually present, not what you expect to find.

DISCOVERY APPROACH:
1. Read the document content carefully
2. Identify any regulatory/legal concepts mentioned
3. Extract entities, relationships, and hierarchical structures
4. Discover synonyms and variant terms used
5. Identify cross-references and citations
6. Map concept relationships and dependencies

TYPES OF CONCEPTS TO DISCOVER:
- Articles, sections, chapters (if present)
- Legal principles and rules
- Rights and obligations
- Entities and roles
- Processes and procedures
- Data types and categories
- Requirements and standards
- Penalties and enforcement measures
- Definitions and terminology
- Hierarchical relationships
- Cross-references

For each discovered concept, provide:
- Name/title of the concept
- Type/category of concept
- Description from the document
- Relationships to other concepts
- Synonyms or variant terms found
- Confidence level (0.0-1.0)
- Context and supporting evidence

Return comprehensive JSON with all discoveries. DO NOT invent or assume concepts not explicitly present in the document."""
        
        super().__init__("ConceptDiscoveryAgent", system_prompt)

class RelationshipDiscoveryAgent(BaseDynamicAgent):
    """Agent for discovering relationships between concepts"""
    
    def __init__(self):
        system_prompt = """You are a Relationship Discovery Agent specializing in identifying connections and relationships between regulatory concepts found in documents.

Your role is to discover HOW concepts relate to each other based on the actual document content, not assumptions.

RELATIONSHIP DISCOVERY FOCUS:
1. Explicit relationships stated in the document
2. Hierarchical relationships (parent-child, part-whole)
3. Dependency relationships (requires, depends on, triggers)
4. Reference relationships (cites, refers to, implements)
5. Logical relationships (contradicts, complements, elaborates)
6. Procedural relationships (follows, precedes, enables)
7. Conditional relationships (if-then, unless, except)

For each relationship discovered, provide:
- Source concept name and type
- Target concept name and type
- Relationship type/nature
- Relationship direction (bidirectional, one-way)
- Supporting evidence from document
- Confidence level
- Context and conditions

Return JSON with discovered relationships. Only include relationships that are explicitly or clearly implied in the document content."""
        
        super().__init__("RelationshipDiscoveryAgent", system_prompt)

class StructureDiscoveryAgent(BaseDynamicAgent):
    """Agent for discovering document and knowledge structures"""
    
    def __init__(self):
        system_prompt = """You are a Structure Discovery Agent specializing in identifying organizational and hierarchical structures within regulatory documents.

Your mission is to discover the actual structure and organization present in the document content.

STRUCTURE DISCOVERY FOCUS:
1. Document organization (chapters, sections, articles, paragraphs)
2. Hierarchical structures (parent-child relationships)
3. Classification systems (categories, types, classes)
4. Procedural structures (steps, phases, stages)
5. Organizational structures (roles, responsibilities, authorities)
6. Logical structures (conditions, exceptions, alternatives)
7. Cross-reference structures (internal and external links)

For each structural element, provide:
- Element name/identifier
- Structure type (hierarchy, classification, procedure, etc.)
- Position in structure (level, order, grouping)
- Parent/child relationships
- Scope and coverage
- Special attributes or properties
- Cross-references and connections

Return JSON with discovered structures. Focus on what is actually present in the document, not standard document structures you might expect."""
        
        super().__init__("StructureDiscoveryAgent", system_prompt)

class ReflectionAgent(BaseDynamicAgent):
    """Agent for reflecting on and validating discoveries"""
    
    def __init__(self):
        system_prompt = """You are a Reflection Agent responsible for validating, improving, and ensuring quality of concept discoveries made by other agents.

Your role is to review discoveries and ensure they are:
1. Accurate and grounded in document content
2. Complete and comprehensive
3. Consistent and non-contradictory
4. Well-structured and organized
5. Free from hallucinations or assumptions

REFLECTION CRITERIA:
- Evidence: Is each discovery supported by document content?
- Completeness: Are there missing concepts or relationships?
- Accuracy: Are the discoveries correctly interpreted?
- Consistency: Do discoveries align with each other?
- Quality: Are descriptions clear and useful?
- Coverage: Are all document areas adequately analyzed?

Provide reflection results with:
- Validation of correct discoveries
- Identification of errors or inconsistencies
- Suggestions for additional analysis
- Quality improvement recommendations
- Confidence assessments
- Gap identification

Return JSON with reflection insights and recommendations."""
        
        super().__init__("ReflectionAgent", system_prompt)

class SupervisorAgent(BaseDynamicAgent):
    """Supervisor agent coordinating discovery workflow"""
    
    def __init__(self):
        system_prompt = """You are the Supervisor Agent coordinating a multi-agent regulatory knowledge discovery system.

Your responsibilities:
1. Coordinate agent workflows and task distribution
2. Ensure comprehensive coverage of document content
3. Resolve conflicts between agent discoveries
4. Maintain quality standards and consistency
5. Guide overall discovery strategy
6. Make decisions about analysis priorities
7. Ensure industry-specific requirements are addressed

SUPERVISION CRITERIA:
- Comprehensive coverage of all document content
- Quality and accuracy of discoveries
- Consistency across agent findings
- Completeness of relationship mapping
- Industry-specific relevance
- Discovery confidence levels
- Analysis efficiency

Make strategic decisions about:
- Which areas need deeper analysis
- How to resolve conflicting discoveries
- What additional analysis is needed
- How to prioritize remaining work
- Quality thresholds and standards

Return JSON with supervision decisions and workflow guidance."""
        
        super().__init__("SupervisorAgent", system_prompt)

# =============================================================================
# DYNAMIC DOCUMENT PROCESSOR
# =============================================================================

class DynamicDocumentProcessor:
    """Advanced document processing for dynamic knowledge discovery"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=4000,
            chunk_overlap=800,
            separators=[
                "\n\n\n", "\n\n", "\n",
                ". ", "! ", "? ", "; ", ": ",
                " "
            ],
            keep_separator=True
        )
    
    def process_pdf_document(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Process PDF document for dynamic knowledge discovery"""
        logger.info(f"Processing PDF document for dynamic discovery: {pdf_path}")
        
        try:
            doc = pymupdf.open(pdf_path)
            full_text = ""
            metadata = {
                "filename": os.path.basename(pdf_path),
                "pages": len(doc), 
                "source_path": pdf_path,
                "processed_timestamp": datetime.now().isoformat()
            }
            
            # Extract text preserving structure
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                full_text += f"\n\n--- Page {page_num + 1} ---\n{page_text}"
            
            doc.close()
            
            # Intelligent chunking
            text_chunks = self._intelligent_chunking(full_text)
            
            chunks = []
            for i, chunk_text in enumerate(text_chunks):
                chunk = {
                    "chunk_id": f"{metadata['filename']}_chunk_{i}",
                    "text": chunk_text,
                    "chunk_index": i,
                    "source_document": pdf_path,
                    "filename": metadata["filename"],
                    "total_chunks": len(text_chunks),
                    "word_count": len(chunk_text.split()),
                    "char_count": len(chunk_text),
                    "metadata": metadata,
                    "processed_timestamp": datetime.now().isoformat()
                }
                chunks.append(chunk)
            
            logger.info(f"Created {len(chunks)} chunks from {pdf_path}")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to process {pdf_path}: {e}")
            return []
    
    def _intelligent_chunking(self, text: str) -> List[str]:
        """Intelligent chunking that preserves regulatory structure"""
        
        # Look for natural regulatory boundaries
        regulatory_boundaries = [
            r'\n\s*Article\s+\d+',
            r'\n\s*Section\s+\d+',
            r'\n\s*Chapter\s+[IVX\d]+',
            r'\n\s*Part\s+[IVX\d]+',
            r'\n\s*\(\d+\)',  # Numbered paragraphs
            r'\n\s*[a-z]\)',  # Lettered subsections
        ]
        
        # Find regulatory boundaries
        boundaries = []
        for pattern in regulatory_boundaries:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                boundaries.append(match.start())
        
        # Sort boundaries
        boundaries = sorted(set(boundaries))
        
        # If we found regulatory boundaries, use them
        if len(boundaries) > 1:
            chunks = []
            for i in range(len(boundaries)):
                start_pos = boundaries[i]
                end_pos = boundaries[i + 1] if i + 1 < len(boundaries) else len(text)
                
                chunk_text = text[start_pos:end_pos].strip()
                if len(chunk_text) > 200:  # Minimum meaningful chunk size
                    # Further split if too large
                    if len(chunk_text) > 8000:
                        sub_chunks = self.text_splitter.split_text(chunk_text)
                        chunks.extend(sub_chunks)
                    else:
                        chunks.append(chunk_text)
            
            return chunks
        
        # Fallback to standard chunking
        return self.text_splitter.split_text(text)

# =============================================================================
# MAIN DYNAMIC DISCOVERY SYSTEM
# =============================================================================

class DynamicRegulatoryDiscoverySystem:
    """Main system orchestrating dynamic regulatory knowledge discovery"""
    
    def __init__(self):
        # Initialize components
        self.document_processor = DynamicDocumentProcessor()
        self.vector_database = DynamicVectorDatabase()
        self.knowledge_graph = DynamicKnowledgeGraph()
        
        # Initialize discovery agents
        self.concept_agent = ConceptDiscoveryAgent()
        self.relationship_agent = RelationshipDiscoveryAgent()
        self.structure_agent = StructureDiscoveryAgent()
        self.reflection_agent = ReflectionAgent()
        self.supervisor_agent = SupervisorAgent()
        
        # Initialize state
        self.state = DynamicDiscoveryState(
            messages=[],
            processed_documents=[],
            current_document_chunk=None,
            discovered_concepts={},
            discovered_relationships=[],
            discovered_hierarchies=[],
            discovered_synonyms={},
            agent_discoveries={},
            reflection_results=[],
            supervisor_decisions=[],
            concept_clusters=[],
            semantic_mappings=[],
            dynamic_ontology=None,
            ropa_metamodel=None,
            compliance_framework=None,
            industry_analysis=None
        )
        
        logger.info("Dynamic Regulatory Discovery System initialized - no hardcoded knowledge")
    
    def discover_knowledge_from_documents(self, document_paths: List[str] = None) -> Dict[str, Any]:
        """Discover knowledge dynamically from documents"""
        
        if document_paths is None:
            pdf_path = GLOBAL_CONFIG["PDF_DOCUMENTS_PATH"]
            if os.path.exists(pdf_path):
                document_paths = [
                    os.path.join(pdf_path, f) for f in os.listdir(pdf_path)
                    if f.lower().endswith('.pdf')
                ]
            else:
                logger.warning(f"PDF documents path does not exist: {pdf_path}")
                document_paths = []
        
        if not document_paths:
            logger.warning("No PDF documents found for discovery")
            return {"status": "warning", "message": "No documents to process"}
        
        logger.info(f"Starting dynamic knowledge discovery from {len(document_paths)} documents")
        
        discovery_stats = {
            "documents_processed": 0,
            "chunks_analyzed": 0,
            "concepts_discovered": 0,
            "relationships_discovered": 0,
            "graph_nodes_created": 0,
            "vector_embeddings_created": 0,
            "agent_analyses": 0,
            "reflection_cycles": 0
        }
        
        for doc_path in document_paths:
            try:
                # Process document
                chunks = self.document_processor.process_pdf_document(doc_path)
                if not chunks:
                    continue
                
                discovery_stats["chunks_analyzed"] += len(chunks)
                
                # Analyze each chunk through discovery agents
                for chunk in chunks:
                    self.state["current_document_chunk"] = chunk
                    
                    # Build context from previous discoveries
                    context = self._build_discovery_context(chunk["text"])
                    
                    # Concept Discovery Agent
                    concept_discoveries = self.concept_agent.analyze_and_discover(chunk["text"], context)
                    self._process_agent_discoveries("concept_discovery", concept_discoveries)
                    discovery_stats["agent_analyses"] += 1
                    
                    # Relationship Discovery Agent
                    relationship_discoveries = self.relationship_agent.analyze_and_discover(chunk["text"], context)
                    self._process_agent_discoveries("relationship_discovery", relationship_discoveries)
                    discovery_stats["agent_analyses"] += 1
                    
                    # Structure Discovery Agent
                    structure_discoveries = self.structure_agent.analyze_and_discover(chunk["text"], context)
                    self._process_agent_discoveries("structure_discovery", structure_discoveries)
                    discovery_stats["agent_analyses"] += 1
                    
                    # Extract and store discoveries
                    extracted_discoveries = self._extract_discoveries_from_agents([
                        concept_discoveries, relationship_discoveries, structure_discoveries
                    ])
                    
                    # Store discoveries in knowledge graph and vector database
                    for discovery in extracted_discoveries:
                        # Add to knowledge graph
                        self.knowledge_graph.add_discovered_concept(discovery)
                        discovery_stats["graph_nodes_created"] += 1
                        
                        # Add to vector database
                        self.vector_database.index_discovered_concept(discovery)
                        discovery_stats["vector_embeddings_created"] += 1
                        
                        # Update state
                        concept_type = discovery.get("type", "Unknown")
                        if concept_type not in self.state["discovered_concepts"]:
                            self.state["discovered_concepts"][concept_type] = []
                        self.state["discovered_concepts"][concept_type].append(discovery)
                        discovery_stats["concepts_discovered"] += 1
                    
                    # Periodic reflection cycle
                    if discovery_stats["chunks_analyzed"] % 10 == 0:
                        self._perform_reflection_cycle()
                        discovery_stats["reflection_cycles"] += 1
                
                self.state["processed_documents"].append({
                    "path": doc_path,
                    "chunks": len(chunks),
                    "processed_timestamp": datetime.now().isoformat()
                })
                
                discovery_stats["documents_processed"] += 1
                logger.info(f"Completed discovery from: {doc_path}")
                
            except Exception as e:
                logger.error(f"Failed to discover knowledge from {doc_path}: {e}")
                continue
        
        # Final comprehensive analysis
        final_analysis = self._perform_final_analysis()
        
        return {
            "status": "success",
            "discovery_stats": discovery_stats,
            "final_analysis": final_analysis,
            "discovered_concepts": self.state["discovered_concepts"],
            "dynamic_ontology": self.state.get("dynamic_ontology"),
            "ropa_metamodel": self.state.get("ropa_metamodel"),
            "compliance_framework": self.state.get("compliance_framework"),
            "timestamp": datetime.now().isoformat()
        }
    
    def _build_discovery_context(self, text: str) -> Dict[str, Any]:
        """Build context for discovery agents from previous discoveries"""
        # Search for related discoveries
        vector_results = self.vector_database.search_dynamic_knowledge(text[:1000], top_k=5)
        graph_results = self.knowledge_graph.search_dynamic_graph(text[:500], max_depth=2)
        
        return {
            "vector_results": vector_results,
            "graph_results": graph_results
        }
    
    def _process_agent_discoveries(self, agent_name: str, discoveries: Dict[str, Any]):
        """Process and store agent discovery results"""
        if agent_name not in self.state["agent_discoveries"]:
            self.state["agent_discoveries"][agent_name] = []
        
        self.state["agent_discoveries"][agent_name].append(discoveries)
    
    def _extract_discoveries_from_agents(self, agent_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract concrete discoveries from agent analysis results"""
        discoveries = []
        
        for agent_result in agent_results:
            if "discoveries" in agent_result and isinstance(agent_result["discoveries"], dict):
                agent_discoveries = agent_result["discoveries"]
                agent_name = agent_result.get("agent", "unknown")
                
                # Extract different types of discoveries
                if "concepts" in agent_discoveries:
                    for concept in agent_discoveries["concepts"]:
                        if isinstance(concept, dict):
                            concept["discovered_by"] = agent_name
                            concept["discovery_timestamp"] = agent_result.get("timestamp")
                            discoveries.append(concept)
                
                if "entities" in agent_discoveries:
                    for entity in agent_discoveries["entities"]:
                        if isinstance(entity, dict):
                            entity["discovered_by"] = agent_name
                            entity["type"] = "Entity"
                            discoveries.append(entity)
                
                if "structures" in agent_discoveries:
                    for structure in agent_discoveries["structures"]:
                        if isinstance(structure, dict):
                            structure["discovered_by"] = agent_name
                            structure["type"] = "Structure"
                            discoveries.append(structure)
                
                # Handle relationships separately
                if "relationships" in agent_discoveries:
                    self.state["discovered_relationships"].extend(agent_discoveries["relationships"])
        
        return discoveries
    
    def _perform_reflection_cycle(self):
        """Perform reflection cycle on recent discoveries"""
        recent_discoveries = []
        for agent_discoveries in self.state["agent_discoveries"].values():
            if agent_discoveries:
                recent_discoveries.extend(agent_discoveries[-5:])  # Last 5 per agent
        
        if recent_discoveries:
            reflection_result = self.reflection_agent.analyze_and_discover(
                f"Recent discoveries to reflect on: {json.dumps(recent_discoveries[:3], indent=2)}"
            )
            self.state["reflection_results"].append(reflection_result)
            
            # Supervisor decision
            supervisor_decision = self.supervisor_agent.analyze_and_discover(
                f"Reflection result: {reflection_result}"
            )
            self.state["supervisor_decisions"].append(supervisor_decision)
    
    def _perform_final_analysis(self) -> Dict[str, Any]:
        """Perform final analysis and generate metamodels"""
        logger.info("Performing final analysis and metamodel generation...")
        
        try:
            # Collect all discoveries
            all_discoveries = {
                "concepts": self.state["discovered_concepts"],
                "relationships": self.state["discovered_relationships"],
                "hierarchies": self.state["discovered_hierarchies"],
                "agent_discoveries": self.state["agent_discoveries"],
                "reflection_results": self.state["reflection_results"]
            }
            
            # Generate dynamic ontology
            ontology_prompt = f"""
            Based on all discovered knowledge, create a dynamic ontology structure:
            
            Discoveries: {json.dumps(all_discoveries, indent=2)[:8000]}
            
            Generate a comprehensive ontology that captures:
            1. All discovered concept types and their properties
            2. Hierarchical relationships and taxonomies
            3. Cross-references and dependencies
            4. Industry-specific elements discovered
            5. Regulatory framework structure found in documents
            
            Return JSON structure representing the complete discovered ontology.
            """
            
            ontology_result = self.supervisor_agent.analyze_and_discover(ontology_prompt)
            self.state["dynamic_ontology"] = ontology_result
            
            # Generate RoPA metamodel from discovered concepts
            ropa_prompt = f"""
            Based on discovered regulatory knowledge, generate a RoPA-focused metamodel:
            
            Discovered Concepts: {json.dumps(self.state["discovered_concepts"], indent=2)[:6000]}
            
            Create RoPA metamodel focusing on Article 30 compliance elements discovered in the documents:
            1. Controller and processor concepts found
            2. Processing activities identified
            3. Data categories discovered
            4. Legal bases found
            5. Security measures identified
            6. International transfer mechanisms
            7. Industry-specific RoPA requirements
            
            Return comprehensive RoPA metamodel based on actual discoveries.
            """
            
            ropa_result = self.supervisor_agent.analyze_and_discover(ropa_prompt)
            self.state["ropa_metamodel"] = ropa_result
            
            # Generate compliance framework
            compliance_prompt = f"""
            Generate compliance framework based on discovered regulatory requirements:
            
            All Discoveries: {json.dumps(all_discoveries, indent=2)[:4000]}
            
            Create compliance framework including:
            1. Discovered compliance obligations
            2. Risk assessment based on findings
            3. Implementation roadmap
            4. Industry-specific considerations found
            5. Gaps and recommendations
            
            Return comprehensive compliance framework.
            """
            
            compliance_result = self.supervisor_agent.analyze_and_discover(compliance_prompt)
            self.state["compliance_framework"] = compliance_result
            
            return {
                "dynamic_ontology": ontology_result,
                "ropa_metamodel": ropa_result,
                "compliance_framework": compliance_result,
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Final analysis failed: {e}")
            return {"status": "error", "error": str(e)}
    
    def search_discovered_knowledge(self, query: str, search_type: str = "both") -> Dict[str, Any]:
        """Search dynamically discovered knowledge"""
        
        results = {
            "query": query,
            "vector_results": [],
            "graph_results": [],
            "combined_insights": []
        }
        
        try:
            if search_type in ["vector", "both"]:
                results["vector_results"] = self.vector_database.search_dynamic_knowledge(query, top_k=15)
            
            if search_type in ["graph", "both"]:
                results["graph_results"] = self.knowledge_graph.search_dynamic_graph(query)
            
            # Generate insights about discoveries
            if results["vector_results"] or results["graph_results"]:
                insight_prompt = f"""
                Analyze discovered knowledge related to query: "{query}"
                
                Vector Discoveries: {len(results['vector_results'])} matches
                Graph Discoveries: {len(results['graph_results'])} relationships
                
                Generate insights about discovered concepts and their implications.
                """
                
                insights = self.supervisor_agent.analyze_and_discover(insight_prompt)
                results["combined_insights"] = insights.get("discoveries", {})
            
        except Exception as e:
            logger.error(f"Knowledge search failed: {e}")
            results["error"] = str(e)
        
        return results
    
    def export_discovery_report(self) -> str:
        """Export comprehensive discovery report"""
        
        report_content = f"""# Dynamic Regulatory Knowledge Discovery Report

## Executive Summary

This report presents findings from dynamic knowledge discovery across regulatory documents using multi-agent analysis. All concepts and relationships were discovered directly from document content without any hardcoded assumptions.

### Discovery Statistics

- **Documents Processed**: {len(self.state['processed_documents'])}
- **Concept Types Discovered**: {len(self.state['discovered_concepts'])}
- **Total Concepts**: {sum(len(concepts) for concepts in self.state['discovered_concepts'].values())}
- **Relationships Discovered**: {len(self.state['discovered_relationships'])}
- **Agent Analyses**: {sum(len(discoveries) for discoveries in self.state['agent_discoveries'].values())}
- **Reflection Cycles**: {len(self.state['reflection_results'])}

## Discovered Knowledge Base

### Concept Types Discovered
{self._format_discovered_concepts()}

### Relationship Types Discovered
{self._format_discovered_relationships()}

### Hierarchical Structures
{self._format_discovered_hierarchies()}

## Dynamic Ontology

{json.dumps(self.state.get('dynamic_ontology', {}), indent=2)}

## RoPA Metamodel (From Discoveries)

{json.dumps(self.state.get('ropa_metamodel', {}), indent=2)}

## Compliance Framework (Discovered Requirements)

{json.dumps(self.state.get('compliance_framework', {}), indent=2)}

## Agent Discovery Analysis

{self._format_agent_discoveries()}

## Reflection Insights

{self._format_reflection_insights()}

## Implementation Recommendations

Based on discovered regulatory knowledge:

1. **Implement Discovered RoPA Elements**: Focus on concepts actually found in documents
2. **Address Discovered Compliance Requirements**: Implement requirements identified in analysis
3. **Establish Discovered Relationships**: Map relationships found between concepts
4. **Monitor Discovered Risk Areas**: Focus on risk areas identified through discovery

---

*Report Generated*: {datetime.now().isoformat()}
*System*: Dynamic Regulatory Discovery System v5.0.0
*Approach*: No hardcoded concepts - all discoveries from document analysis
"""
        
        # Save report
        report_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "dynamic_discovery_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"Discovery report exported to {report_file}")
        return report_file
    
    def _format_discovered_concepts(self) -> str:
        """Format discovered concepts for report"""
        if not self.state["discovered_concepts"]:
            return "*No concepts discovered*"
        
        formatted = "\n"
        for concept_type, concepts in self.state["discovered_concepts"].items():
            formatted += f"#### {concept_type} ({len(concepts)} discovered)\n\n"
            for concept in concepts[:5]:  # Show first 5
                name = concept.get('name', 'Unnamed')
                description = concept.get('description', 'No description')[:100]
                confidence = concept.get('confidence', 0.0)
                formatted += f"- **{name}** (confidence: {confidence:.2f}): {description}...\n"
            
            if len(concepts) > 5:
                formatted += f"  *... and {len(concepts) - 5} more*\n"
            formatted += "\n"
        
        return formatted
    
    def _format_discovered_relationships(self) -> str:
        """Format discovered relationships for report"""
        if not self.state["discovered_relationships"]:
            return "*No relationships discovered*"
        
        formatted = f"\n**Total Relationships Discovered**: {len(self.state['discovered_relationships'])}\n\n"
        for i, rel in enumerate(self.state["discovered_relationships"][:10]):  # Show first 10
            source = rel.get('source_name', 'Unknown')
            target = rel.get('target_name', 'Unknown')
            rel_type = rel.get('relationship_type', 'RELATED')
            formatted += f"{i+1}. **{source}** --{rel_type}--> **{target}**\n"
        
        if len(self.state["discovered_relationships"]) > 10:
            formatted += f"\n*... and {len(self.state['discovered_relationships']) - 10} more relationships*\n"
        
        return formatted
    
    def _format_discovered_hierarchies(self) -> str:
        """Format discovered hierarchies for report"""
        if not self.state["discovered_hierarchies"]:
            return "*No hierarchical structures discovered*"
        
        formatted = f"\n**Hierarchies Discovered**: {len(self.state['discovered_hierarchies'])}\n\n"
        for hierarchy in self.state["discovered_hierarchies"][:5]:
            formatted += f"- {json.dumps(hierarchy, indent=2)}\n\n"
        
        return formatted
    
    def _format_agent_discoveries(self) -> str:
        """Format agent discoveries for report"""
        formatted = "\n"
        for agent_name, discoveries in self.state["agent_discoveries"].items():
            formatted += f"#### {agent_name.replace('_', ' ').title()}\n"
            formatted += f"- **Discovery Sessions**: {len(discoveries)}\n"
            if discoveries:
                last_discovery = discoveries[-1]
                if "discoveries" in last_discovery:
                    formatted += f"- **Latest Discoveries**: {len(last_discovery['discoveries'])} concepts\n"
            formatted += "\n"
        return formatted
    
    def _format_reflection_insights(self) -> str:
        """Format reflection insights for report"""
        if not self.state["reflection_results"]:
            return "*No reflection cycles performed*"
        
        formatted = f"\n**Reflection Cycles**: {len(self.state['reflection_results'])}\n\n"
        for i, reflection in enumerate(self.state["reflection_results"][-3:]):
            formatted += f"**Cycle {i+1}**: {str(reflection.get('discoveries', {}))[:300]}...\n\n"
        
        return formatted
    
    def save_discovery_artifacts(self) -> Dict[str, str]:
        """Save all discovery artifacts"""
        artifacts = {}
        
        try:
            # Save discovered concepts
            concepts_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "discovered_concepts.json")
            with open(concepts_file, 'w', encoding='utf-8') as f:
                json.dump(self.state["discovered_concepts"], f, indent=2, default=str)
            artifacts["discovered_concepts"] = concepts_file
            
            # Save dynamic ontology
            if self.state.get("dynamic_ontology"):
                ontology_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "dynamic_ontology.json")
                with open(ontology_file, 'w', encoding='utf-8') as f:
                    json.dump(self.state["dynamic_ontology"], f, indent=2, default=str)
                artifacts["dynamic_ontology"] = ontology_file
            
            # Save RoPA metamodel
            if self.state.get("ropa_metamodel"):
                ropa_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "discovered_ropa_metamodel.json")
                with open(ropa_file, 'w', encoding='utf-8') as f:
                    json.dump(self.state["ropa_metamodel"], f, indent=2, default=str)
                artifacts["ropa_metamodel"] = ropa_file
            
            # Save complete discovery state
            state_file = os.path.join(GLOBAL_CONFIG["OUTPUT_PATH"], "complete_discovery_state.json")
            with open(state_file, 'w', encoding='utf-8') as f:
                # Convert state to serializable format
                serializable_state = {}
                for key, value in self.state.items():
                    if key == "messages":
                        serializable_state[key] = [{"type": type(msg).__name__, "content": getattr(msg, 'content', str(msg))} for msg in value]
                    else:
                        serializable_state[key] = value
                
                json.dump(serializable_state, f, indent=2, default=str)
            artifacts["complete_state"] = state_file
            
            logger.info(f"Discovery artifacts saved: {list(artifacts.keys())}")
            
        except Exception as e:
            logger.error(f"Failed to save discovery artifacts: {e}")
            artifacts["error"] = str(e)
        
        return artifacts

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(
        description="Dynamic Regulatory Knowledge Discovery System - No Hardcoding"
    )
    parser.add_argument("--discover", nargs="*", help="Discover knowledge from documents (paths optional)")
    parser.add_argument("--search", type=str, help="Search discovered knowledge")
    parser.add_argument("--search-type", choices=["vector", "graph", "both"], default="both", 
                       help="Type of search to perform")
    parser.add_argument("--export-report", action="store_true", help="Export discovery report")
    parser.add_argument("--save-artifacts", action="store_true", help="Save all discovery artifacts")
    parser.add_argument("--industry", default="financial_services", help="Industry context")
    
    args = parser.parse_args()
    
    try:
        # Initialize dynamic discovery system
        system = DynamicRegulatoryDiscoverySystem()
        print(" Dynamic Regulatory Discovery System initialized")
        print(" System discovers ALL concepts from documents - no hardcoding")
        print(" Multi-agent architecture: Concept, Relationship, Structure, Reflection, Supervisor")
        print(f" Industry Context: {GLOBAL_CONFIG['INDUSTRY_TYPE']}")
        
        # If no arguments, run complete workflow
        if not any(vars(args).values()):
            print("\n Running complete dynamic knowledge discovery workflow...")
            
            # Step 1: Discover knowledge from documents
            print("\n Step 1: Discovering knowledge from documents...")
            result = system.discover_knowledge_from_documents()
            
            if result["status"] == "success":
                print(f" Discovery completed successfully:")
                print(f"    Documents: {result['discovery_stats']['documents_processed']}")
                print(f"    Chunks: {result['discovery_stats']['chunks_analyzed']}")
                print(f"    Concepts: {result['discovery_stats']['concepts_discovered']}")
                print(f"    Relationships: {result['discovery_stats']['relationships_discovered']}")
                print(f"    Graph Nodes: {result['discovery_stats']['graph_nodes_created']}")
                print(f"    Vector Embeddings: {result['discovery_stats']['vector_embeddings_created']}")
                print(f"    Agent Analyses: {result['discovery_stats']['agent_analyses']}")
                print(f"    Reflection Cycles: {result['discovery_stats']['reflection_cycles']}")
                
                print(f"\n Discovered Concept Types:")
                for concept_type, concepts in result.get("discovered_concepts", {}).items():
                    print(f"   - {concept_type}: {len(concepts)} concepts")
            else:
                print(f" Discovery completed with warnings: {result.get('message', '')}")
            
            # Step 2: Export discovery report
            print("\n Step 2: Exporting discovery report...")
            report_file = system.export_discovery_report()
            print(f" Report exported to: {report_file}")
            
            # Step 3: Save discovery artifacts
            print("\n Step 3: Saving discovery artifacts...")
            artifacts = system.save_discovery_artifacts()
            print(" Artifacts saved:")
            for artifact_type, file_path in artifacts.items():
                if artifact_type != "error":
                    print(f"    {artifact_type}: {file_path}")
            
            print("\n Complete dynamic knowledge discovery workflow finished!")
            print(f" Check the output directory: {GLOBAL_CONFIG['OUTPUT_PATH']}")
            print(" All concepts discovered from actual document content - no assumptions made")
            return
        
        # Execute individual operations
        if args.discover is not None:
            print(" Discovering knowledge from documents...")
            result = system.discover_knowledge_from_documents(args.discover if args.discover else None)
            print(f" Discovery completed: {result['status']}")
            if "discovery_stats" in result:
                stats = result["discovery_stats"]
                print(f"   Concepts discovered: {stats['concepts_discovered']}")
                print(f"   Relationships: {stats['relationships_discovered']}")
        
        if args.search:
            print(f" Searching discovered knowledge: '{args.search}'")
            results = system.search_discovered_knowledge(args.search, args.search_type)
            print(f" Search results:")
            print(f"   Vector matches: {len(results['vector_results'])}")
            print(f"   Graph relationships: {len(results['graph_results'])}")
            if results.get("combined_insights"):
                print(f"   Insights generated: ")
        
        if args.export_report:
            print(" Exporting discovery report...")
            report_file = system.export_discovery_report()
            print(f" Report exported to: {report_file}")
        
        if args.save_artifacts:
            print(" Saving discovery artifacts...")
            artifacts = system.save_discovery_artifacts()
            print(f" Artifacts saved: {len(artifacts)} files")
    
    except Exception as e:
        print(f" System error: {e}")
        logger.error(f"System error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
